[
    {
        "id": "2411.18279",
        "title": "Large Language Model-Brained GUI Agents: A Survey",
        "url": "https://arxiv.org/pdf/2411.18279",
        "abstract": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-27",
        "pub_date_card": {
            "ru": "27 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
            "en": "November 27",
            "zh": "11æœˆ27æ—¥"
        },
        "hash": "ad1bbf8a8e8b79d9",
        "authors": [
            "Chaoyun Zhang",
            "Shilin He",
            "Jiaxu Qian",
            "Bowen Li",
            "Liqun Li",
            "Si Qin",
            "Yu Kang",
            "Minghua Ma",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "affiliations": [
            "Microsoft AI, Microsoft, China",
            "M365 Research, Microsoft, USA",
            "Shanghai Artificial Intelligence Laboratory, China",
            "Peking University, China"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.18279.jpg",
        "data": {
            "categories": [
                "#benchmark",
                "#dataset",
                "#agents",
                "#multimodal",
                "#survey"
            ],
            "emoji": "ğŸ¤–",
            "ru": {
                "title": "LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²",
                "desc": "Ğ­Ñ‚Ğ¾ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ’ Ğ½ĞµĞ¹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ, ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑÑ‚Ğ¸Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¸, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ÑĞ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ“ĞŸĞ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ğ¸ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ¿Ğ»Ğ°Ğ½ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸."
            },
            "en": {
                "title": "Revolutionizing GUI Interaction with LLM Agents",
                "desc": "This paper explores the rise of LLM-brained GUI agents, which leverage large language models to automate interactions with graphical user interfaces. These agents can understand natural language commands and perform complex tasks across various platforms, enhancing user experience significantly. The survey covers the historical development, essential components, and advanced techniques related to these agents, while also addressing key research questions and identifying gaps in the current knowledge. By providing a structured overview, the paper aims to guide future research and practical applications in this rapidly evolving field."
            },
            "zh": {
                "title": "LLMé©±åŠ¨çš„GUIä»£ç†ï¼šé©æ–°ç”¨æˆ·äº¤äº’ä½“éªŒ",
                "desc": "æœ¬è®ºæ–‡æ¢è®¨äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†çš„æœ€æ–°è¿›å±•ã€‚è¿™äº›ä»£ç†èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶è‡ªåŠ¨æ‰§è¡Œå¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ï¼Œæå¤§åœ°æå‡äº†ç”¨æˆ·ä¸è½¯ä»¶çš„äº¤äº’ä½“éªŒã€‚è®ºæ–‡è¿˜åˆ†æäº†GUIä»£ç†çš„å†å²æ¼”å˜ã€æ ¸å¿ƒç»„ä»¶å’Œå…ˆè¿›æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚é€šè¿‡å¯¹ç°æœ‰æ¡†æ¶å’Œè¯„ä¼°æŒ‡æ ‡çš„æ¢è®¨ï¼Œæœ¬æ–‡ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›äº†å®è´µçš„æŒ‡å¯¼ã€‚"
            }
        }
    },
    {
        "id": "2411.15124",
        "title": "TÃœLU 3: Pushing Frontiers in Open Language Model Post-Training",
        "url": "https://arxiv.org/pdf/2411.15124",
        "abstract": "Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\n3 approach to more domains.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-22",
        "pub_date_card": {
            "ru": "22 Ğ½Ğ¾ÑĞ±Ñ€Ñ",
            "en": "November 22",
            "zh": "11æœˆ22æ—¥"
        },
        "hash": "18e2114fabdb2867",
        "authors": [
            "Nathan Lambert",
            "Jacob Morrison",
            "Valentina Pyatkin",
            "Shengyi Huang",
            "Hamish Ivison",
            "Faeze Brahman",
            "Lester James V. Miranda",
            "Alisa Liu",
            "Nouha Dziri",
            "Shane Lyu",
            "Yuling Gu",
            "Saumya Malik",
            "Victoria Graf",
            "Jena D. Hwang",
            "Jiangjiang Yang",
            "Ronan Le Bras",
            "Oyvind Tafjord",
            "Chris Wilhelm",
            "Luca Soldaini",
            "Noah A. Smith",
            "Yizhong Wang",
            "Pradeep Dasigi",
            "Hannaneh Hajishirzi"
        ],
        "affiliations": [
            "Allen Institute for AI",
            "University of Washington"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
        "data": {
            "categories": [
                "#rl",
                "#training",
                "#optimization",
                "#rlhf",
                "#open_source",
                "#benchmark",
                "#dataset",
                "#data"
            ],
            "emoji": "ğŸ§ ",
            "ru": {
                "title": "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹",
                "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ T\"ULU 3 - ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€Ğ¾ÑˆĞµĞ´ÑˆĞ¸Ñ… Ğ¿Ğ¾ÑÑ‚-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, ĞºĞ¾Ğ´ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ´Ğ»Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. T\"ULU 3 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Direct Preference Optimization Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Reinforcement Learning with Verifiable Rewards. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ½Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ²ÑˆĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ."
            },
            "en": {
                "title": "Unlocking Language Model Potential with T\"ULU 3",
                "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It addresses the lack of transparency in post-training techniques by providing open access to training data, code, and methodologies. The models utilize advanced training methods such as supervised finetuning, Direct Preference Optimization, and a new approach called Reinforcement Learning with Verifiable Rewards. T\"ULU 3 not only surpasses the performance of several proprietary models but also offers a comprehensive framework for evaluating and adapting post-training techniques across various applications."
            },
            "zh": {
                "title": "T\"ULU 3ï¼šå¼€æ”¾çš„åè®­ç»ƒæ¨¡å‹æ–°çºªå…ƒ",
                "desc": "æœ¬æ–‡ä»‹ç»äº†T\"ULU 3ï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾çš„æœ€æ–°åè®­ç»ƒæ¨¡å‹ç³»åˆ—ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºå’ŒæŠ€èƒ½ã€‚æˆ‘ä»¬æä¾›äº†è®­ç»ƒæ•°æ®ã€ä»£ç å’Œè®­ç»ƒé…æ–¹ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨åè®­ç»ƒæŠ€æœ¯ã€‚T\"ULU 3åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰çš„é—­æºæ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶ä¼˜è¶Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°è¯„ä¼°åè®­ç»ƒé…æ–¹çš„æ•ˆæœã€‚"
            }
        }
    }
]