[
    {
        "id": "2411.10109",
        "title": "Generative Agent Simulations of 1,000 People",
        "url": "https://arxiv.org/abs/2411.10109",
        "abstract": "The promise of human behavioral simulation--general-purpose computational\nagents that replicate human behavior across domains--could enable broad\napplications in policymaking and social science. We present a novel agent\narchitecture that simulates the attitudes and behaviors of 1,052 real\nindividuals--applying large language models to qualitative interviews about\ntheir lives, then measuring how well these agents replicate the attitudes and\nbehaviors of the individuals that they represent. The generative agents\nreplicate participants' responses on the General Social Survey 85% as\naccurately as participants replicate their own answers two weeks later, and\nperform comparably in predicting personality traits and outcomes in\nexperimental replications. Our architecture reduces accuracy biases across\nracial and ideological groups compared to agents given demographic\ndescriptions. This work provides a foundation for new tools that can help\ninvestigate individual and collective behavior.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-15",
        "pub_date_card": {
            "ru": "15 ноября",
            "en": "November 15",
            "zh": "11月15日"
        },
        "hash": "295021638bf121fb",
        "authors": [
            "Joon Sung Park",
            "Carolyn Q. Zou",
            "Aaron Shaw",
            "Benjamin Mako Hill",
            "Carrie Cai",
            "Meredith Ringel Morris",
            "Robb Willer",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "affiliations": [
            "Computer Science Department, Stanford University",
            "Department of Communication Studies, Northwestern University",
            "Department of Communication, University of Washington",
            "Department of Sociology, Stanford University",
            "Google DeepMind"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.10109.jpg",
        "data": {
            "categories": [
                "#agi",
                "#agents",
                "#ethics",
                "#science"
            ],
            "emoji": "🤖",
            "ru": {
                "title": "Виртуальные двойники: моделирование человеческого поведения с помощью ИИ",
                "desc": "Статья представляет новую архитектуру агентов, имитирующих поведение реальных людей на основе анализа их интервью с помощью больших языковых моделей. Агенты способны воспроизводить ответы участников в социологических опросах с точностью, сравнимой с повторными ответами самих людей. Архитектура демонстрирует меньшую предвзятость по расовым и идеологическим признакам по сравнению с агентами, основанными только на демографических данных. Это исследование закладывает основу для новых инструментов изучения индивидуального и коллективного поведения."
            },
            "en": {
                "title": "Simulating Human Behavior with Generative Agents",
                "desc": "This paper introduces a new type of computational agent designed to simulate human behavior by using large language models. The agents are based on qualitative interviews from 1,052 individuals, allowing them to replicate real human attitudes and behaviors. The study shows that these generative agents can accurately mimic responses from the General Social Survey, achieving an 85% accuracy rate compared to individuals' self-reports. Additionally, the architecture minimizes biases related to race and ideology, paving the way for innovative tools in social science and policymaking."
            },
            "zh": {
                "title": "模拟人类行为的新工具",
                "desc": "本文介绍了一种新型的代理架构，能够模拟1052个真实个体的态度和行为。通过对他们生活的定性访谈应用大型语言模型，研究者测量了这些代理在多大程度上能够复制所代表个体的态度和行为。结果显示，这些生成代理在社会调查中的回答准确率达到85%，与参与者在两周后自我回答的准确率相当。该架构在减少不同种族和意识形态群体之间的准确性偏差方面表现良好，为研究个体和集体行为提供了新的工具基础。"
            }
        }
    },
    {
        "id": "2411.15594",
        "title": "A Survey on LLM-as-a-Judge",
        "url": "https://arxiv.org/abs/2411.15594",
        "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-23",
        "pub_date_card": {
            "ru": "23 ноября",
            "en": "November 23",
            "zh": "11月23日"
        },
        "hash": "5b29ec51248581eb",
        "authors": [
            "Jiawei Gu",
            "Xuhui Jiang",
            "Zhichao Shi",
            "Hexiang Tan",
            "Xuehao Zhai",
            "Chengjin Xu",
            "Wei Li",
            "Yinghan Shen",
            "Shengjie Ma",
            "Honghao Liu",
            "Yuanzhuo Wang",
            "Jian Guo"
        ],
        "affiliations": [
            "Department of Civil and Environmental Engineering, Imperial College London",
            "Gaoling School of Artificial Intelligence, Renmin University of China",
            "IDEA Research, International Digital Economy Academy",
            "Institute of Computing Technology, Chinese Academy of Sciences"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.15594.jpg",
        "data": {
            "categories": [
                "#ethics",
                "#benchmark",
                "#data",
                "#survey"
            ],
            "emoji": "⚖️",
            "ru": {
                "title": "LLM как беспристрастный судья: путь к надежной оценке",
                "desc": "Эта статья представляет собой обзор использования больших языковых моделей (LLM) в качестве оценщиков для сложных задач. Авторы исследуют стратегии повышения надежности таких систем, включая улучшение согласованности оценок и снижение предвзятости. В работе предлагаются методологии для оценки надежности LLM-as-a-Judge систем и представлен новый бенчмарк для этой цели. Статья также рассматривает практические применения, проблемы и будущие направления развития этой быстро развивающейся области."
            },
            "en": {
                "title": "Building Reliable LLM-as-a-Judge Systems for Consistent Evaluations",
                "desc": "This paper discusses the use of Large Language Models (LLMs) as evaluators, termed 'LLM-as-a-Judge', which can provide consistent and scalable assessments across various tasks. It highlights the challenges of ensuring the reliability of these systems, including issues of bias and variability in evaluations. The authors propose strategies to enhance the reliability of LLM-as-a-Judge systems and introduce a novel benchmark for evaluating their performance. This comprehensive survey aims to guide researchers and practitioners in developing effective and trustworthy LLM-based evaluation systems."
            },
            "zh": {
                "title": "LLM：评估的未来选择",
                "desc": "本文探讨了大型语言模型（LLM）作为评估工具的应用，称为“LLM作为评判者”。LLM能够处理多种数据类型，提供可扩展、经济且一致的评估，成为传统专家评估的有力替代方案。尽管如此，确保LLM作为评判者系统的可靠性仍然是一个重大挑战，需要精心设计和标准化。本文还提出了提高可靠性的策略，并介绍了一种新颖的基准测试方法，以评估这些系统的可靠性。"
            }
        }
    }
]