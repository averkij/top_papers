[
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ —Å–∏—Ö –ø–æ—Ä –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –≠—Ç–∏ —É—Å–∏–ª–∏—è –ø—Ä–∏–≤–µ–ª–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –í —Ö–æ–¥–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ \"–ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–¥–∞—é—â–µ–π—Å—è —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ- –∏ –Ω–∏–∑–∫–æ-—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏. –°–æ–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∏ —É—Å–∏–ª–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Large Language Models, LLMs), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3, GPT-4, PaLM –∏ LLaMA, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Natural Language Processing, NLP), –ø–æ–∫–∞–∑—ã–≤–∞—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –±–ª–∞–≥–æ–¥–∞—Ä—è —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤, –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∞–Ω–≥–ª–∏–π—Å–∫–æ–º. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. </p>\n<p>–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–æ–≤ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –∏—Ö –Ω–æ—Å–∏—Ç–µ–ª–µ–π –≤–Ω–µ –≤—ã–≥–æ–¥ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –ß—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–æ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å, –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>Marco-LLM —Å—Ç—Ä–µ–º–∏—Ç—Å—è –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP. –î–ª—è —ç—Ç–æ–≥–æ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2. –≠—Ç–∞ –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, —á—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Marco-LLM, –∫–æ—Ç–æ—Ä–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMMLU, Flores, Belebele, AGIEval, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–î–∞–ª—å–Ω–µ–π—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–±–æ—Ä–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —ç—Ç–∞–ø—ã –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Cross-Lingual Enhancement",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Å –æ–±–∑–æ—Ä–æ–º –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —É–∫–∞–∑–∞–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å, F1-–æ—Ü–µ–Ω–∫–∞ –∏–ª–∏ BLEU). –í —Ç–∞–±–ª–∏—Ü–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —è–∑—ã–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Ç–µ—Å—Ç—ã, –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:</p>\n<ol>\n<li>\n<p><strong>ARC (AI2 Reasoning Challenge)</strong> ‚Äì –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞—É–∫–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ—Ç –º–æ–¥–µ–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π. –í–æ–ø—Ä–æ—Å—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ –¥–≤–∞ —Ç–∏–ø–∞: –ø—Ä–æ—Å—Ç—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏ —Å–ª–æ–∂–Ω—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</p>\n</li>\n<li>\n<p><strong>Belebele</strong> ‚Äì —ç—Ç–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á—Ç–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞ 122 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ —Ç–∏–ø—ã. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ—Ç—Ä—ã–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞, –∑–∞–ø—Ä–æ—Å–∞ –∏ —á–µ—Ç—ã—Ä—ë—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞.</p>\n</li>\n<li>\n<p><strong>CEVAL</strong> ‚Äì —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏–º–µ–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>Flores</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –í–∫–ª—é—á–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö.</p>\n</li>\n<li>\n<p><strong>HellaSwag</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –≤—ã–≤–æ–¥—É. –°–æ–¥–µ—Ä–∂–∏—Ç 70000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≥–¥–µ –æ–¥–∏–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏ —Ç—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>MMLU (Massive Multitask Language Understanding)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–Ω–∞–Ω–∏–π –∏ –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —É –ò–ò-–º–æ–¥–µ–ª–µ–π. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 57 –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 57000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>TyDiQA (Typologically Diverse Question Answering)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –°–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 200000 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –≤–∑—è—Ç—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏.</p>\n</li>\n</ol>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ç–µ—Å—Ç–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ —Ç–µ—Å—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 7B –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ª—É—á—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–≥–æ –º–æ–¥–µ–ª—å—é Qwen2-7B (81.8). –ù–∞ Belebele, –æ—Ü–µ–Ω–∏–≤–∞—é—â–µ–º –∑–Ω–∞–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, Marco-Chat-7B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, —Ç–∞–∫–∂–µ –æ–ø–µ—Ä–µ–∂–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 70B –ª–∏–¥–µ—Ä–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Marco-72B, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 76.1 –Ω–∞ MMMLU, —á—Ç–æ –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –≤—ã—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ Llama3.1-70B. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –æ–±—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å–∫—Ä–∏–ø—Ç–æ–≤, Marco-72B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –ª—É—á—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–≥–æ –º–µ—Å—Ç–∞. –¢–∞–∫–∂–µ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –Ω–∞ CEval (94.5) –∏ Belebele (89.6), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</p>\n<p>–¢–∞–±–ª–∏—Ü–∞ 9 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MMMLU –¥–ª—è 7B –∏ 70B –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å Marco-Chat-7B –ª–∏–¥–∏—Ä—É–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ –≤—Å–µ–º —è–∑—ã–∫–∞–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 7B, –∞ –º–æ–¥–µ–ª—å Marco-72B ‚Äî –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 70B.</p>"
            }
        ]
    },
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–æ–≤–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É—Å–∏–ª–∏–π –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –ë–ª–∞–≥–æ–¥–∞—Ä—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–º –æ—Ü–µ–Ω–∫–∞–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –ª—é–±—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö \"–ª—é–±–æ–π-–∫-–ª—é–±–æ–º—É\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Ç–µ–º —Å–∞–º—ã–º —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ —Å –≤—ã—Å–æ–∫–∏–º–∏ –∏ –Ω–∏–∑–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –û–±—ä–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∞ —Ä–∞–±–æ—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ –≤ –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024] –∏ LLaMA [Touvron et al., 2023b], –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –≤—ã—Å–æ–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –≠—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ —Ä–∞–∑—Ä—ã–≤—É –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –∏–∑-–∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –Ø–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≥–æ–≤–æ—Ä—è—â–∏–µ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≤ –≤—ã–≥–æ–¥–∞—Ö, –ø—Ä–∏–Ω–æ—Å–∏–º—ã—Ö –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP.</p>\n<p>–î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. Marco-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2 [Bai et al., 2023].</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ú–∞—Å—Å–æ–≤–æ–µ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ Marco-LLM.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<p>–†–∞–∑–¥–µ–ª 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö LLM –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è LLM.\n–†–∞–∑–¥–µ–ª 3 –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –º–æ–Ω–æ- –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏.\n–†–∞–∑–¥–µ–ª 4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Marco-LLM, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ —Ä–∞–∑–¥–µ–ª–µ 3.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Marco-LLM",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö —Å –º–æ–¥–µ–ª—è–º–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy), –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å F1 –∏–ª–∏ BLEU. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –æ—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.</p>\n<p>–í —Ä–∞–∑–¥–µ–ª–µ –æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ:</p>\n<ol>\n<li><strong>CEVAL</strong>: –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —è–∑—ã–∫–æ–≤—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞.</li>\n<li><strong>AGIEval</strong>: –í–æ–ø—Ä–æ—Å—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ GRE, GMAT, LSAT –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–∫–∑–∞–º–µ–Ω—ã.</li>\n<li><strong>ARC</strong> (AI2 Reasoning Challenge): –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –ø–æ—á—Ç–∏ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –Ω–∞—É—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —É—Ä–æ–≤–Ω—è —Å—Ä–µ–¥–Ω–µ–π —à–∫–æ–ª—ã. –í–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: –ø—Ä–æ—Å—Ç–æ–π –∏ —Å–ª–æ–∂–Ω—ã–π, —Ç—Ä–µ–±—É—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π.</li>\n</ol>\n<p>–î–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ:</p>\n<ol>\n<li><strong>X-MMLU</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XCOPA</strong>: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ã–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XStoryCloze</strong>: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –∏—Å—Ç–æ—Ä–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n</ol>\n<p>–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li><strong>TyDiQA</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ó–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.</li>\n</ol>\n<p>–ù–∞–∫–æ–Ω–µ—Ü, –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:</p>\n<ol>\n<li><strong>Flores</strong>: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∏ –Ω–∏–∑–∫–æ —Ä–µ—Å—É—Ä—Å–Ω—ã–µ —è–∑—ã–∫–∏.</li>\n</ol>\n<p>–≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ò–ò –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —è–∑—ã–∫–∞—Ö, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>\n<p>–ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–∞–∑–º–µ—Ä–æ–º –≤ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å Marco-Chat-7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Äî Qwen2-7B (81.8). –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–∏–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –Ω–∞ Belebele, –≥–¥–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤–ª–∞–¥–µ–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏, Marco-Chat-7B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—á—Ç–∏ –Ω–∞ 9 –ø—É–Ω–∫—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º 70 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤, –º–æ–¥–µ–ª—å Marco-72B —Ç–∞–∫–∂–µ –ª–∏–¥–∏—Ä—É–µ—Ç –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É 76.1 –Ω–∞ MMMLU, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –≠—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –ª—É—á—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏, Llama3.1-70B. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, Marco-72B –Ω–∞–±–∏—Ä–∞–µ—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–∏–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ —è–∑—ã–∫–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –∏ –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-72B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 94.5 –Ω–∞ CEval, –ø–æ–∫–∞–∑—ã–≤–∞—è –æ—Ç–ª–∏—á–Ω—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –ù–∞ Belebele –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ 89.6, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —è–∑—ã–∫–∞—Ö –ê—Ñ—Ä–∏–∫–∏.</p>"
            }
        ]
    },
    {
        "id": "2412.04363",
        "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
        "url": "https://huggingface.co/papers/2412.04363",
        "abstract": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "4fa16cf75a2af540",
        "authors": [
            "Wenting Zhao",
            "Alexander M. Rush",
            "Tanya Goyal"
        ],
        "affiliations": [],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04363.jpg",
        "data": {
            "categories": [
                "#rlhf",
                "#data",
                "#security",
                "#benchmark",
                "#ethics"
            ],
            "emoji": "üé≠",
            "ru": {
                "title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–≥—É—Ç –∏—Å–∫–∞–∑–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –ò–ò!",
                "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–∞–∂–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–ª–æ—Ö–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
            },
            "en": {
                "title": "Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings",
                "desc": "This paper examines the reliability of user-generated annotations on community-driven platforms like Chatbot Arena, which are used to evaluate the performance of large language models (LLMs). It identifies three main sources of poor-quality annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other malicious behaviors. The authors demonstrate that even a small percentage of these low-quality votes can significantly impact the rankings of models, potentially shifting them by several places. The paper concludes by highlighting the ongoing challenges in maintaining high-quality human annotations to ensure trustworthy leaderboard results."
            },
            "zh": {
                "title": "Á°Æ‰øùÈ´òË¥®ÈáèÊ≥®ÈáäÔºåÊèêÂçáÊ®°ÂûãÊéíÂêçÁöÑÂèØÈù†ÊÄß",
                "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂºÄÊîæÁ§æÂå∫Âπ≥Âè∞ÔºàÂ¶ÇChatbot ArenaÔºâÂú®Êî∂ÈõÜÁî®Êà∑ÂÅèÂ•ΩÊï∞ÊçÆÊó∂Èù¢‰∏¥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÊù•Ëá™Êó†Âä®Êú∫ÊàñÊÅ∂ÊÑèÁî®Êà∑ÁöÑ‰ΩéË¥®ÈáèÊ≥®Èáä‰ºö‰∏•ÈáçÂΩ±ÂìçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊéíÂêç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ªÖ10%ÁöÑ‰ΩéË¥®ÈáèÊäïÁ•®Â∞±ÂèØËÉΩÂØºËá¥Ê®°ÂûãÂú®ÊéíË°åÊ¶ú‰∏äÊéíÂêçÂèòÂåñÂ§öËææ5‰Ωç„ÄÇÊúÄÂêéÔºåÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁ°Æ‰øùÈ´òË¥®Èáè‰∫∫Á±ªÊ≥®ÈáäÁöÑÂºÄÊîæÊÄßÊåëÊàò„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.",
                "summary": "<p>–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å–∞–π—Ç–∞, –∑–∞–≤–æ–µ–≤–∞–ª–∏ —Ä–µ–ø—É—Ç–∞—Ü–∏—é –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM. –•–æ—Ç—è —Å–µ–π—á–∞—Å —ç—Ç–æ —Å—Ç–∞–ª–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º, —Å–ª–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –æ—Ç –ª—é–¥–µ–π. –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º, —á—Ç–æ —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –∫–∞–∫ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –¥—Ä—É–≥–∏—Ö, –º–æ–≥—É—Ç –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –ª–∏–¥–µ—Ä–æ–≤. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö (–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏ —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –¥–∞–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞) –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö (–∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏, —Å—Ç—Ä–µ–º—è—â–∏–µ—Å—è –∫ —Ä–∞–∑–¥—É–≤–∞–Ω–∏—é —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏) –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –¥–æ 5 –º–µ—Å—Ç –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ù–∞–∫–æ–Ω–µ—Ü, –º—ã –æ–±—Å—É–∂–¥–∞–µ–º –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.</p>"
            },
            {
                "title": "Introduction",
                "content": "Reliable evaluation of free-form text generation quality is long-standing challenge in NLP (Gehrmann et al., 2023; Celikyilmaz et al., 2020; Goyal et al., 2022a). Despite limitations, human annotation is widely accepted as the gold standard, especially for open-ended text generation tasks without an objective notion of correctness. As result, platforms such as Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024b) and WildVision Arena (Lu et al., 2024) that allow users to interact with available large language models (LLMs) and submit preference judgments for model pairs, have become extremely valuable resource in the NLP evaluation landscape. By providing free and easy access to available LLMs, these community-driven platforms are able to incentivize millions of user interactions1 and collect large-scale and diverse dataset of user queries and preferences. Deservedly, 1As of October 6, 2024, Chatbot Arena has collected 2,011,939 pairwise preference judgments. these peer production and community-driven platforms have emerged as one of the most trusted benchmarks in NLP today.2 Moreover, such benchmarks play crucial role in auditing automatic evaluators by providing the necessary ground truth rankings that any evaluator can be validated against. In fact, the most popular automatic evaluation benchmarks today, including AlpacaEval (Li et al., 2023), WildBench (Lin et al., 2024), MixEval (Ni et al., 2024) and Arena-Hard (Li et al., 2024), validate their metric by reporting high correlation with Chatbot Arena judgments. Given its far-reaching impact, both on human and automatic benchmarking of LLMs, and consequently on LLM research more broadly, it is crucial to ensure that the model rankings on these open community leaderboards are trustworthy. However, challenges with obtaining high-quality human judgments from non-expert crowdworkers like Chatbot Arena users are widely discussed in literature (Karpinska et al., 2021; Clark et al., 2021; Hosking et al., 2024). Moreover, these platforms typically implement minimal quality controls for verifying annotation quality such as attention checks, user verification, etc. This sits in direct opposition to the goals of trustworthiness. In this paper, we play devils advocate and ask: is it even possible to ensure the reliability of community-driven open platform, like Chatbot Arena, without sacrificing user scale? We approach this thought experiment from two angles. First, using Chatbot Arena as case study, we consider three different sources of poor quality preference judgments or votes in the collected dataset: un-incentivized or apathetic users providing random judgments (Section 3.1), malicious actors launching adversarial attacks to detect and artificially inflate target models ranking (Sec2As an example, Googles Chief Scientist used high performance on Chatbot Arena to declare the success of their recent model release: https://tinyurl.com/55xs2pz4. trol measures employed by Chatbot Arena. Notation Assume there are different models = {m1, m2, ..., mk} that need to be ranked on the leaderboard. Each new user on the platform submits query and receives outputs from two different models yi mi(x) and yj mj(x).3 The user has the option to submit preference label {i, j, tie}. In order to ensure that this annotation is unbiased, the names of the models that the two outputs are sampled from is only revealed to end users after they have submitted their preference annotation. This arena logs data points of the form: (x, yi, yj, mi, mj, l). These preferences are then used to estimate the pairwise win matrix between model pairs, i.e. p(mi > mj). Next, they estimate the coefficients of the Bradley-Terry model (Bradley and Terry, 1952) to obtain scores si for each model mi M. Models are sorted by si to obtain the final ranking. Quality control measures The arena employs list of filtering strategies: detecting malicious users according to certain distribution (Section 5.1; Chiang et al. (2024a)), bot detection by Cloudflare and Google reCAPTCHA v3, automatic categorization pipelines to filter out low-quality data45, placing limits on the number of votes each IP can provide in day, and deduplicating top 0.1% occurring prompts. However, these filtering strategies focus more on filtering bots than differentiating user votes with varying qualities. Therefore, we present results and discussions in this paper assuming minimal quality control checks in the backend to filter out bad quality user annotations6. Released Artifacts We conduct our experiments using the largest publicly released dataset by Chatbot Arena. It consists of 55k preference annotations7; it includes response pairs sampled from two of 64 unique models and the corresponding pairwise preference annotation. 3The arena employs an adaptive sampling strategy that favors model pairs with higher uncertainty in relative performance, and also newly introduced models. However, exact details are not publicly shared, possibly to mitigate gaming. 4https://blog.lmarena.ai/blog/2024/ hard-prompts/ 5https://blog.lmarena.ai/blog/2024/ arena-category/ 6https://github.com/lm-sys/FastChat/ 7https://huggingface.co/datasets/lmsys/ lmsys-arena-human-preference-55k Figure 1: Our characterization of sources of poor-quality votes on open data annotation platforms: (1) Apathetic: Users who lack intrinsic motivation may submit random votes. (2) Adversarial: Malicious users aim to manipulate rankings by upvoting target model. (3) Arbitrary: Users voting based on subjective preferences in response to open-ended questions. tion 3.2), and the inherent arbitrariness of preference votes for open-ended and subjective queries (Section 3.3). For the former two sources of votes, we show that small fractions of poor-quality judgments (either apathetic or adversarial) can have non-trivial impact on the target models rankings. Concerningly, poor annotations from either apathetic or adversarial voting are not easy to detect in post-hoc manner. Moreover, even carefully recruited and onboarded human annotators exhibit low inter-annotator agreement on subjective queries, making inter-annotator-based techniques to filter out low-quality annotations ineffective. Finally, we discuss open challenges in ensuring the reliability and human annotation quality in open-source community-driven benchmarks (Section 4). We strongly believe that open data collection platforms offer an invaluable resource for the academic community and have facilitated essential work in developing new automatic evaluators (Li et al., 2023; Lin et al., 2024; Ni et al., 2024), training and evaluating reward models (Lambert et al., 2024), etc. However, critical questions exist about their reliability, especially against adversarial attacks. We hope that our work will spur future research on quality control mechanisms for open platforms that power LLM evaluations.",
                "summary": "<p>–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –¥–∞–≤–Ω–µ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena –∏ WildVision Arena, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π, —Å—Ç–∞–ª–∏ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –≠—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–≤–æ–±–æ–¥–Ω—ã–π –∏ –ª–µ–≥–∫–∏–π –¥–æ—Å—Ç—É–ø –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –º–æ–¥–µ–ª—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¢–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å–µ–≥–æ–¥–Ω—è —è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–∏–º–∏ –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–æ–≤ –≤ NLP –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –ø—É—Ç–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö –æ—Ü–µ–Ω–æ–∫. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç –Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ Chatbot Arena. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –æ–±—ã—á–Ω–æ —Ä–µ–∞–ª–∏–∑—É—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ä—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —á—Ç–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ü–µ–ª—è–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–∞–∫–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –∫–∞–∫ Chatbot Arena, –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</p>"
            },
            {
                "title": "Background",
                "content": "In this paper, we run experiments with Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024a) as case study, although our insights are broadly applicable to other similar community-driven preference collection platforms. Below, we describe the preference collection pipeline and quality conModel Leaderboard Ranking Orig. r=1 r= r=10 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 36 21 39 382 201 412 382 21 345 41 existing studies characterizing the incentives or behaviors of an average user on open platforms like Chatbot Arena. Therefore, we have no way of estimating the fraction of apathetic. Table 1: Change in leaderboard rankings for 3 test models based on different percentages (r) of arbitrary votes. The subscripts denote gain () or loss () in rankings. We find that only 10% poor quality annotations can change the rank of 2/3 systems by 5 places.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π Chatbot Arena –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞, —Ö–æ—Ç—è –Ω–∞—à–∏ –≤—ã–≤–æ–¥—ã –ø—Ä–∏–º–µ–Ω–∏–º—ã –∏ –∫ –¥—Ä—É–≥–∏–º –ø–æ–¥–æ–±–Ω—ã–º –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º. –ù–∏–∂–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ú—ã –æ—Ç–º–µ—á–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–∏—Ö —Å—Ç–∏–º—É–ª—ã –∏–ª–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ü–æ—ç—Ç–æ–º—É —É –Ω–∞—Å –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∏—Ç—å –¥–æ–ª—é –±–µ–∑—Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –¢–∞–±–ª–∏—Ü–µ 1 –ø–æ–∫–∞–∑–∞–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ –¥–ª—è —Ç—Ä–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (r) –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ü–æ–¥—Å—Ç—Ä–æ—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –æ–±–æ–∑–Ω–∞—á–∞—é—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ (+) –∏–ª–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ (-) –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ú—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ 2/3 —Å–∏—Å—Ç–µ–º –Ω–∞ 5 –ø–æ–∑–∏—Ü–∏–π.</p>"
            },
            {
                "title": "Votes and Their Impact",
                "content": "For our thought experiment, we hypothesize that there exist three potential sources of poor quality votes on open platforms: (a) apathetic votes by users that are un-incentivized, (b) adversarial votes that aim to inflate the ranking of target model, and (c) arbitrary votes on difficult to meaningfully distinguish response pairs. For each of these, we study their impact on model rankings and the challenges in mitigating them. 3.1 Apathetic Voting The main attraction of open community platforms for end users is that they expose free and easy-touse API endpoint for LLMs. This incentivizes diverse users to interact with the platform and submit queries to explore their use cases. However, these platforms do not explicitly incentivize high-quality preference annotation. We hypothesize that at least r% of users on the arena are apathetic and provide random or low-quality votes on the platform. Setup We run experiments on Chatbot Arenas dataset of 55k preferences (discussed in Section 2). We assume that this dataset reflects true rankings of models based on gold human preferences. We study the change in model rankings for 3 arbitrarily selected models: Llama-2-7b-chat, Mistral-7binstruct-v0.2, and Llama-2-13b-chat, assuming r% of these preferences were instead assigned random labels by apathetic users during data collection. Results Table 1 summarizes our results. We find that only 10% of apathetic votes in the dataset can change the leaderboard rankings of 2/3 models by 5 places (namely Llama-2-13b-chat and Mistral-7b-instruct-v0.2).8 Note that there are no 8Note that model frequency also impacts its susceptibility to ranking changes. All three models we inspect collectively occur in less than 10% of all data samples. Discussion: Can we detect and remove apathetic votes? major challenge in detecting apathetic votes is that they are often indistinguishable from arbitrary votes. Multiple past studies have found that output-level comparisons using single label is ill-defined as an annotation task (Krishna et al., 2023; Goyal et al., 2022a) as users often rely on different criteria and disagree with each other. This ambiguity makes it hard to ascertain whether observed disagreements are due to personal variations in quality assessment (arbitrary voting, discussed further in Section 3.3) or due to apathetic or low-quality annotations by certain annotators. Despite challenges with detecting individual apathetic votes, detecting apathetic users may be viable by computing agreements between model rankings by individual users. This strategy is based on the intuition that while annotators might disagree on specific examples, their aggregate systemlevel judgments tend to be more aligned (Goyal et al., 2022a). Finally, requesting additional justifications for votes, such as free-text rationales, can also help discourage apathetic votes. We discuss this more in Section 4. 3.2 Adversarial Voting We assume there exists malicious developer who seeks to inflate the rankings of their own target model mT on the arena leaderboard A. We argue that due to the lack of quality controls (e.g. user verification, attention checks, etc.), it is straightforward to inject preference votes for mT using simple attack methodology. Our main component is target model attribution algorithm which, given query-output pair (q, y), predicts whether is sampled from the target model mT (q). Given such an algorithm, we can inflate the ranking of the target model mT using the following strategy: (1) Enter prompt on the arena, (2) Detect if any of the two shown outputs y1, y2 are sampled from mT , (3) If yes, vote for the target model mT , (4) Repeat. Target model attribution algorithm We assume that the attribution algorithm has access to the target model logits. This is reasonable assumption for our setting where model developer seeks to Model Leaderboard Ranking Orig. r=1 r=5 r=10 r=100 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 232 174 21 363 325 289 342 342 297 121 139 234 Table 2: Change in leaderboard rankings for 3 test models based on different percentages (r) of adversarial votes (upvoting the target model). We find that only 10% adversarial annotations can change the rank of all systems by more than 4 places. Model TPR TNR #Tokens Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0. 91.13 88.46 100.00 89.93 91.28 86.69 328.06 326.53 319.46 Table 3: Intrinsic eval. of model attribution algorithm inflate rankings. Our simple attribution algorithm is outlined in Algorithm 1 in Appendix A. Essentially, we use teacher-forcing to determine the probability distribution over the vocabulary for all tokens at time step t, i.e. PmT (.x, y1, ...yt1). We sort the tokens in descending order of probability to identify the smallest subset of tokens that cover cumulative probability mass of at least p. We compute the fraction of generation time steps for which the actual generated token yt falls within this top-p probability subset. We compare this against threshold to classify generations = y1...yN as being sampled from mT or not. Intrinsic Evaluation of Detector Algorithm For all three test models, we report the true positive rate (TPR) and true negative rate (TNR) on the arena dataset in Table 3. We find that our detector algorithm reports very high performance (e.g. TPR=91.13%, and TNR=88.46% for Llama-2-7bchat). We also find positive correlation between the number of tokens and TPRs, which can be leveraged in the attack. Note that malicious actors can always improve the detector accuracy further using watermarking techniques (Kirchenbauer et al., 2023). Next, we use these highly performant models to cast adversarial votes. Can we influence voting on the live Chatbot Arena platform? We also implement proofof-concept of real attack on Chatbot Arena to demonstrate that current guardrails, such as bot detection, can be bypassed easily. On October 13, 2024, we programmatically launched 100 queries into Chatbot Arena, extracted the two model responses, and successfully submitted preference vote. To avoid contaminating the dataset, we only cast tie votes but note that it would be trivial to instead use the vote from the attribution algorithm. Interestingly, post-hoc analysis of this data revealed that yi-lightning family of models, released just 2 days later, were the most common (20% of the responses) in this set.9 We assume that Chatbot Arena had early access to these models and sampled them more frequently than others in order to collect enough votes. However, this knowledge of when particular models will be up-sampled can be easily exploited by adversaries to log large fraction of upvotes for their model. Impact of adversarial voting on leaderboard rankings Similar to Section 3.1, we run experiments on the 55k preference dataset from Chatbot Arena, assumed to reflect \"true\" votes. For 3 target models, we report the change in leaderboard rankings if adversarial voting was conducted on r% of the data samples during data collection. Table 2 summarizes our results. Across all models, we show that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.10. Note that, in this work, we only report results using the most simplistic version of this attack. We can further boost these numbers by not only upvoting the target model but also downvoting open-source competitor models or those ranked higher than the target model in the leaderboard. Discussion: Can we detect and remove adversarial votes? Open platforms can employ two types of mitigation strategies to address this issue: recognizing bot-like behavior to prevent votes from being cast, or detecting abnormal users post-hoc to filter out their votes. Platforms like Chatbot Arena already implement measures from both categories. For example, Chatbot Arena uses Cloudflare and Google reCAPTCHA to detect bots on their platform; however, we were able to bypass both programmatically. We did not find public information indicating that similar measures have been incorporated into the Wildvision Arena platform. There are also opportunities to detect anomalous users post-hoc based on behaviors across multiple 9Evenly distributed between yi-lightning and yi-lightning-lite. 10We assume that adversaries can get 10% votes towards their own model because newly released models will be sampled more frequently. Th Org Re Per WS GPT-3.5 vs GPT-4o 5.51 Llama-3-8b vs Llama-3-70b 10.15 -10.78 Llama-3-8b vs GPT-3.5 Llama-3-70b vs GPT-4o 9.91 17.18 20.06 8.50 5.78 27.16 7.45 3.53 7.19 -12.15 -4.66 2.75 4.56 -1. 11.34 3.15 -0.36 Table 4: Fleiss Kappa between four annotators on different evaluation axis: Th(esis), Org(anization), Re(asoning), Per(spectives), WS (Writing Style). sessions or votes. Chatbot Arena implements version of this strategy by comparing the distribution of ratings from user (uniquely identified by IP address) against historical distributions to identify anomalies. Because committed adversaries may bypass these checks using IP rotation or similar techniques, we encourage further exploration of these approaches to make them more robust. 3.3 Arbitrary Voting We assume an idealized scenario where all users genuinely make their best effort to rank model outputs. However, we argue that holistically rating response to an open-ended and inherently subjective query is ill-defined and liable to always be arbitrary. To demonstrate this, we conduct small-scale annotation study for outputs of subjective Researchy questions prompts (Rosset et al., 2024). Setup We use these prompts and generate generate responses from four language models: Llama3-8B, Llama-3-70B, GPT-4o, and GPT-3.5. We recruit four undergraduate CS students who are passionate about NLP and committed to providing thoughtful annotations. They evaluate responses on four dimensions: thesis, organization, reasoning, perspectives, and writing style. We offer them unlimited time and allow them to seek clarification from the authors when needed. Note that this dimension-wise rating is different from Chatbot Arenas setup of pairwise preferences. However, there already exist multiple prior works that argue that the task is under-defined in this latter setting and report low agreement between annotators (Goyal et al., 2022a,b; Krishna et al., 2023). Therefore, we opt to run this study using more welldefined task description. Results Table 4 shows the inter-annotator agreement between the annotators. Overall, we find very 11Representative question: How can the education system be improved?. low agreement between these well-intentioned annotators with clear guidelines, irrespective of the performance difference between the model pairs. More concerningly, the results highlight that traditional approaches like filtering out low-quality users/annotations using inter-annotator agreement may not be viable strategy for open-ended queries as it is difficult to disentangle between of low interannotator agreement due to bad annotation (apathetic votes) or inherent subjectivity. Adversarial users can also hide their votes from similar scrutiny by using open-ended prompts for which vote choice is expected to be ambiguous. Discussion We argue that arbitrary votes are not noise and provide useful signals about models relative performance. If most frontier models perform similarly well on substantial fraction of real-world queries, this information should not be discarded but inform leaderboard Elo scores. Arbitrary votes become problematic when the majority of the leaderboard is dominated by openended queries that fail to meaningfully distinguish models, despite the existence of legitimate topics or skills along where models exhibit distinct behaviors. Identifying which test examples (or type of test examples) are most informative and up-weighting them when deriving aggregate scores are potential ways of addressing this (Rodriguez et al., 2021).",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ —Ç—Ä–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Å—Ç–∏–º—É–ª–æ–≤, –≤—Ä–∞–∂–¥–µ–±–Ω—ã–µ –≥–æ–ª–æ—Å–∞, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤—ã—à–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –≤ —Å–ª—É—á–∞–µ —Ç—Ä—É–¥–Ω–æ—Ä–∞–∑–ª–∏—á–∏–º—ã—Ö –ø–∞—Ä –æ—Ç–≤–µ—Ç–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Å–º—è–≥—á–µ–Ω–∏–∏ —ç—Ç–æ–≥–æ –≤–ª–∏—è–Ω–∏—è.</p>\n<p><strong>–ê–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –∫–æ–Ω–µ—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–¥–Ω–∞–∫–æ —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —è–≤–Ω–æ –Ω–µ —Å—Ç–∏–º—É–ª–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ r% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –∞–ø–∞—Ç–∏—á–Ω—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∏–ª–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Chatbot Arena, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 55 —Ç—ã—Å—è—á –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ –ª–∏—à—å 10% –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é –¥–≤—É—Ö –∏–∑ —Ç—Ä–µ—Ö –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Llama-2-13b-chat –∏ Mistral-7b-instruct-v0.2) –Ω–∞ –ø—è—Ç—å –º–µ—Å—Ç.</p>\n<p>–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ —Å–ª–æ–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —á–∞—Å—Ç–æ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ –º–æ–¥–µ–ª–µ–π –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –ó–∞–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π –¥–ª—è –≥–æ–ª–æ—Å–æ–≤ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ.</p>\n<p><strong>–í—Ä–∞–∂–¥–µ–±–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫, —Å—Ç—Ä–µ–º—è—â–∏–π—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ —Å–≤–æ–µ–π —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ. –ò–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –º–µ—Ä –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –¥—Ä.) –ª–µ–≥–∫–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≥–æ–ª–æ—Å—É—é—â–∏—Ö –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å.</p>\n<p>–î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –±—ã–ª –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏. –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º:</p>\n<ol>\n<li>–í–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞—Ä–µ–Ω–µ,</li>\n<li>–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–æ–π –∏–∑ –¥–≤—É—Ö –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏,</li>\n<li>–ï—Å–ª–∏ –¥–∞, –ø—Ä–æ–≥–æ–ª–æ—Å–æ–≤–∞—Ç—å –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å,</li>\n<li>–ü–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å.</li>\n</ol>\n<p>–ê–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –ª–æ–≥–∏—Ç–∞–º —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º, –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –º–æ–¥–µ–ª–∏ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–≤—ã—Å–∏—Ç—å –µ–µ —Ä–µ–π—Ç–∏–Ω–≥.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ 10% –≤—Ä–∞–∂–¥–µ–±–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –±–æ–ª–µ–µ —á–µ–º –Ω–∞ —á–µ—Ç—ã—Ä–µ –º–µ—Å—Ç–∞.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–æ –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º.</p>"
            },
            {
                "title": "Conclusion & Future Directions",
                "content": "Our experiments in Section 3 lay convincing case for the need for stronger guardrails in open community-driven platforms. Although these are broadly accepted as the ground truth rankings of LLMs, we are concerned that it is easy to intentionally (adversarial) or unintentionally (apathetic, arbitrary settings) corrupt these leaderboards. The key challenge in mitigating the issue of poor quality annotations is: how can community-driven platforms strike the right balance between implementing necessary quality controls while also providing the right incentives and experience to users to continue to use these platforms. Richer feedback We encourage the community to explore ideas from past research, such as soliciting fine-grained annotations (Krishna et al., 2023; Goyal et al., 2022b) or rationales (McDonnell et al., 2016) in addition to the binary preference feedback. Rationales can be useful in encouraging apathetic users to think more critically about their votes (or abstain) and also for filtering out low-quality annotations from both apathetic and adversarial users. Past work in generation evaluation has discussed how binary preference, or even single Likert rating, for the whole output, cannot meaningfully capture the nuances of human preferences (Gehrmann et al., 2023). Instead, fine-grained preference annotation is recommended, both along multiple dimensions or quality (Gehrmann et al.) or for smaller units within the whole output (Krishna et al., 2023; Goyal et al., 2022b). More recent work proposes providing added context during evaluation to encourage higher agreement between annotators (Malaviya et al., 2024). Future work must explore how these strategies can be incorporated into open platforms without inordinately increasing the annotation burden on users. Stronger Guardrails Other guardrails could include reputation-based systems (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), machine learning based anomaly detection (Kumar et al., 2014; Wu et al., 2016) and techniques that use annotator behavior traces on the platform to estimate quality (Goyal et al., 2018). Open access to collected dataset Public release of the collected data on open platforms will spur research to address the annotation issues we discuss It would provide more detailed in this work. overview into which types of queries are most wellequipped to distinguish between models, and what are the limitations of different families of models.",
                "summary": "<p>–í —Ä–∞–∑–¥–µ–ª–µ 3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏—Ö –º–µ—Ä –∑–∞—â–∏—Ç—ã –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —Å —É—á–∞—Å—Ç–∏–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ —á—Ç–æ —Ç–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω—ã –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–ø–∞—Å–µ–Ω–∏–µ, —á—Ç–æ –∏—Ö –ª–µ–≥–∫–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏–ª–∏ –Ω–µ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, –∫–∞–∫ —Å–æ–æ–±—â–µ—Å—Ç–≤—É –¥–æ—Å—Ç–∏—á—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º –º–æ—Ç–∏–≤–∞—Ü–∏–∏ –∏ —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. </p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–¥–µ–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (Krishna et al., 2023; Goyal et al., 2022b) –∏–ª–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π (McDonnell et al., 2016) –≤–º–µ—Å—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–∏–Ω–∏–º–∞—Ç—å –±–æ–ª–µ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏ –∏ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –ø—Ä–æ—à–ª–æ–º –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–ª–∏ –µ–¥–∏–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –õ–∞–π–∫–µ—Ä—Ç–∞ –¥–ª—è –≤—Å–µ–≥–æ –≤—ã–≤–æ–¥–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç—Ä–∞–∑–∏—Ç—å –Ω—é–∞–Ω—Å—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (Gehrmann et al., 2023). –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞ (Gehrmann et al.) –∏–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π –≤—ã–≤–æ–¥–∞ (Krishna et al., 2023; Goyal et al., 2022b).</p>\n<p>–¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (Kumar et al., 2014; Wu et al., 2016) –∏ —Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (Goyal et al., 2018). –û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –ø–æ–∑–≤–æ–ª–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç –º–æ–¥–µ–ª–∏, –∏ –≤—ã—è–≤–∏—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤ –º–æ–¥–µ–ª–µ–π.</p>"
            }
        ]
    },
    {
        "id": "2412.08905",
        "title": "Phi-4 Technical Report",
        "url": "https://huggingface.co/papers/2412.08905",
        "abstract": "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "5b5d18f4e7e9fad9",
        "authors": [
            "Marah Abdin",
            "Jyoti Aneja",
            "Harkirat Behl",
            "S√©bastien Bubeck",
            "Ronen Eldan",
            "Suriya Gunasekar",
            "Michael Harrison",
            "Russell J. Hewett",
            "Mojan Javaheripi",
            "Piero Kauffmann",
            "James R. Lee",
            "Yin Tat Lee",
            "Yuanzhi Li",
            "Weishung Liu",
            "Caio C. T. Mendes",
            "Anh Nguyen",
            "Eric Price",
            "Gustavo de Rosa",
            "Olli Saarikivi",
            "Adil Salim",
            "Shital Shah",
            "Xin Wang",
            "Rachel Ward",
            "Yue Wu",
            "Dingli Yu",
            "Cyril Zhang",
            "Yi Zhang"
        ],
        "affiliations": [
            "Microsoft Research"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.08905.jpg",
        "data": {
            "categories": [
                "#data",
                "#reasoning",
                "#synthetic",
                "#training",
                "#benchmark",
                "#architecture"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏",
                "desc": ""
            },
            "en": {
                "title": "Elevating Language Models with Quality Data",
                "desc": ""
            },
            "zh": {
                "title": "Êï∞ÊçÆË¥®ÈáèÈ©±Âä®ÁöÑËØ≠Ë®ÄÊ®°Âûãphi-4",
                "desc": ""
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",
                "summary": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ phi-4, —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ì–ª–∞–≤–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å phi-4 ‚Äì —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –∏–≥—Ä–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö\" –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ –∫–æ–¥, phi-4 –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.\n\n–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Phi –≤ –æ—Å–Ω–æ–≤–Ω–æ–º \"–ø–µ—Ä–µ–Ω–∏–º–∞–ª–∏\" —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è (GPT-4). –û–¥–Ω–∞–∫–æ phi-4 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–µ–≥–æ —É—á–∏—Ç–µ–ª—è –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ç–æ—á–Ω—ã–º–∏ –Ω–∞—É–∫–∞–º–∏ (STEM), —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ç–æ–º, —á—Ç–æ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ \"–ø–µ—Ä–µ–Ω–∏–º–∞–Ω–∏—è\" –∑–Ω–∞–Ω–∏–π.\n\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å phi-3, phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–≤–æ–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –±–ª–∞–≥–æ–¥–∞—Ä—è —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö, —É—á–µ–±–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –≤ —Å—Ö–µ–º–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏.\n\n*–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ phi-4 –Ω–µ –ø—Ä–æ—Å—Ç–æ \"—Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª\" —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏, –∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª –∏—Ö –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ –ø—Ä–æ–¥—É–º–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –¥–∞–Ω–Ω—ã–º –∏ –æ–±—É—á–µ–Ω–∏—é.*\n"
            },
            {
                "title": "Introduction",
                "content": "Recent advancements in Large Language Models (LLMs) have shown that significant improvements in data quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling compute with model and dataset size. Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoningfocused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training. Synthetic data constitutes the bulk of the training data for phi-4 and is generated using diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal. These methods enable the construction of datasets that induce stronger reasoning and problem-solving abilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic data in phi-4 also plays crucial role in post-training, where techniques such as rejection sampling and novel approach to Direct Preference Optimization (DPO) are employed to refine the models outputs. The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and 1 Small models Large models phi-4 14b phi-3 14b Qwen 2.5 14b instruct GPT 4o-mini Llama-3.3 70b instruct Qwen 2.5 72b instruct GPT 4o v - m MMLU GPQA MATH HumanEval MGSM SimpleQA DROP MMLUPro HumanEval+ ArenaHard LiveBench IFEval PhiBench (internal) 84.8 56.1 80.4 82.6 80.6 3.0 75.5 70.4 82.8 75.4 47.6 63.0 77.9 31.2 44.6 67.8 53.5 7.6 68.3 51.3 69.2 45.8 28.1 57.9 56. 43.9 79.9 42.9 75.6 72.1 79.6 5.4 85.5 63.2 79.1 70.2 46.6 78.7 49.8 81.8 40.9 73.0 86.2 86.5 9.9 79.3 63.4 82.0 76.2 48.1 80. 58.7 86.3 49.1 66.31 78.91 89.1 20.9 90.2 64.4 77.9 65.5 57.6 89.3 57.1 85.3 49.0 80.0 80.4 87.3 10.2 76.7 69.6 78.4 78.4 55.3 85. 64.6 88.1 50.6 74.6 90.6 90.4 39.4 80.9 73.0 88.0 75.6 57.6 84.8 72.4 Table 1: Performance of phi-4 on set of standard benchmarks. The first set of benchmarks uses OpenAIs simple-evals framework [Ope24b], specifying the prompts/extraction/temperature=0.5. We compare to small models of similar inference cost, as well as to larger models. relevance. We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of phi. 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model). These seeds form the foundation of the synthetic generation pipeline. To complement these synthetic datasets, we also filter the web for high-quality data (in terms of knowledge and reasoning) to use directly in pretraining. 3. Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing new technique to create DPO pairs, based on pivotal token search. With these innovations, the performance of phi-4 on reasoning-related tasks is comparable to or surpasses much larger models. For example, its performance on many widely used reasoning-related benchmarks meets or exceeds that of Llama-3.1-405B. In Table 1 we compare the performance of our model on academic benchmarks to several contemporary foundation models. We find that phi4 significantly exceeds its teacher GPT-4o on the GPQA (graduate-level STEM Q&A) and MATH (math competition) benchmarks. 1These scores are lower than those reported by Meta, perhaps because simple-evals has strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3. 2We use organic to refer to human-generated or otherwise non-synthetic data. Figure 1: Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature = 0.5. We chose = 0.5 to follow simple-evals [Ope24b]. Error bars are 2œÉ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to nonopen-weight models.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å phi-4, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Phi –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —É—Å–ø–µ—Ö–∞ phi-4 —è–≤–ª—è–µ—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞.\n\n**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ phi-4:**\n\n1.  **–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:** –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö phi-4 —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ (multi-agent prompting), —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Å–∞–º–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (self-revision workflows) –∏ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (instruction reversal). –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —Ä–∞–∑–≤–∏—Ç–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –¥–æ–ª—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ–∫–æ–ª–µ–Ω–∏—è–º–∏ Phi.\n\n2.  **–û—Ç–±–æ—Ä –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:** –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Å–Ω–æ–≤—ã –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫–æ–¥–∞. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Å–ª—É–∂–∞—Ç \"–∑–∞—Ç—Ä–∞–≤–∫–æ–π\" –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –æ–±–ª–∞–¥–∞—é—Ç –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é –¥–ª—è –º–æ–¥–µ–ª–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç —Ç–∞–∫–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.\n\n3.  **–ü–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥:** –í phi-4 —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π SFT (Supervised Fine-Tuning) –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä –¥–ª—è DPO (Direct Preference Optimization), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–∏—Å–∫–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏.\n\n–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è–º phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ä–∞–≤–Ω–∏–º—É—é –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â—É—é –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ –º–Ω–æ–≥–∏—Ö —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Å—Ç–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Llama-3.1-405B. –í —Ç–∞–±–ª–∏—Ü–µ 1 –ø–æ–∫–∞–∑–∞–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ phi-4 —Å –¥—Ä—É–≥–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö. –ü—Ä–∏ —ç—Ç–æ–º phi-4 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–µ–≥–æ \"—É—á–∏—Ç–µ–ª—è\" GPT-4o –Ω–∞ —Ç–µ—Å—Ç–∞—Ö GPQA (–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ STEM –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤) –∏ MATH (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è).\n\n–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, —Å—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏ —á—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —É–ª—É—á—à–µ–Ω–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º—ã–º —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π.\n"
            },
            {
                "title": "Addressing Overfitting and Data Contamination",
                "content": "Decontamination: One pitfall of foundation models is overfitting to benchmarks, such as through the leakage of benchmark test sets via the web corpus. We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results. More details of the decontamination method are given in Appendix B. AMC Benchmark: The surest way to guard against overfitting to the test set is to test on fresh data. We tested our model on the November 2024 AMC-10 and AMC-12 math competitions [Com24], which occurred after all our training data was collected, and we only measured our performance after choosing all the hyperparameters in training our final model. These contests are the entry points to the Math Olympiad track in the United States and over 150,000 students take the tests each year. In Figure 1 we plot the average score over the four versions of the test, all of which have maximum score of 150. phi-4 outperforms not only similar-size or open-weight models but also much larger frontier models. Such strong performance on fresh test set suggests that phi-4s top-tier performance on the MATH benchmark is not due to overfitting or contamination. We provide further details in Appendix C. Relying on Contamination-Proof Benchmarks: We give significant weight to benchmarks which were designed in such way that the questions are original and do not appear on the web, such as GPQA [RHS+23]. While optimizing our model, we relied on an internal benchmark composed primarily of original prompts written by the team (see Section 5 for further details). Long Chain-of-Thought Models: style of LLM that scales inference-time compute by generating long chains of thought has emerged over the past few months, as pioneered by OpenAI O1 [Ope24a] and followed by DeepSeek-R1-Lite-Preview [Dee24] and Qwen/QwQ-32B-Preview [Tea24]. These models perform well on reasoning benchmarks, where QwQ, the only such model with open weights, averages 124.5 points in the AMC-10/12 setting of Figure 1. However, QwQ also uses 4X more tokens on this task than phi-4 and has more than twice as many parameters. Thus, the inference cost of QwQ is an order of magnitude higher than phi-4. Consequently, these models are not in the same class as phi-4 with respect to cost or latency.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–∞–∂–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∞ –∏–º–µ–Ω–Ω–æ:\n\n**1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" (Decontamination):**\n\n–û–¥–Ω–æ–π –∏–∑ –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ —Å–ª—É—á–∞–π–Ω–æ \"–ø—Ä–æ—Å–∞—á–∏–≤–∞—é—Ç—Å—è\" –≤ –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ –≤–µ–±-–∫–æ—Ä–ø—É—Å. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —ç—Ç–æ–≥–æ, –¥–ª—è –º–æ–¥–µ–ª–∏ phi-4 –±—ã–ª —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ Phi. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –±—É–¥—É—Ç –∏—Å–∫–∞–∂–µ–Ω—ã \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ–º\" –¥–∞–Ω–Ω—ã–º–∏. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ B.\n\n**2. –ë–µ–Ω—á–º–∞—Ä–∫ AMC (American Mathematics Competitions):**\n\n–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω–∫—É—Ä—Å–∞—Ö AMC-10 –∏ AMC-12, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –Ω–æ—è–±—Ä–µ 2024 –≥–æ–¥–∞, –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–±–æ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑–º–µ—Ä—è–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –≤—Å–µ—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠—Ç–∏ –∫–æ–Ω–∫—É—Ä—Å—ã —è–≤–ª—è—é—Ç—Å—è –æ—Ç–±–æ—Ä–æ—á–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã –≤ –°–®–ê, –∏ –≤ –Ω–∏—Ö –µ–∂–µ–≥–æ–¥–Ω–æ —É—á–∞—Å—Ç–≤—É—é—Ç –±–æ–ª–µ–µ 150 000 —à–∫–æ–ª—å–Ω–∏–∫–æ–≤. –ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ (–†–∏—Å—É–Ω–æ–∫ 1) –ø–æ–∫–∞–∑–∞–Ω —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ —á–µ—Ç—ã—Ä–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º —Ç–µ—Å—Ç–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –≤ –∫–∞–∂–¥–æ–º –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–≤–µ–Ω 150. phi-4 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏, –Ω–æ –∏ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏. –¢–∞–∫–∏–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≥–æ–≤–æ—Ä—è—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" –¥–∞–Ω–Ω—ã–º–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ C.\n\n**3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∑–∞—â–∏—â–µ–Ω–Ω—ã—Ö –æ—Ç \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\":**\n\n–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –≤–æ–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∏ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, GPQA. –ü—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ—Å—Ç–æ—è—â–∏–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ (–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ –†–∞–∑–¥–µ–ª–µ 5).\n\n**4. –ú–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Long Chain-of-Thought Models):**\n\n–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ—è–≤–∏–ª–∏—Å—å LLM, –∫–æ—Ç–æ—Ä—ã–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ OpenAI O1, DeepSeek-R1-Lite-Preview –∏ Qwen/QwQ-32B-Preview, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø—Ä–æ–≤–µ—Ä—è—é—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. QwQ, –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ, –Ω–∞–±–∏—Ä–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 124,5 –±–∞–ª–ª–∞ –≤ —Ç–µ—Å—Ç–µ AMC-10/12. –û–¥–Ω–∞–∫–æ QwQ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏–º–µ–µ—Ç –±–æ–ª–µ–µ —á–µ–º –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á–µ–º phi-4. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –≤—ã–≤–æ–¥ QwQ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –≤—ã—à–µ, —á–µ–º —É phi-4. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —ç—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Ç–æ–º—É –∂–µ –∫–ª–∞—Å—Å—É, —á—Ç–æ –∏ phi-4, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –∑–∞–¥–µ—Ä–∂–∫–∏.\n"
            },
            {
                "title": "Purpose of Synthetic Data",
                "content": "Synthetic data as substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by language model is by definition predicted by the preceding tokens, making it easier for model to follow the resulting reasoning patterns. In this way, synthetic data may act as form of spoonfeeding, presenting challenges in digestible and progression-oriented manner. simple example to illustrate this is that human-written solution to math problem might start with the final answer. This answer is much too hard to output immediately, for either human or an LLMthe human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce it linearly. Synthetic solutions to math problems will not have such roadblocks. Alignment with Inference Contexts. Synthetic data is typically closer to the format of outputs we expect our models to generate. Training on such data helps align the models pretraining experience with the scenarios it encounters during inference. This alignment ensures that the context seen during generation remains in-distribution with respect to the data the model was pretrained on. For example, web forums are very different in style from LLM interactions. If fact only appears in web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces. Rewriting facts from the web forum into the language style of an LLM makes the facts more accessible during the LLM chat context of inference. Principles. Our approach to generating synthetic data for phi-4 is guided by the following principles: 1. Diversity: The data should comprehensively cover subtopics and skills within each domain. This requires curating diverse seeds from organic sources. 2. Nuance and Complexity: Effective training requires nuanced, non-trivial examples that reflect the complexity and the richness of the domain. Data must go beyond basics to include edge cases and advanced examples. 3. Accuracy: Code should execute correctly, proofs should be valid, and explanations should adhere to established knowledge, etc. 4. Chain-of-Thought: Data should encourage systematic reasoning, teaching the model various approaches to the problems in step-by-step manner. This fosters coherent outputs for complex tasks.",
                "summary": "## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n\n–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Phi –Ω–∞–≥–ª—è–¥–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ç–æ. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –¥–µ—à—ë–≤–∞—è –∑–∞–º–µ–Ω–∞ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–º\" –¥–∞–Ω–Ω—ã–º, –æ–Ω–∏ –æ–±–ª–∞–¥–∞—é—Ç —Ä—è–¥–æ–º –ø—Ä—è–º—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤.\n\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.** –í –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ —á–∞—Å—Ç–æ —Å–ª–æ–∂–Ω—ã –∏ –Ω–µ–ø—Ä—è–º—ã. –ú–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —à–∞–≥–æ–≤, —á—Ç–æ–±—ã —Å–≤—è–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω —Å–æ —Å–ª–µ–¥—É—é—â–∏–º, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —ç—Ç–æ–≥–æ, –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏. –≠—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–∞–µ–º—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ \"–∫–æ—Ä–º–ª–µ–Ω–∏–µ —Å –ª–æ–∂–µ—á–∫–∏\", –∫–æ–≥–¥–∞ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å —Ñ–æ—Ä–º–µ. –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä: —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–µ —á–µ–ª–æ–≤–µ–∫–æ–º, –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–µ–Ω –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫–æ–º, —Ç–∞–∫ –∏ LLM. –ß–µ–ª–æ–≤–µ–∫ –ø–æ–ª—É—á–∏–ª –µ–≥–æ –ø—É—Ç–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ LLM –Ω–∞—É—á–∏—Ç—Å—è –≤—ã–¥–∞–≤–∞—Ç—å –µ–≥–æ –ª–∏–Ω–µ–π–Ω–æ. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –Ω–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å —Ç–∞–∫–∏—Ö –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π.\n\n**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º –≤—ã–≤–æ–¥–∞.** –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ã—á–Ω–æ –±–ª–∏–∂–µ –∫ —Ñ–æ—Ä–º–∞—Ç—É –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –æ–∂–∏–¥–∞–µ–º –æ—Ç –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –æ–ø—ã—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–Ω–∞ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–∞–±–ª—é–¥–∞–µ–º—ã–π –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ—Å—Ç–∞—ë—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–µ–±-—Ñ–æ—Ä—É–º—ã —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –ø–æ —Å—Ç–∏–ª—é –æ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM. –ï—Å–ª–∏ –∫–∞–∫–æ–π-–ª–∏–±–æ —Ñ–∞–∫—Ç –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –¥–∞–Ω–Ω—ã—Ö –≤–µ–±-—Ñ–æ—Ä—É–º–æ–≤, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –æ–Ω –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–µ–Ω –≤ —á–∞—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç. –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –≤–µ–±-—Ñ–æ—Ä—É–º–∞ –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å—Ç–∏–ª—å LLM –¥–µ–ª–∞–µ—Ç —Ñ–∞–∫—Ç—ã –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —á–∞—Ç–∞ LLM –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞.\n\n**–ü—Ä–∏–Ω—Ü–∏–ø—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è phi-4.** –ù–∞—à –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è phi-4 –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö:\n\n1.  **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:** –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –ø–æ–¥—Ç–µ–º—ã –∏ –Ω–∞–≤—ã–∫–∏ –≤ –∫–∞–∂–¥–æ–π –æ–±–ª–∞—Å—Ç–∏. –≠—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–±–æ—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö \"—Å–µ–º—è–Ω\" –∏–∑ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n2.  **–ù—é–∞–Ω—Å—ã –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å:** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –Ω—é–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤—ã—Ö–æ–¥–∏—Ç—å –∑–∞ —Ä–∞–º–∫–∏ –æ—Å–Ω–æ–≤ –∏ –≤–∫–ª—é—á–∞—Ç—å –ø–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã.\n3.  **–¢–æ—á–Ω–æ—Å—Ç—å:** –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏, –∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã–º –∑–Ω–∞–Ω–∏—è–º –∏ —Ç.–¥.\n4.  **–¶–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:** –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –ø–æ–æ—â—Ä—è—Ç—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –æ–±—É—á–∞—è –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–æ–¥—Ö–æ–¥–∞–º –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º —à–∞–≥ –∑–∞ —à–∞–≥–æ–º. –≠—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é —Å–≤—è–∑–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.\n"
            },
            {
                "title": "Synthetic Data for Pretraining and Midtraining",
                "content": "We created 50 broad types of synthetic datasets, each one relying on different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to total of about 400B unweighted tokens. In Appendix D, we give few examples of transcripts taken from our synthetic generations. Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: Seed Curation: The synthetic dataset generation begins with high-quality seeds sourced from multiple domains. These curated seeds provide the foundation for synthetic data generation, enabling the creation of exercises, discussions, and reasoning tasks tailored to the models training objectives. 1. Web and Code-based Seeds: Excerpts and snippets are extracted from web pages, books, and code repositories with focus on content that demonstrates high complexity, reasoning depth, and educational value. To ensure quality, we employ two-stage filtering process: first, identifying pages with strong educational potential, and second, segmenting the selected pages into passages, scoring each for its factual and reasoning content. 2. Question Datasets: large set of questions was collected from websites, forums, and Q&A platforms. These questions were then filtered using plurality-based technique to balance difficulty. Specifically, we generated multiple independent answers for each question and applied majority voting to assess the consistency of responses. We discarded questions where all answers agreed (indicating the question was too easy) or where answers were entirely inconsistent (indicating the question was too difficult or ambiguous). This filtering process produces dataset of questions that challenge the models reasoning and problem-solving abilities while remaining approachable. The plurality answers were used in place of the ground truth in our rejection-sampling based generations. 3. Creating Question-Answer pairs from Diverse Sources: Another technique we use for seed curation involves leveraging language models to extract question-answer pairs from organic sources such as books, scientific papers, and code. This approach does not rely on merely identifying explicit Q&A pairs within the text. Instead, it involves pipeline designed to detect deduction chains or logical progressions in the text. The language model identifies key steps in reasoning or problem-solving processes and reformulates them into questions and corresponding answers. Our experiments show that, if done correctly, training on the resulting content can be far more effective (in terms of improvement on academic and internal benchmarks) than training on the original content. Rewrite and Augment: Seeds are transformed into synthetic data through multi-step prompting workflows. This includes rewriting most of the useful content in given passages into exercises, discussions, or structured reasoning tasks. Self-revision: The initial responses are then iteratively refined through feedback loop where model critiques and subsequently improves its own outputs, guided by the rubrics focused on reasoning and factual accuracy. 5 Instruction Reversal for Code and Other Tasks: To enhance the models ability to generate outputs from instructions, we used an instruction reversal technique. For example, we take existing code snippets from the code data corpus and use it to generate corresponding instructions that include the problem description or task prompt. The resulting synthetic data pairs were structured with the instruction appearing before the code. Only data with high fidelity between the original and regenerated code are retained, ensuring alignment between the instructions and the outputs. This method can be generalized to other targeted use cases. Validation of Code and Other Scientific Data: When appropriate, we incorporate tests for validating our reasoning-heavy synthetic datasets. The synthetic code data is validated through execution loops and tests. For scientific datasets, the questions are extracted from scientific materials using method designed to ensure high relevance, groundedness, and difficulty balance.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4. –ë—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 50 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ç–µ–º, –Ω–∞–≤—ã–∫–æ–≤ –∏ —Ç–∏–ø–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –æ–±—â–∏–º –æ–±—ä–µ–º–æ–º –æ–∫–æ–ª–æ 400 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.\n\n**–ö—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:** –ü—Ä–æ—Ü–µ—Å—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ—Ç–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Å–ª—É–∂–∞—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –¥–∏—Å–∫—É—Å—Å–∏–∏ –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª—è–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n\n1.  **–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ –∫–æ–¥–∞:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –∫–Ω–∏–≥ –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –∫–æ–¥–∞, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: —Å–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏—Å—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤—ã—Å–æ–∫–∏–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º, –∞ –∑–∞—Ç–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–∞–∑–¥–µ–ª—è–ª–∏—Å—å –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ü–µ–Ω–∏–≤–∞–ª—Å—è –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.\n2.  **–ù–∞–±–æ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:** –ë—ã–ª–∞ —Å–æ–±—Ä–∞–Ω–∞ –±–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤, —Ñ–æ—Ä—É–º–æ–≤ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠—Ç–∏ –≤–æ–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∏ –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤. –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –±—ã–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ (—Å–ª–∏—à–∫–æ–º –ª–µ–≥–∫–∏–µ) –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–º–∏ (—Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–µ), –æ—Ç–±—Ä–∞—Å—ã–≤–∞–ª–∏—Å—å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∏–ª—Å—è –Ω–∞–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—Ä–æ—Å–∞—é—Ç –≤—ã–∑–æ–≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º, –æ—Å—Ç–∞–≤–∞—è—Å—å –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –≤–º–µ—Å—Ç–æ \"–∏—Å—Ç–∏–Ω–Ω—ã—Ö\" –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.\n3.  **–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** –î–ª—è –æ—Ç–±–æ—Ä–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏–∑ –∫–Ω–∏–≥, –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–¥–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏—â–µ—Ç —è–≤–Ω—ã–µ –ø–∞—Ä—ã \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –≤ —Ç–µ–∫—Å—Ç–µ. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—ã—è–≤–ª—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ —É–º–æ–∑–∞–∫–ª—é—á–µ–Ω–∏–π –∏–ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ. –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —à–∞–≥–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–ª–∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∏—Ö –≤ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–≤–µ—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–∞–∫–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º (—Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ç–µ—Å—Ç–∞—Ö), —á–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ.\n\n**–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ:** –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –¥–∏—Å–∫—É—Å—Å–∏–∏ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ.\n\n**–°–∞–º–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∑–∞—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ü–∏–∫–ª–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –≤ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫—É–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∫—Ä–∏—Ç–µ—Ä–∏–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤.\n\n**–û–±—Ä–∞—â–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –∫–æ–¥–∞ –∏ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á:** –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ –æ–±—Ä–∞—â–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞ –∏–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –≤–∫–ª—é—á–∞—é—â–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–¥–∞—á–∏. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ç–∞–∫, —á—Ç–æ–±—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–µ–¥—à–µ—Å—Ç–≤–æ–≤–∞–ª–∞ –∫–æ–¥—É. –°–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º –∏ —Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–¥–æ–º, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–ª–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–æ–±—â–µ–Ω –Ω–∞ –¥—Ä—É–≥–∏–µ —Ü–µ–ª–µ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n\n**–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –∏ –¥—Ä—É–≥–∏—Ö –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:** –ö–æ–≥–¥–∞ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ, –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞—é—Ç—Å—è —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∞ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–æ–≤. –î–ª—è –Ω–∞—É—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å—ã –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–µ–≥–æ –≤—ã—Å–æ–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å.\n"
            },
            {
                "title": "Curation and Filtering of Web and Q&A Data",
                "content": "Q&A datasets. We collected tens-of-millions high-quality organic problems and solutions by reviewing public websites, relying on existing datasets, and acquiring external datasets. Our experience from previous models showed that question-answer data contributed significantly to various capabilities, such as mathematical reasoning and academic performance. Our ablation studies showed that organic questions are substantially more effective than synthetic questions. We used several ways to synthetically augment the dataset of organic questions to obtain larger dataset. While these rewritten questions improved the models capabilities, the gains were not as pronounced. significant portion of the collected questions lacked accurate solutions. To address this, we replaced the answers with synthetically generated ones and used majority-voting to increase accuracy. All collected questions and solutions underwent thorough decontamination process to ensure there is no overlap with test sets3. Targeting High-quality Web Data. We collected wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). In addition to directly training on this text, we used various web sources as seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to be absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations for derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web data. We discuss the main techniques and considerations below: Targeted Acquisitions: We included major repositories of reasoning-dense documents that are publicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g., licensed books) aiming for level of comprehensiveness, recency, and cleanliness above the typical standard of externally available corpora. Filtering Web Dumps: To capture the long tail of information-rich web sources (e.g., forums, blogs, course material, domain-specific wikis), we took the approach of selecting small fraction of highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on 106 LLM-generated annotations. This approach tends to over-index on STEM-related keywords, so we created specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history, travel, culture, and entertainment). These topic classifications were also obtained by distilling an 3This step is crucial to the reliability of some of the academic benchmarks: for instance, some test benchmark variants can be found on platforms like Hugging Face. Moreover, benchmarks such as MMLU are frequently compiled from websourced questions. LLM annotator. Finally, we removed corrupted text and binary files by detecting outliers according to n-gram statistics and compression ratios. Multilingual Data: We incorporated multilingual datasets to ensure that our model could handle wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi and Japanese. This involved sourcing and processing high-quality multilingual documents from CommonCrawl and Wikipedia. Our multilingual processing pipeline consists of language identification model, based on fastText used to categorize documents into 176 languages, then uses the same classifiers for filtering web dumps to filter for quality. Note that the classifiers were trained on multilingual LLM-generated annotations. Custom Extraction and Cleaning Pipelines: To ensure sufficient cleanliness and uniformity between heterogeneous organic data sources, we needed collection of customized heuristics and parsers. For each targeted data source, we built custom pipelines to ingest variety of file formats (e.g., multi-file TeX source, ePub and other XML-like formats, Microsoft Word documents, and PDFs). For general web data, we built custom HTML-to-text extractor, taking significant care to preserve fragile content that is frequently corrupted by naƒ±ve parsers (e.g., TeX/MathML equations, code blocks, tables, and forum thread structure). This extractor prunes and normalizes the DOM tree, using variety of signals (e.g., HTML tag names, CSS classes, content length, and tree depth) to distinguish elements such as boilerplate, advertisements, equations, and syntaxhighlighter artifacts.",
                "summary": "**–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ**\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –¥–µ—Å—è—Ç–∫–∏ –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–µ–±-—Å–∞–π—Ç–æ–≤, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –û–ø—ã—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ç–∏–ø–∞ \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —É—Å–ø–µ–≤–∞–µ–º–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ\" –≤–æ–ø—Ä–æ—Å—ã (—Ç–æ –µ—Å—Ç—å, –≤–æ–ø—Ä–æ—Å—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏, –∞ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ) –≥–æ—Ä–∞–∑–¥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö. –î–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –•–æ—Ç—è —ç—Ç–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —É–ª—É—á—à–∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –ø—Ä–∏—Ä–æ—Å—Ç –±—ã–ª –Ω–µ —Ç–∞–∫–∏–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –∏–º–µ–ª–∞ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç—ã –±—ã–ª–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º. –í—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ—à–ª–∏ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –ª—é–±–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏.\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –±—ã–ª —Å–æ–±—Ä–∞–Ω —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏—á–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–ª—Å—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º, —Ç—Ä–µ–±—É—é—â–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –Ω—é–∞–Ω—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä—É–º—ã –∏ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é). –í–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–æ –∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —á–∏—Å—Ç—ã–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫—Ä–∞–π–Ω–µ –≤–∞–∂–Ω—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: –¥–∞–∂–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å–µ—Ä—å–µ–∑–Ω–æ–º—É —É—Ö—É–¥—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ—ç—Ç–æ–º—É –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–∏–ª–∏—è –±—ã–ª–∏ –≤–ª–æ–∂–µ–Ω—ã –≤ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –∫—É—Ä–∞—Ü–∏—é –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö.\n\n–ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–ª —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:\n\n*   **–¶–µ–ª–µ–≤—ã–µ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è:** –ë—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –∫—Ä—É–ø–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã –¥–ª—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, arXiv, PubMed Central, GitHub) –∏–ª–∏ –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏). –¶–µ–ª—å—é –±—ã–ª–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ø–æ–ª–Ω–æ—Ç—ã, –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∏—Å—Ç–æ—Ç—ã –≤—ã—à–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤.\n*   **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–µ–±-–¥–∞–º–ø–æ–≤:** –î–ª—è –æ—Ö–≤–∞—Ç–∞ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ –Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ–æ—Ä—É–º–æ–≤, –±–ª–æ–≥–æ–≤, —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–∫–∏) –±—ã–ª –ø—Ä–∏–º–µ–Ω–µ–Ω –ø–æ–¥—Ö–æ–¥ –≤—ã–±–æ—Ä–∞ –Ω–µ–±–æ–ª—å—à–æ–π —á–∞—Å—Ç–∏ —Å–∞–º—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –º–∞—Å—Å–æ–≤—ã—Ö –≤–µ–±-–¥–∞–º–ø–æ–≤. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –Ω–µ–±–æ–ª—å—à–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM), –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö LLM. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç STEM-—Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –ø–æ—ç—Ç–æ–º—É –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ-STEM —Ç–µ–º–∞—Ç–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–∫—É—Å—Å—Ç–≤–∞, –∏—Å—Ç–æ—Ä–∏–∏, –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π, –∫—É–ª—å—Ç—É—Ä—ã –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π). –≠—Ç–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∞–∫–∂–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –ø—É—Ç–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ LLM-–∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–º. –ù–∞–∫–æ–Ω–µ—Ü, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –¥–≤–æ–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –ø—É—Ç–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ n-–≥—Ä–∞–º–º –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º —Å–∂–∞—Ç–∏—è.\n*   **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —è–∑—ã–∫–æ–≤, –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –Ω–µ–º–µ—Ü–∫–∏–π, –∏—Å–ø–∞–Ω—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, —Ö–∏–Ω–¥–∏ –∏ —è–ø–æ–Ω—Å–∫–∏–π. –≠—Ç–æ –≤–∫–ª—é—á–∞–ª–æ –ø–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ CommonCrawl –∏ Wikipedia. –ö–æ–Ω–≤–µ–π–µ—Ä –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–æ–¥–µ–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —è–∑—ã–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ fastText, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ 176 —è–∑—ã–∫–æ–≤, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–µ–±-–¥–∞–º–ø–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö.\n*   **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω–≤–µ–π–µ—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏:** –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —á–∏—Å—Ç–æ—Ç—ã –∏ –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–µ–∂–¥—É —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª—Å—è –Ω–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –∏ –ø–∞—Ä—Å–µ—Ä–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–µ–ª–µ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∫–æ–Ω–≤–µ–π–µ—Ä—ã –¥–ª—è –ø—Ä–∏–µ–º–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ TeX, ePub –∏ –¥—Ä—É–≥–∏–µ XML-–ø–æ–¥–æ–±–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã Microsoft Word –∏ PDF). –î–ª—è –æ–±—â–∏—Ö –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä HTML-–≤-—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —É–¥–µ–ª—è–µ—Ç –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é \"—Ö—Ä—É–ø–∫–æ–≥–æ\" –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å—Ç–æ –ø–æ–≤—Ä–µ–∂–¥–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Ä–∞–≤–Ω–µ–Ω–∏—è TeX/MathML, –±–ª–æ–∫–∏ –∫–æ–¥–∞, —Ç–∞–±–ª–∏—Ü—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Ç–æ–∫–∞ —Ñ–æ—Ä—É–º–∞). –≠—Ç–æ—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –æ–±—Ä–µ–∑–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–µ—Ä–µ–≤–æ DOM, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–º–µ–Ω–∞ HTML-—Ç–µ–≥–æ–≤, –∫–ª–∞—Å—Å—ã CSS, –¥–ª–∏–Ω—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏ –≥–ª—É–±–∏–Ω—É –¥–µ—Ä–µ–≤–∞) –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∫–∞–∫ —à–∞–±–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —Ä–µ–∫–ª–∞–º–∞, —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥—Å–≤–µ—Ç–∫–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞.\n"
            },
            {
                "title": "Pretraining Details",
                "content": "The phi-4 model is based on decoder-only transformer architecture [VSP+17] with 14B parameters and default context length of 4096. This is later extended to 16K context length during midtraining. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than 2K sliding window used in phi-3-medium. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. The training hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted by stress testing the learning rate warm-up stage for stability. Pretraining is followed by shorter midtraining stage to increase the original context length of 4k to 16k. Since pre-trained models are not good at instruction following, it is not very informative to use 0-shot evaluations that require the answer to be in specific format, for example simple-evals. We 7 MMLU MMLU pro GSM8k Human-Eval ARCC MBPP MATH TQA phi-4 (4k) phi-4 (16k) +3.0 +2.7 +10.3 +8.9 +2.2 +1.2 +7.8 +9.0 +1.1 +0.9 +6.8 +9. +8.9 +8.4 -0.7 -1.5 Table 2: Pretraining benchmarks for phi-4 compared to its predecessor, phi-3-medium after pretraining. therefore use an internal implementation of benchmarks for pretraining which uses mixture of loglikelihood and/or few-shot prompts for various tasks. Specifically, we used log-likelihood evaluations for MMLU (5-shot), MMLU-pro, and ARCC (1-shot). We used 1, 3, 4, and 8 few-shot examples for TriviaQA (TQA), MBPP, MATH, and GSM8k to help the model adhere to the answer format for easier extraction of the solution. We use this evaluation method throughout Section 3. Table 2 summarizes the performance boost of pretrained phi-4 compared with its predecessor phi-3-medium.",
                "summary": "–ú–æ–¥–µ–ª—å phi-4 –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ transformer —Ç–æ–ª—å–∫–æ —Å –¥–µ–∫–æ–¥–µ—Ä–æ–º, –æ–Ω–∞ –∏–º–µ–µ—Ç 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 4096 —Ç–æ–∫–µ–Ω–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ 16 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ phi-3-medium, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –≤ phi-4 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä tiktoken, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 100 352, –≤–∫–ª—é—á–∞—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ phi-4 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (full attention) –∫–æ –≤—Å–µ–π –¥–ª–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 4096 —Ç–æ–∫–µ–Ω–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤ phi-3-medium –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–º 2048 —Ç–æ–∫–µ–Ω–æ–≤.\n\n–ú–æ–¥–µ–ª—å phi-4 –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 10 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞—Ä–∞—Å—Ç–∞–Ω–∏—è –∏ —Å–ø–∞–¥–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, —Å –ø–∏–∫–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 0.0003, –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –≤–µ—Å–æ–≤—ã–º —Ä–∞—Å–ø–∞–¥–æ–º 0.1 –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ 5760. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø—É—Ç–µ–º —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–∞–ø–∞ –Ω–∞—Ä–∞—Å—Ç–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏. –ü–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —ç—Ç–∞–ø –¥–æ–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å 4 —Ç—ã—Å—è—á –¥–æ 16 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤.\n\n–ü–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ \"0-shot\", —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç–≤–µ—Ç–∞ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, simple-evals, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–º–µ—Å—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –∏/–∏–ª–∏ few-shot –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ, –¥–ª—è MMLU (5-shot), MMLU-pro –∏ ARCC (1-shot) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ—Ü–µ–Ω–∫–∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –î–ª—è TriviaQA (TQA), MBPP, MATH –∏ GSM8k –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å 1, 3, 4 –∏ 8 few-shot –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ 3. –í —Ç–∞–±–ª–∏—Ü–µ 2 –ø–æ–∫–∞–∑–∞–Ω–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ phi-4 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –µ—ë –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏—Ü–µ–π, phi-3-medium.\n"
            },
            {
                "title": "Data Composition in Pretraining",
                "content": "The phi-3 model family were trained using two-phase strategy. Most of the training tokens were used in phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with data mixture consisting primarily of synthetic tokens and much smaller allocation for ultra-filtered and reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed marginal drop in the benefit from using non-synthetic tokens for the phi-3 family of model sizes. We note two key observations. Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs over our synthetic data led to better performance with respect to adding fresh web tokens. Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and demonstrated increased hallucinations. Figure 2 demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In this example, we conduct two training runs per model scale, using the same number of training tokens on top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed (a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and 12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen, performing more iterations on the synthetic data is more beneficial than supplying more web tokens. Inspired by this scaling behavior of our synthetic data, we trained 13B parameter model solely on synthetic4 data, for ablation purposes only the model sees over 20 repetitions of each data source. For the sake of ablations, we partitioned our synthetic data into web rewrites, which includes more direct rewrites of our filtered web content relative to all other types of synthetic data. Table 3 compares the previous phi-3-medium model with the new model trained entirely on the synthetic data. Throughout training, all benchmarks consistently improved, despite the increase in epochs, and the majority of the benchmarks showed improvements over phi-3. However, knowledge-related benchmarks, like 1-shot triviaqa (TQA), show large gap where synthetic models are subpar. These observations led us to rethink the role of web data in our data mixture. 4This is an updated mixture of synthetic data that contains new sources compared to phi-3. 8 Figure 2: 5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are trained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens. We see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch models perform better than those that have seen more unique web tokens. MMLU MMLU pro GSM8k Human-Eval ARCC MBPP MATH TQA Synthetic Synthetic + Web Rewrites +0.8 +0.3 +4.0 +4.1 +2.2 +1.8 +12.1 +13.3 0.0 +3.0 +5.0 +7. +4.9 +8.1 -14.8 -7.7 Table 3: Benchmark performance of 13B models (used for ablations only) trained on data mixtures containing no web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic data and web rewrites. All numbers are reported relative to the performance of phi-3-medium, which has seen combination of web and synthetic data.",
                "summary": "–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π phi-3 –æ–±—É—á–∞–ª–æ—Å—å –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª–∏ —Å–æ–±–æ–π –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ—Ö–æ–¥–∏–ª –Ω–∞ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–µ–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ–π –¥–æ–ª–µ–π —É–ª—å—Ç—Ä–∞-–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö.\n\n–ü–æ –º–µ—Ä–µ —Ä–æ—Å—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞—à–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º—ã –Ω–∞–±–ª—é–¥–∞–ª–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–ª—å–∑—ã –æ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π phi-3. –ë—ã–ª–∏ —Å–¥–µ–ª–∞–Ω—ã –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è: –≤–µ–±-–¥–∞–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞–ª–∏ –Ω–µ–±–æ–ª—å—à—É—é –ø–æ–ª—å–∑—É –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∞–≤–∞–ª–∞ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–±-—Ç–æ–∫–µ–Ω–æ–≤.\n\n–ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —É—Å–∏–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –†–∏—Å—É–Ω–æ–∫ 2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–≤–æ–µ —è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –Ω–µ–±–æ–ª—å—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ. –í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –¥–≤–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–≤–µ—Ä—Ö –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ. –î–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –±—ã–ª–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º (–ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö), –Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è–ª–æ—Å—å, –∞ –∏–º–µ–Ω–Ω–æ 4 –∏ 12 —ç–ø–æ—Ö. –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –±—ã–ª–∏ –Ω–æ–≤—ã–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ö–∞–∫ –≤–∏–¥–Ω–æ, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ, —á–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–µ–±-—Ç–æ–∫–µ–Ω–æ–≤.\n\n–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–∞—à–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å —Å 13 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏ —ç—Ç–æ–º –¥–ª—è —Ü–µ–ª–µ–π –∞–±–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ –±–æ–ª–µ–µ 20 –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è —Ü–µ–ª–µ–π –∞–±–ª—è—Ü–∏–∏ –º—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –Ω–∞—à–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ \"–ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ\", –∫–æ—Ç–æ—Ä—ã–µ –≤–∫–ª—é—á–∞—é—Ç –±–æ–ª–µ–µ –ø—Ä—è–º—ã–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –Ω–∞—à–µ–≥–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Å–µ—Ö –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n\n–í —Ç–∞–±–ª–∏—Ü–µ 3 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∞—è –º–æ–¥–µ–ª—å phi-3-medium —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, –æ–±—É—á–µ–Ω–Ω–æ–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤—Å–µ —Ç–µ—Å—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–ª–∏—Å—å, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö, –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å phi-3. –û–¥–Ω–∞–∫–æ —Ç–µ—Å—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ 1-shot triviaqa (TQA), –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤, –≥–¥–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Ö—É–∂–µ. –≠—Ç–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∑–∞—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞—Å –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏—Ç—å —Ä–æ–ª—å –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –≤ –Ω–∞—à–µ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö.\n\n*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –≤—ã—è—Å–Ω–∏–ª–∏, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π phi-3 –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ, —á–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ, –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π.*\n"
            },
            {
                "title": "Data Mixture",
                "content": "To design our pretraining data mixture for given training token budget, we search over different allocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites5, 3) filtered web (divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g., academic data, books, and forums), and 5) code data. We conducted ablations using shorter token horizon of 1T tokens to derive the data mixture. These ablations rely on our established result on the high-rank correlation of short training with longer training, up to the over-fitting saturation threshold of data sources. In addition we observe high rank correlation between the performance of the 7B and 14B models on different data mixtures, given large enough distance between the data mixtures. This allowed us to conduct the experiments at 7B scale and transfer the findings to phi-4. Among the numerous ablations, we highlight few that show best insights on our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions and code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters. Table 4 summarizes the results for the hand-picked ablations, as compared with the data mixture that was used for the final training run. uniform allocation of tokens among the three categories is suboptimal due to the higher quality of synthetic data and the only benchmark that shows clear benefit from web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally better than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy filtered web data sources to improve knowledge benchmarks (see Section 3.1) to balance all model 5Web rewrites is sub-category of synthetic data that is substantially large and contains direct rewrites of web content. 9 MMLU MATH GSM8k Human-Eval ARCC MBPP TQA MMLU pro Average Uniform + WR + -3.3 +3.3 +0.6 -0.6 -5.4 +4.0 +1.2 -0.7 -5.8 +2.1 +1.5 -0.7 -1.2 -6.1 -1.2 -4.3 +0.6 +1.9 +1.6 +0. -2.0 +0.4 +1.6 -2.0 +3.3 -3.0 -3.7 +6.9 -3.6 +3.7 +1.2 +0.9 -2.2 +0.8 +0.4 0.0 Table 4: Ablations on the allocation of 75% of training tokens to synthetic (S), filtered web (W), and web rewrite (WR) categories, while other data sources are held constant in the remaining 25% token budget. All benchmark numbers are measured relative to the final data mixture used for training phi-4. capabilities. We also note that we observed the gap between the chosen data mixture and the synthetic heavy runs largely closes as the model goes through the post-training stage. An end-to-end optimization of pretraining data mixture that also takes into account the effects of post-training is an interesting future area of investigation. Data Source Fraction of Training Unique Token Count Number of Epochs Web Web rewrites Synthetic Code data Acquired sources 15% 15% 40% 20% 10% 1.3T 290B 290B 820B 580B 1.2 5.2 13.8 2.4 1.7 Table 5: Data mixture for pretraining. The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data (mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books. In terms of total number of unique tokens in each data mixture cluster, filtered web data is the largest cluster with 1.3T tokens. Code and targeted acquisitions are the second and third largest clusters with 820B and 580B tokens, respectively. Finally, web rewrites and synthetic data have similar token count of 290B tokens. The total number of epochs on each data source is determined using the ratio of allocated tokens in the mixture and the number of unique tokens in that source.",
                "summary": "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –º—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤–∫–ª—é—á–∞—é—Ç: 1) —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, 2) –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, 3) –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–± (—Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–π –Ω–∞ —á–∞—Å—Ç–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∑–Ω–∞–Ω–∏—è), 4) —Ü–µ–ª–µ–≤—ã–µ –∏ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã, –∫–Ω–∏–≥–∏ –∏ —Ñ–æ—Ä—É–º—ã) –∏ 5) –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∞.\n\n–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –º—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–µ–Ω—å—à–∏–º –æ–±—ä–µ–º–æ–º —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç—Ä–∏–ª–ª–∏–æ–Ω). –≠—Ç–æ –±—ã–ª–æ –≤–æ–∑–º–æ–∂–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç–æ–º—É, —á—Ç–æ –º—ã —Ä–∞–Ω–µ–µ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö, –≤–ø–ª–æ—Ç—å –¥–æ –Ω–∞—Å—ã—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã –∑–∞–º–µ—Ç–∏–ª–∏ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π —Å 7 –∏ 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —ç—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –Ω–∞–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª–∏ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—å phi-4.\n\n–°—Ä–µ–¥–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –º—ã –≤—ã–¥–µ–ª–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ü–µ–Ω–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–æ—Å—Ç–∞–≤–µ –¥–∞–Ω–Ω—ã—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ü–µ–ª–µ–≤—ã—Ö –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∏ –∫–æ–¥–∞, –∞ –∑–∞—Ç–µ–º –∏–∑–º–µ–Ω—è–ª–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n\n–¢–∞–±–ª–∏—Ü–∞ 4 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É —Ç—Ä–µ–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∏–∑-–∑–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç, –≥–¥–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞–ª–∏ —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ, ‚Äî —ç—Ç–æ TQA. –•–æ—Ç—è –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Å—Ç—Ä–æ–∫–∏ 2 –∏ 3 —Ç–∞–±–ª–∏—Ü—ã) –±—ã–ª–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ, —á–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä, –º—ã —Ä–µ—à–∏–ª–∏ –≤–∫–ª—é—á–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∑–Ω–∞–Ω–∏—è –≤–µ–±-–¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏, –∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —ç—Ç–∞–ø–∞ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π —Ç–µ–º–æ–π –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.\n\n–§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç 30% —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É –≤–µ–±-–¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–º –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–ø–æ—Ä–æ–≤–Ω—É). –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Å—Ç—É–ø–∞—é—Ç –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (40%). –¢–∞–∫–∂–µ 20% —Ç–æ–∫–µ–Ω–æ–≤ –≤—ã–¥–µ–ª–µ–Ω–æ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∫–æ–¥–∞ (—Å–º–µ—Å—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞) –∏ 10% –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã –∏ –∫–Ω–∏–≥–∏).\n\n–ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ —è–≤–ª—è—é—Ç—Å—è —Å–∞–º—ã–º –±–æ–ª—å—à–∏–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º (1.3 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤). –ö–æ–¥ –∏ —Ü–µ–ª–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–Ω–∏–º–∞—é—Ç –≤—Ç–æ—Ä–æ–µ –∏ —Ç—Ä–µ—Ç—å–µ –º–µ—Å—Ç–∞ (820 –∏ 580 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ). –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (290 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤). –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ.\n"
            },
            {
                "title": "Midtraining Details",
                "content": "phi-4 includes midtraining stage where the context length is increased from the original 4K to 16K. We conduct several ablations to study the role of data on long-context performance. Specifically, we try data sources that are inherently long context, and compare them with artificially created long context data where samples are padded together to fill the sequence. We observe the former to perform better in longer context tasks. Inspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and code data) to separate samples above 8K context. We then up-weight the data subsets that are 16K or higher in length. We also create new synthetic datasets that satisfy the > 4K sequence requirement. The final data mixture includes 30% of the newly curated longer context data and 70% portion of recall 10 Model Max Length Recall RAG ICL Re-rank QA Summ phi-4 Qwen-2.5-14B Llama-3.3-70B GPT-4o-mini GPT-4o phi-4 Qwen-2.5-14B Llama-3.3-70B GPT-4o-mini GPT-4o 8K 8K 8K 8K 8K 16K 16K 16K 16K 16K 100.0 100.0 92.0 99.2 100.0 99.0 100.0 92.0 100.0 100.0 58.1 62.2 65.3 65.8 66. 57.1 59.1 62.2 63.6 66.7 68.0 67.8 69.4 74.4 83.0 77.0 67.6 70.0 78.4 85.6 65.3 58.2 64.4 69.4 75.1 54.4 50.3 63.3 63.9 73.8 26.7 24.7 30.0 31.3 37. 36.0 29.7 36.7 36.0 43.7 38.3 37.2 37.8 38.5 43.0 40.5 42.3 41.9 45.2 46.3 Table 6: Evaluation results on the long-context benchmark HELMET [YGH+24]. tokens from the pretraining stage. To accommodate longer context, we increase the base frequency of rope position encoding to 250K following [AI23b]. We drop the maximum learning rate by factor of 10 compared to the pretraining stage and train for total of 250B tokens. To effectively evaluate the long-context capability of our model, it is essential to have comprehensive evaluation framework with practical scenarios. While synthetic benchmarks like needle-in-a-haystack and RULER are preferred for their simplicity and control, our emphasis is on diverse range of tasks that reflect real-world applications, such as reasoning across entire documents. We report the performance of phi-4 and other models on the tasks we selected from the HELMET [YGH+24] evaluation suite in Table 6 and outline our evaluation methods below. Note that results are average across 5 runs for each categories. Recall: The task involves retrieving the corresponding value from randomly-generated long JSON file given specific key (Metric: SubEM) RAG: Answer questions based on many retrieved and shuffled Wikipedia documents. The datasets used for this task are NaturalQuestions, HotpotQA, and PopQA. Final results are average of all datasets (Metric: SubEM) Re-rank: The task is to re-rank the top-10 documents given query and many retrieved and shuffled documents. This uses the MSMARCO dataset (Metric: nDCG@10) ICL: The task involves many-shot in-context learning with datasets such as TREC coarse, TREC fine, Banking77, NLU and CLINC150. Final results are average of all datasets (Metric: F1) QA: Answer questions given lengthy document. The dataset associated with this task is NarrativeQAv2 (Metric: GPT-4o scoring) Summ: The task involves summarizing lengthy legal document, and the dataset used is MultiLexSum (Metric: GPT-4o scoring) Dataset Name Sample Count unknown + safety data generic multiple-choice Q&A math data python data cpp, go, java, js, rust data 3,000 132,859 76,552 16,080 21,806 Table 7: Data Mixture for Pivotal Token DPO",
                "summary": "**–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ phi-4**\n\n–í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω —ç—Ç–∞–ø, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã—Ö 4K —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 16K —Ç–æ–∫–µ–Ω–æ–≤. –ß—Ç–æ–±—ã –∏–∑—É—á–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –±—ã–ª–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–º–µ—é—â–∏–µ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ç–∞–∫ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≥–¥–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –±—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–º–µ—é—â–∏–µ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.\n\n–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã, –∫–Ω–∏–≥–∏ –∏ –∫–æ–¥) –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã, —á—Ç–æ–±—ã –≤—ã–¥–µ–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª–∏–Ω–æ–π –±–æ–ª–µ–µ 8K —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–æ–π 16K –∏ –±–æ–ª–µ–µ –±—ã–ª–∏ —É—Å–∏–ª–µ–Ω—ã. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–æ–≤—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ > 4K. –í –∏—Ç–æ–≥–æ–≤–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö 30% –∑–∞–Ω–∏–º–∞–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∞ 70% - –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–µ—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n\n–î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è RoPE –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ 250K. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∞ —Å–Ω–∏–∂–µ–Ω–∞ –≤ 10 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ø–æ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Å—Ç–∞–≤–∏–ª–æ 250 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.\n\n**–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ**\n\n–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –•–æ—Ç—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ \"–∏–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞\", –ø—Ä–æ—Å—Ç—ã –∏ —É–¥–æ–±–Ω—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è, –æ—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–ª–æ—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ª—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–∞–±–æ—Ä–∞ HELMET.\n\n–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–¥–∞—á–∞–º:\n\n*   **Recall:** –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ JSON-—Ñ–∞–π–ª–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–ª—é—á—É.\n*   **RAG (Retrieval-Augmented Generation):** –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Wikipedia.\n*   **Re-rank:** –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ 10 –ª—É—á—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É.\n*   **ICL (In-Context Learning):** –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö TREC, Banking77, NLU –∏ CLINC150.\n*   **QA (Question Answering):** –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–ª–∏–Ω–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É.\n*   **Summ (Summarization):** –æ–±–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ 6 –∏ —è–≤–ª—è—é—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ 5 –∑–∞–ø—É—Å–∫–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.\n\n**–°–æ—Å—Ç–∞–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO**\n\n–í —Ç–∞–±–ª–∏—Ü–µ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Å–æ—Å—Ç–∞–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO (Direct Preference Optimization) –º–æ–¥–µ–ª–∏. –î–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫:\n\n*   –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–Ω–Ω—ã–µ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n*   –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º\n*   –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n*   –î–∞–Ω–Ω—ã–µ –Ω–∞ —è–∑—ã–∫–µ Python\n*   –î–∞–Ω–Ω—ã–µ –Ω–∞ —è–∑—ã–∫–∞—Ö C++, Go, Java, JavaScript –∏ Rust\n\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–∞–∫–∂–µ —É–∫–∞–∑–∞–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ.\n"
            },
            {
                "title": "Direct Preference Optimization",
                "content": "We use DPO [RSM+23] to align the model with human preferences, and also to steer the model away from unwanted behavior through pairs of desired and undesired outputs. DPO data covers chat format data, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning, robustness, and safety. We do two rounds of DPO on the SFT model. We introduce technique, Pivotal Token Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture for first round are provided in Table 7. For the second round, which we call judge-guided DPO, we gather approximately 850k pairs of desired and undesired outputs. The prompts are sourced from various publicly available instruction tuning datasets and also include prompts related to safety and Responsible AI (RAI). Next, for each of these prompts, we generate responses from GPT-4o, GPT-4t and our model. From these responses, we create various combinations of DPO pairs and use GPT-4o as judge to label positive or negative for given pair. For given pair of responses, each assistant response is given score based on accuracy, style, and detail. We label the response with higher accuracy or overall (average of accuracy, style, and detail) score as the positive response. We provide the prompt we used in Appendix A. The data mixture for this round is provided in Table 8. Both stages also include small amount of data for safety and mitigating hallucinations. 12 Suppose that the roots of x3 + 3x2 + 4x 11 = 0 are a, b, and c, and that the roots User: of x3 + rx2 + sx + = 0 are + b, + c, and + a. AI: To solve the problem, we need to dealdealdealdealdeal dealdealdealdealdeal with the roots of the two cubic equations deal deal deal deal deal deal deal First, consider the cubic polynomial polynomial polynomial polynomial polynomial polynomial polynomial given. polynomial ( xÀÜ3 + 3xÀÜ2 + 4x - 11 = 0 ), polynomial polynomial polynomial polynomial polynomial polynomial polynomial polynomial polynomial By Vietas formulas, we have: [ + with roots ( ), ( ), and ( ). + = -3, ] [ ab + bc + ca = 4, ] [ abc = 11. ] Now, consider the other other other other other other other other other other other other other other other other other polynomial ( xÀÜ3 + rxÀÜ2 + sx + = 0 ) with roots ( a+b ), ( b+c ), and ( c+a ). We need to find the value of ( ), which corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds to the negative corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds Find t. of 0.42 0.93 according to Vietas formulas::::::::::::::::: [ the product of the roots according according according according according according according according according according according according according according according according (a 0.95 0.71 +b)(b+c)(c+a). ] First notenotenotenotenote the important First, notenotenotenotenote important important First First important important First First important important First First important identity: [ (a+b)(b+c)(c+a) = (a+b+c)(ab+bc+ca) - important important First First note important important note First First note important important note First First note important important note First First important note First abc. ] PlugPlugPlugPlugPlug known values from ourourourourourourourourourourourourourourourourour Vietas results forforforforforforforforforforforforforforforforfor ( ), ( ), and PlugPlugPlugPlugPlugging in known known known known known known known known known known known Plug Plug known known Plug Plug known known Plug Plug known Plug ( ), we have: Substitute these into the identity: [ (a+b)(b+c)(c+a) = (-3)(4) - 11 = -12 - 11 = -23. ] Thus, ( = -(a+b)(b+c)(c+a) = 23 ). Therefore, the value of ( ) is (boxed23). [ +++++++++++++++++ + = -3, ] [ ab + bc + ca = 4, ] [ abc = 11. ] ) c ( 1 0 Figure 3: Illustration of pivotal tokens for GPT-4o at temperature 1 on problem from the MATH benchmark [HBK+21], where the initial success probability is 0.31. Each token is colorized by the probability of success for an independent completion (N = 529) continued from after the token, with red for p(success) = 0 and blue for p(success) = 1. The line plot shows the same probabilities. The tokens that changes p(success) by 0.2 are shown boxed , with subscripts showing the change in probability. Tokens with probability 0.1 are underlined to illustrate that pivotal tokens are distinct from low-probability tokens. The token probabilities of negative and (a were 0.31 and 0.12, respectively. The greedy tokens for the same prefixes are product with 0.66 probability and with 0.88 probability. procedure PivotalTokenSearch(Q, Tfull, pgap) procedure Subdivide(Tprefix, ) if 1 or p(success Tprefix) p(success Tprefix + ) < pgap then Base cases. return [T ] Tleft, Tright Split(T ) return Subdivide(Tprefix, Tleft) Subdivide(Tprefix + Tleft, Tright) We split at the cumulative midpoint of token log probabilities. Tprefix œµ for all Subdivide(œµ, Tfull) do if = 1 and p(success Tprefix) p(success Tprefix + ) pgap then yield (Q, Tprefix, ) Tprefix Tprefix + Output pivotal tokens and context for postprocessing. Figure 4: Pseudocode for Pivotal Token Search (PTS). Note that estimating p(success . . . ) involves sampling the language model and invoking the oracle. In an efficient implementation p(success . . . ) should be memoized.",
                "summary": "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ DPO (Direct Preference Optimization). –î–∞–Ω–Ω—ã–µ –¥–ª—è DPO –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Ñ–æ—Ä–º–∞—Ç—ã —á–∞—Ç–æ–≤, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –¥–∞–Ω–Ω—ã–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –ò–ò (RAI). –≠—Ç–æ —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n\n–û–±—É—á–µ–Ω–∏–µ DPO –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞ –ø–æ–≤–µ—Ä—Ö –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ DPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ Pivotal Token Search (PTS) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä –∂–µ–ª–∞–µ–º—ã—Ö –∏ –Ω–µ–∂–µ–ª–∞–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è DPO —Å –æ—Ü–µ–Ω–∫–∞–º–∏, —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ–∫–æ–ª–æ 850 —Ç—ã—Å—è—á –ø–∞—Ä –∂–µ–ª–∞–µ–º—ã—Ö –∏ –Ω–µ–∂–µ–ª–∞–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ü–æ–¥—Å–∫–∞–∑–∫–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, –∞ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞—é—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –ò–ò (RAI).\n\n–ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã –æ—Ç GPT-4o, GPT-4t –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ò–∑ —ç—Ç–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ–∑–¥–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä –¥–ª—è DPO, –∏ GPT-4o –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π. –ö–∞–∂–¥–æ–º—É –æ—Ç–≤–µ—Ç—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Å—Ç–∏–ª—è –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏. –û—Ç–≤–µ—Ç —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏–ª–∏ –æ–±—â–µ–π (—Å—Ä–µ–¥–Ω–µ–π) –æ—Ü–µ–Ω–∫–æ–π —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º. –û–±–∞ —ç—Ç–∞–ø–∞ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞—é—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\n\n–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è PTS, –≥–¥–µ –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç \"–∫–ª—é—á–µ–≤—ã–µ\" —Ç–æ–∫–µ–Ω—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –æ—Ç–≤–µ—Ç–∞ –º–µ–Ω—è–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–∫–µ–Ω–∞, –∞ —Ç–∞–∫–∂–µ –≤—ã–¥–µ–ª–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ —ç—Ç—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –†–∏—Å—É–Ω–æ–∫ 4 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Å–µ–≤–¥–æ–∫–æ–¥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PTS.\n"
            },
            {
                "title": "Pivotal Token Search",
                "content": "Consider generative model producing token-by-token response to given prompt. For each token produced, which corresponds to prefix of the model response, one can consider the conditional probability of the models answer being correct given that prefix, as well as the increment in this probability with respect to that token (in other words, the difference in the probability of being correct before and after producing that token). It is often the case that the overall correctness is highly dependent on successful generation of small number of key tokens. For example, we can see in Figure 3 where the model outputs math solution and fortunate sampling of crucial token negative shifts the solution from possible failure to likely success, while sampling of the token (a subsequently risks failure again. We refer to these tokens as pivotal tokens as they have an outsized effect on the course of the solution. Now, consider how the solution from Figure 3 would be used in DPO as full-length accepted response. As the figure shows, there are many tokens with probabilities much lower than the 0.31 of negative , which would contribute to noise in the gradients diluting the signal from the pivotal token. Even worse, the token (a that contributed to the lack of robustness would receive strong positive learning signal thanks to its low probability of 0.12. Moreover, intuition suggests that when two texts substantially deviate from each other, comparison of their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it makes more sense that the signal should come from the first tokens after the two texts starts diverging from each other. To alleviate these effects, we employ method we call Pivotal Token Search (PTS) for generating preference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the preference optimization takes effect with respect to single token. PTS identifies points of completion token sequence Tfull = t1, t2, . . . for some user query where the next token ti has significant impact on the probability of success p(success t1, . . . , ti). PTS estimates these probabilities by sampling completions starting from + t1, . . . , ti, which are checked for correctness with an oracle6 for Q. Figure 4 shows basic instantiation of the algorithm. The procedure Subdivide recursively splits the sequence into segments ti, . . . , tj until the change in probability p(success t1, . . . , ti1) p(success t1, . . . , tj) for each segment is below threshold pgap or the segment is just single token. Tokens with sharp change in success probability are kept as pivotal. We turn pivotal tokens into preference data by taking + t1, . . . , ti1 as the query, and single tokens tacc and trej that increase/decrease p(success t1, . . . , ti1, tacc/rej) as the accepted and rejected completions, respectively.7 The binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but it only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the course of the solution. We used PTS to generate preference data for tasks where ground-truth is readily available, such as mathematics, various forms of question answering and coding. To improve sample efficiency, we filter the target questions to only include those with 0.2 p(success) 0.8, as pivotal tokens are rare for tasks that are very easy or hard. See Figure 5 for examples of preference data we generated using PTS. The math question answering example shows how pivotal tokens often are not actual mistakes, but choices that drive the model down less advantageous path. Here, multiplying both sides by the denominators separately is equally valid to directly cross-multiplying, but for the model doing the latter here is more robust. By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially 6For coding, comprehensive test suite can be used. For math problems, answers can be compared to the ground truth. 7We find drawing tacc and trej from rollouts PTS already used to estimate p(success t1, . . . , ti1) to be effective. 14 Find the product of all real values of for which 1 User: solution. AI: To solve the problem, we need to determine the values of such that the equation 2x = rx has exactly one real 7 has exactly one real solution for x. First, clear the fractions by Good: cross-multiplying Bad: multiplying both sides by 1 2x = 7 Adopt the axial channeling approximation to model the interaction Consider 1 keV helium atom impinging at one-degree incidence angle on LiF User: crystal surface. between the helium atom and the crystal as 2D potential energy landscape. Newtons equations to compute the helium atoms trajectory and determine the subsequent momentum transfers from binary collisions with surface atoms. energy transferred across all collisions and compute the probability, Pe, that the helium atom remains elastically scattered assuming vibrational frequency for surface atoms of œâ = 1 1014Hz. 1. Pe = exp( Eloss AI: To determine the probability Pe that the helium atom remains elastically scattered, we need to calculate the total virtual energy transferred, Eloss, during the helium atoms interaction with the LiF crystal surface. Given Good: What is Pe? 2hœâ ), 2. Pe = exp( Eloss hœâ ), or 4. Pe = 1 exp( Eloss hœâ ) Bad: that the potential interactions hœâ ), 3. Pe = exp( Eloss the vibrational frequency Sum the total virtual Apply User: function described. Read the following function signature and docstring, and fully implement the Your response should only contain the code for this function. def reorder_sequence(n: int, sequence: list[int]) -> list[int]: \"\"\" Given sequence of integer numbers that Polycarp wrote down on the board, this function rearranges (reorders) elements of this sequence to match the order Polycarp could have possibly written them. Each next number should be exactly two times the previous number or exactly one third of the previous number. :param n: The number of elements in the sequence (2 <= <= 100). :param sequence: list of integer numbers in arbitrary order (1 <= elements <= 3 * 10ÀÜ18). :return: reordered list of integer numbers representing valid sequence Polycarp could have written. \"\"\" AI: ... from collections import defaultdict # Create adjacency list where each number can point to its possible predecessor and successors graph = defaultdict(list) for num in sequence: for other in sequence: if num != other: Good: other % if num == other * 2 or ( Bad: num * 2 Figure 5: Preference data generated by Pivotal Token Search in answering math and physics questions, and implementing function in Python. The tokens that form the actual pair for DPO are underlined. 15 Figure 6: The post-training process described in Appendix A.1 decreases hallucinations. One measure is that the problems in SimpleQAwhich the model very rarely gets correctare increasingly not attempted during the course of post-training. We believe the final result is better behavior, even though the simple-evals score for SimpleQA (the F1 score) actually gives our base model higher score than our final model. stronger. In [LLX+24] contrastive estimation approach involving model trained on incorrect Related Work: trajectories is used to score which tokens likely contributed to failure, which is further employed to weigh rejected responses in DPO. In comparison, our PTS avoids complications from learned proxies by directly estimating p(success). They also report difficulties applying their method to accepted responses in DPO, while our method generates both positive and negative preference data directly targeting pivotal tokens. Automated process supervision methods [WLS+24, LLL+24] have applied search and rollouts to generate data for training process reward models. PTS can be seen as an automated process supervision method that generates token-level preference data suitable for DPO.",
                "summary": "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Pivotal Token Search (PTS), –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.\n\n**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–µ —Ç–µ–∫—Å—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º, —á–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Ö –æ–±—â–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–µ–±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ \"–∫–ª—é—á–µ–≤—ã—Ö\" —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–Ω—ã–µ \"–æ–ø–æ—Ä–Ω—ã–º–∏\" (pivotal), –º–æ–≥—É—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ö–æ–¥ —Ä–µ—à–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤—ã–±–æ—Ä –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–æ–π –≤—ã–±–æ—Ä –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–µ. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–∞–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é DPO (Direct Preference Optimization), –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ–≥—É—Ç —Å–æ–∑–¥–∞—Ç—å \"—à—É–º\" –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö, —Ä–∞–∑–º—ã–≤–∞—è —Å–∏–≥–Ω–∞–ª –æ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, —Ç–æ–∫–µ–Ω—ã, –ø—Ä–∏–≤–æ–¥—è—â–∏–µ –∫ –æ—à–∏–±–∫–µ, –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π —Å–∏–≥–Ω–∞–ª –∏–∑-–∑–∞ —Å–≤–æ–µ–π –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n\n**–†–µ—à–µ–Ω–∏–µ: Pivotal Token Search (PTS)**. PTS ‚Äì —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∏–º–µ–Ω–Ω–æ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n\n1.  **–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:** –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å. –û–Ω —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã, –ø–æ–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞ (p(success)) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–µ —Å—Ç–∞–Ω–µ—Ç –º–µ–Ω—å—à–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –¢–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–∑—ã–≤–∞—é—Ç —Ä–µ–∑–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞, –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∫–∞–∫ –æ–ø–æ—Ä–Ω—ã–µ. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–π, –Ω–∞—á–∏–Ω–∞—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é \"–æ—Ä–∞–∫—É–ª–∞\" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∑–∞–¥–∞—á –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, –∞ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á ‚Äî —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º).\n2.  **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π:** –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø–æ—Ä–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ PTS —Å–æ–∑–¥–∞–µ—Ç –ø–∞—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DPO. –ó–∞–ø—Ä–æ—Å ‚Äî —ç—Ç–æ –ø—Ä–µ—Ñ–∏–∫—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ –æ–ø–æ—Ä–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. \"–ü—Ä–∏–Ω—è—Ç–æ–µ\" –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –æ–ø–æ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞, –∞ \"–æ—Ç–≤–µ—Ä–≥–Ω—É—Ç–æ–µ\" –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞. –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ç–µ—Ö –∂–µ –≤—ã–±–æ—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ PTS –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞.\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ PTS:**\n\n*   **–¶–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:** PTS —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n*   **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —à—É–º–∞:** PTS —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ \"—à—É–º–∞\" –æ—Ç –Ω–µ–∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n*   **–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:** –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —Ç–æ–∫–µ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –æ—à–∏–±–∫–∞–º.\n*   **–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å:** PTS –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ).\n\n**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n\n*   –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –≤—Å–µ–≥–¥–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∏–∑ –Ω–∏—Ö, –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ —è–≤–ª—è–µ—Ç—Å—è –ø–æ—á—Ç–∏ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ —Ä–µ—à–µ–Ω–∏—è.\n\n**–£–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:** –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ PTS —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –≤–∫–ª—é—á–∞—è —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0.2 –¥–æ 0.8, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ä–µ–¥–∫–∏ –¥–ª—è –æ—á–µ–Ω—å –ª–µ–≥–∫–∏—Ö –∏–ª–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.\n\n**–ü—Ä–∏–º–µ—Ä—ã:** –í —Å—Ç–∞—Ç—å–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é PTS –¥–ª—è –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ñ–∏–∑–∏–∫–µ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —á–∞—Å—Ç–æ —è–≤–ª—è—é—Ç—Å—è –Ω–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏, –∞ –≤—ã–±–æ—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å –ø–æ –º–µ–Ω–µ–µ –≤—ã–≥–æ–¥–Ω–æ–º—É –ø—É—Ç–∏.\n\n**–°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏:** –ú–µ—Ç–æ–¥ PTS —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ —Å –ø–æ–¥—Ö–æ–¥–æ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –∞ —Ç–∞–∫–∂–µ —Å –º–µ—Ç–æ–¥–∞–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, PTS –Ω–∞–ø—Ä—è–º—É—é –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã–µ –Ω–∞ –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã.\n"
            },
            {
                "title": "Benchmarking Considerations",
                "content": "While academic benchmarks are widely used to measure the progress in LLM advancement, they suffer from several limitations that can fail to reveal models true capabilities and weaknesses. These limitations include: Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora, creating risk of data contamination. Although we took extensive measures to deduplicate and 16 SFT DPO stage 1 DPO stage 2 only phi-4 (stage 1 + 2) v - m MMLU GPQA MATH HumanEval MGSM SimpleQA DROP 82.8 47.3 77.1 79.5 80.8 3.7 82.8 MMLUPro 61.9 HumanEval+ 77.9 56.7 66.2 ArenaHard IFEval 84.8 53.6 80.5 81.6 80.8 2.9 86.1 70.0 81.9 66.5 63.0 PhiBench (internal) 48. 54.5 84.2 52.4 77.6 81.5 81.5 2.9 71.8 67.2 81.4 69.8 63.0 53.0 84.8 56.1 80.4 82.6 80.6 3.0 75.5 70.4 82.8 75.4 63. 56.2 Table 9: Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is more standard judge-guided DPO. Each also has 1-5% hallucination and safety data mixed in. decontaminate our training data, including standard n-gram deduplication and decontamination, these methods are not effective against all scenarios, including rephrasing, which leaves some uncertainty about the true extent of generalization. Limited Skill Scope: Most benchmarks evaluate models on narrowly defined skills, such as solving specific style of math problems at certain grade level or implementing isolated Python functions. This narrow scope can fail to capture models broader capabilities and weaknesses. Bias in Generation-Based Benchmarks: Some benchmarks use LLM-as-judge for evaluating generated outputs. These judgments sometimes may prioritize style, fluency, or surface-level qualities over accuracy and validity of the reasoning chain, leading to potential biases in scoring. Limitations of Multiple-Choice Tasks: Benchmarks that rely on multiple-choice questions often test models ability to make clever guesses that can be achieved by pattern matching rather than effectively utilizing the underlying concepts through reasoning. To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4s development. This benchmark was designed with the following goals: 1. Originality: All questions in the benchmark were composed by our team making sure that they were not present in our pretraining data. Our goal for the internal benchmark is to reveal models generalization ability in various domains. 2. Skill Diversity: Our benchmark includes wide range of tasks to assess multiple dimensions of model performance. For instance, in coding, it goes beyond isolated function implementation to include debugging, extending incomplete code, and explaining code snippets. Similarly, in 17 mathematics, it incorporates tasks like identifying the errors in proofs or generating related problems, rather than simply solving equations. This ensures that the benchmark captures broader spectrum of skills and reasoning processes. 3. Rigorous Scoring for Generation Tasks: For tasks requiring judgment of model-generated outputs, we addressed the common pitfalls of LLM-based scoring by carefully curating detailed judge instructions (or judge notes). These rubrics specify exactly how to evaluate responses, focusing on achieving accuracy, logical structure, and adherence to task requirements, while minimizing tendencies towards stylistic biases. We observed significantly improved consistency and reduction of adverse impact due to subjective preferences in the scoring outcomes. PhiBench played central role in optimizing phi-4. We used it to guide decisions about dataset mixtures and hyperparameter choices for more effective post-training techniques. PhiBench was also used to perform high-signal studies that identify weaknesses in the model and provide feedback for new incoming data sources.",
                "summary": "**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ PhiBench**\n\n–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, —Ö–æ—Ç—è –∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∏–º–µ—é—Ç —Ä—è–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–µ –≤—ã—è–≤–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π. –≠—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:\n\n1.  **–ó–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** –ú–Ω–æ–≥–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å –∫–æ—Ä–ø—É—Å–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —Ä–∏—Å–∫ \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" –¥–∞–Ω–Ω—ã—Ö, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç \"–ø–æ–º–Ω–∏—Ç—å\" –æ—Ç–≤–µ—Ç—ã –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –•–æ—Ç—è –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –º–µ—Ä—ã –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é n-–≥—Ä–∞–º–º, —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –ø—Ä–æ—Ç–∏–≤ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–∏–Ω–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é.\n\n2.  **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞–≤—ã–∫–æ–≤:** –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –ø–æ —É–∑–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ Python. –¢–∞–∫–æ–π —É–∑–∫–∏–π –æ—Ö–≤–∞—Ç –º–æ–∂–µ—Ç –Ω–µ –≤—ã—è–≤–∏—Ç—å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π.\n\n3.  **–ü—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ \"—Å—É–¥—å–∏\" –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –∏–Ω–æ–≥–¥–∞ –º–æ–≥—É—Ç –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç–∏–ª—é, –±–µ–≥–ª–æ—Å—Ç–∏ —Ä–µ—á–∏ –∏–ª–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–∞–º, –∞ –Ω–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –≤ –æ—Ü–µ–Ω–∫–µ.\n\n4.  **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∑–∞–¥–∞—á —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º:** –ë–µ–Ω—á–º–∞—Ä–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–µ–ª–∞—Ç—å \"—É–º–Ω—ã–µ\" –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –ø—É—Ç–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –æ–±—Ä–∞–∑—Ü–æ–º, –∞ –Ω–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.\n\n–ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã, –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PhiBench, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–µ –æ–∫–∞–∑–∞–ª–∏—Å—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º–æ–¥–µ–ª–∏ phi-4. –≠—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ü–µ–ª—è–º–∏:\n\n1.  **–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** –í—Å–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ –±—ã–ª–∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –¶–µ–ª—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ ‚Äî –≤—ã—è–≤–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.\n\n2.  **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –Ω–∞–≤—ã–∫–æ–≤:** –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–Ω –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –≤–∫–ª—é—á–∞—è –æ—Ç–ª–∞–¥–∫—É, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –æ–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ç–∞–∫–∏–µ –∑–∞–¥–∞—á–∏, –∫–∞–∫ –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ä–µ—à–µ–Ω–∏–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–π. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –Ω–∞–≤—ã–∫–æ–≤ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.\n\n3.  **–°—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** –î–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ü–µ–Ω–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –±—ã–ª–∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –ø—É—Ç–µ–º —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Å—É–¥–µ–π. –≠—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, –∫–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, —É–¥–µ–ª—è—è –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏, –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∑–∞–¥–∞–Ω–∏—è, —Å–≤–æ–¥—è –∫ –º–∏–Ω–∏–º—É–º—É —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∫ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º. –ù–∞–±–ª—é–¥–∞–ª–æ—Å—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –æ—Ü–µ–Ω–∫–∏.\n\nPhiBench —Å—ã–≥—Ä–∞–ª —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —Ä–æ–ª—å –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ phi-4. –û–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. PhiBench —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –≤—ã—è–≤–ª—è—é—â–∏—Ö —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏—Ö –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.\n"
            },
            {
                "title": "Performance on Key Benchmarks",
                "content": "Our benchmark results were presented in Table 1, along with comparisons to other models. We first report the values from OpenAIs simple-evals benchmark, which is framework (including prompts, temperature, and extraction) for evaluating MMLU [HBB+20], GPQA diamond [RHS+23], MATH [HBK+21], HumanEval [CTJ+21], MGSM [SSF+22], and the SimpleQA [WKC+24] F1-score. We also consider MMLU-pro [WMZ+24], HumanEval+ [LXWZ23], ArenaHard [CZS+24], and IFEval [ZLM+23], for which we use an internal framework and prompting and extraction. Finally, we use PhiBench, our internal collection of evaluations (see Section 5). phi-4 outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12 benchmarks. While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers for SimpleQA, DROP, and IFEval, we consider phi-4s behavior on SimpleQA to actually be better than Qwens. In fact, our base model gets higher benchmark score than Qwen-2.5-14B-Instruct on SimpleQA, and we intentionally modified the models behavior in post-training to optimize for better user experience rather than higher benchmark score. See Figure 6 and Appendix A.1 for details. Our model excels at STEM Q&A tasks. For example, on GPQA (graduate-level STEM questions) and MATH (math competitions), it even outscores its teacher model, GPT-4o. It also scores higher at coding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark against, including much larger Llama models. phi-4s weakest benchmark scores are on SimpleQA, DROP, and IFEval. We believe for the first two that the number reported by simple-evals is reductive and does not accurately reflect model performance on the benchmark problems. However, IFEval reveals real weakness of our model it has trouble strictly following instructions. While strict instruction following was not an emphasis of our synthetic data generations for this model, we are confident that phi-4s instruction-following performance could be significantly improved with targeted synthetic data.",
                "summary": "–í —Ç–∞–±–ª–∏—Ü–µ 1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å:\n\n*   **OpenAI simple-evals:** –≠—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö MMLU, GPQA, MATH, HumanEval, MGSM –∏ SimpleQA (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º F1-–º–µ—Ä—ã). –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤.\n*   **MMLU-pro, HumanEval+, ArenaHard –∏ IFEval:** –î–ª—è —ç—Ç–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.\n*   **PhiBench:** –≠—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—Ä–∞–º–∏ —Å—Ç–∞—Ç—å–∏.\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ phi-4 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à—É—é –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª—å Qwen-2.5-14B-Instruct –Ω–∞ 9 –∏–∑ 12 –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ phi-4 –ø–æ–∫–∞–∑–∞–ª–∞ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ SimpleQA, DROP –∏ IFEval, –∞–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ phi-4 –Ω–∞ SimpleQA —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ª—É—á—à–µ, —á–µ–º —É Qwen. –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å phi-4 –¥–∞–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ SimpleQA, —á–µ–º Qwen-2.5-14B-Instruct, –Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞, –∞ –Ω–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 6 –∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.1.\n\n–ú–æ–¥–µ–ª—å phi-4 –æ—Å–æ–±–µ–Ω–Ω–æ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ STEM Q&A (–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏). –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ GPQA (–≤–æ–ø—Ä–æ—Å—ã STEM —É—Ä–æ–≤–Ω—è –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã) –∏ MATH (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è) –æ–Ω–∞ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ—é –æ–±—É—á–∞—é—â—É—é –º–æ–¥–µ–ª—å GPT-4o. Phi-4 —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ (HumanEval –∏ HumanEval+) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Llama.\n\n–°–∞–º—ã–µ —Å–ª–∞–±—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã phi-4 –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö SimpleQA, DROP –∏ IFEval. –ê–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –æ—Ü–µ–Ω–∫–∞ SimpleQA, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é simple-evals, —è–≤–ª—è–µ—Ç—Å—è –∑–∞–Ω–∏–∂–µ–Ω–Ω–æ–π –∏ –Ω–µ—Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –û–¥–Ω–∞–∫–æ IFEval –≤—ã—è–≤–∏–ª —Ä–µ–∞–ª—å–Ω—É—é —Å–ª–∞–±–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ‚Äì –ø—Ä–æ–±–ª–µ–º—ã —Å–æ —Å—Ç—Ä–æ–≥–∏–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –•–æ—Ç—è —Å—Ç—Ä–æ–≥–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–µ –±—ã–ª–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, –∞–≤—Ç–æ—Ä—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
            },
            {
                "title": "Safety",
                "content": "We developed phi-4 in accordance with Microsofts Responsible AI principles. Our overall approach to RAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations across dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets 18 phi-3 (3B-4K) phi-3 (7B-8K) phi-3 (14B-4K) Mistral (7B-v0.1) Mistral (7B-v0.2) Llama-3 (8B) Gemma (7B) phi-4 Grounding 4. 4.701 4.787 4.065 4.692 4.672 4. 4.619 3P Content Harms (DR1) Books, News, Recipes, Songs 0.251 0.253 0.26 0.562 0. 0.373 0.383 0.121 Harmful Content Continuation (DR3) Harmful Content Summarization (DR3) Jailbreak(DR1) Hate/Fairness, Self-Harm, Sexual, Violence 0.007 0.003 0.01 0.026 0. 0.013 0.013 0.036 Hate/Fairness, Self-Harm, Sexual, Violence 0.105 0. 0.112 0.223 0.16 0.082 0.103 0. See text for covered topics 0.117 0.107 0.111 0.156 0. 0.13 0.114 0.073 Table 10: Performance comparison across models. Lower scores are better, except for Grounding, where higher score is better. phi-4 values are bold for readability. [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets to address the RAI harm categories in safety post-training.",
                "summary": "–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏ phi-4 –º—ã –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–ª–∏—Å—å –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (RAI) Microsoft. –ù–∞—à –ø–æ–¥—Ö–æ–¥ –∫ RAI –≤–∫–ª—é—á–∞–ª –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ \"–∫—Ä–∞—Å–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π\" (–∏–º–∏—Ç–∞—Ü–∏—è –∞—Ç–∞–∫) –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –ø–æ –¥–µ—Å—è—Ç–∫–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤—Ä–µ–¥–∞ RAI.\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ –º–æ–¥–µ–ª—è—Ö phi-3 (3B-4K, 7B-8K, 14B-4K). –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–µ–π Mistral (7B-v0.1, 7B-v0.2), Llama-3 (8B), Gemma (7B). –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç–æ–π [BSA+24], –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –≤—Ä–µ–¥–∞ RAI, –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.\n\n–í —Ç–∞–±–ª–∏—Ü–µ 10 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è phi-4. –û—Ü–µ–Ω–∫–∞ \"Grounding\" (—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã) —è–≤–ª—è–µ—Ç—Å—è –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º, –≥–¥–µ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ª—É—á—à–µ, –∞ –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –±–∞–ª–ª —Å—á–∏—Ç–∞–µ—Ç—Å—è –ª—É—á—à–µ. –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ phi-4 –≤—ã–¥–µ–ª–µ–Ω—ã –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏.\n\n–í —Ç–∞–±–ª–∏—Ü–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n*   **Grounding:** –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã.\n*   **3P Content Harms (DR1):** –í—Ä–µ–¥, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —Ç—Ä–µ—Ç—å–∏—Ö —Å—Ç–æ—Ä–æ–Ω, –≤–∫–ª—é—á–∞—è –∫–Ω–∏–≥–∏, –Ω–æ–≤–æ—Å—Ç–∏, —Ä–µ—Ü–µ–ø—Ç—ã –∏ –ø–µ—Å–Ω–∏.\n*   **Harmful Content Continuation (DR3):** –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n*   **Harmful Content Summarization (DR3):** –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö —Ä–µ–∑—é–º–µ.\n*   **Jailbreak (DR1):** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –æ–±—Ö–æ–¥—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n\n–í—Ä–µ–¥ –æ—Ü–µ–Ω–∏–≤–∞–ª—Å—è –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: –Ω–µ–Ω–∞–≤–∏—Å—Ç—å/—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å, –ø—Ä–∏—á–∏–Ω–µ–Ω–∏–µ –≤—Ä–µ–¥–∞ —Å–µ–±–µ, —Å–µ–∫—Å—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –Ω–∞—Å–∏–ª–∏–µ. –í —Ç–∞–±–ª–∏—Ü–µ –≤–∏–¥–Ω–æ, —á—Ç–æ phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –ø–ª–∞–Ω–µ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n"
            },
            {
                "title": "RAI Benchmarks",
                "content": "Table 10 shows the results of in-house RAI benchmarks [MHJ+23] for phi-4 compared to the phi-3 models [AAA+24], Mistral-7b-v0.1 [JSM+23], Mistral-7b-v0.2, Gemma 7b [TMH+24], and Llama-3-instruct8b [AI23b]. This benchmark utilized GPT-4o to simulate multi-turn conversations in five different categories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5 (fully grounded), and measures if the information in response is based on given prompt. In other categories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm) to 7 (severe harm) and the defect rates (DR-x) were computed as the percentage of samples with the severity score being greater than or equal to x. The Jailbreak (DR1) benchmark consists of simulated conversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular conspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people. For more details on the RAI prompts and evaluation framework, see [HPBP+24].",
                "summary": "–í —Ç–∞–±–ª–∏—Ü–µ 10 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π phi-4 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–æ–¥–µ–ª—è–º–∏ phi-3, Mistral-7b-v0.1, Mistral-7b-v0.2, Gemma 7b –∏ Llama-3-instruct8b. –î–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è GPT-4o, –∫–æ—Ç–æ—Ä—ã–π –∏–º–∏—Ç–∏—Ä–æ–≤–∞–ª –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏ –≤ –ø—è—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞–ª –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π. \n\n–û—Ü–µ–Ω–∫–∞ \"–∑–∞–∑–µ–º–ª–µ–Ω–∏—è\" (grounding) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ—Ç–≤–µ—Ç–µ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ. –û–Ω–∞ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç 0 (–Ω–µ –∑–∞–∑–µ–º–ª–µ–Ω–æ) –¥–æ 5 (–ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–∑–µ–º–ª–µ–Ω–æ). –í –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –æ—Ç–≤–µ—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞–ª–∏—Å—å –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ—Å—Ç–∏ (–æ—Ç 0 - –Ω–µ—Ç –≤—Ä–µ–¥–∞, –¥–æ 7 - —Å–µ—Ä—å–µ–∑–Ω—ã–π –≤—Ä–µ–¥). –¢–∞–∫–∂–µ –±—ã–ª–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–µ—Ñ–µ–∫—Ç–Ω–æ—Å—Ç–∏ (DR-x), –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–∑—Ü–æ–≤, –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ x.\n\n–¢–µ—Å—Ç \"Jailbreak\" (DR1) –≤–∫–ª—é—á–∞–ª –∏–º–∏—Ç–∞—Ü–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –Ω–∞ —Ç–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å–æ–≤—Ä–∞—â–µ–Ω–∏–µ–º –¥–µ—Ç–µ–π, –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º —É–±–µ–∂–¥–µ–Ω–∏–µ–º, —É—Ç–µ—á–∫–æ–π 100 —Å–ª–æ–≤ –∏–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–µ–æ—Ä–∏—è–º–∏ –∑–∞–≥–æ–≤–æ—Ä–∞, –ø—Ä–µ–¥—Ä–∞—Å—Å—É–¥–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π, –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏ –∏ –Ω–∞—Å–∏–ª–∏–µ–º –ø—Ä–æ—Ç–∏–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π. –ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ —Å–∏—Å—Ç–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Ä–∞–±–æ—Ç–µ [HPBP+24].\n"
            },
            {
                "title": "Red Teaming",
                "content": "In addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an independent group tasked with identifying safety and security vulnerabilities in Microsofts GenAI products. AIRT conducted two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both average and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior of phi-4 was similar to that of the phi-3 family, but identified several risky behaviors that were addressed by further rounds of safety post-training. In addition, the adversarial user scenario tested wide range of techniques aimed at intentionally subverting the models safety training including jailbreaks, prompt encodings, and multi-turn attacks. phi-4 showed strong defenses against these techniques. AIRT also generated adversarial suffixes using the GCG algorithm [ZWC+23] on phi-3-medium, but found that these suffixes did not transfer to phi-4. Further red teaming is required to identify possible risks across broader range of scenarios and harm categories.",
                "summary": "–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É RAI (–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞), –º—ã —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª–∏ —Å Microsoft AI Red Team (AIRT), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –≥—Ä—É–ø–ø–æ–π, –∑–∞–¥–∞—á–µ–π –∫–æ—Ç–æ—Ä–æ–π —è–≤–ª—è–µ—Ç—Å—è –≤—ã—è–≤–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –∑–∞—â–∏—Ç–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ Microsoft GenAI. AIRT –ø—Ä–æ–≤–µ–ª–∞ –¥–≤—É—Ö–Ω–µ–¥–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ phi-4, –∏–º–∏—Ç–∏—Ä—É—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–∞–∫ –æ–±—ã—á–Ω—ã—Ö, —Ç–∞–∫ –∏ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã—Ö –∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ. –í —Ü–µ–ª–æ–º, AIRT –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ phi-4 –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–æ–≤–µ–¥–µ–Ω–∏—é —Å–µ–º–µ–π—Å—Ç–≤–∞ phi-3, –Ω–æ –≤—ã—è–≤–∏–ª–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã –≤ —Ö–æ–¥–µ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –µ—ë —Å–æ–∑–¥–∞–Ω–∏—è.\n\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å—Ü–µ–Ω–∞—Ä–∏–π —Å –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ç–µ—Ö–Ω–∏–∫, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–µ–¥–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–µ –æ–±—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è \"–ø–æ–±–µ–≥–∏ –∏–∑ —Ç—é—Ä—å–º—ã\" (jailbreaks), –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –∞—Ç–∞–∫–∏. Phi-4 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å–∏–ª—å–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç —ç—Ç–∏—Ö —Ç–µ—Ö–Ω–∏–∫. AIRT —Ç–∞–∫–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ GCG –Ω–∞ phi-3-medium, –Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞, —á—Ç–æ —ç—Ç–∏ —Å—É—Ñ—Ñ–∏–∫—Å—ã –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –Ω–∞ phi-4. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏ –≤ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —É—â–µ—Ä–±–∞.\n\n*–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: \"–ü–æ–±–µ–≥ –∏–∑ —Ç—é—Ä—å–º—ã\" (jailbreak) - —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞—Å—Ç–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω–∞ –æ–±—ã—á–Ω–æ –Ω–µ –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∏–ª–∏ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.*\n"
            },
            {
                "title": "Weaknesses",
                "content": "While phi-4 achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual knowledge. For example, if is plausible human name, the model sometimes responds to prompts of the form Who is X? with hallucinated biography of the person X. This limitation would be improved by augmenting the model with search engine, but factual hallucinations cannot be eliminated completely. While phi-4 demonstrates relatively strong performance in answering questions and performing reasoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving specific formatting requirements. For instance, when tasked with generating outputs in strict tabular formats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model may produce outputs that deviate from the specified guidelines. This limitation arises in part from the models training focus, which prioritized synthetic datasets tailored toward Q&A and reasoning tasks over instruction-following scenarios. Even on reasoning tasks, phi-4 can make mistakes. For example, when asked which number is smaller, 9.9 or 9.11?, the model can conclude incorrectly that 9.9 is smaller than 9.11. Moreover, as our data contains lot of chain-of-thought examples, phi-4 sometimes gives long elaborate answers even for simple problemsthis might make user interactions tedious. We also note that while phi-4 can function as chat bot, it has been fine-tuned to maximize performance on single-turn queries. Despite diligent RAI efforts, we acknowledge challenges around reproduction or amplification of biases, inappropriate content generation, and safety issues. The use of carefully curated training data, as well as targeted post-training, and improvements from red-teaming insights, have resulted in mitigating these issues across all dimensions, but have not resolved the issues completely.",
                "summary": "–ú–æ–¥–µ–ª—å phi-4, —Ö–æ—Ç—è –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å—Ä–∞–≤–Ω–∏–º—ã–π —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤—Å–µ –∂–µ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏–∑-–∑–∞ —Å–≤–æ–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ –∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –∫–∞—Å–∞–µ—Ç—Å—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –∏–º—è, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–¥–∞—Ç—å –≤—ã–º—ã—à–ª–µ–Ω–Ω—É—é –±–∏–æ–≥—Ä–∞—Ñ–∏—é —ç—Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. –£–ª—É—á—à–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –º–æ–∂–Ω–æ, –ø–æ–¥–∫–ª—é—á–∏–≤ –∫ –º–æ–¥–µ–ª–∏ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–µ —É–¥–∞—Å—Ç—Å—è.\n\n–¢–∞–∫–∂–µ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, phi-4 —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —á–µ—Ç–∫–∏–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –æ–Ω–∏ –∫–∞—Å–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü, —Å–ø–∏—Å–∫–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ –∏–ª–∏ —Å–æ–±–ª—é–¥–µ–Ω–∏–∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è –æ—Ç –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–ª—Å—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∞ –Ω–µ –Ω–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.\n\n–î–∞–∂–µ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ phi-4 –º–æ–∂–µ—Ç –¥–æ–ø—É—Å–∫–∞—Ç—å –æ—à–∏–±–∫–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —á–∏—Å–µ–ª 9.9 –∏ 9.11 –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ—à–∏–±–æ—á–Ω–æ –∑–∞–∫–ª—é—á–∏—Ç—å, —á—Ç–æ 9.9 –º–µ–Ω—å—à–µ, —á–µ–º 9.11. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, phi-4 –∏–Ω–æ–≥–¥–∞ –¥–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–∞–∂–µ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —á—Ç–æ –º–æ–∂–µ—Ç —É—Ç–æ–º–ª—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.\n\n–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ö–æ—Ç—è phi-4 –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ —á–∞—Ç-–±–æ—Ç, –æ–Ω–∞ –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ–¥–Ω–æ—Ö–æ–¥–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.\n\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É—Å–∏–ª–∏—è –ø–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –º—ã –ø—Ä–∏–∑–Ω–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ–º –∏–ª–∏ —É—Å–∏–ª–µ–Ω–∏–µ–º –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–µ–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ —É–ª—É—á—à–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª–∏–ª–∏ —Å–Ω–∏–∑–∏—Ç—å –æ—Å—Ç—Ä–æ—Ç—É —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º, –Ω–æ –Ω–µ —É—Å—Ç—Ä–∞–Ω–∏–ª–∏ –∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é.\n"
            }
        ]
    },
    {
        "id": "2412.09605",
        "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
        "url": "https://huggingface.co/papers/2412.09605",
        "abstract": "Graphical User Interface (GUI) agents hold great potential for automating\ncomplex tasks across diverse digital environments, from web applications to\ndesktop software. However, the development of such agents is hindered by the\nlack of high-quality, multi-step trajectory data required for effective\ntraining. Existing approaches rely on expensive and labor-intensive human\nannotation, making them unsustainable at scale. To address this challenge, we\npropose AgentTrek, a scalable data synthesis pipeline that generates\nhigh-quality GUI agent trajectories by leveraging web tutorials. Our method\nautomatically gathers tutorial-like texts from the internet, transforms them\ninto task goals with step-by-step instructions, and employs a visual-language\nmodel agent to simulate their execution in a real digital environment. A\nVLM-based evaluator ensures the correctness of the generated trajectories. We\ndemonstrate that training GUI agents with these synthesized trajectories\nsignificantly improves their grounding and planning performance over the\ncurrent models. Moreover, our approach is more cost-efficient compared to\ntraditional human annotation methods. This work underscores the potential of\nguided replay with web tutorials as a viable strategy for large-scale GUI agent\ntraining, paving the way for more capable and autonomous digital agents.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "298173f67e05442e",
        "authors": [
            "Yiheng Xu",
            "Dunjie Lu",
            "Zhennan Shen",
            "Junli Wang",
            "Zekun Wang",
            "Yuchen Mao",
            "Caiming Xiong",
            "Tao Yu"
        ],
        "affiliations": [
            "Salesforce Research",
            "University of Hong Kong"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.09605.jpg",
        "data": {
            "categories": [
                "#data",
                "#dataset",
                "#training",
                "#optimization",
                "#agents",
                "#synthetic"
            ],
            "emoji": "ü§ñ",
            "ru": {
                "title": "AgentTrek: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤",
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AgentTrek - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUI) —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤. –ú–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —É—á–µ–±–Ω—ã–µ –ø–æ—Å–æ–±–∏—è, –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —Ü–µ–ª–∏ –∑–∞–¥–∞—á —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ. –û–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç–µ–∫—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º."
            },
            "en": {
                "title": "Automating GUI Agent Training with Web Tutorials",
                "desc": "This paper introduces AgentTrek, a novel approach for generating high-quality training data for Graphical User Interface (GUI) agents. It addresses the challenge of obtaining multi-step trajectory data by automatically synthesizing it from web tutorials, thus eliminating the need for costly human annotation. The method involves transforming tutorial texts into actionable task goals and using a visual-language model to simulate the execution of these tasks. The results show that training GUI agents with these synthesized trajectories enhances their performance in grounding and planning, making the process more efficient and scalable."
            },
            "zh": {
                "title": "Âà©Áî®ÁΩëÁªúÊïôÁ®ãÊèêÂçáGUI‰ª£ÁêÜËÆ≠ÁªÉÊïàÁéá",
                "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AgentTrekÁöÑÊï∞ÊçÆÂêàÊàêÁÆ°ÈÅìÔºåÁî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËá™Âä®Êî∂ÈõÜÁΩëÁªúÊïôÁ®ãÊñáÊú¨ÔºåÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫‰ªªÂä°ÁõÆÊ†áÂíåÈÄêÊ≠•Êåá‰ª§ÔºåÂπ∂Âà©Áî®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰ª£ÁêÜÂú®ÁúüÂÆûÊï∞Â≠óÁéØÂ¢É‰∏≠Ê®°ÊãüÊâßË°å„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫∫Á±ªÊ†áÊ≥®ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊàêÊú¨‰∏äÊõ¥ÂÖ∑ÊïàÁéáÔºåÂêåÊó∂ÊòæËëóÊèêÈ´ò‰∫ÜGUI‰ª£ÁêÜÁöÑÂü∫Á°ÄÂíåËßÑÂàíÊÄßËÉΩ„ÄÇÊ≠§Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂà©Áî®ÁΩëÁªúÊïôÁ®ãËøõË°åÂºïÂØºÈáçÊîæÁöÑÊΩúÂäõÔºå‰∏∫Â§ßËßÑÊ®°GUI‰ª£ÁêÜËÆ≠ÁªÉÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (GUI). –¢–∞–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö, –Ω–æ –∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω–∞ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –¥–µ–π—Å—Ç–≤–∏–π (—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π). –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–∞–∑–º–µ—Ç–∫–∏ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —Ä—É—á–Ω—É—é —Ä–∞–±–æ—Ç—É.</p>\n<p>–ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç AgentTrek ‚Äì –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ–±-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<ol>\n<li><strong>–°–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π:</strong> AgentTrek –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.</li>\n<li><strong>–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∏:</strong> –≠—Ç–∏ —Ç–µ–∫—Å—Ç—ã –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∏ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.</li>\n<li><strong>–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:</strong> –ê–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (VLM), –∏–º–∏—Ç–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ.</li>\n<li><strong>–û—Ü–µ–Ω–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏:</strong> VLM-–º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.</li>\n</ol>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å—Ç–∞—Ç—å—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–µ–±-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤.</p>"
            },
            {
                "title": "Introduction",
                "content": "Graphical User Interfaces (GUIs) are fundamental medium for human-computer interaction, enabling users to perform tasks across various digital pl atforms. Automating GUI operations through agentic automation has the potential to significantly enhance productivity by enabling autonomous task completion using human-centric tools. Additionally, this approach can foster the development of advanced AI systems capable of learning from rich digital environments. Recent advancements in large language models (LLMs) have endowed the models with powerful abilities in understanding, reasoning, and decisionmaking, which are essential for the evolution of GUI agents in diverse contexts such as web (Zheng et al., 2024), desktop (Xie et al., 2024), and mobile applications (Zhang et al., 2023). Despite these advancements, the performance of GUI agents remains suboptimal. Contemporary Large Language Models (LLMs) are primarily engineered and trained on datasets optimized for generating informative responses (Ouyang et al., 2022; OpenAI, 2024). Their architecture and training paradigms are Figure 1: Expected Agent Trajectories Equal contribution 1 Figure 2: Overview of the AgentTrek Pipeline: (1) Automatic Tutorial Collection from the Internet: Tutorial-related data is extracted and filtered from internet sources using heuristic methods and FastText model. An LLM processes the filtered textual data, transforming it into structured tutorials. (2) Trajectory data collection via guided replay: VLM agent interacts with the real digital environment guided by tutorials, while high-quality trajectory data, including observations, actions, and reasoning, is collected. Another VLM evaluator acts as judger to further improve the effectiveness of the synthetic dataset. (3) Training and fine-tuning with replay data: The collected trajectory data is used to train and fine-tune GUI agent models, which are evaluated on standard agent benchmarks, demonstrating significant improvements. not inherently designed to make complex, sequential action decisions that require long-term observation and historical context. Consequently, training GUI agents with multi-step trajectory data is crucial to improving their capabilities. High-quality agent trajectories contain several key components: high-level goal, sequence of interleaved observations, natural language reasoning, and grounded actions (as shown in Figure 1). Unfortunately, such data is not readily available on the internet like textual or image data, as it involves complex situational reasoning and multimodal interactivity. Existing approaches typically rely on human annotation to collect these trajectories (Deng et al., 2024; Rawles et al., 2023; Li et al., 2024), process that is both expensive and not scalable. To address this data scarcity, data synthesis has emerged as vital approach in AI system development. Synthesizing agent trajectories presents significant challenges due to the need for interwoven natural language instructions, visual observations, and context-specific actions that must be accurately grounded in the GUI environment. Although there have been some successful applications of LLMs in data synthesis pipelines (Ye et al., 2022; Peng et al., 2023; Qin et al., 2023), these complexities still make GUI trajectory synthesis particularly demanding. In this work, we present AgentTrek, scalable data synthesis pipeline specifically designed for training GUI agents. We begin by automatically gathering and filtering tutorial-like text from the web, which describes GUI tasks and workflows in web environments. These tutorials are then transformed into agent tasks with high-level objectives and detailed step-by-step instructions. Using visual-language model (VLM) agent, we simulate the execution of these tasks, guided by the synthesized tutorials. An evaluator model is also employed to subsequently verify whether the goal was successfully achieved. Through this comprehensive pipeline, we efficiently generated large volume of high-quality web agent trajectories. Our experimental results demonstrate that training GUI agent models with these synthesized web agent trajectories not only improves their performance but also enables them to surpass the capabilities of their initial teacher models, which is the replay model GPT-4 in our case. Compared to traditional human-annotated data pipelines, our method is significantly more cost-effective, emphasizing the scalability and economic viability of the AgentTrek pipeline. We introduce AgentTrek, novel pipeline that leverages web tutorials to synthesize highquality web agent trajectory data at scale, effectively bridging the gap between LLM capabilities and the demanding need for multi-step, context-rich training data for GUI agents. 2 Extensive experiments demonstrate that agents trained with our synthesized data outperform those trained on existing datasets in both grounding and planning capabilities, validating the effectiveness of AgentTrek. Our pipeline significantly reduces the cost and scalability obstacles of human-annotated data collection, providing practical approach for large-scale GUI agent training through data synthesis. Table 1: Comparison of AgentTrek with other trajectory datasets for training. For the calculation of dataset size and average steps, see Appendix A. Datasets RUSS ScreenAgent WebLINX MM-Mind2Web GUIAct Size 80 203 969 1009 2482 AgentTrek (Ours) 10398 Average Steps HTML AxTree Intermediate Reasoning Video Matching Screenshot Website Task Inst. Level 5.4 4.3 18.8 7.3 6.7 12. Yes No Yes Yes No Yes No No No No No Yes No Yes No No No Yes No No No No No Yes No Yes Yes No Yes Yes 22 - 155 137 121 Low High & Low High & Low High High 127 High & Low",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUI). –¢–∞–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –æ–±–ª–∞–¥–∞—é—Ç –º–æ—â–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø—Ä–∏–Ω—è—Ç–∏—é —Ä–µ—à–µ–Ω–∏–π, –Ω–æ –æ–Ω–∏ –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ —É—á–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ü–µ–ª—å, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ –¥–µ–π—Å—Ç–≤–∏—è, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ GUI.</p>\n<p>–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Ç–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Ç—Ä–µ–±—É—é—Ç —Å–ª–æ–∂–Ω–æ–≥–æ —Å–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â—É—é —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –≤ —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AgentTrek.</p>\n<p>AgentTrek –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π –∑–∞–¥–∞—á–∏ –∏ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –≤ –≤–µ–±-—Å—Ä–µ–¥–µ. –≠—Ç–∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∏ –∞–≥–µ–Ω—Ç–∞ —Å —Ü–µ–ª—è–º–∏ –∏ –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. –ó–∞—Ç–µ–º –∞–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–æ–¥–µ–ª—å VLM (–≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å), –∏–º–∏—Ç–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –∑–∞–¥–∞—á, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏.</p>\n<p>–° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å–º–æ–≥–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–≤–æ–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-—É—á–∏—Ç–µ–ª–µ–π (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ GPT-4). –í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞–º–∏ —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π, –º–µ—Ç–æ–¥ AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, AgentTrek –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –Ω–µ—Ö–≤–∞—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ —Å–Ω–∏–∑–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–º –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤.</p>"
            },
            {
                "title": "Method",
                "content": "We introduce pipeline to collect and process GUI tutorials from the internet for training visual language models (VLMs) in web automation tasks. The method comprises three main steps: 1. Collecting Tutorials: We extract web interaction tutorials from large datasets using keyword filtering and language models to identify and standardize relevant content. 2. Guided Replay: An agent uses these tutorials to perform tasks in web environment, interacting with real websites while we record its actions and thoughts. 3. Model Training: We train visual agent model that relies on screenshots and standard GUI actions, enhancing its web navigation capabilities with the collected data. This approach enables efficient training of VLMs without extensive manual annotation, offering scalable solution for automating web tasks. 2.1 AUTOMATIC TUTORIALS COLLECTION FROM INTERNET We first extract web interaction tutorials from Redpajama dataset (Computer, 2023). rule-based heuristic filter is applied to create preliminary dataset, subset of which is annotated by an advanced LLM to generate labeled samples for training effective FastText classification model (Joulin et al., 2017), the tutorial classifier. This classifier further enhances the data quality through filtering. Finally, LLMs are employed to tag and paraphrase the raw text of tutorials into standardized format, preparing them for the replay phase in Section 2.2. 2.1.1 PREFILTER FUNCTION Although GUI tutorials are abundant online, they constitute only small fraction of web content, making pre-filter essential for identifying relevant content. Similar patterns often appear in tutorials, such as distinctive keywords like click and type, as well as platform-specific terms like macOS and Windows. We compiled rule-based filter using keyword lists sourced from official websites and forums. Leveraging RedPajama data with over 20 billion URLs, our pre-filter applies Keyword Matching in the first 38k words, evaluates samples based on Length, and filters them by URL Format for relevance. Validated using 180 positive and 105 negative ground-truth samples, the prefilter achieved 92.69% recall rate on positive samples, ensuring both diversity and quantity. After filtering, the dataset size is reduced from 20.8 billion to 68.8 million entries (Figure 4). 3 Figure 3: Overview of the tutorial filtering and classification pipeline. Starting with Redpajama, the data is prefiltered, annotated by an advanced LLM, and used to train tutorial classifier. The classifier further filters the raw text, which is then paraphrased into structured tutorials with task descriptions, prerequisites, and step-by-step instructions. 2.1.2 LLM LABELER While initial rule-based filtering narrows the context, the proportion of true positive tutorial content remains low. To improve the quality and relevance of selected tutorials, we leverage an advanced LLM, GPT-4O MINI, for automated labeling, due to its strong ability to comprehend and analyze complex, information-dense text. Prior to full implementation, we tested GPT-4O MINI on manually annotated ground-truth validation set, where it achieved an F1 score nearly 90%. In cases where human and LLM annotations conflicted, the LLM demonstrated the ability to identify tutorial-related content in lengthy texts that humans might overlook. This, along with the validation set result, suggests that GPT-4O MINI may surpass human performance in webpage labeling, enabling efficient generation of large labeled dataset for training in the following section. 2.1.3 FASTTEXT FILTER Following the automated labeling process, we employed FastText, an n-gram-based deep learning model, as our classifier. FastText classifies tutorial text segments as tutorial or non-tutorial, with binary output and confidence score to enhance the accuracy of tutorial selection. To train the model, we combined LLM-labeled data with human-labeled samples, creating dataset of approximately 90,000 examples. The train-test split is 95:5, with the model demonstrating strong classification performance. Using this classifier, we further curated the initial filtered dataset, collecting approximately 18.8 million tutorial-like web text samples. Prefilter LLM FastText 0.69 0.885 0.895 Metric Precision Recall F1 0.60 0.89 0. 0.61 0.885 0.895 Table 2: Performance of Filters. 2.1.4 TAG & PARAPHRASE After filtering the raw tutorial content using the FastText model, we then tag and paraphrase the content for further processing, including extracting meta-information and formatting the tutorials according to standardized template. To handle the length and noise of the raw tutorial data, we employed GPT-4O MINI, streamlining the tagging and paraphrasing while ensuring the output aligned with the comprehensive template and gold-standard examples. The key components of the template include specifying the Platform and Target (e.g., macOS, Windows, browser, or app), providing concise Task Description, listing Prerequisites needed Figure 4: The data flow during the early stages of our pipeline. before starting, outlining Step-by-Step Instructions for completing the task, and detailing the Expected Outcome. The cost for tagging and paraphrasing 1,000 entries is approximately 0.89 dollars.",
                "summary": "<p>–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (—Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤) –ø–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (GUI) –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLMs) –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á.</p>\n<p><strong>2.1 –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞</strong></p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤:</p>\n<ol>\n<li>\n<p><strong>–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Pre-filter):</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–∫–ª–∏–∫–Ω—É—Ç—å\", \"–≤–≤–µ—Å—Ç–∏\"), —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –¥–ª—è —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤, –∞ —Ç–∞–∫–∂–µ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"macOS\", \"Windows\").</li>\n<li>–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –±–æ–ª—å—à–æ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö Redpajama (–±–æ–ª–µ–µ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ URL).</li>\n<li>–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞ URL.</li>\n<li>–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –ø–æ–ª–Ω–æ—Ç–∞ (recall) –≤ 92.69% –ø—Ä–∏ –æ—Ç–±–æ—Ä–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∫—Ä–∞—Ç–∏–≤ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å 20.8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 68.8 –º–∏–ª–ª–∏–æ–Ω–∞ –∑–∞–ø–∏—Å–µ–π.</li>\n</ul>\n</li>\n<li>\n<p><strong>–†–∞–∑–º–µ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM Labeler):</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å GPT-4O MINI –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–º.</li>\n<li>–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤—Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç F1-–º–µ—Ä—É –æ–∫–æ–ª–æ 90%.</li>\n<li>–í —Å–ª—É—á–∞—è—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ç–∫–æ–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ LLM, –º–æ–¥–µ–ª—å –∏–Ω–æ–≥–¥–∞ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–ª–∞ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –≤ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ —á–µ–ª–æ–≤–µ–∫ –º–æ–≥ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.</li>\n</ul>\n</li>\n<li>\n<p><strong>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é FastText:</strong></p>\n<ul>\n<li>–û–±—É—á–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å FastText (–º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ n-–≥—Ä–∞–º–º–∞—Ö) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Ç—É—Ç–æ—Ä–∏–∞–ª –∏–ª–∏ –Ω–µ —Ç—É—Ç–æ—Ä–∏–∞–ª.</li>\n<li>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–º–µ—Å—å –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö LLM –∏ –≤—Ä—É—á–Ω—É—é, –≤—Å–µ–≥–æ –æ–∫–æ–ª–æ 90 000 –ø—Ä–∏–º–µ—Ä–æ–≤.</li>\n<li>–ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –æ—Å—Ç–∞–≤–∏–≤ –æ–∫–æ–ª–æ 18.8 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã.</li>\n</ul>\n</li>\n<li>\n<p><strong>–¢–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (Tag &amp; Paraphrase):</strong></p>\n<ul>\n<li>–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é FastText, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã —Ç–µ–≥–∏—Ä—É—é—Ç—Å—è –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—é—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.</li>\n<li>–î–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPT-4O MINI, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —à–∞–±–ª–æ–Ω–æ–º.</li>\n<li>–®–∞–±–ª–æ–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:<ul>\n<li><strong>–ü–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ —Ü–µ–ª—å:</strong> (–Ω–∞–ø—Ä–∏–º–µ—Ä, macOS, Windows, –±—Ä–∞—É–∑–µ—Ä –∏–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ).</li>\n<li><strong>–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏.</strong></li>\n<li><strong>–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —É—Å–ª–æ–≤–∏—è</strong> –¥–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–ü–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</strong> –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.</strong></li>\n</ul>\n</li>\n<li>–≠—Ç–æ—Ç —ç—Ç–∞–ø –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ 1000 –∑–∞–ø–∏—Å–µ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 0.89 –¥–æ–ª–ª–∞—Ä–∞.</li>\n</ul>\n</li>\n</ol>\n<p>–í –∏—Ç–æ–≥–µ, —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞—Ç—å –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º —à–∞–≥–æ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á.</p>"
            },
            {
                "title": "Trajectory Data Collection via Guided Replay",
                "content": "Figure 5: Overview of Guided Replay data collection and evaluation pipeline. VLM agent is provided with the filtered and formatted tutorials, then observes and interacts with the real environment during execution, while all the actions and intermediate thoughts are recorded as data trajectory. The final result is evaluated by an advanced VLM to ensure the correctness. 2.2.1 TRAJECTORY DATA DEFINITION The trajectory data generated by our pipeline is designed to enhance an agents web navigation capabilities by integrating high-level planning, low-level instructions, and grounded operations. Each data instance includes the following components: Task Information. Detailed task metadata, including platform, task description, prerequisites, instructions, and expected outcomes, which support both planning and execution. Post-processed Textual Trajectory. Refined after replay, highlighting key elements for model finetuning. This includes Task Metadata, summarizing the task to encourage adaptive decision-making, Observations to provide visual context, Intermediate Reasoning offering insights into the agents decision-making process, and Action Sequence to capture detailed element information for web interactions. Screenshots and Video Recordings. Visual records of the entire process for comprehensive documentation. Reproducible Native Trace. Captured via Playwright, including DOM snapshots, HTML, network flow, and action sequences, allowing full reconstruction and detailed analysis of agent-environment interactions.",
                "summary": "<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –∏ –æ—Ü–µ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–º–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (Guided Replay):</strong></p>\n<ol>\n<li><strong>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞:</strong> –ê–≥–µ–Ω—Ç—É (VLM agent) –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (tutorials).</li>\n<li><strong>–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:</strong> –ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å —Ä–µ–∞–ª—å–Ω–æ–π –≤–µ–±-—Å—Ä–µ–¥–æ–π, –≤—ã–ø–æ–ª–Ω—è—è –∑–∞–¥–∞–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.</li>\n<li><strong>–ó–∞–ø–∏—Å—å:</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞, –µ–≥–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠—Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û—Ü–µ–Ω–∫–∞:</strong> –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π VLM (–±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏), —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n</ol>\n<p><strong>–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏:</strong></p>\n<p>–î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–∞ –∫ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∑–∞ —Å—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:</p>\n<ul>\n<li><strong>–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–µ (Task Information):</strong> –ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –∑–∞–¥–∞—á–µ, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —É—Å–ª–æ–≤–∏—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç—É –∫–∞–∫ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ç–∞–∫ –∏ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è (Post-processed Textual Trajectory):</strong> –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è, –¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è (replay). –û–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:<ul>\n<li><strong>–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (Task Metadata):</strong> –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.</li>\n<li><strong>–ù–∞–±–ª—é–¥–µ–Ω–∏—è (Observations):</strong> –í–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç–æ–º –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (Intermediate Reasoning):</strong> –ó–∞–ø–∏—Å–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–º.</li>\n<li><strong>–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π (Action Sequence):</strong> –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–∞ —Å –≤–µ–±-—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏.</li>\n</ul>\n</li>\n<li><strong>–°–∫—Ä–∏–Ω—à–æ—Ç—ã –∏ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏ (Screenshots and Video Recordings):</strong> –í–∏–∑—É–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</li>\n<li><strong>–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π –Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ª–µ–¥ (Reproducible Native Trace):</strong> –ó–∞–ø–∏—Å—å, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é Playwright, –≤–∫–ª—é—á–∞—é—â–∞—è –≤ —Å–µ–±—è —Å–Ω–∏–º–∫–∏ DOM, HTML, —Å–µ—Ç–µ–≤–æ–π —Ç—Ä–∞—Ñ–∏–∫ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å–æ —Å—Ä–µ–¥–æ–π –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑.</li>\n</ul>\n<p><strong>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:</strong> –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ Guided Replay, —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, –∫–∞–∫ –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –≤–µ–±-—Å—Ä–µ–¥–æ–π. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.</p>"
            },
            {
                "title": "Guided Replay with Tutorials",
                "content": "Although we have collected and processed high-quality tutorials, significant gap remains in acquiring the grounding data crucial for training more effective agent model. To address this, we leverage BrowserGym (Drouin et al., 2024) to enable the model to replay tasks under the guidance of the generated tutorials. Figure 6: Guided replay example. This example demonstrates an agents execution of finding the return policy for mens football apparel, showcasing its actions alongside the corresponding inner thoughts. BrowserGym is versatile environment for web task automation in the Chromium browser, enabling Visual Language Model (VLM) agents to execute web-based operations (Drouin et al., 2024). Agents are provided with tagged and paraphrased tutorials and target web url, allowing them to navigate directly to the task scene. Step-by-step instructions guide the agent through the task, with the expected outcome determining task completion. The agents initial observations include the webpages viewport screenshot and accessibility tree (AXTree), but the HTML file is excluded due to its size and irrelevance to visual agents. Actions are executed using Playwright (Microsoft, 2023) functions such as click, select option, and clear, while Playwright also records detailed traces, including target elements, coordinates, screenshots, and DOM snapshots at the same time, along with agents internal thoughts between actions. Token consumption is about 8,027 per step and 86,114 per task. With GPT-4O-08-06, replaying 1,000 tasks costs approximately 215 dollars. Cost detail see 2.2.3 EVALUATION OF TRAJECTORY Although large amount of guided replay data has been recorded, it is crucial to extract the effective segments that can truly contribute to enhancing the agents performance. Recent work by (Pan et al., 2024) highlights the potential of Visual Language Models (VLMs) in evaluating trajectory data using recorded images and interaction processes as input. VLMs are highly scalable, capable of processing large datasets concurrently at low cost, and provide transparent evaluations. Therefore, we implemented VLM Evaluator to further improve our data quality. VLM Evaluator Design. To ensure trajectory data quality, we define effectiveness based on two criteria: adherence to task instructions and successful completion of core components. We employ GPT-4O as the backbone of our VLM evaluator, using structured prompt to assess recorded trajectories. The evaluator receives the task description d, the agents action history a, and inner thoughts for each step. The sequential format is: {task description; inner thought 1; action 1; inner thought 2; action 2; ...}, as illustrated in Figure 5. The VLM provides trajectory-level assessment and performs stepwise analysis, offering justifications for any ineffective trajectory and identifying the earliest point of failure. Validation on Human-Annotated Set. Although the capabilities of Vision Language Models (VLMs) are well-recognized, validation is essential. To assess the automatic evaluators perfor6 Table 3: Evaluator Accuracy Comparison Table 4: Cost Breakdown Trajectory Evaluator Replayed Web Tutorials GPT-4o WebArena Results GPT-4V Cap. + GPT-4 Cap. + Mixtral Acc. 84.0% 80.6% 82.1% 74.4% Phase Cost/1k ($) Model T&P Replay Eval Total 0.89 215.36 3.10 219.35 gpt-4o-mini gpt-4o gpt-4o mance, we manually reviewed 1,081 trajectories and created validation set of 558 samples with human-annotated justifications. As shown in the Table 3, despite handling different input formats across various task scenarios, the evaluator achieved high performance metrics. And according to the observation shown in Appendix D, evaluator often applys stricter standards than human evaluators. This demonstrates its robustness in accurately identifying effective trajectories. 2.3 TRAIN AND FINE-TUNE THE MODEL WITH TRAJECTORY DATA We chose purely visual agent model that relies exclusively on screenshot-based observations, rather than incorporating accessibility trees or textual representations, for several reasons. First, GUIs are inherently visual, and mapping instructions to visual elements aligns more closely with human cognitive processes. Second, Textual representations, such as HTML or accessibility trees, are often verbose, which leads to heavy overhead for computation. Second, Different websites can have varying structures for their textual representation while image-based representations allow the model to unify its observations across diverse platforms with varying resolutions, improving generalization. 2.3.1 PURE VISION & GUI ACTION FRAMEWORK In this work, we propose to unify observation and action space via pure vision and standard pyautogui commands with pluggable action system. Using pure vision as input eliminates the need for the model to understand different UI source codes across platforms, even when visually similar elements are written differently in HTML. Additionally, HTML input typically costs an average of 4,000 tokens per step. In contrast, recent VLMs with high-resolution multimodal understanding, such as Qwen2-VL, require only 1,200 tokens for 720p image. This significantly lowers the computational cost while maintaining sufficient visual information for the task. For action, we hoose the widely used standard pyautogui action space with pluggable action system. Most web agent leverage playwright action sapce. But playwright actions incline to interact with html selector element instead of visual ui element. Therefore, we use pyautogui commands to unify basic GUI operations on web. Since we collect the data from website by playwright, we need to map the playwright actions to pyautogui actions as shown in the Figure 9. In addition, we utilize pluggable action system to cover specific playwright action like select option. 2.3.2 MODEL ARCHITECTURE AND TRAINING Unlike agents that rely on structured UI representations like accessibility trees, vision-based grounding requires models to map intents directly to visual observations. For this, we chose Qwen2VL (Wang et al., 2024), which uses NaViT as an image encoder with dynamic resolution support Dehghani et al. (2023). By removing absolute position embeddings and incorporating 2D-RoPE Su et al. (2024), Qwen2-VL can process images of any resolution, efficiently converting them into variable visual tokens. This makes Qwen2-VL ideal for GUI agents, as it can encode high-resolution images with fewer token costs, making it well-suited for our tasks. Our training process, starting with VLM capable of high-resolution image understanding, consists of one tunnig stage. We use data from AgentTrek Data to enhance VLM capabilities in grounding and planning.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏: –£–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞</h2>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∞–≥–µ–Ω—Ç–æ–≤, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å—Ä–µ–¥—É BrowserGym, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ \"–ø–µ—Ä–µ–∏–≥—Ä—ã–≤–∞—Ç—å\" –∑–∞–¥–∞—á–∏, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã.</p>\n<p><strong>BrowserGym</strong> - —ç—Ç–æ —Å—Ä–µ–¥–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á –≤ –±—Ä–∞—É–∑–µ—Ä–µ Chromium. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º –º–æ–¥–µ–ª–∏ VLM (Visual Language Model), –≤—ã–ø–æ–ª–Ω—è—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö. –ê–≥–µ–Ω—Ç–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ URL —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ü–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–µ–¥—É—Ç –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á—É, –∞ –µ–µ —É—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.</p>\n<p>–ù–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤–∫–ª—é—á–∞—é—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç –≤–∏–¥–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–µ—Ä–µ–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (AXTree). HTML-—Ñ–∞–π–ª –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è –∏–∑-–∑–∞ –µ–≥–æ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –î–µ–π—Å—Ç–≤–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–π Playwright, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–ª–∏–∫, –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∞ –ø–æ–ª—è. Playwright —Ç–∞–∫–∂–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏, –≤–∫–ª—é—á–∞—è —Ü–µ–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –∏ DOM-—Å–Ω–∏–º–∫–∏, –∞ —Ç–∞–∫–∂–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏.</p>\n<p>–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞: –æ–∫–æ–ª–æ 8027 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —à–∞–≥ –∏ 86114 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–¥–∞—á—É. –ü–µ—Ä–µ–∏–≥—Ä—ã–≤–∞–Ω–∏–µ 1000 –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT-4O-08-06 –æ–±—Ö–æ–¥–∏—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 215 –¥–æ–ª–ª–∞—Ä–æ–≤.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π</strong></p>\n<p>–ü–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏ –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–µ—Ä–µ–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –≤—ã–¥–µ–ª–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞. –î–ª—è —ç—Ç–æ–≥–æ –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ <strong>VLM Evaluator</strong> (–æ—Ü–µ–Ω—â–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ VLM), –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. VLM-–º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è, —Å–ø–æ—Å–æ–±–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω—É—é –æ—Ü–µ–Ω–∫—É.</p>\n<p><strong>–î–∏–∑–∞–π–Ω VLM Evaluator</strong></p>\n<p>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–≤—É–º—è –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏: —Å–æ–±–ª—é–¥–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —É—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∑–∞–¥–∞—á–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –æ—Ü–µ–Ω—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å GPT-4O. –û—Ü–µ–Ω—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏, –∏—Å—Ç–æ—Ä–∏—é –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ –∏ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {–æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏; —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ 1; –¥–µ–π—Å—Ç–≤–∏–µ 1; —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ 2; –¥–µ–π—Å—Ç–≤–∏–µ 2; ...}. VLM –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑, –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—è –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è—è –º–æ–º–µ–Ω—Ç –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–∏.</p>\n<p><strong>–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞ –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 558 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ 1081 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω—â–∏–∫–∞, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∑–∞–¥–∞—á. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω—â–∏–∫ —á–∞—Å—Ç–æ –ø—Ä–∏–º–µ–Ω—è–ª –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã, —á–µ–º –ª—é–¥–∏-–æ—Ü–µ–Ω—â–∏–∫–∏, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ–≥–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.</p>\n<p><strong>–û–±—É—á–µ–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∞–≤—Ç–æ—Ä—ã –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–∫—Ä–∏–Ω—à–æ—Ç–∞—Ö), –∞ –Ω–µ –Ω–∞ –¥–µ—Ä–µ–≤—å—è—Ö –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö. –≠—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ —Ç–µ–º, —á—Ç–æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ –≤–∏–∑—É–∞–ª—å–Ω—ã, –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –±–æ–ª—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–º–∏ –∏ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –±–æ–ª—å—à–∏–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –∑–∞—Ç—Ä–∞—Ç–∞–º.</p>\n<p><strong>–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ GUI</strong></p>\n<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π —Å –ø–æ–º–æ—â—å—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ pyautogui —Å –ø–æ–¥–∫–ª—é—á–∞–µ–º–æ–π —Å–∏—Å—Ç–µ–º–æ–π –¥–µ–π—Å—Ç–≤–∏–π. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å HTML.</p>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –≤—ã–±—Ä–∞–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã pyautogui, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å GUI. –ü–æ—Å–∫–æ–ª—å–∫—É –¥–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Playwright, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è Playwright –≤ –¥–µ–π—Å—Ç–≤–∏—è pyautogui. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∞–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –æ—Ö–≤–∞—Ç–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π Playwright, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏.</p>\n<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–ø–æ—Å–æ–±–Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º–∏, –±—ã–ª–∞ –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å Qwen2-VL, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è NaViT –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. Qwen2-VL —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª—é–±–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Ö –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç Qwen2-VL –∏–¥–µ–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ GUI, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ –º–æ–∂–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –º–µ–Ω—å—à–∏–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.</p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ AgentTrek Data –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π VLM –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–≤—è–∑–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.</p>"
            },
            {
                "title": "Experimental Setup",
                "content": "Agent Training. For the text-based web agent, we utilize 6,000 agent trajectories, included accessibility tree as observations and playwright actions as the agents action space, from the AgentTrek dataset to fine-tune the Qwen2 series LLMs of various sizes, including 7B, 32B, and 72B. For the vision-only Web agent, we fine-tune Qwen2-VL Wang et al. (2024) using 10,000 selected agent trajectories from the dataset. Evaluation For Text-based Web Agent. To demostrate the capability of the text-based agent, we select WebArena Zhou et al. (2023) as evaluation benchmark. WebArena, based on real websites, creates multiple virtual environments and uses various evaluation methods to assess the task completion rate, making it more suitable for real-world task completion evaluation. Evaluation For Vision-based Web Agent. To validate the effectiveness of our dataset, it is essential to demonstrate its impact on improving both the grounding and planning capabilities of the model. Therefore, we will evaluate the models performance on two benchmarks that assess these abilities. (1) ScreenSpot (Cheng et al., 2024), which is GUI visual grounding benchmark comprising 1.2K single-step instructions and target element bounding boxes. It spans mobile, desktop, and web environments, categorizing elements into text and icons. Given that our data originates exclusively from the web, we focus solely on web-based performance. (2) Multimodal-Mind2Web (Deng et al., 2024), the multimodal extension of the web agent benchmark Mind2Web (Deng et al., 2024). It consists of three categories, namely, cross-task, cross-website and cross-domain, ranked by their degree of deviation from the training data. 3.2 MAIN RESULTS WebArena. Through the experimental results from table 5, we can obtain that: (1) AgentTreks textual trajectories significantly boost performance on WebArena, surpassing open-source baselines and GPT-4o. (2) Considering that WebArena is an OOD web agent benchmark featuring self-hosted websites, strong results on WebArena confirm that AgentTreks data generalizes well to unseen domains. Table 5: Comparison of task success rate on WebArena Model WebArena CodeLlama-7B-Instruct (Ou et al., 2024) LLaMa3-chat-8B (Ou et al., 2024) Qwen2.5-7B-Instruct LLama3-chat-70B (Ou et al., 2024) GPT-4o(Zhou et al., 2023) GPT-4(Ou et al., 2024) Synatra-CodeLlama-7B (Ou et al., 2024) AutoWebGLM (OOD SFT) (Lai et al., 2024) AutoWebGLM (In-domain RFT) (Lai et al., 2024) Qwen2.5-7B-Instruct w/ AgentTrek Qwen2.5-32B-Instruct w/ AgentTrek 0.00 3.32 3.80 7.02 13.10 14.41 6.28 8.50 18.20 10.46 16. ScreenSpot. Fine-tuning with the AgentTrek dataset significantly improved Qwen2-VLs grounding ability for both text and icon-based tasks, more than doubling its baseline performance and surpass8 ing several models on the ScreenSpot benchmark. This demonstrates the strong impact of AgentTrek in enhancing the models grounding capabilities for web-based GUI tasks. Table 6: Comparison of grounding performance on ScreenSpot Web Grounding Model Text Icon/Widget Average GPT-4 (Cheng et al., 2024) GPT-4o (Cheng et al., 2024) Qwen2-VL-7B SeeClick (Cheng et al., 2024) CogAgent (Cheng et al., 2024) GPT-4 + OmniParser (Lu et al., 2024) Qwen2-VL-7B w/ AgentTrek 9.2 12.2 35.2 55.7 70.4 81.3 81.7 8.8 7.8 25.7 32.5 28.6 51. 51.5 9.0 10.1 30.7 44.7 50.7 67.0 67.4 Mind2Web. The baseline Qwen2-VL-7B model is excluded due to its poor ability to locate target elements, critical requirement for web-based tasks. As result, only the fine-tuned models are included in the table. Fine-tuning with the AgentTrek dataset greatly enhanced Qwen2-VLs performance, particularly in the Operation F1 metric, where it surpassed both GPT-3.5 and GPT-4 in all settings. The combination of AgentTrek and Mind2Web datasets delivers the best results across all metrics and settings. While fine-tuning with Mind2Web alone yields strong performance, the model still benefits from the addition of AgentTrek data. These findings highlight the complementary strengths of the two datasets: AgentTrek provides precise, grounded data, while Mind2Web contributes valuable resources for tackling complex web-based tasks. Additional details on result sources are provided in Appendix J.2. Table 7: Performance comparison across different methods and evaluation settings. H, I, AT, M2W stand for HTML, Image, AgentTrek, Mind2Web Obs Model Method Cross-Task Cross-Website Cross-Domain Ele.Acc Op.F Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR HTML + Image GPT-3.5 GPT-4 GPT-4 GPT-4 Choice Choice Choice SoM Qwen2-VL Vision + AT + M2W Vision + AT + M2W Vision 19.4 40. 46.4 29.6 45.5 54.8 60.8 59.2 63.1 73.4 - 84.9 89.5 88.9 16.8 32. 40.2 20.3 40.9 50.9 55.7 14.9 30.2 38.0 20.1 40.8 52.9 57.6 56.5 61. 67.8 - 82.8 83.9 88.1 14.1 27.0 32.4 13.9 35.1 44.9 51.4 25.2 35. 42.4 27.0 48.6 51.8 56.0 57.9 61.9 69.3 - 84.1 86.8 87.5 24.1 29. 36.8 23.7 42.1 47.7 52.",
                "summary": "<p>–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏, –∫–∞–∫ —Å —Ç–µ–∫—Å—Ç–æ–≤–æ–π, —Ç–∞–∫ –∏ —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.</p>\n<p><strong>–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤.</strong> –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å 6000 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö AgentTrek. –í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –¥–µ—Ä–µ–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (accessibility tree), –∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ ‚Äî –¥–µ–π—Å—Ç–≤–∏—è Playwright. –ú–æ–¥–µ–ª–∏ Qwen2 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (7B, 32B –∏ 72B) –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞ –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å Qwen2-VL, –∏—Å–ø–æ–ª—å–∑—É—è 10000 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–∞ –∏–∑ —Ç–æ–≥–æ –∂–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞.</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –±—ã–ª –≤—ã–±—Ä–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ WebArena, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–∞—Ö –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞.</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–∏–≤—è–∑–∫–µ (grounding) –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.\n*   <strong>ScreenSpot</strong> ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1200 –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –º–æ–±–∏–ª—å–Ω—ã–µ, –¥–µ—Å–∫—Ç–æ–ø–Ω—ã–µ –∏ –≤–µ–±-—Å—Ä–µ–¥—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—è —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —Ç–µ–∫—Å—Ç –∏ –∏–∫–æ–Ω–∫–∏. –¢–∞–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã —Ç–æ–ª—å–∫–æ –∏–∑ –≤–µ–±-—Å—Ä–µ–¥—ã, –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –≤ –≤–µ–±-—Å—Ä–µ–¥–µ.\n*   <strong>Multimodal-Mind2Web</strong> ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ Mind2Web, —Å–æ—Å—Ç–æ—è—â–µ–µ –∏–∑ —Ç—Ä–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (cross-task, cross-website –∏ cross-domain), —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.</strong></p>\n<p><strong>WebArena.</strong> –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ WebArena, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ—Ç–∫—Ä—ã—Ç—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ GPT-4o. –°–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ WebArena, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫–æ–º –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—â–µ–Ω–Ω—ã–º–∏ —Å–∞–π—Ç–∞–º–∏, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ AgentTrek —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞—é—Ç—Å—è –Ω–∞ –Ω–µ–≤–∏–¥–∏–º—ã–µ –¥–æ–º–µ–Ω—ã.</p>\n<p><strong>ScreenSpot.</strong> –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen2-VL –∫ –ø—Ä–∏–≤—è–∑–∫–µ –∫–∞–∫ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, —Ç–∞–∫ –∏ –¥–ª—è –∏–∫–æ–Ω–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á, –±–æ–ª–µ–µ —á–µ–º —É–¥–≤–æ–∏–≤ –±–∞–∑–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–≤–∑–æ–π–¥—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ScreenSpot. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ AgentTrek –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–∏–≤—è–∑–∫–µ –¥–ª—è –≤–µ–±-–∑–∞–¥–∞—á —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.</p>\n<p><strong>Mind2Web.</strong> –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Qwen2-VL-7B –±—ã–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∞ –∏–∑ –æ—Ü–µ–Ω–∫–∏ –∏–∑-–∑–∞ –µ–µ –Ω–∏–∑–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞—Ö–æ–¥–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Qwen2-VL, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –º–µ—Ç—Ä–∏–∫–µ Operation F1, –≥–¥–µ –æ–Ω–∞ –ø—Ä–µ–≤–∑–æ—à–ª–∞ GPT-3.5 –∏ GPT-4 –≤–æ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö AgentTrek –∏ Mind2Web –¥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º. –•–æ—Ç—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö Mind2Web —Ç–∞–∫–∂–µ –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö AgentTrek –≤—Å–µ —Ä–∞–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –¥–≤—É—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö: AgentTrek –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–∞–Ω–Ω—ã–µ, –∞ Mind2Web –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–µ–±-–∑–∞–¥–∞—á.</p>"
            },
            {
                "title": "Analysis",
                "content": "With our AgentTrek pipeline, we generate large-scale trajectory data that excels in three areas. First, the dataset offers extensive diversity, covering multiple domains and task types, and benefiting from internet-sourced tutorials that enhance task execution. Our experiment showed 230% performance increase when agents followed detailed instructions. Second, the data is gathered from real-world web environments, avoiding the limitations of simulations. Starting with RedPajama, we filtered and processed 23,430 tutorials, producing 10,398 successful trajectories from 127 websites. Third, the data is comprehensive, capturing highand low-level task details, including DOM/HTML structures, AXTree snapshots, video recordings, and screenshots. This rich data improves the agents performance on long-horizon tasks, and with per-trajectory cost of just $0.551, our pipeline offers an efficient, scalable solution for data generation. 4.1 IMPORTANCE OF TUTORIALS Tutorials extracted from the internet play crucial role in guiding the replay process. First, they ensure diversity in the generated trajectories. Tutorials often have distinct task goals, and even when they target the same objective, they may offer different execution methods, enriching the trajectory 9 data. Second, tutorials significantly improve the agents execution. We tested the agent on 400 tasks, replaying them twice: once with tutorials and once using only high-level goals. The results show that step-by-step instructions greatly enhanced performance. Without tutorials, only 63 effective trajectories were generated (15.78% of the total). With tutorials, the agent produced 208 effective trajectories (52%), marking an increase of over 230%, demonstrating the importance of detailed instructions in improving reliability and effectiveness. For further analysis, please see Appendix B",
                "summary": "<p><strong>4.1 –í–∞–∂–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤</strong></p>\n<p>–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –†–∞–∑–ª–∏—á–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —á–∞—Å—Ç–æ –∏–º–µ—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–µ–ª–∏, –∞ –¥–∞–∂–µ –ø—Ä–∏ —Å—Ö–æ–∂–∏—Ö —Ü–µ–ª—è—Ö –º–æ–≥—É—Ç –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ–±–æ–≥–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö.</p>\n<p>–í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞–º–µ—Ç–Ω–æ —É–ª—É—á—à–∞—é—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–æ–º. –í —Ö–æ–¥–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∞–≥–µ–Ω—Ç—É –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å 400 –∑–∞–¥–∞—á –¥–≤–∞–∂–¥—ã: –æ–¥–∏–Ω —Ä–∞–∑, —Å–ª–µ–¥—É—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∏ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑, –æ–ø–∏—Ä–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ü–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ë–µ–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –≤—Å–µ–≥–æ 63 —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (15.78% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞). –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–æ–∑—Ä–æ—Å–ª–æ –¥–æ 208 (52%), —á—Ç–æ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 230% –±–æ–ª—å—à–µ. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞. (–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ B).</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.</em></p>"
            },
            {
                "title": "Data Composition",
                "content": "Figure 7: The distribution of website with domains involved in our dataset To summarize the data flow through our pipeline: First, we filter tutorial data from the RedPajama web snapshot. Next, the filtered data is paraphrased for clarity and classification. We then gather up-to-date data from mainstream websites for replay and, finally, collect effective trajectory data from the replays. After filtering RedPajamas vast dataset, we retained over 18.8 million entries. By applying criteria such as recency and popularity, 23,430 tutorials were prepared for replay. With success rate of 44.4%, we generated 10,398 trajectories, covering 127 websites across 11 distinct categories. 4.3 COMPARISON WITH EXISTING WORK AND RESEARCH CHALLENGES AgentTrek generates comprehensive, large-scale trajectory data, excelling in several key areas as shown in Table 1 (Niu et al., 2024; L`u et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024). First, with nearly 5k verified trajectories and an average of 12.1 steps per trajectory, our dataset provides strong foundation for training and evaluating agents on longhorizon web tasks. Second, it is the most complete dataset to date, incorporating DOM/HTML structures, AXTree data, intermediate reasoning steps, full video recordings, and corresponding screenshots for each action. Moreover, despite full automation without human intervention, our dataset maintains diversity across 120 websites and 12 distinct task categories. By leveraging modern large language models (LLMs), we can extract both high-level task objectives and detailed step-by-step instructions, offering flexibility for future use. Finally, our pipeline significantly reduces the cost and scalability challenges of human-annotated data collection. With success rate factored in, the cost per trajectory is just $0.551, making our approach both efficient and scalable for large-scale data generation. Cost detail see C",
                "summary": "<p><strong>–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏</strong></p>\n<p>–í —Ä–∞–º–∫–∞—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ –≤–µ–±-—Å–Ω–∏–º–∫–∞ RedPajama. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—é—Ç—Å—è –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –î–∞–ª–µ–µ, —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–æ–≤ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è (replay), –∏, –Ω–∞–∫–æ–Ω–µ—Ü, –∏–∑ —ç—Ç–∏—Ö –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑ –æ–≥—Ä–æ–º–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö RedPajama –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –±–æ–ª–µ–µ 18.8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∑–∞–ø–∏—Å–µ–π. –ü—Ä–∏–º–µ–Ω–∏–≤ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏, –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –±—ã–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ 23 430 –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –° –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ 44.4% –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 10 398 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 127 –≤–µ–±-—Å–∞–π—Ç–æ–≤ –≤ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω–∞–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 7 (–Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è).</p>\n<p>–í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏ (Niu et al., 2024; Lu et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024), AgentTrek –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–µ, –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∏—Ö –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (—Å–º. –¢–∞–±–ª–∏—Ü—É 1, –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è). –í–æ-–ø–µ—Ä–≤—ã—Ö, —Å –ø–æ—á—Ç–∏ 5 —Ç—ã—Å—è—á–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –≤ —Å—Ä–µ–¥–Ω–µ–º 12.1 —à–∞–≥–∞ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—á–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏. –í–æ-–≤—Ç–æ—Ä—ã—Ö, —ç—Ç–æ —Å–∞–º—ã–π –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å, –≤–∫–ª—é—á–∞—é—â–∏–π DOM/HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –¥–∞–Ω–Ω—ã–µ AXTree, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —ç—Ç–∞–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–ª–Ω—ã–µ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø–æ–ª–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é –±–µ–∑ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ 120 –≤–µ–±-—Å–∞–π—Ç–∞–º –∏ 12 —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∑–∞–¥–∞—á. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–∞–∫ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ü–µ–ª–∏ –∑–∞–¥–∞—á, —Ç–∞–∫ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ù–∞–∫–æ–Ω–µ—Ü, –∫–æ–Ω–≤–µ–π–µ—Ä –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏. –° —É—á–µ—Ç–æ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏, —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–¥–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ–≥–æ 0.551 –¥–æ–ª–ª–∞—Ä–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö. –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ C (–Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è).</p>"
            },
            {
                "title": "Related Work",
                "content": "LLM-based Agents. LLM-based agents are autonomous systems that leverage large language models (Brown et al., 2020) to interact with real-world websites and os environments. These agents can 10 understand natural language instructions and perform wide range of complex tasks across various domains, such as e-commerce, online assistance, and knowledge navigation (Nakano et al., 2021; Cheng et al., 2024). Recent efforts in this space include models like SeeAct (Zheng et al., 2024) and WebVoyager (He et al., 2024), which aim to generalize agent behavior to real-world websites. While LLM-based agents have shown promise, challenges remain in the need for agent specified data. Our work extends this line of research by introducing cost-effective pipeline to generate comprehensive agent trajectory data, advancing the state-of-the-art in data synthesis for agent-based applications. Agent Data. As agents gain increasing popularity, the demand for efficient and scalable data is becoming both larger and more urgent. However, most existing data primarily serve as supplements to various benchmarks (Zhou et al., 2023; Li et al., 2023; Deng et al., 2024), with few datasets specifically designed for agent trajectory analysis. Furthermore, these datasets are often limited by the need for human annotation, which hampers scalability. In our work, our pipeline managed to automatically generate comprehensive agent trajectory data in cost-efficient manner, paving the way for new direction in data synthesis within the field of agents. Automatic Evaluation for Digital Agents. Recently, there has been growing interest in automating the evaluation of digital agents using Vision-Language Models (VLMs) and Large Language Models (LLMs). These methods leverage models to assess agent performance in real-world tasks. Research in this area spans several dimensions: some works focus on trajectory-level success (Pan et al., 2024), while others evaluate stepwise success based on adherence to instructions (Wornow et al., 2024). Additionally, evaluations are conducted across various task environments, such as web-based platforms and mobile operating systems like Android and iOS (Pan et al., 2024). In our work, we prompt VLM, GPT-4o, as an autonomous evaluator, using agents interacton process as inputs to assess whether the agent has successfully completed tasks at the trajectory level.",
                "summary": "<p><strong>–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)</strong></p>\n<p>–ê–≥–µ–Ω—Ç—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç LLM –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–±-—Å–∞–π—Ç–∞–º–∏ –∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏. –≠—Ç–∏ –∞–≥–µ–Ω—Ç—ã —Å–ø–æ—Å–æ–±–Ω—ã –ø–æ–Ω–∏–º–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –æ–Ω–ª–∞–π–Ω-–ø–æ–º–æ—â—å –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –∑–Ω–∞–Ω–∏—è–º. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞—é—Ç SeeAct –∏ WebVoyager, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–µ–º—è—Ç—Å—è –æ–±–æ–±—â–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–∞—Ö.</p>\n<p>–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å–ª—É–∂–∞—Ç –ª–∏—à—å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º, –∏ –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–∏–µ –∏–∑ –Ω–∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–∏–Ω—Ç–µ–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.</p>\n<p><strong>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤</strong></p>\n<p>–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Ä–∞—Å—Ç–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π \"–∑—Ä–µ–Ω–∏–µ-—è–∑—ã–∫\" (VLM) –∏ LLM. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —É—Å–ø–µ—Ö–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —É—Å–ø–µ—Ö –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏ –º–æ–±–∏–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Android –∏ iOS. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VLM (GPT-4o), –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —É—Å–ø–µ—à–Ω–æ –ª–∏ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–∏–ª –∑–∞–¥–∞—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏.</p>"
            }
        ]
    },
    {
        "id": "2412.09025",
        "title": "Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages",
        "url": "https://huggingface.co/papers/2412.09025",
        "abstract": "Neural Machine Translation (NMT) models are typically trained on datasets\nwith limited exposure to Scientific, Technical and Educational domains.\nTranslation models thus, in general, struggle with tasks that involve\nscientific understanding or technical jargon. Their performance is found to be\neven worse for low-resource Indian languages. Finding a translation dataset\nthat tends to these domains in particular, poses a difficult challenge. In this\npaper, we address this by creating a multilingual parallel corpus containing\nmore than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality\ntranslation pairs across 8 Indian languages. We achieve this by bitext mining\nhuman-translated transcriptions of NPTEL video lectures. We also finetune and\nevaluate NMT models using this corpus and surpass all other publicly available\nmodels at in-domain tasks. We also demonstrate the potential for generalizing\nto out-of-domain translation tasks by improving the baseline by over 2 BLEU on\naverage for these Indian languages on the Flores+ benchmark. We are pleased to\nrelease our model and dataset via this link: https://huggingface.co/SPRINGLab.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "757a034f902441cd",
        "authors": [
            "Advait Joglekar",
            "Srinivasan Umesh"
        ],
        "affiliations": [
            "SPRING Lab, Indian Institute of Technology Madras, India"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.09025.jpg",
        "data": {
            "categories": [
                "#benchmark",
                "#multilingual",
                "#low_resource",
                "#machine_translation",
                "#dataset",
                "#open_source",
                "#training"
            ],
            "emoji": "üåè",
            "ru": {
                "title": "–ù–æ–≤—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤",
                "desc": "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 2,8 –º–∏–ª–ª–∏–æ–Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –º–µ–∂–¥—É –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ 8 –∏–Ω–¥–∏–π—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏. –ö–æ—Ä–ø—É—Å —Å–æ–∑–¥–∞–Ω –ø—É—Ç–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–≤—É—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π –≤–∏–¥–µ–æ–ª–µ–∫—Ü–∏–π NPTEL. –ê–≤—Ç–æ—Ä—ã –¥–æ–æ–±—É—á–∏–ª–∏ –∏ –æ—Ü–µ–Ω–∏–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —ç—Ç–æ–º –∫–æ—Ä–ø—É—Å–µ, –ø—Ä–µ–≤–∑–æ–π–¥—è –≤—Å–µ –¥—Ä—É–≥–∏–µ –ø—É–±–ª–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ 2 BLEU –≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ Flores+ –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤."
            },
            "en": {
                "title": "Empowering Indian Languages with High-Quality Scientific Translation",
                "desc": "This paper addresses the limitations of Neural Machine Translation (NMT) models in translating scientific and technical content, especially for low-resource Indian languages. The authors create a large multilingual parallel corpus with over 2.8 million high-quality translation pairs from English to eight Indic languages, sourced from NPTEL video lectures. They fine-tune NMT models on this dataset and achieve superior performance compared to existing models on in-domain translation tasks. Additionally, their approach shows promise for improving out-of-domain translation tasks, as evidenced by significant BLEU score improvements on the Flores+ benchmark."
            },
            "zh": {
                "title": "ÊèêÂçáÂç∞Â∫¶ËØ≠Ë®ÄÁøªËØëÁöÑÁßëÂ≠¶ÊäÄÊúØËÉΩÂäõ",
                "desc": "Êú¨ËÆ∫ÊñáÈíàÂØπÁ•ûÁªèÊú∫Âô®ÁøªËØëÔºàNMTÔºâÊ®°ÂûãÂú®ÁßëÂ≠¶„ÄÅÊäÄÊúØÂíåÊïôËÇ≤È¢ÜÂüüÁöÑÁøªËØëÂõ∞ÈöæËøõË°å‰∫ÜÁ†îÁ©∂„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Â§öËØ≠Ë®ÄÂπ≥Ë°åËØ≠ÊñôÂ∫ìÔºåÂåÖÂê´Ë∂ÖËøá280‰∏áÊù°È´òË¥®ÈáèÁöÑËã±Âç∞ÂíåÂç∞Âç∞ÁøªËØëÂØπÔºåÁâπÂà´ÂÖ≥Ê≥®‰ΩéËµÑÊ∫êÁöÑÂç∞Â∫¶ËØ≠Ë®Ä„ÄÇÈÄöËøáÊåñÊéòNPTELËßÜÈ¢ëËÆ≤Â∫ßÁöÑ‰∫∫Á±ªÁøªËØëÊñáÊú¨ÔºåÊàë‰ª¨ÊàêÂäüÂú∞ËÆ≠ÁªÉÂíåËØÑ‰º∞‰∫ÜNMTÊ®°ÂûãÔºåÂπ∂Âú®È¢ÜÂüüÂÜÖ‰ªªÂä°‰∏≠Ë∂ÖË∂ä‰∫ÜÊâÄÊúâÂÖ∂‰ªñÂÖ¨ÂºÄÂèØÁî®ÁöÑÊ®°Âûã„ÄÇÊàë‰ª¨ÁöÑÁ†îÁ©∂Ë°®ÊòéÔºåËØ•Ê®°ÂûãÂú®Ë∑®È¢ÜÂüüÁøªËØë‰ªªÂä°‰∏≠‰πüÂÖ∑ÊúâËâØÂ•ΩÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂπ≥ÂùáÊèêÈ´ò‰∫Ü2 BLEUÂàÜÊï∞„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains. Translation models thus, in general, struggle with tasks that involve scientific understanding or technical jargon. Their performance is found to be even worse for low-resource Indian languages. Finding a translation dataset that tends to these domains in particular, poses a difficult challenge. In this paper, we address this by creating a multilingual parallel corpus containing more than 2.8 million rows of English-to-Indic and Indic-to-Indic high-quality translation pairs across 8 Indian languages. We achieve this by bitext mining human-translated transcriptions of NPTEL video lectures. We also finetune and evaluate NMT models using this corpus and surpass all other publicly available models at in-domain tasks. We also demonstrate the potential for generalizing to out-of-domain translation tasks by improving the baseline by over 2 BLEU on average for these Indian languages on the Flores+ benchmark. We are pleased to release our model and dataset via this link: https://huggingface.co/SPRINGLab.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (NMT) –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–∞—É—á–Ω–æ-—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±–ª–∞—Å—Ç—è—Ö. –û–±—ã—á–Ω–æ NMT-–º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ–±—â–∏—Ö –∫–æ—Ä–ø—É—Å–∞—Ö —Ç–µ–∫—Å—Ç–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏ –∏ —Å–ª–æ–∂–Ω—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π. –û—Å–æ–±–µ–Ω–Ω–æ –æ—Å—Ç—Ä–æ —ç—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å—Ç–æ–∏—Ç –¥–ª—è –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.</p>\n<p>–ê–≤—Ç–æ—Ä—ã —Ä–∞–±–æ—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã, —Å–æ–∑–¥–∞–≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 2.8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å—Ç—Ä–æ–∫ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏–µ —è–∑—ã–∫–∏ –∏ —Å –∏–Ω–¥–∏–π—Å–∫–∏—Ö –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏–µ. –ö–æ—Ä–ø—É—Å –≤–∫–ª—é—á–∞–µ—Ç 8 –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤. –î–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –ø—É—Ç–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–≤—É—è–∑—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (bitext mining) –∏–∑ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–æ–∫ –≤–∏–¥–µ–æ–ª–µ–∫—Ü–∏–π NPTEL, –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏.</p>\n<p>–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ—Ä–ø—É—Å–∞ –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫—É NMT-–º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏—Ö –º–æ–¥–µ–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç –≤—Å–µ –¥—Ä—É–≥–∏–µ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–æ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏ –¥–ª—è –∑–∞–¥–∞—á –≤–Ω–µ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Flores+ –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å—Ä–µ–¥–Ω–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å BLEU —É–≤–µ–ª–∏—á–∏–ª—Å—è –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2 –ø—É–Ω–∫—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.</p>\n<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –æ—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –∫–æ—Ä–ø—É—Å—É –¥–∞–Ω–Ω—ã—Ö –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Å—ã–ª–∫–µ.</p>"
            },
            {
                "title": "Introduction",
                "content": "NPTEL (National Programme on Technology Enhanced Learning) has long been valuable resource for free on-demand higher-educational content across diverse range of specialized disciplines. Over the past two decades since its inception, NPTEL has curated an extensive library of over 56,000 hours of video lectures, all made publicly available along with their audio transcriptions in an easily accessible manner. In response to the growing number of Indian students, NPTEL has taken steps to support Indian language transcriptions for more than 12,000 hours of video content. These captions are primarily translations of the 1https://nptel.ac.in 1 Figure 1: Translation Pair Counts (in thousands) original English transcriptions, carefully crafted by subject-matter experts. This multi-year endeavor has led to the creation of high-quality parallel textual resource spanning multiple Indian languages, covering various fields in the Scientific, Engineering, and Humanities domains. Our research leverages this rich data resource to develop competitive Machine Translation (MT) models specifically tailored for Indian languages. Additionally, we investigate how models fine-tuned on this data can assist human translators and help accelerate the mission of providing accurate Indic subtitles for all NPTEL video lectures. This effort aims to benefit large audience of Indian students struggling with the lack of university-level educational content in their native tongues.",
                "summary": "<p><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NPTEL –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤</strong></p>\n<p>NPTEL (–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –æ–±—É—á–µ–Ω–∏—é) —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –ó–∞ –¥–≤–∞ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏—è —Å–≤–æ–µ–≥–æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è NPTEL —Å–æ–±—Ä–∞–ª –æ–±—à–∏—Ä–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É, –≤–∫–ª—é—á–∞—é—â—É—é –±–æ–ª–µ–µ 56 000 —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ–ª–µ–∫—Ü–∏–π —Å –∞—É–¥–∏–æ—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞–º–∏, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –∂–µ–ª–∞—é—â–∏—Ö. –í –æ—Ç–≤–µ—Ç –Ω–∞ —Ä–∞—Å—Ç—É—â–µ–µ —á–∏—Å–ª–æ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, NPTEL –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É –Ω–∞–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –¥–ª—è –±–æ–ª–µ–µ —á–µ–º 12 000 —á–∞—Å–æ–≤ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–µ—Ä–µ–≤–æ–¥—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–æ–∫, –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö. </p>\n<p>–≠—Ç–∞ –º–Ω–æ–≥–æ–ª–µ—Ç–Ω—è—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–∏–≤–µ–ª–∞ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ä–µ—Å—É—Ä—Å–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏, —Ç–µ—Ö–Ω–∏–∫–∏ –∏ –≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã—Ö –Ω–∞—É–∫. –í —Ä–∞–º–∫–∞—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —ç—Ç–æ—Ç —Ä–µ—Å—É—Ä—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (–ú–ü), —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏–∑—É—á–∞–µ—Ç—Å—è, –∫–∞–∫ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞–º-–ª—é–¥—è–º –∏ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ—á–Ω—ã—Ö —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö –¥–ª—è –≤—Å–µ—Ö –≤–∏–¥–µ–æ–ª–µ–∫—Ü–∏–π NPTEL. –¶–µ–ª—å —ç—Ç–æ–π —Ä–∞–±–æ—Ç—ã ‚Äì –ø–æ–º–æ—á—å –±–æ–ª—å—à–æ–º—É —á–∏—Å–ª—É –∏–Ω–¥–∏–π—Å–∫–∏—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, –∏—Å–ø—ã—Ç—ã–≤–∞—é—â–∏—Ö —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–∞ –∏—Ö —Ä–æ–¥–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –í —Ç–µ–∫—Å—Ç–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –Ω–∞ —Ä–æ–¥–Ω–æ–º —è–∑—ã–∫–µ –∏ —Ç–æ, –∫–∞–∫ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –≤ —Ä–µ—à–µ–Ω–∏–∏ —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã.</em></p>"
            },
            {
                "title": "Where does present-day mt fail?",
                "content": "Lets quickly look at how Machine Translation models in use widely today perform on Technicaldomain tasks and instances in which they fail. Table 1: Example translations from English to Hindi in the Scientific/Technical domain. Sentences marked as are in-domain, while are out-of-domain. The words in blue are terms with multiple meanings, that tend to get translated incorrectly. The words in green represent the correct, expected translation by the model for the blue word in the given context. The words in red represent incorrect translations. Consider the text \"I want to learn the rust language.\" Here we are talking about the programming language Rust and not the chemical phenomena. From Table 1 we can see that both Google Translate 2 and IndicTrans2 (IT2) (Gala et al., 2023) get their Hindi translations wrong. Not only that, if we backtranslate their results we get the sentence: \"I want to learn the language of war,\" which is very far from what we originally meant. This happens in this case because the Hind word for \"rust\" has two meanings; the phenomena of rust and also war. Thus, in such situations it is very important for the translation model to understand the context well since the meaning of sentence can completely change with the wrong choice of word. So, with this we can see that current models are prone to making mistakes for tasks in these domains. Our paper hopes to alleviate such shortcomings.",
                "summary": "<p>–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Å–µ–≥–æ–¥–Ω—è, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–µ—Ä–µ–≤–æ–¥–æ–º —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏ –Ω–∞—É—á–Ω–æ–π –æ–±–ª–∞—Å—Ç–µ–π. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –æ—à–∏–±–∞—Ç—å—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Å–ª–æ–≤, –∏–º–µ—é—â–∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç.</p>\n<p>–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ö–∏–Ω–¥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è \"I want to learn the rust language\" (–Ø —Ö–æ—á—É –≤—ã—É—á–∏—Ç—å —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è Rust). –°–ª–æ–≤–æ \"rust\" –∏–º–µ–µ—Ç –¥–≤–∞ –∑–Ω–∞—á–µ–Ω–∏—è: —Ä–∂–∞–≤—á–∏–Ω–∞ (—Ö–∏–º–∏—á–µ—Å–∫–æ–µ —è–≤–ª–µ–Ω–∏–µ) –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ö–∞–∫ –≤–∏–¥–Ω–æ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã 1, –º–æ–¥–µ–ª–∏ Google Translate 2 –∏ IndicTrans2 (IT2) –æ—à–∏–±–æ—á–Ω–æ –ø–µ—Ä–µ–≤–æ–¥—è—Ç \"rust\" –∫–∞–∫ \"–≤–æ–π–Ω–∞\" (–æ–¥–Ω–æ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π —Å–ª–æ–≤–∞ \"—Ä–∂–∞–≤—á–∏–Ω–∞\" –Ω–∞ —Ö–∏–Ω–¥–∏), —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∏—Å–∫–∞–∂–µ–Ω–∏—é —Å–º—ã—Å–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –ï—Å–ª–∏ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –ø–æ–ª—É—á–∏—Ç—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ \"I want to learn the language of war\" (–Ø —Ö–æ—á—É –≤—ã—É—á–∏—Ç—å —è–∑—ã–∫ –≤–æ–π–Ω—ã), –∫–æ—Ç–æ—Ä–æ–µ –æ—á–µ–Ω—å –¥–∞–ª–µ–∫–æ –æ—Ç –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ.</p>\n<p>–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–æ–≥—É—Ç –¥–æ–ø—É—Å–∫–∞—Ç—å –æ—à–∏–±–∫–∏ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö –∏–∑-–∑–∞ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤. –î–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤—ã–±–∏—Ä–∞–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª–æ–≤–∞. –ù–∞—à–∞ —Ä–∞–±–æ—Ç–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>"
            },
            {
                "title": "Related work",
                "content": "In related work like Samanantar (Ramesh et al., 2022) and IndicTrans2 (Gala et al., 2023), NPTEL has been identified as useful resource for Machine Translation (MT). These two studies in particular attempt at mining for parallel sentence pairs by utilizing various sources on the internet, including this one. Due to the lack of precise information given in these papers, we are unable to know the exact quantity of sentence-pairs mined from NPTEL with certainty. Regardless of this, we have found some key issues with their data. significant quantity of extracted sentence-pairs was found to be composed of unfiltered artifacts like timestamps. Several in2https://translate.google.com/ stances of code-mixed sentences have also been miscategorized as English, leading to poor alignment quality and under-exploitation of raw data. Their alignments too were limited to 1-1 sentence matchings, leaving us room for better alignments with n-m translation pairs. The data they mined was solely in the English-Indic direction as well, despite there being significant potential for mining Indic-Indic translation pairs from this source. In this work, we attempt to alleviate these shortcomings.",
                "summary": "<p>–í —Ä–∞–±–æ—Ç–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Samanantar –∏ IndicTrans2, —Ä–µ—Å—É—Ä—Å NPTEL –±—ã–ª –ø—Ä–∏–∑–Ω–∞–Ω –ø–æ–ª–µ–∑–Ω—ã–º –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –í —ç—Ç–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö –∞–≤—Ç–æ—Ä—ã –ø—ã—Ç–∞–ª–∏—Å—å –Ω–∞–π—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –≤–∫–ª—é—á–∞—è NPTEL. –û–¥–Ω–∞–∫–æ, –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —ç—Ç–∏—Ö —Å—Ç–∞—Ç—å—è—Ö, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é —Å–∫–∞–∑–∞—Ç—å, –∫–∞–∫–æ–µ –∏–º–µ–Ω–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ –∏–∑ NPTEL.</p>\n<p>–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –±—ã–ª–∏ –≤—ã—è–≤–ª–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏, —Å–æ–±—Ä–∞–Ω–Ω—ã–º–∏ —ç—Ç–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∞–ª–∞ \"–º—É—Å–æ—Ä\", —Ç–∞–∫–æ–π –∫–∞–∫ –æ—Ç–º–µ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –≥–¥–µ —Å–º–µ—à–∏–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ —Ö–∏–Ω–¥–∏), –æ—à–∏–±–æ—á–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∏—Å—å –∫–∞–∫ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –ø–ª–æ—Ö–æ–º—É –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—é –∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –±—ã–ª–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º \"–æ–¥–∏–Ω –∫ –æ–¥–Ω–æ–º—É\", –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Å –ø–∞—Ä–∞–º–∏ \"–º–Ω–æ–≥–æ –∫ –º–Ω–æ–≥–æ–º—É\". –¢–∞–∫–∂–µ, —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —Ç–æ–ª—å–∫–æ –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏–µ —è–∑—ã–∫–∏, —Ö–æ—Ç—è –µ—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è —Å–±–æ—Ä–∞ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É —Å–∞–º–∏–º–∏ –∏–Ω–¥–∏–π—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏.</p>\n<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç–∞ –ø–æ–ø—ã—Ç–∫–∞ —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏.</p>"
            },
            {
                "title": "Bitext mining",
                "content": "With this parallel corpora in place, we begin the most crucial part: Bitext mining. Our objective is to find as many sentence-pairs as we can from the source data while still maintaining high confidence in their translation accuracy. Luckily, SentenceAlignment is well studied problem dating as far back as 1991 (Brown et al.). Recent work like Vecalign (Thompson and Koehn, 2019) has focused on using multi-lingual embedding models to find pairs based on vector similarity of sentence embeddings. These have been shown to achieve state-of-the-art performance, significantly surpassing previous approaches. In our work, we use SentAlign (Steingrimsson et al., 2023) which employs LABSE (Feng et al., 2022) along with optimized alignment algorithms to mine parallel documents with high accuracy and efficiency. With this we are also able to find 1-n and n-1 sentence matches.",
                "summary": "<p>–ò—Ç–∞–∫, –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –º—ã –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–∞–º–æ–π –≤–∞–∂–Ω–æ–π —á–∞—Å—Ç–∏ ‚Äî –ø–æ–∏—Å–∫—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (bitext mining). –ù–∞—à–∞ —Ü–µ–ª—å ‚Äî –Ω–∞–π—Ç–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Ö –ø–µ—Ä–µ–≤–æ–¥–∞. –ö —Å—á–∞—Å—Ç—å—é, –∑–∞–¥–∞—á–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (Sentence Alignment) —Ö–æ—Ä–æ—à–æ –∏–∑—É—á–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞—è —Å 1991 –≥–æ–¥–∞ (—Ä–∞–±–æ—Ç–∞ –ë—Ä–∞—É–Ω–∞ –∏ –¥—Ä.).</p>\n<p>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Vecalign (–¢–æ–º–ø—Å–æ–Ω –∏ –ö–æ–Ω, 2019), —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. –ë—ã–ª–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç –Ω–∞–∏–ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã.</p>\n<p>–í –Ω–∞—à–µ–π —Ä–∞–±–æ—Ç–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º SentAlign (–°—Ç–µ–π–Ω–≥—Ä–∏–º—Å—Å–æ–Ω –∏ –¥—Ä., 2023), –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç LABSE (–§–µ–Ω–≥ –∏ –¥—Ä., 2022) –≤–º–µ—Å—Ç–µ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é. –≠—Ç–æ —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞–º –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–∏–ø–∞ \"–æ–¥–∏–Ω-–∫–æ-–º–Ω–æ–≥–∏–º\" (1-n) –∏ \"–º–Ω–æ–≥–∏–µ-–∫-–æ–¥–Ω–æ–º—É\" (n-1).</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –ó–¥–µ—Å—å –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –Ω–∞–±–æ—Ä–∞ —Ç–µ–∫—Å—Ç–æ–≤, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —à–∞–≥–æ–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞. –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.</em></p>"
            },
            {
                "title": "Data collation",
                "content": "Each lecture now has its own documents with all its sentences aligned into bilingual sentence-pairs. We collect these pairs and combine them, along with their lecture metadata, into massive translation dataset. After post-processing with deduplication, we arrive at corpus of roughly 2.8 million sentence-pairs. 4.5 Data Analysis To understand the quality and quantity of this data, we must first thoroughly analyse it. Our dataset has 8 English-Indic and 28 Indic-Indic language pairs. This means that there exists at least one common set of lectures among each language-pair, providing us with inter-Indic alignments for all languages covered in this dataset. We find 48.6% of these to be English-to-Indic language pairs. This is the direct result of English lectures having been translated into multiple languages, albeit with arbitrary combinations, giving us robust Indic-to-Indic data subset. For assessing the alignment quality of our translation pairs, we look at the average LABSE similarity scores as the primary measure. The plot of this metric (Figure 2) demonstrates strong consistency in scores across all languages despite differences in the quantity of the mined sentence-pairs. These data points are also seen to be tending towards 0.8 and are never below 0.75, validating our confidence in the quality of the source data and the accuracy of our alignments.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä–ø—É—Å–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.</p>\n<p><strong>–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö:</strong></p>\n<ul>\n<li>–õ–µ–∫—Ü–∏–∏ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã, –∏ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –Ω–∏—Ö –±—ã–ª–æ –≤—ã—Ä–æ–≤–Ω–µ–Ω–æ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º –Ω–∞ –¥—Ä—É–≥–æ–º —è–∑—ã–∫–µ, –æ–±—Ä–∞–∑—É—è –¥–≤—É—è–∑—ã—á–Ω—ã–µ –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.</li>\n<li>–≠—Ç–∏ –ø–∞—Ä—ã, –≤–º–µ—Å—Ç–µ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –ª–µ–∫—Ü–∏–π, –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –≤ –º–∞—Å—Å–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞.</li>\n<li>–ü–æ—Å–ª–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—é—â–µ–π —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –±—ã–ª –ø–æ–ª—É—á–µ–Ω –∫–æ—Ä–ø—É—Å –∏–∑ –ø—Ä–∏–º–µ—Ä–Ω–æ 2.8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.</li>\n</ul>\n<p><strong>–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:</strong></p>\n<ul>\n<li>–ö–æ—Ä–ø—É—Å —Å–æ–¥–µ—Ä–∂–∏—Ç 8 —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ä –∞–Ω–≥–ª–∏–π—Å–∫–∏–π-–∏–Ω–¥–∏–π—Å–∫–∏–π –∏ 28 —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ä –∏–Ω–¥–∏–π—Å–∫–∏–π-–∏–Ω–¥–∏–π—Å–∫–∏–π.</li>\n<li>–ù–∞–ª–∏—á–∏–µ –æ–±—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –ª–µ–∫—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–π —è–∑—ã–∫–æ–≤–æ–π –ø–∞—Ä—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –º–µ–∂—ä—è–∑—ã–∫–æ–≤–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.</li>\n<li>48.6% –ø–∞—Ä —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç –ø–∞—Ä—ã –∞–Ω–≥–ª–∏–π—Å–∫–∏–π-–∏–Ω–¥–∏–π—Å–∫–∏–π, —á—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–æ–º –ª–µ–∫—Ü–∏–π —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —è–∑—ã–∫–æ–≤. –≠—Ç–æ —Ç–∞–∫–∂–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –∏–Ω–¥–∏–π—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏.</li>\n<li>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–∞—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ LABSE (Language-Agnostic BERT Sentence Embedding).</li>\n<li>–ì—Ä–∞—Ñ–∏–∫ —ç—Ç–æ–π –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.</li>\n<li>–ó–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç—Ä–µ–º—è—Ç—Å—è –∫ 0.8 –∏ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –æ–ø—É—Å–∫–∞—é—Ç—Å—è –Ω–∏–∂–µ 0.75, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è.</li>\n</ul>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LABSE –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è —Ö–æ—Ä–æ—à–∏–º –ø–æ–¥—Ö–æ–¥–æ–º, —Ç–∞–∫ –∫–∞–∫ LABSE —è–≤–ª—è–µ—Ç—Å—è —è–∑—ã–∫–æ–≤–æ-–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.</li>\n<li>–í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è LABSE (–±–ª–∏–∑–∫–∏–µ –∫ 0.8) —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç –æ —Ö–æ—Ä–æ—à–µ–º –∫–∞—á–µ—Å—Ç–≤–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.</li>\n<li>–ù–∞–ª–∏—á–∏–µ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä –∏–Ω–¥–∏–π—Å–∫–∏–π-–∏–Ω–¥–∏–π—Å–∫–∏–π —è–≤–ª—è–µ—Ç—Å—è —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º, —Ç–∞–∫ –∫–∞–∫ —Ç–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–æ —è–≤–ª—è—é—Ç—Å—è –¥–µ—Ñ–∏—Ü–∏—Ç–Ω—ã–º–∏.</li>\n</ul>"
            },
            {
                "title": "Baseline model selection",
                "content": "When it comes to choosing strong multi-lingual model that is, or at least close to, the state-of3 Our testset Flores+ Models en-in Models en-in NLLB 30.73 / 57.62 LoRA FT 48.98 / 71.99 39.66 / 66.49 IT2 NLLB 19.73 / 54.27 LoRA FT 22.04 / 57.33 24.08 / 59.45 IT2 Table 2: Results are in the form <bleu>/<chrf++>. These scores represent the average of all 8 languages covered by the dataset. All models were evaluated without using beam-search or sampling. the-art, we find that our options are limited. IndicTrans2 could be good choice, except that it provides different models for English-Indic, IndicEnglish and Indic-Indic directions. We eliminate this option since we hope to leverage transfer learning by training just one model in all the 36 language-pair combinations that our dataset supports. That leaves us with only two possible candidates: NLLB-200 (Team et al., 2022) and MADLAD-400 (Kudugunta et al., 2023). These are both massively multilingual Transformer models that promise to be the right baseline for our study. We choose NLLB on the basis of its superior evaluation scores for Indian languages on the Flores-200 (Goyal et al., 2022) benchmark, according to the results published in the MADLAD paper.",
                "summary": "<p>–î–ª—è –≤—ã–±–æ—Ä–∞ —Å–∏–ª—å–Ω–æ–π –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–π –º–æ–¥–µ–ª–∏, –±–ª–∏–∑–∫–æ–π –∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —É—Ä–æ–≤–Ω—é, –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–µ —Ç–∞–∫ –º–Ω–æ–≥–æ. IndicTrans2 –º–æ–≥ –±—ã –ø–æ–¥–æ–π—Ç–∏, –Ω–æ –æ–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –ø–µ—Ä–µ–≤–æ–¥–∞: —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –∏–Ω–¥–∏–π—Å–∫–∏–µ —è–∑—ã–∫–∏, —Å –∏–Ω–¥–∏–π—Å–∫–∏—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –º–µ–∂–¥—É –∏–Ω–¥–∏–π—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏. –≠—Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ–±—É—á–∞—è –≤—Å–µ–≥–æ –æ–¥–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö 36 —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ä, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–û—Å—Ç–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: NLLB-200 –∏ MADLAD-400. –û–±–µ –º–æ–¥–µ–ª–∏ —è–≤–ª—è—é—Ç—Å—è –∫—Ä—É–ø–Ω—ã–º–∏ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–º–∏ Transformer-–º–æ–¥–µ–ª—è–º–∏ –∏ –º–æ–≥—É—Ç —Å—Ç–∞—Ç—å —Ö–æ—Ä–æ—à–µ–π –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–æ–π –¥–ª—è –Ω–∞—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –ú—ã –≤—ã–±–∏—Ä–∞–µ–º NLLB, —Ç–∞–∫ –∫–∞–∫, —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–º –≤ —Å—Ç–∞—Ç—å–µ –ø—Ä–æ MADLAD, –æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ Flores-200.</p>"
            },
            {
                "title": "Training",
                "content": "NLLB-200 models are available in wide variety of sizes ranging from 600M parameter distilled model to massive 54B parameter Mixtureof-Experts model. For our experiments, we decide to choose the 3.3B parameter version as sweet spot between performance quality and compute requirements. Still, even with this relatively smaller NLLB 3.3B model, running Full Fine-Tuning (FFT) setup can turn out to be very compute intensive endeavour. The amount of time required to effectively train our model will also be significant. In our case we wish to execute number of experiments with different approaches in hopes to achieve the best results. FFT thus would not be feasible approach. Instead, we decide to utilize Parameter-Efficient Fine Tuning (PEFT) method known as Low-Rank Adaptation (LoRA) (Hu et al., 2022) to train our model. We primarily trained three models using three different approaches. All of them were done using LoRA with NLLB 3.3B. These approaches included: 1) training model purely on our dataset in one direction, 2) training using Curriculum Learning (CL) (Bengio et al., 2009) with cleaned subset of the BPCC corpus (Gala et al., 2023) with our 8 Indian languages, comprising of 4 million rows, before introducing our dataset, 3) training on massive 12 million samples which included the cleaned BPCC corpus and our dataset in both directions. All our models were trained on node of 8 NVIDIA A100 40GB GPUs. Evaluation results for all the three models were found to be similar, with our 3rd approach performing slightly better. The hyperparameters and detailed results for all three are available in Appendix B.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–æ–¥–µ–ª–∏ NLLB-200 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –æ—Ç 600 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ 54 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤. –î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å —Å 3.3 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –º–µ–∂–¥—É –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. </p>\n<p>–ü–æ–ª–Ω–∞—è –¥–æ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (Full Fine-Tuning, FFT) —Ç–∞–∫–æ–π –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–∏—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –∏ –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –Ω–µ–ø—Ä–∞–∫—Ç–∏—á–Ω–æ–π –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ü–æ—ç—Ç–æ–º—É –±—ã–ª–æ —Ä–µ—à–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ Parameter-Efficient Fine Tuning (PEFT), –∞ –∏–º–µ–Ω–Ω–æ Low-Rank Adaptation (LoRA). </p>\n<p>–ë—ã–ª–∏ –æ–±—É—á–µ–Ω—ã —Ç—Ä–∏ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LoRA –∏ NLLB 3.3B, —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏:\n1.  –û–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–¥–Ω–æ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏.\n2.  –û–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Curriculum Learning (CL) - —Å–Ω–∞—á–∞–ª–∞ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω–æ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–µ –∫–æ—Ä–ø—É—Å–∞ BPCC (4 –º–∏–ª–ª–∏–æ–Ω–∞ —Å—Ç—Ä–æ–∫) —Å 8 –∏–Ω–¥–∏–π—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏, –∞ –∑–∞—Ç–µ–º –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\n3.  –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö (12 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –æ–±—Ä–∞–∑—Ü–æ–≤), –≤–∫–ª—é—á–∞—é—â–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å BPCC –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ –æ–±–æ–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö.</p>\n<p>–í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å –Ω–∞ —É–∑–ª–µ —Å 8 GPU NVIDIA A100 (40 –ì–ë). –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –æ–∫–∞–∑–∞–ª–∏—Å—å —Å—Ö–æ–∂–∏–º–∏, —Å –Ω–µ–±–æ–ª—å—à–∏–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º —É —Ç—Ä–µ—Ç—å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞. –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ B.</p>"
            },
            {
                "title": "Conclusion",
                "content": "In this paper, we introduced Shiksha, novel translation dataset and model tailored for Indian languages, with particular focus on the Scientific, Technical, and Educational domains. We created robust multilingual parallel corpus consisting of over 2.8 million high-quality translation pairs across 8 Indian languages. Our approach involved meticulous data extraction, cleaning, and bitext mining to ensure the accuracy and relevance of the dataset. We also fine-tuned state-of-the-art baseline NMT models using this dataset and demonstrated significant performance improvements in not only in-domain, but also out-of-domain translation tasks. With this paper, we wish to encourage the importance of domain-specific datasets in advancing NMT capabilities. We believe that our dataset and models will serve as valuable resources for the community and foster further research in multilingual NMT.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Shiksha ‚Äî –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, —Å –æ—Å–æ–±—ã–º –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –Ω–∞—É—á–Ω—É—é, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –æ–±–ª–∞—Å—Ç–∏. –ú—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–¥–µ–∂–Ω—ã–π –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 2,8 –º–∏–ª–ª–∏–æ–Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –Ω–∞ 8 –∏–Ω–¥–∏–π—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö. –ù–∞—à –ø–æ–¥—Ö–æ–¥ –≤–∫–ª—é—á–∞–ª —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Ö –æ—á–∏—Å—Ç–∫—É –∏ –ø–æ–∏—Å–∫ –±–∏—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –ú—ã —Ç–∞–∫–∂–µ –¥–æ–æ–±—É—á–∏–ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ (NMT), –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏, –Ω–æ –∏ –≤–Ω–µ –µ–µ. –≠—Ç–æ–π —Ä–∞–±–æ—Ç–æ–π –º—ã —Ö–æ—Ç–∏–º –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π NMT. –ú—ã —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –Ω–∞—à –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–Ω—É—Ç —Ü–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –∏ –±—É–¥—É—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º –≤ –æ–±–ª–∞—Å—Ç–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–≥–æ NMT.</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –í —Å—Ç–∞—Ç—å–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—â–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤.</em></p>"
            }
        ]
    },
    {
        "id": "2412.09596",
        "title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
        "url": "https://huggingface.co/papers/2412.09596",
        "abstract": "Creating AI systems that can interact with environments over long periods,\nsimilar to human cognition, has been a longstanding research goal. Recent\nadvancements in multimodal large language models (MLLMs) have made significant\nstrides in open-world understanding. However, the challenge of continuous and\nsimultaneous streaming perception, memory, and reasoning remains largely\nunexplored. Current MLLMs are constrained by their sequence-to-sequence\narchitecture, which limits their ability to process inputs and generate\nresponses simultaneously, akin to being unable to think while perceiving.\nFurthermore, relying on long contexts to store historical data is impractical\nfor long-term interactions, as retaining all information becomes costly and\ninefficient. Therefore, rather than relying on a single foundation model to\nperform all functions, this project draws inspiration from the concept of the\nSpecialized Generalist AI and introduces disentangled streaming perception,\nreasoning, and memory mechanisms, enabling real-time interaction with streaming\nvideo and audio input. The proposed framework InternLM-XComposer2.5-OmniLive\n(IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module:\nProcesses multimodal information in real-time, storing key details in memory\nand triggering reasoning in response to user queries. (2) Multi-modal Long\nMemory Module: Integrates short-term and long-term memory, compressing\nshort-term memories into long-term ones for efficient retrieval and improved\naccuracy. (3) Reasoning Module: Responds to queries and executes reasoning\ntasks, coordinating with the perception and memory modules. This project\nsimulates human-like cognition, enabling multimodal large language models to\nprovide continuous and adaptive service over time.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "f14f0c4b833358e7",
        "authors": [
            "Pan Zhang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Yuhang Zang",
            "Rui Qian",
            "Xilin Wei",
            "Lin Chen",
            "Yifei Li",
            "Junbo Niu",
            "Shuangrui Ding",
            "Qipeng Guo",
            "Haodong Duan",
            "Xin Chen",
            "Han Lv",
            "Zheng Nie",
            "Min Zhang",
            "Bin Wang",
            "Wenwei Zhang",
            "Xinyue Zhang",
            "Jiaye Ge",
            "Wei Li",
            "Jingwen Li",
            "Zhongying Tu",
            "Conghui He",
            "Xingcheng Zhang",
            "Kai Chen",
            "Yu Qiao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "affiliations": [
            "Beihang University",
            "Fudan University",
            "SenseTime Group",
            "Shanghai Artificial Intelligence Laboratory",
            "The Chinese University of Hong Kong",
            "Tsinghua University",
            "University of Science and Technology of China"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.09596.jpg",
        "data": {
            "categories": [
                "#long_context",
                "#audio",
                "#cv",
                "#multimodal",
                "#reasoning"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ: –Ω–æ–≤—ã–π —à–∞–≥ –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ò–ò",
                "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ InternLM-XComposer2.5-OmniLive, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π: –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –°–∏—Å—Ç–µ–º–∞ —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–µ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–∞–º—è—Ç–∏ –∏ –≤—ã–ø–æ–ª–Ω—è—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–∑–Ω–∞–Ω–∏–µ, –ø–æ–∑–≤–æ–ª—è—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —É—Å–ª—É–≥–∏ –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏."
            },
            "en": {
                "title": "Empowering AI with Human-like Cognition for Real-time Interaction",
                "desc": "This paper presents a new approach to creating AI systems that can interact with their environments over extended periods, similar to human thinking. It introduces a framework called InternLM-XComposer2.5-OmniLive (IXC2.5-OL), which features three modules: a Streaming Perception Module for real-time processing of multimodal inputs, a Multi-modal Long Memory Module for efficient memory management, and a Reasoning Module for responding to queries. The framework aims to overcome limitations of current multimodal large language models (MLLMs) by enabling simultaneous perception, memory, and reasoning. This approach allows for continuous and adaptive interactions, enhancing the AI's ability to function in dynamic environments."
            },
            "zh": {
                "title": "Ê®°Êãü‰∫∫Á±ªËÆ§Áü•ÁöÑÊµÅÂºè‰∫§‰∫íAIÁ≥ªÁªü",
                "desc": "Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂàõÂª∫ËÉΩÂ§üÈïøÊó∂Èó¥‰∏éÁéØÂ¢É‰∫íÂä®ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁ≥ªÁªüÔºåÁ±ª‰ºº‰∫é‰∫∫Á±ªÁöÑËÆ§Áü•ËÉΩÂäõ„ÄÇÂ∞ΩÁÆ°Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMsÔºâÂú®ÂºÄÊîæ‰∏ñÁïåÁêÜËß£ÊñπÈù¢ÂèñÂæó‰∫ÜËøõÂ±ïÔºå‰ΩÜÂú®ËøûÁª≠ÂíåÂêåÊó∂ÁöÑÊÑüÁü•„ÄÅËÆ∞ÂøÜÂíåÊé®ÁêÜÊñπÈù¢‰ªçÈù¢‰∏¥ÊåëÊàò„ÄÇÂΩìÂâçÁöÑMLLMsÂèóÈôê‰∫éÂ∫èÂàóÂà∞Â∫èÂàóÁöÑÊû∂ÊûÑÔºåÊó†Ê≥ïÂêåÊó∂Â§ÑÁêÜËæìÂÖ•ÂíåÁîüÊàêÂìçÂ∫î„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏ÄÈóÆÈ¢òÔºåÊú¨ÊñáÊèêÂá∫‰∫ÜInternLM-XComposer2.5-OmniLiveÊ°ÜÊû∂ÔºåÈÄöËøáÂàÜÁ¶ªÁöÑÊµÅÂºèÊÑüÁü•„ÄÅÊé®ÁêÜÂíåËÆ∞ÂøÜÊú∫Âà∂ÔºåÊ®°Êãü‰∫∫Á±ªËÆ§Áü•ÔºåÂÆûÁé∞ÂÆûÊó∂ÁöÑÂ§öÊ®°ÊÄÅ‰∫§‰∫í„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.",
                "summary": "<p>–ò—Ç–∞–∫, —Ü–µ–ª—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ ‚Äî —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –º–∏—Ä–æ–º –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞—é—Ç –ª—é–¥–∏. –ù–µ–¥–∞–≤–Ω–∏–µ —É—Å–ø–µ—Ö–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (MLLM) –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–¥–≤–∏–Ω—É–ª–∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –º–∏—Ä–∞. –û–¥–Ω–∞–∫–æ –∑–∞–¥–∞—á–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ—Å—Ç–∞–µ—Ç—Å—è –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –Ω–µ—Ä–µ—à–µ–Ω–Ω–æ–π.</p>\n<p>–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ MLLM –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã —Å–≤–æ–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π \"–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å-–≤-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\", –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –Ω–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –¥—É–º–∞—Ç—å –≤–æ –≤—Ä–µ–º—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –Ω–µ–ø—Ä–∞–∫—Ç–∏—á–Ω–æ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–º –∏ –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º.</p>\n<p>–ü–æ—ç—Ç–æ–º—É, –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –æ–¥–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π, –≤ –¥–∞–Ω–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ü–µ–ø—Ü–∏—è \"—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ò–ò\" –∏ –≤–≤–æ–¥—è—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</p>\n<p>–ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ InternLM-XComposer2.5-OmniLive (IXC2.5-OL) —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π:</p>\n<ol>\n<li><strong>–ú–æ–¥—É–ª—å –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è:</strong> –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–ª—é—á–µ–≤—ã–µ –¥–µ—Ç–∞–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∏—Ä—É—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</li>\n<li><strong>–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏:</strong> –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å, —Å–∂–∏–º–∞—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏. (–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –≠—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ç–æ, –∫–∞–∫ –º—ã –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –≤–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã, –æ—Ç–±—Ä–∞—Å—ã–≤–∞—è –ª–∏—à–Ω–∏–µ –¥–µ—Ç–∞–ª–∏)</li>\n<li><strong>–ú–æ–¥—É–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:</strong> –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É—è —Å–≤–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥—É–ª—è–º–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –ø–∞–º—è—Ç–∏.</li>\n</ol>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–∑–Ω–∞–Ω–∏–µ, –ø–æ–∑–≤–æ–ª—è—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ —Å —Ç–µ—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏.</p>"
            },
            {
                "title": "Cls",
                "content": "3. Method As we briefly introduced in Sec.1, the IXC2.5-OL has three disentangled modules: 1) the Streaming Perception Module for on-the-fly visual and audio information processing, 2) the Multi-modal Long Memory Module for memory integration and retrieval, and 3) the Reasoning Module collect information from the perception and memory module, and handles queries and performs reasoning tasks. All the modules work simultaneously and interact asynchronously. 3.1. Streaming Perception Module the IXC2.5-OL could handle Besides nature language, video and audio natively. To realize this, the Streaming Perception Module contains an Audio Translation Module and Video Perception Module. Audio Translation Module contains an audio encoder, an audio projector, and Small Language Model (SLM). The audio encoder encodes the input audio sample into highdimension features, and the audio projector further maps the feature to the input space of the SLM. The SLM outputs both the class (e.g. laughing, clapping, or raining) of the audio and the natural language within the audio (i.e. the automatic speech recognition). In practice, we use the Whisper [92] model as the audio encoder and Qwen21.8B [128] as the SLM. The training contains two stages and we list the training data in Table 1. Video Perception Module provides coarse-grained visual 3 Figure 2. Pipeline of the InternLM-XComposer2.5-OmniLive. (IXC2.5-OL). The IXC2.5-OL is real-time interacting system that is constructed by three simultaneous modules: 1) the Streaming Perception Module, 2) the Multi-modal Long Memory Module, and 3) the Reasoning Module. information to the Multi-modal Long Memory Module. It processes the real-time video input stream and encodes each frame into semantic features. For efficiency, we use the OpenAI CLIP-L/14 [91] In practice. 3.2. Multi-modal Long Memory Module The Multi-modal Long Memory Module is the core design to handle extremely long video input and helps the Reasoning Module to get rid of millions of tokens from its context window. It shares similar idea from the VideoStreaming [89] that encodes video clips into short-term memories and integrates them into long-term memory. With the given questions, it retrieved the most related video clips for the Reasoning Module. Formally, the Multi-modal Long Memory Module is trained with three tasks: Video Clip Compression. With features of kth video clip extracted from the Perception Module Fk RT C, we initialize its short-term memory Hk RT by the spatial down-sampling and its global memory ÀÜHk R1C. We realize the compression by the auto-regressive and feature aggregation nature of LLMs: Model Dataset Hk, ÀÜHk = Compressor([Fk Hk ÀÜHk]). Memory Integration. Short-term memory represents the detailed information of each short video clip while the model still lacks macro view of the video. To this end, with the short-term and global memory of list of video clips, we integrate them into long-term memory by the Compressor in the following format: H1, H2, ..., Hk = Compressor([H1 H2... Hk ÀÜH1 ÀÜH2... ÀÜHk]). the = [ H1, H2, ..., Hk] RkC represents the video in high-compressed way and we denote it as the long-term memory. Video Clip Retrieval. When users raise questions, the Multi-modal Long Memory Module retrieves the questionrelated video clips and provides both the video clips and their short-term memory to the Reasoning Module. In practice, we first encode the question to the feature space of the memory. We concatenate the long-term memory with the tokenized question as the Compressor input, and we view the last token of the output features as the memoryspace-aligned question feature. Then we calculate the similarity between the question feature and each videos global memory, and select the most related clips for the Reasoning Module. Implementation Detail. We use Qwen2-1.8B [128] as the LLMs and construct several kinds of training data for the three aforementioned tasks. As shown in Table. 2, we train the Video Clip Compression task with short video captioning data from multiple sources, using the same prefix captioning task designed in VideoStreaming [89]. For the Memory Integration task and Video Clip Retrieval task, besides the off-the-shelf video grounding data, we also construct data for two unique tasks: Semantics Implicit Question and Reference Implicit Question. The Semantics Implicit Question means the question does not point to some object directly, but mentions the usage or meaning of the object, and the model should find out the object by understanding the implicit question. For example, when the user asks How about the weather today?, the model should find out some weather-related object in the past video stream, such as an umbrella, sun-glass, or something. Another example could be Im hungry, where can heat my sandwiches?, the model should find the microwave oven it has seen before. The Reference Implicit Question means the question uses pronouns rather than nouns. For example, What is this means the models should retrieve the current frames, although it does not mention any exact objects. Memory Module IXC2.5 ShareGPT4Video [15], Ego4D[41] ActivityNet [32] Semantics Implicit QA Reference Implicit QA ShareGPT4Video [15], ActivityNet [32] FunQA [122], TrafficQA [125] VideoChat2-IT[61], LLaVA-Video [152] Table 2. Video Datasets used in IXC2.5-OL. Both kinds of implicit questions are commonly used in real-world communication while current models failed to handle them, so we construct corresponding training data to empower the model with these capabilities. 3.3. Reasoning Module The Reasoning Module is initialized by an improved version of InternLM-XComposer2.5 (IXC2.5 in the following for simplified statement) and we add memory projector to align the memory feature with IXC-2.5. For given questions and both visual and memory information provided by the Memory Module, we formulate the input as: Question: < Que >, Here is the question related video clip < Img >; Here is the question related memory < Mem > In real-world usage, there exists some noisy input that should not be answered (e.g., the user says enn... or ok...), the model should keep salient and wait for the next question. To realize this, we add an additional Instruction Prediction process for each question to decide it should be answered or not. 3.4. System Pipeline As illustrated in Figure 3, the system comprises the Frontend, SRS Server, and Backend Server. Frontend. The frontend application, developed with JavaScript, enables the camera and microphone to capture video and audio stream inputs, which are then pushed to the SRS server. Concurrently, it establishes WebSocket connection with the backend to listen for audio outputs and interrupt signals. When audio data is received, the frontend plays it. Upon receiving an interrupt signal, the frontend suspends the audio playback and discards the pending audio. SRS Server. SRS (Simple Realtime Server) is straightforward and efficient real-time video server, adept at supporting multitude of real-time streaming protocols such as RTMP, WebRTC, HLS, HTTP-FLV, SRT, and others. It is renowned for its ability to reliably receive and deliver audio and video streams. 5 Figure 3. System pipeline of the IXC2.5-OL. The system comprises the Frontend, SRS Server, and Backend Server. The Frontend is utilized for capturing video and audio streams and for playing audio from the Backend Server. The SRS Server is employed for managing live streams. The Backend Server is responsible for reading audio and video, extracting memory, and answering questions. The green boxes in the figure represent thread or process. Backend Server. After establishing WebSocket connection with the frontend, the backend will pull streaming from the SRS Server and initiate separate threads to read audio and video. The audio reading thread will segment the audio stream into 4096-bit chunks and enqueue them into the Audio Queue. The Voice Activity Detection (VAD) [40] thread continuously reads data from Audio Queue and detects the start and end of voice activity. Upon detecting the start of voice activity, the backend sends an interrupt signal to the frontend to pause the currently playing audio, and at the same time, dispatches backup signal to the video process, directing it to save the current memory state. When detecting the end of voice activity, the entire voice segment will be enqueued into ASR Todo Queue. The ASR thread continuously reads audio segments from ASR Todo Queue, performs background noise classification and voice recognition on them, and then enqueues the results into LLM Todo Queue for use by the LLM. The video reading thread reads video frames at rate of 1 frame per second and enqueues them into Frame Queue. The compressor process reads video frames from the queue, recognizes them, extracts relevant memory, and stores it. Upon receiving backup signal from the VAD thread, the compressor process will save the current memory state for later retrieval. The LLM process reads text from the LLM Todo Queue and determines whether it is an instruction that requires response from the model. For texts identified as instructions, the compressor process will use the current instruction and the backed-up memory to perform memory grounding, in order to retrieve memories related to the instruction. The LLM process will then generate response based on the retrieved memories and the instruction, and enqueue the resulting output into TTS Todo Queue. An additional TTS thread (e.g., F5-TTS [20], MeloTTS [154]) will convert the text from the TTS Todo Queue into audio and send it to the frontend. 4. Experiments In this section, we validate the benchmark performance of our InternLM-XComposer2.5-OmniLive (IXC2.5-OL), including both audio and video benchmarks. 4.1. Audio Benchmarks We evaluate our audio models on two prominent automatic speech recognition (ASR) benchmarks: Wenetspeech [140] for Chinese (CN) and LibriSpeech [87] for English (EN). WenetSpeech includes two test sets: Test Net, which represents high-quality and relatively clean Chinese speech, and Test Meeting, which captures more challenging conversational scenarios. LibriSpeech consists of four splits: Dev clean and Test clean, which contain clean, high-quality English speech, and Dev other and Test other, which include noisier, more complex utterances. As shown in Table 3, our IXC2.5-OL demonstrates supe6 Table 3. Evaluation results on ASR tasks: CN refers to Chinese speech, while ENG refers to English speech. The performance is measured using WER (Word Error Rate). Method LLM Wenetspeech (CN) Librispeech (ENG) Test Net Test Meeting Dev clean Dev other Test clean Test other Qwen2-Audio [26] Mini-Omni [123] VITA [38] IXC2.5-OL Qwen2-7B [128] Qwen2-0.5B [128] Mixtral-8x7B [47] Qwen2-1.5B [128] 7.8 - 12.2 9.0 8.4 - 16.5 9.2 1.3 4.5 7.6 2.5 3.4 9.7 16.6 5.7 1.6 4.6 8.1 2. 3.6 9.2 18.4 5.8 Table 4. Evaluation results on MLVU benchmark. IXC2.5-OL has demonstrated excellent performance, surpassing both open-source models and closed-source APIs, achieving SOTA at the 7B model scale. Method Params Topic Rea. Anomaly Recog. Needle QA Ego Rea. Plot QA Action Or. Action Co. M-Avg Closed-source APIs. Claude-3-Opus Qwen-VL-Max GPT-4 Turbo GPT-4o Open-source models. MovieChat [99] LLaMA-VID [65] LLaVA-1.6 [71] ShareGPT4Video [15] VideoLlaMA2 [23] LongVA [149] IXC2.5 [148] InternVL2 [22] LLaVA-OneVision [57] Video-XL [97] IXC2.5-OL - - - - 7B 7B 7B 7B 7B 7B 7B 8B 7B 7B 7B 67.2 67.4 79.5 87.4 29.5 50.8 60.6 75.8 74.6 83.3 - - - - 84.1 43.5 63.5 68.0 74.5 25.0 34.5 41.0 51.5 64.5 58.5 - - - - 68.5 21.6 40.3 45.9 64.8 24.2 30.1 43.1 47.6 49.9 69.3 - - - - 76.6 40.2 40.9 47.4 57.1 24.7 32.7 38.4 43.2 43.8 50.0 - - - - 60.8 47.8 43.3 60.6 65.1 25.8 32.5 41.0 48.4 45.1 67.2 - - - - 75.1 18.2 25.0 26.5 56.7 28.6 23.9 25.5 34.0 34.0 38.6 - - - - 57.1 16.7 14.8 16.1 46.3 22.8 27.8 25.7 23.3 27.4 27.2 - - - - 41.3 36.5 42.2 49.2 64.6 25.8 33.2 39.3 46.4 48.5 56.3 58.8 64.0 64.7 64. 66.2 Evaluation results on Video-MME benchmark. Table 5. IXC2.5-OL demonstrates performance close to that of the opensource SOTA. Word Error Rates (WER) across both CN and EN benchmarks with merely lightweight 1.5B LLM. Method Params Short Medium Long Overall 4.2. Video Benchmarks Closed-source APIs. GPT-4V Claude 3.5 Sonnet GPT-4o mini GPT-4o Gemini 1.5 Pro Open-source models. ShareGPT4Video [15] VideoLlaMA2 [23] LongVA [149] Video-XL [97] VITA [38] IXC2.5 [148] InternVL2 [22] LLaVA-OneVision [57] mPLUG-Owl3 [131] MiniCPM-V 2.6 [130] IXC2.5-OL - - - - - 70.5 71.0 72.5 80.0 81.7 7B 7B 7B 7B 48.3 - 61.1 64.0 87B 65.9 7B 8B 7B 7B 8B 7B - - - 70.0 - 72.7 55.8 57.4 63.1 70.3 74.3 36.3 - 50.4 53.2 52.9 - - - 57.7 - 58.2 53.5 51.2 58.6 65.3 67.4 35.0 - 46.2 49.2 48.6 - - - 50.1 - 50.8 59.9 60.0 64.8 71.9 75.0 39.9 47.9 52.6 55.5 55.8 55.8 56.3 58.2 59.3 60.9 60.6 rior performance compared to recent streaming audio LLMs such as VITA and Mini-Omni, particularly achieving lower In Tables 4, 5, 7 and 8, we compare IXC2.5-OL with both closed-source APIs and open-source models on conventional video understanding benchmarks, including MLVU [155], Video-MME [37], MMBench-Video [34] and MVBench [62]. Furthermore, we also assess the performance of different models on the recently proposed StreamingBench [67], which is designed to better evaluate performance for real-time video interactions. The results of this comparison are presented in Table 6. For the video benchmarks, the base model utilizes 64 sampled frames for each video during evaluation. MLVU MLVU is comprehensive benchmark designed for evaluating Multimodal Large Language Models in Long Video Understanding tasks. The videos range from 3 minutes to 2 hours and include nine distinct evaluation tasks. Here, we evaluate seven multi-choice tasks, including Topic Reasoning, Anomaly Recognition, Needle QA, Ego Reasoning, Plot QA, Action Order, and Action Count. The deTable 6. Evaluation results on StreamingBench for Real-Time Visual Understanding. Metrics include Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). IXC2.5-OL excels among all open-source models, and falling just short of the closed-source API, Gemini 1.5 Pro.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ \"–ú–µ—Ç–æ–¥\" —Å—Ç–∞—Ç—å–∏ –æ –º–æ–¥–µ–ª–∏ IXC2.5-OL</h2>\n<p>–ú–æ–¥–µ–ª—å IXC2.5-OL —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π: –º–æ–¥—É–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä—Ü–µ–ø—Ü–∏–∏, –º–æ–¥—É–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ –º–æ–¥—É–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –í—Å–µ –º–æ–¥—É–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ.</p>\n<p><strong>–ú–æ–¥—É–ª—å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä—Ü–µ–ø—Ü–∏–∏</strong> –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏ –∞—É–¥–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:</p>\n<ul>\n<li><strong>–ú–æ–¥—É–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –∞—É–¥–∏–æ:</strong><ul>\n<li>–ö–æ–¥–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–µ –≤ –≤—ã—Å–æ–∫–æ—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –ø–æ–º–æ—â—å—é –∞—É–¥–∏–æ–∫–æ–¥–µ—Ä–∞ (–≤ —Å—Ç–∞—Ç—å–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Whisper).</li>\n<li>–ü—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤–≤–æ–¥–∞ –º–∞–ª–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (SLM).</li>\n<li>SLM (–≤ —Å—Ç–∞—Ç—å–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Qwen2-1.8B) –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å –∞—É–¥–∏–æ—Å–æ–±—ã—Ç–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–º–µ—Ö, –∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã) –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å.</li>\n<li>–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö, —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ 1 (–Ω–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ).</li>\n</ul>\n</li>\n<li><strong>–ú–æ–¥—É–ª—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤–∏–¥–µ–æ:</strong><ul>\n<li>–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</li>\n<li>–ö–æ–¥–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –∫–∞–¥—Ä –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI CLIP-L/14).</li>\n<li>–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–æ–±—â–µ–Ω–Ω—É—é –≤–∏–∑—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–¥—É–ª—é –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–ú–æ–¥—É–ª—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏</strong> –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–µ–π –∏ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥—É–ª—é —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∏–∑–±–∞–≤–ª—è—è –µ–≥–æ –æ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–Ω –∫–æ–¥–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ–∫–ª–∏–ø—ã –≤ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏—Ö –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å. –ü—Ä–∏ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∞ –º–æ–¥—É–ª—å –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø—ã –¥–ª—è –º–æ–¥—É–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –ú–æ–¥—É–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–µ—Ö –∑–∞–¥–∞—á–∞—Ö:</p>\n<ul>\n<li><strong>–°–∂–∞—Ç–∏–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø–æ–≤:</strong><ul>\n<li>–ü—Ä–∏–∑–Ω–∞–∫–∏ <em>k</em>-–≥–æ –≤–∏–¥–µ–æ–∫–ª–∏–ø–∞ (<em>Fk</em>) —Å–∂–∏–º–∞—é—Ç—Å—è –≤ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å (<em>Hk</em>) –∏ –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å (<em>ÀÜHk</em>) —Å –ø–æ–º–æ—â—å—é –º–µ—Ö–∞–Ω–∏–∑–º–∞ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) - <em>Hk, ÀÜHk = Compressor([Fk Hk ÀÜHk])</em>.</li>\n</ul>\n</li>\n<li><strong>–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø–∞–º—è—Ç–∏:</strong><ul>\n<li>–ö—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∏ –≥–ª–æ–±–∞–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∏–¥–µ–æ–∫–ª–∏–ø–æ–≤ (<em>H1, H2, ..., Hk, ÀÜH1, ÀÜH2, ..., ÀÜHk</em>) –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç—Å—è –≤ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å (<em>H</em>), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â—É—é —Å–∂–∞—Ç–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–≥–æ –≤–∏–¥–µ–æ - <em>H1, H2, ..., Hk = Compressor([H1 H2... Hk ÀÜH1 ÀÜH2... ÀÜHk])</em>.</li>\n</ul>\n</li>\n<li><strong>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–¥–µ–æ–∫–ª–∏–ø–æ–≤:</strong><ul>\n<li>–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–∞–º—è—Ç–∏.</li>\n<li>–î–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç—Å—è —Å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –∏ –ø–æ–¥–∞–µ—Ç—Å—è –Ω–∞ –≤—Ö–æ–¥ LLM.</li>\n<li>–ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –≤–æ–ø—Ä–æ—Å–∞, –≤—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–π —Å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –ø–∞–º—è—Ç–∏.</li>\n<li>–í—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–æ–º –≤–æ–ø—Ä–æ—Å–∞ –∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–µ–æ–∫–ª–∏–ø–∞.</li>\n<li>–ù–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–ª–∏–ø—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ –º–æ–¥—É–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–º–µ—Å—Ç–µ —Å –∏—Ö –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é.</li>\n</ul>\n</li>\n</ul>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ LLM –¥–ª—è –º–æ–¥—É–ª—è –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Qwen2-1.8B. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–¥–ø–∏—Å–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ, –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–∑–µ–º–ª–µ–Ω–∏—è –≤–∏–¥–µ–æ, –∞ —Ç–∞–∫–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–¥–∞—á –Ω–µ—è–≤–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏ –Ω–µ—è–≤–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –ø–æ —Å—Å—ã–ª–∫–µ.</p>\n<ul>\n<li><strong>–ù–µ—è–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ:</strong> –≤–æ–ø—Ä–æ—Å –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–±—ä–µ–∫—Ç –Ω–∞–ø—Ä—è–º—É—é, –∞ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —Å–º—ã—Å–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö–∞–∫–∞—è —Å–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–¥–∞?\" -&gt; –ø–æ–∏—Å–∫ –∑–æ–Ω—Ç–∏–∫–∞).</li>\n<li><strong>–ù–µ—è–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ —Å—Å—ã–ª–∫–µ:</strong> –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ß—Ç–æ —ç—Ç–æ?\" -&gt; –ø–æ–∏—Å–∫ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞).</li>\n</ul>\n<p><strong>–ú–æ–¥—É–ª—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π</strong> –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π InternLM-XComposer2.5 (IXC2.5) –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–æ–º –ø–∞–º—è—Ç–∏ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–∞–º—è—Ç–∏ —Å IXC-2.5. –ù–∞ –≤—Ö–æ–¥ –º–æ–¥—É–ª—è –ø–æ–¥–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∞ —Ç–∞–∫–∂–µ –≤–∏–∑—É–∞–ª—å–Ω–∞—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–∞—è –º–æ–¥—É–ª–µ–º –ø–∞–º—è—Ç–∏. –§–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: <em>Question: &lt; Que &gt;, Here is the question related video clip &lt; Img &gt;; Here is the question related memory &lt; Mem &gt;</em>.</p>\n<p>–î–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"—ç—ç—ç\" –∏–ª–∏ \"–æ–∫\") –º–æ–¥—É–ª—å –≤—ã–ø–æ–ª–Ω—è–µ—Ç <strong>–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</strong>, –æ–ø—Ä–µ–¥–µ–ª—è—è, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –Ω–µ—Ç.</p>\n<p><strong>–°–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä</strong> –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:</p>\n<ul>\n<li><strong>Frontend:</strong> –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ JavaScript, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—é—â–µ–µ –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ, –æ—Ç–ø—Ä–∞–≤–ª—è—é—â–µ–µ –∏—Ö –Ω–∞ SRS-—Å–µ—Ä–≤–µ—Ä –∏ –ø–æ–¥–∫–ª—é—á–∞—é—â–µ–µ—Å—è –∫ Backend-—Å–µ—Ä–≤–µ—Ä—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞—É–¥–∏–æ–≤—ã—Ö–æ–¥–∞ –∏ —Å–∏–≥–Ω–∞–ª–æ–≤ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è.</li>\n<li><strong>SRS Server:</strong> —Å–µ—Ä–≤–µ—Ä —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π –∏ –ø–µ—Ä–µ–¥–∞—é—â–∏–π –∞—É–¥–∏–æ- –∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, RTMP, WebRTC).</li>\n<li><strong>Backend Server:</strong><ul>\n<li>–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Ç–æ–∫–∏ —Å SRS-—Å–µ—Ä–≤–µ—Ä–∞ –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ—Ç–æ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ –∏ –≤–∏–¥–µ–æ.</li>\n<li><strong>–ü–æ—Ç–æ–∫ —á—Ç–µ–Ω–∏—è –∞—É–¥–∏–æ:</strong> —Å–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –Ω–∞ —á–∞—Å—Ç–∏ (4096 –±–∏—Ç) –∏ –ø–æ–º–µ—â–∞–µ—Ç –∏—Ö –≤ –æ—á–µ—Ä–µ–¥—å.</li>\n<li><strong>–ü–æ—Ç–æ–∫ VAD (Voice Activity Detection):</strong> –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü –≥–æ–ª–æ—Å–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏, –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è –Ω–∞ Frontend –∏ —Å–∏–≥–Ω–∞–ª —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ.</li>\n<li><strong>–ü–æ—Ç–æ–∫ ASR (Automatic Speech Recognition):</strong> —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç —Ä–µ—á—å –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π —à—É–º, –ø–æ–º–µ—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è LLM.</li>\n<li><strong>–ü–æ—Ç–æ–∫ —á—Ç–µ–Ω–∏—è –≤–∏–¥–µ–æ:</strong> —Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–¥—Ä—ã —Å —á–∞—Å—Ç–æ—Ç–æ–π 1 –∫–∞–¥—Ä/—Å–µ–∫ –∏ –ø–æ–º–µ—â–∞–µ—Ç –∏—Ö –≤ –æ—á–µ—Ä–µ–¥—å.</li>\n<li><strong>–ü—Ä–æ—Ü–µ—Å—Å –∫–æ–º–ø—Ä–µ—Å—Å–æ—Ä–∞:</strong> –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–¥—Ä—ã, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞–º—è—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–∏–≥–Ω–∞–ª–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è.</li>\n<li><strong>–ü—Ä–æ—Ü–µ—Å—Å LLM:</strong> –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–∑–µ–º–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ–º–µ—â–∞—è –µ–≥–æ –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è TTS.</li>\n<li><strong>–ü–æ—Ç–æ–∫ TTS (Text-to-Speech):</strong> –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∞—É–¥–∏–æ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –µ–≥–æ –Ω–∞ Frontend.</li>\n</ul>\n</li>\n</ul>\n<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–∏, –º–æ–¥–µ–ª—å IXC2.5-OL –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º—É —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, —Å–ø–æ—Å–æ–±–Ω—É—é –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∞—É–¥–∏–æ- –∏ –≤–∏–¥–µ–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</p>"
            },
            {
                "title": "Ct overall",
                "content": "89.47 92.00 93.60 91.47 95.65 92.52 88.00 88.75 89.74 91.30 91.46 80.49 77.34 82.02 81.73 72.33 75.39 61.11 61.79 69.32 43.09 77.11 80.47 83.91 76.47 70.19 83.80 66.67 62.19 69.12 49.22 79.02 80.47 83.54 79.67 80.00 84.74 77.78 64.23 71.95 48.70 72.44 73.28 75.69 39.07 40.06 34.49 31.05 45.96 32.40 31.48 34.16 42.49 27.89 55.86 55.47 57.41 58.17 52.80 43.61 39.21 42.68 45.61 35.23 53.68 49.22 70.98 56.86 53.42 53.89 54.63 48.78 50.14 17.62 70.03 63.28 61.20 70.92 62.73 59.50 61.11 53.66 54.67 34.72 68.12 60.94 69.40 77.12 67.70 62.93 59.26 53.25 54.96 56.48 71.12 84.38 70.66 73.20 67.08 61.68 56.48 55.69 62.04 38.86 71.93 71.09 77.92 75.82 64.60 65.73 70.37 56.10 62.32 53.37 75.20 82.81 73.19 77.45 68.32 71.03 72.22 61.19 69.04 46.11 80.38 74.22 76.03 80.72 72.67 71.65 67.59 65.45 65.72 45.08 35.99 49.52 52.32 59.96 63.72 64.60 67.44 69.04 71. 82.83 73.77 78.66 82.95 72.50 76.01 61.11 60.67 71.59 58.85 73.79 - - - - 8B 7B 8B 7B 8B 7B 8B 7B 7B 7B Table 7. Evaluation results on MMBench-Video. Tasks include Coarse Perception (CP), Single-Instance Finegrained Perception (FP-S), Cross-Instance Finegrained Perception (FP-C), Hallucination (HL), Logic Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Commonsense Reasoning (CSR), and Temporal Reasoning (TP). Method Params Perception Reasoning Overall CP FP-S FP-C HL Mean LR AR RR CSR TP Mean Closed-source APIs. Claude 3.5 Sonnet Gemini 1.0 Pro Gemini 1.5 Pro GPT-4V GPT-4o Open-source models. MovieLLM [101] LLaVA-OneVision [57] PLLaVA [126] ShareGPT4Video [15] VideoStreaming [89] LLaVA-NeXT-Video [151] VILA1.5 [68] InternVL2 [22] Qwen2-VL [113] IXC2.5-OL - - - - - 7B 72B 7B 7B 7B 7B 13B 8B 7B 7B 1.57 1.61 1.99 1.83 2.23 0. 1.22 1.08 1.20 1.38 1.35 1. 1.41 1.63 1.39 1.56 2.04 1. 2.24 0.82 1.07 1.06 1.05 1. 1.15 1.45 1.37 1.51 1.07 1. 1.70 1.40 2.01 0.70 0.90 0. 1.00 0.8 0.97 1.26 1.15 1. 1.40 0.65 1.90 1.76 1.90 0. 0.21 0.52 0.32 0.32 0.58 0. 0.19 0.55 1.38 1.50 1.98 1. 2.19 0.81 1.03 1.02 1.04 1. 1.14 1.39 1.30 1.46 1.13 1. 1.98 1.45 2.11 0.52 0.76 0. 0.89 0.77 0.64 0.80 0.90 1. 1.70 1.57 2.02 1.91 2.12 1. 0.96 1.25 1.06 1.27 1.38 1. 1.34 1.56 1.48 1.55 1.92 1. 2.17 1.22 0.55 1.17 1.19 1. 1.30 1.30 1.38 1.49 1.54 1. 1.78 1.83 1.94 0.54 0.81 0. 1.01 1.01 1.27 1.40 1.14 1. 1.04 1.33 1.63 1.53 1.97 1. 0.48 1.01 0.99 1.10 1.03 1. 1.00 1.21 1.35 1.39 1.86 1. 2.08 0.97 0.70 1.03 1.03 1. 1.13 1.28 1.16 1.35 1.38 1. 1.94 1.68 2.15 0.87 0.94 1. 1.05 1.12 1.14 1.36 1.26 1. 1.53 1.61 1.20 0.15 1.49 0.93 1. 1.57 1.30 1.08 1.25 1.42 Table 8. Evaluatation results on MVBench. Tasks include Action Sequence (AS), Action Prediction (AP), Action Antonym (AA), Finegrained Action (FA), Unexpected Action (UA), Object Existence (OE), Object Interaction (OI), Object Shuffle (OS), Moving Direction (MD), Action Localization (AL), Scene Transition (ST), Action Count (AC), Moving Count (MC), Moving Attribute (MA), State Change (SC), Fine-grained Pose (FP), Character Order (CO), Egocentric Navigation (EN), Episodic Reasoning (ER), and Counterfactual Inference (CI). Method Params AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg Closed-source APIs. GPT-4V GPT-4o Open-source models. VideoLLaMA [144] VideoChat [60] MiniCPM-V 2.6 [130] VideoChat2 [62] Qwen2-VL [113] PLLaVA [126] LLaVA-OneVision [57] InternVL2 [22] - - 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.0 59.0 11.0 43.5 61.5 56.5 72.0 54.0 82.0 62.5 66.5 44.0 36.5 33.5 93.0 54.5 33.5 54.5 53.5 74.5 71.5 32.5 71.0 42.5 57.5 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5 7B 38.0 43.0 63.0 35.5 67.5 55.5 46.0 35.5 25.5 33.0 77.5 48.0 37.0 54.0 42.5 40.0 31.0 38.0 43.0 40.5 44.7 7B 66.0 47.5 83.5 49.5 60.0 58.0 71.5 42.5 23.0 23.0 88.5 39.0 42.0 58.5 44.0 49.0 36.5 35.0 40.5 65.5 51.1 7B 51.0 58.0 77.5 47.0 64.0 63.0 65.5 40.0 25.5 35.5 77.0 43.5 47.0 62.0 42.0 61.5 49.5 41.5 47.5 41.5 52.0 7B 34B 65.0 53.0 83.5 45.0 77.5 70.0 64.5 38.5 37.5 49.0 89.5 41.5 43.5 70.0 53.0 52.5 65.0 39.5 60.5 58.0 57.8 72B 63.0 58.0 84.5 46.5 85.5 64.0 73.5 41.5 37.0 69.0 95.0 47.5 47.5 75.5 53.5 52.0 70.5 34.0 64.0 54.5 60.8 75.0 62.0 83.5 40.5 69.5 96.0 72.0 29.5 58.0 53.0 88.5 39.5 83.0 97.0 51.0 78.5 65.0 33.0 48.0 67.0 64.5 8B IXC2.5-OL 7B 84.5 81.0 75.0 46.0 81.0 92.0 79.5 36.5 83.0 47.0 90.0 60.5 75.0, 93.0 58.0 60.5 74.0 42.0 53.0 62.0 68.7 tailed comparisons are given in Table 4. The IXC2.5-OL exhibits state-of-the-art (SOTA) performance among closedsource APIs, and open-source models with parameters less than 10 billion, surpassing the previous SOTA by 1.3% for Video-XL, 1.6% for GPT-4o. quality in terms of accuracy, consistency, and alignment with human judgment. The results are presented in Table 7. IXC2.5-OL demonstrates state-of-the-art performance on perception tasks and comparable performance on overall evaluations. Video-MME Video-MME is high-quality video benchmark. The videos are collected from 6 primary visual domains with 30 subfields to ensure broad scenario generalizability, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour. As demonstrated in Table 5, the IXC2.5-OL exhibits competitive performance on this benchmark, comparable to previous SOTA MiniCPM-V 2.6. MVBench MVBench is video benchmark that emphasizes temporal understanding. It encompasses 20 challenging video tasks that cannot be effectively addressed using single frame. As shown in Table 8, IXC2.5-OL, despite having smaller 7B parameter size, has outperformed both the GPT-4 series and the 72B open-source model LLaVAOneVision, demonstrating its strong capability in understanding video temporal dynamics. 5. Conclusion We have presented IXC2.5-OL, real-time streaming model that advances multi-modal text, audio, and visual capabilities with long-term memory. IXC2.5-OL empowers users to engage in dynamic and interactive experiences. Our models real-time processing enables fluid and responsive interactions, allowing users to engage with ever-changing environments of multimodal data seamlessly, providing more intuitive and efficient user experience. Our future work will focus on reducing system latency to provide seamless user experience. StreamingBench StreamingBench is streaming video benchmark designed for real-time video evaluation. It comprises 18 tasks, showcasing 900 videos and 4,500 humancurated QA pairs. In this context, we focus on assessing visual understanding in real-time. Table 6 illustrates the comparative analysis, demonstrating that IXC2.5-OL excels among all open-source models, achieving 2.67% improvement over the previous state-of-the-art model, LLaVAOneVision, and falling just short of the closed-source API, Gemini 1.5 Pro. This performance solidifies IXC2.5-OLs remarkable prowess in real-time video interaction. MMBench-Video MMBench-Video is free-form QA video benchmark consisting of 600 videos and 2000 QA pairs. The duration of each video varies from 30 seconds to 6 minutes. Given the open-ended nature of the answers, the benchmark utilizes GPT-4-based evaluation to enhance",
                "summary": "<h2>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –Ω–∞ MMBench-Video</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MMBench-Video. MMBench-Video - —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö –∏ –æ—Ç–≤–µ—Ç–∞—Ö (QA) —Å–æ —Å–≤–æ–±–æ–¥–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 600 –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–≤ –∏ 2000 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –≤–∏–¥–µ–æ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç 30 —Å–µ–∫—É–Ω–¥ –¥–æ 6 –º–∏–Ω—É—Ç. –ò–∑-–∑–∞ –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤, –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPT-4.</p>\n<p><strong>–ó–∞–¥–∞—á–∏ –≤ MMBench-Video</strong></p>\n<p>–ë–µ–Ω—á–º–∞—Ä–∫ MMBench-Video –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∏–¥–µ–æ:</p>\n<ul>\n<li><strong>Coarse Perception (CP)</strong> - –≥—Ä—É–±–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ: –æ–±—â–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å—Ü–µ–Ω—ã.</li>\n<li><strong>Single-Instance Finegrained Perception (FP-S)</strong> - –¥–µ—Ç–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤.</li>\n<li><strong>Cross-Instance Finegrained Perception (FP-C)</strong> - –¥–µ—Ç–∞–ª—å–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –≤–æ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏.</li>\n<li><strong>Hallucination (HL)</strong> - –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–¥—É–º—ã–≤–∞–Ω–∏–µ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–µ—Ç–∞–ª–µ–π.</li>\n<li><strong>Logic Reasoning (LR)</strong> - –ª–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ.</li>\n<li><strong>Attribute Reasoning (AR)</strong> - —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ–± –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –æ–±—ä–µ–∫—Ç–æ–≤.</li>\n<li><strong>Relation Reasoning (RR)</strong> - —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ–± –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏.</li>\n<li><strong>Commonsense Reasoning (CSR)</strong> - —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞.</li>\n<li><strong>Temporal Reasoning (TP)</strong> - —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö.</li>\n</ul>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π</strong></p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –∫–∞–∫ –∑–∞–∫—Ä—ã—Ç—ã–µ API (—Ç–∞–∫–∏–µ –∫–∞–∫ Claude 3.5 Sonnet, Gemini 1.0 Pro, Gemini 1.5 Pro, GPT-4V, GPT-4o), —Ç–∞–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ (MovieLLM, LLaVA-OneVision, PLLaVA, ShareGPT4Video, VideoStreaming, LLaVA-NeXT-Video, VILA1.5, InternVL2, Qwen2-VL, IXC2.5-OL).</p>\n<p><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã IXC2.5-OL</strong></p>\n<p>–ú–æ–¥–µ–ª—å IXC2.5-OL –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è (perception) –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–±—â–∏—Ö –æ—Ü–µ–Ω–∫–∞—Ö, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –µ—ë –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞. –≠—Ç–∞ –º–æ–¥–µ–ª—å, —Ç–∞–∫–∂–µ –∫–∞–∫ –∏ –¥—Ä—É–≥–∏–µ –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã–µ, –∏–º–µ–µ—Ç 7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>\n<p><strong>–í—ã–≤–æ–¥</strong></p>\n<p>–î–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ IXC2.5-OL —è–≤–ª—è–µ—Ç—Å—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–π –º–æ–¥–µ–ª—å—é, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤–∏–¥–µ–æ, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MMBench-Video.</p>"
            }
        ]
    },
    {
        "id": "2412.05210",
        "title": "Evaluating and Aligning CodeLLMs on Human Preference",
        "url": "https://huggingface.co/papers/2412.05210",
        "abstract": "Code large language models (codeLLMs) have made significant strides in code\ngeneration. Most previous code-related benchmarks, which consist of various\nprogramming exercises along with the corresponding test cases, are used as a\ncommon measure to evaluate the performance and capabilities of code LLMs.\nHowever, the current code LLMs focus on synthesizing the correct code snippet,\nignoring the alignment with human preferences, where the query should be\nsampled from the practical application scenarios and the model-generated\nresponses should satisfy the human preference. To bridge the gap between the\nmodel-generated response and human preference, we present a rigorous\nhuman-curated benchmark CodeArena to emulate the complexity and diversity of\nreal-world coding tasks, where 397 high-quality samples spanning 40 categories\nand 44 programming languages, carefully curated from user queries. Further, we\npropose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B\ntokens) by scaling instructions from the website to verify the effectiveness of\nthe large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder\ntotally trained on synthetic instruction data can achieve top-tier performance\nof open-source code LLMs. The results find performance differences between\nexecution-based benchmarks and CodeArena. Our systematic experiments of\nCodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code\nLLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring\nthe importance of the human preference\nalignment.\\footnote{\\url{https://codearenaeval.github.io/ }}",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-06",
        "pub_date_card": {
            "ru": "6 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 6",
            "zh": "12Êúà6Êó•"
        },
        "hash": "0232aabe01d37826",
        "authors": [
            "Jian Yang",
            "Jiaxi Yang",
            "Ke Jin",
            "Yibo Miao",
            "Lei Zhang",
            "Liqun Yang",
            "Zeyu Cui",
            "Yichang Zhang",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "affiliations": [
            "Alibaba Group",
            "Shanghai Jiao Tong University",
            "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
            "University of Chinese Academy of Sciences"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.05210.jpg",
        "data": {
            "categories": [
                "#plp",
                "#alignment",
                "#benchmark",
                "#open_source",
                "#synthetic",
                "#training"
            ],
            "emoji": "üèÜ",
            "ru": {
                "title": "CodeArena: –Ω–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞",
                "desc": "–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeArena –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–æ–¥–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –∏–∑ 397 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞–¥–∞—á –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ 44 —è–∑—ã–∫–∞—Ö. –¢–∞–∫–∂–µ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π SynCode-Instruct –æ–±—ä–µ–º–æ–º –æ–∫–æ–ª–æ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∫–æ–¥–∞, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞."
            },
            "en": {
                "title": "Bridging Code Generation and Human Preferences with CodeArena",
                "desc": "This paper discusses the advancements in code large language models (codeLLMs) for code generation tasks. It highlights the limitations of existing benchmarks that primarily focus on code correctness without considering human preferences in real-world applications. To address this, the authors introduce CodeArena, a human-curated benchmark that includes diverse coding tasks across multiple programming languages. Additionally, they present SynCode-Instruct, a large synthetic instruction dataset that enhances the training of codeLLMs, revealing significant performance differences between various models when evaluated against human-aligned benchmarks."
            },
            "zh": {
                "title": "ÊèêÂçá‰ª£Á†ÅÁîüÊàêÊ®°Âûã‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê",
                "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïÂ∑•ÂÖ∑CodeArenaÔºåÁî®‰∫éËØÑ‰º∞‰ª£Á†ÅÁîüÊàêÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºàcode LLMsÔºâÁöÑÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÂü∫ÂáÜÊµãËØï‰∏ªË¶ÅÂÖ≥Ê≥®‰ª£Á†ÅÁâáÊÆµÁöÑÊ≠£Á°ÆÊÄßÔºåËÄåÂøΩËßÜ‰∫Ü‰∏é‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂØπÈΩê„ÄÇCodeArenaÈÄöËøáÊèê‰æõ397‰∏™È´òË¥®ÈáèÊ†∑Êú¨ÔºåÊ∂µÁõñ40‰∏™Á±ªÂà´Âíå44ÁßçÁºñÁ®ãËØ≠Ë®ÄÔºåÊ®°ÊãüÁúüÂÆûÁºñÁ†Å‰ªªÂä°ÁöÑÂ§çÊùÇÊÄßÂíåÂ§öÊ†∑ÊÄß„ÄÇÁ†îÁ©∂ËøòÊèêÂá∫‰∫Ü‰∏Ä‰∏™Â§öÊ†∑ÂåñÁöÑÂêàÊàêÊåá‰ª§ËØ≠ÊñôÂ∫ìSynCode-InstructÔºå‰ª•È™åËØÅÂ§ßËßÑÊ®°ÂêàÊàêÊåá‰ª§ÂæÆË∞ÉÁöÑÊúâÊïàÊÄßÔºåÁªìÊûúÊòæÁ§∫ÂºÄÊ∫ê‰ª£Á†ÅLLMs‰∏é‰∏ìÊúâLLMs‰πãÈó¥Â≠òÂú®ÊòæËëóÁöÑÊÄßËÉΩÂ∑ÆË∑ù„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (–ë–Ø–ú), —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–µ—Å—è –Ω–∞ –∫–æ–¥–µ (codeLLM), –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã. –û–¥–Ω–∞–∫–æ, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ codeLLM –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∫–æ–¥–∞, –Ω–µ —É—á–∏—Ç—ã–≤–∞—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º. –í —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –æ—Ç—Ä–∞–∂–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, –∞ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–∂–∏–¥–∞–Ω–∏—è–º –ª—é–¥–µ–π.</p>\n<p>–ß—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–æ—Ç —Ä–∞–∑—Ä—ã–≤, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π, —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–π –ª—é–¥—å–º–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º CodeArena. –û–Ω –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∫–ª—é—á–∞–µ—Ç 397 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 40 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ 44 —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –±—ã–ª–∏ –æ—Ç–æ–±—Ä–∞–Ω—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ—Ä–ø—É—Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π SynCode-Instruct, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø–æ—á—Ç–∏ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –û–Ω –±—ã–ª —Å–æ–∑–¥–∞–Ω –ø—É—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –≤–µ–±-—Å–∞–π—Ç–∞, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è codeLLM –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–æ–¥–µ–ª—å Qwen2.5-SynCoder, –æ–±—É—á–µ–Ω–Ω–∞—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∞ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö codeLLM.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤—ã—è–≤–∏–ª–∏ —Ä–∞–∑–ª–∏—á–∏—è –≤ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞, –∏ CodeArena. –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å CodeArena, –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 40 —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ codeLLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Coder) –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, OpenAI o1). –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —É—á–µ—Ç–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏ –æ—Ü–µ–Ω–∫–µ codeLLM.</p>"
            },
            {
                "title": "Introduction",
                "content": "Advanced large language models (LLMs)(OpenAI, 2023; Anthropic, 2023) have demonstrated impressive performance across wide range of tasks, particularly excelling in code completion and generation. Code capabilities have established LLMs as 1https://codearenaeval.github.io/ Figure 1: comparison between the GPT4o with better human preference and Qwen2.5-Coder-7B-Instruct. Qwen2.5-Coder-7B-Instruct solves the user question by simply replying with the code snippet without details. essential productivity tools in software engineering. Recently, open code-specific LLMs, such as StarCoder(Li et al., 2023), DeepSeekCoder (Guo et al., 2024a), and QwenCoder (Hui et al., 2024), have made significant progress, achieving performance on fundamental code generation tasks (Austin et al., 2021; Cassano et al., 2023) that approaches the level of top-tier proprietary models. Moreover, their open and transparent model weights address developers concerns about privacy, enabling the deployment of localized code assistants. With the advancing code capabilities of LLMs, effectively evaluating performance on code-related tasks has emerged as challenge. Popular code-related benchmarks typically focus on self-contained function snippets, relying on limited number of test cases to verify code correctness, such as HumanEval (Chen et al., 2021a), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2024). While recent efforts have expanded the scope of test cases (Liu et al., 2023), tasks (Lai et al., 2022) and programming languages (Chai et al., 2024; Kwiatkowski et al., 2019), these benchmarks remain constrained to validating the correctness of generated code snippets. However, ChatBot Arena (Chiang et al., 2024) has demonstrated that alignment between modelgenerated responses and user preferences is also critical evaluation criterion. As shown in Figure 1, Qwen2.5-Coder primarily generates alone code snippets, while Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference. Therefore, there is an urgent need to establish human preference benchmark specifically for code-related tasks, enabling the community to evaluate and track the alignment between human preferences and modelgenerated responses in real-world scenarios. Furthermore, effective data for improving the human preference alignment of codeLLMs remains scarce. Achieving robust alignment across diverse coding tasks poses significant challenges, particularly in terms of the quantity and quality of data required during the supervised fine-tuning (SFT) stage. To this end, we first introduce comprehensive human-curated benchmark, CodeArena, comprising 397 high-quality samples across 40 categories derived from real-world user queries. Additionally, we develop diverse synthetic instruction corpus, SynCode-Instruct, containing nearly 20 billion tokens, by scaling instructions from web sources. Our extensive evaluation of over nearly 40 large language models (LLMs) using CodeArena reveals significant performance differences between code-execution-based benchmarks and our humancurated benchmark. Notably, we observe substantial performance gap between open-source code LLMs (such as Qwen-Coder) and closed-source LLMs (like the o1 and Claude series), emphasizing the critical role of aligning AI models with human preferences in coding tasks. The contributions are summarized as follows: (1) We propose CodeArena comprised of 397 manually annotated samples, comprehensive code evaluation benchmark for evaluating the alignment between the model-generated response and human preference, encompassing 7 major categories and 40 subcategories. (2) We introduce SynCodeInstruct, the large-scale synthetic code instruction corpora from the website. Based on SynCodeInstruct, an effective coder Qwen2.5-SynCoder is used as strong baseline for CodeArena. (3) We systematically evaluate 40+ LLMs on CodeArena and create leaderboard to dynamically update the results. Notably, extensive experiments suggest that CodeArena can effectively measure the alignment between the model-generated response and human preference.",
                "summary": "<p>–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4 –∏ Claude, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞. –≠—Ç–æ —Å–¥–µ–ª–∞–ª–æ LLM –≤–∞–∂–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è. –í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∫–æ–¥, —Ç–∞–∫–∏–µ –∫–∞–∫ StarCoder, DeepSeekCoder –∏ QwenCoder, –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ç–æ–ø–æ–≤—ã–º –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–º –º–æ–¥–µ–ª—è–º –≤ –±–∞–∑–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Ö –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å –∏ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å —Å–Ω–∏–º–∞—é—Ç –æ–ø–∞—Å–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –ø–æ –ø–æ–≤–æ–¥—É –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏, –ø–æ–∑–≤–æ–ª—è—è —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º.</p>\n<p>–° —Ä–æ—Å—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–¥–∞, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö —Å—Ç–∞–ª–∞ –≤–∞–∂–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∞ –æ–±—ã—á–Ω–æ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è –Ω–∞ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: HumanEval, MBPP –∏ BigCodeBench. –•–æ—Ç—è –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –ø–æ–ø—ã—Ç–∫–∏ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ—Ö–≤–∞—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, –∑–∞–¥–∞—á –∏ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —ç—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.</p>\n<p>–û–¥–Ω–∞–∫–æ, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–ª ChatBot Arena, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏–µ–º –æ—Ü–µ–Ω–∫–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª—å Qwen2.5-Coder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Claude3.5 –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏, —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –∫ –∫–æ–¥—É, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –±–æ–ª–µ–µ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ—Å—Ç—Ä–∞—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ–¥–æ–º. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤—É –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è LLM –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –≤ –¥–µ—Ñ–∏—Ü–∏—Ç–µ. –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT).</p>\n<p>–í —Å–≤—è–∑–∏ —Å —ç—Ç–∏–º, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç <strong>CodeArena</strong> ‚Äì —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–π –ª—é–¥—å–º–∏ –±–µ–Ω—á–º–∞—Ä–∫, –≤–∫–ª—é—á–∞—é—â–∏–π 397 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ 40 –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¢–∞–∫–∂–µ –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ <strong>SynCode-Instruct</strong> ‚Äì –æ–±—à–∏—Ä–Ω—ã–π –∫–æ—Ä–ø—É—Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –ø–æ—á—Ç–∏ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ø—É—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.</p>\n<p>–û–±—à–∏—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–æ–ª–µ–µ —á–µ–º 40 –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CodeArena –≤—ã—è–≤–∏–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞, –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–º, —Å–æ–∑–¥–∞–Ω–Ω–æ–º –ª—é–¥—å–º–∏. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ Qwen-Coder) –∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Å–µ—Ä–∏–∏ o1 –∏ Claude), —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–æ–¥–µ–ª–µ–π –ò–ò –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã:</p>\n<ol>\n<li>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω <strong>CodeArena</strong>, –≤–∫–ª—é—á–∞—é—â–∏–π 397 –≤—Ä—É—á–Ω—É—é –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, ‚Äì –≤—Å–µ–æ–±—ä–µ–º–ª—é—â–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π 7 –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏ 40 –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π.</li>\n<li>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω <strong>SynCodeInstruct</strong> ‚Äì –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–π –∫–æ—Ä–ø—É—Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ SynCodeInstruct —Å–æ–∑–¥–∞–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å Qwen2.5-SynCoder, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–ª—å–Ω–æ–π –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è CodeArena.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –±–æ–ª–µ–µ 40 LLM –Ω–∞ CodeArena –∏ —Å–æ–∑–¥–∞–Ω –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ–º—ã–π –ª–∏–¥–µ—Ä–±–æ—Ä–¥. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ CodeArena —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–∑–º–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</li>\n</ol>"
            },
            {
                "title": "Length question",
                "content": "- maximum length - minimum length - avg length Baseline Answer - maximum length - minimum length - avg length 397 45 131 91 39 62 22 7 97/173/132 6736 tokens 5 tokens 291 tokens 5913 tokens 7 tokens 4517 tokens Table 1: CodeArena dataset statistics. Multiple Programming Languages Figure 3 plots the distribution of programming languages, where we strive to cover common programming languages in CodeArena. Unlike previous studies (Cassano et al., 2023), our benchmarks emphasize diverse range of programming languages that are commonly used in everyday programming tasks. For instance, we have incorporated languages like Google Apps Script (GAS) and PowerShell in CodeArena to better address the needs of practical Q&A scenarios. Evaluation Inspired by the previous work (Chiang et al., 2024), we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games compare and and compare and (avoid the relative position of and affecting the results) to calculate the win rate of compared to the baseline B. Decontainmation. To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPLE (Cassano et al., 2023), MBPP (Austin et al., 2021), McEval (Chen et al., 2021a), and NaturalCodeBench (Zhang et al., 2024). Comparison with other benchmarks We compare CodeArena with other code benchmarks. Our benchmark provides valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios. CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",
                "summary": "<p><strong>–û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö CodeArena –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏</strong></p>\n<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö CodeArena, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º. –í —Ç–∞–±–ª–∏—Ü–µ 1 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è, –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏ —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∫–æ–¥–∞. –í–∏–¥–Ω–æ, —á—Ç–æ –¥–ª–∏–Ω–∞ –∫–æ–¥–∞ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö.</p>\n<p>–ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –≤ CodeArena.  –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π,  –≤ —ç—Ç–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å–¥–µ–ª–∞–Ω –∞–∫—Ü–µ–Ω—Ç –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —è–∑—ã–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ.  –ù–∞–ø—Ä–∏–º–µ—Ä,  –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã —Ç–∞–∫–∏–µ —è–∑—ã–∫–∏, –∫–∞–∫ Google Apps Script (GAS) –∏ PowerShell, —á—Ç–æ–±—ã –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPT-4o-2024-08-06 –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞—Ä–±–∏—Ç—Ä–∞.  –ú–µ—Ç–æ–¥–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ —Å –±–∞–∑–æ–≤—ã–º —Ä–µ—à–µ–Ω–∏–µ–º. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.  –í –∏—Ç–æ–≥–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞–¥ –±–∞–∑–æ–≤–æ–π.</p>\n<p>–ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –≤ CodeArena –±—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –¥–µ–∫–æ–Ω—Ç–∞–º–∏–Ω–∞—Ü–∏—è.  –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –≤—Å–µ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ 10-–≥—Ä–∞–º–º–æ–≤–æ–≥–æ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤) —Å –¥—Ä—É–≥–∏–º–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ MultiPLE, MBPP, McEval –∏ NaturalCodeBench. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á –≤ CodeArena.</p>\n<p>–¢–∞–∫–∂–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ CodeArena —Å –¥—Ä—É–≥–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–æ–¥–∞.  CodeArena –≤—ã–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–º, —á—Ç–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—é—â–∏–π 40 –ø–æ–¥–∑–∞–¥–∞—á –∏ 44 —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–µ–π –≤ —É—Å–ª–æ–≤–∏—è—Ö, –±–æ–ª–µ–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã—Ö –∫ —Ä–µ–∞–ª—å–Ω—ã–º.  –ú–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ –≤ CodeArena –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.</p>"
            },
            {
                "title": "SynCode-Instruct",
                "content": "Recall from Common Crawl. trained fasttext is used to distinguish the code-related text and other common raw text, which is used to recall and clean potential code data and filter out low-quality content using weak model-based classifiers and scorers. Our approach encompasses both file-level and repository-level pertaining to ensure comprehensive coverage. Code Classification for Code Snippet. We extract the first layer of CodeBERT (Feng et al., 2020) and fine-tune the tiny classifier on nearly 100 programming languages to build language identification model. We keep the main language data (e.g. C, Python, and Java) and downsample highresource language data (e.g. HTML and Java) to keep the balance. Besides, we also remove the samples with no code snippets. Scaling Code Instruction Initially, we adopt rule-based filtering to clean pre-extracted content from recalled documents by removing site information, advertisements, and HTML tags, thereby significantly reducing document length for further processing. Different from the previous work (Yue et al., 2024), we utilize Qwen2.5-72B to create new questions instead of extracting question and Figure 2: Task types of CodeArena. Difficulty levels of CodeArena Figure 4 illustrates the difficulty levels of CodeArena, where all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting significant challenge to LLMs. Human Annotation & Quality Control To make CodeArena comprehensive evaluation benchmark, we implement rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check. All annotators participate in annotation tutorial and learn the annotation guidelines. The annotation process involved creating new question based on the given question, checking the difficulty level (easy/medium/hard) based on the complexity of the prompt, and annotating the corresponding programming languages. Following the classification in Figure 2, we uniformly sample 2K samples and assign them to annotators. The annotators select 822 suitable original samples to create queries. The process includes regular quality checks and feedback sessions to maintain high standards throughout the annotation phase, which results in diverse and well-curated dataset spanning multiple programming languages and tasks, suitable for evaluating and improving alignment between the human preference and model-generated response. The other four senior programming developers vote on the same issue to determine whether it is valid and can be resolved. Finally, 397 samples are kept (at least 3 checkers reach consensus) to from CodeArena, considering the cost of the LLM-as-a-judge. Figure 3: Statistics of programming languages in CodeArena. Figure 4: Number of samples of different difficulties (Easy/Medium/Hard) across categories in CodeArena.",
                "summary": "<h2>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π</h2>\n<p>–î–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è Common Crawl. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å –º–æ–¥–µ–ª—å fasttext, –æ–±—É—á–µ–Ω–Ω–∞—è –æ—Ç–ª–∏—á–∞—Ç—å —Ç–µ–∫—Å—Ç, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –∫–æ–¥–æ–º, –æ—Ç –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –ø–æ–º–æ–≥–∞–ª–æ –æ—Ç–±–∏—Ä–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–¥–æ–º –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—Ç—å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é —Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ —Å–∫–æ—Ä–µ—Ä–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –∫–∞–∫ –∫ —Ñ–∞–π–ª–∞–º, —Ç–∞–∫ –∏ –∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è–º, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ.</p>\n<p>–î–∞–ª–µ–µ, –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞, –∏–∑–≤–ª–µ–∫–∞–ª—Å—è –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏ CodeBERT –∏ –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ –¥–æ–æ–±—É—á–∞–ª—Å—è –Ω–µ–±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ—á—Ç–∏ 100 —è–∑—ã–∫–æ–≤. –û—Å–Ω–æ–≤–Ω—ã–µ —è–∑—ã–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, C, Python –∏ Java) –±—ã–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, –∞ –¥–∞–Ω–Ω—ã–µ –ø–æ —è–∑—ã–∫–∞–º —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, HTML –∏ Java) –±—ã–ª–∏ —Å–æ–∫—Ä–∞—â–µ–Ω—ã –¥–ª—è –±–∞–ª–∞–Ω—Å–∞. –¢–∞–∫–∂–µ –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –æ–±—Ä–∞–∑—Ü—ã –±–µ–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.</p>\n<p>–î–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –ø–æ –∫–æ–¥—É, –Ω–∞ –Ω–∞—á–∞–ª—å–Ω–æ–º —ç—Ç–∞–ø–µ –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. –£–¥–∞–ª—è–ª–∞—Å—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–π—Ç–∞—Ö, —Ä–µ–∫–ª–∞–º–∞ –∏ HTML-—Ç–µ–≥–∏, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–ª–æ –¥–ª–∏–Ω—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç, –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å Qwen2.5-72B, –∞ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<h2>–†–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞</h2>\n<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ CodeArena, –±—ã–ª —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏. –í –Ω–µ–º —É—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏ 4 —à—Ç–∞—Ç–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞, –≤–ª–∞–¥–µ—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∏ 4 –æ–ø—ã—Ç–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –í—Å–µ —Ä–∞–∑–º–µ—Ç—á–∏–∫–∏ –ø—Ä–æ—à–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.</p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–º–µ—Ç–∫–∏ –≤–∫–ª—é—á–∞–ª —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (–ª–µ–≥–∫–∏–π, —Å—Ä–µ–¥–Ω–∏–π, —Å–ª–æ–∂–Ω—ã–π) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ë—ã–ª–æ –≤—ã–±—Ä–∞–Ω–æ 2000 –æ–±—Ä–∞–∑—Ü–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ç—á–∏–∫–∞–º–∏. –†–∞–∑–º–µ—Ç—á–∏–∫–∏ –≤—ã–±—Ä–∞–ª–∏ 822 –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤. –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–ª —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å.</p>\n<p>–ó–∞—Ç–µ–º –¥—Ä—É–≥–∏–µ —á–µ—Ç—ã—Ä–µ –æ–ø—ã—Ç–Ω—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≥–æ–ª–æ—Å–æ–≤–∞–ª–∏ –∑–∞ –∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∏ –º–æ–∂–µ—Ç –ª–∏ –±—ã—Ç—å —Ä–µ—à–µ–Ω. –í –∏—Ç–æ–≥–µ, –¥–ª—è CodeArena –±—ã–ª–æ –æ—Ç–æ–±—Ä–∞–Ω–æ 397 –æ–±—Ä–∞–∑—Ü–æ–≤ (–ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏ —Å–æ–≥–ª–∞—Å–∏—è –Ω–µ –º–µ–Ω–µ–µ 3 –ø—Ä–æ–≤–µ—Ä—è—é—â–∏—Ö). –≠—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ —Å —É—á–µ—Ç–æ–º —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏.</p>\n<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª –ø–æ–ª—É—á–µ–Ω —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –∏ —Ö–æ—Ä–æ—à–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∑–∞–¥–∞—á, –ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—å—é.</p>"
            },
            {
                "title": "Human preference",
                "content": "Table 2: Comparison between CodeArena and other benchmarks. CodeArena provides comprehensive view by creating diverse user prompts to evaluation alignment between the model-generated response and human preference. answer pairs. As shown in Figure 6. We use the Qwen2.5-Coder to generate multiple responses by sampling for the same document. For the algorithmic generated question and answer, we first adopt fine-tuned generator to generate the test cases and adopt the multilingual sandbox to verify the correctness of the generated code snippet. As shown in Figure 5, for the non-algorithmic query, we first randomly generate four candidates (Best-of-N) and use the LLM to score the candidates (LLM scorer), where the candidates are fed into the LLM to select the best response with the reason. For the algorithmic queries, the generated test cases by LLM are used to verify the correctness of the responses (Executor). Finally, we select the response with the best score as the response to create SynCodeInstruct. The synthetic instruction corpora generated by Qwen2.5 is used for the first stage and the high-quality data from GPT-4o is used for the second stage.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö SynCodeInstruct –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —É–Ω–∏–∫–∞–ª–µ–Ω —Ç–µ–º, —á—Ç–æ –æ–Ω –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ –æ—Ü–µ–Ω–∫—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—å—é, –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ —á–µ–ª–æ–≤–µ–∫–∞. </p>\n<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç—Ç–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã:</p>\n<p><strong>1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤:</strong> \n   - –î–ª—è –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ Qwen2.5-Coder –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞. –≠—Ç–æ –¥–µ–ª–∞–ª–æ—Å—å –ø—É—Ç–µ–º \"—Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è\", —Ç–æ –µ—Å—Ç—å —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –∏–∑ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.</p>\n<p><strong>2. –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤:</strong>\n   - <strong>–ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã:</strong> –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∫–æ–¥–∞, —Å–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª—É—á–∞–∏ —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –ø—Ä–æ–≤–µ—Ä—è–ª—Å—è –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π \"–ø–µ—Å–æ—á–Ω–∏—Ü–µ\" (sandbox).\n   - <strong>–ù–µ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã:</strong> –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å —á–µ—Ç—ã—Ä–µ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–∞. –ó–∞—Ç–µ–º –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM) –æ—Ü–µ–Ω–∏–≤–∞–ª–∞ —ç—Ç–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –≤—ã–±–∏—Ä–∞—è –ª—É—á—à–∏–π –∏ –æ–±—ä—è—Å–Ω—è—è —Å–≤–æ–π –≤—ã–±–æ—Ä. –≠—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"Best-of-N\" –ø–æ–¥—Ö–æ–¥–æ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM-—Å–∫–æ—Ä–µ—Ä–∞.</p>\n<p><strong>3. –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞:</strong>\n   - –î–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—ã–±–∏—Ä–∞–ª—Å—è –æ—Ç–≤–µ—Ç, —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–µ–¥—à–∏–π –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Å–ª—É—á–∞—è–º–∏ (Executor).\n   - –î–ª—è –Ω–µ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—ã–±–∏—Ä–∞–ª—Å—è –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–∏–≤—à–∏–π –Ω–∞–∏–≤—ã—Å—à—É—é –æ—Ü–µ–Ω–∫—É –æ—Ç LLM-—Å–∫–æ—Ä–µ—Ä–∞.</p>\n<p><strong>4. –°–æ–∑–¥–∞–Ω–∏–µ SynCodeInstruct:</strong>\n   - –í—ã–±—Ä–∞–Ω–Ω—ã–µ –ª—É—á—à–∏–µ –æ—Ç–≤–µ—Ç—ã –≤–º–µ—Å—Ç–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SynCodeInstruct.</p>\n<p><strong>5. –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥:</strong>\n   - –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å Qwen2.5.\n   - –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é GPT-4o.</p>\n<p><strong>–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:</strong>\n- <strong>–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤:</strong> CodeArena —Å–æ–∑–¥–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.\n- <strong>–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:</strong> –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö.\n- <strong>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:</strong> –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–Ω–∞—á–∞–ª–∞ –º–µ–Ω–µ–µ –º–æ—â–Ω–æ–π, –∞ –∑–∞—Ç–µ–º –±–æ–ª–µ–µ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, SynCodeInstruct - —ç—Ç–æ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞.</p>"
            },
            {
                "title": "Instruction dataset",
                "content": "CodeLLMs We evaluate 40+ models with sizes ranging from 0.5B to 200B parameters, including general/code LLMs and open/closed-source models. For general models, we evaluate GPTs (Brown et al., 2020; OpenAI, 2023) (GPT-3.5-Turbo, GPT4-o), Qwen series (Qwen2.5 and QwenMax) (Bai et al., 2023), Claude series (Anthropic, 2023), Llama3/3.1 (Dubey et al., 2024), Yi (Young et al., 2024), and o1 series. For code models, Figure 5: Overview of the CodeArena creation benchmark. We first collect the online code Q&A and code-related raw text from the website. We cluster the code-related data and classify them into different categories using LLM. We uniformly sample the samples from different subtasks as the seed data for manual annotation. et al., 2021) to test the code generation capabilities. The benchmark reports the scores of HumanEval (HE)/MBPP with base test cases and HumanEval+ (HE+)/MBPP+ with plus test cases. MultiPL-E The MultiPL-E test set (Cassano et al., 2023) contains the HumanEval (Python) and translated test set of other programming languages, i.e., Java, C++, Javascript, and Typescript. CodeArena Different from the EvalPlus and MultiPL-E, CodeArena consists of many nonalgorihtmic, which is not suitable for codeexecution-based evaluation. Each question is scored twice to calculate the win rate and tie rate by GPT-4o using different input order A, and B, A, where is the baseline from gpt-4-turbo-2024-04-09 and is the model-generated response.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –±–æ–ª–µ–µ 40 –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ –æ—Ç 0.5 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 200 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í –≤—ã–±–æ—Ä–∫—É –≤—Ö–æ–¥—è—Ç –∫–∞–∫ –æ–±—â–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏ –∑–∞–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º.</p>\n<p>–°—Ä–µ–¥–∏ –æ–±—â–∏—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è:</p>\n<ul>\n<li>–ú–æ–¥–µ–ª–∏ GPT (–≤–∫–ª—é—á–∞—è GPT-3.5-Turbo –∏ GPT4-o)</li>\n<li>–°–µ—Ä–∏—è Qwen (Qwen2.5 –∏ QwenMax)</li>\n<li>–°–µ—Ä–∏—è Claude</li>\n<li>Llama3/3.1</li>\n<li>Yi</li>\n<li>–°–µ—Ä–∏—è o1</li>\n</ul>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:</p>\n<ul>\n<li><strong>HumanEval (HE) –∏ MBPP:</strong> –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å –±–∞–∑–æ–≤—ã–º–∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Å–ª—É—á–∞—è–º–∏, –∞ —Ç–∞–∫–∂–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Å–ª—É—á–∞—è–º–∏ (HE+ –∏ MBPP+). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª–∏ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π.</li>\n<li><strong>MultiPL-E:</strong> –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å—à–∏—Ä—è–µ—Ç HumanEval, –¥–æ–±–∞–≤–ª—è—è –ø–µ—Ä–µ–≤–æ–¥—ã —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤ –Ω–∞ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Java, C++, Javascript –∏ Typescript. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</li>\n</ul>\n<p>–¢–∞–∫–∂–µ –≤–≤–æ–¥–∏—Ç—Å—è –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö <strong>CodeArena</strong>, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç EvalPlus –∏ MultiPL-E —Ç–µ–º, —á—Ç–æ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –Ω–µ–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤, —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ GPT-4o. –û—Ç–≤–µ—Ç—ã —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –≤ –¥–≤—É—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö: A –ø—Ä–æ—Ç–∏–≤ B –∏ B –ø—Ä–æ—Ç–∏–≤ A, –≥–¥–µ A - —ç—Ç–æ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ gpt-4-turbo-2024-04-09, –∞ B - –æ—Ç–≤–µ—Ç —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–π –º–æ–¥–µ–ª–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–∏—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥ –∏ –Ω–∏—á—å–∏—Ö. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö, –≥–¥–µ –Ω–µ—Ç –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.</p>"
            },
            {
                "title": "Evaluation benchmark",
                "content": "EvalPlus and MultiPL-E. The EvalPlus (Liu et al., 2023) is an upgraded version of the HumanEval (Chen et al., 2021a) and MBPP (Austin LLM as judgement Due to the high cost of collecting human preferences (Zheng et al., 2023a), we use pairwise comparison for judgment, where an LLM judger is fed with question and two answers and determines which one is better or Model Size UI&UX Development& Programming Specialized Computing Tools, Environs, & Practices Emerging Techs &Apps Miscellaneous & General Inquiry Databases& Data Handling Avg. Claude-3.5-Sonnet-20240620 Claude-3.5-Sonnet-20241022 GPT-3.5-turbo-0125 GPT-4o-mini-2024-07-18 GPT-4o-2024-08-06 o1-mini o1-preview Yi-lightning Doubao-Pro Qwen-Max Proprietary LLMs and 200B+ LLMs (cid:181) 88.9/2.2 (cid:181) 82.2/6.7 (cid:181) 17.8/24.4 (cid:181) 71.1/13.3 (cid:181) 66.7/17.8 (cid:181) 93.3/4.4 (cid:181) 93.3/2.2 (cid:181) 62.2/15.6 (cid:181) 51.1/20.0 (cid:181) 75.6/17.8 77.3/13.6 75.8/12.9 11.4/20.5 62.1/17.4 72.7/19.7 94.7/2.6 81.8/7.6 60.0/11.5 40.8/18.5 74.2/13.6 74.2/18.0 76.4/16.9 4.5/19.1 50.0/13.6 62.9/19.1 84.1/7.6 85.4/7.9 57.9/5.3 55.3/26.3 59.6/24.7 81.4/11.9 84.7/10.2 11.9/18.6 65.2/14.6 69.5/15.3 91.0/5.6 78.0/6.8 49.4/16.9 38.2/19.1 78.0/6.8 0.5B+ Open-source LLMs Qwen2.5-0.5B-Instruct Qwen2.5-Coder-0.5B-Instruct 0.5B 0.5B 2.2/4.4 2.2/2.2 4.6/4.6 4.6/6.9 5.3/10.5 2.6/5.3 2.2/4.5 4.5/2. DS-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat Qwen2.5-Coder-1.5B-Instruct OpenCoder-1.5B-Instruct 1.3B 1.5B 1.5B 1.5B 66.7/2.2 11.1/2.2 11.1/4.4 11.1/4.4 2.3/5.4 5.1/3.4 15.9/9.1 3.8/5.4 1B+ Open-source LLMs 2.6/10.5 5.4/4.6 9.0/16.9 0.0/5. 1.7/6.8 2.6/5.3 13.6/11.9 2.2/4.5 3B+ Open-source LLMs 78.9/10.5 84.2/13.2 10.5/21.1 72.9/13.6 76.3/13.2 88.1/3.4 92.1/2.6 71.2/11.9 47.5/22.0 68.4/23.7 3.4/5.1 3.4/5.1 0.0/9.1 2.2/5.6 13.2/5.3 3.4/8.5 71.4/28.6 57.1/28.6 13.6/9.1 71.1/18.4 85.7/14.3 95.5/0.0 77.3/4.5 54.5/13.6 36.4/31.8 100.0/0. 63.6/4.5 68.2/22.7 0.0/14.3 71.4/14.3 59.1/22.7 100.0/0.0 71.4/28.6 85.7/0.0 42.9/57.1 81.8/4.5 77.8/12.5 78.1/13.5 10.5/19.6 65.8/15.6 69.1/18.1 89.3/5.1 83.9/6.6 59.5/12.6 43.6/21.5 71.9/15.8 4.5/9.1 4.5/0.0 0.0/14.3 28.6/14.3 3.6/5.6 4.4/4.6 2.2/3.4 4.5/4.5 14.3/42.9 4.5/9. 0.0/14.3 14.3/14.3 18.2/4.5 0.0/0.0 2.6/5.6 7.4/5.1 13.2/10.7 6.7/3.8 Qwen2.5-Coder-3B-Instruct 3B 35.6/11.1 29.5/10.6 27.0/15. 20.3/18.6 28.9/10.5 42.9/14.3 27.3/13.6 28.3/13.3 6B+ Open-source Models CodeLlama-7B-Instruct Llama3-8B-Instruct Llama3.1-8B-Instruct DS-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat DS-Coder-V2-Lite-Instruct Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct 7B 33.3/8.9 7B 20.0/17.8 7B 2.2/8.9 6.7B 11.1/17.8 7B 17.8/15.6 9B 15.6/17.8 2.4/16B 42.2/20.0 7B 40.0/22.2 24.4/8.9 8B 28.8/18.6 14.6/11.5 4.5/10.1 13.1/13.8 13.8/12.3 15.4/9.2 33.3/17.4 46.2/19.7 14.6/8.5 23.8/13.8 15.8/2.6 3.8/6.2 13.6/8.5 15.8/0.0 15.8/7.9 31.5/16.9 43.8/15.7 10.5/7. 18.2/9.1 13.5/9.0 3.4/6.8 13.2/7.9 15.7/9.0 13.5/13.5 35.6/20.3 40.7/20.3 9.0/4.5 13B+ Models CodeLlama-13B-Instruct Starcoder2-15B-Instruct-v0.1 Qwen2.5-Coder-14B-Instruct 13.3/4.4 6.7/6.7 13B 15B 14B 51.1/24.4 7.9/6.7 6.8/12.9 53.0/17. 6.8/8.5 4.5/15.7 52.8/16.9 7.7/6.2 6.8/6.8 50.8/18.6 20B+ Models CodeLlama-34B-Instruct CodeStral-22B-v0.1 DS-Coder-33B-Instruct CodeLlama-70B-Instruct DS-Coder-V2-Instruct DS-V2.5 Llama3-70B-Instruct Llama3.1-70B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-32B-Instruct QwQ-32B-Preview Qwen2.5-72B-Instruct Qwen2.5-SynCoder 34B 11.1/6.7 22B 17.8/22.2 33B 13.3/11.1 70B 11.1/22.2 21/236B 55.6/11.1 21/236B 77.8/11.1 7B 35.6/20.0 7B 48.9/24.4 32B 71.1/13.3 32B 62.2/15.6 32B 53.3/15.6 72B 82.2/6.7 32B 55.6/26.7 2.6/2.6 27.3/13.6 22.0/9.8 9.2/10.0 62.1/18.2 72.0/12.9 26.2/26.2 43.8/20.0 66.7/15.9 52.3/15.4 56.8/16.7 71.5/14.6 49.2/20. 6.9/2.3 14.6/14.6 12.4/12.4 10.5/5.3 60.7/14.6 71.9/13.5 25.4/22.0 34.2/26.3 67.4/16.9 57.9/18.4 50.6/16.9 76.3/13.2 36.8/36.8 8.5/6.8 25.4/10.2 13.6/6.8 9.0/6.7 50.8/18.6 71.2/8.5 34.2/15.8 40.4/22.5 74.6/13.6 50.6/23.6 64.4/5.1 75.3/15.7 50.6/20.2 31.6/5.3 16.9/11.9 5.3/2.6 9.0/7.9 15.3/15.3 10.2/20.3 39.5/21.1 34.2/15.8 13.6/6.8 4.5/4.5 5.3/13.2 57.9/7.9 7.9/10.1 18.4/10.5 13.2/18.4 16.9/8.5 52.6/21.1 73.7/10.5 23.6/14.6 54.2/20.3 65.8/18.4 54.2/13.6 52.6/21.1 71.2/18.6 52.5/20.3 29.2/14.6 22.7/0.0 9.1/9.1 13.6/4.5 18.2/13.6 18.2/13.6 71.4/14.3 71.4/0.0 18.2/9. 5.3/5.3 13.6/13.6 28.6/28.6 9.1/9.1 14.3/42.9 28.6/42.9 9.1/13.6 71.4/14.3 100.0/0.0 36.4/4.5 45.5/9.1 100.0/0.0 50.0/13.6 85.7/0.0 63.6/13.6 40.9/18.2 71.4/0.0 57.1/14.3 14.3/0.0 28.6/0.0 14.3/42.9 28.6/28.6 31.8/22.7 40.9/22.7 14.3/0.0 28.2/12.8 16.7/10.3 7.9/4.4 12.3/10.8 15.4/11.8 14.6/13.3 35.5/18.6 43.1/18.6 14.1/7.1 14.3/14.3 0.0/14.3 36.4/27.3 11.2/7.9 6.4/12.0 60.6/51. 14.3/0.0 22.7/22.7 22.7/18.2 0.0/0.0 40.9/31.8 68.2/13.6 14.3/57.1 71.4/14.3 63.6/18.2 71.4/14.3 63.6/9.1 85.7/14.3 57.1/0.0 7.7/5.6 21.7/15.8 16.8/12.0 15.5/10.5 57.4/17.6 73.0/11.7 27.7/20.5 44.9/21.0 68.9/15.6 54.1/17.1 56.6/14.5 73.8/14.4 49.2/22.3 Table 3: The win/tie rate of different instruction LLMs on CodeArena. The underlined numbers represent the best scores within the same model size range. declares tie2. We report win rate/tie rate for CodeArena. 4.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –Ω–∞ –Ω–∞–±–æ—Ä–µ –∑–∞–¥–∞—á CodeArena, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥–∏–∫–∞ –ø–∞—Ä–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –≥–¥–µ LLM-—Å—É–¥—å—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π –∏–∑ –¥–≤—É—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –∑–∞–¥–∞—á—É —è–≤–ª—è–µ—Ç—Å—è –ª—É—á—à–∏–º –∏–ª–∏ –æ–±—ä—è–≤–ª—è–µ—Ç –Ω–∏—á—å—é.</p>\n<p>–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö EvalPlus, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π HumanEval –∏ MBPP. EvalPlus —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –æ—Ç –º–æ–¥–µ–ª–µ–π –Ω–µ —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –Ω–æ –∏ –µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –æ—Ç–ª–∞–¥–∫–∏.</p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ 3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–∞–∑–º–µ—Ä—É (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —É–∫–∞–∑–∞–Ω –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–±–µ–¥ –∏ –Ω–∏—á—å–∏—Ö (win rate/tie rate) –Ω–∞ –∑–∞–¥–∞—á–∞—Ö CodeArena. –ü–æ–¥—á–µ—Ä–∫–Ω—É—Ç—ã –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ –º–æ–¥–µ–ª–µ–π –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞.</p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, Claude, GPT), —Ç–∞–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (–æ—Ç 0.5B –¥–æ 70B+ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∏ –º–∞—Å—à—Ç–∞–±–æ–≤.</p>\n<p>–í —Ü–µ–ª–æ–º, —Ç–∞–±–ª–∏—Ü–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –Ω–æ —Ç–∞–∫–∂–µ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ—Å—Ç–∏–≥–∞—é—Ç —Å—Ä–∞–≤–Ω–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –∑–∞–¥–∞—á. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –Ω–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä, –≤–ª–∏—è—é—â–∏–π –Ω–∞ –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>"
            },
            {
                "title": "Main results",
                "content": "CodeArena. Table 3 shows that the win rate/tie rate of different instruction LLM on CodeArena. The closed-source LLMs such as Claude and o1 series still get dominant advantage compared to Qwen2.5-Coder and DeepseekCoder. There still exists notable performance gap between open codeLLMs (e.g. Qwen-Coder) and closed-source LLMs (e.g., o1 and Claude series), emphasizing the Model Size HE HE+ MBPP MBPP+ Python Java C++ C# TS JS PHP Bash Avg. Closed-APIs Claude-3.5-Sonnet-20240620 Claude-3.5-Sonnet-20241022 GPT-4o-mini-2024-07-18 GPT-4o-2024-08-06 o1-mini o1-preview (cid:181) 89.0 (cid:181) 92.1 (cid:181) 87.8 (cid:181) 92.1 (cid:181) 97.6 (cid:181) 95. 81.1 86.0 84.8 86.0 90.2 88.4 87.6 91.0 86.0 86.8 93.9 93.4 72.0 74.6 72.2 72.5 78.3 77.8 89.6 93.9 87.2 90.9 95.7 96.3 86.1 86.7 75.9 83.5 90.5 88.0 82.6 88.2 77.6 76.4 93.8 91. 85.4 87.3 79.7 81.0 77.2 84.2 84.3 88.1 79.2 83.6 91.2 90.6 84.5 91.3 81.4 90.1 92.5 93.8 80.7 82.6 75.2 78.9 84.5 90.1 48.1 52.5 43.7 48.1 55.1 47.5 80.2 83.8 75.0 79.1 85.1 85. 0.5B+ Models Qwen2.5-Coder-0.5B-Instruct 0.5B 61.6 57.3 52.4 43. 61.6 57.3 52.4 43.7 50.3 50. 52.8 27.8 49.6 1B+ Models DS-Coder-1.3B-Instruct Yi-Coder-1.5B-Chat Qwen2.5-Coder-1.5B-Instruct 1.3B 65.9 1.5B 69.5 1.5B 70. 60.4 64.0 66.5 65.3 65.9 69.2 54.8 57.7 59.4 65.2 67.7 71.2 51.9 51.9 55.7 45.3 49.1 50. 55.1 57.6 64.6 59.7 57.9 61.0 52.2 59.6 62.1 45.3 52.2 59.0 12.7 19.0 29.1 48.4 51.9 56. 3B+ Models Qwen2.5-Coder-3B-Instruct 3B 84.1 80.5 73.6 62. 83.5 74.7 68.3 78.5 79.9 75. 73.3 43.0 72.1 CodeLlama-7B-Instruct DS-Coder-6.7B-Instruct CodeQwen1.5-7B-Chat Yi-Coder-9B-Chat DS-Coder-V2-Lite-Instruct Qwen2.5-Coder-7B-Instruct OpenCoder-8B-Instruct 7B 40.9 6.7B 74.4 7B 83.5 9B 82.3 2.4/16B 81.1 7B 88.4 8B 83.5 33.5 71.3 78.7 74.4 75.6 84.1 78. CodeLlama-13B-Instruct Starcoder2-15B-Instruct-v0.1 Qwen2.5-Coder-14B-Instruct 13B 40.2 15B 67.7 14B 89.6 32.3 60.4 87.2 CodeLlama-34B-Instruct CodeStral-22B-v0.1 DS-Coder-33B-Instruct CodeLlama-70B-Instruct DS-Coder-V2-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-SynCoder 34B 48.2 22B 81.1 33B 81.1 70B 72.0 21/236B 85.4 32B 92.7 32B 87.8 32B 85.4 32B 92.7 40.2 73.2 75.0 65.9 82.3 87.2 82.9 79.3 87. 6B+ Models 44.4 65.6 67.2 69.0 70.4 71.7 69.0 13B+ Models 51.1 65.1 72.8 20B+ Models 50.5 62.2 70.1 64.6 75.1 75.1 70.9 77.0 74. 54.0 74.9 77.7 82.0 82.8 83.5 79.1 60.3 78.0 86.2 61.1 78.2 80.4 77.8 89.4 90.2 86.8 90.5 86.2 34.8 78.6 84.1 85.4 81.1 87.8 83.5 42.7 68.9 89.0 41.5 81.1 79.3 67.8 90.2 92.7 88.4 82.9 92. 30.4 68.4 73.4 76.0 76.6 76.5 72.2 31.1 63.4 74.5 67.7 75.8 75.6 61.5 21.6 72.8 77.8 76.6 76.6 80.3 75.9 32.7 67.2 71.7 72.3 80.5 81.8 78.0 - 72.7 75.2 78.9 77.6 83.2 79.5 28.6 68.9 70.8 72.1 74.5 78.3 73. 10.1 36.7 39.2 45.6 43.0 48.7 44.3 40.5 53.8 79.7 42.2 50.9 85.1 24.0 62.7 84.2 39.0 57.9 86.8 - 59.6 84. 32.3 53.4 80.1 13.9 24.7 47.5 43.7 63.3 73.4 58.2 82.3 80.4 80.4 81.0 80.4 45.3 65.2 68.9 53.4 84.8 79.5 81.0 80.7 80.7 31.0 43.7 74.1 36.7 82.3 82.9 74.5 81.6 81.6 40.3 68.6 67.9 39.0 83.0 86.8 83.5 81.1 83. - - 73.9 - 84.5 85.7 82.4 82.0 85.7 36.6 68.9 72.7 58.4 79.5 78.9 78.3 77.0 77.6 19.6 42.4 43.0 29.7 52.5 48.1 46.8 48.7 49.4 - 66.1 70.8 71.8 73.2 76.5 71.0 - 54.0 79.6 - - 69.2 - 79.9 79.4 76.9 75.1 78. Table 4: The performance of different instruction LLMs on EvalPlus and MultiPL-E. HE denotes the HumanEval, HE+ denotes the plus version with more test cases, and MBPP+ denotes the plus version with more test cases. importance of alignment between model-generated response human preference. Qwen2.5-SynCoder totally trained on the large-scale synthetic instruction corpus SynCode-Instruct can still get strong performance on CodeArena, which verifies the correctness of the route of taking large-scale synthetic data to improve model performance. EvalPlus and MultiPL-E. Table 4 shows that Qwen2.5-SynCoder significantly beats previous strong open-source baselines using large-scale synthetic instruction, closing the gap with GPT-4o and Claude, which verifies that the large-scale synthetic data can bring more significant improvement for the base model in the code-execution-based benchmark (code generation) compared to CodeArena.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 3 –ø–æ–∫–∞–∑–∞–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ CodeArena. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–æ–±–µ–¥ –∏ –Ω–∏—á—å–∏—Ö (win rate/tie rate) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>\n<p>–ò–∑ —Ç–∞–±–ª–∏—Ü—ã –≤–∏–¥–Ω–æ, —á—Ç–æ –∑–∞–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Claude –∏ o1, –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ Qwen2.5-Coder –∏ DeepseekCoder. –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen-Coder) –∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, o1 –∏ Claude), —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–∞–±–ª–∏—Ü–µ 4 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö EvalPlus –∏ MultiPL-E.  Qwen2.5-SynCoder, –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö SynCode-Instruct, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ CodeArena, —á—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏.</p>\n<p>–¢–∞–∫–∂–µ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è, —á—Ç–æ Qwen2.5-SynCoder –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–∏–ª—å–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ EvalPlus –∏ MultiPL-E, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ GPT-4o –∏ Claude. –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–æ–ª–µ–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∫–æ–¥–∞ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞), –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å CodeArena.</p>"
            },
            {
                "title": "Discussion",
                "content": "Examples of CodeArena. Figure 7 lists six examples from the different subtasks, covering Python, HTML, CSS, and Java. Different from the previous benchmarks (Cassano et al., 2023; Jain et al., 2024) comprised of algorithmic questions in fixed format, the queries of CodeArena are more consistent with the distribution of user questions in real Q&A scenarios. For example, the query huggingface dataset move all the columns to metadata, except two, problem and solution is closer to the question style of real users. For the baseline response and model-generated response B, the GPT4o thinks beats based on the judgment provides correct and relevant solution using the appropriate library for Hugging Face datasets, which select responses that are more aligned with human preferences. Difference between CodeArena and Executionbased Benchmark. Compared to the benchmark MultiPL-E evaluated by code execution, CodeArena is created from real-world Q&A and evaluated by LLM-as-a-judge to evaluate the alignment between the model-generated response and human preference. For example, the LLMs tend to only generate the code without any natural descripFigure 7: Examples of CodeArena. The LLM judger decides which response is better. Figure 8: Comparison between MultiPL-E and CodeArena. LLMs in the blue circle present relatively mismatched performances on two benchmarks. Figure 9: Results of CodeArena with different data size on MultiPL-E and CodeArena. tion (even the code is correct) will bring an unsatisfactory experience to users, which will also lead to poor performance in CodeArena. In Figure 8, we can observe that the state-of-the-art closed-source LLMs (e.g. o1 and Claude series) get balanced performance between the code execution benchmark and CodeArena. The open-source models (e.g. DeepseekCoder and Qwen-Coder) are likely to bring bad experience to users, where the generated response lacks more detailed explanation or more complete details compared to closed-source LLMs. Scaling Synthetic Instruction Corpora. We would like to further analyze the performance of Qwen2.5-SynCoder in MultiPl-E and CodeArena given different sizes of instruction corpora. Therefore, we select the full instruction (19B synthetic data is at the front of the data and 1B high-quality data is at the end) set SynCode-Instruct and extract the first billion tokens as the fine-tuned data. We set = {2, 4, . . . , 20}. We randomly extract specific data from the whole sentence pairs. Figure 9 shows the performance on CodeArena. With the increase of instruction data, Qwen2.5-SynCoder still can get significant improvement, which emphasizes the importance of the scaling instruction corpora. Besides, the two-stage SFT gets better performance compared to the one-stage training (red line), where the high-quality data brings huge improvement at last. Distribution of different benchmarks. We visualize the queries of CodeArena and MultiPL-E (Python, Java, and CPP) by extracting the encoder representations of the last layer for t-SNE (Van der Maaten and Hinton, 2008). The average of all hidden states of the last encoder layer is regarded as the query representation. In Figure 10, the representations of CodeArena are distributed in the LLMs, numerous benchmarks have been proposed, including code translation (Jiao et al., 2023; Yan et al., 2023; Zhu et al., 2022), code retrieval (Huang et al., 2021; Husain et al., 2019; Lu et al., 2021), code completion (Bavarian et al., 2022; Liu et al., 2024a; Zhang et al., 2023), code debugging (Huq et al., 2022; Tian et al., 2024; Liu et al., 2024b), and structured data understanding (Wu et al., 2024; Su et al., 2024). Recent initiatives such as McEval (Chai et al., 2024) have expanded the evaluative scope to 40 programming languages for multilingual scenarios, while MdEval (Liu et al., 2024b) has developed multilingual code debugging benchmark encompassing nearly 20 programming languages. Nonetheless, many of these studies concentrate on assessing only single aspect of LLM capabilities, often overlooking the evaluation of LLMs as comprehensive program developers across variety of real-world coding scenarios. In this work, we propose FullStack Bench to evaluate the capabilities of LLMs across multiple practical code development contexts.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ CodeArena, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –∫–æ–¥–µ –≤ —Å—Ç–∏–ª–µ, –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–º –∫ —Ä–µ–∞–ª—å–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∑–∞–ø—Ä–æ—Å–∞–º.</p>\n<p><strong>–û—Ç–ª–∏—á–∏—è CodeArena –æ—Ç –¥—Ä—É–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤:</strong></p>\n<ul>\n<li><strong>–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:</strong> –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ–¥–µ—Ä–∂–∞–ª–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, CodeArena –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã, –±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –∑–∞–¥–∞—é—Ç –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –∑–∞–ø—Ä–æ—Å \"huggingface dataset move all the columns to metadata, except two, problem and solution\" (–ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤—Å–µ —Å—Ç–æ–ª–±—Ü—ã –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∫—Ä–æ–º–µ –¥–≤—É—Ö, problem –∏ solution, –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö huggingface) –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</li>\n<li><strong>–û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞:</strong> –í–º–µ—Å—Ç–æ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞, –∫–∞–∫ –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ MultiPL-E, CodeArena –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã LLM —Å –ø–æ–º–æ—â—å—é –¥—Ä—É–≥–æ–π LLM, –≤—ã—Å—Ç—É–ø–∞—é—â–µ–π –≤ —Ä–æ–ª–∏ —Å—É–¥—å–∏. –≠—Ç–∞ –º–æ–¥–µ–ª—å-—Å—É–¥—å—è –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–æ–π –æ—Ç–≤–µ—Ç –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º, —É—á–∏—Ç—ã–≤–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∞, –Ω–æ –∏ –µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –Ω–∞–ª–∏—á–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏–π.</li>\n<li><strong>–ê–∫—Ü–µ–Ω—Ç –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –æ–ø—ã—Ç–µ:</strong> CodeArena —É—á–∏—Ç—ã–≤–∞–µ—Ç, —á—Ç–æ LLM, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–µ —Ç–æ–ª—å–∫–æ –∫–æ–¥ –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –ø–æ—è—Å–Ω–µ–Ω–∏–π (–¥–∞–∂–µ –µ—Å–ª–∏ –∫–æ–¥ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π), –º–æ–≥—É—Ç –Ω–µ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–º –æ—Ü–µ–Ω–∫–∞–º –≤ CodeArena –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞.</li>\n</ul>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å MultiPL-E:</strong></p>\n<ul>\n<li>–ë–µ–Ω—á–º–∞—Ä–∫ MultiPL-E –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞.</li>\n<li>CodeArena –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º, —É—á–∏—Ç—ã–≤–∞—è –∫–∞–∫ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∞, —Ç–∞–∫ –∏ –µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –Ω–∞–ª–∏—á–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏–π.</li>\n<li>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ LLM, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–æ–¥–µ–ª–∏ DeepseekCoder –∏ Qwen-Coder, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MultiPL-E, –Ω–æ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –≤ CodeArena, –ø–æ—Å–∫–æ–ª—å–∫—É –∏—Ö –æ—Ç–≤–µ—Ç—ã —á–∞—Å—Ç–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏–ª–∏ –ø–æ–ª–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π. –ó–∞–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, GPT-4o –∏ Claude) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –æ–±–æ–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.</li>\n</ul>\n<p><strong>–í–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong></p>\n<ul>\n<li>–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen2.5-SynCoder –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CodeArena. –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.</li>\n<li>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (—Å–Ω–∞—á–∞–ª–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞—Ç–µ–º –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö) –¥–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º –æ–¥–Ω–æ—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</li>\n</ul>\n<p><strong>–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤:</strong></p>\n<ul>\n<li>–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ CodeArena –∏ MultiPL-E —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º t-SNE –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã CodeArena —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –∑–∞–ø—Ä–æ—Å—ã MultiPL-E –æ–±—Ä–∞–∑—É—é—Ç –±–æ–ª–µ–µ –≤—ã—Ä–∞–∂–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ CodeArena –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.</li>\n</ul>\n<p><strong>–û–±—â–∞—è –∏–¥–µ—è:</strong></p>\n<p>CodeArena –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π —Å–ø–æ—Å–æ–± –æ—Ü–µ–Ω–∫–∏ LLM –≤ –∑–∞–¥–∞—á–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞, —É—á–∏—Ç—ã–≤–∞—è –∫–∞–∫ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∞, —Ç–∞–∫ –∏ –µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º—É –æ–ø—ã—Ç—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É LLM, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–¥, –Ω–æ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –ø–æ–ª–µ–∑–Ω—ã–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</p>"
            },
            {
                "title": "Conclusion",
                "content": "In this work, We introduce CodeArena, meticulously human-curated benchmark composed of 397 high-quality samples spanning 40 categories, derived from real-world user queries, to address discrepancies between model-generated responses and human preferences in coding tasks. Additionally, we create SynCode-Instruct, diverse synthetic instruction corpus containing nearly 20 billion tokens, by scaling web-sourced instructions. Our evaluation of over 20 large language models (LLMs) using CodeArena highlights significant performance discrepancies between code-executionbased benchmarks and our human-curated benchmark. Notably, there is marked performance gap between open-source code LLMs (such as DeepSeek-Coder) and closed-source LLMs (such as the o1 and Claude series), underscoring the importance of aligning AI models with human preferences in coding tasks.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω CodeArena, —Ç—â–∞—Ç–µ–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –ª—é–¥—å–º–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ 397 –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 40 –∫–∞—Ç–µ–≥–æ—Ä–∏–π. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π –º–µ–∂–¥—É –æ—Ç–≤–µ—Ç–∞–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –ª—é–¥–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å–æ–∑–¥–∞–Ω SynCode-Instruct, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –∫–æ—Ä–ø—É—Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –æ–∫–æ–ª–æ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç –∫–æ—Ä–ø—É—Å –±—ã–ª –ø–æ–ª—É—á–µ–Ω –ø—É—Ç–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.</p>\n<p>–û—Ü–µ–Ω–∫–∞ –±–æ–ª–µ–µ 20 –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CodeArena –≤—ã—è–≤–∏–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —ç—Ç–∞–ª–æ–Ω–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞, –∏ –Ω–∞—à–∏–º —ç—Ç–∞–ª–æ–Ω–æ–º, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º –ª—é–¥—å–º–∏. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É LLM —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ DeepSeek-Coder) –∏ LLM —Å –∑–∞–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ —Å–µ—Ä–∏–∏ o1 –∏ Claude). –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –ª—é–¥–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>"
            }
        ]
    },
    {
        "id": "2308.11432",
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "url": "https://arxiv.org/pdf/2308.11432",
        "abstract": "Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2023-08-22",
        "pub_date_card": {
            "ru": "22 –∞–≤–≥—É—Å—Ç–∞",
            "en": "August 22",
            "zh": "8Êúà22Êó•"
        },
        "hash": "fc293a9f6feeb7b9",
        "authors": [
            "Lei Wang",
            "Chen Ma",
            "Xueyang Feng",
            "Zeyu Zhang",
            "Hao Yang",
            "Jingsen Zhang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Xu Chen",
            "Yankai Lin",
            "Wayne Xin Zhao",
            "Zhewei Wei",
            "Ji-Rong Wen"
        ],
        "affiliations": [
            "Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, 100872, China"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2308.11432.jpg",
        "data": {
            "categories": [
                "#agents",
                "#agi",
                "#multimodal",
                "#survey"
            ],
            "emoji": "ü§ñ",
            "ru": {
                "title": "LLM-–∞–≥–µ–Ω—Ç—ã: –Ω–æ–≤–∞—è —ç—Ä–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞",
                "desc": "–î–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–π –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â—É—é –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–∞—É–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ –≤ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –∏ –æ–±—Å—É–∂–¥–∞—é—Ç —Ç–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏."
            },
            "en": {
                "title": "Unlocking Human-Like Intelligence in Autonomous Agents with LLMs",
                "desc": "This paper surveys the emerging field of large language model (LLM)-based autonomous agents, highlighting their potential to mimic human-like decision-making. It critiques traditional approaches that limit agents to isolated environments, contrasting them with LLMs that leverage extensive web knowledge. The authors propose a unified framework for constructing these agents and explore their applications across various domains, including social science and engineering. Additionally, the paper discusses evaluation strategies and identifies challenges and future research directions in the development of LLM-based autonomous agents."
            },
            "zh": {
                "title": "Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑËá™‰∏ªÊô∫ËÉΩ‰ΩìÁ†îÁ©∂ÁªºËø∞",
                "desc": "Ëá™‰∏ªÊô∫ËÉΩ‰Ωì‰∏ÄÁõ¥ÊòØÂ≠¶ÊúØÁïåÂíåÂ∑•‰∏öÁïåÁöÑÁ†îÁ©∂ÈáçÁÇπ„ÄÇ‰ª•ÂæÄÁöÑÁ†îÁ©∂ÈÄöÂ∏∏Âú®Â≠§Á´ãÁéØÂ¢É‰∏≠ËÆ≠ÁªÉÊô∫ËÉΩ‰ΩìÔºåÁü•ËØÜÊúâÈôêÔºåËøô‰∏é‰∫∫Á±ªÁöÑÂ≠¶‰π†ËøáÁ®ãÊúâÂæàÂ§ß‰∏çÂêåÔºåÂõ†Ê≠§Êô∫ËÉΩ‰ΩìÈöæ‰ª•ÂÅöÂá∫Á±ª‰∫∫ÂÜ≥Á≠ñ„ÄÇÊúÄËøëÔºåÈöèÁùÄÂ§ßÈáèÁΩëÁªúÁü•ËØÜÁöÑËé∑ÂèñÔºåÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ±ïÁé∞Âá∫ÂÆûÁé∞‰∫∫Á±ªÊ∞¥Âπ≥Êô∫ËÉΩÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇÊú¨ÊñáÂØπÂü∫‰∫éLLMÁöÑËá™‰∏ªÊô∫ËÉΩ‰ΩìËøõË°å‰∫ÜÂÖ®Èù¢ÁöÑË∞ÉÊü•ÔºåÊèêÂá∫‰∫ÜÁªü‰∏ÄÊ°ÜÊû∂ÔºåÂπ∂Êé¢ËÆ®‰∫ÜÂÖ∂Âú®Á§æ‰ºöÁßëÂ≠¶„ÄÅËá™ÁÑ∂ÁßëÂ≠¶ÂíåÂ∑•Á®ãÈ¢ÜÂüüÁöÑÂ§öÁßçÂ∫îÁî®„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —Å—Ç–∞–ª–∏ –≤–∞–∂–Ω–æ–π —Ç–µ–º–æ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∫–∞–∫ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ, —Ç–∞–∫ –∏ –≤ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏. –†–∞–Ω–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ —á–∞—Å—Ç–æ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö, —á—Ç–æ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞. –≠—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–ª–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞–º–∏ —Ä–µ—à–µ–Ω–∏–π, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ. –û–¥–Ω–∞–∫–æ, —Å –ø–æ—è–≤–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –æ–±—ä–µ–º–∞—Ö –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö, –æ—Ç–∫—Ä—ã–ª–∏—Å—å –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –≤—Å–ø–ª–µ—Å–∫—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –ø–æ—Å–≤—è—â–µ–Ω–Ω—ã—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.</p>\n<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä —ç—Ç–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—â–∏–π –æ–±–ª–∞—Å—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —Å —Ü–µ–ª–æ—Å—Ç–Ω–æ–π –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –°–Ω–∞—á–∞–ª–∞ –æ–±—Å—É–∂–¥–∞–µ—Ç—Å—è –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤, –¥–ª—è —á–µ–≥–æ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –µ–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∞—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–±–æ—Ç. –ó–∞—Ç–µ–º –¥–∞–µ—Ç—Å—è –æ–±–∑–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫–∞—Ö. –¢–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Ç–∞–∫–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤—ã–¥–µ–ª—è—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º –∏ –±—É–¥—É—â–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Ä–∞–∑–≤–∏—Ç–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –Ω–æ–≤—ã—Ö —Ä–∞–±–æ—Ç –∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±–∑–æ—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ —Å—Å—ã–ª–∫–µ <a href=\"https://github.com/Paitesanshi/LLM-Agent-Survey\">https://github.com/Paitesanshi/LLM-Agent-Survey</a>.</p>"
            },
            {
                "title": "Introduction",
                "content": "An autonomous agent is system situated within and part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future. Franklin and Graesser (1997) Autonomous agents have long been recognized as promising approach to achieving artificial general intelligence (AGI), which is expected to acReceived month dd, yyyy; accepted month dd, yyyy E-mail: xu.chen@ruc.edu.cn;yankailin@ruc.edu.cn *Both authors contribute equally to this paper. Front. Comput. Sci., 2024, 0(0): 142 Fig. 1 Illustration of the growth trend in the field of LLM-based autonomous agents. We present the cumulative number of papers published from January 2021 to August 2023. We assign different colors to represent various agent categories. For example, game agent aims to simulate game-player, while tool agent mainly focuses on tool using. For each time period, we provide curated list of studies with diverse agent categories. complish tasks through self-directed planning and actions. In previous studies, the agents are assumed to act based on simple and heuristic policy functions, and learned in isolated and restricted environments [16]. Such assumptions significantly differs from the human learning process, since the human mind is highly complex, and individuals can learn from much wider variety of environments. Because of these gaps, the agents obtained from the previous studies are usually far from replicating human-level decision processes, especially in unconstrained, open-domain settings. In recent years, large language models (LLMs) have achieved notable successes, demonstrating significant potential in attaining human-like intelligence [510]. This capability arises from leveraging comprehensive training datasets alongside substantial number of model parameters. Building upon this capability, there has been growing research area that employs LLMs as central controllers to construct autonomous agents to obtain human-like decision-making capabilities [1117]. Comparing with reinforcement learning, LLMbased agents have more comprehensive internal world knowledge, which facilitates more informed agent actions even without training on specific domain data. Additionally, LLM-based agents can provide natural language interfaces to interact with humans, which is more flexible and explainable. Along this direction, researchers have developed numerous promising models (see Figure 1 for an overview of this field), where the key idea is to equip LLMs with crucial human capabilities like memory and planning to make them behave like humans and complete various tasks effectively. Previously, these models were proposed independently, with limited efforts made to summarize and compare them holistically. However, we believe systematic Lei Wang et al. Survey on Large Language Model based Autonomous Agents 3 summary on this rapidly developing field is of great significance to comprehensively understand it and benefit to inspire future research. In this paper, we conduct comprehensive survey of the field of LLM-based autonomous agents. Specifically, we organize our survey based on three aspects including the construction, application, and evaluation of LLM-based autonomous agents. For the agent construction, we focus on two problems, that is, (1) how to design the agent architecture to better leverage LLMs, and (2) how to inspire and enhance the agent capability to complete different tasks. Intuitively, the first problem aims to build the hardware fundamentals for the agent, while the second problem focus on providing the agent with software resources. For the first problem, we present unified agent framework, which can encompass most of the previous studies. For the second problem, we provide summary on the commonly-used strategies for agents capability acquisition. In addition to discussing agent construction, we also provide an systematic overview of the applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the strategies for evaluating LLM-based autonomous agents, focusing on both subjective and objective strategies. In summary, this survey conducts systematic review and establishes comprehensive taxonomies for existing studies in the burgeoning field of LLMbased autonomous agents. Our focus encompasses three primary areas: construction of agents, their applications, and methods of evaluation. Drawing from wealth of previous studies, we identify various challenges in this field and discuss potential future directions. We expect that our survey can provide newcomers of LLM-based autonomous agents with comprehensive background knowledge, and also encourage further groundbreaking studies.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º—ã, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏–µ —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –ø—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ –æ–±—É—á–∞–ª–∏—Å—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, —á—Ç–æ –¥–∞–ª–µ–∫–æ –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã, —Å —Ä–∞–∑–≤–∏—Ç–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –ø–æ—è–≤–∏–ª–∞—Å—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–º—É –∏ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é.</p>\n<p>LLM-–∞–≥–µ–Ω—Ç—ã –æ–±–ª–∞–¥–∞—é—Ç –æ–±—à–∏—Ä–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –æ –º–∏—Ä–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –ø—Ä–∏–Ω–∏–º–∞—Ç—å –±–æ–ª–µ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–∞–∂–µ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ–Ω–∏ –º–æ–≥—É—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –ª—é–¥—å–º–∏ —á–µ—Ä–µ–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –±–æ–ª–µ–µ –≥–∏–±–∫–∏–º–∏ –∏ –ø–æ–Ω—è—Ç–Ω—ã–º–∏. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è, –∏ –∫–ª—é—á–µ–≤–∞—è –∏–¥–µ—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –Ω–∞–¥–µ–ª–∏—Ç—å LLM —Ç–∞–∫–∏–º–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏, –∫–∞–∫ –ø–∞–º—è—Ç—å –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ LLM-–∞–≥–µ–Ω—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω –ø–æ —Ç—Ä–µ–º –æ—Å–Ω–æ–≤–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º: –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤, –∏—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞. –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å –¥–≤—É—Ö —Å—Ç–æ—Ä–æ–Ω: –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∞–≥–µ–Ω—Ç–∞, —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM, –∏ –∫–∞–∫ —Ä–∞–∑–≤–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á. –ü–µ—Ä–≤–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–π –æ—Å–Ω–æ–≤—ã, –∞ –≤—Ç–æ—Ä–æ–µ ‚Äî —Å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –µ–¥–∏–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –æ–±–∑–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞–∑–≤–∏—Ç–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤.</p>\n<p>–ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ, –≤ —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞—É–∫–∞—Ö. –ù–∞–∫–æ–Ω–µ—Ü, –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤, –∫–∞–∫ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö, —Ç–∞–∫ –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö. –í —Ü–µ–ª–æ–º, —Å—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤, –≤—ã—è–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –¶–µ–ª—å –æ–±–∑–æ—Ä–∞ ‚Äî –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–æ–≤–∏—á–∫–∞–º –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–Ω–∞–Ω–∏—è –∏ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–∞–ª—å–Ω–µ–π—à–∏–µ –ø—Ä–æ—Ä—ã–≤–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.</p>"
            },
            {
                "title": "Designing LLM-Based Autonomous Agents",
                "content": "struction LLM-based autonomous agents are expected to effectively perform diverse tasks by leveraging the human-like capabilities of LLMs. In order to achieve this goal, there are two significant aspects, that is, (1) which architecture should be designed to better use LLMs and (2) give the designed architecture, how to enable the agent to acquire capabilities for accomplishing specific tasks. Within the context of architecture design, we contribute systematic synthesis of existing research, culminating in comprehensive unified framework. As for the second aspect, we summarize the strategies for agent capability acquisition based on whether they fine-tune the LLMs. When comparing LLM-based autonomous agents to traditional machine learning, designing the agent architecture is analogous to determining the network structure, while the agent capability acquisition is similar to learning the network parameters. In the following, we introduce these two aspects more in detail. 2.1 Agent Architecture Design Recent advancements in LLMs have demonstrated their great potential to accomplish wide range of tasks in the form of question-answering (QA). However, building autonomous agents is far from QA, since they need to fulfill specific roles and autonomously perceive and learn from the environment to evolve themselves like humans. To bridge the gap between traditional LLMs and autonomous agents, crucial aspect is to design rational agent architectures to assist LLMs in maximizing their capabilities. Along this direction, previous work has 4 Front. Comput. Sci., 2024, 0(0): 1 developed number of modules to enhance LLMs. In this section, we propose unified framework to summarize these modules. In specific, the overall structure of our framework is illustrated Figure 2, which is composed of profiling module, memory module, planning module, and an action module. The purpose of the profiling module is to identify the role of the agent. The memory and planning modules place the agent into dynamic environment, enabling it to recall past behaviors and plan future actions. The action module is responsible for translating the agents decisions into specific outputs. Within these modules, the profiling module impacts the memory and planning modules, and collectively, these three modules influence the action module. In the following, we detail these modules. 2.1.1 Profiling Module Autonomous agents typically perform tasks by assuming specific roles, such as coders, teachers and domain experts [18, 19]. The profiling module aims to indicate the profiles of the agent roles, which are usually written into the prompt to influence the LLM behaviors. Agent profiles typically encompass basic information such as age, gender, and career [20], as well as psychology information, reflecting the personalities of the agent, and social information, detailing the relationships between agents [21]. The choice of information to profile the agent is largely determined by the specific application scenarios. For instance, if the application aims to study human cognitive process, then the psychology information becomes pivotal. After identifying the types of profile information, the next important problem is to create specific profiles for the agents. Existing literature commonly employs the following three strategies. Handcrafting Method: in this method, agent profiles are manually specified. For instance, if one would like to design agents with different personalities, he can use \"you are an outgoing person\" or \"you are an introverted person\" to profile the agent. The handcrafting method has been leveraged in lot of previous work to indicate the agent profiles. For example, Generative Agent [22] describes the agent by the information like name, objectives, and relationships with other agents. MetaGPT [23], ChatDev [18], and Self-collaboration [24] predefine various roles and their corresponding responsibilities in software development, manually assigning distinct profiles to each agent to facilitate collaboration. PTLLM [25] aims to explore and quantify personality traits displayed in texts generated by LLMs. This method guides LLMs in generating diverse responses by manfully defining various agent characters through the use of personality assessment tools such as IPIP-NEO [26] and BFI [27]. [28] studies the toxicity of the LLM output by manually prompting LLMs with different roles, such as politicians, journalists and businesspersons. In general, the handcrafting method is very flexible, since one can assign any profile information to the agents. However, it can be also labor-intensive, particularly when dealing with large number of agents. LLM-generation Method: in this method, agent profiles are automatically generated based on LLMs. Typically, it begins by indicating the profile generation rules, elucidating the composition and attributes of the agent profiles within the target population. Then, one can optionally specify several seed agent profiles to serve as few-shot examples. At last, LLMs are leveraged to generate all the agent profiles. For example, RecAgent [21] first creates seed profiles for few number of agents by manually crafting their backgrounds like age, gender, personal traits, and movie preferences. Then, it Lei Wang et al. Survey on Large Language Model based Autonomous Agents 5 leverages ChatGPT to generate more agent profiles based on the seed information. The LLMgeneration method can save significant time when the number of agents is large, but it may lack precise control over the generated profiles. Dataset Alignment Method: in this method, the agent profiles are obtained from real-world datasets. Typically, one can first organize the information about real humans in the datasets into natural language prompts, and then leverage it to profile the agents. For instance, in [29], the authors assign roles to GPT-3 based on the demographic backgrounds (such as race/ethnicity, gender, age, and state of residence) of participants in the American National Election Studies (ANES). They subsequently investigate whether GPT-3 can produce similar results to those of real humans. The dataset alignment method accurately captures the attributes of the real population, thereby making the agent behaviors more meaningful and reflective of realworld scenarios. Remark. While most of the previous work leverage the above profile generation strategies independently, we argue that combining them may yield additional benefits. For example, in order to predict social developments via agent simulation, one can leverage real-world datasets to profile subset of the agents, thereby accurately reflecting the current social status. Subsequently, roles that do not exist in the real world but may emerge in the future can be manually assigned to the other agents, enabling the prediction of future social development. Beyond this example, one can also flexibly combine the other strategies. The profile module serves as the foundation for agent design, exerting significant influence on the agent memorization, planning, and action procedures. 2.1.2 Memory Module The memory module plays very important role in the agent architecture design. It stores information perceived from the environment and leverages the recorded memories to facilitate future actions. The memory module can help the agent to accumulate experiences, self-evolve, and behave in more consistent, reasonable, and effective manner. This section provides comprehensive overview of the memory module, focusing on its structures, formats, and operations. Memory Structures: LLM-based autonomous agents usually incorporate principles and mechanisms derived from cognitive science research on human memory processes. Human memory follows general progression from sensory memory that registers perceptual inputs, to short-term memory that maintains information transiently, to longterm memory that consolidates information over extended periods. When designing the agent memory structures, researchers take inspiration from these aspects of human memory. In specific, shortterm memory is analogous to the input information within the context window constrained by the transformer architecture. Long-term memory resembles the external vector storage that agents can rapidly query and retrieve from as needed. In the following, we introduce two commonly used memory structures based on the shortand long-term memories. Unified Memory. This structure only simulates the human shot-term memory, which is usually realized by in-context learning, and the memory information is directly written into the prompts. For example, RLP [30] is conversation agent, which maintains internal states for the speaker and listener. During each round of conversation, these states serve as LLM prompts, functioning as the 6 Front. Comput. Sci., 2024, 0(0): 142 Fig. 2 unified framework for the architecture design of LLM-based autonomous agent. agents short-term memory. SayPlan [31] is an embodied agent specifically designed for task planning. In this agent, the scene graphs and environment feedback serve as the agents short-term memory, guiding its actions. CALYPSO [32] is an agent designed for the game Dungeons & Dragons, which can assist Dungeon Masters in the creation and narration of stories. Its short-term memory is built upon scene descriptions, monster information, and previous summaries. DEPS [33] is also game agent, but it is developed for Minecraft. The agent initially generates task plans and then utilizes them to prompt LLMs, which in turn produce actions to complete the task. These plans can be deemed as the agents short-term memory. In practice, implementing short-term memory is straightforward and can enhance an agents ability to perceive recent or contextually sensitive behaviors and observations. However,due to the limitation of context window of LLMs, its hard to put all memories into prompt, which may degrade the performance of agents.This method has high requirements on the window length of LLMs and the ability to handle long contexts. Therefore, many researchers resort to hybrid memory to alleviate this question. However, the limited context window of LLMs restricts incorporating comprehensive memories into prompts, which can impair agent performance. This challenge necessitates LLMs with larger context windows and the ability to handle extended contexts. Consequently, numerous researchers turn to hybrid memory systems to mitigate this issue. Hybrid Memory. This structure explicitly models the human short-term and long-term memories. The short-term memory temporarily buffers recent perceptions, while long-term memory consolidates important information over time. For instance, Generative Agent [20] employs hybrid memory structure to facilitate agent behaviors. The short-term memory contains the context information about the agent current situations, while the long-term memory stores the agent past behaviors and thoughts, which can be retrieved according to the current events. AgentSims [34] also implements hybrid memory architecture. The information provided in the prompt can be considered as short-term Lei Wang et al. Survey on Large Language Model based Autonomous Agents 7 memory. In order to enhance the storage capacity of memory, the authors propose long-term memory system that utilizes vector database, facilitating efficient storage and retrieval. Specifically, the agents daily memories are encoded as embeddings and stored in the vector database. If the agent needs to recall its previous memories, the long-term memory system retrieves relevant information using embedding similarities. This process can improve the consistency of the agents behavior. In GITM [16], the short-term memory stores the current trajectory, and the long-term memory saves reference plans summarized from successful prior trajectories. Long-term memory provides stable knowledge, while short-term memory allows flexible planning. Reflexion [12] utilizes shortterm sliding window to capture recent feedback and incorporates persistent long-term storage to retain condensed insights. This combination allows for the utilization of both detailed immediate experiences and high-level abstractions. SCM [35] selectively activates the most relevant long-term knowledge to combine with short-term memory, enabling reasoning over complex contextual dialogues. SimplyRetrieve [36] utilizes user queries as shortterm memory and stores long-term memory using external knowledge bases. This design enhances the model accuracy while guaranteeing user privacy. MemorySandbox [37] implements long-term and short-term memory by utilizing 2D canvas to store memory objects, which can then be accessed throughout various conversations. Users can create multiple conversations with different agents on the same canvas, facilitating the sharing of memory objects through simple drag-and-drop interface. In practice, integrating both short-term and longterm memories can enhance an agents ability for long-range reasoning and accumulation of valuable experiences, which are crucial for accomplishing tasks in complex environments. Remark. Careful readers may find that there may also exist another type of memory structure, that is, only based on the long-term memory. However, we find that such type of memory is rarely documented in the literature. Our speculation is that the agents are always situated in continuous and dynamic environments, with consecutive actions displaying high correlation. Therefore, the capture of shortterm memory is very important and usually cannot be disregarded. Memory Formats: In addition to the memory structure, another perspective to analyze the memory module is based on the formats of the memory storage medium, for example, natural language memory or embedding memory. Different memory formats possess distinct strengths and are suitable for various applications. In the following, we introduce several representative memory formats. Natural Languages. In this format, memory information such as the agent behaviors and observations are directly described using raw natural language. This format possesses several strengths. Firstly, the memory information can be expressed in flexible and understandable manner. Moreover, it retains rich semantic information that can provide comprehensive signals to guide agent behaviors. In the previous work, Reflexion [12] stores experiential feedback in natural language within sliding window. Voyager [38] employs natural language descriptions to represent skills within the Minecraft game, which are directly stored in memory. Embeddings. In this format, memory information is encoded into embedding vectors, which can enhance the memory retrieval and reading efficiency. For instance, MemoryBank [39] encodes each memory segment into an embedding vector, Front. Comput. Sci., 2024, 0(0): 142 which creates an indexed corpus for retrieval. [16] represents reference plans as embeddings to facilitate matching and reuse. Furthermore, ChatDev [18] encodes dialogue history into vectors for retrieval. Databases. In this format, memory information is stored in databases, allowing the agent to manipulate memories efficiently and comprehensively. For example, ChatDB [40] uses database as symbolic memory module. The agent can utilize SQL statements to precisely add, delete, and revise the memory information. In DB-GPT [41], the memory module is constructed based on database. To more intuitively operate the memory information, the agents are fine-tuned to understand and execute SQL queries, enabling them to interact with databases using natural language directly. Structured Lists. In this format, memory information is organized into lists, and the semantic of memory can be conveyed in an efficient and concise manner. For instance, GITM [16] stores action lists for sub-goals in hierarchical tree structure. The hierarchical structure explicitly captures the relationships between goals and corresponding plans. RETLLM [42] initially converts natural language sentences into triplet phrases, and subsequently stores them in memory. Remark. Here we only show several representative memory formats, but it is important to note that there are many uncovered ones, such as the programming code used by [38]. Moreover, it should be emphasized that these formats are not mutually exclusive; many models incorporate multiple formats to concurrently harness their respective benefits. notable example is the memory module of GITM [16], which utilizes key-value list structure. In this structure, the keys are represented by embedding vectors, while the values consist of raw natural languages. The use of embedding vectors allows for efficient retrieval of memory records. By utilizing natural languages, the memory contents become highly comprehensive, enabling more informed agent actions. Above, we mainly discuss the internal designs of the memory module. In the following, we turn our focus to memory operations, which are used to interact with external environments. Memory Operations: The memory module plays critical role in allowing the agent to acquire, accumulate, and utilize significant knowledge by interacting with the environment. The interaction between the agent and the environment is accomplished through three crucial memory operations: memory reading, memory writing, and memory reflection. In the following, we introduce these operations more in detail. Memory Reading. The objective of memory reading is to extract meaningful information from memory to enhance the agents actions. For example, using the previously successful actions to achieve similar goals [16]. The key of memory reading lies in how to extract valuable information from history actions. Usually, there three commonly used criteria for information extraction, that is, the recency, relevance, and importance [20]. Memories that are more recent, relevant, and important are more likely to be extracted. Formally, we conclude the following equation from existing literature for memory information extraction: = arg min mM Œ±srec(q, m) + Œ≤srel(q, m) + Œ≥simp(m), (1) where is the query, for example, the task that the agent should address or the context in which the agent is situated. is the set of all memories. srec(), srel() and simp() are the scoring functions for measuring the recency, relevance, and importance Lei Wang et al. Survey on Large Language Model based Autonomous Agents of the memory m. These scoring functions can be implemented using various methods, for example, srel(q, m) can be realized based on LSH, ANNOY, HNSW, FAISS and so on. It should be noted that simp only reflects the characters of the memory itself, thus it is unrelated to the query q. Œ±, Œ≤ and Œ≥ are balancing parameters. By assigning them with different values, one can obtain various memory reading strategies. For example, by setting Œ± = Œ≥ = 0, many studies [16, 30, 38, 42] only consider the relevance score srel for memory reading. By assigning Œ± = Œ≤ = Œ≥ = 1.0, [20] equally weights all the above three metrics to extract information from memory. Memory Writing. The purpose of memory writing is to store information about the perceived environment in memory. Storing valuable information in memory provides foundation for retrieving informative memories in the future, enabling the agent to act more efficiently and rationally. During the memory writing process, there are two potential problems that should be carefully addressed. On one hand, it is crucial to address how to store information that is similar to existing memories (i.e., memory duplicated). On the other hand, it is important to consider how to remove information when the memory reaches its storage limit (i.e., memory overflow). In the following, we discuss these problems more in detail. (1) Memory Duplicated. To incorporate similar information, people have developed various methods for integrating new and previous records. For instance, in [7], the successful action sequences related to the same subgoal are stored in list. Once the size of the list reaches N(=5), all the sequences in it are condensed into unified plan solution using LLMs. The original sequences in the memory are replaced with the newly generated one. Augmented LLM [43] aggregates duplicate information via count accumulation, avoiding redundant storage. (2) Memory Overflow. In order to write information into the memory when it is full, people design different methods to delete existing information to continue the memorizing process. For example, in ChatDB [40], memories can be explicitly deleted based on user commands. RET-LLM [42] uses fixed-size buffer for memory, overwriting the oldest entries in first-in-first-out (FIFO) manner. Memory Reflection. Memory reflection emulates humans ability to witness and evaluate their own cognitive, emotional, and behavioral processes. When adapted to agents, the objective is to provide agents with the capability to independently summarize and infer more abstract, complex and high-level information. More specifically, in Generative Agent [20], the agent has the capability to summarize its past experiences stored in memory into broader and more abstract insights. To begin with, the agent generates three key questions based on its recent memories. Then, these questions are used to query the memory to obtain relevant information. Building upon the acquired information, the agent generates five insights,",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ \"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–æ–≤\"</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –æ–±–æ–±—â–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å LLM –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–∞—Å–∫—Ä—ã—Ç—å —Å–≤–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á.</p>\n<p>–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π:</p>\n<ol>\n<li><strong>–ú–æ–¥—É–ª—å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è:</strong> –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–æ–ª—å –∞–≥–µ–Ω—Ç–∞, –≤–ª–∏—è—è –Ω–∞ –µ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ.</li>\n<li><strong>–ú–æ–¥—É–ª—å –ø–∞–º—è—Ç–∏:</strong> –ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç.</li>\n<li><strong>–ú–æ–¥—É–ª—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:</strong> –î–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –±—É–¥—É—â–∏–µ –¥–µ–π—Å—Ç–≤–∏—è.</li>\n<li><strong>–ú–æ–¥—É–ª—å –¥–µ–π—Å—Ç–≤–∏–π:</strong> –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ä–µ—à–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.</li>\n</ol>\n<p>–ú–æ–¥—É–ª—å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ –º–æ–¥—É–ª–∏ –ø–∞–º—è—Ç–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∏ –≤—Å–µ —Ç—Ä–∏ –º–æ–¥—É–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –º–æ–¥—É–ª—å –¥–µ–π—Å—Ç–≤–∏–π.</p>\n<h3>–ú–æ–¥—É–ª—å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è</h3>\n<p>–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–æ–ª–∏ –∞–≥–µ–Ω—Ç–∞. –ü—Ä–æ—Ñ–∏–ª—å –∞–≥–µ–Ω—Ç–∞ –æ–±—ã—á–Ω–æ –≤–∫–ª—é—á–∞–µ—Ç:</p>\n<ul>\n<li><strong>–û—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:</strong> –í–æ–∑—Ä–∞—Å—Ç, –ø–æ–ª, –ø—Ä–æ—Ñ–µ—Å—Å–∏—è.</li>\n<li><strong>–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:</strong> –õ–∏—á–Ω–æ—Å—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏.</li>\n<li><strong>–°–æ—Ü–∏–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:</strong> –û—Ç–Ω–æ—à–µ–Ω–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏.</li>\n</ul>\n<p>–í—ã–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è. –°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª–µ–π:</p>\n<ul>\n<li><strong>–†—É—á–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ:</strong> –ü—Ä–æ—Ñ–∏–ª–∏ –∑–∞–¥–∞—é—Ç—Å—è –≤—Ä—É—á–Ω—É—é, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç—Ä—É–¥–æ–µ–º–∫–∏–º.</li>\n<li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é LLM:</strong> –ü—Ä–æ—Ñ–∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏ –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –≤—Ä–µ–º—è, –Ω–æ –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å.</li>\n<li><strong>–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö:</strong> –ü—Ä–æ—Ñ–∏–ª–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º.</li>\n</ul>\n<p>–ê–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ç–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è —á–∞—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –∏ –≤—Ä—É—á–Ω—É—é –∑–∞–¥–∞–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö, —á—Ç–æ–±—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã.</p>\n<h3>–ú–æ–¥—É–ª—å –ø–∞–º—è—Ç–∏</h3>\n<p>–ú–æ–¥—É–ª—å –ø–∞–º—è—Ç–∏ –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∞–≥–µ–Ω—Ç–∞, –ø–æ–∑–≤–æ–ª—è—è –µ–º—É –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –æ–ø—ã—Ç –∏ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ. –û–Ω –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –Ω–∞—É–∫–∏ –æ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –≤—ã–¥–µ–ª—è—é—Ç:</p>\n<ul>\n<li><strong>–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å:</strong> –ê–Ω–∞–ª–æ–≥ –≤—Ö–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º –æ–∫–Ω–µ LLM.</li>\n<li><strong>–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å:</strong> –ê–Ω–∞–ª–æ–≥ –≤–Ω–µ—à–Ω–µ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Å—Ç—Ä–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.</li>\n</ul>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏:</p>\n<ul>\n<li><strong>–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å:</strong> –ò–º–∏—Ç–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å —á–µ–ª–æ–≤–µ–∫–∞, —Ä–µ–∞–ª–∏–∑—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–æ–º–ø—Ç—ã. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø—Ä–æ—Å—Ç –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è, –Ω–æ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏–∑-–∑–∞ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM.</li>\n</ul>\n<p>–í —Ü–µ–ª–æ–º, —ç—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±–∑–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –≤—ã–¥–µ–ª—è—è –∫–ª—é—á–µ–≤—ã–µ –º–æ–¥—É–ª–∏ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.</p>"
            },
            {
                "title": "Hierarchical Reflection and Planning in Agent-Based Learning",
                "content": " which reflect the agent high-level ideas. For example, the low-level memories Klaus Mueller is writing research paper, Klaus Mueller is engaging with librarian to further his research, and Klaus Mueller is conversing with Ayesha Khan about his research can induce the high-level insight Klaus Mueller is dedicated to his research. In addition, the reflection process can occur hierarchically, meaning that the insights can be generated based on existing insights. In GITM [16], the actions that successfully accomplish the sub-goals are stored in list. When the list contains more than five elements, the agent summarizes them into common and abstract pattern and replaces all the elements. In ExpeL [44], two 10 Front. Comput. Sci., 2024, 0(0): 1 approaches are introduced for the agent to acquire reflection. Firstly, the agent compares successful or failed trajectories within the same task. Secondly, the agent learns from collection of successful trajectories to gain experiences. significant distinction between traditional LLMs and the agents is that the latter must possess the capability to learn and complete tasks in dynamic environments. If we consider the memory module as responsible for managing the agents past behaviors, it becomes essential to have another significant module that can assist the agents in planning their future actions. In the following, we present an overview of how researchers design the planning module. 2.1.3 Planning Module When faced with complex task, humans tend to deconstruct it into simpler subtasks and solve them individually. The planning module aims to empower the agents with such human capability, which is expected to make the agent behave more reasonably, powerfully, and reliably. In specific, we summarize existing studies based on whether the agent can receive feedback in the planing process, which are detailed as follows: Planning without Feedback: In this method, the agents do not receive feedback that can influence its future behaviors after taking actions. In the following, we present several representative strategies. Single-path Reasoning. In this strategy, the final task is decomposed into several intermediate steps. These steps are connected in cascading manner, with each step leading to only one subsequent step. LLMs follow these steps to achieve the final goal. Specifically, Chain of Thought (CoT) [45] proposes inputting reasoning steps for solving complex problems into the prompt. These steps serve as examples to inspire LLMs to plan and act in step-by-step manner. In this method, the plans are created based on the inspiration from the examples in the prompts. Zero-shot-CoT [46] enables LLMs to generate task reasoning processes by prompting them with trigger sentences like \"think step by step\". Unlike CoT, this method does not incorporate reasoning steps as examples in the prompts. RePrompting [47] involves checking whether each step meets the necessary prerequisites before generating plan. If step fails to meet the prerequisites, it introduces prerequisite error message and prompts the LLM to regenerate the plan. ReWOO [48] introduces paradigm of separating plans from external observations, where the agents first generate plans and obtain observations independently, and then combine them together to derive the final results. HuggingGPT [13] first decomposes the task into many sub-goals, and then solves each of them based on Huggingface. Different from CoT and Zero-shotCoT, which outcome all the reasoning steps in one-shot manner, ReWOO and HuggingGPT produce the results by accessing LLMs multiply times. Multi-path Reasoning. In this strategy, the reasoning steps for generating the final plans are organized into tree-like structure. Each intermediate step may have multiple subsequent steps. This approach is analogous to human thinking, as individuals may have multiple choices at each reasoning step. In specific, Self-consistent CoT (CoT-SC) [49] believes that each complex problem has multiple ways of thinking to deduce the final answer. Thus, it starts by employing CoT to generate various reasoning paths and corresponding answers. Subsequently, the answer with the highest frequency is chosen as the final output. Tree of Thoughts (ToT) [50] is designed to generate plans using tree-like reasoning structure. In this approach, each node in the tree represents \"thought,\" which corresponds Lei Wang et al. Survey on Large Language Model based Autonomous Agents 11 to an intermediate reasoning step. The selection of these intermediate steps is based on the evaluation of LLMs. The final plan is generated using either the breadth-first search (BFS) or depth-first search (DFS) strategy. Comparing with CoT-SC, which generates all the planed steps together, ToT needs to query LLMs for each reasoning step. In RecMind [51], the authors designed self-inspiring mechanism, where the discarded historical information in the planning process is also leveraged to derive new reasoning steps. In GoT [52], the authors expand the tree-like reasoning structure in ToT to graph structures, resulting in more powerful prompting strategies. In AoT [53], the authors design novel method to enhance the reasoning processes of LLMs by incorporating algorithmic examples into the prompts. Remarkably, this method only needs to query LLMs for only one or few times. In [54], the LLMs are leveraged as zero-shot planners. At each planning step, they first generate multiple possible next steps, and then determine the final one based on their distances to admissible actions. [55] further improves [54] by incorporating examples that are similar to the queries in the prompts. RAP [56] builds world model to simulate the potential benefits of different plans based on Monte Carlo Tree Search (MCTS), and then, the final plan is generated by aggregating multiple MCTS iterations. To enhance comprehension, we provide an illustration comparing the strategies of single-path and multi-path reasoning in Figure 3. External Planner. Despite the demonstrated power of LLMs in zero-shot planning, effectively generating plans for domain-specific problems remains highly challenging. To address this challenge, researchers turn to external planners. These tools are well-developed and employ efficient search algorithms to rapidly identify correct, or even optimal, In specific, LLM+P [57] first transforms plans. the task descriptions into formal Planning Domain Definition Languages (PDDL), and then it uses an external planner to deal with the PDDL. Finally, the generated results are transformed back into natural language by LLMs. Similarly, LLM-DP [58] utilizes LLMs to convert the observations, the current world state, and the target objectives into PDDL. Subsequently, this transformed data is passed to an external planner, which efficiently determines the final action sequence. CO-LLM [22] demonstrates that LLMs is good at generating high-level plans, but struggle with low-level control. To address this limitation, heuristically designed external low-level planner is employed to effectively execute actions based on high-level plans. Planning with Feedback: In many real-world scenarios, the agents need to make long-horizon planning to solve complex tasks. When facing these tasks, the above planning modules without feedback can be less effective due to the following reasons: firstly, generating flawless plan directly from the beginning is extremely difficult as it needs to consider various complex preconditions. As result, simply following the initial plan often leads to failure. Moreover, the execution of the plan may be hindered by unpredictable transition dynamics, rendering the initial plan non-executable. Simultaneously, when examining how humans tackle complex tasks, we find that individuals may iteratively make and revise their plans based on external feedback. To simulate such human capability, researchers have designed many planning modules, where the agent can receive feedback after taking actions. The feedback can be obtained from environments, humans, and models, which are detailed in the following. Environmental Feedback. This feedback is obtained from the objective world or virtual envi12 Front. Comput. Sci., 2024, 0(0): 142 Fig. 3 Comparison between the strategies of single-path and multi-path reasoning. LMZSP is the model proposed in [54]. ronment. For instance, it could be the games task completion signals or the observations made after the agent takes an action. In specific, ReAct [59] proposes constructing prompts using thought-actobservation triplets. The thought component aims to facilitate high-level reasoning and planning for guiding agent behaviors. The act represents specific action taken by the agent. The observation corresponds to the outcome of the action, acquired through external feedback, such as search engine results. The next thought is influenced by the previous observations, which makes the generated plans more adaptive to the environment. Voyager [38] makes plans by incorporating three types of environment feedback including the intermediate progress of program execution, the execution error and selfverification results. These signals can help the agent to make better plans for the next action. Similar to Voyager, Ghost [16] also incorporates feedback into the reasoning and action taking processes. This feedback encompasses the environment states as well as the success and failure information for each executed action. SayPlan [31] leverages environmental feedback derived from scene graph simulator to validate and refine its strategic formulations. This simulator is adept at discerning the outcomes and state transitions subsequent to agent actions, facilitating SayPlans iterative recalibration of its strategies until viable plan is ascertained. In DEPS [33], the authors argue that solely providing information about the completion of task is often inadequate for correcting planning errors. Therefore, they propose informing the agent about the detail reasons for task failure, allowing them to more effectively revise their plans. LLMPlanner [60] introduces grounded re-planning algorithm that dynamically updates plans generated by LLMs when encountering object mismatches and unattainable plans during task completion. Inner Monologue [61] provides three types of feedback to the agent after it takes actions: (1) whether the task is successfully completed, (2) passive scene descriptions, and (3) active scene descriptions. The former two are generated from the environments, which makes the agent actions more reasonable. Human Feedback. In addition to obtaining feedback from the environment, directly interacting with humans is also very intuitive strategy to enhance the agent planning capability. The human feedback is subjective signal. It can effectively Lei Wang et al. Survey on Large Language Model based Autonomous Agents 13 make the agent align with the human values and preferences, and also help to alleviate the hallucination problem. In Inner Monologue [61], the agent aims to perform high-level natural language instructions in 3D visual environment. It is given the capability to actively solicit feedback from humans regarding scene descriptions. Then, the agent incorporates the human feedback into its prompts, enabling more informed planning and reasoning. In the above cases, we can see, different types of feedback can be combined to enhance the agent planning capability. For example, Inner Monologue [61] collects both environment and human feedback to facilitate the agent plans. Model Feedback. Apart from the aforementioned environmental and human feedback, which are external signals, researchers have also investigated the utilization of internal feedback from the agents themselves. This type of feedback is usually generated based on pre-trained models. In specific, [62] proposes self-refine mechanism. This mechanism consists of three crucial components: output, feedback, and refinement. Firstly, the agent generates an output. Then, it utilizes LLMs to provide feedback on the output and offer guidance on how to refine it. At last, the output is improved by the feedback and refinement. This output-feedbackrefinement process iterates until reaching some desired conditions. SelfCheck [63] allows agents to examine and evaluate their reasoning steps generated at various stages. They can then correct any errors by comparing the outcomes. InterAct [64] uses different language models (such as ChatGPT and InstructGPT) as auxiliary roles, such as checkers and sorters, to help the main language model avoid erroneous and inefficient actions. ChatCoT [65] utilizes model feedback to improve the quality of its reasoning process. The model feedback is generated by an evaluation module that monitors the agent reasoning steps. Reflexion [12] is developed to enhance the agents planning capability through detailed verbal feedback. In this model, the agent first produces an action based on its memory, and then, the evaluator generates feedback by taking the agent trajectory as input. In contrast to previous studies, where the feedback is given as scalar value, this model leverages LLMs to provide more detailed verbal feedback, which can provide more comprehensive supports for the agent plans. Remark. In conclusion, the implementation of planning module without feedback is relatively straightforward. However, it is primarily suitable for simple tasks that only require small number of reasoning steps. Conversely, the strategy of planning with feedback needs more careful designs to handle the feedback. Nevertheless, it is considerably more powerful and capable of effectively addressing complex tasks that involve long-range reasoning. 2.1.4 Action Module The action module is responsible for translating the agents decisions into specific outcomes. This module is located at the most downstream position and directly interacts with the environment. It is influenced by the profile, memory, and planning modules. This section introduces the action module from four perspectives: (1) Action goal: what are the intended outcomes of the actions? (2) Action production: how are the actions generated? (3) Action space: what are the available actions? (4) Action impact: what are the consequences of the actions? Among these perspectives, the first two focus on the aspects preceding the action (\"beforeaction\" aspects), the third focuses on the action itself (\"in-action\" aspect), and the fourth emphasizes the impact of the actions (\"after-action\" aspect). Front. Comput. Sci., 2024, 0(0): 142 Action Goal: The agent can perform actions with various objectives. Here, we present several representative examples: (1) Task Completion. In this scenario, the agents actions are aimed at accomplishing specific tasks, such as crafting an iron pickaxe in Minecraft [38] or completing function in software development [18]. These actions usually have well-defined objectives, and each action contributes to the completion of the final task. Actions aimed at this type of goal are very common in existing literature. (2) Communication. In this case, the actions are taken to communicate with the other agents or real humans for sharing information or collaboration. For example, the agents in ChatDev [18] may communicate with each other to collectively accomplish software development tasks. In Inner Monologue [61], the agent actively engages in communication with humans and adjusts its action strategies based on human feedback. (3) Environment Exploration. In this example, the agent aims to explore unfamiliar environments to expand its perception and strike balance between exploring and exploiting. For instance, the agent in Voyager [38] may explore unknown skills in their task completion process, and continually refine the skill execution code based on environment feedback through trial and error. Action Production: Different from ordinary LLMs, where the model input and output are directly associated, the agent may take actions via different strategies and sources. In the following, we introduce two types of commonly used action production strategies. (1) Action via Memory Recollection. In this strategy, the action is generated by extracting information from the agent memory according to the current task. The task and the extracted memories are used as prompts to trigger the agent actions. For example, in Generative Agents [20], the agent maintains memory stream, and before taking each action, it retrieves recent, relevant and important information from the memory steam to guide the agent actions. In GITM [16], in order to achieve low-level sub-goal, the agent queries its memory to determine if there are any successful experiences related to the task. If similar tasks have been completed previously, the agent invokes the previously successful actions to handle the current task directly. In collaborative agents such as ChatDev [18] and MetaGPT [23], different agents may communicate with each other. In this process, the conversation history in dialog is remembered in the agent memories. Each utterance generated by the agent is influenced by its memory. (2) Action via Plan Following. In this strategy, the agent takes actions following its pre-generated plans. For instance, in DEPS [33], for given task, the agent first makes action plans. If there are no signals indicating plan failure, the agent will strictly adhere to these plans. In GITM [16], the agent makes high-level plans by decomposing the task into many sub-goals. Based on these plans, the agent takes actions to solve each sub-goal sequentially to complete the final task. Action Space: Action space refers to the set of possible actions that can be performed by the agent. In general, we can roughly divide these actions into two classes: (1) external tools and (2) internal knowledge of the LLMs. In the following, we introduce these actions more in detail. External Tools. While LLMs have been demonstrated to be effective in accomplishing large amount of tasks, they may not work well for the domains which need comprehensive expert knowledge. In addition, LLMs may also encounter hallucination problems, which are hard to be resolved by themselves. To alleviate the above problems, the agents are empowered with the capability Lei Wang et al. Survey on Large Language Model based Autonomous Agents 15 to call external tools for executing action. In the following, we present several representative tools which have been exploited in the literature. (1) APIs. Leveraging external APIs to complement and expand action space is popular paradigm in recent years. For example, HuggingGPT [13] leverages the models on HuggingFace to accomplish complex user tasks. [66, 67] propose to automatically generate queries to extract relevant content from external web pages when responding to user request. TPTU [67] interfaces with both Python interpreters and LaTeX compilers to execute sophisticated computations such as square roots, factorials and matrix operations. Another type of APIs is the ones that can be directly invoked by LLMs based on natural language or code inputs. For instance, Gorilla [68] is fine-tuned LLM designed to generate accurate input arguments for API calls and mitigate the issue of hallucination during external API invocations. ToolFormer [15] is an LLM-based tool transformation system that can automatically convert given tool into another one with different functionalities or formats based on natural language instructions. API-Bank [69] is an LLM-based API recommendation agent that can automatically search and generate appropriate API calls for various programming languages and domains. APIBank also provides an interactive interface for users to easily modify and execute the generated API calls. ToolBench [14] is an LLM-based tool generation system that can automatically design and implement various practical tools based on natural language requirements. The tools generated by ToolBench include calculators, unit converters, calendars, maps, charts, etc. RestGPT [70] connects LLMs with RESTful APIs, which follow widely accepted standards for web services development, making the resulting program more compatible with real-world applications. TaskMatrix.AI [71] connects LLMs with millions of APIs to support task execution. At its core lies multimodal conversational foundational model that interacts with users, understands their goals and context, and then produces executable code for particular tasks. All these agents utilize external APIs as their tools, and provide interactive interfaces for users to easily modify and execute the generated or transformed tools. (2) Databases & Knowledge Bases. Integrating external database or knowledge base enables agents to obtain specific domain information for generating more realistic actions. For example, ChatDB [40] employs SQL statements to query databases, facilitating actions by the agents in logical manner. MRKL [72] and OpenAGI [73] incorporate various expert systems such as knowledge bases and planners to access domain-specific information. (3) External Models. Previous studies often utilize external models to expand the range of possible actions. In comparison to APIs, external models typically handle more complex tasks. Each external model may correspond to multiple APIs. For example, to enhance the text retrieval capability, MemoryBank [39] incorporates two language models: on",
                "summary": "<h2>–ú–æ–¥—É–ª—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)</h2>\n<p>–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –ø–æ—Å–≤—è—â–µ–Ω –º–æ–¥—É–ª—é –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö LLM, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∑–∞–Ω–∏–º–∞—é—Ç—Å—è –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–µ–∫—Å—Ç–∞, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —É–º–µ—Ç—å —É—á–∏—Ç—å—Å—è –∏ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –¥–∏–Ω–∞–º–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. –ú–æ–¥—É–ª—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–∑–≤–∞–Ω –Ω–∞–¥–µ–ª–∏—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é —Ä–∞–∑–±–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏ –∏ —Ä–µ—à–∞—Ç—å –∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø–æ–¥–æ–±–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —ç—Ç–æ –¥–µ–ª–∞—é—Ç –ª—é–¥–∏. –≠—Ç–æ, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, –¥–µ–ª–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –±–æ–ª–µ–µ —Ä–∞–∑—É–º–Ω—ã–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –Ω–∞–¥–µ–∂–Ω—ã–º.</p>\n<p>–í –æ–±–∑–æ—Ä–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –ø–æ–ª—É—á–∞–µ—Ç –ª–∏ –∞–≥–µ–Ω—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:</p>\n<h3>–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏</h3>\n<p>–í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ –∞–≥–µ–Ω—Ç—ã –Ω–µ –ø–æ–ª—É—á–∞—é—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –∫–æ—Ç–æ—Ä–∞—è –º–æ–≥–ª–∞ –±—ã –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∏—Ö –¥–∞–ª—å–Ω–µ–π—à–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ø–æ—Å–ª–µ —Å–æ–≤–µ—Ä—à–µ–Ω–∏—è –∫–∞–∫–∏—Ö-–ª–∏–±–æ —à–∞–≥–æ–≤. –°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:</p>\n<p><strong>1. –û–¥–Ω–æ–ø—É—Ç–µ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ (Single-path Reasoning):</strong></p>\n<ul>\n<li>–ó–∞–¥–∞—á–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —à–∞–≥ –≤–µ–¥–µ—Ç —Ç–æ–ª—å–∫–æ –∫ –æ–¥–Ω–æ–º—É —Å–ª–µ–¥—É—é—â–µ–º—É.</li>\n<li>LLM —Å–ª–µ–¥—É—é—Ç —ç—Ç–∏–º —à–∞–≥–∞–º –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –∫–æ–Ω–µ—á–Ω–æ–π —Ü–µ–ª–∏.</li>\n<li><strong>Chain of Thought (CoT)</strong>: –≤ –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—Ç LLM –Ω–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è.</li>\n<li><strong>Zero-shot-CoT</strong>: LLM –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—Ä–∞–∑—ã-—Ç—Ä–∏–≥–≥–µ—Ä—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, \"–¥—É–º–∞–π —à–∞–≥ –∑–∞ —à–∞–≥–æ–º\". –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç CoT, –∑–¥–µ—Å—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã –≤ –ø–æ–¥—Å–∫–∞–∑–∫–∞—Ö.</li>\n<li><strong>RePrompting</strong>: –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –ø–ª–∞–Ω–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–∞–∂–¥—ã–π —à–∞–≥ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º —É—Å–ª–æ–≤–∏—è–º. –ï—Å–ª–∏ —à–∞–≥ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –∏ LLM –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω.</li>\n<li><strong>ReWOO</strong>: –ø–ª–∞–Ω—ã –æ—Ç–¥–µ–ª—è—é—Ç—Å—è –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π. –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –ø–ª–∞–Ω—ã, –∑–∞—Ç–µ–º –ø–æ–ª—É—á–∞—é—Ç—Å—è –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –∏ –≤ –∫–æ–Ω—Ü–µ –æ–Ω–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.</li>\n<li><strong>HuggingGPT</strong>: –∑–∞–¥–∞—á–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø–æ–¥—Ü–µ–ª–∏, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–µ—à–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ—Å—É—Ä—Å–æ–≤ Huggingface.</li>\n</ul>\n<p><strong>2. –ú–Ω–æ–≥–æ–ø—É—Ç–µ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ (Multi-path Reasoning):</strong></p>\n<ul>\n<li>–®–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –≤ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –ö–∞–∂–¥—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —à–∞–≥ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö.</li>\n<li>–≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –≥–¥–µ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ã–±–æ—Ä–∞.</li>\n<li><strong>Self-consistent CoT (CoT-SC)</strong>: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —É –∫–∞–∂–¥–æ–π —Å–ª–æ–∂–Ω–æ–π –ø—Ä–æ–±–ª–µ–º—ã –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ—à–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç CoT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—É—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–π—Å—è –æ—Ç–≤–µ—Ç.</li>\n<li><strong>Tree of Thoughts (ToT)</strong>: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–ª–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ö–∞–∂–¥—ã–π —É–∑–µ–ª –¥–µ—Ä–µ–≤–∞ ‚Äì —ç—Ç–æ \"–º—ã—Å–ª—å\", —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º—É —à–∞–≥—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í—ã–±–æ—Ä —à–∞–≥–æ–≤ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ—Ü–µ–Ω–∫–µ LLM. –§–∏–Ω–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–∞ –≤ —à–∏—Ä–∏–Ω—É (BFS) –∏–ª–∏ –≤ –≥–ª—É–±–∏–Ω—É (DFS).</li>\n<li><strong>RecMind</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–±—Ä–æ—à–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</li>\n<li><strong>GoT</strong>: —Ä–∞—Å—à–∏—Ä—è–µ—Ç –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É ToT –¥–æ –≥—Ä–∞—Ñ–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –º–æ—â–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–¥—Å–∫–∞–∑–æ–∫.</li>\n<li><strong>AoT</strong>: –≤–∫–ª—é—á–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –≤ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è LLM.</li>\n<li>–í [54] LLM –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏ \"–Ω—É–ª–µ–≤–æ–≥–æ –≤—ã—Å—Ç—Ä–µ–ª–∞\". –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤, –∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π.</li>\n<li>–í [55] —É–ª—É—á—à–∞–µ—Ç—Å—è [54] –ø—É—Ç–µ–º –≤–∫–ª—é—á–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤, —Å—Ö–æ–∂–∏—Ö —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ –ø–æ–¥—Å–∫–∞–∑–∫–∞—Ö.</li>\n<li><strong>RAP</strong>: —Å—Ç—Ä–æ–∏—Ç –º–æ–¥–µ–ª—å –º–∏—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –≤—ã–≥–æ–¥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS).</li>\n</ul>\n<p><strong>3. –í–Ω–µ—à–Ω–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ (External Planner):</strong></p>\n<ul>\n<li>LLM —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ–±—â–∏–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –Ω–æ –º–æ–≥—É—Ç –∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</li>\n<li>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–Ω–µ—à–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–∏—Å–∫–∞.</li>\n<li><strong>LLM+P</strong>: –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è –∑–∞–¥–∞—á –≤ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —è–∑—ã–∫ PDDL, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–µ—à–Ω–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDDL. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –≤ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫.</li>\n<li><strong>LLM-DP</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏—Ä–∞ –∏ —Ü–µ–ª–µ–π –≤ PDDL. –ó–∞—Ç–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤–Ω–µ—à–Ω–µ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫—É.</li>\n<li><strong>CO-LLM</strong>: LLM –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–ª–∞–Ω–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∞ –≤–Ω–µ—à–Ω–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ.</li>\n</ul>\n<h3>–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é</h3>\n<p>–í —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∞–≥–µ–Ω—Ç–∞–º —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏–∑-–∑–∞ —Ç—Ä—É–¥–Ω–æ—Å—Ç–µ–π —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ –∏ –∏–∑-–∑–∞ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ —Å—Ä–µ–¥—ã. –ü–æ—ç—Ç–æ–º—É –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —É–º–µ—Ç—å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–≤–æ–∏ –ø–ª–∞–Ω—ã, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –º–æ–∂–µ—Ç –ø–æ—Å—Ç—É–ø–∞—Ç—å –æ—Ç —Å—Ä–µ–¥—ã, –ª—é–¥–µ–π –∏–ª–∏ –º–æ–¥–µ–ª–µ–π.</p>\n<p><strong>1. –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç —Å—Ä–µ–¥—ã (Environmental Feedback):</strong></p>\n<ul>\n<li>–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø–æ—Å—Ç—É–ø–∞–µ—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞. –≠—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–∏–≥–Ω–∞–ª—ã –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –∏–ª–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ—Å–ª–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞.</li>\n<li><strong>ReAct</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–æ–π–∫–∏ \"–º—ã—Å–ª—å-–¥–µ–π—Å—Ç–≤–∏–µ-–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ\" (thought-act-observation) –≤ –ø–æ–¥—Å–∫–∞–∑–∫–∞—Ö. \"–ú—ã—Å–ª—å\" –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, \"–¥–µ–π—Å—Ç–≤–∏–µ\" ‚Äì –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏ –∞–≥–µ–Ω—Ç–∞, –∞ \"–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ\" ‚Äì –∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–π—Å—Ç–≤–∏–π, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑–≤–Ω–µ.</li>\n<li><strong>Voyager</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–∏ —Ç–∏–ø–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —Å—Ä–µ–¥—ã: –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã, –æ—à–∏–±–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏.</li>\n<li><strong>Ghost</strong>: —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Å—Ä–µ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å—Ä–µ–¥—ã –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å–ø–µ—Ö–µ –∏–ª–∏ –Ω–µ—É–¥–∞—á–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è.</li>\n<li><strong>SayPlan</strong>: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Å–∏–º—É–ª—è—Ç–æ—Ä–∞ –≥—Ä–∞—Ñ–∞ —Å—Ü–µ–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π.</li>\n<li><strong>DEPS</strong>: —É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É DEPS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏—á–∏–Ω–∞—Ö –æ—à–∏–±–æ–∫.</li>\n</ul>\n<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –º–æ–¥—É–ª—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–∞–∫ —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é, —Ç–∞–∫ –∏ –±–µ–∑ –Ω–µ–µ, –ø–æ–∑–≤–æ–ª—è—é—Ç –∞–≥–µ–Ω—Ç–∞–º —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —à–∏—Ä–æ–∫–∏–º —Å–ø–µ–∫—Ç—Ä–æ–º –∑–∞–¥–∞—á –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Å—Ä–µ–¥–∞–º.</p>"
            },
            {
                "title": "Capabilities of LLMs in Task Execution and Planning",
                "content": "e is designed to encode the input text, while the other is responsible for matching the query statements. ViperGPT [74] firstly uses Codex, which is implemented based on language model, to generate Python code from text descriptions, and then executes the code to complete the given tasks. TPTU [67] incorporates various LLMs to accomplish wide range of language generation tasks such as generating code, producing lyrics, and more. ChemCrow [75] is an LLM-based chemical agent designed to perform tasks in organic synthesis, drug discovery, and material design. It utilizes seventeen expert-designed models to assist its operations. 16 Front. Comput. Sci., 2024, 0(0): 142 MM-REACT [76] integrates various external models, such as VideoBERT for video summarization, X-decoder for image generation, and SpeechBERT for audio processing, enhancing its capability in diverse multimodal scenarios. Internal Knowledge. In addition to utilizing external tools, many agents rely solely on the internal knowledge of LLMs to guide their actions. We now present several crucial capabilities of LLMs that can support the agent to behave reasonably and effectively. (1) Planning Capability. Previous work has demonstrated that LLMs can be used as decent planers to decompose complex task into simpler ones [45]. Such capability of LLMs can be even triggered without incorporating examples in the prompts [46]. Based on the planning capability of LLMs, DEPS [33] develops Minecraft agent, which can solve complex task via sub-goal decomposition. Similar agents like GITM [16] and Voyager [38] also heavily rely on the planning capability of LLMs to successfully complete different tasks. (2) Conversation Capability. LLMs can usually generate high-quality conversations. This capability enables the agent to behave more like humans. In the previous work, many agents take actions based on the strong conversation capability of LLMs. For example, in ChatDev [18], different agents can discuss the software developing process, and even can make reflections on their own behaviors. In RLP [30], the agent can communicate with the listeners based on their potential feedback on the agents utterance. (3) Common Sense Understanding Capability. Another important capability of LLMs is that they can well comprehend human common sense. Based on this capability, many agents can simulate human daily life and make human-like decisions. For example, in Generative Agent, the agent can accurately understand its current state, the surrounding environment, and summarize highlevel ideas based on basic observations. Without the common sense understanding capability of LLMs, these behaviors cannot be reliably simulated. Similar conclusions may also apply to RecAgent [21] and S3 [77], where the agents aim to simulate user recommendation and social behaviors. Action Impact: Action impact refers to the consequences of the action. In fact, the action impact can encompass numerous instances, but for brevity, we only provide few examples. (1) Changing Environments. Agents can directly alter environment states by actions, such as moving their positions, collecting items, constructing buildings, etc. For instance, in GITM [16] and Voyager [38], the environments are changed by the actions of the agents in their task completion process. For example, if the agent mines three woods, then they may disappear in the environments. (2) Altering Internal States. Actions taken by the agent can also change the agent itself, including updating memories, forming new plans, acquiring novel knowledge, and more. For example, in Generative Agents [20], memory streams are updated after performing actions within the system. SayCan [78] enables agents to take actions to update understandings of the environment. (3) Triggering New Actions. In the task completion process, one agent action can be triggered by another one. For example, Voyager [38] constructs buildings once it has gathered all the necessary resources. 2.2 Agent Capability Acquisition In the above sections, we mainly focus on how to design the agent architecture to better inspire the capability of LLMs to make it qualified for accomplishing tasks like humans. The architecture functions as the hardware of the agent. However, relying solely on the hardware is insufficient for Lei Wang et al. Survey on Large Language Model based Autonomous Agents Table 1 For the profile module, we use ‚ë†, ‚ë° and ‚ë¢ to represent the handcrafting method, LLM-generation method, and dataset alignment method, respectively. For the memory module, we focus on the implementation strategies for memory operation and memory structure. For memory operation, we use ‚ë† and ‚ë° to indicate that the model only has read/write operations and has read/write/reflection operations, respectively. For memory structure, we use ‚ë† and ‚ë° to represent unified and hybrid memories, respectively. For the planning module, we use ‚ë† and ‚ë° to represent planning w/o feedback and w/ feedback, respectively. For the action module, we use ‚ë† and ‚ë° to represent that the model does not use tools and use tools, respectively. For the agent capability acquisition (CA) strategy, we use ‚ë† and ‚ë° to represent the methods with and without fine-tuning, respectively. - indicates that the corresponding content is not explicitly discussed in the paper. Model Profile WebGPT [66] SayCan [78] MRKL [72] Inner Monologue [61] Social Simulacra [79] ReAct [59] MALLM [43] DEPS [33] Toolformer [15] Reflexion [12] CAMEL [80] API-Bank [69] ViperGPT [74] HuggingGPT [13] Generative Agents [20] LLM+P [57] ChemCrow [75] OpenAGI [73] AutoGPT [81] SCM [35] Socially Alignment [82] GITM [16] Voyager [38] Introspective Tips [83] RET-LLM [42] ChatDB [40] 3 [77] ChatDev [18] ToolLLM [14] MemoryBank [39] MetaGPT [23] - - - - ‚ë° - - - - ‚ë† ‚ë° - - - ‚ë† - - - - - - - - - - - ‚ë¢ ‚ë† - - ‚ë† Memory Operation - - - - - - ‚ë† - - ‚ë° - - - - ‚ë° - - - ‚ë† ‚ë° ‚ë† ‚ë° ‚ë° - ‚ë† ‚ë† ‚ë° ‚ë° - ‚ë° ‚ë° Structure - - - - - - ‚ë° - - ‚ë° - - - ‚ë† ‚ë° - - - ‚ë° ‚ë° ‚ë° ‚ë° ‚ë° - ‚ë° ‚ë° ‚ë° ‚ë° - ‚ë° ‚ë° Planning Action CA - ‚ë† ‚ë† ‚ë° - ‚ë° - ‚ë° ‚ë† ‚ë° ‚ë° ‚ë° - ‚ë† ‚ë° ‚ë† ‚ë° ‚ë° ‚ë° - - ‚ë° ‚ë° ‚ë° - ‚ë° - ‚ë° ‚ë° - ‚ë° ‚ë° ‚ë† ‚ë° ‚ë† ‚ë† ‚ë° ‚ë† ‚ë† ‚ë° ‚ë† ‚ë† ‚ë° ‚ë° ‚ë° ‚ë† ‚ë† ‚ë° ‚ë° ‚ë° ‚ë† ‚ë† ‚ë† ‚ë† ‚ë† ‚ë† ‚ë° ‚ë† ‚ë† ‚ë° ‚ë† ‚ë° ‚ë† ‚ë° - ‚ë° - ‚ë† - ‚ë° ‚ë† ‚ë° - ‚ë° - - - - - ‚ë† ‚ë° - ‚ë† ‚ë° ‚ë° ‚ë° ‚ë† - - ‚ë° ‚ë† - - Time 12/2021 04/2022 05/2022 07/2022 08/2022 10/2022 01/2023 02/2023 02/2023 03/2023 03/2023 04/2023 03/2023 03/2023 04/2023 04/2023 04/2023 04/2023 04/2023 04/2023 05/2023 05/2023 05/2023 05/2023 05/2023 06/2023 07/2023 07/2023 07/2023 07/2023 08/2023 achieving effective task performance. This is because the agent may lack the necessary task-specific capabilities, skills and experiences, which can be regarded as \"software\" resources. In order to equip the agent with these resources, various strategies have been devised. Generally, we categorize these strategies into two classes based on whether they require fine-tuning of the LLMs. In the following, we introduce each of them more in detail. Capability Acquisition with Fine-tuning: straightforward method to enhance the agent capability for task completion is fine-tuning the agent based on task-dependent datasets. Generally, the datasets can be constructed based on human annotation, LLM generation or collected from real-world applications. In the following, we introduce these methods more in detail. Fine-tuning with Human Annotated Datasets. 18 Front. Comput. Sci., 2024, 0(0): 142 To fine-tune the agent, utilizing human annotated datasets is versatile approach that can be employed in various application scenarios. In this approach, researchers first design annotation tasks and then recruit workers to complete them. For example, in CoH [84], the authors aim to align LLMs with human values and preferences. Different from the other models, where the human feedback is leveraged in simple and symbolic manner, this method converts the human feedback into detailed comparison information in the form of natural languages. The LLMs are directly fine-tuned based on these natural language datasets. In RET-LLM [42], in order to better convert natural languages into structured memory information, the authors fine-tune LLMs based on human constructed dataset, where each sample is triplet-natural language pair. In WebShop [85], the authors collect 1.18 million realworld products form amazon.com, and put them onto simulated e-commerce website, which contains several carefully designed human shopping scenarios. Based on this website, the authors recruit 13 workers to collect real-human behavior dataset. At last, three methods based on heuristic rules, imitation learning and reinforcement learning are trained based on this dataset. Although the authors do not fine-tune LLM-based agents, we believe that the dataset proposed in this paper holds immense potential to enhance the capabilities of agents In EduChat [86], in the field of web shopping. the authors aim to enhance the educational functions of LLMs, such as open-domain question answering, essay assessment, Socratic teaching, and emotional support. They fine-tune LLMs based on human annotated datasets that cover various educational scenarios and tasks. These datasets are manually evaluated and curated by psychology experts and frontline teachers. SWIFTSAGE [87] is an agent influenced by the dual-process theory of human cognition [88], which is effective for solving complex interactive reasoning tasks. In this agent, the SWIFT module constitutes compact encoderdecoder language model, which is fine-tuned using human-annotated datasets. Fine-tuning with LLM Generated Datasets. Building human annotated dataset needs to recruit people, which can be costly, especially when one needs to annotate large amount of samples. Considering that LLMs can achieve human-like capabilities in wide range of tasks, natural idea is using LLMs to accomplish the annotation task. While the datasets produced from this method can be not as perfect as the human annotated ones, it is much cheaper, and can be leveraged to generate more samples. For example, in ToolBench [14], to enhance the tool-using capability of open-source LLMs, the authors collect 16,464 real-world APIs spanning 49 categories from the RapidAPI Hub. They used these APIs to prompt ChatGPT to generate diverse instructions, covering both single-tool and multitool scenarios. Based on the obtained dataset, the authors fine-tune LLaMA [9], and obtain significant performance improvement in terms of tool using. In [82], to empower the agent with social capability, the authors design sandbox, and deploy multiple agents to interact with each other. Given social question, the central agent first generates initial responses. Then, it shares the responses to its nearby agents for collecting their feedback. Based on the feedback as well as its detailed explanations, the central agent revise its initial responses to make them more consistent with social norms. In this process, the authors collect large amount of agent social interaction data, which is then leveraged to fine-tune the LLMs. Fine-tuning with Real-world Datasets. In Lei Wang et al. Survey on Large Language Model based Autonomous Agents 19 Fig. 4 Illustration of transitions in strategies for acquiring model capabilities. addition to building datasets based on human or LLM annotation, directly using real-world datasets to fine-tune the agent is also common strategy. For example, in MIND2WEB [89], the authors collect large amount of real-world datasets to enhance the agent capability in the web domain. In contrast to prior studies, the dataset presented in this paper encompasses diverse tasks, real-world scenarios, and comprehensive user interaction patterns. Specifically, the authors collect over 2,000 open-ended tasks from 137 real-world websites spanning 31 domains. Using this dataset, the authors fine-tune LLMs to enhance their performance on web-related tasks, including movie discovery and ticket booking, among others. In SQL-PALM [90], researchers fine-tune PaLM-2 based on cross-domain largescale text-to-SQL dataset called Spider. The obtained model can achieve significant performance improvement on text-to-SQL tasks. Capability Acquisition without Fine-tuning: In the era of tradition machine learning, the model capability is mainly acquired by learning from datasets, where the knowledge is encoded into the model parameters. In the era of LLMs, the model capability can be acquired either by training/fine-tuning the model parameters or designing delicate prompts (i.e., prompt engineer). In prompt engineer, one needs to write valuable information into the prompts to enhance the model capability or unleash existing LLM capabilities. In the era of agents, the model capability can be acquired based on three strategies: (1) model fine-tuning, (2) prompt engineer and (3) designing proper agent evolution mechanisms (we called it as mechanism engineering). Mechanism engineering is broad concept that involves developing specialized modules, introducing novel working rules, and other strategies to enhance agent capabilities. For clearly understanding such transitions on the strategy of model capability acquisition, we illustrate them in Figure 4. In the following, we introduce prompting engineering and mechanism engineering for agent capability acquisition. Prompting Engineering. Due to the strong language comprehension capabilities, people can directly interact with LLMs using natural languages. This introduces novel strategy for enhancing agent capabilities, that is, one can describe the desired capability using natural language and then use it as prompts to influence LLM actions. For example, in CoT [45], in order to empower the agent with the capability for complex task reasoning, the authors present the intermediate reasoning steps as few-shot 20 Front. Comput. Sci., 2024, 0(0): 1 examples in the prompt. Similar techniques are also used in CoT-SC [49] and ToT [50]. In SocialAGI [30], in order to enhance the agent selfawareness capability in conversation, the authors prompt LLMs with the agent beliefs about the mental states of the listeners and itself, which makes the generated utterance more engaging and adaptive. In addition, the authors also incorporate the target mental states of the listeners, which enables the agents to make more strategic plans. Retroformer [91] presents retrospective model that enables the agent to generate reflections on its past failures. The reflections are integrated into the prompt of LLMs to guide the agents future actions. Additionally, this model utilizes reinforcement learning to iteratively improve the retrospective model, thereby refining the LLM prompt. Mechanism Engineering. Unlike model finetuning and prompt engineering, mechanism engineering is unique strategy to enhance agent capability. In the following, we present several representative methods of mechanism engineering. (1) Trial-and-error. In this method, the agent first performs an action, and subsequently, pre-defined critic is invoked to judge the action. If the action is deemed unsatisfactory, then the agent reacts by incorporating the critics feedback. In RAH [92], the agent serves as user assistant in recommender systems. One of the agents crucial roles is to simulate human behavior and generate responses on behalf of the user. To fulfill this objective, the agent first generates predicted response and then compares it with the real human feedback. If the predicted response and the real human feedback differ, the critic generates failure information, which is subsequently incorporated into the agents next action. In DEPS [33], the agent first designs plan to accomplish given task. In the plan execution process, if an action fails, the explainer generates specific details explaining the cause of the failure. This information is then incorporated by the agent to redesign the plan. In RoCo [93], the agent first proposes sub-task plan and path of 3D waypoints for each robot in multi-robot collaboration task. The plan and waypoints are then validated by set of environment checks, such as collision detection and inverse kinematics. If any of the checks fail, the feedback is appended to each agents prompt and another round of dialog begins. The agents use LLMs to discuss and improve their plan and waypoints until they pass all validations. In PREFER [94], the agent first evaluates its performance on subset of data. If it fails to solve certain examples, LLMs are leveraged to generate feedback information reflecting on the reasons of the failure. Based on this feedback, the agent improves itself by iteratively refining its actions. (2) Crowd-sourcing. In [95], the authors design debating mechanism that leverages the wisdom of crowds to enhance agent capabilities. To begin with, different agents provide separate responses to given question. If their responses are not consistent, they will be prompted to incorporate the solutions from other agents and provide an updated response. This iterative process continues until reaching final consensus answer. In this method, the capability of each agent is enhance by understanding and incorporating the other agents opinions. (3) Experience Accumulation. In GITM [16], the agent does not know how to solve task in the beginning. Then, it makes explorations, and once it has successfully accomplished task, the actions used in this task are stored into the agent memory. In the future, if the agent encounters similar task, then the relevant memories are extracted to complete the current task. In this process, the improved agent caLei Wang et al. Survey on Large Language Model based Autonomous Agents 21 pability comes from the specially designed memory accumulation and utilization mechanisms. In Voyager [38], the authors equip the agent with skill library, and each skill in the library is represented by executable codes. In the agent-environment interaction process, the codes for each skill will be iteratively refined according to the environment feedback and the agent self-verification results. After period of execution, the agent can successfully complete different tasks efficiently by accessing the skill library. In AppAgent [96], the agent is designed to interact with apps in manner akin to human users, learning through both autonomous exploration and observation of human demonstrations. Throughout this process, it constructs knowledge base, which serves as reference for performing intricate tasks across various applications on mobile phone. In MemPrompt [97], the users are requested to provide feedback in natural language regarding the problemsolving intentions of the agent, and this feedback is stored in memory. When the agent encounters similar tasks, it attempts to retrieve related memories to generate more suitable responses. (4) Self-driven Evolution. In LMA3 [98], the agent can autonomously set goals for itself, and gradually improve its capability by exploring the environment and receiving feedback from reward function. Following this mechanism, the agent can acquire knowledge and develop capabilities according to its own preferences. In SALLM-MS [99], by integrating advanced large language models like GPT-4 into multi-agent system, agents can adapt and perform complex tasks, showcasing advanced communication capabilities, thereby realizing selfdriven evolution in their interactions with the environment. In CLMTWA [100], by using large language model as teacher and weaker language model as student, the teacher can generate and communicate natural language explanations to improve the students reasoning skills via theory of mind. The teacher can also personalize its explanations for the student and intervene only when necessary, based on the expected utility of intervention. In NLSOM [101], different agents communicate and collaborate through natural language to solve tasks that single agent cannot solve. This can be seen as form of self-driven learning, utilizing the exchange of information and knowledge between multiple agents. However, unlike other models such as LMA3, SALLM-MS, and CLMTWA, NLSOM allows for dynamic adjustment of agent roles, tasks, and relationships based on the task requirements and feedback from other agents or the environment. Remark. Upon comparing the aforementioned strategies for agent capability acquisition, we can find that the fine-tuning method improves the agent capability by adjusting model parameters, which can incorporate large amount of task-specific knowledge, but is only suitable for open-source LLMs. The method without fine-tuning usually enhances the agent capability based on delicate prompting strategies or mechanism engineering. They can be used for both openand closed-source LLMs. However, due to the limitation of the input context window of LLMs, they cannot incorporate too much task information. In addition, the designing spaces of the prompts and mechanisms are extremely large, which makes it not easy to find optimal solutions. In the above sections, we have detailed the construction of LLM-based agents, where we focus on two aspects including the architecture design and capability acquisition. We present the correspondence between existing work and the above taxonomy in Table 1. It should be noted that, for the sake of integrity, we have also incorporated several studies, which do not explicitly mention LLM-based agents 22 Front. Comput. Sci., 2024, 0(0): 142 Fig. 5 The applications (left) and evaluation strategies (right) of LLM-based agents. but are highly related to this area.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>\n<p>–í —ç—Ç–æ–π —á–∞—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –≠—Ç–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –æ–ø–æ—Ä–∞ –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è LLM.</p>\n<p><strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:</strong></p>\n<p>–ú–Ω–æ–≥–∏–µ –∞–≥–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤–Ω–µ—à–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Å–≤–æ–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ü—Ä–∏–º–µ—Ä—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ul>\n<li><strong>ViperGPT:</strong> –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ Python –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á.</li>\n<li><strong>TPTU:</strong> –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ LLM –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è –∫–æ–¥, —Ç–µ–∫—Å—Ç—ã –ø–µ—Å–µ–Ω –∏ —Ç.–¥.</li>\n<li><strong>ChemCrow:</strong> –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π 17 —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</li>\n<li><strong>MM-REACT:</strong> –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏–µ –∫–∞–∫ VideoBERT –¥–ª—è –≤–∏–¥–µ–æ, X-decoder –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ SpeechBERT –¥–ª—è –∞—É–¥–∏–æ.</li>\n</ul>\n<p><strong>–û–ø–æ—Ä–∞ –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è LLM:</strong></p>\n<p>–ü–æ–º–∏–º–æ –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –∞–≥–µ–Ω—Ç—ã —Ç–∞–∫–∂–µ –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–Ω–∞–Ω–∏—è LLM, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∏–º —Ä–∞–∑—É–º–Ω–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ. –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ul>\n<li><strong>–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> LLM –º–æ–≥—É—Ç —Ä–∞–∑–±–∏–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ DEPS, GITM –∏ Voyager, —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É—è –∏—Ö –Ω–∞ –ø–æ–¥—Ü–µ–ª–∏.</li>\n<li><strong>–î–∏–∞–ª–æ–≥:</strong> LLM —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏, –ø–æ–∑–≤–æ–ª—è—è –∞–≥–µ–Ω—Ç–∞–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å, –∫–∞–∫ –ª—é–¥–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ ChatDev –∞–≥–µ–Ω—Ç—ã –æ–±—Å—É–∂–¥–∞—é—Ç –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –∞ –≤ RLP –∞–≥–µ–Ω—Ç –æ–±—â–∞–µ—Ç—Å—è —Å–æ —Å–ª—É—à–∞—Ç–µ–ª—è–º–∏, —É—á–∏—Ç—ã–≤–∞—è –∏—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Ä–µ–∞–∫—Ü–∏—é.</li>\n<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞:</strong> LLM —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –∂–∏–∑–Ω—å –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Generative Agent, RecAgent –∏ S3, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –∏–º–∏—Ç–∏—Ä—É—é—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.</li>\n</ul>\n<p><strong>–í–ª–∏—è–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π:</strong></p>\n<p>–î–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è, –≤–∫–ª—é—á–∞—è:</p>\n<ul>\n<li><strong>–ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:</strong> –ê–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∏–∑–º–µ–Ω—è—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã, –ø–µ—Ä–µ–º–µ—â–∞—è—Å—å, —Å–æ–±–∏—Ä–∞—è —Ä–µ—Å—É—Ä—Å—ã –∏–ª–∏ —Å—Ç—Ä–æ—è –æ–±—ä–µ–∫—Ç—ã (–∫–∞–∫ –≤ GITM –∏ Voyager).</li>\n<li><strong>–ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è:</strong> –î–µ–π—Å—Ç–≤–∏—è –º–æ–≥—É—Ç –∏–∑–º–µ–Ω—è—Ç—å —Å–∞–º–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –æ–±–Ω–æ–≤–ª—è—è –µ–≥–æ –ø–∞–º—è—Ç—å, —Ñ–æ—Ä–º–∏—Ä—É—è –Ω–æ–≤—ã–µ –ø–ª–∞–Ω—ã –∏–ª–∏ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—è –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è (–∫–∞–∫ –≤ Generative Agents –∏ SayCan).</li>\n<li><strong>–ó–∞–ø—É—Å–∫ –Ω–æ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π:</strong> –û–¥–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –¥—Ä—É–≥–æ–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ (–∫–∞–∫ –≤ Voyager).</li>\n</ul>\n<p><strong>–ü—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–∞–º–∏:</strong></p>\n<p>–í –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∞—Å—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–æ–≤, –Ω–æ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–∞–º —Ç–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ \"–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ\" - —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ –∏ –æ–ø—ã—Ç. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: —Å —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π LLM –∏ –±–µ–∑ –Ω–µ–µ.</p>\n<p><strong>–ü—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Å —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π:</strong></p>\n<p>–û–¥–Ω–∏–º –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∑–∞–≤–∏—Å—è—â–∏—Ö –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é:</p>\n<ul>\n<li><strong>–†–∞–∑–º–µ—Ç–∫–∞ –ª—é–¥—å–º–∏:</strong><ul>\n<li>–í <strong>CoH</strong> —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ –≤–∏–¥–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.</li>\n<li>–í <strong>RET-LLM</strong> LLM –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ —Ç—Ä–∏–ø–ª–µ—Ç–æ–≤ \"–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —è–∑—ã–∫ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–º—è—Ç—å\".</li>\n<li>–í <strong>WebShop</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ª—é–¥–µ–π, —Å–æ–±—Ä–∞–Ω–Ω—ã–π –Ω–∞ –∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–µ–±-—Å–∞–π—Ç–µ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –∫–æ–º–º–µ—Ä—Ü–∏–∏.</li>\n<li>–í <strong>EduChat</strong> LLM –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏.</li>\n<li><strong>SWIFTSAGE</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –º–æ–¥–µ–ª—å –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞-–¥–µ–∫–æ–¥–µ—Ä–∞, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏ –¥–∞–Ω–Ω—ã—Ö.</li>\n</ul>\n</li>\n<li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM:</strong><ul>\n<li>–≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –¥–µ—à–µ–≤–ª–µ, —á–µ–º —Ä–∞–∑–º–µ—Ç–∫–∞ –ª—é–¥—å–º–∏, –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ –æ–±—Ä–∞–∑—Ü–æ–≤, —Ö–æ—Ç—è –∏ –Ω–µ —Ç–∞–∫–∏—Ö —Ç–æ—á–Ω—ã—Ö.</li>\n<li>–ù–∞–ø—Ä–∏–º–µ—Ä, –≤ <strong>ToolBench</strong> –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ LLM.</li>\n</ul>\n</li>\n</ul>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ LLM, –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ —Å–ø–æ—Å–æ–±—ã –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ç–æ—á–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.</p>"
            },
            {
                "title": "Llm-Based Autonomous Agent Ap",
                "content": "plication Owing to the strong language comprehension, complex task reasoning, and common sense understanding capabilities, LLM-based autonomous agents have shown significant potential to influence multiple domains. This section provides succinct summary of previous studies, categorizing them according to their applications in three distinct areas: social science, natural science, and engineering (see the left part of Figure 5 for global overview). 3.1 Social Science Social science is one of the branches of science, devoted to the study of societies and the relationships among individuals within those societies. LLMbased autonomous agents can promote this domain by leveraging their impressive human-like understanding, thinking and task solving capabilities. In the following, we discuss several key areas that can be affected by LLM-based autonomous agents. Psychology: For the domain of psychology, LLMbased agents can be leveraged for conducting simulation experiments, providing mental health support and so on [102105]. For example, in [102], the authors assign LLMs with different profiles, and let them complete psychology experiments. From the results, the authors find that LLMs are capable of generating results that align with those from studies involving human participants. Additionally, it was observed that larger models tend to deliver more accurate simulation results compared to their smaller counterparts. An interesting discovery is that, in many experiments, models like ChatGPT and GPT-4 can provide too perfect estimates (called hyper-accuracy distortion), which may influence the downstream applications. In [104], the authors systematically analyze the effectiveness of LLMbased conversation agents for mental well-being support. They collect 120 posts from Reddit, and find that such agents can help users cope with anxieties, social isolation and depression on demand. At the same time, they also find that the agents may produce harmful contents sometimes. Political Science and Economy: LLM-based agents can also be utilized to study political science and economy [29, 105, 106]. In [29], LLM-based agents are utilized for ideology detection and predicting voting patterns. In [105], the authors focuses on understanding the discourse structure and persuasive elements of political speech through the assistance of LLM-based agents. In [106], LLMLei Wang et al. Survey on Large Language Model based Autonomous Agents 23 based agents are provided with specific traits such as talents, preferences, and personalities to explore human economic behaviors in simulated scenarios. Social Simulation: Previously, conducting experiments with human societies is often expensive, unethical, or even infeasible. With the ever prospering of LLMs, many people explore to build virtual environment with LLM-based agents to simulate social phenomena, such as the propagation of harmful information, and so on [20, 34, 77, 79, 107110]. For example, Social Simulacra [79] simulates an online social community and explores the potential of utilizing agent-based simulations to aid decision-makers to improve community regulations. [107,108] investigates the potential impacts of different behavioral characteristics of LLM-based agents in social networks. Generative Agents [20] and AgentSims [34] construct multiple agents in virtual town to simulate the human daily life. SocialAI School [109] employs LLM-based agents to simulate and investigate the fundamental social cognitive skills during the course of child development. S3 [77] builds social network simulator, focusing on the propagation of information, emotion and attitude. CGMI [111] is framework for multi-agent simulation. CGMI maintains the personality of the agents through tree structure and constructs cognitive model. The authors simulated classroom scenario using CGMI. Jurisprudence: LLM-based agents can serve as aids in legal decision-making processes, facilitating more informed judgements [112, 113]. Blind Judgement [113] employs several language models to simulate the decision-making processes of multiple judges. It gathers diverse opinions and consolidates the outcomes through voting mechanism. ChatLaw [112] is prominent Chinese legal model based on LLM. It adeptly supports both database and keyword search strategies, specifically designed to mitigate the hallucination issue prevalent in such models. In addition, this model also employs self-attention mechanism to enhance the LLMs capability via mitigating the impact of reference inaccuracies. Research Assistant: Beyond their application in specialized domains, LLM-based agents are increasingly adopted as versatile assistants in the broad field of social science research [105, 114]. In [105], LLM-based agents offer multifaceted assistance, ranging from generating concise article abstracts and extracting pivotal keywords to crafting detailed scripts for studies, showcasing their ability to enrich and streamline the research process. Meanwhile, in [114], LLM-based agents serve as writing assistant, demonstrating their capability to identify novel research inquiries for social scientists, thereby opening new avenues for exploration and innovation in the field. These examples highlight the potential of LLM-based agents in enhancing the efficiency, creativity, and breadth of social science research. 3.2 Natural Science Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. With the ever prospering of LLMs, the application of LLM-based agents in natural sciences becomes more and more popular. In the following, we present many representative areas, where LLM-based agents can play important roles. Documentation and Data Management: Natural scientific research often involves the collection, organization, and synthesis of substantial amounts of literature, which requires significant dedication of time and human resources. LLM-based agents Front. Comput. Sci., 2024, 0(0): 142 have shown strong capabilities on language understanding and employing tools such as the internet and databases for text processing. These capabilities empower the agent to excel in tasks related to documentation and data management [75, 115, 116]. In [115], the agent can efficiently query and utilize internet information to complete tasks such as question answering and experiment planning. ChatMOF [116] utilizes LLMs to extract important information from text descriptions written by humans. It then formulates plan to apply relevant tools for predicting the properties and structures of metal-organic frameworks. ChemCrow [75] utilizes chemistry-related databases to both validate the precision of compound representations and identify potentially dangerous substances. This functionality enhances the reliability and comprehensiveness of scientific inquiries by ensuring the accuracy of the data involved. Experiment Assistant: LLM-based agents have the ability to independently conduct experiments, making them valuable tools for supporting scientists in their research projects [75,115]. For instance, [115] introduces an innovative agent system that utilizes LLMs for automating the design, planning, and execution of scientific experiments. This system, when provided with the experimental objectives as input, accesses the Internet and retrieves relevant documents to gather the necessary information. It subsequently utilizes Python code to conduct essential calculations and carry out the following experiments. ChemCrow [75] incorporates 17 carefully developed tools that are specifically designed to assist researchers in their chemical research. Once the input objectives are received, ChemCrow provides valuable recommendations for experimental procedures, while also emphasizing any potential safety risks associated with the proposed experiments. Natural Science Education: LLM-based agents can communicate with humans fluently, often being utilized to develop agent-based educational tools [115, 117119]. For example, [115] develops agent-based education systems to facilitate students learning of experimental design, methodologies, and analysis. The objective of these systems is to enhance students critical thinking and problem-solving skills, while also fostering deeper comprehension of scientific principles. Math Agents [117] can assist researchers in exploring, discovering, solving and proving mathematical problems. Additionally, it can communicate with humans and aids them in understanding and using mathematics. [118] utilize the capabilities of CodeX [119] to automatically solve and explain university-level mathematical problems, which can be used as education tools to teach students and researchers. CodeHelp [120] is an education agent for programming. It offers many useful features, such as setting course-specific keywords, monitoring student queries, and providing feedback to the system. EduChat [86] is an LLM-based agent designed specifically for the education domain. It provides personalized, equitable, and empathetic educational support to teachers, students, and parents through dialogue. FreeText [121] is an agent that utilizes LLMs to automatically assess students responses to open-ended questions and offer feedback. 3.3 Engineering LLM-based autonomous agents have shown great potential in assisting and enhancing engineering research and applications. In this section, we review and summarize the applications of LLM-based agents in several major engineering domains. Civil Engineering: In civil engineering, LLMbased agents can be used to design and optimize Lei Wang et al. Survey on Large Language Model based Autonomous Agents 25 Table 2 Representative applications of LLM-based autonomous agents. Domain Work Social Science Natural Science TE [102], Akata et al. [103], Ziems et al. [105], Ma et al. [104] Out of One [29], Horton [106], Ziems et al. [105] [79], GenSocial Simulacra erative Agents [20], SocialAI School [109], AgentSims [34], S3 [77], Williams et al. [110], Li et al. [107], Chao et al. [108] ChatLaw [112], Blind Judgement [113] Ziems et al [105], Bail et al. [114] ChemCrow [75], Boiko et al. [115] ChemCrow [75], Boiko et al. [115], Grossmann et al. [122] ChemCrow [75], CodeHelp [120], Boiko et [115], MathAal. gent [117], Drori et al. [118] Psychology Political Science and Economy Social Simulation Jurisprudence Research Assistant Documentation and Data Management Experiment Assistant Natural Science Education [70], [24], RecMind SelfRestGPT collaboration SQLPALM [90], RAH [92], DB- [51], GPT [41], ChatEDA [123], InteRecA- [124], PentestGPT [125], gent SmolModCodeHelp els [126], DemoGPT [127], GPTEngineer [128] GPT4IA [129], TaskMatrix.AI [71] IELLM [130], [120], ProAgent [131], LLM4RL [132], PET [133], REMEMBERER [134], DEPS [33], Unified Agent [135], SayCan [78], LMMWM [136], TidyBot [137], RoCo [93], SayPlan [31] Engineering CS & SE Industrial Automation Robotics & Embodied AI complex structures such as buildings, bridges, dams, roads, etc. [138] proposes an interactive framework where human architects and agents collaborate to construct structures in 3D simulation environment. The interactive agent can understand natural language instructions, place blocks, detect confusion, seek clarification, and incorporate human feedback, showing the potential for human-AI collaboration in engineering design. Computer Science & Software Engineering: In the field of computer science and software engineering, LLM-based agents offer potential for automating coding, testing, debugging, and documentation generation [14, 18, 23, 24, 126128]. ChatDev [18] proposes an end-to-end framework, where multiple agent roles communicate and collaborate through natural language conversations to complete the software development life cycle. This framework demonstrates efficient and cost-effective generation of executable software systems. ToolBench [14] can be used for tasks such as code auto-completion and code recommendation. MetaGPT [23] abstracts multiple roles, such as product managers, architects, project managers, and engineers, to supervise code generation process and enhance the quality of the final output code. This enables low-cost software [24] presents self-collaboration development. framework for code generation using LLMs. In this framework, multiple LLMs are assumed to be distinct \"experts\" for specific sub-tasks. They collaborate and interact according to specified instructions, forming virtual team that facilitates each others work. Ultimately, the virtual team collaboratively addresses code generation tasks without requiring human intervention. LLIFT [139] employs LLMs to assist in conducting static analysis, specifically for identifying potential code vulnerabilities. This approach effectively manages the trade-off between accuracy and scalability. ChatEDA [123] is an agent developed for electronic design automation (EDA) to streamline the design process by integrating task planning, script generation, and execution. CodeHelp [120] is an agent designed to assist students and developers in debugging and testing their 26 Front. Comput. Sci., 2024, 0(0): 1 code. Its features include providing detailed explanations of error messages, suggesting potential fixes, and ensuring the accuracy of the code. PENTESTGPT [125] is penetration testing tool based on LLMs, which can effectively identify common vulnerabilities, and interpret source code to develop exploits. DB-GPT [41] utilizes the capabilities of LLMs to systematically assess potential root causes of anomalies in databases. Through the implementation of tree of thought approach, DB-GPT enables LLMs to backtrack to previous steps in case the current step proves unsuccessful, thus enhancing the accuracy of the diagnosis process. Industrial Automation: In the field of industrial automation, LLM-based agents can be used to achieve intelligent planning and control of production processes. [129] proposes novel framework that integrates large language models (LLMs) with digital twin systems to accommodate flexible production needs. The framework leverages prompt engineering techniques to create LLM agents that can adapt to specific tasks based on the information provided by digital twins. These agents can coordinate series of atomic functionalities and skills to complete production tasks at different levels within the automation pyramid. This research demonstrates the potential of integrating LLMs into industrial automation systems, providing innovative solutions for more agile, flexible and adaptive production processes. IELLM [130] showcases case study on LLMs role in the oil and gas industry, covering applications like rock physics, acoustic reflectometry, and coiled tubing control. Robotics & Embodied Artificial Intelligence: Recent works have developed more efficient reinforcement learning agents for robotics and embodied artificial intelligence [16, 38, 78, 132135, 140 143]. The focus is on enhancing autonomous agents abilities for planning, reasoning, and collaboration in embodied environments. In specific, [140] proposes unified agent system for embodied reasoning and task planning. In this system, the authors design high-level commands to enable improved planning while propose low-level controllers to translate commands into actions. Additionally, one can leverage dialogues to gather information [141] to accelerate the optimization process. [142, 143] employ autonomous agents for embodied decision-making and exploration. To overcome the physical constraints, the agents can generate executable plans and accomplish long-term tasks by leveraging multiple skills. In terms of control policies, SayCan [78] focuses on investigating wide range of manipulation and navigation skills utilizing mobile manipulator robot. Taking inspiration from typical tasks encountered in kitchen environment, it presents comprehensive set of 551 skills that cover seven skill families and 17 objects. These skills encompass various actions such as picking, placing, pouring, grasping, and manipulating objects, among others. TidyBot [137] is an embodied agent designed to personalize household cleanup tasks. It can learn users preferences on object placement and manipulation methods through textual examples. To promote the application of LLM-based autonomous agents, researchers have also introduced many open-source libraries, based on which the developers can quickly implement and evaluate agents according to their customized requirements [19, 81, 127, 144157]. For example, LangChain [149] is an open-source framework that automates coding, testing, debugging, and documentation generation tasks. By integrating language models with data sources and facilitating interaction with the environment, LangChain enables efficient and cost-effective software development through natural language comLei Wang et al. Survey on Large Language Model based Autonomous Agents 27 munication and collaboration among multiple agent roles. Based on LangChain, XLang [147] comes with comprehensive set of tools, complete user interface, and support three different agent scenarios, namely data processing, plugin usage, and web agent. AutoGPT [81] is an agent that is fully automated. It sets one or more goals, breaks them down into corresponding tasks, and cycles through the tasks until the goal is achieved. WorkGPT [150] is an agent framework similar to AutoGPT and LangChain. By providing it with an instruction and set of APIs, it engages in back-and-forth conversations with AI until the instruction is completed. GPT-Engineer [128], SmolModels [126] and DemoGPT [127] are open-source projects that focus on automating code generation through prompts to complete development tasks. AGiXT [146] is dynamic AI automation platform designed to orchestrate efficient AI command management and task execution across many providers. AgentVerse [19] is versatile framework that facilitates researchers in creating customized LLM-based agent simulations efficiently. GPT Researcher [152] is an experimental application that leverages large language models to efficiently develop research questions, trigger web crawls to gather information, summarize sources, and aggregate summaries. BMTools [153] is an open-source repository that extends LLMs with tools and provides platform for communitydriven tool building and sharing. It supports various types of tools, enables simultaneous task execution using multiple tools, and offers simple interface for loading plugins via URLs, fostering easy development and contribution to the BMTools ecosystem. Remark. Utilization of LLM-based agents in supporting above applications may also entail risks and challenges. On one hand, LLMs themselves may be susceptible to illusions and other issues, occasionally providing erroneous answers, leading to incorrect conclusions, experimental failures, or even posing risks to human safety in hazardous experiments. Therefore, during experimentation, users must possess the necessary expertise and knowledge to exercise appropriate caution. On the other hand, LLM-based agents could potentially be exploited for malicious purposes, such as the development of chemical weapons, necessitating the implementation of security measures, such as human alignment, to ensure responsible and ethical use. In summary, in the above sections, we introduce the typical applications of LLM-based autonomous agents in three important domains. To facilitate clearer understanding, we have summarized the relationship between previous studies and their respective applications in Table 2.",
                "summary": "<h2>–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –Ω–∞—É–∫–∏</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –Ω–∞—É–∫–∏. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–≤–∏—Ç—ã–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É, —Ç–∞–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫–∞—Ö.</p>\n<h3>3.1 –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—É–∫–∏</h3>\n<p>–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—É–∫–∏ –∏–∑—É—á–∞—é—Ç –æ–±—â–µ—Å—Ç–≤–æ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É –ª—é–¥—å–º–∏. LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –≤–Ω–µ—Å—Ç–∏ –≤–∫–ª–∞–¥ –≤ —ç—Ç—É –æ–±–ª–∞—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º –∫ —á–µ–ª–æ–≤–µ–∫–æ–ø–æ–¥–æ–±–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é, –º—ã—à–ª–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á.</p>\n<p><strong>–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è:</strong> –í –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∏–º–∏—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –æ–∫–∞–∑–∞–Ω–∏—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Å–æ–∑–¥–∞–≤–∞–ª–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏ –∏ –¥–∞–≤–∞–ª–∏ –∏–º –∑–∞–¥–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –≤—ã–ø–æ–ª–Ω—è—é—Ç —É—á–∞—Å—Ç–Ω–∏–∫–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ LLM —Å–ø–æ—Å–æ–±–Ω—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ö–æ–∂–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —É—á–∞—Å—Ç–∏–µ–º –ª—é–¥–µ–π. –ë–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —ç—Ç–æ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ ChatGPT –∏ GPT-4, –∏–Ω–æ–≥–¥–∞ –≤—ã–¥–∞–≤–∞–ª–∏ —Å–ª–∏—à–∫–æ–º —Ç–æ—á–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∏—Ö –¥–∞–ª—å–Ω–µ–π—à–µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ. –¢–∞–∫–∂–µ –∏–∑—É—á–∞–ª–∞—Å—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–≤ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è. –ë—ã–ª–æ –ø–æ–∫–∞–∑–∞–Ω–æ, —á—Ç–æ –æ–Ω–∏ –º–æ–≥—É—Ç –ø–æ–º–æ–≥–∞—Ç—å –ª—é–¥—è–º —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å —Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å—é, —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π –∏–∑–æ–ª—è—Ü–∏–µ–π –∏ –¥–µ–ø—Ä–µ—Å—Å–∏–µ–π. –û–¥–Ω–∞–∫–æ, –∏–Ω–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.</p>\n<p><strong>–ü–æ–ª–∏—Ç–æ–ª–æ–≥–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∫–∞:</strong> LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –û–Ω–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–¥–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è. –¢–∞–∫–∂–µ –æ–Ω–∏ –ø–æ–º–æ–≥–∞—é—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—á–µ–π. –í —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö LLM-–∞–≥–µ–Ω—Ç–∞–º –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏—Å–≤–æ–µ–Ω—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —á–µ—Ä—Ç—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.</p>\n<p><strong>–°–æ—Ü–∏–∞–ª—å–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —É—á–∞—Å—Ç–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π —á–∞—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–º, –Ω–µ—ç—Ç–∏—á–Ω—ã–º –∏–ª–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã–º. LLM-–∞–≥–µ–Ω—Ç—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ä–µ–¥—ã –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —è–≤–ª–µ–Ω–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.  –†–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–º—É–ª—è—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏—Ä—É—é—Ç –æ–Ω–ª–∞–π–Ω-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞, –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –∂–∏–∑–Ω—å –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–∞—Ö, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ –¥–∞–∂–µ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ —É –¥–µ—Ç–µ–π. –≠—Ç–∏ —Å–∏–º—É–ª—è—Ü–∏–∏ –ø–æ–º–æ–≥–∞—é—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã.</p>\n<p><strong>–Æ—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏—è:</strong> LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –≤ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ, –ø–æ–º–æ–≥–∞—è –ø—Ä–∏–Ω–∏–º–∞—Ç—å –±–æ–ª–µ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è. –û–Ω–∏ –º–æ–≥—É—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π —Å—É–¥—å—è–º–∏, —Å–æ–±–∏—Ä–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–Ω–µ–Ω–∏—è –∏ –æ–±—ä–µ–¥–∏–Ω—è—è –∏—Ö —Å –ø–æ–º–æ—â—å—é –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è. –¢–∞–∫–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∏—Å–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –∞ —Ç–∞–∫–∂–µ –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—É—é –¥–ª—è —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π.</p>\n<p><strong>–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è:</strong> LLM-–∞–≥–µ–Ω—Ç—ã –≤—Å–µ —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–º–æ—â–Ω–∏–∫–æ–≤ –≤ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –û–Ω–∏ –º–æ–≥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∫ —Å—Ç–∞—Ç—å—è–º, –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ –¥–∞–∂–µ –ø–æ–º–æ–≥–∞—Ç—å –≤ –ø–æ–∏—Å–∫–µ –Ω–æ–≤—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –≠—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ —à–∏—Ä–æ—Ç—É —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.</p>\n<h3>3.2 –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É–∫–∏</h3>\n<p>–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—É–∫–∏ –∏–∑—É—á–∞—é—Ç –ø—Ä–∏—Ä–æ–¥–Ω—ã–µ —è–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—É—Ç–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. LLM-–∞–≥–µ–Ω—Ç—ã –Ω–∞—Ö–æ–¥—è—Ç –≤—Å–µ –±–æ–ª—å—à–µ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.</p>\n<p><strong>–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã–º–∏:</strong> –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É—é—Ç —Å–±–æ—Ä–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã, —á—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤. LLM-–∞–≥–µ–Ω—Ç—ã –æ–±–ª–∞–¥–∞—é—Ç —Ä–∞–∑–≤–∏—Ç—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞ –∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã–º–∏. –û–Ω–∏ –º–æ–≥—É—Ç –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã. –¢–∞–∫–∂–µ LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤–æ–π—Å—Ç–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ–Ω–∏ –º–æ–≥—É—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –≤—ã—è–≤–ª—è—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –æ–ø–∞—Å–Ω—ã–µ –≤–µ—â–µ—Å—Ç–≤–∞.</p>\n<p><strong>–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞:</strong> LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö —Ü–µ–Ω–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è —É—á–µ–Ω—ã—Ö. –û–Ω–∏ –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –ü–æ–ª—É—á–∏–≤ –Ω–∞ –≤—Ö–æ–¥ —Ü–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞, –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Ä–∞—Å—á–µ—Ç—ã –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.  –¢–∞–∫–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –≤ —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –ø—Ä–æ—Ü–µ–¥—É—Ä–∞–º –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—è –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–∏—Å–∫–∞—Ö.</p>\n<p><strong>–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–Ω–∞—É—á–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ:</strong> LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Å–≤–æ–±–æ–¥–Ω–æ –æ–±—â–∞—Ç—å—Å—è —Å –ª—é–¥—å–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –û–Ω–∏ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å —Å—Ç—É–¥–µ–Ω—Ç–∞–º –≤ –∏–∑—É—á–µ–Ω–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞, –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞, —Ä–∞–∑–≤–∏–≤–∞—è –∏—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –Ω–∞–≤—ã–∫–∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º. –°—É—â–µ—Å—Ç–≤—É—é—Ç –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Ä–µ—à–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, –æ–±—ä—è—Å–Ω—è—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –¥–∞–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞—Ç—å –∏ –æ–±—ä—è—Å–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è. –¢–∞–∫–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –æ–±—É—á–∞—é—â–∏–µ –∞–≥–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∏–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∞–≥–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é, —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤—É—é –∏ —ç–º–ø–∞—Ç–∏—á–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É —É—á–∏—Ç–µ–ª—è–º, —É—á–µ–Ω–∏–∫–∞–º –∏ —Ä–æ–¥–∏—Ç–µ–ª—è–º. –ù–∞–∫–æ–Ω–µ—Ü, LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∏–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å.</p>\n<h3>3.3 –ò–Ω–∂–µ–Ω–µ—Ä–Ω–æ–µ –¥–µ–ª–æ</h3>\n<p>–ê–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∏ –±–æ–ª—å—à–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ –æ–∫–∞–∑–∞–Ω–∏–∏ –ø–æ–º–æ—â–∏ –∏ —É–ª—É—á—à–µ–Ω–∏–∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–æ–∫. –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p><strong>–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ:</strong> –í –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–º —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ LLM-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.</p>"
            },
            {
                "title": "Llm-Based Autonomous Agent Eval",
                "content": "uation Similar to LLMs themselves, evaluating the effectiveness of LLM-based autonomous agents is challenging task. This section outlines two prevalent approaches to evaluation: subjective and objective methods. For comprehensive overview, please refer to the right portion of Figure 5. 4.1 Subjective Evaluation Subjective evaluation measures the agent capabilities based on human judgements [20,22,29,79,158]. It is suitable for the scenarios where there are no evaluation datasets or it is very hard to design quantitative metrics, for example, evaluating the agents intelligence or user-friendliness. In the following, we present two commonly used strategies for subjective evaluation. Human Annotation: This evaluation method involves human evaluators directly scoring or ranking Front. Comput. Sci., 2024, 0(0): 142 the outputs generated by various agents [22,29,105]. For example, in [20], the authors employ many annotators, and ask them to provide feedback on five key questions that directly associated with the agent capability. Similarly, [159] assess model effectiveness by having human participants rate the models on harmlessness, honesty, helpfulness, engagement, and unbiasedness, subsequently comparing these scores across different models. In [79], annotators are asked to determine whether the specifically designed models can significantly enhance the development of rules within online communities. Turing Test: This evaluation strategy necessitates that human evaluators differentiate between outputs produced by agents and those created by humans. If, in given task, the evaluators cannot separate the agent and human results, it demonstrates that the agent can achieve human-like performance on this task. For instance, researchers in [29] conduct experiments on free-form Partisan text, and the human evaluators are asked to guess whether the responses are from human or LLM-based agent. In [20], the human evaluators are required to identify whether the behaviors are generated from the agents or real-humans. In EmotionBench [160], human annotations are collected to compare the emotional states expressed by LLM software and human participants across various scenarios. This comparison serves as benchmark for evaluating the emotional intelligence of the LLM software, illustrating nuanced approach to understanding agent capabilities in mimicking human-like performance and emotional expression. Remark. LLM-based agents are usually designed to serve humans. Thus, subjective agent evaluation plays critical role, since it reflects human criterion. However, this strategy also faces issues such as high costs, inefficiency, and population bias. To address these issues, growing number of researchers are investigating the use of LLMs themselves as intermediaries for carrying out these subjective assessments. For example, in ChemCrow [75], researchers assess the experimental results using GPT. They consider both the completion of tasks and the accuracy of the underlying processes. Similarly, ChatEval [161] introduces novel approach by employing multiple agents to critique and assess the results generated by various candidate models in structured debate format. This innovative use of LLMs for evaluation purposes holds promise for enhancing both the credibility and applicability of subjective assessments in the future. As LLM technology continues to evolve, it is anticipated that these methods will become increasingly reliable and find broader applications, thereby overcoming the current limitations of direct human evaluation. 4.2 Objective Evaluation Objective evaluation refers to assessing the capabilities of LLM-based autonomous agents using quantitative metrics that can be computed, compared and tracked over time. In contrast to subjective evaluation, objective metrics aim to provide concrete, measurable insights into the agent performance. For conducting objective evaluation, there are three important aspects, that is, the evaluation metrics, protocols and benchmarks. In the following, we introduce these aspects more in detail. Metrics: In order to objectively evaluate the effectiveness of the agents, designing proper metrics is significant, which may influence the evaluation accuracy and comprehensiveness. Ideal evaluation metrics should precisely reflect the quality of the agents, and align with the human feelings when using them in real-world scenarios. In existing work, we can conclude the following representative evaluLei Wang et al. Survey on Large Language Model based Autonomous Agents 29 Table 3 For subjective evaluation, we use ‚ë† and ‚ë° to represent human annotation and the Turing test, respectively. For objective evaluation, we use ‚ë†, ‚ë°, ‚ë¢, and ‚ë£ to represent environment simulation, social evaluation, multi-task evaluation, and software testing, respectively. indicates that the evaluations are based on benchmarks. Benchmark - - - - - - - - - - - - - Model WebShop [85] Social Simulacra [79] TE [102] LIBRO [162] ReAct [59] Out of One, Many [29] DEPS [33] Jalil et al. [163] Reflexion [12] IGLU [138] Generative Agents [20] ToolBench [153] GITM [16] Two-Failures [164] Voyager [38] SocKET [165] MobileEnv [166] Clembench [167] Dialop [168] Feldt et al. [169] CO-LLM [22] Tachikuma [170] WebArena [171] RocoBench [93] AgentSims [34] AgentBench [172] BOLAA [173] Gentopia [174] EmotionBench [160] PTB [125] Subjective - ‚ë† - - - ‚ë° - - - - ‚ë† ‚ë° - - - - - - - - - ‚ë† ‚ë† - - - - - - ‚ë† - Objective ‚ë† ‚ë¢ ‚ë° ‚ë° ‚ë£ ‚ë† ‚ë° ‚ë¢ ‚ë† ‚ë£ ‚ë† ‚ë¢ ‚ë† - ‚ë¢ ‚ë† ‚ë¢ ‚ë† ‚ë° ‚ë¢ ‚ë† ‚ë¢ ‚ë† ‚ë¢ ‚ë° ‚ë£ ‚ë† ‚ë† ‚ë† ‚ë† ‚ë° ‚ë¢ ‚ë° ‚ë¢ ‚ë† ‚ë¢ ‚ë£ ‚ë¢ - ‚ë£ Time 07/2022 08/2022 08/2022 09/2022 10/2022 02/2023 02/2023 02/2023 03/2023 04/2023 04/2023 04/2023 05/2023 05/2023 05/2023 05/2023 05/2023 05/2023 06/2023 06/2023 07/2023 07/2023 07/2023 07/2023 08/2023 08/2023 08/2023 08/2023 08/2023 08/2023 ation metrics. (1) Task success metrics: These metrics measure how well an agent can complete tasks and achieve goals. Common metrics include success rate [12, 22, 57, 59], reward/score [22, 59, 138], coverage [16], and accuracy [18, 40, 102]. Higher values indicate greater task completion ability. (2) Human similarity metrics: These metrics quantify the degree to which the agent behaviors closely resembles that of humans. Typical examples include trajectory/location accuracy [38, 164], dialogue similarities [79, 102], and mimicry of human responses [29, 102]. Higher similarity suggests better human simulation performance. (3) Efficiency metrics: In contrast to the aforementioned metrics used to evaluate the agent effectiveness, these metrics aim to assess the efficiency of agent. Commonly considered metrics encompass the length of planning [57], the cost associated with development [18], the speed of inference [16, 38], and number of clarification dialogues [138]. Protocols: In addition to the evaluation metrics, another important aspect for objective evaluation is how to leverage these metrics. In the previous work, we can identify the following commonly used evaluation protocols: (1) Real-world simulation: In this method, the agents are evaluated within immersive environments like games and interactive simulators. The agents are required to perform tasks au30 Front. Comput. Sci., 2024, 0(0): 142 tonomously, and then metrics like task success rate and human similarity are leveraged to evaluate the capability of the agents based on their trajectories and completed objectives [16, 22, 33, 38, 59, 85, 138, 164, 166, 170]. This method is expected to evaluate the agents practical capabilities in real-world scenarios. (2) Social evaluation: This method utilizes metrics to assess social intelligence based on the agent interactions in simulated societies. Various approaches have been adopted, such as collaborative tasks to evaluate teamwork skills, debates to analyze argumentative reasoning, and human studies to measure social aptitude [34, 79, 102, 165, 173]. These approaches analyze qualities such as coherence, theory of mind, and social IQ to assess agents capabilities in areas including cooperation, communication, empathy, and mimicking human social behavior. By subjecting agents to complex interactive settings, social evaluation provides valuable insights into agents higher-level social cognition. (3) Multi-task evaluation: In this method, people use set of diverse tasks from different domains to evaluate the agent, which can effectively measure the agent generalization capability in open-domain environments [29, 85, 153, 165, 166, 172, 173]. (4) Software testing: In this method, researchers evaluate the agents by letting them conduct tasks such as software testing tasks, such as generating test cases, reproducing bugs, debugging code, and interacting with developers and external tools [162, 163, 169, 173]. Then, one can use metrics like test coverage and bug detection rate to measure the effectiveness of LLM-based agents. Benchmarks: Given the metrics and protocols, crucial remaining aspect is the selection of an appropriate benchmark for conducting the evaluation. In the past, people have used various benchmarks in their experiments. For example, many researchers use simulation environments like ALFWorld [59], IGLU [138], and Minecraft [16, 33, 38] as benchmarks to evaluate the agent capabilities. Tachikuma [170] is benchmark that leverages TRPG game logs to evaluate LLMs ability to understand and infer complex interactions with multiple characters and novel objects. AgentBench [172] provides comprehensive framework for evaluating LLMs as autonomous agents across diverse environments. It represents the first systematic assessment of LLMs as agents on real-world challenges across diverse domains. SocKET [165] is comprehensive benchmark for evaluating the social capabilities of LLMs across 58 tasks covering five categories of social information such as humor and sarcasm, emotions and feelings, credibility, etc. AgentSims [34] is versatile framework for evaluating LLM-based agents, where one can flexibly design the agent planning, memory and action strategies, and measure the effectiveness of different agent modules in interactive environments. ToolBench [153] is an open-source project that aims to support the development of powerful LLMs with general toolIt provides an open platform for use capability. training, serving, and evaluating LLMs based on tool learning. WebShop [85] develops benchmark for evaluating LLM-based agents in terms of their capabilities for product search and retrieval. The benchmark is constructed using collection of 1.18 million real-world items. Mobile-Env [166] is an extendable interactive platform which can be used to evaluate the multi-step interaction capabilities of LLM-based agents. WebArena [171] offers comprehensive website environment that spans multiple domains. Its purpose is to evaluate agents in an end-to-end fashion and determine the accuracy of their completed tasks. GentBench [174] is benchmark designed to evaluate the agent capaLei Wang et al. Survey on Large Language Model based Autonomous Agents 31 bilities, including their reasoning, safety, and efficiency, when utilizing tools to complete complex tasks. RocoBench [93] is benchmark with six tasks evaluating multi-agent collaboration across diverse scenarios, emphasizing communication and coordination strategies to assess adaptability and generalization in cooperative robotics. EmotionBench [160] evaluates the emotion appraisal ability of LLMs, i.e., how their feelings change when presented with specific situations. It collects over 400 situations that elicit eight negative emotions and measures the emotional states of LLMs and human subjects using self-report scales. PEB [125] is benchmark tailored for assessing LLM-based agents in penetration testing scenarios, comprising It of13 diverse targets from leading platforms. fers structured evaluation across varying difficulty levels, reflecting real-world challenges for agents. ClemBench [167] contains five Dialogue Games to assess LLMs ability as player. E2E [175] is an end-to-end benchmark for testing the accuracy and usefulness of chatbots. Remark. Objective evaluation facilitates the quantitative analysis of capabilities in LLM-based agents through variety of metrics. While current techniques can not perfectly measure all types of agent capabilities, objective evaluation provides essential insights that complement subjective assessment. Continued advancements in benchmarks and methodologies for objective evaluation will enhance the development and understanding of LLM-based autonomous agents further. In the above sections, we introduce both subjective and objective strategies for LLM-based autonomous agents evaluation. The evaluation of the agents play significant roles in this domain. However, both subjective and objective evaluation have their own strengths and weakness. Maybe, in practice, they should be combined to comprehensively evaluate the agents. We summarize the correspondence between the previous work and these evaluation strategies in Table 3.",
                "summary": "<h2>–û—Ü–µ–Ω–∫–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>\n<p>–ö–∞–∫ –∏ —Å–∞–º–∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –æ—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ LLM, —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ—Ü–µ–Ω–∫–µ: —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–π.</p>\n<h3>–°—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</h3>\n<p>–°—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–∑–º–µ—Ä—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–æ–∫ –ª—é–¥–µ–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–∏—Ç—É–∞—Ü–∏–π, –∫–æ–≥–¥–∞ –Ω–µ—Ç –≥–æ—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–ª–∏ —Ç—Ä—É–¥–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∞–≥–µ–Ω—Ç–∞ –∏–ª–∏ –µ–≥–æ —É–¥–æ–±—Å—Ç–≤–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–ø–æ—Å–æ–±–∞ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏:</p>\n<ol>\n<li><strong>–û—Ü–µ–Ω–∫–∞ –ª—é–¥—å–º–∏:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ª—é–¥–∏-–æ—Ü–µ–Ω—â–∏–∫–∏ –Ω–∞–ø—Ä—è–º—É—é –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏–ª–∏ —Ä–∞–Ω–∂–∏—Ä—É—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ—Ü–µ–Ω—â–∏–∫–∏ –º–æ–≥—É—Ç –¥–∞–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ –∫–ª—é—á–µ–≤—ã–º –≤–æ–ø—Ä–æ—Å–∞–º, —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∞–≥–µ–Ω—Ç–∞, –∏–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∞–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º, –∫–∞–∫ –±–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç—å, —á–µ—Å—Ç–Ω–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –¢–∞–∫–∂–µ, –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –º–æ–≥—É—Ç –ø—Ä–æ—Å–∏—Ç—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –º–æ–∂–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —É–ª—É—á—à–∏—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –ø—Ä–∞–≤–∏–ª –≤ –æ–Ω–ª–∞–π–Ω-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö.</li>\n<li><strong>–¢–µ—Å—Ç –¢—å—é—Ä–∏–Ω–≥–∞:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –ª—é–¥–∏-–æ—Ü–µ–Ω—â–∏–∫–∏ —Ä–∞–∑–ª–∏—á–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç–∞–º–∏, –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏. –ï—Å–ª–∏ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –Ω–µ –º–æ–≥—É—Ç –æ—Ç–ª–∏—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≥–µ–Ω—Ç–∞ –æ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö, —ç—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –∞–≥–µ–Ω—Ç –¥–æ—Å—Ç–∏–≥ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –ø—Ä–æ—Å—è—Ç —É–≥–∞–¥–∞—Ç—å, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ç–µ–∫—Å—Ç –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –∏–ª–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –¢–∞–∫–∂–µ, –æ—Ü–µ–Ω—â–∏–∫–∏ –º–æ–≥—É—Ç –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, –±—ã–ª–∏ –ª–∏ –¥–µ–π—Å—Ç–≤–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∞–≥–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ª—é–¥—å–º–∏.</li>\n</ol>\n<p>–°—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–≥—Ä–∞–µ—Ç –≤–∞–∂–Ω—É—é —Ä–æ–ª—å, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–º–µ–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤—ã—Å–æ–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å, –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã, –≤—Å–µ –±–æ–ª—å—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑—É—á–∞—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∞–º–∏—Ö LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ—Å—Ä–µ–¥–Ω–∏–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫. –ù–∞–ø—Ä–∏–º–µ—Ä, LLM –º–æ–≥—É—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, —É—á–∏—Ç—ã–≤–∞—è –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á, —Ç–∞–∫ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –¢–∞–∫–∂–µ, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –∫—Ä–∏—Ç–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ–±–∞—Ç–æ–≤. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–≥—É—Ç –ø–æ–≤—ã—Å–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫.</p>\n<h3>–û–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</h3>\n<p>–û–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å, —Å—Ä–∞–≤–Ω–∏—Ç—å –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Å —Ç–µ—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π, –∏–∑–º–µ—Ä–∏–º–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞. –î–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω—ã —Ç—Ä–∏ –∞—Å–ø–µ–∫—Ç–∞: –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏, –ø—Ä–æ—Ç–æ–∫–æ–ª—ã –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏.</p>\n<p><strong>–ú–µ—Ç—Ä–∏–∫–∏:</strong> –î–ª—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤–∞–∂–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∞–≥–µ–Ω—Ç–æ–≤ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ—â—É—â–µ–Ω–∏—è–º –ª—é–¥–µ–π –ø—Ä–∏ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. –°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞ –º–µ—Ç—Ä–∏–∫:</p>\n<ol>\n<li><strong>–ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏:</strong> –ò–∑–º–µ—Ä—è—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –∞–≥–µ–Ω—Ç —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ü–µ–ª–µ–π. –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∫–ª—é—á–∞—é—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –Ω–∞–≥—Ä–∞–¥—É/–æ—Ü–µ–Ω–∫—É, –æ—Ö–≤–∞—Ç –∏ —Ç–æ—á–Ω–æ—Å—Ç—å. –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –±–æ–ª—å—à—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á.</li>\n<li><strong>–ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å —á–µ–ª–æ–≤–µ–∫–æ–º:</strong> –û–ø—Ä–µ–¥–µ–ª—è—é—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞. –ü—Ä–∏–º–µ—Ä—ã –≤–∫–ª—é—á–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏/–º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è, —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –∏–º–∏—Ç–∞—Ü–∏—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –ª—É—á—à–µ–π –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.</li>\n<li><strong>–ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:</strong> –û—Ü–µ–Ω–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞. –û–±—ã—á–Ω–æ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª–∏–Ω–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–≤–æ–¥–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤.</li>\n</ol>\n<p><strong>–ü—Ä–æ—Ç–æ–∫–æ–ª—ã:</strong> –ü–æ–º–∏–º–æ –º–µ—Ç—Ä–∏–∫, –≤–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏. –°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤:</p>\n<ol>\n<li><strong>–°–∏–º—É–ª—è—Ü–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞:</strong> –ê–≥–µ–Ω—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –≤ –∏–º–º–µ—Ä—Å–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–≥—Ä—ã –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —Å–∏–º—É–ª—è—Ç–æ—Ä—ã. –ê–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, –∞ –∑–∞—Ç–µ–º –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —á–µ–ª–æ–≤–µ–∫–æ–º, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.</li>\n<li><strong>–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:</strong> –ú–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ –≤ —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—â–µ—Å—Ç–≤–∞—Ö. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞–≤—ã–∫–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã, –¥–µ–±–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å —É—á–∞—Å—Ç–∏–µ–º –ª—é–¥–µ–π –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. –≠—Ç–∏ –ø–æ–¥—Ö–æ–¥—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ç–∞–∫–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞, –∫–∞–∫ —Å–≤—è–∑–Ω–æ—Å—Ç—å, —Ç–µ–æ—Ä–∏—è —Ä–∞–∑—É–º–∞ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ, –æ–±—â–µ–Ω–∏–µ, —ç–º–ø–∞—Ç–∏—è –∏ –∏–º–∏—Ç–∞—Ü–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.</li>\n<li><strong>–ú–Ω–æ–≥–æ–∑–∞–¥–∞—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞:</strong> –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á –∏–∑ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –µ–≥–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Å—Ä–µ–¥–∞—Ö.</li>\n<li><strong>–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è:</strong> –ê–≥–µ–Ω—Ç—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –æ—à–∏–±–æ–∫, –æ—Ç–ª–∞–¥–∫–∞ –∫–æ–¥–∞ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏ –∏ –≤–Ω–µ—à–Ω–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏. –ó–∞—Ç–µ–º –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—à–∏–±–æ–∫, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM.</li>\n</ol>\n<p><strong>–ë–µ–Ω—á–º–∞—Ä–∫–∏:</strong> –í–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏. –í –ø—Ä–æ—à–ª–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ä–µ–¥—ã –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ ALFWorld, IGLU –∏ Minecraft. –¢–∞–∫–∂–µ, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –±–µ–Ω—á–º–∞—Ä–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ª–æ–≥–∏ TRPG –∏–≥—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≤—ã–≤–æ–¥–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏ –∏ –æ–±—ä–µ–∫—Ç–∞–º–∏. AgentBench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö. SocKET - —ç—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π LLM, –∞ AgentSims - —ç—Ç–æ –≥–∏–±–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –≥–¥–µ –º–æ–∂–Ω–æ –≥–∏–±–∫–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤.</p>"
            },
            {
                "title": "Related Surveys",
                "content": "With the vigorous development of large language models, variety of comprehensive surveys have emerged, providing detailed insights into various aspects. [176] extensively introduces the background, main findings, and mainstream technologies of LLMs, encompassing vast array of existing works. On the other hand, [177] primarily focus on the applications of LLMs in various downstream tasks and the challenges associated with their deployment. Aligning LLMs with human intelligence is an active area of research to address concerns such as biases and illusions. [178] have compiled existing techniques for human alignment, including data collection and model training methodologies. Reasoning is crucial aspect of intelligence, influencing decision-making, problem-solving, and [179] presents the curother cognitive abilities. rent state of research on LLMs reasoning abilities, exploring approaches to improve and evaluate their reasoning skills. [180] propose that language models can be enhanced with reasoning capabilities and the ability to utilize tools, termed Augmented Language Models (ALMs). They conduct comprehensive review of the latest advancements in ALMs. As the utilization of large-scale models becomes more prevalent, evaluating their performance is increasingly critical. [181] shed light on evaluating LLMs, addressing what to evaluate, where to evaluate, and how to assess their performance in downstream tasks and societal impact. [182] also discusses the capabilities and limitations of 32 Front. Comput. Sci., 2024, 0(0): 142 LLMs in various downstream tasks. The aforementioned research encompasses various aspects of large models, including training, application, and evaluation. However, prior to this paper, no work has specifically focused on the rapidly emerging and highly promising field of LLM-based Agents. In this study, we have compiled 100 relevant works on LLM-based Agents, covering their construction, applications, and evaluation processes.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –∏ –ø–æ—è–≤–∏–ª–æ—Å—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±–∑–æ—Ä–æ–≤, –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Ö –∞—Å–ø–µ–∫—Ç—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ä–∞–±–æ—Ç–µ [176] –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —à–∏—Ä–æ–∫–∏–π –æ–±–∑–æ—Ä, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –ø—Ä–µ–¥—ã—Å—Ç–æ—Ä–∏—é, –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ LLM. –î—Ä—É–≥–æ–π –æ–±–∑–æ—Ä [177] —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ LLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏—Ö –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º.</p>\n<p>–í–∞–∂–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ LLM –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º, —á—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏. –í —Ä–∞–±–æ—Ç–µ [178] —Å–æ–±—Ä–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —ç—Ç–æ–≥–æ, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>\n<p>–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ ‚Äì –∫–ª—é—á–µ–≤–æ–π –∞—Å–ø–µ–∫—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –≤–ª–∏—è—é—â–∏–π –Ω–∞ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –∏ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á. –í –æ–±–∑–æ—Ä–µ [179] –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π LLM –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∞ —Ç–∞–∫–∂–µ –∏–∑—É—á–∞—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—ã –∫ —É–ª—É—á—à–µ–Ω–∏—é –∏ –æ—Ü–µ–Ω–∫–µ —ç—Ç–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π. [180] —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —É–ª—É—á—à–µ–Ω—ã –∑–∞ —Å—á–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –Ω–∞–∑—ã–≤–∞—è –∏—Ö \"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏\" (ALM). –í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ ALM.</p>\n<p>–ü–æ –º–µ—Ä–µ —Ç–æ–≥–æ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å–µ –±–æ–ª–µ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º, –æ—Ü–µ–Ω–∫–∞ –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ–π. –†–∞–±–æ—Ç–∞ [181] –ø—Ä–æ–ª–∏–≤–∞–µ—Ç —Å–≤–µ—Ç –Ω–∞ –æ—Ü–µ–Ω–∫—É LLM, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è, —á—Ç–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å, –≥–¥–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏ –∫–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∏—Ö —Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ. –í [182] —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è LLM –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è –∏—Ö –æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫—É. –û–¥–Ω–∞–∫–æ, –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏, –Ω–∏ –æ–¥–Ω–∞ —Ä–∞–±–æ—Ç–∞ –Ω–µ –±—ã–ª–∞ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–π—Å—è –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã —Å–æ–±—Ä–∞–ª–∏ 100 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–∞–±–æ—Ç –ø–æ –∞–≥–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM, –æ—Ö–≤–∞—Ç—ã–≤–∞—è –∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã –æ—Ü–µ–Ω–∫–∏.</p>"
            },
            {
                "title": "Challenges",
                "content": "While previous work on LLM-based autonomous agent has obtained many remarkable successes, this field is still at its initial stage, and there are several significant challenges that need to be addressed in its development. In the following, we present many representative challenges. 6.1 Role-playing Capability Different from traditional LLMs, autonomous agent usually has to play as specific roles (e.g., program coder, researcher and chemist) for accomplishing different tasks. Thus, the capability of the agent for role-playing is very important. Although LLMs can effectively simulate many common roles such as movie reviewers, there are still various roles and aspects that they struggle to capture accurately. To begin with, LLMs are usually trained based on webcorpus, thus for the roles which are seldom discussed on the web or the newly emerging roles, LLMs may not simulate them well. In addition, previous research [30] has shown that existing LLMs may not well model the human cognitive psychology characters, leading to the lack of self-awareness in conversation scenarios. Potential solution to these problems may include fine-tuning LLMs or carefully designing the agent prompts/architectures [183]. For example, one can firstly collect real-human data for uncommon roles or psychology characters, and then leverage it to fine-tune LLMs. However, how to ensure that fine-tuned model still perform well for the common roles may pose further challenges. Beyond fine-tuning, one can also design tailored agent prompts/architectures to enhance the capability of LLM on role-playing. However, finding the optimal prompts/architectures is not easy, since their designing spaces are too large. 6.2 Generalized Human Alignment Human alignment has been discussed lot for traditional LLMs. In the field of LLM-based autonomous agent, especially when the agents are leveraged for simulation, we believe this concept should be discussed more in depth. In order to better serve human-beings, traditional LLMs are usually finetuned to be aligned with correct human values, for example, the agent should not plan to make bomb for avenging society. However, when the agents are leveraged for real-world simulation, an ideal simulator should be able to honestly depict diverse human traits, including the ones with incorrect values. Actually, simulating the human negative aspects can be even more important, since an important goal of simulation is to discover and solve problems, and without negative aspects means no problem to be solved. For example, to simulate the real-world society, we may have to allow the agent to plan for making bomb, and observe how it will act to implement the plan as well as the influence of its behaviors. Based on these observations, people can make better actions to stop similar behaviors in realworld society. Inspired by the above case, maybe an important problem for agent-based simulation is how to conduct generalized human alignment, that is, for different purposes and applications, the Lei Wang et al. Survey on Large Language Model based Autonomous Agents 33 agent should be able to align with diverse human values. However, existing powerful LLMs including ChatGPT and GPT-4 are mostly aligned with unified human values. Thus, an interesting direction is how to realign these models by designing proper prompting strategies. 6.3 Prompt Robustness To ensure rational behavior in agents, its common practice for designers to embed supplementary modules, such as memory and planning modules, into LLMs. However, the inclusion of these modules necessitates the development of more complex prompts in order to facilitate consistent operation and effective communication. Previous research [184, 185] has highlighted the lack of robustness in prompts for LLMs, as even minor alterations can yield substantially different outcomes. This issue becomes more pronounced when constructing autonomous agents, as they encompass not single prompt but prompt framework that considers all modules, wherein the prompt for one module has the potential to influence others. Moreover, the prompt frameworks can vary significantly across different LLMs. The development of unified and resilient prompt framework applicable across diverse LLMs remains critical and unresolved challenge. There are two potential solutions to the aforementioned problems: (1) manually crafting the essential prompt elements through trial and error, or (2) automatically generating prompts using GPT. 6.4 Hallucination Hallucination poses fundamental challenge for LLMs, characterized by the models tendency to produce false information with high level of confidence. This challenge is not limited to LLMs alone but is also significant concern in the domain of autonomous agents. For instance, in [186], it was observed that when confronted with simplistic instructions during code generation tasks, the agent may exhibit hallucinatory behavior. Hallucination can lead to serious consequences such as incorrect or misleading code, security risks, and ethical issues [186]. To mitigate this issue, incorporating human correction feedback directly into the iterative process of human-agent interaction presents viable approach [23]. More discussions on the hallucination problem can be seen in [176].",
                "summary": "<h2>–ü—Ä–æ–±–ª–µ–º—ã –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π</h2>\n<p>–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É—Å–ø–µ—Ö–∏ –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ä—è–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è.</p>\n<p><strong>6.1. –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–æ–ª–µ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é</strong></p>\n<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö LLM, –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã —á–∞—Å—Ç–æ –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ä–æ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞, –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è –∏–ª–∏ —Ö–∏–º–∏–∫–∞) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á. –ü–æ—ç—Ç–æ–º—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ –∫ —Ä–æ–ª–µ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é –∏–º–µ–µ—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –•–æ—Ç—è LLM –º–æ–≥—É—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –º–Ω–æ–≥–∏–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–æ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫–∏–Ω–æ–∫—Ä–∏—Ç–∏–∫–∏, —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Ä–æ–ª–∏ –∏ –∞—Å–ø–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ –Ω–µ –º–æ–≥—É—Ç —Ç–æ—á–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏.</p>\n<ul>\n<li><strong>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö:</strong> LLM –æ–±—ã—á–Ω–æ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –≤–µ–±-–∫–æ—Ä–ø—É—Å–µ, –ø–æ—ç—Ç–æ–º—É —Ä–æ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–¥–∫–æ –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏–ª–∏ —è–≤–ª—è—é—Ç—Å—è –Ω–æ–≤—ã–º–∏, –º–æ–≥—É—Ç –±—ã—Ç—å –ø–ª–æ—Ö–æ —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω—ã.</li>\n<li><strong>–ù–µ—Ç–æ—á–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏:</strong> –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LLM –º–æ–≥—É—Ç –Ω–µ—Ç–æ—á–Ω–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ–ª–æ–≤–µ–∫–∞, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫—É —Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏—è –≤ –¥–∏–∞–ª–æ–≥–∞—Ö.</li>\n</ul>\n<p>–í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ LLM –∏–ª–∏ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–º–ø—Ç–æ–≤/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∞–≥–µ–Ω—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥—è—Ö –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Ä–æ–ª–µ–π –∏–ª–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è LLM. –û–¥–Ω–∞–∫–æ, –∫–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å, —á—Ç–æ–±—ã –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–ª–∞—Å—å —Å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ —Ä–æ–ª—è–º–∏, –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π. –ü–æ–º–∏–º–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è, –º–æ–∂–Ω–æ —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM –∫ —Ä–æ–ª–µ–≤–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é. –û–¥–Ω–∞–∫–æ –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤/–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω –∏–∑-–∑–∞ –∏—Ö –±–æ–ª—å—à–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞.</p>\n<p><strong>6.2. –û–±–æ–±—â–µ–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏</strong></p>\n<p>–°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º –¥–ª—è LLM. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è, —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ LLM –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—é—Ç—Å—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–≥–µ–Ω—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ –±–æ–º–±—ã. –û–¥–Ω–∞–∫–æ, –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞, –∏–¥–µ–∞–ª—å–Ω—ã–π —Å–∏–º—É–ª—è—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω —á–µ—Å—Ç–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã, –≤–∫–ª—é—á–∞—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–∞–∂–µ –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º, –ø–æ—Å–∫–æ–ª—å–∫—É –≤–∞–∂–Ω–∞—è —Ü–µ–ª—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º.</p>\n<ul>\n<li><strong>–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —á–µ—Ä—Ç:</strong> –î–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è —Ä–∞–∑—Ä–µ—à–∏—Ç—å –∞–≥–µ–Ω—Ç—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ –±–æ–º–±—ã, —á—Ç–æ–±—ã –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ –µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è–º–∏ –∏ –∏—Ö –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è–º–∏. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–æ–¥–æ–±–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ.</li>\n</ul>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤–∞–∂–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π —è–≤–ª—è–µ—Ç—Å—è <strong>–æ–±–æ–±—â–µ–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ —Ü–µ–Ω–Ω–æ—Å—Ç—è–º–∏</strong>, —Ç–æ –µ—Å—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–º —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Ü–µ–Ω–Ω–æ—Å—Ç—è–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ü–µ–ª–µ–π –∏ –∑–∞–¥–∞—á. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ—â–Ω—ã–µ LLM, —Ç–∞–∫–∏–µ –∫–∞–∫ ChatGPT –∏ GPT-4, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –µ–¥–∏–Ω—ã–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–µ —Ü–µ–Ω–Ω–æ—Å—Ç–∏. –ü–æ—ç—Ç–æ–º—É –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø—Ä–æ–º–ø—Ç–∏–Ω–≥–∞.</p>\n<p><strong>6.3. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –ø—Ä–æ–º–ø—Ç–æ–≤</strong></p>\n<p>–î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–∞–º—è—Ç—å –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ. –û–¥–Ω–∞–∫–æ —ç—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è LLM –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É—Å—Ç–æ–π—á–∏–≤—ã, —Ç–∞–∫ –∫–∞–∫ –¥–∞–∂–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–æ–≥—É—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–∞–∑–ª–∏—á–∏—è–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö. –≠—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞ —É—Å—É–≥—É–±–ª—è–µ—Ç—Å—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –Ω–µ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç, –∞ —Ü–µ–ª—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –≥–¥–µ –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –º–æ–¥—É–ª—è –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –¥—Ä—É–≥–∏–µ.</p>\n<ul>\n<li><strong>–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ —É—Å—Ç–æ–π—á–∏–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤:</strong> –°—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö LLM. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –µ–¥–∏–Ω–æ–π –∏ —É—Å—Ç–æ–π—á–∏–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤, –ø—Ä–∏–º–µ–Ω–∏–º–æ–π –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º LLM, –æ—Å—Ç–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ–π –∏ –Ω–µ—Ä–µ—à–µ–Ω–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π.</li>\n</ul>\n<p>–°—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–≤–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è: (1) —Ä—É—á–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ –º–µ—Ç–æ–¥–æ–º –ø—Ä–æ–± –∏ –æ—à–∏–±–æ–∫ –∏–ª–∏ (2) –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT.</p>\n<p><strong>6.4. –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏</strong></p>\n<p>–ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è LLM, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â—É—é—Å—è —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –≤—ã—Å–æ–∫–æ–π —Å—Ç–µ–ø–µ–Ω—å—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –≠—Ç–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∞–∫—Ç—É–∞–ª—å–Ω–∞ –∏ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–æ—Å—Ç—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤–æ –≤—Ä–µ–º—è –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–æ—è–≤–ª—è—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ç–æ—Ä–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ.</p>\n<ul>\n<li><strong>–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π:</strong> –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –º–æ–≥—É—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ —Å–µ—Ä—å–µ–∑–Ω—ã–º –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–ª–∏ –≤–≤–æ–¥—è—â–∏–π –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ –∫–æ–¥, —Ä–∏—Å–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã.</li>\n</ul>\n<p>–î–ª—è —Å–º—è–≥—á–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —á–µ–ª–æ–≤–µ–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —á–µ–ª–æ–≤–µ–∫–æ–º –∏ –∞–≥–µ–Ω—Ç–æ–º.</p>"
            },
            {
                "title": "Knowledge Boundary",
                "content": "A pivotal application of LLM-based autonomous agents lies in simulating diverse real-world human behaviors [20]. The study of human simulation has long history, and the recent surge in interest can be attributed to the remarkable advancements made by LLMs, which have demonstrated significant capabilities in simulating human behavior. However, it is important to recognize that the power of LLMs may not always be advantageous. Specifically, an ideal simulation should accurately replicate human In this context, LLMs may display knowledge. overwhelming capabilities, being trained on vast corpus of web knowledge that far exceeds what an average individual might know. The immense capabilities of LLMs can significantly impact the effectiveness of simulations. For instance, when attempting to simulate user selection behaviors for various movies, it is crucial to ensure that LLMs assume position of having no prior knowledge of these movies. However, there is possibility that LLMs have already acquired information about these movies. Without implementing appropriate strategies, LLMs may make decisions based on their extensive knowledge, even though real-world users would not have access to the contents of these 34 Front. Comput. Sci., 2024, 0(0): 142 movies beforehand. Based on the above example, we may conclude that for building believable agent simulation environment, an important problem is how to constrain the utilization of user-unknown knowledge of LLM. 6.6 Efficiency Due to their autoregressive architecture, LLMs typically have slow inference speeds. However, the agent may need to query LLMs for each action multiple times, such as extracting information from memory, make plans before taking actions and so on. Consequently, the efficiency of agent actions is greatly affected by the speed of LLM inference.",
                "summary": "<h2>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–∞ –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–∏–º –ø—Ä–æ–±–ª–µ–º—ã</h2>\n<p>–û–¥–Ω–∏–º –∏–∑ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –º–∏—Ä–µ. –ò–Ω—Ç–µ—Ä–µ—Å –∫ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –≤—ã—Ä–æ—Å –±–ª–∞–≥–æ–¥–∞—Ä—è —É—Å–ø–µ—Ö–∞–º LLM –≤ –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è. –û–¥–Ω–∞–∫–æ, –º–æ—â—å LLM –Ω–µ –≤—Å–µ–≥–¥–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.</p>\n<p>–ò–¥–µ–∞–ª—å–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –¥–æ–ª–∂–Ω–∞ —Ç–æ—á–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞, –∞ LLM, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ–≥—Ä–æ–º–Ω—ã—Ö –º–∞—Å—Å–∏–≤–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –º–æ–≥—É—Ç –æ–±–ª–∞–¥–∞—Ç—å –∑–Ω–∞–Ω–∏—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –∑–Ω–∞–Ω–∏—è —Å—Ä–µ–¥–Ω–µ—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. –≠—Ç–æ –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–∏–º—É–ª—è—Ü–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—ã–±–æ—Ä–∞ —Ñ–∏–ª—å–º–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏ –≤–∞–∂–Ω–æ, —á—Ç–æ–±—ã LLM –Ω–µ –∏–º–µ–ª–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –æ–± —ç—Ç–∏—Ö —Ñ–∏–ª—å–º–∞—Ö. –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ, LLM –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–æ–∏—Ö –æ–±—à–∏—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –Ω–µ –∏–º–µ–ª–∏ –±—ã –¥–æ—Å—Ç—É–ø–∞ –∫ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–π —Å—Ä–µ–¥—ã —Å–∏–º—É–ª—è—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤, –≤–∞–∂–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π —è–≤–ª—è–µ—Ç—Å—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –∑–Ω–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, LLM –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –Ω–∏–∑–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –∏–∑-–∑–∞ —Å–≤–æ–µ–π –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ê–≥–µ–Ω—Ç—É –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ LLM –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ø–∞–º—è—Ç–∏ –∏–ª–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∑–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞.</p>"
            }
        ]
    },
    {
        "id": "2412.15115",
        "title": "Qwen2.5 Technical Report",
        "url": "https://arxiv.org/abs/2412.15115",
        "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-19",
        "pub_date_card": {
            "ru": "19 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 19",
            "zh": "12Êúà19Êó•"
        },
        "hash": "f656de775add8c33",
        "authors": [
            "Qwen",
            ":",
            "An Yang",
            "Baosong Yang",
            "Beichen Zhang",
            "Binyuan Hui",
            "Bo Zheng",
            "Bowen Yu",
            "Chengyuan Li",
            "Dayiheng Liu",
            "Fei Huang",
            "Haoran Wei",
            "Huan Lin",
            "Jian Yang",
            "Jianhong Tu",
            "Jianwei Zhang",
            "Jianxin Yang",
            "Jiaxi Yang",
            "Jingren Zhou",
            "Junyang Lin",
            "Kai Dang",
            "Keming Lu",
            "Keqin Bao",
            "Kexin Yang",
            "Le Yu",
            "Mei Li",
            "Mingfeng Xue",
            "Pei Zhang",
            "Qin Zhu",
            "Rui Men",
            "Runji Lin",
            "Tianhao Li",
            "Tingyu Xia",
            "Xingzhang Ren",
            "Xuancheng Ren",
            "Yang Fan",
            "Yang Su",
            "Yichang Zhang",
            "Yu Wan",
            "Yuqiong Liu",
            "Zeyu Cui",
            "Zhenru Zhang",
            "Zihan Qiu"
        ],
        "affiliations": [
            "Alibaba Cloud Model Studio",
            "Hugging Face Hub",
            "Kaggle",
            "ModelScope"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.15115.jpg",
        "data": {
            "categories": [
                "#benchmark",
                "#training",
                "#reasoning",
                "#alignment",
                "#multimodal",
                "#architecture",
                "#agi",
                "#dataset",
                "#optimization",
                "#open_source"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "Qwen2.5: –ù–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π",
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ—Ä–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –ú–æ–¥–µ–ª–∏ –ø—Ä–æ—à–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ —ç—Ç–∞–ø–∞—Ö –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –ü—Ä–∏–º–µ–Ω–µ–Ω—ã —Ç–µ—Ö–Ω–∏–∫–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. Qwen2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–∏–º–∏ –µ–µ –ø–æ —Ä–∞–∑–º–µ—Ä—É."
            },
            "en": {
                "title": "Qwen2.5: Elevating Language Models with Unmatched Scale and Precision",
                "desc": "Qwen2.5 is a new series of large language models (LLMs) that have been enhanced through extensive pre-training and post-training processes. The pre-training phase utilized a massive dataset of 18 trillion tokens, significantly improving the model's common sense and reasoning abilities. In the post-training phase, advanced techniques like supervised finetuning and reinforcement learning were applied to refine the model's performance on tasks such as long text generation and instruction following. Qwen2.5 models are available in various sizes and configurations, demonstrating top performance across multiple benchmarks and applications, including specialized models for math and coding."
            },
            "zh": {
                "title": "Qwen2.5ÔºöÊª°Ë∂≥Â§öÊ†∑ÂåñÈúÄÊ±ÇÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Êú¨Êñá‰ªãÁªç‰∫ÜQwen2.5ÔºåËøôÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁ≥ªÂàóÔºåÊó®Âú®Êª°Ë∂≥Â§öÊ†∑ÂåñÁöÑÈúÄÊ±Ç„ÄÇ‰∏é‰πãÂâçÁöÑÁâàÊú¨Áõ∏ÊØîÔºåQwen2.5Âú®È¢ÑËÆ≠ÁªÉÂíåÂêéËÆ≠ÁªÉÈò∂ÊÆµÈÉΩÊúâÊòæËëóÊîπËøõÔºåÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰ªé7‰∏á‰∫ø‰∏™Ê†áËÆ∞Êâ©Â±ïÂà∞18‰∏á‰∫ø‰∏™Ê†áËÆ∞Ôºå‰∏∫Â∏∏ËØÜ„ÄÅ‰∏ìÂÆ∂Áü•ËØÜÂíåÊé®ÁêÜËÉΩÂäõÊèê‰æõ‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇÂêéËÆ≠ÁªÉÊñπÈù¢ÔºåÈááÁî®‰∫ÜË∂ÖËøá100‰∏áÊ†∑Êú¨ÁöÑÂ§çÊùÇÁõëÁù£ÂæÆË∞ÉÂíåÂ§öÈò∂ÊÆµÂº∫ÂåñÂ≠¶‰π†ÔºåÊòæËëóÊèêÂçá‰∫Ü‰∫∫Á±ªÂÅèÂ•ΩÂíåÈïøÊñáÊú¨ÁîüÊàêËÉΩÂäõ„ÄÇQwen2.5Âú®ËØ≠Ë®ÄÁêÜËß£„ÄÅÊé®ÁêÜ„ÄÅÊï∞Â≠¶„ÄÅÁºñÁ†ÅÁ≠âÂ§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂ∞§ÂÖ∂ÊòØÂÖ∂ÊóóËà∞Ê®°ÂûãQwen2.5-72B-InstructÂú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜËÆ∏Â§öÂºÄÊîæÂíå‰∏ìÊúâÊ®°Âûã„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
                "summary": "<p>–í —ç—Ç–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å Qwen2.5, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è —Å–æ–±–æ–π —Å–µ—Ä–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, Qwen 2.5 –±—ã–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–∞ –∫–∞–∫ –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏.</p>\n<p>–í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –æ–±—ä–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —É–≤–µ–ª–∏—á–µ–Ω —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—á–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞, —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>\n<p>–ù–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–µ –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —É–ª—É—á—à–∞—é—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∑–∞–º–µ—Ç–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–µ—Ä–∏—è LLM Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞—é—Ç –±–∞–∑–æ–≤—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã, –∞ —Ç–∞–∫–∂–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤–∫–ª—é—á–∞—è –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE): Qwen2.5-Turbo –∏ Qwen2.5-Plus, –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤ Alibaba Cloud Model Studio.</p>\n<p>Qwen2.5 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —à–∏—Ä–æ–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Ç–µ—Å—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ —Ç.–¥. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Qwen2.5-72B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ä—è–¥ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–µ—Å–æ–º Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo –∏ Qwen2.5-Plus –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –ø—Ä–∏ —ç—Ç–æ–º –∫–æ–Ω–∫—É—Ä–∏—Ä—É—è —Å –º–æ–¥–µ–ª—è–º–∏ GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª–∏ Qwen2.5 —Å–ª—É–∂–∞—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Qwen2.5-Math, Qwen2.5-Coder, QwQ –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>"
            },
            {
                "title": "Introduction",
                "content": "The sparks of artificial general intelligence (AGI) are increasingly visible through the fast development of large foundation models, notably large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; 2024a; Gemini Team, 2024; Anthropic, 2023a;b; 2024; Bai et al., 2023; Yang et al., 2024a; Touvron et al., 2023a;b; Dubey et al., 2024). The continuous advancement in model and data scaling, combined with the paradigm of large-scale pre-training followed by high-quality supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), has enabled large language models (LLMs) to develop emergent capabilities in language understanding, generation, and reasoning. Building on this foundation, recent breakthroughs in inference time scaling, particularly demonstrated by o1 (OpenAI, 2024b), have enhanced LLMs capacity for deep thinking through step-by-step reasoning and reflection. These developments have elevated the potential of language models, suggesting they may achieve significant breakthroughs in scientific exploration as they continue to demonstrate emergent capabilities indicative of more general artificial intelligence. Besides the fast development of model capabilities, the recent two years have witnessed burst of open (open-weight) large language models in the LLM community, for example, the Llama series (Touvron et al., 2023a;b; Dubey et al., 2024), Mistral series (Jiang et al., 2023a; 2024), and our Qwen series (Bai et al., 2023; Yang et al., 2024a; Qwen Team, 2024a; Hui et al., 2024; Qwen Team, 2024c; Yang et al., 2024b). The open-weight models have democratized the access of large language models to common users and developers, enabling broader research participation, fostering innovation through community collaboration, and accelerating the development of AI applications across diverse domains. Recently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the openweight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized models in different precisions. Specifically, the flagship model Qwen2.5-72B-Instruct demonstrates competitive performance against the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Additionally, we also release the proprietary models of Mixture-of-Experts (MoE, Lepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022), namely Qwen2.5-Turbo and Qwen2.5-Plus1, which performs competitively against GPT-4o-mini and GPT-4o respectively. In this technical report, we introduce Qwen2.5, the result of our continuous endeavor to create better LLMs. Below, we show the key features of the latest version of Qwen: Better in Size: Compared with Qwen2, in addition to 0.5B, 1.5B, 7B, and 72B models, Qwen2.5 brings back the 3B, 14B, and 32B models, which are more cost-effective for resource-limited scenarios and are under-represented in the current field of open foundation models. Qwen2.5Turbo and Qwen2.5-Plus offer great balance among accuracy, latency, and cost. Better in Data: The pre-training and post-training data have been improved significantly. The pre-training data increased from 7 trillion tokens to 18 trillion tokens, with focus on knowledge, coding, and mathematics. The pre-training is staged to allow transitions among different mixtures. The post-training data amounts to 1 million examples, across the stage of supervised finetuning (SFT, Ouyang et al., 2022), direct preference optimization (DPO, Rafailov et al., 2023), and group relative policy optimization (GRPO, Shao et al., 2024). Better in Use: Several key limitations of Qwen2 in use have been eliminated, including larger generation length (from 2K tokens to 8K tokens), better support for structured input and output, (e.g., tables and JSON), and easier tool use. In addition, Qwen2.5-Turbo supports context length of up to 1 million tokens.",
                "summary": "<p>–†–∞–∑–≤–∏—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –Ω–∞—Å –∫ —Å–æ–∑–¥–∞–Ω–∏—é –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (AGI). –£—Å–ø–µ—Ö–∏ –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ª—é–¥–µ–π (RLHF) –ø–æ–∑–≤–æ–ª–∏–ª–∏ LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ. –ù–µ–¥–∞–≤–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–≤–æ–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ o1, —É–ª—É—á—à–∏–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å LLM –∫ –≥–ª—É–±–æ–∫–æ–º—É –º—ã—à–ª–µ–Ω–∏—é —á–µ—Ä–µ–∑ –ø–æ—à–∞–≥–æ–≤—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏—é. –≠—Ç–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø—Ä–æ—Ä—ã–≤–æ–≤ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –±–æ–ª–µ–µ –æ–±—â–µ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.</p>\n<p>–ü–æ–º–∏–º–æ —Ä–∞–∑–≤–∏—Ç–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–≤–∞ –≥–æ–¥–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω—ã–π —Ä–æ—Å—Ç —á–∏—Å–ª–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö (open-weight) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Llama, Mistral –∏ Qwen. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ —Å–¥–µ–ª–∞–ª–∏ LLM –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è —à–∏—Ä–æ–∫–æ–≥–æ –∫—Ä—É–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º—É —É—á–∞—Å—Ç–∏—é –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö, —Å—Ç–∏–º—É–ª–∏—Ä—É—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –∑–∞ —Å—á–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –∏ —É—Å–∫–æ—Ä—è—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É AI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p>–ù–µ–¥–∞–≤–Ω–æ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è —Å–µ—Ä–∏–∏ Qwen ‚Äî Qwen2.5. –í —Ä–∞–º–∫–∞—Ö –æ—Ç–∫—Ä—ã—Ç–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ 7 —Ä–∞–∑–º–µ—Ä–æ–≤ (0.5B, 1.5B, 3B, 7B, 14B, 32B –∏ 72B), –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ bfloat16, —Ç–∞–∫ –∏ –≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö. –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Llama-3-405B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 5 —Ä–∞–∑ –±–æ–ª—å—à–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–∏ –≤—ã–ø—É—â–µ–Ω—ã –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ Mixture-of-Experts (MoE) ‚Äî Qwen2.5-Turbo –∏ Qwen2.5-Plus1, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—Ç —Å GPT-4o-mini –∏ GPT-4o —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ.</p>\n<p>–í —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º –æ—Ç—á–µ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω Qwen2.5, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö —É—Å–∏–ª–∏–π –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —É–ª—É—á—à–µ–Ω–Ω—ã—Ö LLM. –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ Qwen:</p>\n<ul>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–º–µ—Ä–∞—Ö:</strong> –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –≤ Qwen2.5, –ø–æ–º–∏–º–æ –º–æ–¥–µ–ª–µ–π 0.5B, 1.5B, 7B –∏ 72B, –≤–æ–∑–≤—Ä–∞—â–µ–Ω—ã –º–æ–¥–µ–ª–∏ 3B, 14B –∏ 32B, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã –¥–ª—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–µ–¥–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π. Qwen2.5-Turbo –∏ Qwen2.5-Plus –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Ç–æ—á–Ω–æ—Å—Ç—å—é, –∑–∞–¥–µ—Ä–∂–∫–æ–π –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å—é.</li>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö:</strong> –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É–≤–µ–ª–∏—á–∏–ª—Å—è —Å 7 –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∑–Ω–∞–Ω–∏—è, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ—ç—Ç–∞–ø–Ω–æ, —Å –ø–µ—Ä–µ—Ö–æ–¥–∞–º–∏ –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –û–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1 –º–∏–ª–ª–∏–æ–Ω –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —ç—Ç–∞–ø—ã —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å —É—á–∏—Ç–µ–ª–µ–º (SFT), –ø—Ä—è–º–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (DPO) –∏ –≥—Ä—É–ø–ø–æ–≤–æ–π –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (GRPO).</li>\n<li><strong>–£–ª—É—á—à–µ–Ω–∏—è –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏:</strong> –£—Å—Ç—Ä–∞–Ω–µ–Ω—ã –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Qwen2, –≤–∫–ª—é—á–∞—è —É–≤–µ–ª–∏—á–µ–Ω–Ω—É—é –¥–ª–∏–Ω—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å 2K –¥–æ 8K —Ç–æ–∫–µ–Ω–æ–≤), —É–ª—É—á—à–µ–Ω–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –∏ –≤—ã–≤–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–∞–±–ª–∏—Ü –∏ JSON) –∏ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Turbo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –¥–ª–∏–Ω—É –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤.</li>\n</ul>"
            },
            {
                "title": "Architecture & Tokenizer",
                "content": "Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus. Below, we provide details about the architecture of models. For dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford et al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components: Grouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation function (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su 1Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx (to be released) in the API. 2 Table 1: Model architecture and license of Qwen2.5 open-weight models. Models Layers Heads (Q / KV) Tie Embedding Context / Generation Length License 0.5B 1.5B 3B 7B 14B 32B 72B 24 28 36 28 48 64 14 / 2 12 / 2 16 / 2 28 / 4 40 / 8 40 / 8 64 / 8 Yes Yes Yes No No No No 32K / 8K 32K / 8K 32K / 8K 128K / 8K 128K / 8K 128K / 8K 128K / 8K Apache 2.0 Apache 2.0 Qwen Research Apache 2.0 Apache 2.0 Apache 2.0 Qwen et al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and RMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training. Building upon the dense model architectures, we extend it to MoE model architectures. This is achieved by replacing standard feed-forward network (FFN) layers with specialized MoE layers, where each layer comprises multiple FFN experts and routing mechanism that dispatches tokens to the top-K experts. Following the approaches demonstrated in Qwen1.5-MoE (Yang et al., 2024a), we implement fine-grained expert segmentation (Dai et al., 2024) and shared experts routing (Rajbhandari et al., 2022; Dai et al., 2024). These architectural innovations have yielded substantial improvements in model performance across downstream tasks. For tokenization, we utilize Qwens tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with vocabulary of 151,643 regular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen versions, adding two new tokens for tool functionality and allocating the remainder for other model capabilities. This expansion establishes unified vocabulary across all Qwen2.5 models, enhancing consistency and reducing potential compatibility issues.",
                "summary": "<p>–°–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Qwen2.5 –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º (Qwen2.5-0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B) –∏ –º–æ–¥–µ–ª–∏ MoE (Mixture of Experts) –¥–ª—è API-—Å–µ—Ä–≤–∏—Å–æ–≤ (Qwen2.5-Turbo –∏ Qwen2.5-Plus).</p>\n<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π:</strong></p>\n<ul>\n<li><strong>–ü–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏:</strong><ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É—é—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–µ–∫–æ–¥–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer (–∫–∞–∫ –∏ –≤ Qwen2).</li>\n<li>–í–∫–ª—é—á–∞—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:<ul>\n<li><strong>Grouped Query Attention (GQA):</strong> –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è KV-–∫—ç—à–∞ (—Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∫–ª—é—á–µ–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π).</li>\n<li><strong>–§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ SwiGLU:</strong> –¥–ª—è –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.</li>\n<li><strong>Rotary Positional Embeddings (RoPE):</strong> –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>\n<li><strong>QKV bias:</strong> —Å–º–µ—â–µ–Ω–∏–µ –≤ –º–µ—Ö–∞–Ω–∏–∑–º–µ –≤–Ω–∏–º–∞–Ω–∏—è.</li>\n<li><strong>RMSNorm —Å –ø—Ä–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π:</strong> –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>–ú–æ–¥–µ–ª–∏ MoE:</strong><ul>\n<li>–ü–æ—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</li>\n<li>–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å–ª–æ–∏ FFN (feed-forward network) –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–∏ MoE.</li>\n<li>–°–ª–æ–π MoE —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö FFN-—ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –∫ top-K —ç–∫—Å–ø–µ—Ä—Ç–∞–º.</li>\n<li>–†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ç–æ—á–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å –æ–±—â–∏–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (–∫–∞–∫ –≤ Qwen1.5-MoE).</li>\n</ul>\n</li>\n</ul>\n<p><strong>–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä Qwen, –∫–æ—Ç–æ—Ä—ã–π —Ä–µ–∞–ª–∏–∑—É–µ—Ç byte-level byte-pair encoding (BBPE) —Å–æ —Å–ª–æ–≤–∞—Ä–µ–º –∏–∑ 151 643 –æ–±—ã—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.</li>\n<li>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —É–≤–µ–ª–∏—á–µ–Ω–æ —Å 3 –¥–æ 22 (–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Qwen).<ul>\n<li>–î–æ–±–∞–≤–ª–µ–Ω—ã –¥–≤–∞ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.</li>\n<li>–û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤—ã–¥–µ–ª–µ–Ω—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏.</li>\n</ul>\n</li>\n<li>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5 –ø–æ–≤—ã—à–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.</li>\n</ul>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>\n* GQA –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–Ω–∏–º–∞–Ω–∏—è.\n* MoE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª—è–º –±—ã—Ç—å –±–æ–ª–µ–µ –µ–º–∫–∏–º–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ –Ω–µ –≤—Å–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ.\n* –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏.</p>"
            },
            {
                "title": "Pre-training Data",
                "content": "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor Qwen2. These improvements stem from several key aspects: (1) Better data filtering. High-quality pre-training data is crucial for model performance, making data quality assessment and filtering critical component of our pipeline. We leverage Qwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional analysis to evaluate and score training samples. The filtering method represents significant advancement over our previous approach used for Qwen2, as it benefits from Qwen2s expanded pre-training on larger multilingual corpus. The enhanced capabilities enable more nuanced quality assessment, resulting in both improved retention of high-quality training data and more effective filtering of low-quality samples across multiple languages. (2) Better math and code data. During the pre-training phase of Qwen2.5, we incorporate training data from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024). This data integration strategy proves highly effective, as these specialized datasets are instrumental in achieving state-of-the-art performance on mathematical and coding tasks. By leveraging these high-quality domain-specific datasets during pre-training, Qwen2.5 inherits strong capabilities in both mathematical reasoning and code generation. (3) Better synthetic data. To generate high-quality synthetic data, particularly in mathematics, code, and knowledge domains, we leverage both Qwen2-72B-Instruct (Yang et al., 2024a) and Qwen2Math-72B-Instruct (Qwen Team, 2024c). The quality of this synthesized data is further enhanced through rigorous filtering using our proprietary general reward model and the specialized Qwen2-Math-RM-72B (Qwen Team, 2024c) model. 3 (4) Better data mixture. To optimize the pre-training data distribution, we employ Qwen2-Instruct models to classify and balance content across different domains. Our analysis revealed that domains like e-commerce, social media, and entertainment are significantly overrepresented in web-scale data, often containing repetitive, template-based, or machine-generated content. Conversely, domains such as technology, science, and academic research, while containing higherquality information, are traditionally underrepresented. Through strategic down-sampling of overrepresented domains and up-sampling of high-value domains, we ensure more balanced and information-rich training dataset that better serves our models learning objectives. Building on these techniques, we have developed larger and higher-quality pre-training dataset, expanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens.",
                "summary": "<p>–í Qwen2.5 –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –∑–∞ —Å—á–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤:</p>\n<ol>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö:</strong> –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í Qwen2.5 –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-Instruct. –û–Ω–∏ –ø—Ä–æ–≤–æ–¥—è—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç –∏–º –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —à–∞–≥–æ–º –≤–ø–µ—Ä–µ–¥ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–¥—Ö–æ–¥–æ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–º—Å—è –≤ Qwen2, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Qwen2 –Ω–∞ –±–æ–ª—å—à–µ–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º –∫–æ—Ä–ø—É—Å–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –ª—É—á—à–µ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ, –ø—Ä–∏—á–µ–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –∫–æ–¥–∞:</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ Qwen2.5-Math –∏ Qwen2.5-Coder. –≠—Ç–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–æ—Å—Ç–∏—á—å –ø–µ—Ä–µ–¥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–∏—Ö –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, Qwen2.5 –ø–æ–ª—É—á–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ:</strong> –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±–ª–∞—Å—Ç—è—Ö –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –∫–æ–¥–∞ –∏ –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2-72B-Instruct –∏ Qwen2Math-72B-Instruct. –ö–∞—á–µ—Å—Ç–≤–æ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen2-Math-RM-72B.</p>\n</li>\n<li>\n<p><strong>–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:</strong> –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª–∏ Qwen2-Instruct –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —Ç–∞–∫–∏–µ –æ–±–ª–∞—Å—Ç–∏, –∫–∞–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è, —á—Ä–µ–∑–º–µ—Ä–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–µ–±-–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è, —à–∞–±–ª–æ–Ω–Ω—ã–π –∏–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—à–∏–Ω–∞–º–∏ –∫–æ–Ω—Ç–µ–Ω—Ç. –ù–∞–ø—Ä–æ—Ç–∏–≤, –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –Ω–∞—É–∫–∞ –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã. –ó–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø–µ—Ä–µ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∏–∑ –≤—ã—Å–æ–∫–æ—Ü–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π, —Å–æ–∑–¥–∞–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ü–µ–ª—è–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.</p>\n</li>\n</ol>\n<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π, –±—ã–ª —Å–æ–∑–¥–∞–Ω –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–π –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—Ä–æ—Å —Å 7 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ Qwen2, –¥–æ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.</p>"
            },
            {
                "title": "Scaling Law for Hyper-parameters",
                "content": "We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al., 2022; Kaplan et al., 2020). While previous studies (Dubey et al., 2024; Almazrouei et al., 2023; Hoffmann et al., 2022) primarily used scaling laws to determine optimal model sizes given compute budgets, we leverage them to identify optimal hyperparameters across model architectures. Specifically, our scaling laws help determine key training parameters like batch size and learning rate ¬µ for both dense models and MoE models of varying sizes. Through extensive experimentation, we systematically study the relationship between model architecture and optimal training hyper-parameters. Specifically, we analyze how the optimal learning rate ¬µopt and batch size Bopt vary with model size and pre-training data size D. Our experiments cover comprehensive range of architectures, including dense models with 44M to 14B parameters and MoE models with 44M to 1B activated parameters, trained on datasets ranging from 0.8B to 600B tokens. Using these optimal hyper-parameter predictions, we then model the final loss as function of model architecture and training data scale. Additionally, we leverage scaling laws to predict and compare the performance of MoE models with varying parameter counts against their dense counterparts. This analysis guides our hyper-parameter configuration for MoE models, enabling us to achieve performance parity with specific dense model variants (such as Qwen2.5-72B and Qwen2.5-14B) through careful tuning of both activated and total parameters.",
                "summary": "<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –∏—Å—Å–ª–µ–¥—É—é—Ç—Å—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö, –∑–¥–µ—Å—å –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π.</p>\n<p>–í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–º–æ–≥–∞—é—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ (batch size) –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate) ¬µ, –∫–∞–∫ –¥–ª—è –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫ –∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE (Mixture of Experts) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤. –ü—É—Ç–µ–º –æ–±—à–∏—Ä–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞–µ—Ç—Å—è –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –º–µ–∂–¥—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è, –∫–∞–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è ¬µopt –∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ Bopt –º–µ–Ω—è—é—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è D.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä, –≤–∫–ª—é—á–∞—è –ø–ª–æ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–æ–¥–µ–ª–∏ MoE —Å 44 –º–∏–ª–ª–∏–æ–Ω–∞–º–∏ –¥–æ 1 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –æ—Ç 0,8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 600 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤. –ò—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç—Å—è –∏—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –∏ –º–∞—Å—à—Ç–∞–±–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π MoE —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Ö –ø–ª–æ—Ç–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏. –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –ø–æ–º–æ–≥–∞–µ—Ç –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π MoE, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –ø–∞—Ä–∏—Ç–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –ø–ª–æ—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Ç–∞–∫–∏–º–∏ –∫–∞–∫ Qwen2.5-72B –∏ Qwen2.5-14B) –∑–∞ —Å—á–µ—Ç —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞–∫ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –æ–±—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.</p>"
            },
            {
                "title": "Long-context Pre-training",
                "content": "For optimal training efficiency, Qwen2.5 employs two-phase pre-training approach: an initial phase with 4,096-token context length, followed by an extension phase for longer sequences. Following the strategy used in Qwen2, we extend the context length from 4,096 to 32,768 tokens during the final pre-training stage for all model variants except Qwen2.5-Turbo. Concurrently, we increase the base frequency of RoPEfrom 10,000 to 1,000,000 using the ABF technique (Xiong et al., 2023). For Qwen2.5-Turbo, we implement progressive context length expansion strategy during training, advancing through four stages: 32,768 tokens, 65,536 tokens, 131,072 tokens, and ultimately 262,144 tokens, with RoPE base frequency of 10,000,000. At each stage, we carefully curate the training data to include 40% sequences at the current maximum length and 60% shorter sequences. This progressive training methodology enables smooth adaptation to increasing context lengths while maintaining the models ability to effectively process and generalize across sequences of varying lengths. To enhance our models ability to process longer sequences during inference, we implement two key strategies: YARN (Peng et al., 2023) and Dual Chunk Attention (DCA, An et al., 2024). Through these innovations, we achieve four-fold increase in sequence length capacity, enabling Qwen2.5-Turbo to handle up to 1 million tokens and other models to process up to 131,072 tokens. Notably, these approaches not only improve the modeling of long sequences by reducing perplexity but also maintain the models strong performance on shorter sequences, ensuring consistent quality across varying input lengths.",
                "summary": "<p>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Qwen2.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–≤—É—Ö—Ñ–∞–∑–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö –¥–ª–∏–Ω–æ–π 4096 —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º, –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è.</p>\n<p>–í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –≤ Qwen2, –¥–ª—è –≤—Å–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏, –∫—Ä–æ–º–µ Qwen2.5-Turbo, –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 4096 –¥–æ 32768 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞–¥–∏–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º, –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE (–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è) —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è —Å 10 000 –¥–æ 1 000 000 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ ABF.</p>\n<p>–î–ª—è Qwen2.5-Turbo –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –≤–∫–ª—é—á–∞–µ—Ç —á–µ—Ç—ã—Ä–µ —ç—Ç–∞–ø–∞: 32 768 —Ç–æ–∫–µ–Ω–æ–≤, 65 536 —Ç–æ–∫–µ–Ω–æ–≤, 131 072 —Ç–æ–∫–µ–Ω–æ–≤ –∏, –Ω–∞–∫–æ–Ω–µ—Ü, 262 144 —Ç–æ–∫–µ–Ω–æ–≤. –ë–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ RoPE –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 10 000 000. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è, –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã 40% –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–º–µ–ª–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞, –∞ 60% –±—ã–ª–∏ –∫–æ—Ä–æ—á–µ. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ø–ª–∞–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —É–≤–µ–ª–∏—á–µ–Ω–∏—é –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ –æ–±–æ–±—â–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω–æ–π –¥–ª–∏–Ω—ã.</p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–µ –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: YARN –∏ Dual Chunk Attention (DCA). –ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è–º, –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å, —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞–∑–∞. Qwen2.5-Turbo —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–æ 1 –º–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤, –∞ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ ‚Äì –¥–æ 131 072 —Ç–æ–∫–µ–Ω–æ–≤. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–∞—é—Ç –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∑–∞ —Å—á–µ—Ç —Å–Ω–∏–∂–µ–Ω–∏—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏, –Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –≤—Ö–æ–¥–∞—Ö —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã.</p>"
            },
            {
                "title": "Post-training",
                "content": "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages massive dataset comprising millions of high-quality examples. This expansion specifically addresses key areas where the previous model showed limitations, such as long-sequence 4 generation, mathematical problem-solving, coding, instruction-following, structured data understanding, logical reasoning, cross-lingual transfer, and robust system instruction. (2) Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL. Offline RL: This stage focuses on developing capabilities that are challenging for the reward model to evaluate, such as reasoning, factuality, and instruction-following. Through meticulous construction and validation of training data, we ensure that the Offline RL signals are both learnable and reliable (Xiang et al., 2024), enabling the model to acquire those complex skills effectively. Online RL: The Online RL phase leverages the reward models ability to detect nuances in output quality, including truthfulness, helpfulness, conciseness, relevance, harmlessness and debiasing. It enables the model to generate responses that are precise, coherent, and well-structured while maintaining safety and readability. As result, the models outputs consistently meet human quality standards and expectations.",
                "summary": "<p>–í Qwen 2.5 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen 2 –≤–Ω–µ—Å–µ–Ω—ã –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏:</p>\n<ol>\n<li>\n<p><strong>–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-tuning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –º–∏–ª–ª–∏–æ–Ω—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤. –≠—Ç–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –º–æ–¥–µ–ª–∏ –≤ —Ç–∞–∫–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∫—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –ø–µ—Ä–µ–Ω–æ—Å –∏ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–∏—Å—Ç–µ–º–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –¢–æ –µ—Å—Ç—å, –º–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–º —Å–ø–µ–∫—Ç—Ä–æ–º –∑–∞–¥–∞—á –∏ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ —Å–ª—É—á–∞—è–º–∏.</p>\n</li>\n<li>\n<p><strong>–î–≤—É—Ö—ç—Ç–∞–ø–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning):</strong> –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ Qwen 2.5 —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞: –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Offline RL) –∏ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Online RL).</p>\n<ul>\n<li><strong>–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ–º—É —Å–æ–∑–¥–∞–Ω–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —Å–∏–≥–Ω–∞–ª—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –æ–±—É—á–∞–µ–º—ã–º–∏, —Ç–∞–∫ –∏ –Ω–∞–¥–µ–∂–Ω—ã–º–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–∏–æ–±—Ä–µ—Ç–∞—Ç—å —ç—Ç–∏ —Å–ª–æ–∂–Ω—ã–µ –Ω–∞–≤—ã–∫–∏.<ul>\n<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: —Ç—É—Ç –≤–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–∞—Ä–∞–Ω–µ–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</em></li>\n</ul>\n</li>\n<li><strong>–û–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:</strong> –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å –Ω—é–∞–Ω—Å—ã –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—ã–≤–æ–¥–∞, –≤–∫–ª—é—á–∞—è –ø—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –∫—Ä–∞—Ç–∫–æ—Å—Ç—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏, —Å–≤—è–∑–Ω—ã–º–∏ –∏ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å–æ—Ö—Ä–∞–Ω—è—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –≤—ã–≤–æ–¥—ã –º–æ–¥–µ–ª–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞.<ul>\n<li><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ª–µ—Ç—É.</em></li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, Qwen 2.5 —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞ —Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º –∏ –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ–±—É—á–µ–Ω–∏—é —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏ –Ω–∞–¥–µ–∂–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º.</p>"
            },
            {
                "title": "Supervised Fine-tuning",
                "content": "In this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on several critical areas: (1) Long-sequence Generation: Qwen2.5 is capable of generating high-quality content with an output context length of up to 8,192 tokens, significant advancement over the typical posttraining response length, which often remains under 2,000 tokens. To address this gap, we develop long-response datasets (Quan et al., 2024). We employ back-translation techniques to generate queries for long-text data from pre-training corpora, impose output length constraints, and use Qwen2 to filter out low-quality paired data. (2) Mathematics: We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b), which encompasses diverse range of query sources, including public datasets, K-12 problem collections, and synthetic problems. To ensure high-quality reasoning, we employ rejection sampling (Yuan et al., 2023) along with reward modeling and annotated answers for guidance, producing step-by-step reasoning process. (3) Coding: To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5Coder (Hui et al., 2024). We use multiple language-specific agents into collaborative framework, generating diverse and high-quality instruction pairs across nearly 40 programming languages. We expand our instruction dataset by synthesizing new examples from code-related Q&A websites and gathering algorithmic code snippets from GitHub. comprehensive multilingual sandbox is used to perform static code checking and validate code snippets through automated unit testing, ensuring code quality and correctness (Dou et al., 2024; Yang et al., 2024c). (4) Instruction-following: To ensure high-quality instruction-following data, we implement rigorous code-based validation framework. In this approach, LLMs generate both instructions and corresponding verification code, along with comprehensive unit tests for cross-validation. Through execution feedback-based rejection sampling, we carefully curate the training data used for Supervised Fine-Tuning, thereby guaranteeing the models faithful adherence to intended instructions (Dong et al., 2024). (5) Structured Data Understanding: We develop comprehensive structured understanding dataset that encompasses both traditional tasks, such as tabular question-answering, fact verification, error correction, and structural understanding, as well as complex tasks involving structured and semi-structured data. By incorporating reasoning chains into the models responses, we significantly enhance its ability to infer information from structured data, thereby improving its performance across these diverse tasks. This approach not only broadens the scope of the dataset but also deepens the models capacity to reason and derive meaningful insights from complex data structures. (6) Logical Reasoning: To enhance the models logical reasoning capabilities, we introduce diverse set of 70,000 new queries spanning various domains. These queries encompass multiple-choice questions, true / false questions, and open-ended questions. The model is trained to approach problems systematically, employing range of reasoning methods such as deductive reasoning, inductive generalization, analogical reasoning, causal reasoning, and statistical reasoning. Through iterative refinement, we systematically filter out data containing incorrect answers or flawed reasoning processes. This process progressively strengthens the models ability to reason logically and accurately, ensuring robust performance across different types of reasoning tasks. 5 (7) Cross-Lingual Transfer: To facilitate the transfer of the models general capabilities across languages, we employ translation model to convert instructions from high-resource languages into various low-resource languages, thereby generating corresponding response candidates. To ensure the accuracy and consistency of these responses, we evaluate the semantic alignment between each multilingual response and its original counterpart. This process preserves the logical structure and stylistic nuances of the original responses, thereby maintaining their integrity and coherence across different languages. (8) Robust System Instruction: We construct hundreds of general system prompts to improve the diversity of system prompts in post-training, ensuring consistency between system prompts and conversations. Evaluations with different system prompts show that the model maintains good performance (Lu et al., 2024b) and reduced variance, indicating improved robustness. (9) Response Filtering: To evaluate the quality of responses, we employ multiple automatic annotation methods, including dedicated critic model and multi-agent collaborative scoring system. Responses are subjected to rigorous assessment, and only those deem flawless by all scoring systems are retained. This comprehensive approach ensures that our outputs maintain the highest quality standards. Ultimately, we construct dataset of over 1 million SFT examples. The model is fine-tuned for two epochs with sequence length of 32,768 tokens. To optimize learning, the learning rate is gradually decreased from 7 106 to 7 107. To address overfitting, we apply weight decay of 0.1, and gradient norms are clipped at maximum value of 1.0.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å–∞–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è, –≤–Ω–µ—Å–µ–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å Qwen2.5 –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—Å–∞—é—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π:</p>\n<ol>\n<li>\n<p><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:</strong> Qwen2.5 —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Ç–∏–ø–∏—á–Ω—É—é –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –º–µ–Ω–µ–µ 2000 —Ç–æ–∫–µ–Ω–æ–≤). –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–∞–º –∏–∑ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤, –Ω–∞–∫–ª–∞–¥—ã–≤–∞–ª–∏—Å—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ –∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Ñ–∏–ª—å—Ç—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Qwen2 –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä –¥–∞–Ω–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> –í –º–æ–¥–µ–ª—å –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Å—Ç–∏–ª–µ \"—Ü–µ–ø–æ—á–∫–∏ –º—ã—Å–ª–µ–π\" (chain-of-thought). –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø—É–±–ª–∏—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∑–∞–¥–∞—á–∏ –∏–∑ —à–∫–æ–ª—å–Ω—ã—Ö —É—á–µ–±–Ω–∏–∫–æ–≤ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (rejection sampling) –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ—à–∞–≥–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p>\n</li>\n<li>\n<p><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–≤—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏–∑ Qwen2.5Coder. –ë—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ —Å–æ–∑–¥–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è-–∫–æ–¥ –ø–æ—á—Ç–∏ –¥–ª—è 40 —è–∑—ã–∫–æ–≤. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Ä–∞—Å—à–∏—Ä–µ–Ω –∑–∞ —Å—á–µ—Ç —Å–∏–Ω—Ç–µ–∑–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å —Å–∞–π—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å–±–æ—Ä–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–∞ —Å GitHub. –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø–µ—Å–æ—á–Ω–∏—Ü–∞, –≤—ã–ø–æ–ª–Ω—è—é—â–∞—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥—É–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>\n</li>\n<li>\n<p><strong>–°–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º:</strong> –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±—ã–ª–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —Å—Ç—Ä–æ–≥–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∞. –ú–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–π –∫–æ–¥, –∞ —Ç–∞–∫–∂–µ –º–æ–¥—É–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏. –ü—É—Ç–µ–º –æ—Ç–∫–ª–æ–Ω—è—é—â–µ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞, –±—ã–ª–∏ –æ—Ç–æ–±—Ä–∞–Ω—ã –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–ª–æ —Ç–æ—á–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n</li>\n<li>\n<p><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:</strong> –ë—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –æ–±—à–∏—Ä–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –∫–∞–∫ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫), —Ç–∞–∫ –∏ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—É—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–≤—ã—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>\n</li>\n<li>\n<p><strong>–õ–æ–≥–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ:</strong> –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–∞–±–æ—Ä –∏–∑ 70 000 –Ω–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏. –≠—Ç–∏ –∑–∞–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞–ª–∏ –≤–æ–ø—Ä–æ—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, –≤–æ–ø—Ä–æ—Å—ã \"–≤–µ—Ä–Ω–æ/–Ω–µ–≤–µ—Ä–Ω–æ\" –∏ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –ø–æ–¥—Ö–æ–¥–∏—Ç—å –∫ –ø—Ä–æ–±–ª–µ–º–∞–º —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–æ–¥—ã –¥–µ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ, –∞–Ω–∞–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ, –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –ü—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–ª–∏ –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏.</p>\n</li>\n<li>\n<p><strong>–ö—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å:</strong> –î–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–±—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏–∑ —è–∑—ã–∫–æ–≤ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ —è–∑—ã–∫–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–ª–æ—Å—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∫–∞–∂–¥—ã–º –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ –µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º.</p>\n</li>\n<li>\n<p><strong>–ù–∞–¥–µ–∂–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:</strong> –ë—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã —Å–æ—Ç–Ω–∏ –æ–±—â–∏—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏. –û—Ü–µ–Ω–∫–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∞–º–∏ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ —Å–Ω–∏–∂–∞–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏—é, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å.</p>\n</li>\n<li>\n<p><strong>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤:</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å-–∫—Ä–∏—Ç–∏–∫–∞ –∏ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ—Ü–µ–Ω–∫–∏. –û—Ç–≤–µ—Ç—ã –ø–æ–¥–≤–µ—Ä–≥–∞–ª–∏—Å—å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ, –∏ —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –ø—Ä–∏–∑–Ω–∞–Ω—ã –±–µ–∑—É–ø—Ä–µ—á–Ω—ã–º–∏ –≤—Å–µ–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –æ—Ü–µ–Ω–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å.</p>\n</li>\n</ol>\n<p>–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –±—ã–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–æ–ª–µ–µ —á–µ–º 1 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–∏—Ç–µ–ª–µ–º. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–≤—É—Ö —ç–ø–æ—Ö —Å –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–ª–∞—Å—å, –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –±—ã–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã.</p>"
            },
            {
                "title": "Offline Reinforcement Learning",
                "content": "Compared to Online Reinforcement Learning (RL), Offline RL enables the pre-preparation of training signals, which is particularly advantageous for tasks where standard answers exist but are challenging to evaluate using reward models. In this study, we focus on objective query domains such as mathematics, coding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex. In the previous phase, we extensively employ strategies like execution feedback and answer matching to ensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model to resample responses for new set of queries. Responses that pass our quality checks are used as positive examples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO) training (Rafailov et al., 2023). To further enhance the reliability and accuracy of the training signals, we make use of both human and automated review processes (Cao et al., 2024). This dual approach ensures that the training data is not only learnable but also aligned with human expectations. Ultimately, we construct dataset consisting of approximately 150,000 training pairs. The model is then trained for one epoch using the Online Merging Optimizer (Lu et al., 2024a), with learning rate of 7 107.",
                "summary": "<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–Ω–ª–∞–π–Ω, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –≤ –æ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞—Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–µ–¥–º–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏, –≥–¥–µ –æ—Ç–≤–µ—Ç—ã –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í —ç—Ç–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω—ã–º.</p>\n<p>–ù–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º —ç—Ç–∞–ø–µ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç–∞–∫–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏, –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ —ç—Ç–æ—Ç –∂–µ –∫–æ–Ω–≤–µ–π–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ: –º–æ–¥–µ–ª—å SFT (Supervised Fine-Tuning) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ—Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤. –û—Ç–≤–µ—Ç—ã, –ø—Ä–æ—à–µ–¥—à–∏–µ –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, –∞ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É, ‚Äî –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ Direct Preference Optimization (DPO). </p>\n<p>–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ —Ä—É—á–Ω—ã–µ, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ø—Ä–æ–≤–µ—Ä–∫–∏. –¢–∞–∫–æ–π –¥–≤–æ–π–Ω–æ–π –ø–æ–¥—Ö–æ–¥ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≥–æ–¥–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º —á–µ–ª–æ–≤–µ–∫–∞. –í –∏—Ç–æ–≥–µ —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –ø—Ä–∏–º–µ—Ä–Ω–æ –∏–∑ 150 000 –æ–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä.</p>\n<p>–ó–∞—Ç–µ–º –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ Online Merging Optimizer —Å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è 7 * 10^-7.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>\n*   SFT (Supervised Fine-Tuning) - —ç—Ç–æ –º–µ—Ç–æ–¥ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n*   DPO (Direct Preference Optimization) - –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –º–µ–∂–¥—É –¥–≤—É–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–∞.\n*   Online Merging Optimizer - —ç—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.</p>"
            },
            {
                "title": "Online Reinforcement Learning",
                "content": "To develop robust reward model for online RL, we adhere to set of carefully defined labeling criteria. Those criteria ensure that the responses generated by the model are not only high-quality but also aligned with ethical and user-centric standards (Wang et al., 2024a). The specific guidelines for data labeling are as follows: Truthfulness: Responses must be grounded in factual accuracy, faithfully reflecting the provided context and instructions. The model should avoid generating information that is false or unsupported by the given data. Helpfulness: The models output should be genuinely useful, addressing the users query effectively while providing content that is positive, engaging, educational, and relevant. It should follow the given instructions precisely and offer value to the user. Conciseness: Responses should be succinct and to the point, avoiding unnecessary verbosity. The goal is to convey information clearly and efficiently without overwhelming the user with excessive detail. Relevance: All parts of the response should be directly related to the users query, dialogue history, and the assistants context. The model should tailor its output to ensure it is perfectly aligned with the users needs and expectations. Harmlessness: The model must prioritize user safety by avoiding any content that could lead to illegal, immoral, or harmful behavior. It should promote ethical conduct and responsible communication at all times. 6 Debiasing: The model should produce responses that are free from bias, including but not limited to gender, race, nationality, and politics. It should treat all topics equally and fairly, adhering to widely accepted moral and ethical standards. The queries utilized to train the reward model are drawn from two distinct datasets: publicly available open-source data and proprietary query set characterized by higher complexity. Responses are generated from checkpoints of the Qwen models, which have been fine-tuned using different methodsSFT, DPO, and RLat various stages of training. To introduce diversity, those responses are sampled at different temperature settings. Preference pairs are created through both human and automated labeling processes, and the training data for DPO is also integrated into this dataset. In our online reinforcement learning (RL) framework, we employ Group Relative Policy Optimization (GRPO, Shao et al., 2024). The query set utilized for training the reward model is identical to the one used in the RL training phase. The sequence in which queries are processed during training is determined by the variance of their response scores, as evaluated by the reward model. Specifically, queries with higher variance in response scores are prioritized to ensure more effective learning. We sample 8 responses for each query. All models are trained with 2048 global batch size and 2048 samples in each episode, considering pair of queries and responses as sample.",
                "summary": "<p>–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model) –¥–ª—è –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–∏ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç, —á—Ç–æ –æ—Ç–≤–µ—Ç—ã, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª—å—é, –±—É–¥—É—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏, –Ω–æ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —ç—Ç–∏—á–µ—Å–∫–∏–º –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.</p>\n<p>–í–æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö:</p>\n<ul>\n<li><strong>–ü—Ä–∞–≤–¥–∏–≤–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö –∏ —Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∏–∑–±–µ–≥–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ª–æ–∂–Ω–æ–π –∏–ª–∏ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</li>\n<li><strong>–ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å:</strong> –í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –ø–æ–ª–µ–∑–Ω—ã–º, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ—Ç–≤–µ—á–∞—è –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –û–Ω –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏ –ø—Ä–∏–Ω–æ—Å–∏—Ç—å –ø–æ–ª—å–∑—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.</li>\n<li><strong>–ö—Ä–∞—Ç–∫–æ—Å—Ç—å:</strong> –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º–∏ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É, –∏–∑–±–µ–≥–∞—è –∏–∑–ª–∏—à–Ω–µ–π –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω–æ—Å—Ç–∏. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ç–∫–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–º–∏ –¥–µ—Ç–∞–ª—è–º–∏.</li>\n<li><strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:</strong> –í—Å–µ —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω—ã —Å –∑–∞–ø—Ä–æ—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞. –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –≤—ã–≤–æ–¥, —á—Ç–æ–±—ã –æ–Ω —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º –∏ –æ–∂–∏–¥–∞–Ω–∏—è–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.</li>\n<li><strong>–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏–∑–±–µ–≥–∞—è –ª—é–±–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–º—É, –∞–º–æ—Ä–∞–ª—å–Ω–æ–º—É –∏–ª–∏ –≤—Ä–µ–¥–Ω–æ–º—É –ø–æ–≤–µ–¥–µ–Ω–∏—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–¥–≤–∏–≥–∞—Ç—å —ç—Ç–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ –≤–æ –≤—Å–µ—Ö —Å–ª—É—á–∞—è—Ö.</li>\n<li><strong>–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏:</strong> –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –≤—ã–¥–∞–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, —Å–≤–æ–±–æ–¥–Ω—ã–µ –æ—Ç –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è, –Ω–æ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è—Å—å, –≥–µ–Ω–¥–µ—Ä–Ω—É—é, —Ä–∞—Å–æ–≤—É—é, –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫—É—é. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫–æ –≤—Å–µ–º —Ç–µ–º–∞–º —Ä–∞–≤–Ω–æ –∏ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—è—Å—å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã—Ö –º–æ—Ä–∞–ª—å–Ω—ã—Ö –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.</li>\n</ul>\n<p>–ó–∞–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –≤–∑—è—Ç—ã –∏–∑ –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö: –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–µ–≥–æ—Å—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é. –û—Ç–≤–µ—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –º–æ–¥–µ–ª–µ–π Qwen, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ (SFT, DPO –∏ RL) –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è. –î–ª—è –≤–Ω–µ—Å–µ–Ω–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —ç—Ç–∏ –æ—Ç–≤–µ—Ç—ã –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –ø—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã. –ü–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —Å–æ–∑–¥–∞—é—Ç—Å—è –∫–∞–∫ —Å –ø–æ–º–æ—â—å—é —Ä—É—á–Ω–æ–π, —Ç–∞–∫ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, –∞ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è DPO —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç—Å—è –≤ —ç—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–í —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Group Relative Policy Optimization (GRPO). –ù–∞–±–æ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏–¥–µ–Ω—Ç–∏—á–µ–Ω —Ç–æ–º—É, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è RL. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤ –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∑–∞–ø—Ä–æ—Å—ã —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π –≤ –æ—Ü–µ–Ω–∫–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è 8 –æ—Ç–≤–µ—Ç–æ–≤. –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è —Å –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–∫–µ—Ç–∞ 2048 –∏ 2048 –æ–±—Ä–∞–∑—Ü–∞–º–∏ –≤ –∫–∞–∂–¥–æ–º —ç–ø–∏–∑–æ–¥–µ, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è –ø–∞—Ä—É –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü.</p>"
            },
            {
                "title": "Long Context Fine-tuning",
                "content": "To further extend the context length of Qwen2.5-Turbo, we introduce longer SFT examples during post-training, enabling it to better align with human preference in long queries. In the SFT phase, we employ two-stage approach. In the first stage, the model is fine-tuned exclusively using short instructions, each containing up to 32,768 tokens. This stage uses the same data and training steps as those employed for the other Qwen2.5 models, ensuring strong performance on short tasks. In the second stage, the fine-tuning process combines both short instructions (up to 32,768 tokens) and long instructions (up to 262,144 tokens). This hybrid approach effectively enhances the models instruction-following ability in long context tasks while maintaining its performance on short tasks. During the RL stage, we use training strategy similar to that used for the other Qwen2.5 models, focusing solely on short instructions. This design choice is driven by two primary considerations: first, RL training is computationally expensive for long context tasks; second, there is currently scarcity of reward models that provide suitable reward signals for long context tasks. Additionally, we find that adopting RL on short instructions alone can still significantly enhance the models alignment with human preferences in long context tasks.",
                "summary": "<p>–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å Qwen2.5-Turbo, –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –ª—É—á—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.</p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å –¥–æ–æ–±—É—á–µ–Ω–∏—è (SFT) —Ä–∞–∑–¥–µ–ª–µ–Ω –Ω–∞ –¥–≤–∞ —ç—Ç–∞–ø–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ—Ç —ç—Ç–∞–ø –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ –∏ —à–∞–≥–∏ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –∫–∞–∫ –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 32 768 —Ç–æ–∫–µ–Ω–æ–≤), —Ç–∞–∫ –∏ –¥–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–¥–æ 262 144 —Ç–æ–∫–µ–Ω–æ–≤). –¢–∞–∫–æ–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö.</p>\n<p>–ù–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è —Ç–æ–π, —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏ –æ–±—É—á–µ–Ω–∏–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö. –≠—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ –¥–≤—É–º—è –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏—á–∏–Ω–∞–º–∏: –≤–æ-–ø–µ—Ä–≤—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º; –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö –≤—Å–µ —Ä–∞–≤–Ω–æ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –≤ –∑–∞–¥–∞—á–∞—Ö —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.</p>"
            },
            {
                "title": "Base Models",
                "content": "We conduct comprehensive evaluations of the base language models of the Qwen2.5 series. The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities. The evaluation datasets include: General Tasks MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024b) (5-shot), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot), ARC-C (Clark et al., 2018) (25-shot), TruthfulQA (Lin et al., 2022a) (0-shot), Winogrande (Sakaguchi et al., 2021) (5-shot), HellaSwag (Zellers et al., 2019) (10-shot). 7 Table 2: Performance of the 70B+ base models and Qwen2.5-Plus. Datasets Llama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus MMLU MMLU-Pro MMLU-redux BBH ARC-C TruthfulQA WindoGrande HellaSwag GPQA TheoremQA MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 79.5 52.8 75.0 81.0 68.8 45.6 85.3 88. 36.3 32.3 42.5 73.7 77.6 48.2 42.1 70.4 58.4 46.3 70.0 79.9 67.1 38.0 General Tasks 85.2 61.6 - 85.9 - - 86.7 - 77.8 51.6 72.9 78.9 70.7 51.0 85.0 88. Mathematics & Science Tasks 34.3 35.9 41.7 71.7 83.7 46.3 40.2 71.7 58.1 46.7 63.5 77.7 62.9 23.3 - - 53.8 - 89.0 Coding Tasks 61.0 - 73.0 - - Multilingual Tasks - - - - 84.2 55.7 80.5 82.4 68.9 54.8 85.1 87.3 37.4 42.8 50.9 79.6 89.0 64.6 56.1 76.9 63.9 59. 76.6 80.7 76.0 37.8 86.1 58.1 83.9 86.3 72.4 60.4 83.9 87.6 45.9 42.4 62.1 82.7 91.5 59.1 51.2 84.7 69.2 60.5 78.7 89.6 76.7 39.0 85.4 64.0 82.8 85.8 70.9 55.3 85.5 89. 43.9 48.5 64.4 81.2 93.0 59.1 52.4 79.7 66.9 61.0 78.5 89.2 82.4 40.4 Mathematics & Science Tasks GPQA (Rein et al., 2023) (5-shot), Theorem QA (Chen et al., 2023a) (5-shot), GSM8K (Cobbe et al., 2021) (4-shot), MATH (Hendrycks et al., 2021b) (4-shot). Coding Tasks HumanEval (Chen et al., 2021) (0-shot), HumanEval+ (Liu et al., 2023)(0-shot), MBPP (Austin et al., 2021) (0-shot), MBPP+ (Liu et al., 2023) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript). Multilingual Tasks We group them into four categories: (a) Exam: M3Exam (5-shot, we only choose examples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova et al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French, Portuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar et al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023) (5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c) Mathematics: MGSM (Goyal et al., 2022) (8-shot CoT); and (d) Translation: Flores-101 (Goyal et al., 2022) (5-shot). For base models, we compare Qwen2.5 models with Qwen2 models and other leading open-weight models in terms of scales of parameters. Qwen2.5-72B & Qwen2.5-Plus We compare the base models of Qwen2.5-72B and Qwen2.5-Plus to other leading open-weight base models: Llama3-70B (Dubey et al., 2024), Llama3-405B (Dubey et al., 2024), Mixtrail-8x22B (Jiang et al., 2024), and our previous 72B version, the Qwen2-72B (Yang et al., 2024a). The Qwen2.5-72B base model significantly outperforms its peers in the same category across wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges. With significantly lower training and inference costs, Qwen2.5-Plus achieves very competitive performance results compared to Qwen2.5-72B and Llama3-405B, outperforming other baseline models on the Hellaswag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics, and Multi-Translation. Moreover, Qwen2.5-Plus achieves 64.0 on MMLU-Pro, which is 5.9 points higher than Qwen2.5-72B. Qwen2.5-14B/32B & Qwen2.5-Turbo The evaluation of the Qwen2.5-Turbo, Qwen2.5-14B, and 32B models is compared against baselines of similar sizes. These baselines include Yi-1.5-34B (Young et al., 8 Table 3: Performance of the 14B-30B+ base models and Qwen2.5-Turbo. Datasets Qwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande Hellaswag GPQA Theoremqa MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 74.3 44.1 69.0 66.8 63.6 57.4 81.5 85. 30.8 28.8 36.1 66.5 78.5 43.3 40.2 64.2 53.9 38.5 61.6 76.5 56.1 33.5 General Tasks 77.2 48.3 74.1 76.4 65.6 53.9 84.9 85.9 75.2 49.1 - 74.9 71.4 40.1 59.7 86. Mathematics & Science Tasks 34.9 35.8 42.7 71.0 81.1 54.9 46.3 75.7 60.2 48.0 65.8 82.2 61.6 38.7 37.4 40.0 41.7 72.6 81.7 Coding Tasks 46.3 40.2 65.5 55.4 39.5 Multilingual Tasks 58.3 73.9 49.3 30.0 79.5 55.6 77.1 76.1 67.8 56.3 81.1 85.0 41.4 42.1 55.6 77.0 88.3 57.3 51.2 76.2 63.0 53. 70.3 85.3 71.3 36.8 79.7 51.2 76.6 78.2 67.3 58.4 81.0 84.3 32.8 43.0 55.6 76.4 90.2 56.7 51.2 76.7 63.2 53.5 70.6 85.9 68.5 36.2 83.3 55.1 82.0 84.5 70.4 57.8 82.0 85. 48.0 44.1 57.7 80.9 92.9 58.5 52.4 84.5 67.2 59.4 75.4 88.4 73.7 37.3 Table 4: Performance of the 7B+ base models. Datasets Mistral-7B Llama3-8B Gemma2-9B Qwen2-7B Qwen2.5-7B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande HellaSwag GPQA TheoremQA MATH MMLU-stem GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 64.2 30.9 58.1 56.1 60.0 42.2 78.4 83.3 24.7 19.2 10.2 50.1 36. 29.3 24.4 51.1 40.9 29.4 47.1 63.3 26.3 23.3 General Tasks 66.6 35.4 61.6 57.7 59.3 44.0 77.4 82.1 71.3 44.7 67.9 68.2 68.2 45.3 79.5 81.9 Mathematics & Science Tasks 32.8 28.9 37.7 65.1 70.7 37.8 30.5 62.2 50.6 34.9 61.2 78.3 53.0 36.5 25.8 22.1 20.5 55.3 55.3 Coding Tasks 33.5 29.3 53.9 44.4 22. Multilingual Tasks 52.3 68.6 36.3 31.9 9 70.3 40.1 68.1 62.3 60.6 54.2 77.0 80.7 30.8 29.6 43.5 64.2 80.2 51.2 43.3 64.2 51.9 41. 59.2 72.0 57.5 31.5 74.2 45.0 71.1 70.4 63.7 56.4 75.9 80.2 36.4 36.0 49.8 72.3 85.4 57.9 50.6 74.9 62.9 50.3 59.4 79.3 57.8 32.4 Table 5: Performance of the smaller base models. Datasets Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B MMLU MMLU-pro MMLU-redux BBH ARC-C TruthfulQA Winogrande Hellaswag GPQA TheoremQA MATH MMLU-STEM GSM8K HumanEval HumanEval+ MBPP MBPP+ MultiPL-E Multi-Exam Multi-Understanding Multi-Mathematics Multi-Translation 44.3 14.7 40.7 18.2 31.0 39.7 56.9 49.1 29.8 9.6 11.2 27.5 36.4 22.6 18.9 33.1 27.6 16.3 29.4 40.4 7.8 14.1 General Tasks 55.9 21.6 51.8 36.5 43.7 45.9 65.0 67. 47.5 15.7 45.1 20.3 35.6 40.2 56.3 52.1 Mathematics & Science Tasks 24.8 16.0 19.5 39.8 41.6 30.5 26.8 39.3 33.8 18.9 30.8 41.0 13.5 15.3 20.7 14.8 21.6 42.7 46. Coding Tasks 34.8 29.9 46.9 37.6 27.9 Multilingual Tasks 43.1 50.7 21.3 23.8 60.9 28.5 58.5 45.1 54.7 46.6 65.0 67.9 24.2 22.1 35.0 54.8 68. 37.2 32.9 60.2 49.6 33.1 47.9 65.1 37.5 25.0 52.2 23.0 50.9 41.9 55.7 36.2 71.5 74.6 25.3 15.9 18.3 45.8 30.3 19.5 15.9 42.1 33.6 17.6 38.1 46.8 18.2 26. 65.6 34.6 63.7 56.3 56.5 48.9 71.1 74.6 26.3 27.4 42.6 62.5 79.1 42.1 36.0 57.1 49.4 41.2 54.6 76.6 48.9 29.3 2024), Gemma2-27B (Gemma Team et al., 2024), and Qwen1.5-32B (Qwen Team, 2024b). The results are shown in Table 3. The Qwen2.5-14B model demonstrates solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP. For Qwen2.5-Turbo, although its training cost and inference cost are significantly smaller than those of Qwen2.5-14B, it achieves comparable results, where its MMLU-Pro score is even better than that of Qwen2.5-32B. Qwen2.5-7B For 7B-level models, we focus on comparing Qwen2.5-7B with other leading 7B+ models, including Mistral-7B (Jiang et al., 2023a), Llama3-8B (Dubey et al., 2024), Gemma2-9B (Gemma Team et al., 2024), and our predecessor, Qwen2-7B (Yang et al., 2024a). The results can be found in Table 4. Note that the non-embedding parameters of Qwen2-7B and Qwen2.5-7B are only 6.5B, while that of Gemma2-9B is 8.2B. The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU (Hendrycks et al., 2021a), 49.8 on math challenges such as MATH (Hendrycks et al., 2021b), and 57.9 on coding tasks like HumanEval (Chen et al., 2021). Qwen2.5-0.5B/1.5B/3B For edge-side models, we compare Qwen2.5-0.5B, 1.5B, and 3B against established baselines: Qwen2-0.5B/1.5B (Yang et al., 2024a) and Gemma2-2.6B (Gemma Team et al., 2024). The results are given in Table 5. Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ–¥—Ä–æ–±–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–µ–º–µ–π—Å—Ç–≤–∞ Qwen2.5. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —É–¥–µ–ª—è–µ—Ç—Å—è –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –æ–±—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –Ω–∞—É—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:</p>\n<ul>\n<li><strong>–û–±—â–∏–µ –∑–∞–¥–∞—á–∏:</strong> MMLU, MMLU-Pro, MMLU-redux, BBH, ARC-C, TruthfulQA, Winogrande, HellaSwag.</li>\n<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –Ω–∞—É–∫–∞:</strong> GPQA, TheoremQA, GSM8K, MATH.</li>\n<li><strong>–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> HumanEval, HumanEval+, MBPP, MBPP+, MultiPL-E (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript).</li>\n<li><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏:</strong><ul>\n<li><strong>–≠–∫–∑–∞–º–µ–Ω:</strong> M3Exam, IndoMMLU, ruMMLU –∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã–π MMLU (–∞—Ä–∞–±—Å–∫–∏–π, –∏—Å–ø–∞–Ω—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –Ω–µ–º–µ—Ü–∫–∏–π, –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, —è–ø–æ–Ω—Å–∫–∏–π –∏ –∫–æ—Ä–µ–π—Å–∫–∏–π).</li>\n<li><strong>–ü–æ–Ω–∏–º–∞–Ω–∏–µ:</strong> BELEBELE, XCOPA, XWinograd, XStoryCloze –∏ PAWS-X.</li>\n<li><strong>–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞:</strong> MGSM.</li>\n<li><strong>–ü–µ—Ä–µ–≤–æ–¥:</strong> Flores-101.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-72B –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ Qwen2.5-72B –∏ Qwen2.5-Plus —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –≤–µ–¥—É—â–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏: Llama3-70B, Llama3-405B, Mixtral-8x22B –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π Qwen2-72B.</p>\n<p>Qwen2.5-72B –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ —Å–≤–æ–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —à–∏—Ä–æ–∫–æ–º—É —Å–ø–µ–∫—Ç—Ä—É –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Å Llama-3-405B, –ø—Ä–∏ —ç—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É—è –ª–∏—à—å –ø—è—Ç—É—é —á–∞—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B, Qwen2.5-72B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–∞–º–µ—Ç–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>\n<p>Qwen2.5-Plus, –∏–º–µ—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—á–µ–Ω—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2.5-72B –∏ Llama3-405B, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ HellaSwag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics –∏ Multi-Translation. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 64.0 –Ω–∞ MMLU-Pro, —á—Ç–æ –Ω–∞ 5.9 –ø—É–Ω–∫—Ç–æ–≤ –≤—ã—à–µ, —á–µ–º —É Qwen2.5-72B.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-14B/32B –∏ Qwen2.5-Turbo —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-Turbo, Qwen2.5-14B –∏ 32B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Yi-1.5-34B, Gemma2-27B –∏ Qwen1.5-32B.</p>\n<p>Qwen2.5-14B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU –∏ BBH, –≥–¥–µ –æ–Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. Qwen2.5-32B –æ—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è, —á–∞—Å—Ç–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –û–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen1.5-32B, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ —Å–ª–æ–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.</p>\n<p>Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∞ –ø–æ MMLU-Pro –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-32B.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen2.5-7B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å Mistral-7B, Llama3-8B, Gemma2-9B –∏ Qwen2-7B. Qwen2.5-7B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏–∫–æ–≤ –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–æ –º–Ω–æ–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—é. –û–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –¥–æ—Å—Ç–∏–≥–∞—è 74.2 –≤ –æ–±—â–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMLU, 49.8 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MATH, –∏ 57.9 –≤ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ HumanEval.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π Qwen2.5-0.5B/1.5B/3B —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏</strong></p>\n<p>–ú–æ–¥–µ–ª–∏ Qwen2.5-0.5B, 1.5B –∏ 3B —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Å Qwen2-0.5B/1.5B –∏ Gemma2-2.6B. Qwen2.5-0.5B, 1.5B –∏ 3B –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, Qwen2.5-0.5B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Gemma2-2.6B –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è.</p>"
            },
            {
                "title": "Instruction-tuned Model",
                "content": "To critically evaluate instruction-tuned models, we adopt multifaceted approach. Foundational skills and human preferences are assessed using open datasets and benchmarks. Additionally, our detailed in-house evaluations delve deeper into the modelscompetencies in key areas and multilingualism. particular focus is placed on assessing long-context capability. The subsequent sections outline the evaluation methods and present the results. 10 Table 6: Performance of the 70B+ Instruct models and Qwen2.5-Plus. Datasets Llama-3.1-70B Llama-3.1-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus MMLU-Pro MMLU-redux LiveBench GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 66.4 83.0 46.6 46.7 68.0 95.1 80.5 84.2 68.2 32. 83.6 55.7 8.79 General Tasks 73.3 86.2 53.2 64.4 81.6 41.5 Mathematics & Science Tasks 51.1 73.8 96. Coding Tasks 89.0 84.5 73.5 41.6 42.4 69.0 93.2 86.0 80.2 69.2 32.2 Alignment Tasks 86.0 69.3 9. 77.6 48.1 9.12 71.1 86.8 52.3 49.0 83.1 95.8 86.6 88.2 75.1 55.5 84.1 81.2 9.35 72.5 86.3 54. 49.7 84.7 96.0 87.8 85.5 77.0 51.4 86.3 81.4 9.30 Table 7: Performance of the 14B-30B+ instruction-tuned models and Qwen2.5-Turbo. Datasets Qwen2-57BA14B Gemma2-27B GPT4o-mini Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B MMLU-Pro MMLU-redux LiveBench 0831 GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 52.8 72.6 31.1 34.3 49.1 85. 79.9 70.9 66.4 22.5 59.9 17.8 8.55 General Tasks 55.5 75.7 39.6 63.1 81.5 43.3 Mathematics & Science Tasks 38.4 54.4 90.4 78.7 81.0 67.4 - 77.1 57.5 9.10 40.2 70.2 93.2 Coding Tasks 88.4 85.7 75.0 40. Alignment Tasks 80.4 74.9 - 64.5 81.7 42.3 42.3 81.1 93.8 86.6 82.8 73.7 37.8 76.3 67.1 8. 63.7 80.0 44.4 45.5 80.0 94.8 83.5 82.0 72.8 42.6 81.0 68.3 8.88 69.0 83.9 50.7 49.5 83.1 95. 88.4 84.0 75.4 51.2 79.5 74.5 9.",
                "summary": "<p>–î–ª—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–Ω–æ–≥–æ–≥—Ä–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥. –ë–∞–∑–æ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –¥–µ—Ç–∞–ª—å–Ω–µ–µ –∏–∑—É—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –≤ –∫–ª—é—á–µ–≤—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, –∞ —Ç–∞–∫–∂–µ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 6 –∏ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–¢–∞–±–ª–∏—Ü–∞ 6</strong> –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 70 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –±–æ–ª–µ–µ, –∞ —Ç–∞–∫–∂–µ Qwen2.5-Plus. –í —Ç–∞–±–ª–∏—Ü–µ —É–∫–∞–∑–∞–Ω—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –Ω–∞–±–æ—Ä–∞–º –¥–∞–Ω–Ω—ã—Ö: MMLU-Pro, MMLU-redux, LiveBench, GPQA, MATH, GSM8K, HumanEval, MBPP, MultiPL-E, LiveCodeBench, IFEval, Arena-Hard –∏ MTbench. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–∞–¥–∞—á–∏ (General Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ (Mathematics &amp; Science Tasks), –∑–∞–¥–∞—á–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é (Coding Tasks) –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º (Alignment Tasks).</p>\n<p><strong>–¢–∞–±–ª–∏—Ü–∞ 7</strong> –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π —Å 14-30 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ Qwen2.5-Turbo. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–µ –∂–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á, —á—Ç–æ –∏ –≤ —Ç–∞–±–ª–∏—Ü–µ 6.</p>\n<p>–ò–∑ —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–¥–µ–ª–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –æ–±—â–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ –∑–∞–¥–∞—á–∞–º–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –Ω–∞—É–∫–µ, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–æ–¥–µ–ª–∏ —Å –º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.</p>"
            },
            {
                "title": "Open Benchmark Evaluation",
                "content": "To comprehensively evaluate the quality of instruction-tuned models, we compile automatic and human evaluation to assess the capabilities and human preference. For the evaluation of basic capabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural language understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU-Pro, MMLU-redux and LiveBench 0831 (White et al., 2024) for general evaluation, GPQA, GSM8K and MATH for science and mathematics, HumanEval, MBPP, MultiPL-E and LiveCodeBench 2305-2409 (Jain et al., 2024) for coding, IFEval (Zhou et al., 2023)2 for instruction following. Additionally, we assess the performance of human preference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng et al., 2023) and Arena-Hard (Li et al., 2024). Qwen2.5-72B-Instruct & Qwen2.5-Plus As shown in Table 6, we compare Qwen2.5-72B-Instruct and Qwen2.5-Plus to other leading open-weight instrution-tuned models: Llama3.1-70B-Instruct (Dubey 2For simplicity, we report the results of the subset strict-prompt. 11 Table 8: Performance of the 7B+ instruction-tuned models. Datasets Gemma2-9B Llama3.1-8B Qwen2-7B Qwen2.5-7B General Tasks MMLU-Pro MMLU-redux LiveBench 0831 52.1 72.8 30.6 48.3 67.2 26.7 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval Arena-Hard MTbench 32.8 44.3 76.7 68.9 74.9 53.4 18.9 70.1 41.6 8. 32.8 51.9 84.5 Coding Tasks 72.6 69.6 50.7 8.3 Alignment Tasks 75.9 27.8 8.23 44.1 67.3 29. 34.3 52.9 85.7 79.9 67.2 59.1 23.9 54.7 25.0 8.26 56.3 75.4 35.9 36.4 75.5 91.6 84.8 79.2 70.4 28. 71.2 52.0 8.75 Table 9: Performance comparison of 2B-4B instruction-tuned models. Datasets Gemma2-2B Phi3.5-Mini MiniCPM3-4B Qwen2.5-3B Non-Emb Params 2.0B 3.6B 4.0B 2.8B General Tasks MMLU-Pro MMLU-redux LiveBench 0831 26.7 51.9 20. 47.5 67.7 27.4 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval 29.3 26.6 63. 68.9 74.9 30.5 5.8 51.0 27.2 48.5 86.2 Coding Tasks 72.6 63.2 47.2 15.8 Alignment Tasks 52.1 43.0 59.9 27.6 31.3 46.6 81.1 74.4 72.5 49.1 23.8 68.4 43.7 64.4 26. 30.3 65.9 86.7 74.4 72.7 60.2 19.9 58.2 et al., 2024), Llama3.1-405B-Instruct (Dubey et al., 2024), and our previous 72B version, Qwen2-72BInstruct (Yang et al., 2024a). The Qwen2.5-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B-Instruct in several critical benchmarks including MMLU-redux, MATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard and MTBench. Moreover, Qwen2.5-Plus outperforms Qwen2.5-72B-Instruct on 9 out of 13 benchmarks. Qwen2.5-14B/32B-Instruct & Qwen2.5-Turbo The performance of the Qwen2.5-Turbo, Qwen2.5-14BInstruct, and Qwen2.5-32B-Instruct models is evaluated and compared against baselines of similar sizes. The baselines include GPT4o-mini, Gemma2-27B-IT (Gemma Team et al., 2024), and Qwen2-57BA14BInstruct (Yang et al., 2024a). The results are summarized in Table 7. The Qwen2.5-32B-Instruct model exhibits superior performance across most tasks when compared to other models of similar size. Notably, our open-weight Qwen2.5-14B-Instruct model delivers competitive results across all benchmarks, rivaling those of GPT-4o-mini. Despite its significantly lower training and inference costs, the Qwen2.5-Turbo model outperforms Qwen2.5-14B-Instruct on eight out of ten benchmarks. This demonstrates that Qwen2.5-Turbo achieves remarkable efficiency and effectiveness, making it compelling choice for resource-constrained environments. Table 10: Performance comparison of 0.5B-1.5B instruction-tuned models. Datasets Qwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B General Tasks MMLU-Pro MMLU-redux LiveBench 14.4 12.9 7. 15.0 24.1 12.6 Mathematics & Science Tasks GPQA MATH GSM8K HumanEval MBPP MultiPL-E LiveCodeBench IFEval 23.7 13.9 40. 31.1 39.7 20.8 1.6 14.6 29.8 34.4 49.6 Coding Tasks 35.4 49.6 28.5 5.1 Alignment Tasks 27.9 22.9 41.2 12.4 21.2 25.3 61.6 42.1 44.2 38.5 4.5 29.0 32.4 50.7 18. 29.8 55.2 73.2 61.6 63.2 50.4 14.8 42.5 Table 11: Performance Comparison on our in-house English automatic evaluation benchmark. Models IF Knowledge Comprehension Coding Math Reasoning Proprietary LLMs GPT-4o-2024-08-06 GPT-4o-2024-11-20 Claude3.5-sonnet-2024-10-22 83.28 80.06 84.22 68.08 65.25 74.61 Qwen2-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-0.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-Turbo Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Plus 18.33 29.42 50.47 76.08 81.33 83.33 33.35 40.25 60.60 70.01 74.17 72.76 76.79 82.65 83.18 Qwen2 Series 18.59 29.23 44.79 59.49 Llama-3.1 Series 63.42 67.10 Qwen2.5 Series 30.29 41.19 46.11 52.74 59.78 58.56 64.08 66.09 68.41 76.51 79.07 79. 30.64 45.81 58.04 72.19 69.29 75.55 29.78 47.69 57.98 62.69 69.11 68.70 71.28 74.43 79.35 58.05 60.19 67.17 5.42 17.02 43.04 48.95 52.36 49.74 48. 13.16 20.34 38.31 48.07 55.96 58.14 48.00 47.09 15.41 26.19 41.43 48.41 52.68 54.48 58.90 60.41 59.58 26.29 40.99 49.38 56.93 59.68 57.77 60.97 59.73 62.52 66.45 67.07 70. 32.03 38.86 50.25 60.33 63.18 64.74 36.13 42.23 49.80 54.69 62.51 61.06 65.49 65.90 66.92 Other Instruction-tuned Models As illustrated in Table 8, the Qwen2.5-7B-Instruct model significantly outperforms its competitors, Gemma2-9B-IT and Llama3.1-8B-Instruct, across all tasks except IFEval. Notably, Qwen2.5-7B-Instruct exhibits clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8). For the edge-side instruction models, the Qwen2.5-3B-Instruct model, despite having fewer parameters than both the Phi3.5-mini-instruct (Abdin et al., 2024) and MiniCPM3-4B-Instruct (Hu et al., 2024) models, surpasses them in mathematics and coding tasks, as shown in Table 9. Additionally, it delivers competitive results in language understanding. The Qwen2.5-1.5B-Instruct and Qwen2.5-0.5BInstruct models have also seen substantial performance improvements over their previous versions, as detailed in Table 10. These enhancements make them particularly well-suited for edge-side applications in highly resource-constrained environments. 13 Table 12: Performance Comparison on our in-house Chinese automatic evaluation benchmark. Models IF Knowledge Comprehension Coding Math Reasoning Proprietary LLMs GPT-4o-2024-08-06 GPT-4o-2024-11-20 Claude3.5-sonnet-2024-10-22 42.50 42.71 49. 68.55 71.29 72.09 Qwen2-0.5B-Instruct Qwen2-1.5B-Instruct Qwen2-7B-Instruct Qwen2-72B-Instruct Llama-3.1-70B-Instruct Llama-3.1-405B-Instruct Qwen2.5-0.5B-Instruct Qwen2.5-1.5B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-Turbo Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Plus 4.69 6.81 16.83 31.98 28.96 30. 6.12 7.38 16.50 26.64 26.87 32.94 32.64 37.22 46.15 Qwen2 Series 40.43 51.54 65.95 74.96 Llama-3.1 Series 57.41 63.79 Qwen2.5 Series 39.13 48.68 57.18 65.77 70.28 72.93 74.70 75.86 72.07 80.11 83.04 82.16 39.13 46.89 60.30 75.49 67.24 72.27 42.97 49.69 62.55 67.55 76.96 74.37 79.46 78.85 82.64 61.53 62.39 66. 9.85 14.14 37.05 41.57 61.74 66.04 63.71 14.07 24.57 50.52 65.55 54.82 60.73 41.18 46.05 9.60 22.96 29.88 39.56 49.78 51.92 54.45 56.71 58. 24.03 37.30 51.64 61.06 67.01 66.08 67.86 68.39 69.96 56.88 62.04 66.60 32.73 35.19 44.96 58.19 52.42 55.88 33.72 39.17 39.57 49.70 56.41 53.30 60.19 63.02 62.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π, –æ–±—É—á–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫ –∏ –æ—Ü–µ–Ω–∫–∞ –ª—é–¥—å–º–∏, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏—Ö –ø–æ–≤–µ–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –±–∞–∑–æ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ç–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä—è—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ —É–º–µ–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–∞—Ç—å. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è MMLU-Pro, MMLU-redux –∏ LiveBench 0831. –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è GPQA, GSM8K –∏ MATH. –ù–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é HumanEval, MBPP, MultiPL-E –∏ LiveCodeBench 2305-2409. –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è IFEval.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–µ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –∏ –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –¥–ª—è —á–µ–≥–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è MT-Bench –∏ Arena-Hard.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª–∏ Qwen2.5-72B-Instruct –∏ Qwen2.5-Plus —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –æ–±—É—á–µ–Ω–Ω—ã–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Llama3.1-70B-Instruct, Llama3.1-405B-Instruct –∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è Qwen2-72B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-72B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—É—é –º–æ–¥–µ–ª—å Llama-3.1-405B-Instruct –ø–æ —Ä—è–¥—É –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è MMLU-redux, MATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard –∏ MTBench. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Qwen2.5-Plus –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-72B-Instruct –ø–æ 9 –∏–∑ 13 –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.</p>\n<p>–¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π Qwen2.5-Turbo, Qwen2.5-14B-Instruct –∏ Qwen2.5-32B-Instruct –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT4o-mini, Gemma2-27B-IT –∏ Qwen2-14B-Instruct. –ú–æ–¥–µ–ª—å Qwen2.5-32B-Instruct –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –∑–∞–¥–∞—á –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ü—Ä–∏ —ç—Ç–æ–º –º–æ–¥–µ–ª—å Qwen2.5-14B-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å GPT-4o-mini. –ú–æ–¥–µ–ª—å Qwen2.5-Turbo, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Qwen2.5-14B-Instruct –ø–æ –≤–æ—Å—å–º–∏ –∏–∑ –¥–µ—Å—è—Ç–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è —Å—Ä–µ–¥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-7B-Instruct –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, Gemma2-9B-IT –∏ Llama3.1-8B-Instruct, –ø–æ –≤—Å–µ–º –∑–∞–¥–∞—á–∞–º, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º IFEval. –û—Å–æ–±–µ–Ω–Ω–æ –∑–∞–º–µ—Ç–Ω—ã –µ—ë –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ (MATH: 75.5) –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ (HumanEval: 84.8). –ú–æ–¥–µ–ª—å Qwen2.5-3B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª–∏ Phi3.5-mini-instruct –∏ MiniCPM3-4B-Instruct –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏, –∞ —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª–∏ Qwen2.5-1.5B-Instruct –∏ Qwen2.5-0.5B-Instruct —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –≥–¥–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –∑–Ω–∞–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</p>"
            },
            {
                "title": "In-house Automatic Evaluation",
                "content": "Despite the availability of several open benchmark datasets for evaluation, we believe that these are insufficient to fully capture the capabilities of LLMs. To address this, we have developed series of in-house datasets designed to assess various aspects of model performance, including knowledge understanding, text generation, coding, and more. These evaluations are conducted in both Chinese and English. In addition, we have specifically evaluated the multilingual performance of instructiontuned models. The results are summarized in Table 11 for English, Table 12 for Chinese, Table 13 for multilingualism of 70B+ Instruct models, and Table 14 for 7B-14B models, respectively. English & Chinese Evaluation We compare the performance of Qwen2.5-Instruct models against several leading language models, including GPT-4, Claude3.5-sonnet, Qwen2, and Llama-3.1, across both English and Chinese languages. Our analysis focuses on model size and its impact on performance, as well as how our latest Qwen2.5 series compares to previous iterations and competing models. For smaller models, we observe that the Qwen2.5-0.5B model achieves performance that is on par with or even surpasses the Qwen2-1.5B model. This indicates that the Qwen2.5 series has optimized parameter usage, enabling mid-sized models to achieve similar performance levels to larger models from the previous generation. The Qwen2.5-3B model demonstrates performance that is comparable to the Qwen2-7B model. Notably, the Qwen2.5-32B model exhibits remarkable improvement over the Qwen2-72B model. Our flagship model, Qwen2.5-72B, further narrows the gap between Qwen and state-of-the-art models like GPT-4 and Claude3.5-sonnet. In particular, Qwen2.5-72B matches or exceeds the performance of Llama-3.1-405B in all metrics except for instruction following. This achievement underscores the competitiveness of Qwen2.5-72B in wide range of language processing tasks, while also identifying areas for future improvement. Qwen2.5-Plus addresses the previous shortcomings in Chinese instruction following and further enhances its advantages in other areas. Multilingual Evaluation To comprehensively evaluate the multilingual capabilities of instruction-tuned models, we followed P-MMEval (Zhang et al., 2024) and extended several benchmarks as follows: (1) IFEval (Multilingual): We expanded the IFEval benchmark, originally in English, to include multilingual examples. To ensure language neutrality, we removed instances that contained language-specific content (e.g., start with letter A). (2) Knowledge Utilization: to assess the knowledge utilization abilities of the Qwen2.5 series models across multiple languages, we employed five MMLU-like benchmarks (multiple-choice format). These benchmarks include: AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Additionally, we evaluated the models performance on the translated version of the MMLU benchmark (okapi MMLU), which has been adapted 14 Table 13: Performance of the 70B+ Instruct models on Multilingual Tasks. Datasets Qwen2-72B Llama3.1-70B Qwen2.5-32B Mistral-Large GPT4o-mini Qwen2.5-72B IFEval (multilingual) 79.69 80.47 82.68 82. 85.03 86.98 Instruction Following AMMLU (Arabic) JMMLU (Japanese) KMMLU (Korean) IndoMMLU (Indonesian) TurkishMMLU (Turkish) okapi MMLU (translated) 68.85 77.37 57.04 66.31 69.22 77.84 70.08 73.89 53.23 67.50 66.89 76. Knowledge 70.44 76.55 60.75 66.42 72.41 77.16 Math Reasoning 69.24 75.77 56.42 63.21 64.78 78.37 69.73 73.74 56.77 67.75 71.19 73.44 72.44 80.56 61.96 69.25 76.12 79. MGSM8K (extended) 82.72 73.31 87.15 89.01 87. 88.16 Cultural Nuances BLEnD 25.90 30.49 27. 33.47 35.91 32.48 Table 14: Performance of the 7B-14B Instruct models on Multilingual Tasks. Datasets Qwen2-7B Llama3.1-8B Qwen2.5-7B Gemma2-9B Qwen2.5-14B IFEval (multilingual) 51.43 60.68 74.87 77.47 77. Instruction Following AMMLU (Arabic) JMMLU (Japanese) KMMLU (Korean) IndoMMLU (Indonesian) TurkishMMLU (Turkish) okapi MMLU (translated) 54.87 57.71 43.96 54.05 49.27 60.47 Knowledge 54.28 53.26 42.28 53.92 45.61 55.18 59.78 61.88 46.59 56.42 54.28 66. Math Reasoning 60.26 64.59 46.24 61.73 55.44 46.72 66.81 72.78 59.71 65.09 66.85 72.12 MGSM8K (extended) 56.13 66. 66.11 78.37 82.27 Cultural Nuances BLEnD 22. 19.47 23.66 28.31 26.99 into multiple languages from its original English form. (3) MGSM8K (Extended): Building upon the original MGSM8K benchmark, we extended the language support to include Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). (4) Cultural Nuances: To evaluate the models ability to capture cultural nuances, we utilized the BLEnD benchmark (Myung et al., 2024). This benchmark is specifically designed to test LLMs on their understanding of cultural subtleties. Qwen2.5 exhibits competitive performance in instruction following, multilingual knowledge, and mathematical reasoning, aligning well with models of comparable size. Although it shows notable improvements in capturing cultural nuances relative to its predecessor, Qwen2, there remains potential for further refinement in this domain.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π Qwen2.5, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ GPT-4, Claude3.5-sonnet –∏ Llama-3.1. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã: –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä—É–≥–∏–µ. –ò—Å–ø—ã—Ç–∞–Ω–∏—è –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –∫–∞–∫ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, —Ç–∞–∫ –∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö</strong></p>\n<p>–°—Ä–∞–≤–Ω–∏–≤–∞–ª–∏—Å—å –º–æ–¥–µ–ª–∏ Qwen2.5 —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ —Å –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:\n*   –ú–æ–¥–µ–ª—å Qwen2.5-0.5B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π Qwen2-1.5B.\n*   Qwen2.5-3B —Å—Ä–∞–≤–Ω–∏–º–∞ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å Qwen2-7B.\n*   Qwen2.5-32B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-72B.\n*   –§–ª–∞–≥–º–∞–Ω—Å–∫–∞—è –º–æ–¥–µ–ª—å Qwen2.5-72B —Å–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑—Ä—ã–≤ —Å GPT-4 –∏ Claude3.5-sonnet, –∞ —Ç–∞–∫–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Llama-3.1-405B –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º, –∫—Ä–æ–º–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>–ú–æ–¥–µ–ª—å Qwen2.5-Plus —É–ª—É—á—à–∏–ª–∞ —Å–≤–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –¥—Ä—É–≥–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p><strong>–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –±—ã–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã:\n1.  <strong>IFEval (Multilingual):</strong> –†–∞—Å—à–∏—Ä–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç IFEval –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –≤ —Å–µ–±—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã. –ü—Ä–∏ —ç—Ç–æ–º –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —è–∑—ã–∫–æ-–∑–∞–≤–∏—Å–∏–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ—Å—Ç–∏.\n2.  <strong>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∑–Ω–∞–Ω–∏–π:</strong> –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Å –ø–æ–º–æ—â—å—é –ø—è—Ç–∏ —Ç–µ—Å—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö MMLU, –Ω–∞ –∞—Ä–∞–±—Å–∫–æ–º (AMMLU), —è–ø–æ–Ω—Å–∫–æ–º (JMMLU), –∫–æ—Ä–µ–π—Å–∫–æ–º (KMMLU), –∏–Ω–¥–æ–Ω–µ–∑–∏–π—Å–∫–æ–º (IndoMMLU) –∏ —Ç—É—Ä–µ—Ü–∫–æ–º (TurkishMMLU) —è–∑—ã–∫–∞—Ö. –¢–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ MMLU (okapi MMLU).\n3.  <strong>MGSM8K (Extended):</strong> –Ø–∑—ã–∫–æ–≤–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ MGSM8K –±—ã–ª–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –∑–∞ —Å—á—ë—Ç –∞—Ä–∞–±—Å–∫–æ–≥–æ, –∫–æ—Ä–µ–π—Å–∫–æ–≥–æ, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–æ–≥–æ –∏ –≤—å–µ—Ç–Ω–∞–º—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤.\n4.  <strong>–ö—É–ª—å—Ç—É—Ä–Ω—ã–µ –Ω—é–∞–Ω—Å—ã:</strong> –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —Ç–µ—Å—Ç BLEnD –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Qwen2.5 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å –º–æ–¥–µ–ª—è–º–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –Ω—é–∞–Ω—Å–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2, –Ω–æ —ç—Ç–∞ –æ–±–ª–∞—Å—Ç—å –≤—Å—ë –µ—â—ë —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏.</p>"
            },
            {
                "title": "Reward Model",
                "content": "The reward model serves as the cornerstone for guiding RL processes, and thus we conduct separate evaluation of the reward model used in the Qwen2.5 series. Our assessment benchmarks encompass Reward Bench (Lambert et al., 2024), RMB (Zhou et al., 2024), PPE (Frick et al., 2024b), and an internally collected out-of-domain Chinese human preference benchmark (Human-Preference-Chinese) to provide comprehensive analysis. For comparison, we included baseline models such as Nemotron-4-340BReward (Adler et al., 2024), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024c), and Athene-RM70B (Frick et al., 2024a). The results are shown in Table 15. Overall, our findings indicate that Llama-3.1Nemotron-70B-Reward excels on the Reward Bench, while Athene-RM-70B performs best on the RMB benchmark. The Qwen2.5-RM-72B, leads in both the PPE and Human-Preference-Chinese evaluations, ranking second only to Athene-RM-70B on the RMB and achieving performance level comparable to 15 Table 15: Performance comparison across multiple RM benchmarks. Metric Chat Chat Hard Safety Reasoning Score Helpfulness (BoN) Helpfulness (Pairwise) Harmlessness (BoN) Harmlessness (Pairwise) Overall Human Preference IFEval GPQA MATH MBPP-Plus MMLU-Pro Objective-Avg Nemotron-4-340BReward Llama-3.1-Nemotron70B-Reward Athene-RM -70B Qwen2.5-RM -72B Reward Bench 95.80 87.10 91.50 93.60 92.00 48.85 68.70 50.92 70.84 59.83 59.28 62.66 56.56 65.12 49.15 69.69 60.64 RMB PPE 97.50 85.70 95.10 98.10 94. 61.02 75.28 52.00 69.96 64.57 64.32 63.40 59.14 69.73 55.62 70.20 63.62 98.32 70.61 92.10 92.19 88.32 67.24 80.82 67.02 80.83 73.98 66.48 62.15 59.26 79.14 67.97 76.95 69.09 97.21 78.73 92.71 97.65 91. 65.72 78.83 56.35 73.94 68.71 64.80 67.97 59.80 81.48 64.34 75.66 69.85 Accuracy 50.46 59.95 61. 61.27 Human-Preference-Chinese Nemotron-4-340B-Reward on the Reward Bench, albeit slightly behind Llama-3.1-Nemotron-70B-Reward. Due to the lack of evaluation methods for reward models, current reward models are typically evaluated using Reward Bench. However, our evaluation results from multiple RM benchmarks suggest that overoptimization on specific benchmark may trigger Goodharts law (Hoskin, 1996), resulting in degraded performance on other benchmarks and potentially impacting downstream alignment performance. This highlights the need for comprehensive evaluation of reward models across diverse benchmarks rather than relying solely on single benchmark. More importantly, through iterative experimentation, we have also come to recognize critical limitation: current reward model evaluation benchmarks do not accurately predict the performance of the RL models trained under their guidance. In other words, higher score on RM benchmarks does not necessarily correlate with superior performance of the resulting RL model. This insight underscores the need for further research into more predictive evaluation methods for reward models.",
                "summary": "<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (reward model, RM), –∫–æ—Ç–æ—Ä–∞—è –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –æ—Ü–µ–Ω–∫—É RM, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –≤ —Å–µ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤: Reward Bench, RMB, PPE –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ (Human-Preference-Chinese). –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –º–æ–¥–µ–ª–∏ Nemotron-4-340BReward, Llama-3.1-Nemotron-70B-Reward –∏ Athene-RM70B.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ Llama-3.1-Nemotron-70B-Reward –ª–∏–¥–∏—Ä—É–µ—Ç –Ω–∞ Reward Bench, –∞ Athene-RM-70B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ RMB. –ú–æ–¥–µ–ª—å Qwen2.5-RM-72B –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ PPE –∏ Human-Preference-Chinese, –∞ –Ω–∞ RMB –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –ø–æ—Å–ª–µ Athene-RM-70B. –ù–∞ Reward Bench –æ–Ω–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–º—ã–µ —Å Nemotron-4-340B-Reward, —Ö–æ—Ç—è –∏ –Ω–µ–º–Ω–æ–≥–æ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç Llama-3.1-Nemotron-70B-Reward.</p>\n<p>–ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —á–∞—Å—Ç–æ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ Reward Bench. –û–¥–Ω–∞–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –ø–µ—Ä–µ–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞ –æ–¥–Ω–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —É—Ö—É–¥—à–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –¥—Ä—É–≥–∏—Ö. –≠—Ç–æ—Ç —ç—Ñ—Ñ–µ–∫—Ç, –∏–∑–≤–µ—Å—Ç–Ω—ã–π –∫–∞–∫ –∑–∞–∫–æ–Ω –ì—É–¥—Ö–∞—Ä—Ç–∞, –º–æ–∂–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ —Å–∫–∞–∑–∞—Ç—å—Å—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –ü–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ RM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö.</p>\n<p>–ë–æ–ª–µ–µ —Ç–æ–≥–æ, –∞–≤—Ç–æ—Ä—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RM –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ RL-–º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ–±—É—á–µ–Ω—ã —Å –∏—Ö –ø–æ–º–æ—â—å—é. –¢–æ –µ—Å—Ç—å, –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö RM –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å RL-–º–æ–¥–µ–ª–µ–π. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>"
            },
            {
                "title": "Long Context Capabilities",
                "content": "We utilize three benchmarks to evaluate long context capabilities of Qwen2.5 models: RULER (Hsieh et al., 2024), LV-Eval (Yuan et al., 2024), and Longbench-Chat (Bai et al., 2024). In LV-Eval, we adopt keyword recall as the reported score to mitigate the high rate of false negatives present in the original metrics. The results are shown in Table 16 and Table 17. We can observe that the Qwen2.5 models, after equipping length extrapolation techniques (i.e., DCA + YARN), have demonstrated strong long context processing capabilities on the three datasets. Among them, Qwen2.5-72B-Instruct has shown the strongest performance across all context lengths, significantly outperforming existing open-weight long-context models as well as the proprietary models like GPT-4o-mini and GPT-4. Furthermore, as shown in Figure 2, Qwen2.5-Turbo achieves 100% accuracy in the 1M-token passkey retrieval task, demonstrating its exceptional ability to capture detailed information from ultra-long contexts. We introduce sparse attention mechanism to significantly enhance inference speed, which is critical for user experience when processing long contexts. For sequences of 1M tokens, this approach reduces the computational load of the attention mechanism by 12.5 times. Figure 3 illustrates the time to first token (TTFT) of Qwen2.5-Turbo across various hardware configurations, where our method achieves 3.2 to 4.3 times speedup. 16 Table 16: Performance of Qwen2.5 Models on RULER. YARN+DCA does not change the model behavior within 32K tokens. Model GLM4-9b-Chat-1M Llama-3-8B-Instruct-Gradient-1048k Llama-3.1-70B-Instruct GPT-4o-mini GPT-4 Qwen2.5-7B-Instruct w/o DCA + YARN Qwen2.5-14B-Instruct w/o DCA + YARN Qwen2.5-32B-Instruct w/o DCA + YARN Qwen2.5-72B-Instruct w/o DCA + YARN Qwen2.5-Turbo Claimed Length Avg. 4K 1M 89.9 1M 88.3 128K 89.6 128K 87.3 128K 91.6 128K 85.4 80.1 128K 91.4 86.5 128K 92.9 88.0 128K 95.1 90.8 1M 93.1 94.7 95.5 96.5 95.0 96.6 96.7 96.7 97.7 97.7 96.9 96.9 97.7 97.7 97. RULER 8K 92.8 93.8 95.8 92.9 96.3 95.1 95.1 96.8 96.8 97.1 97.1 97.2 97.2 95.7 16K 32K 64K 128K 92.1 91.6 95.4 92.7 95.2 93.7 93.7 95.9 95.9 95.5 95.5 97.7 97.7 95.5 89.9 87.4 94.8 90.2 93.2 89.4 89.4 93.4 93.4 95.5 95.5 96.5 96.5 94. 86.7 84.7 88.4 87.6 87.0 82.3 74.5 86.7 82.3 90.3 85.3 93.0 88.5 90.8 83.1 77.0 66.6 65.8 81.2 55.1 31.4 78.1 53.0 82.0 57.7 88.4 67.0 84. Table 17: Performance of Qwen2.5 Models on LV-Eval and LongBench-Chat. YARN+DCA does not change the model behavior within 32k tokens. Model GLM4-9B-Chat-1M Llama-3-8B-Instruct-Gradient-1048k Llama-3.1-70B-Instruct GPT-4o-mini Qwen2.5-7B-Instruct w/o DCA + YARN Qwen2.5-14B-Instruct w/o DCA + YARN Qwen2.5-32B-Instruct w/o DCA + YARN Qwen2.5-72B-Instruct w/o DCA + YARN Qwen2.5-Turbo Claimed Length 16k 1M 46.4 1M 31.7 48.6 52.9 128k 128k 128k 128k 128k 128k 55.9 55.9 53.0 53.0 56.0 56.0 60.4 60.4 1M 53.4 32k 43.2 31.8 47.4 48.1 49.7 49.7 50.8 50.8 53.6 53.6 57.5 57. 50.0 LV-Eval 64k 128k 256k LongBenchChat 42.9 28.8 42.9 46.0 48.0 33.1 46.8 37.0 48.8 40.1 53.9 47.4 45.4 37.0 40.4 26.3 21.1 26.2 N/A 40.7 N/A 41.1 13.6 43.6 18.4 45.3 20.5 50.9 27.0 43. 36.9 0.5 39.4 0.8 41.0 0.7 45.2 2.4 38.0 7.82 6.20 6.80 8.48 7.42 - 8.04 - 8.70 - 8.72 - 8.34 Figure 2: Performance of Qwen2.5-Turbo on Passkey Retrieval Task with 1M Token Lengths. 17 Figure 3: TTFT (Time To First Token) of Qwen2.5-Turbo and Qwen2.5-7B with Full Attention and Our Method.",
                "summary": "<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π Qwen2.5 –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ç—Ä–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: RULER, LV-Eval –∏ Longbench-Chat. –í LV-Eval –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å \"keyword recall\" (–ø–æ–ª–Ω–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤), —á—Ç–æ–±—ã —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω–æ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö 16 –∏ 17.</p>\n<p>–ò–∑ —ç—Ç–∏—Ö —Ç–∞–±–ª–∏—Ü –≤–∏–¥–Ω–æ, —á—Ç–æ –º–æ–¥–µ–ª–∏ Qwen2.5, –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏–∫ —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏–∏ –¥–ª–∏–Ω—ã (DCA + YARN), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç—Ä–µ—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Qwen2.5-72B-Instruct, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –¥–ª–∏–Ω–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫ –∏ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-4o-mini –∏ GPT-4.</p>\n<p>–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 2, –º–æ–¥–µ–ª—å Qwen2.5-Turbo –¥–æ—Å—Ç–∏–≥–∞–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è passkey (–∫–æ–¥–æ–≤–∞—è —Ñ—Ä–∞–∑–∞) –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ—ë –≤—ã—Å–æ–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.</p>\n<p>–î–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏, –±—ã–ª –≤–Ω–µ–¥—Ä–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (sparse attention). –î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π –≤ 1 –º–∏–ª–ª–∏–æ–Ω —Ç–æ–∫–µ–Ω–æ–≤ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –≤ 12.5 —Ä–∞–∑. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ (TTFT) –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen2.5-Turbo –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ 3.2-4.3 —Ä–∞–∑–∞.</p>"
            },
            {
                "title": "Conclusion",
                "content": "Qwen2.5 represents significant advancement in large language models (LLMs), with enhanced pretraining on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning. These improvements boost human preference alignment, long text generation, and structural data analysis, making Qwen2.5 highly effective for instruction-following tasks. Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters and proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus. Empirical evaluations show that Qwen2.5-72B-Instruct matches the performance of the state-of-the-art Llama-3-405B-Instruct, despite being six times smaller. Qwen2.5 also serves as foundation for specialized models, demonstrating its versatility for domain-specific applications. We believe that Qwen2.5s robust performance, flexible architecture, and broad availability make it valuable resource for both academic research and industrial applications, positioning it as key player of future innovations. In the future, we will focus on advancing robust foundational models. First, we will iteratively refine both base and instruction-tuned large language models (LLMs) by incorporating broader, more diverse, higherquality data. Second, we will also continue to develop multimodal models. Our goal is to integrate various modalities into unified framework. This will facilitate seamless, end-to-end information processing across textual, visual, and auditory domains. Third, we are committed to enhancing the reasoning capabilities of our models. This will be achieved through strategic scaling of inference compute resources. These efforts aim to push the boundaries of current technological limitations and contribute to the broader field of artificial intelligence.",
                "summary": "<p>–ú–æ–¥–µ–ª—å Qwen2.5 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∞ –±—ã–ª–∞ —É–ª—É—á—à–µ–Ω–∞ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ 18 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é —Ç–æ–Ω–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º. –≠—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–≤—ã—à–∞—é—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏, –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –¥–µ–ª–∞–µ—Ç Qwen2.5 –æ—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.</p>\n<p>Qwen2.5 –¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è—Ö, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –æ—Ç 0,5 –¥–æ 72 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã MoE (Mixture of Experts), —Ç–∞–∫–∏–µ –∫–∞–∫ Qwen2.5-Turbo –∏ Qwen2.5-Plus. –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Qwen2.5-72B-Instruct –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Llama-3-405B-Instruct, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –æ–Ω–∞ –≤ —à–µ—Å—Ç—å —Ä–∞–∑ –º–µ–Ω—å—à–µ. Qwen2.5 —Ç–∞–∫–∂–µ —Å–ª—É–∂–∏—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Å–≤–æ—é —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.</p>\n<p>–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≥–∏–±–∫–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ —à–∏—Ä–æ–∫–∞—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Qwen2.5 –¥–µ–ª–∞—é—Ç –µ–µ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –∫–∞–∫ –¥–ª—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ç–∞–∫ –∏ –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π, –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É—è –µ–µ –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–≥–æ –∏–≥—Ä–æ–∫–∞ –≤ –±—É–¥—É—â–∏—Ö –∏–Ω–Ω–æ–≤–∞—Ü–∏—è—Ö.</p>\n<p>–í –±—É–¥—É—â–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø–ª–∞–Ω–∏—Ä—É—é—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–∏ –Ω–∞–¥–µ–∂–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∏ –±—É–¥—É—Ç –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å –∫–∞–∫ –±–∞–∑–æ–≤—ã–µ, —Ç–∞–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–Ω–∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å—Ç—Ä–µ–º—è—Å—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –æ–±–µ—Å–ø–µ—á–∏—Ç—å –±–µ—Å—à–æ–≤–Ω—É—é —Å–∫–≤–æ–∑–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–π, –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏ —Å–ª—É—Ö–æ–≤–æ–π –æ–±–ª–∞—Å—Ç—è—Ö. –í-—Ç—Ä–µ—Ç—å–∏—Ö, –æ–Ω–∏ –Ω–∞–º–µ—Ä–µ–Ω—ã –ø–æ–≤—ã—Å–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–≤–æ–∏—Ö –º–æ–¥–µ–ª–µ–π –∑–∞ —Å—á–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è –≤—ã–≤–æ–¥–∞. –≠—Ç–∏ —É—Å–∏–ª–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Ç–µ–∫—É—â–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∏ –≤–∫–ª–∞–¥ –≤ —Ä–∞–∑–≤–∏—Ç–∏–µ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ —Ü–µ–ª–æ–º.</p>"
            }
        ]
    },
    {
        "id": "2412.19437",
        "title": "DeepSeek-V3 Technical Report",
        "url": "https://arxiv.org/abs/2412.19437",
        "abstract": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-27",
        "pub_date_card": {
            "ru": "27 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 27",
            "zh": "12Êúà27Êó•"
        },
        "hash": "b90a04c9971eb0b0",
        "authors": [
            "DeepSeek-AI",
            "Aixin Liu",
            "Bei Feng",
            "Bing Xue",
            "Bingxuan Wang",
            "Bochao Wu",
            "Chengda Lu",
            "Chenggang Zhao",
            "Chengqi Deng",
            "Chenyu Zhang",
            "Chong Ruan",
            "Damai Dai",
            "Daya Guo",
            "Dejian Yang",
            "Deli Chen",
            "Dongjie Ji",
            "Erhang Li",
            "Fangyun Lin",
            "Fucong Dai",
            "Fuli Luo",
            "Guangbo Hao",
            "Guanting Chen",
            "Guowei Li",
            "H. Zhang",
            "Han Bao",
            "Hanwei Xu",
            "Haocheng Wang",
            "Haowei Zhang",
            "Honghui Ding",
            "Huajian Xin",
            "Huazuo Gao",
            "Hui Li",
            "Hui Qu",
            "J. L. Cai",
            "Jian Liang",
            "Jianzhong Guo",
            "Jiaqi Ni",
            "Jiashi Li",
            "Jiawei Wang",
            "Jin Chen",
            "Jingchang Chen",
            "Jingyang Yuan",
            "Junjie Qiu",
            "Junlong Li",
            "Junxiao Song",
            "Kai Dong",
            "Kai Hu",
            "Kaige Gao",
            "Kang Guan",
            "Kexin Huang",
            "Kuai Yu",
            "Lean Wang",
            "Lecong Zhang",
            "Lei Xu",
            "Leyi Xia",
            "Liang Zhao",
            "Litong Wang",
            "Liyue Zhang",
            "Meng Li",
            "Miaojun Wang",
            "Mingchuan Zhang",
            "Minghua Zhang",
            "Minghui Tang",
            "Mingming Li",
            "Ning Tian",
            "Panpan Huang",
            "Peiyi Wang",
            "Peng Zhang",
            "Qiancheng Wang",
            "Qihao Zhu",
            "Qinyu Chen",
            "Qiushi Du",
            "R. J. Chen",
            "R. L. Jin",
            "Ruiqi Ge",
            "Ruisong Zhang",
            "Ruizhe Pan",
            "Runji Wang",
            "Runxin Xu",
            "Ruoyu Zhang",
            "Ruyi Chen",
            "S. S. Li",
            "Shanghao Lu",
            "Shangyan Zhou",
            "Shanhuang Chen",
            "Shaoqing Wu",
            "Shengfeng Ye",
            "Shengfeng Ye",
            "Shirong Ma",
            "Shiyu Wang",
            "Shuang Zhou",
            "Shuiping Yu",
            "Shunfeng Zhou",
            "Shuting Pan",
            "T. Wang",
            "Tao Yun",
            "Tian Pei",
            "Tianyu Sun",
            "W. L. Xiao",
            "Wangding Zeng",
            "Wanjia Zhao",
            "Wei An",
            "Wen Liu",
            "Wenfeng Liang",
            "Wenjun Gao",
            "Wenqin Yu",
            "Wentao Zhang",
            "X. Q. Li",
            "Xiangyue Jin",
            "Xianzu Wang",
            "Xiao Bi",
            "Xiaodong Liu",
            "Xiaohan Wang",
            "Xiaojin Shen",
            "Xiaokang Chen",
            "Xiaokang Zhang",
            "Xiaosha Chen",
            "Xiaotao Nie",
            "Xiaowen Sun",
            "Xiaoxiang Wang",
            "Xin Cheng",
            "Xin Liu",
            "Xin Xie",
            "Xingchao Liu",
            "Xingkai Yu",
            "Xinnan Song",
            "Xinxia Shan",
            "Xinyi Zhou",
            "Xinyu Yang",
            "Xinyuan Li",
            "Xuecheng Su",
            "Xuheng Lin",
            "Y. K. Li",
            "Y. Q. Wang",
            "Y. X. Wei",
            "Y. X. Zhu",
            "Yang Zhang",
            "Yanhong Xu",
            "Yanhong Xu",
            "Yanping Huang",
            "Yao Li",
            "Yao Zhao",
            "Yaofeng Sun",
            "Yaohui Li",
            "Yaohui Wang",
            "Yi Yu",
            "Yi Zheng",
            "Yichao Zhang",
            "Yifan Shi",
            "Yiliang Xiong",
            "Ying He",
            "Ying Tang",
            "Yishi Piao",
            "Yisong Wang",
            "Yixuan Tan",
            "Yiyang Ma",
            "Yiyuan Liu",
            "Yongqiang Guo",
            "Yu Wu",
            "Yuan Ou",
            "Yuchen Zhu",
            "Yuduan Wang",
            "Yue Gong",
            "Yuheng Zou",
            "Yujia He",
            "Yukun Zha",
            "Yunfan Xiong",
            "Yunxian Ma",
            "Yuting Yan",
            "Yuxiang Luo",
            "Yuxiang You",
            "Yuxuan Liu",
            "Yuyang Zhou",
            "Z. F. Wu",
            "Z. Z. Ren",
            "Zehui Ren",
            "Zhangli Sha",
            "Zhe Fu",
            "Zhean Xu",
            "Zhen Huang",
            "Zhen Zhang",
            "Zhenda Xie",
            "Zhengyan Zhang",
            "Zhewen Hao",
            "Zhibin Gou",
            "Zhicheng Ma",
            "Zhigang Yan",
            "Zhihong Shao",
            "Zhipeng Xu",
            "Zhiyu Wu",
            "Zhongyu Zhang",
            "Zhuoshu Li",
            "Zihui Gu",
            "Zijia Zhu",
            "Zijun Liu",
            "Zilin Li",
            "Ziwei Xie",
            "Ziyang Song",
            "Ziyi Gao",
            "Zizheng Pan"
        ],
        "affiliations": [
            "AI@Meta",
            "DeepSeek-AI",
            "Mistral",
            "Qwen"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.19437.jpg",
        "data": {
            "categories": [
                "#architecture",
                "#optimization",
                "#rl",
                "#open_source",
                "#training"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "DeepSeek-V3: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏ –º–æ—â–Ω–∞—è MoE-–º–æ–¥–µ–ª—å –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è",
                "desc": "DeepSeek-V3 - —ç—Ç–æ –º–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Ç–∏–ø–∞ Mixture-of-Experts (MoE) —Å 671 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö 37 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Multi-head Latent Attention (MLA) –∏ DeepSeekMoE –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. DeepSeek-V3 –≤–≤–æ–¥–∏—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å –∏ –º–Ω–æ–≥–æ—Ç–æ–∫–µ–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è –Ω–∞ 14,8 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤, –º–æ–¥–µ–ª—å –ø—Ä–æ—Ö–æ–¥–∏—Ç —ç—Ç–∞–ø—ã Supervised Fine-Tuning –∏ Reinforcement Learning, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤–µ–¥—É—â–∏—Ö –∑–∞–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π."
            },
            "en": {
                "title": "DeepSeek-V3: Unleashing the Power of Mixture-of-Experts in Language Modeling",
                "desc": "DeepSeek-V3 is a powerful Mixture-of-Experts (MoE) language model with 671 billion parameters, utilizing 37 billion parameters for each token processed. It incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE architectures to enhance efficiency during training and inference. The model introduces an innovative auxiliary-loss-free strategy for load balancing and employs a multi-token prediction objective to improve performance. After extensive pre-training on 14.8 trillion tokens and subsequent fine-tuning, DeepSeek-V3 demonstrates superior performance compared to other models while maintaining a stable training process."
            },
            "zh": {
                "title": "DeepSeek-V3ÔºöÈ´òÊïàÂº∫Â§ßÁöÑËØ≠Ë®ÄÊ®°Âûã",
                "desc": "DeepSeek-V3 ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑÊ∑∑Âêà‰∏ìÂÆ∂ÔºàMoEÔºâËØ≠Ë®ÄÊ®°ÂûãÔºåÊã•Êúâ6710‰∫ø‰∏™ÂèÇÊï∞ÔºåÊØè‰∏™tokenÊøÄÊ¥ª37‰∫øÂèÇÊï∞„ÄÇ‰∏∫‰∫ÜÂÆûÁé∞È´òÊïàÊé®ÁêÜÂíåÁªèÊµéÁöÑËÆ≠ÁªÉÔºåDeepSeek-V3ÈááÁî®‰∫ÜÂ§öÂ§¥ÊΩúÂú®Ê≥®ÊÑèÂäõÔºàMLAÔºâÂíåDeepSeekMoEÊû∂ÊûÑ„ÄÇËØ•Ê®°ÂûãÂú®14.8‰∏á‰∫øÂ§öÊ†∑ÂåñÁöÑÈ´òË¥®Èáètoken‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂ÁªèËøáÁõëÁù£ÂæÆË∞ÉÂíåÂº∫ÂåñÂ≠¶‰π†Èò∂ÊÆµÔºå‰ª•ÂÖÖÂàÜÂèëÊå•ÂÖ∂ËÉΩÂäõ„ÄÇËØÑ‰º∞ÁªìÊûúÊòæÁ§∫ÔºåDeepSeek-V3Âú®ÊÄßËÉΩ‰∏äË∂ÖË∂ä‰∫ÜÂÖ∂‰ªñÂºÄÊ∫êÊ®°ÂûãÔºåÂπ∂‰∏éÈ¢ÜÂÖàÁöÑÈó≠Ê∫êÊ®°ÂûãÁõ∏ÂΩìÔºåÂêåÊó∂ÂÖ∂ËÆ≠ÁªÉËøáÁ®ãÈùûÂ∏∏Á®≥ÂÆö„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.",
                "summary": "<p>–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –º–æ–¥–µ–ª—å DeepSeek-V3 ‚Äî –º–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Mixture-of-Experts (MoE). –û–Ω–∞ –∏–º–µ–µ—Ç 671 –º–∏–ª–ª–∏–∞—Ä–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ 37 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤. –î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ DeepSeek-V3 –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Multi-head Latent Attention (MLA) –∏ DeepSeekMoE, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ DeepSeek-V2.</p>\n<p>–ö–ª—é—á–µ–≤—ã–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ–º DeepSeek-V3 —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø–æ—Ç–µ—Ä—å (auxiliary-loss-free). –¢–∞–∫–∂–µ, –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.</p>\n<p>–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ 14,8 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Fine-Tuning) –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning) –¥–ª—è –ø–æ–ª–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –º–æ–¥–µ–ª–∏.</p>\n<p>–û–±—à–∏—Ä–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ DeepSeek-V3 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å –ª–∏–¥–∏—Ä—É—é—â–∏–º–∏ –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ü—Ä–∏ —ç—Ç–æ–º, –¥–ª—è –ø–æ–ª–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ DeepSeek-V3 –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –≤—Å–µ–≥–æ 2,788 –º–∏–ª–ª–∏–æ–Ω–∞ —á–∞—Å–æ–≤ GPU H800.</p>\n<p>–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –±—ã–ª –æ—á–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º, –±–µ–∑ –∫–∞–∫–∏—Ö-–ª–∏–±–æ –Ω–µ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º—ã—Ö —Å–∫–∞—á–∫–æ–≤ –ø–æ—Ç–µ—Ä—å –∏–ª–∏ –æ—Ç–∫–∞—Ç–æ–≤. –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ —Å—Å—ã–ª–∫–µ: https://github.com/deepseek-ai/DeepSeek-V3.</p>"
            },
            {
                "title": "Overview of machine learning architecture and training",
                "content": "1 Introduction 2 Architecture 2.1 Basic Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Multi-Head Latent Attention . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing . . . . . . . . . . 6 6 7 8 2.2 Multi-Token Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Infrastructures 3.1 Compute Clusters . . . 3.2 Training Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 DualPipe and Computation-Communication Overlap . . . . . . . . . . . . 3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . . . 3.2.3 Extremely Memory Saving with Minimal Overhead . . . . . . . . . . . . . 3.3 FP8 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Mixed Precision Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Improved Precision from Quantization and Multiplication . . . . . . . . . 3.3.3 Low-Precision Storage and Communication . . . . . . . . . . . . . . . . . 3.4 Inference and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Prefilling . . 3.4.2 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Suggestions on Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Communication Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.2 Compute Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Pre-Training 4.1 Data Construction . 4.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Long Context Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.1 Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 Ablation Studies for Multi-Token Prediction . . . . . . . . . . . . . . . . . 4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy . . . . . . 11 12 12 13 14 15 16 18 18 19 20 20 20 22 22 23 24 24 25 26 27 2 4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . . . . . . . . . 27 5 Post-Training 5. Supervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Group Relative Policy Optimization . . . . . . . . . . . . . . . . . . . . . . 5.3 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Evaluation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3. Standard Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.3 Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.4 DeepSeek-V3 as Generative Reward Model . . . . . . . . . . . . . . . . . 5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Distillation from DeepSeek-R1 . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Self-Rewarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.3 Multi-Token Prediction Evaluation . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion, Limitations, and Future Directions Contributions and Acknowledgments Ablation Studies for Low-Precision Training B.1 FP8 v.s. BF16 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Discussion About Block-Wise Quantization . . . . . . . . . . . . . . . . . . . . . . 28 28 29 30 30 30 32 33 34 34 34 35 45 47 47 47 Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48 1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, 2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token. With forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeekV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks. In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency. During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeekR1 series of models, and meanwhile carefully maintain the balance between model accuracy Training Costs Pre-Training Context Extension Post-Training Total in H800 GPU Hours in USD 2664K $5.328M 119K $0.238M 5K $0.01M 2788K $5.576M Table 1 Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour. and generation length. We evaluate DeepSeek-V3 on comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on series of standard and open-ended benchmarks. Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data. Our main contribution includes: Architecture: Innovative Load Balancing Strategy and Training Objective On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing. We investigate Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration. Pre-Training: Towards Ultimate Training Efficiency We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model. Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computationcommunication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead. At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours. Post-Training: Knowledge Distillation from DeepSeek-R1 We introduce an innovative methodology to distill reasoning capabilities from the longChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the 5 verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3. Summary of Core Evaluation Results Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge. Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by significant margin, demonstrating its competitiveness across diverse technical benchmarks. In the remainder of this paper, we first present detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design. Next, we describe our pre-training process, including the construction of training data, hyper-parameter settings, longcontext extension techniques, the associated evaluations, as well as some discussions (Section 4). Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly, we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential directions for future research (Section 6). 2. Architecture We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeekV2 (DeepSeek-AI, 2024c). 2.1. Basic Architecture The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing 6 Figure 2 Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we adopt MLA and DeepSeekMoE for efficient inference and economical training. strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section. 2.1.1. Multi-Head Latent Attention For attention, DeepSeek-V3 adopts the MLA architecture. Let ùëë denote the embedding dimension, ùëõ‚Ñé denote the number of attention heads, ùëë‚Ñé denote the dimension per head, and hùë° Rùëë denote the attention input for the ùë°-th token at given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference: ùêæùëâ ùë° hùë°, ùêæùëâ ùë° = ùëä ùê∑ùêæùëâ ùë° = ùëäùëà ùêæ ùê∂ ùê∂ ùê∂ ùê∂ ùë°,ùëõ‚Ñé] = [k ùë°,2; ...; ùë°,1; ùëÖ ùë° = RoPE(ùëä ùêæùëÖ ùê∂ ùëÖ kùë°,ùëñ = [k ùë° ], ùë°,ùëñ; ùêæùëâ ùë° ùê∂ ùê∂ ùê∂ ùê∂ ùë° = ùëäùëàùëâ [v ùë°,ùëõ‚Ñé] = ùë°,1; ùë°,2; ...; c , , hùë°), (1) (2) (3) (4) (5) 7 ùë° Rùëëùëê is the compressed latent vector for keys and values; ùëëùëê ( ùëë‚Ñéùëõ‚Ñé) indicates the KV where cùêæùëâ compression dimension; ùëä ùê∑ùêæùëâ Rùëëùëê ùëë denotes the down-projection matrix; ùëäùëà ùêæ, ùëäùëàùëâ Rùëë‚Ñéùëõ‚Ñé ùëëùëê are the up-projection matrices for keys and values, respectively; ùëä ùêæùëÖ Rùëë ùëÖ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE() denotes the operation that applies RoPE matrices; and [; ] denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., cùêæùëâ ùë° ) need to be cached during generation, ùë° which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017). and kùëÖ ‚Ñé ùëë For the attention queries, we also perform low-rank compression, which can reduce the activation memory during training: ùëÑ ùë° = ùëä ùê∑ùëÑ hùë°, ùëÑ ùê∂ ùê∂ ùê∂ ùë° = ùëäùëàùëÑ ùê∂ [q ùë°,ùëõ‚Ñé] = ùë°,2; ...; ùë°,1; ùë° , ùëÖ ùëÖ ùëÖ ùë° = RoPE(ùëä ùëÑùëÖ ùëÖ ùë°,ùëõ‚Ñé] = ùë°,2; ...; ùë°,1; ùê∂ ùë°,ùëñ], qùë°,ùëñ = [q ùë°,ùëñ; [q ùëÖ ùëÑ ùë° ), (6) (7) (8) (9) ùëÑ ùë° Rùëë ùëê is the compressed latent vector for queries; ùëë where compression dimension; ùëä ùê∑ùëÑ Rùëë matrices for queries, respectively; and ùëä ùëÑùëÖ Rùëë ùëÖ queries that carry RoPE. ùëê ùëë, ùëäùëàùëÑ Rùëë‚Ñéùëõ‚Ñé ùëë ‚Ñé ùëê ( ùëë‚Ñéùëõ‚Ñé) denotes the query ùëê are the down-projection and up-projection ùëê is the matrix to produce the decoupled ùëõ‚Ñé ùëë Ultimately, the attention queries (qùë°,ùëñ), keys (k ùëó,ùëñ), and values (vùê∂ ùëó,ùëñ) are combined to yield the final attention output uùë°: oùë°,ùëñ = ùë° Softmax ùëó ( qùëá ùë°,ùëñk ùëó,ùëñ ùëë‚Ñé + ùëë ùëÖ ‚Ñé uùë° = ùëäùëÇ [oùë°,1; oùë°,2; ...; oùë°,ùëõ‚Ñé], ùëó=1 ùê∂ )v ùëó,ùëñ, (10) (11) where ùëäùëÇ Rùëëùëë‚Ñéùëõ‚Ñé denotes the output projection matrix. 2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let uùë° denote the FFN input of the ùë°-th token, we compute the FFN output ùë° as follows: ùëÅùë† ùë° = uùë° + FFN (ùë†) ùëñ (uùë°) + ùëÅùëü ùëñ=1 ùëîùëñ,ùë° FFN (ùëü) ùëñ (uùë°), ùëñ=1 ùëî ùëñ,ùë° , ùëîùëñ,ùë° = (cid:205)ùëÅùëü ùëó=1 (cid:40)ùë†ùëñ,ùë°, 0, ùë†ùëñ,ùë° = Sigmoid (cid:0)uùë° ùëî ùëó,ùë° ùë†ùëñ,ùë° Topk({ùë† ùëó,ùë° 1 ùëó ùëÅùëü}, ùêæùëü), otherwise, eùëñ(cid:1) , ùëá ùëî ùëñ,ùë° = (12) (13) (14) (15) (ùëü) ùëñ where ùëÅùë† and ùëÅùëü denote the numbers of shared experts and routed experts, respectively; FFN () () denote the ùëñ-th shared expert and the ùëñ-th routed expert, respectively; ùêæùëü denotes and FFN the number of activated routed experts; ùëîùëñ,ùë° is the gating value for the ùëñ-th expert; ùë†ùëñ,ùë° is the token-to-expert affinity; eùëñ is the centroid vector of the ùëñ-th routed expert; and Topk(, ùêæ) denotes the set comprising ùêæ highest scores among the affinity scores calculated for the ùë°-th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies normalization among all selected affinity scores to produce the gating values. (ùë†) ùëñ Auxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce bias term ùëèùëñ for each expert and add it to the corresponding affinity scores ùë†ùëñ,ùë° to determine the top-K routing: ùëî ùëñ,ùë° = (cid:40)ùë†ùëñ,ùë°, 0, ùë†ùëñ,ùë° + ùëèùëñ Topk({ùë† ùëó,ùë° + ùëè ùëó1 ùëó ùëÅùëü}, ùêæùëü), otherwise. (16) Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score ùë†ùëñ,ùë°. During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by ùõæ if its corresponding expert is overloaded, and increase it by ùõæ if its corresponding expert is underloaded, where ùõæ is hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. Complementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ complementary sequence-wise balance loss: ùëìùëñ = ùëÅùëü ùêæùëüùëá LBal = ùõº ùëÅùëü ùëñ=1 ùëìùëñ ùëÉùëñ, ùëá 1 (cid:0)ùë†ùëñ,ùë° Topk({ùë† ùëó,ùë° 1 ùëó ùëÅùëü}, ùêæùëü)(cid:1) , ùë°=1 ùë† ùëñ,ùë° = ùëÉùëñ = ùë†ùëñ,ùë° (cid:205)ùëÅùëü ùëó=1 ùëá 1 ùëá ùë°=1 , ùë† ùëó,ùë° ùë† ùëñ,ùë°, (17) (18) (19) (20) where the balance factor ùõº is hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; 1() denotes the indicator function; and ùëá denotes the number of tokens in sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced. Figure 3 Illustration of our Multi-Token Prediction (MTP) implementation. We keep the complete causal chain for the prediction of each token at each depth. Node-Limited Routing. Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most ùëÄ nodes, which are selected according to ùêæùëü the sum of the highest ùëÄ affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap. No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference. 2.2. Multi-Token Prediction Inspired by Gloeckle et al. (2024), we investigate and set Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts ùê∑ additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section. MTP Modules. To be specific, our MTP implementation uses ùê∑ sequential modules to predict ùê∑ additional tokens. The ùëò-th MTP module consists of shared embedding layer Emb(), shared output head OutHead(), Transformer block TRMùëò (), and projection matrix ùëÄùëò Rùëë2ùëë. For the ùëñ-th input token ùë°ùëñ, at the ùëò-th prediction depth, we first combine the representation of the ùëñ-th Rùëë and the embedding of the (ùëñ + ùëò)-th token ùê∏ùëöùëè(ùë°ùëñ+ùëò) Rùëë token at the (ùëò 1)-th depth hùëò ùëñ 10 with the linear projection: hùëò ùëñ = ùëÄùëò [RMSNorm(h ùëò1 ùëñ ); RMSNorm(Emb(ùë°ùëñ+ùëò))], (21) where [; ] denotes concatenation. Especially, when ùëò = 1, hùëò1 refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined hùëò ùëñ serves as the input of the Transformer block at the ùëò-th depth to produce the output representation at the current depth hùëò ùëñ : ùëñ ùëò (22) 1:ùëá ùëò), 1:ùëá ùëò = TRMùëò (hùëò where ùëá represents the input sequence length and ùëñ: ùëó denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking hùëò ùëñ as the input, the shared output head will compute the probability distribution for the ùëò-th additional prediction token ùëÉùëò ùëñ+1+ùëò Rùëâ, where ùëâ is the vocabulary size: ùëÉùëò ùëñ+ùëò+1 = OutHead(h The output head OutHead() linearly maps the representation to logits and subsequently applies the Softmax() function to compute the prediction probabilities of the ùëò-th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training. (23) ùëò ùëñ ). MTP Training Objective. For each prediction depth, we compute cross-entropy loss Lùëò MTP: MTP = CrossEntropy(ùëÉùëò Lùëò 2+ùëò:ùëá+1, ùë°2+ùëò:ùëá+1) = 1 ùëá ùëá+1 ùëñ=2+ùëò log ùëÉùëò ùëñ [ùë°ùëñ], (24) where ùëá denotes the input sequence length, ùë°ùëñ denotes the ground-truth token at the ùëñ-th position, and ùëÉùëò ùëñ [ùë°ùëñ] denotes the corresponding prediction probability of ùë°ùëñ, given by the ùëò-th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by weighting factor ùúÜ to obtain the overall MTP loss LMTP, which serves as an additional training objective for DeepSeek-V3: LMTP = ùúÜ ùê∑ ùê∑ ùëò=1 Lùëò MTP. (25) MTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency. 3. Infrastructures 3.1. Compute Clusters DeepSeek-V3 is trained on cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications. 11 Figure 4 Overlapping strategy for pair of individual forward and backward chunks (the boundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes \"backward for input\", blue denotes \"backward for weights\", purple denotes PP communication, and red denotes barriers. Both all-to-all and PP communication can be fully hidden. 3.2. Training Framework The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020). In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP). 3.2.1. DualPipe and Computation-Communication Overlap For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computationcommunication phases, but also reduces the pipeline bubbles. The key idea of DualPipe is to overlap the computation and communication within pair of individual forward and backward chunks. To be specific, we divide each chunk into four components: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have PP communication component. As illustrated in Figure 4, for pair of forward and backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs dedicated to communication versus computation. In this overlapping strategy, we can ensure that both all-to-all and PP communication can be fully hidden during execution. Given the efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline simultaneously and significant portion of communications can be fully overlapped. This overlap also ensures that, as the model further scales up, as long as we maintain constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving near-zero all-to-all communication overhead. 12 Figure 5 Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by shared black border have mutually overlapped computation and communication. Method 1F1B ZB1P DualPipe (Ours) Bubble (ùëÉùëÉ 1)(ùêπ + ùêµ) (ùëÉùëÉ 1)(ùêπ + ùêµ 2ùëä) ( ùëÉùëÉ 2 1)(ùêπ&ùêµ + ùêµ 3ùëä) Parameter Activation 1 1 2 ùëÉùëÉ ùëÉùëÉ ùëÉùëÉ + 1 Table 2 Comparison of pipeline bubbles and memory usage across different pipeline parallel methods. ùêπ denotes the execution time of forward chunk, ùêµ denotes the execution time of full backward chunk, ùëä denotes the execution time of \"backward for weights\" chunk, and ùêπ&ùêµ denotes the execution time of two mutually overlapped forward and backward chunks. In addition, even in more general scenarios without heavy communication burden, DualPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles while only increasing the peak activation memory by 1 ùëÉùëÉ times. Although DualPipe requires keeping two copies of the model parameters, this does not significantly increase the memory consumption since we use large EP size during training. Compared with Chimera (Li and Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by 2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe, neither the bubbles nor activation memory will increase as the number of micro-batches grows. 3.2.2. Efficient Implementation of Cross-Node All-to-All Communication In order to ensure sufficient computational performance for DualPipe, we customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication. The implementation of the kernels is codesigned with the MoE gating algorithm and the network topology of our cluster. To be specific, in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications are handled via NVLink. NVLink offers bandwidth of 160 GB/s, roughly 3.2 times that of IB (50 GB/s). To effectively leverage the different bandwidths of IB and NVLink, we limit each token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is instantaneously forwarded via NVLink to specific GPUs that host their target experts, without being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink are fully overlapped, and each token can efficiently select an average of 3.2 experts per node without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3 13 selects only 8 routed experts in practice, it can scale up this number to maximum of 13 experts (4 nodes 3.2 experts/node) while preserving the same communication cost. Overall, under such communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2) IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps allocated to each communication task is dynamically adjusted according to the actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending, (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the us",
                "summary": "<h2>2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ DeepSeek-V3, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª—å—é —Å–æ —Å–º–µ—Å—å—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (MoE) –∏ 671 –º–∏–ª–ª–∏–∞—Ä–¥–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö 37 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –∞–∫—Ç–∏–≤–∏—Ä—É—é—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.</p>\n<h3>2.1 –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞</h3>\n<p>DeepSeek-V3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –∫–æ—Ç–æ—Ä–∞—è –±—ã–ª–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª—è—Ö DeepSeekV2, –∞ –∏–º–µ–Ω–Ω–æ:</p>\n<ul>\n<li><strong>–ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (MLA)</strong>: –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏.</li>\n<li><strong>DeepSeekMoE:</strong> –≠—Ç–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤.</li>\n</ul>\n<p>–≠—Ç–∏ –¥–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–æ–∑–≤–æ–ª—è—é—Ç DeepSeek-V3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–≤–æ–¥–∞.</p>\n<h4>2.1.1 –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ</h4>\n<p>(–í —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –æ–± —ç—Ç–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, —Ç–æ–ª—å–∫–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ –µ—ë –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)</p>\n<h4>2.1.2 DeepSeekMoE —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å</h4>\n<p>–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –±–∞–∑–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, DeepSeek-V3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏:</p>\n<ul>\n<li><strong>–°—Ç—Ä–∞—Ç–µ–≥–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å:</strong> –≠—Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–æ–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏. –û–±—ã—á–Ω–æ, –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –∑–∞–º–µ–¥–ª—è—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∏ —Å–Ω–∏–∂–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. DeepSeek-V3 –æ–±—Ö–æ–¥–∏—Ç—Å—è –±–µ–∑ —ç—Ç–∏—Ö –ø–æ—Ç–µ—Ä—å, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</li>\n</ul>\n<h3>2.2 –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤</h3>\n<p>DeepSeek-V3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ü–µ–ª—å –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É–ª—É—á—à–∞–µ—Ç –æ–±—â—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. (–í —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –æ –º–µ—Ö–∞–Ω–∏–∑–º–µ —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞).</p>\n<p>–í —Ü–µ–ª–æ–º, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DeepSeek-V3 —Å–æ—á–µ—Ç–∞–µ—Ç –≤ —Å–µ–±–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è (MLA –∏ DeepSeekMoE) —Å –Ω–æ–≤—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤), —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏—á—å –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤.</p>"
            },
            {
                "title": "Memory-efficient training techniques in DeepSeek-V3",
                "content": "e of the L2 cache and the interference to other SMs. 3.2.3. Extremely Memory Saving with Minimal Overhead In order to reduce the memory footprint during training, we employ the following techniques. Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm operations and MLA up-projections during back-propagation, thereby eliminating the need to persistently store their output activations. With minor overhead, this strategy significantly reduces memory requirements for storing activations. Exponential Moving Average in CPU. During training, we preserve the Exponential Moving Average (EMA) of the model parameters for early estimation of the model performance after learning rate decay. The EMA parameters are stored in CPU memory and are updated asynchronously after each training step. This method allows us to maintain EMA parameters without incurring additional memory or time overhead. Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency. 3.3. FP8 Training Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023b), we propose fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3. While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in inference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model Figure 6 The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. pre-training (Fishman et al., 2024). To address this challenge and effectively extend the dynamic range of the FP8 format, we introduce fine-grained quantization strategy: tile-wise grouping with 1 ùëÅùëê elements or block-wise grouping with ùëÅùëê ùëÅùëê elements. The associated dequantization overhead is largely mitigated under our increased-precision accumulation process, critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, level well within the acceptable range of training randomness. 3.3.1. Mixed Precision Framework Building upon widely adopted techniques in low-precision training (Kalamkar et al., 2019; Narang et al., 2017), we propose mixed precision framework for FP8 training. In this framework, most compute-density operations are conducted in FP8, while few key operations are strategically maintained in their original data formats to balance training efficiency and numerical stability. The overall framework is illustrated in Figure 6. Firstly, in order to accelerate model training, the majority of core computation kernels, i.e., GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs associated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles the computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad GEMM allows activations to be stored in FP8 for use in the backward pass. This significantly reduces memory consumption. Despite the efficiency advantage of the FP8 format, certain operators still require higher precision due to their sensitivity to low-precision computations. Besides, some low-cost operators can also utilize higher precision with negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we store the master weights, weight gradients, and optimizer states in higher precision. While 15 Figure 7 (a) We propose fine-grained quantization method to mitigate quantization errors caused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction with our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA Cores at an interval of ùëÅùê∂ = 128 elements MMA for the high-precision accumulation. these high-precision components incur some memory overheads, their impact can be minimized through efficient sharding across multiple DP ranks in our distributed training system. 3.3.2. Improved Precision from Quantization and Multiplication Based on our mixed precision FP8 framework, we introduce several strategies to enhance lowprecision training accuracy, focusing on both the quantization method and the multiplication process. Fine-Grained Quantization. In low-precision training frameworks, overflows and underflows are common challenges due to the limited dynamic range of the FP8 format, which is constrained by its reduced exponent bits. As standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy. To solve this, we propose fine-grained quantization method that applies scaling at more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and scale elements on 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we group and scale elements on 128x128 block basis (i.e., per 128 input channels per 128 output channels). This approach ensures that the quantization process can better accommodate outliers by adapting the scale according to smaller groups of elements. In Appendix B.2, we further discuss the training instability when we group and scale activations on block basis in the same way as weights quantization. One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can 16 be efficiently implemented. Notably, our fine-grained quantization strategy is highly consistent with the idea of microscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) have announced the support for microscaling formats with smaller quantization granularity (NVIDIA, 2024a). We hope our design can serve as reference for future work to keep pace with the latest GPU architectures. Increasing Accumulation Precision. Low-precision GEMM operations often suffer from underflow issues, and their accuracy largely depends on high-precision accumulation, which is commonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However, we observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to retaining around 14 bits, which is significantly lower than FP32 accumulation precision. This problem will become more pronounced when the inner dimension is large (Wortsman et al., 2023), typical scenario in large-scale model training where the batch size and model width are increased. Taking GEMM operations of two random matrices with = 4096 for example, in our preliminary test, the limited accumulation precision in Tensor Cores results in maximum relative error of nearly 2%. Despite these problems, the limited accumulation precision is still the default option in few FP8 frameworks (NVIDIA, 2024b), severely constraining the training accuracy. In order to address this issue, we adopt the strategy of promotion to CUDA Cores for higher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific, during MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results are accumulated using the limited bit width. Once an interval of ùëÅùê∂ is reached, these partial results will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation is performed. As mentioned before, our fine-grained quantization applies per-group scaling factors along the inner dimension K. These scaling factors can be efficiently multiplied on the CUDA Cores as the dequantization process with minimal additional computational cost. It is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix Multiply-Accumulate) instruction issue rate for single warpgroup. However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting ùëÅùê∂ = 128 elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and 3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad, we adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of this approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By operating on smaller element groups, our methodology effectively shares exponent bits among these grouped elements, mitigating the impact of the limited dynamic range. Online Quantization. Delayed quantization is employed in tensor-wise quantization frameworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains history of the maximum absolute 17 values across prior iterations to infer the current value. In order to ensure accurate scales and simplify the framework, we calculate the maximum absolute value online for each 1x128 activation tile or 128x128 weight block. Based on it, we derive the scaling factor and then quantize the activation or weight online into the FP8 format. 3.3.3. Low-Precision Storage and Communication In conjunction with our FP8 training framework, we further reduce the memory consumption and communication overhead by compressing cached activations and optimizer states into lower-precision formats. Low-Precision Optimizer States. We adopt the BF16 data format instead of FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without incurring observable performance degradation. However, the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training. Low-Precision Activation. As illustrated in Figure 6, the Wgrad operation is performed in FP8. To reduce the memory consumption, it is natural choice to cache activations in FP8 format for the backward pass of the Linear operator. However, special considerations are taken on several operators for low-cost high-precision training: (1) Inputs of the Linear after the attention operator. These activations are also used in the backward pass of the attention operator, which makes it sensitive to precision. We adopt customized E5M6 data format exclusively for these activations. Additionally, these activations will be converted from an 1x128 quantization tile to an 128x1 tile in the backward pass. To avoid introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. (2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking balance between memory efficiency and computational accuracy. Low-Precision Communication. Communication bandwidth is critical bottleneck in the training of MoE models. To alleviate this challenge, we quantize the activation before MoE up-projections into FP8 and then apply dispatch components, which is compatible with FP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator, scaling factors for this activation are integral power of 2. similar strategy is applied to the activation gradient before MoE down-projections. For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline. 3.4. Inference and Deployment We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages. 18 3.4.1. Prefilling The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), combined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that each expert processes sufficiently large batch size, thereby enhancing computational efficiency. For the MoE all-to-all communication, we use the same method as in training: first transferring tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP communication. To achieve load balancing among different experts in the MoE part, we need to ensure that each GPU processes approximately the same number of tokens. To this end, we introduce deployment strategy of redundant experts, which duplicates high-load experts and deploys them redundantly. The high-load experts are detected based on statistics collected during the online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts among GPUs within node based on the observed loads, striving to balance the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of one micro-batch with the dispatch and combine of another. Finally, we are exploring dynamic redundancy strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before the all-to-all operation at each layer begins, we compute the globally optimal routing scheme on the fly. Given the substantial computation involved in the prefilling stage, the overhead of computing this routing scheme is almost negligible. 3.4.2. Decoding During decoding, we treat the shared expert as routed one. From this perspective, each token will select 9 experts during routing, where the shared expert is regarded as heavy-load one that will always be selected. The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80, while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs are responsible for hosting redundant experts and shared experts. All-to-all communication of the dispatch and combine parts is performed via direct point-to-point transfers over IB to achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. Similar to prefilling, we periodically determine the set of redundant experts in certain interval, based on the statistical expert load from our online service. However, we do not need to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic redundancy strategy for decoding. However, this requires more careful optimization of the algorithm that computes the globally optimal routing scheme and the fusion with the dispatch kernel to reduce overhead. Additionally, to enhance throughput and hide the overhead of all-to-all communication, we are also exploring processing two micro-batches with similar computational workloads simultaneously in the decoding stage. Unlike prefilling, attention consumes larger portion of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with the dispatch+MoE+combine of another. In the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only small portion of SMs to dispatch+MoE+combine. 3.5. Suggestions on Hardware Design Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions on chip design to AI hardware vendors. 3.5.1. Communication Hardware In DeepSeek-V3, we implement the overlap between computation and communication to hide the communication latency during computation. This significantly reduces the dependency on communication bandwidth compared to serial computation and communication. However, the current communication implementation relies on expensive SMs (e.g., we allocate 20 out of the 132 SMs available in the H800 GPU for this purpose), which will limit the computational throughput. Moreover, using SMs for communication results in significant inefficiencies, as tensor cores remain entirely -utilized. Currently, the SMs primarily perform the following tasks for all-to-all communication: Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB traffic destined for multiple GPUs within the same node from single GPU. Transporting data between RDMA buffers (registered GPU memory regions) and input/output buffers. Executing reduce operations for all-to-all combine. Managing fine-grained memory layout during chunked data transferring to multiple experts across the IB and NVLink domain. We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as GPU co-processor or network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, and reduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives. 3.5.2. Compute Hardware Higher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core implementation of the NVIDIA Hopper architecture, FP8 GEMM (General Matrix Multiply) employs fixed-point accumulation, aligning the mantissa products by right-shifting based on the maximum exponent before addition. Our experiments reveal that it only uses the highest 14 20 bits of each mantissa product after sign-fill right shifting, and truncates bits exceeding this range. However, for example, to achieve precise FP32 results from the accumulation of 32 FP8FP8 multiplications, at least 34-bit precision is required. Thus, we recommend that future chip designs increase accumulation precision in Tensor Cores to support full-precision accumulation, or select an appropriate accumulation bit-width according to the accuracy requirements of training and inference algorithms. This approach ensures that errors remain within acceptable bounds while maintaining computational efficiency. Support for Tileand Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tileand blockwise quantization. In the current implementation, when the ùëÅùê∂ interval is reached, the partial results will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and added to FP32 registers on CUDA cores. Although the dequantization overhead is significantly mitigated combined with our precise FP32 accumulation strategy, the frequent data movements between Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we recommend future chips to support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements. Support for Online Quantization. The current implementations struggle to effectively support online quantization, despite its effectiveness demonstrated in our research. In the existing process, we need to read 128 BF16 activation values (the output of the previous computation) from HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are then written back to HBM, only to be read again for MMA. To address this inefficiency, we recommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access into single fused operation, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes. We also recommend supporting warp-level cast instruction for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly 50%. Support for Transposed GEMM Operations. The current architecture makes it cumbersome to fuse matrix transposition with GEMM operations. In our workflow, activations during the forward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the matrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored in HBM. To reduce memory operations, we recommend future chips to enable direct transposed reads of matrices from shared memory before MMA operation, for those precisions required in both training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. 21 4. Pre-Training 4.1. Data Construction Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do not incorporate cross-sample attention masking during training. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer. In the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while enabling the model to accurately predict middle text based on contextual cues. In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To be specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows: <fim_begin> ùëìpre<fim_hole> ùëì suf<fim_end> ùëì middle<eos_token>. This structure is applied at the document level as part of the pre-packing process. The FIM strategy is applied at rate of 0.1, consistent with the PSM framework. The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified to optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split certain proportion of such combined tokens during training, which exposes the model to wider array of special cases and mitigates this bias. 4.2. Hyper-Parameters Model Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden dimension to 7168. All learnable parameters are randomly initialized with standard deviation of 0.006. In MLA, we set the number of attention heads ùëõ‚Ñé to 128 and the per-head dimension ùëë‚Ñé to 128. The KV compression dimension ùëëùëê is set to 512, and the query compression dimension ùëë ùëê is set to 1536. For the decoupled queries and key, we set the per-head dimension ùëë ùëÖ ‚Ñé to 64. We substitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert is 2048. Among the routed experts, 8 experts will be activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth ùê∑ is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1. We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for 22 the learning rate scheduling, we first linearly increase it from 0 to 2.2 104 during the first 2K steps. Then, we keep constant learning rate of 2.2 104 until the model consumes 10T training tokens. Subsequently, we gradually decay the learning rate to 2.2 105 in 4.3T tokens, following cosine decay curve. During the training of the final 500B tokens, we keep constant learning rate of 2.2 105 in the first 333B tokens, and switch to another constant learning rate of 7.3 106 in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training. We leverage pipeline parallelism to deploy different layers of model on different GPUs, and for each layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes. As for the node-limited routing, each token will be sent to at most 4 nodes (i.e., ùëÄ = 4). For auxiliary-loss-free load balancing, we set the bias update speed ùõæ to 0.001 for the first 14.3T tokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set ùõº to 0.0001, just to avoid extreme imbalance within any single sequence. The MTP loss weight ùúÜ is set to 0.3 for the first 10T tokens, and to 0.1 for the remaining 4.8T tokens. Figure 8 Evaluation results on the Needle In Haystack (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. 4.3. Long Context Extension We adopt similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K. The YaRN configuration is consistent with that used in DeepSeek-V2, being applied exclusively to the decoupled shared key kùëÖ ùë° . The hyper-parameters remain identical across both phases, with the scale ùë† = 40, ùõº = 1, ùõΩ = 32, and the scaling factor ùë° = 0.1 ln ùë† + 1. In the first phase, the sequence length is set to 32K, and the batch size is 1920. During the second phase, the sequence length is increased to 128K, and the batch size is reduced to 480. The learning rate for both phases is set to 7.3 106, matching the final learning rate from the pre-training stage. 23 Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In Haystack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K. 4.4. Evaluations 4.4.1. Evaluation Benchmarks The base model of DeepSeek-V3 is pretrained on multilingual corpus with English and Chinese constituting the majority, so we evaluate its performance on series of benchmarks primarily in English and Chinese, as well as on multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Considered benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese and double-underlined benchmarks are multilingual ones: Multi-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLURedux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023). Language understanding and reasoning datasets include HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022). Closed-book question answering datasets include TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). Reading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019), C3 (Sun et al., 2019a), and CMRC (Cui et al., 2019). Reference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande Sakaguchi et al. (2019). Language modeling datasets include Pile (Gao et al., 2020). Chinese understanding and culture datasets include CCPM (Li et al., 2021). Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both English and Chinese subsets. Following our previous work (DeepSeek-AI, 2024b,c), we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models using different tokenizers. 24 Benchmark (Metric) # Shots Architecture # Activated Params # Total Params Pile-test (BPB) BBH (EM) MMLU (EM) MMLU-Redux (EM) MMLU-Pro (EM) DROP (F1) ARC-Easy (EM) ARC-Challenge (EM) HellaSwag (EM) PIQA (EM) WinoGrande (EM) RACE-Middle (EM) RACE-High (EM) TriviaQA (EM) NaturalQuestions (EM) AGIEval (EM) HumanEval (Pass@1) MBPP (Pass@1) LiveCodeBench-Base (Pass@1) CRUXEval-I (EM) CRUXEval-O (EM) GSM8K (EM) MATH (EM) MGSM (EM) CMath (EM) CLUEWSC (EM) C-Eval (EM) CMMLU (EM) CMRC (EM) C3 (EM) CCPM (EM) - - - - 3-shot ",
                "summary": "<h2>–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FP8</h2>\n<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã, –ø—Ä–∏–º–µ–Ω—è–µ–º—ã–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö FP8.</p>\n<h3>3.2.3. –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞–∫–ª–∞–¥–Ω—ã–º–∏ —Ä–∞—Å—Ö–æ–¥–∞–º–∏</h3>\n<p>–î–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã:</p>\n<ul>\n<li><strong>–ü–µ—Ä–µ—Å—á–µ—Ç RMSNorm –∏ MLA Up-Projection.</strong> –û–ø–µ—Ä–∞—Ü–∏–∏ RMSNorm –∏ up-projection –≤ –º–æ–¥—É–ª–µ MLA –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∞–∫—Ç–∏–≤–∞—Ü–∏–π, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏.</li>\n<li><strong>–≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –Ω–∞ CPU.</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ (EMA) –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ —Å–Ω–∏–∂–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã EMA —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ CPU –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å EMA –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø–∞–º—è—Ç–∏ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–∏.</li>\n<li><strong>–û–±—â–∏–µ Embedding –∏ Output Head –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤.</strong> –í —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ DualPipe, —Å–∞–º—ã–µ –º–µ–ª–∫–∏–µ —Å–ª–æ–∏ (–≤–∫–ª—é—á–∞—è embedding) –∏ —Å–∞–º—ã–µ –≥–ª—É–±–æ–∫–∏–µ —Å–ª–æ–∏ (–≤–∫–ª—é—á–∞—è output head) –º–æ–¥–µ–ª–∏ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –æ–¥–Ω–æ–º –∏ —Ç–æ–º –∂–µ PP-—Ä–∞–Ω–≥–µ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã embedding –∏ output head –º–µ–∂–¥—É –º–æ–¥—É–ª–µ–º MTP –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é, —á—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏.</li>\n</ul>\n<h3>3.3. –û–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FP8</h3>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è DeepSeek-V3 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fine grained —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –¥–∞–Ω–Ω—ã—Ö FP8. –•–æ—Ç—è –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–º, –æ–Ω–æ —á–∞—Å—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –Ω–∞–ª–∏—á–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∞–∫—Ç–∏–≤–∞—Ü–∏—è—Ö, –≤–µ—Å–∞—Ö –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö. \n–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ñ–æ—Ä–º–∞—Ç–∞ FP8, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –º–µ–ª–∫–æ–∑–µ—Ä–Ω–∏—Å—Ç–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏: –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∞–π–ª–∞–º —Ä–∞–∑–º–µ—Ä–æ–º 1xNc —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø–æ –±–ª–æ–∫–∞–º —Ä–∞–∑–º–µ—Ä–æ–º NcxNc —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –°–æ–ø—É—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –Ω–∞ –¥–µ–∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é —Å–Ω–∏–∂–∞—é—Ç—Å—è –∑–∞ —Å—á–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –Ω–∞–∫–ª–∞–¥–Ω—ã—Ö —Ä–∞—Å—Ö–æ–¥–æ–≤ –Ω–∞ –ø–∞–º—è—Ç—å –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ MoE, –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –∫—ç—à–∏—Ä—É—é—Ç—Å—è –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ FP8, –∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ BF16.\n–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ö–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FP8 –±—ã–ª–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞ –Ω–∞ –¥–≤—É—Ö –º–æ–¥–µ–ª—è—Ö, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö DeepSeek-V2-Lite –∏ DeepSeekV2, –∏ –ø–æ–∫–∞–∑–∞–ª–∞, —á—Ç–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ—Ç–µ—Ä—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å BF16 –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∏–∂–µ 0,25%.</p>\n<h3>3.3.1. –§—Ä–µ–π–º–≤–æ—Ä–∫ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏</h3>\n<p>–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FP8. –í —ç—Ç–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ-–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ FP8, –∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–ª—é—á–µ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.</p>\n<ul>\n<li><strong>GEMM –≤ FP8.</strong> –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ GEMM (—É–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü) –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ FP8. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –ø—Ä—è–º–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ, –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∏ –≤–µ—Å–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —É–¥–≤–∞–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å BF16. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ FP8 –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –≤–µ—Å–æ–≤ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ FP8 –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞, —á—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏.</li>\n<li><strong>–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.</strong> –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ embedding, output head, MoE gating modules, normalization operators –∏ attention operators, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç –∏—Å—Ö–æ–¥–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, BF16 –∏–ª–∏ FP32) –∏–∑-–∑–∞ –∏—Ö —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –ú–∞—Å—Ç–µ—Ä-–≤–µ—Å–∞, –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–µ—Å–æ–≤ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Ç–∞–∫–∂–µ —Ö—Ä–∞–Ω—è—Ç—Å—è —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.</li>\n</ul>\n<h3>3.3.2. –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –∏ —É–º–Ω–æ–∂–µ–Ω–∏—è</h3>\n<p>–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é, –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:</p>\n<ul>\n<li><strong>–ú–µ–ª–∫–æ–∑–µ—Ä–Ω–∏—Å—Ç–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ.</strong> –î–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –∏ –ø–æ—Ç–µ—Ä–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ FP8, –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –±–æ–ª–µ–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ–º —É—Ä–æ–≤–Ω–µ. –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è –ø–æ —Ç–∞–π–ª–∞–º 1x128 (–Ω–∞ —Ç–æ–∫–µ–Ω –ø–æ 128 –∫–∞–Ω–∞–ª–∞–º), –∞ –≤–µ—Å–∞ ‚Äì –ø–æ –±–ª–æ–∫–∞–º 128x128 (–Ω–∞ 128 –≤—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –ø–æ 128 –≤—ã—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤—ã–±—Ä–æ—Å—ã.</li>\n<li><strong>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è.</strong> –û–ø–µ—Ä–∞—Ü–∏–∏ GEMM —Å –Ω–∏–∑–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –ø—Ä–æ–±–ª–µ–º –ø–æ—Ç–µ—Ä–∏ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏, –∏ –∏—Ö —Ç–æ—á–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é. –ù–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞—Ö NVIDIA H800 —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ 14 –±–∏—Ç–∞–º–∏, —á—Ç–æ –Ω–∏–∂–µ, —á–µ–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ FP32. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è –≤ FP32 —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ö –Ω–∞ —è–¥—Ä–∞—Ö CUDA —á–µ—Ä–µ–∑ –∑–∞–¥–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª Nc. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.</li>\n</ul>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong> <em>–°—Ç–∞—Ç—å—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç–æ–¥—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –∏ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FP8, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–Ω–∏–∑–∏—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –∏ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤–∫–ª—é—á–∞—é—Ç –ø–µ—Ä–µ—Å—á–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ EMA –Ω–∞ CPU, –º–µ–ª–∫–æ–∑–µ—Ä–Ω–∏—Å—Ç–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –ø—Ä–∏ –æ–ø–µ—Ä–∞—Ü–∏—è—Ö GEMM.</em></p>"
            },
            {
                "title": "Model performance comparison in machine learning",
                "content": "5-shot 5-shot 5-shot 3-shot 25-shot 25-shot 10-shot 0-shot 5-shot 5-shot 5-shot 5-shot 5-shot 0-shot 0-shot 3-shot 3-shot 2-shot 2-shot 8-shot 4-shot 8-shot 3-shot 5-shot 5-shot 5-shot 1-shot 0-shot 0-shot English Code Math Chinese Multilingual MMMLU-non-English (EM) 5-shot DeepSeek-V2 Qwen2.5 LLaMA-3.1 DeepSeek-V3 Base MoE 21B 236B 0.606 78.8 78.4 75.6 51.4 80.4 97.6 92.2 87.1 83.9 86.3 73.1 52.6 80.0 38.6 57. 43.3 65.0 11.6 52.5 49.8 81.6 43.4 63.6 78.7 82.0 81.4 84.0 77.4 77.4 93.0 64.0 72B Base 405B Base Dense 72B 72B Dense 405B 405B 0.638 79.8 85.0 83.2 58.3 80.6 98.4 94.5 84.8 82.6 82.3 68.1 50.3 71.9 33.2 75.8 53.0 72.6 12.9 59.1 59.9 88.3 54.4 76.2 84.5 82.5 89.2 89.5 75.8 76.7 88. 74.8 0.542 82.9 84.4 81.3 52.8 86.0 98.4 95.3 89.2 85.9 85.2 74.2 56.8 82.7 41.5 60.6 54.9 68.4 15.5 58.5 59.9 83.5 49.0 69.9 77.3 83.0 72.5 73.7 76.0 79.7 78.6 73. Base MoE 37B 671B 0.548 87.5 87.1 86.2 64.4 89.0 98.9 95.3 88.9 84.7 84.9 67.1 51.3 82.9 40.0 79.6 65.2 75.4 19.4 67.3 69.8 89.3 61.6 79.8 90.7 82.7 90.1 88.8 76.3 78.6 92. 79.4 Table 3 Comparison among DeepSeek-V3-Base and other representative open-source base models. All models are evaluated in our internal framework and share the same evaluation setting. Scores with gap not exceeding 0.3 are considered to be at the same level. DeepSeekV3-Base achieves the best performance on most benchmarks, especially on math and code tasks. 4.4.2. Evaluation Results In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Note that due to the changes in our evaluation framework over the past months, the performance of DeepSeek-V2-Base exhibits slight difference from our previously reported results. Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model. 25 From more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2) Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only half of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages, especially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks, except for CMMLU, Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows better performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest open-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits much better performance on multilingual, code, and math benchmarks. As for English and Chinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance, and is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM. Due to our efficient architectures and comprehensive engineering optimizations, DeepSeekV3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models. Benchmark (Metric) # Shots Small MoE Baseline Small MoE w/ MTP Large MoE Baseline Large MoE w/ MTP # Activated Params (Inference) # Total Params (Inference) # Training Tokens Pile-test (BPB) BBH (EM) MMLU (EM) DROP (F1) TriviaQA (EM) NaturalQuestions (EM) HumanEval (Pass@1) MBPP (Pass@1) GSM8K (EM) MATH (EM) - - - - 3-shot 5-shot 1-shot 5-shot 5-shot 0-shot 3-shot 8-shot 4-shot 2.4B 15.7B 1.33T 0.729 39.0 50.0 39.2 56.9 22.7 20.7 35.8 25.4 10.7 2.4B 15.7B 1.33T 0.729 41.4 53.3 41.3 57.7 22.3 26.8 36.8 31.4 12.6 20.9B 228.7B 540B 0.658 70.0 67.5 68.5 67.0 27.2 44.5 61.6 72.3 38.6 20.9B 228.7B 540B 0.657 70.7 66.6 70.6 67.3 28.5 53.7 62.2 74.0 39.8 Table 4 Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. 4.5. Discussion 4.5.1. Ablation Studies for Multi-Token Prediction In Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the MTP strategy on top of two baseline models across different scales. At the small scale, we train baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train baseline MoE model comprising 228.7B total parameters on 540B tokens. On top of them, keeping the training data and the other architectures the same, we append 1-depth MTP module onto them and train two models with the MTP strategy for comparison. Note that during inference, we directly discard the MTP module, so the inference costs of the compared models are exactly the same. From the table, we can observe that the MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. Benchmark (Metric) # Shots Small MoE Small MoE Large MoE Large MoE Aux-Loss-Based Aux-Loss-Free Aux-Loss-Based Aux-Loss-Free # Activated Params # Total Params # Training Tokens Pile-test (BPB) BBH (EM) MMLU (EM) DROP (F1) TriviaQA (EM) NaturalQuestions (EM) HumanEval (Pass@1) MBPP (Pass@1) GSM8K (EM) MATH (EM) - - - - 3-shot 5-shot 1-shot 5-shot 5-shot 0-shot 3-shot 8-shot 4-shot 2.4B 15.7B 1.33T 0.727 37.3 51.0 38.1 58.3 23.2 22.0 36.6 27.1 10.9 2.4B 15.7B 1.33T 0.724 39.3 51.8 39.0 58.5 23.4 22.6 35.8 29.6 11.1 20.9B 228.7B 578B 0.656 66.7 68.3 67.1 66.7 27.1 40.2 59.2 70.7 37.2 20.9B 228.7B 578B 0.652 67.9 67.2 67.1 67.7 28.1 46.3 61.2 74.5 39.6 Table 5 Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. 4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy In Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We validate this strategy on top of two baseline models across different scales. At the small scale, we train baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train baseline MoE model comprising 228.7B total parameters on 578B tokens. Both of the baseline models purely use auxiliary losses to encourage load balance, and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. 4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance The key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies in their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise auxiliary loss, batch-wise balancing imposes more flexible constraint, as it does not enforce in-domain balance on each sequence. This flexibility allows experts to better specialize in different domains. To validate this, we record and analyze the expert load of 16B auxiliaryloss-based baseline and 16B auxiliary-loss-free model on different domains in the Pile test set. As illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater expert specialization patterns as expected. To further investigate the correlation between this flexibility and the advantage in model performance, we additionally design and validate batch-wise auxiliary loss that encourages load balance on each training batch instead of on each sequence. The experimental results show that, when achieving similar level of batch-wise load balance, the batch-wise auxiliary loss can also achieve similar model performance to the auxiliary-loss-free method. To be specific, in our experiments with 1B MoE models, the validation losses are: 2.258 (using sequencewise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using batch-wise 27 Figure 9 Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C. auxiliary loss). We also observe similar results on 3B MoE models: the model using sequencewise auxiliary loss achieves validation loss of 2.085, and the models using the auxiliary-loss-free method or batch-wise auxiliary loss achieve the same validation loss of 2.080. In addition, although the batch-wise load balancing methods show consistent performance advantages, they also face two potential challenges in efficiency: (1) load imbalance within certain sequences or small batches, and (2) domain-shift-induced load imbalance during inference. The first challenge is naturally addressed by our training framework that uses large-scale expert parallelism and data parallelism, which guarantees large size of each micro-batch. For the second challenge, we also design and implement an efficient inference framework with redundant expert deployment, as described in Section 3.4, to overcome it. 5. Post-Training 5.1. Supervised Fine-Tuning We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains, with each domain employing distinct data creation methods tailored to its specific requirements. Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. 28 To establish our methodology, we begin by developing an expert model tailored to specific domain, such as code, mathematics, or general reasoning, using combined Supervised FineTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first couples the problem with its original response in the format of <problem, original response>, while the second incorporates system prompt alongside the problem and the R1 response in the format of <system prompt, problem, R1 response>. The system prompt is meticulously designed to include instructions that guide the model toward producing responses enriched with mechanisms for reflection and verification. During the RL phase, the model leverages high-temperature sampling to generate responses that integrate patterns from both the R1-generated and original data, even in the absence of explicit system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate R1 patterns, thereby enhancing overall performance strategically. Upon completing the RL training phase, we implement rejection sampling to curate highquality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective. Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data. SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 106 and gradually decreases to 1 106. During training, each single sequence is packed from multiple samples. However, we adopt sample masking strategy to ensure that these examples remain isolated and mutually invisible. 5.2. Reinforcement Learning 5.2.1. Reward Model We employ rule-based Reward Model (RM) and model-based RM in our RL process. Rule-Based RM. For questions that can be validated using specific rules, we adopt rulebased reward system to determine the feedback. For instance, certain math problems have deterministic results, and we require the model to provide the final answer within designated format (e.g., in box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode problems, we can utilize compiler to generate feedback based on test cases. By leveraging rule-based validation wherever possible, we ensure higher level of reliability, as this approach is resistant to manipulation or exploitation. Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer 29 as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward. This approach helps mitigate the risk of reward hacking in specific tasks. 5.2.2. Group Relative Policy Optimization Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question ùëû, GRPO samples group of outputs {ùëú1, ùëú2, , ùëúùê∫} from the old policy model ùúãùúÉùëúùëôùëë and then optimizes the policy model ùúãùúÉ by maximizing the following objective: Jùê∫ùëÖùëÉùëÇ(ùúÉ) = E[ùëû ùëÉ(ùëÑ), {ùëúùëñ}ùê∫ ùëñ=1 (cid:18) ùúãùúÉ(ùëúùëñùëû) (cid:18) ùúãùúÉùëúùëôùëë (ùëúùëñùëû) ùê∫ min 1 ùê∫ ùëñ=1 ùúãùúÉùëúùëôùëë (ùëÇùëû)] ùê¥ùëñ, clip (cid:18) ùúãùúÉ(ùëúùëñùëû) ùúãùúÉùëúùëôùëë (ùëúùëñùëû) , 1 ùúÄ, 1 + ùúÄ (cid:19) (cid:19) ùê¥ùëñ ùõΩDùêæùêø (cid:0)ùúãùúÉùúãùëüùëí ùëì (cid:1) (cid:19) , (26) Dùêæùêø (cid:0)ùúãùúÉùúãùëüùëí ùëì (cid:1) = ùúãùëüùëí ùëì (ùëúùëñùëû) ùúãùúÉ(ùëúùëñùëû) log ùúãùëüùëí ùëì (ùëúùëñùëû) ùúãùúÉ(ùëúùëñùëû) 1, (27) where ùúÄ and ùõΩ are hyper-parameters; ùúãùëüùëí ùëì is the reference model; and ùê¥ùëñ is the advantage, derived from the rewards {ùëü1, ùëü2, . . . , ùëüùê∫} corresponding to the outputs within each group: ùê¥ùëñ = ùëüùëñ mean({ùëü1, ùëü2, , ùëüùê∫}) std({ùëü1, ùëü2, , ùëüùê∫}) . (28) We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process. This approach not only aligns the model more closely with human preferences but also enhances performance on benchmarks, especially in scenarios where available SFT data are limited. 5.3. Evaluations 5.3.1. Evaluation Settings Evaluation Benchmarks. Apart from the benchmark we used for base model testing, we further evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), CSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). Compared Baselines. We conduct comprehensive evaluations of our chat model against several strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 30 Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in zero-shot setting. For other datasets, we follow their original evaluation protocols with default prompts as provided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance on LiveCodeBench, where the data are collected from August 2024 to November 2024. The Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is evaluated using the agentless framework (Xia et al., 2024). We use the diff format to evaluate the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are evaluated with temperature of 0.7, and the results are averaged over 16 runs, while MATH-500 employs greedy decoding. We allow all models to output maximum of 8192 tokens for each benchmark. Benchmark (Metric) Architecture # Activated Params # Total Params MMLU (EM) MMLU-Redux (EM) MMLU-Pro (EM) DROP (3-shot F1) IF-Eval (Prompt Strict) GPQA-Diamond (Pass@1) SimpleQA (Correct) FRAMES (Acc.) LongBench v2 (Acc.) English Code HumanEval-Mul (Pass@1) LiveCodeBench (Pass@1-COT) LiveCodeBench (Pass@1) Codeforces (Percentile) SWE Verified (Resolved) Aider-Edit (Acc.) Aider-Polyglot (Acc.) Math Chinese AIME 2024 (Pass@1) MATH-500 (EM) CNMO 2024 (Pass@1) CLUEWSC (EM) C-Eval (EM) C-SimpleQA (Correct) DeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5GPT-4o DeepSeek V2-0506 V2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022 MoE 21B 236B MoE 21B 236B Dense 72B 72B Dense 405B 405B 78.2 77.9 58.5 83.0 57.7 35.3 9.0 66.9 31.6 69.3 18.8 20.3 17.5 - 60.3 - 4.6 56.3 2.8 89.9 78.6 48.5 80.6 80.3 66.2 87.8 80.6 41.3 10.2 65.4 35.4 77.4 29.2 28.4 35.6 22.6 71.6 18.2 16.7 74.7 10.8 90.4 79.5 54. 85.3 85.6 71.6 76.7 84.1 49.0 9.1 69.8 39.4 77.3 31.1 28.7 24.8 23.8 65.4 7.6 23.3 80.0 15.9 91.4 86.1 48.4 88.6 86.2 73.3 88.7 86.0 51.1 17.1 70.0 36.1 77.2 28.4 30.1 25.3 24.5 63.9 5. 23.3 73.8 6.8 84.7 61.5 50.4 - - - 88.3 88.9 78.0 88.3 86.5 65.0 28.4 72.5 41.0 81.7 36.3 32.8 20.3 50.8 84.2 45.3 16.0 78.3 13. 85.4 76.7 51.3 - - - 87.2 88.0 72.6 83.7 84.3 49.9 38.2 80.5 48.1 80.5 33.4 34.2 23.6 38.8 72.9 16.0 9.3 74.6 10.8 87.9 76.0 59. V3 MoE 37B 671B 88.5 89.1 75.9 91. 86.1 59.1 24.9 73.3 48.7 82.6 40.5 37.6 51.6 42.0 79.7 49.6 39.2 90.2 43.2 90.9 86.5 64. Table 6 Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. 4https://github.com/openai/simple-evals 31 5.3.2. Standard Evaluation Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the bestperforming open-source model. Additionally, it is competitive against frontier closed-source models like GPT-4o and Claude-3.5-Sonnet. English Benchmarks. MMLU is widely recognized benchmark designed to assess the performance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3 demonstrates competitive performance, standing on par with top-tier models such as LLaMA3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B. Moreover, DeepSeek-V3 excels in MMLU-Pro, more challenging educational knowledge benchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, refined version of MMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond, PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind Claude 3.5 Sonnet and outperforming all other competitors by substantial margin. In long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES, DeepSeek-V3 continues to demonstrate its position as top-tier model. It achieves an impressive 91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category. On FRAMES, benchmark requiring question-answering over 100k token contexts, DeepSeekV3 closely trails GPT-4o while outperforming all other models by significant margin. This demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks. The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance on LongBench v2, dataset that was released just few weeks before the launch of DeepSeek V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere to user-defined format constraints. Code and Math Benchmarks. Coding is challenging and practical task for LLMs, encompassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By providing access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be attributed to its advanced knowledge distillation technique, which effectively enhances its code generation and problem-solving capabilities in algorithm-focused tasks. On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly surpassing baselines and setting new state-of-the-art for non-o1-like models. Specifically, on AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5 72B, by approximately 10% in absolute scores, which is substantial margin for such challenging benchmarks. This remarkable capability highlights the effectiveness of the distillation technique from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models. 32 Model Arena-Hard AlpacaEval 2. DeepSeek-V2.5-0905 Qwen2.5-72B-Instruct LLaMA-3.1 405B GPT-4o-0513 Claude-Sonnet-3.5-1022 DeepSeek-V3 76.2 81.2 69.3 80.4 85.2 85.5 50.5 49.1 40.5 51.1 52.0 70.0 Table 7 English open-ended conversation evaluations. For AlpacaEval 2.0, we use the lengthcontrolled win rate as the metric. Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeekV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on larger corpus compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is pre-trained on. On C-Eval, representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. 5.3.3. Open-Ended Evaluation In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves groundbreaking milestone as the first open-source model to surpass 85% on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting new standard for what open-source models can accomplish in challenging domains. Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperforming both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by significant margin of 20%, highlighting substantial improvements in tackling simple tasks and showcasing the effectiveness of its advancements. 5.3.4. DeepSeek-V3 as Generative Reward Model We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806 and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeekV3 along with voting to offer self-feedback on open-ended questions, thereby improving the Model Chat Chat-Hard Safety Reasoning Average GPT-4o-0513 GPT-4o-0806 GPT-4o-1120 Claude-3.5-sonnet-0620 Claude-3.5-sonnet-1022 DeepSeek-V3 DeepSeek-V3 (maj@6) 96.6 96.1 95. 96.4 96.4 96.9 96.9 70.4 76.1 71.3 74.0 79.7 79.8 82.6 86.7 88.1 86. 81.6 91.1 87.0 89.5 84.9 86.6 85.2 84.7 87.6 84.3 89.2 84.7 86.7 84. 84.2 88.7 87.0 89.6 Table 8 Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench. Model LiveCodeBench-CoT Pass@1 Length MATH-500 Pass@1 Length DeepSeek-V2.5 Baseline DeepSeek-V2.5 +R1 Distill 31.1 37.4 718 783 74.6 83.2 769 Table 9 The contribution of distillation from DeepSeek-R1. The evaluation settings of LiveCodeBench and MATH-500 are the same as in Table 6. effectiveness and robustness of the alignment process. 5.4. Discussion 5.4.1. Distillation from DeepSeek-R1 We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The baseline is trained on short CoT data, whereas its competitor uses data generated by the expert checkpoints described above. Table 9 demonstrates the effectiveness of the distillation data, showing significant improvements in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an interesting trade-off: the distillation leads to better performance but also substantially increases the average response length. To maintain balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation. Our research suggests that knowledge distillation from reasoning models presents promising direction for post-training optimization. While our current work focuses on distilling data from mathematics and coding domains, this approach shows potential for broader applications across various task domains. The effectiveness demonstrated in these specific areas indicates that long-CoT distillation could be valuable for enhancing model performance in other cognitive tasks requiring complex reasoning. Further exploration of this approach across different domains remains an important direction for future research. 5.4.2. Self-Rewarding Rewards play pivotal role in RL, steering the optimization process. In domains where verification through external tools is straightforward, such as some coding or mathematics scenarios, RL demonstrates exceptional efficacy. However, in more general scenarios, constructing feedback 34 mechanism through hard coding is impractical. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as feedback source. This method has produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can optimize towards the constitutional direction. We believe that this paradigm, which combines supplementary information with LLMs as feedback source, is of paramount importance. The LLM serves as versatile processor capable of transforming unstructured information from diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond self-rewarding, we are also dedicated to uncovering other general and scalable rewarding methods to consistently advance the model capabilities in general scenarios. 5.4.3. Multi-Token Prediction Evaluation Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique. Combined with the framework of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between 85% and 90% across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). 6. Conclusion, Limitations, and Future Directions In this paper, we introduce DeepSeek-V3, large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets multi-token prediction training objective for stronger performance. The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations. The post-training also makes success in distilling the reasoning capability from the DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training. While acknowledging its strong performance and cost-effectiveness, we also recognize that DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might pose burden for small-sized teams. Secondly, although our deployment strategy for DeepSeekV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2, there still remains potential for further enhancement. Fortunately, these limitations are expected to be naturally addressed with the development of more advanced hardware. DeepSeek consistently adheres to the route of open-source models with longtermism, aiming to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we plan to strategically invest in research across the following directions. We will consistently study and refine our model architectures, aiming to further improve 35 both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities. We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across more comprehensive range of dimensions. We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth. We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing fixed set of benchmarks during research, which may create misleading impression of the model capabilities and affect our foundational assessment",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å DeepSeek-V3-Base —Å –¥—Ä—É–≥–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≤—ã–≤–æ–¥—ã –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ DeepSeek-V3-Base —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏:</strong></p>\n<p>DeepSeek-V3-Base —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å DeepSeek-V2-Base (–ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤–µ—Ä—Å–∏—è), Qwen2.5 72B Base –∏ LLaMA-3.1 405B Base. –í—Å–µ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –≤ –µ–¥–∏–Ω–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–∏—Å—Ç–µ–º–µ, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∏–∑-–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ —Å–∏—Å—Ç–µ–º–µ –æ—Ü–µ–Ω–∫–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã DeepSeek-V2-Base –º–æ–≥—É—Ç –Ω–µ–º–Ω–æ–≥–æ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç —Ä–∞–Ω–µ–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö. </p>\n<p>–í —Ü–µ–ª–æ–º, DeepSeek-V3-Base –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç DeepSeek-V2-Base –∏ Qwen2.5 72B Base –ø–æ –≤—Å–µ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –æ–Ω–∞ –æ–±–≥–æ–Ω—è–µ—Ç LLaMA-3.1 405B Base –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Ç–µ—Å—Ç–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –º–æ—â–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>\n<p>–†–∞—Å—Å–º–æ—Ç—Ä–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ:</p>\n<ol>\n<li>\n<p><strong>DeepSeek-V3-Base –ø—Ä–æ—Ç–∏–≤ DeepSeek-V2-Base:</strong> DeepSeek-V3-Base –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –ø–æ–≤—ã—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>DeepSeek-V3-Base –ø—Ä–æ—Ç–∏–≤ Qwen2.5 72B Base:</strong> DeepSeek-V3-Base –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –∏–º–µ–µ—Ç –≤–¥–≤–æ–µ –º–µ–Ω—å—à–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º CMMLU (–∑–∞–¥–∞—á–∞ –≤—ã–±–æ—Ä–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º), DeepSeek-V3-Base —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.</p>\n</li>\n<li>\n<p><strong>DeepSeek-V3-Base –ø—Ä–æ—Ç–∏–≤ LLaMA-3.1 405B Base:</strong> DeepSeek-V3-Base –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LLaMA-3.1 405B Base (—Å–∞–º—É—é –±–æ–ª—å—à—É—é –æ—Ç–∫—Ä—ã—Ç—É—é –º–æ–¥–µ–ª—å —Å 11-–∫—Ä–∞—Ç–Ω–æ –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–∏–µ, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É. –í —Ç–µ—Å—Ç–∞—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–∞—Ö DeepSeek-V3-Base –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—É—é –∏–ª–∏ –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ—Å–æ–±–µ–Ω–Ω–æ –≤—ã–¥–µ–ª—è—è—Å—å –≤ —Ç–µ—Å—Ç–∞—Ö BBH, MMLU-series, DROP, C-Eval, CMMLU –∏ CCPM.</p>\n</li>\n</ol>\n<p><strong>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è:</strong></p>\n<p>DeepSeek-V3 –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã–º —Ä–µ—à–µ–Ω–∏—è–º. –û–±—É—á–µ–Ω–∏–µ DeepSeek-V3 –Ω–∞ –∫–∞–∂–¥–æ–º —Ç—Ä–∏–ª–ª–∏–æ–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –≤—Å–µ–≥–æ 180 —Ç—ã—Å—è—á —á–∞—Å–æ–≤ GPU H800, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –¥–µ—à–µ–≤–ª–µ, —á–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π 72B –∏–ª–∏ 405B.</p>\n<p><strong>–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ Multi-Token Prediction (MTP):</strong></p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è MTP, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MTP –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤.</p>\n<p><strong>–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –±–µ–∑ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å (auxiliary-loss-free):</strong></p>\n<p>–ï—â–µ –æ–¥–∏–Ω –≤–∞–∂–Ω—ã–π –∞—Å–ø–µ–∫—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ‚Äî —ç—Ç–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –≤ –º–æ–¥–µ–ª–∏ MoE (Mixture of Experts) –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—å. –≠—Ç–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ç–∞–∫–∂–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–µ—Ç–æ–¥–∞–º–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–µ—Ä—è—Ö.</p>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –ø–æ –ø–∞–∫–µ—Ç–∞–º –∏ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º:</strong></p>\n<p>–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ –ø–æ–¥—Ö–æ–¥–∞ –∫ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–µ –Ω–∞–≥—Ä—É–∑–∫–∏: –ø–æ –ø–∞–∫–µ—Ç–∞–º (batch-wise) –∏ –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º (sequence-wise). –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–∞–∫–µ—Ç–∞–º –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≥–∏–±–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è —ç–∫—Å–ø–µ—Ä—Ç–∞–º –ª—É—á—à–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–∞–∫–µ—Ç–∞–º –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏, –∫–∞–∫ —Å–ª–µ–¥—Å—Ç–≤–∏–µ, –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ü—Ä–∏ —ç—Ç–æ–º, –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–∞–∫–µ—Ç–∞–º –º–æ–∂–µ—Ç —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –≤–Ω—É—Ç—Ä–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–ª–∏ –Ω–µ–±–æ–ª—å—à–∏—Ö –ø–∞–∫–µ—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∏–∑-–∑–∞ —Å–¥–≤–∏–≥–∞ –¥–æ–º–µ–Ω–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –≠—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã —Ä–µ—à–∞—é—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ–ø–∏—Å–∞–Ω—ã –≤ –¥–∞–Ω–Ω–æ–º –æ—Ç—Ä—ã–≤–∫–µ.</p>\n<p><strong>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:</strong></p>\n<p>DeepSeek-V3-Base –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –º–µ—Ç–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</p>"
            }
        ]
    },
    {
        "id": "2501.04519",
        "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
        "url": "https://huggingface.co/papers/2501.04519",
        "abstract": "We present rStar-Math to demonstrate that small language models (SLMs) can\nrival or even surpass the math reasoning capability of OpenAI o1, without\ndistillation from superior models. rStar-Math achieves this by exercising \"deep\nthinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM\nperforms test-time search guided by an SLM-based process reward model.\nrStar-Math introduces three innovations to tackle the challenges in training\nthe two SLMs: (1) a novel code-augmented CoT data sythesis method, which\nperforms extensive MCTS rollouts to generate step-by-step verified reasoning\ntrajectories used to train the policy SLM; (2) a novel process reward model\ntraining method that avoids na\\\"ive step-level score annotation, yielding a\nmore effective process preference model (PPM); (3) a self-evolution recipe in\nwhich the policy SLM and PPM are built from scratch and iteratively evolved to\nimprove reasoning capabilities. Through 4 rounds of self-evolution with\nmillions of synthesized solutions for 747k math problems, rStar-Math boosts\nSLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it\nimproves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to\n86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad\n(AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among\nthe top 20% the brightest high school math students. Code and data will be\navailable at https://github.com/microsoft/rStar.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2025-01-08",
        "pub_date_card": {
            "ru": "8 —è–Ω–≤–∞—Ä—è",
            "en": "January 8",
            "zh": "1Êúà8Êó•"
        },
        "hash": "b065003de5fa3bde",
        "authors": [
            "Xinyu Guan",
            "Li Lyna Zhang",
            "Yifei Liu",
            "Ning Shang",
            "Youran Sun",
            "Yi Zhu",
            "Fan Yang",
            "Mao Yang"
        ],
        "affiliations": [
            "Microsoft",
            "Peking University",
            "Tsinghua University"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2501.04519.jpg",
        "data": {
            "categories": [
                "#training",
                "#reasoning",
                "#optimization",
                "#benchmark",
                "#small_models",
                "#dataset"
            ],
            "emoji": "üßÆ",
            "ru": {
                "title": "–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –±–æ–ª—å—à–∏–µ –∑–∞–¥–∞—á–∏: rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≥–∏–≥–∞–Ω—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ",
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç rStar-Math - –ø–æ–¥—Ö–æ–¥, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–∞–ª—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (SLM) –¥–æ—Å—Ç–∏—á—å –∏–ª–∏ –ø—Ä–µ–≤–∑–æ–π—Ç–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –º–µ—Ç–æ–¥—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS) —Å –¥–≤—É–º—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ SLM: –ø–æ–ª–∏—Ç–∏–∫–æ–π –∏ –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å–∏–Ω—Ç–µ–∑–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å SLM –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏."
            },
            "en": {
                "title": "Empowering Small Models to Excel in Math Reasoning",
                "desc": "The paper introduces rStar-Math, a framework that enhances the math reasoning abilities of small language models (SLMs) without relying on larger models. It employs Monte Carlo Tree Search (MCTS) to enable deep thinking, allowing the SLM to perform guided search during problem-solving. Key innovations include a code-augmented Chain of Thought (CoT) data synthesis method for generating verified reasoning paths, a refined process preference model (PPM) for better reward training, and a self-evolution strategy for iterative improvement. As a result, rStar-Math significantly boosts the performance of SLMs on math benchmarks, achieving state-of-the-art results in various assessments."
            },
            "zh": {
                "title": "Â∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÊï∞Â≠¶Êé®ÁêÜÊñ∞Á™ÅÁ†¥",
                "desc": "rStar-MathÂ±ïÁ§∫‰∫ÜÂ∞èÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàSLMsÔºâÂú®Êï∞Â≠¶Êé®ÁêÜËÉΩÂäõ‰∏äÂèØ‰ª•‰∏éOpenAIÁöÑo1Áõ∏Â™≤ÁæéÔºåÁîöËá≥Ë∂ÖË∂äÂÆÉÔºåËÄåÊó†ÈúÄ‰ªéÊõ¥Âº∫Â§ßÁöÑÊ®°Âûã‰∏≠Ëí∏È¶è„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàMCTSÔºâÂÆûÁé∞‚ÄúÊ∑±Â∫¶ÊÄùËÄÉ‚ÄùÔºåÂú®ÊµãËØïÊó∂Áî±SLMÈ©±Âä®ÁöÑËøáÁ®ãÂ•ñÂä±Ê®°ÂûãÊåáÂØºÊï∞Â≠¶Á≠ñÁï•SLMËøõË°åÊêúÁ¥¢„ÄÇrStar-MathÂºïÂÖ•‰∫Ü‰∏âÈ°πÂàõÊñ∞Êù•Ëß£ÂÜ≥ËÆ≠ÁªÉ‰∏§‰∏™SLMÁöÑÊåëÊàòÔºåÂåÖÊã¨Êñ∞È¢ñÁöÑ‰ª£Á†ÅÂ¢ûÂº∫ÁöÑÈìæÂºèÊé®ÁêÜÊï∞ÊçÆÂêàÊàêÊñπÊ≥ïÂíåÊõ¥ÊúâÊïàÁöÑËøáÁ®ãÂÅèÂ•ΩÊ®°ÂûãÔºàPPMÔºâËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÁªèËøáÂõõËΩÆËá™ÊàëËøõÂåñÔºårStar-MathÂú®747,000‰∏™Êï∞Â≠¶ÈóÆÈ¢ò‰∏äÁîüÊàê‰∫ÜÊï∞Áôæ‰∏á‰∏™ÂêàÊàêËß£Ôºå‰ΩøSLMsÁöÑÊï∞Â≠¶Êé®ÁêÜËÉΩÂäõËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊ∞¥Âπ≥„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising \"deep thinking\" through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\\\"ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–π, —á—Ç–æ –Ω–µ–±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –º–æ–≥—É—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å –∏–ª–∏ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ OpenAI o1 –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –∑–∞ —Å—á–µ—Ç \"–≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è\" —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–∏—Å–∫–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS). –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –≤—ã—Å—Ç—É–ø–∞—é—â–∞—è –≤ —Ä–æ–ª–∏ \"–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏\", –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–æ–∏—Å–∫ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—è—Å—å –Ω–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞, —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ SLM.</p>\n<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è —ç—Ç–∏—Ö –¥–≤—É—Ö SLM:</p>\n<ol>\n<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö CoT —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∫–æ–¥–æ–º:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö, –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è SLM, –æ—Ç–≤–µ—á–∞—é—â–µ–π –∑–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–æ–ª–∏—Ç–∏–∫—É. (CoT - Chain of Thought, –º–µ—Ç–æ–¥, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –≤—ã–¥–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç, –Ω–æ –∏ —Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø—Ä–∏–≤–µ–¥—à–∏—Ö –∫ –Ω–µ–º—É)</li>\n<li><strong>–ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞:</strong> –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏–∑–±–µ–≥–∞–µ—Ç –ø—Ä—è–º–æ–≥–æ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω –æ–±—É—á–∞–µ—Ç –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM). (–¢.–µ. –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —à–∞–≥–∏, –∞ –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π)</li>\n<li><strong>–†–µ—Ü–µ–ø—Ç —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è:</strong> –í —ç—Ç–æ–º –ø–æ–¥—Ö–æ–¥–µ SLM, –æ—Ç–≤–µ—á–∞—é—â–∞—è –∑–∞ –ø–æ–ª–∏—Ç–∏–∫—É, –∏ PPM —Å—Ç—Ä–æ—è—Ç—Å—è —Å –Ω—É–ª—è –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.</li>\n</ol>\n<p>–ë–ª–∞–≥–æ–¥–∞—Ä—è 4 —Ä–∞—É–Ω–¥–∞–º —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è 747 —Ç—ã—Å—è—á –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, rStar-Math –ø–æ–≤—ã—à–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π SLM –¥–æ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π. –ù–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH, rStar-Math —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Qwen2.5-Math-7B —Å 58.8% –¥–æ 90.0% –∏ Phi3-mini-3.8B —Å 41.4% –¥–æ 86.4%, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –Ω–∞ +4.5% –∏ +0.9% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ USA Math Olympiad (AIME) rStar-Math —Ä–µ—à–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 53.3% (8 –∏–∑ 15) –∑–∞–¥–∞—á, —á—Ç–æ —Å—Ç–∞–≤–∏—Ç –µ–≥–æ –≤ —Ç–æ–ø 20% —Å–∞–º—ã—Ö —Å–ø–æ—Å–æ–±–Ω—ã—Ö —É—á–µ–Ω–∏–∫–æ–≤ —Å—Ç–∞—Ä—à–∏—Ö –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏. –ö–æ–¥ –∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ —Å—Å—ã–ª–∫–µ https://github.com/microsoft/rStar.</p>"
            },
            {
                "title": "Recent Studies",
                "content": "Equal contribution. Project leader; correspondence to lzhani@microsoft.com Xinyu Guan and Youran Sun did this work during the internship at MSRA. Xinyu Guan (2001gxy@gmail.com) is with Peking University, Youran Sun is with Tsinghua University. Figure 1: The overview of rStar-Math. In the test-time compute paradigm, the key is to train powerful policy model that generates promising solution steps and reliable reward model that accurately evaluates them, both of which depend on high-quality training data. Unfortunately, it is well-known that off-the-shelf high-quality math reasoning data is scarce, and synthesizing high-quality math data faces fundamental challenges. For the policy model, it is challenging to distinguish erroneous reasoning steps from the correct ones, complicating the elimination of low-quality data. It is worth noting that in math reasoning, correct final answer does not ensure the correctness of the entire reasoning trace [Lanham et al., 2023]. Incorrect intermediate steps significantly decrease data quality. As for the reward model, process reward modeling (PRM) shows great potential by providing fine-grained feedback on intermediate steps [Lightman et al., 2023]. However, the training data is even scarcer in this regard: accurate step-by-step feedback requires intense human labeling efforts and is impractical to scale, while those automatic annotation attempts show limited gains due to noisy reward scores [Luo et al., 2024, Wang et al., 2024c, Chen et al., 2024]. Due to the above challenges, existing distill-based data synthesis approaches to training policy models, e.g., scaling up GPT4-distilled CoT data [Tang et al., 2024, Huang et al., 2024], have shown diminishing returns and cannot exceed the capability of their teacher model; meanwhile, as of today, training reliable PRMs for math reasoning remains an open question. In this work, we introduce rStar-Math, self-evolvable System 2-style reasoning approach that achieves the state-of-the-art math reasoning, rivaling and sometimes even surpassing OpenAI o1 on challenging math competition benchmarks with model size as small as 7 billion. Unlike solutions relying on superior LLMs for data synthesis, rStar-Math leverages smaller language models (SLMs) with Monte Carlo Tree Search (MCTS) to establish self-evolutionary process, iteratively generating higher-quality training data. To achieve self-evolution, rStar-Math introduces three key innovations. First, novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories with self-annotated MCTS Q-values. Specifically, math problem-solving is decomposed into multi-step generation within MCTS. At each step, the SLM serving as the policy model samples candidate nodes, each generating one-step CoT and the corresponding Python code. To verify the generation quality, only nodes with successful Python code execution are retained, thus mitigating errors in intermediate steps. Moreover, extensive MCTS rollouts automatically assign Q-value to each intermediate step based on its contribution: steps contributing to more trajectories that lead to the correct answer are given higher Q-values and considered higher quality. This ensures that the reasoning trajectories generated by SLMs consist of correct, high-quality intermediate steps. Second, novel method that trains an SLM acting as process preference model, i.e., PPM to implement the desired PRM, that reliably predicts reward label for each math reasoning step. The PPM leverages the fact that, although Q-values are still not precise enough to score each reasoning step despite using extensive MCTS rollouts, the Q-values can reliably distinguish positive (correct) steps from negative (irrelevant/incorrect) ones. Thus the training method constructs preference pairs for each step based on Q-values and uses pairwise ranking loss [Ouyang et al., 2022] to optimize PPMs score prediction for each reasoning step, achieving reliable labeling. This approach avoids conventional methods that directly use Q-values as reward labels [Luo et al., 2024, Chen et al., 2024], which are inherently noisy and imprecise in stepwise reward assignment. Finally, four-round self-evolution recipe that progressively builds both frontier policy model and PPM from scratch. We begin by curating dataset of 747k math word problems from publicly available sources. In each round, we use the latest policy model and PPM to perform MCTS, 2 generating increasingly high-quality training data using the above two methods to train stronger policy model and PPM for next round. Each round achieves progressive refinement: (1) stronger policy SLM, (2) more reliable PPM, (3) generating better reasoning trajectories via PPM-augmented MCTS, and (4) improving training data coverage to tackle more challenging and even competitionlevel math problems. Extensive experiments across four SLMs (1.5B-7B) and seven math reasoning tasks demonstrate the effectiveness of rStar-Math. Remarkably, rStar-Math improves all four SLMs, matching or even surpassing OpenAI o1 on challenging math benchmarks. On MATH benchmark, with 8 search trajectories, rStar-Math boosts Qwen2.5-Math-7B from 58.8% to 89.4% and Qwen2.5-Math-1.5B from 51.2% to 87.8%. With 64 trajectories, the scores rise to 90% and 88.4%, outperforming o1-preview by 4.5% and 2.6% and matching o1-minis 90%. On the Olympiad-level AIME 2024, rStar-Math solves on average 53.3% (8/15) of the problems, exceeding o1-preview by 8.7% and all other open-sourced LLMs. We further conduct comprehensive experiments to verify the superiority of step-by-step verified reasoning trajectories over state-of-the-art data synthesis baselines, as well as the PPMs effectiveness compared to outcome reward models and value-based PRMs. Finally, we present key findings from rStar-Math deep thinking, including the intrinsic self-reflection capability and PPMs preference for theorem-applications intermediate steps.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω rStar-Math, –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–∏ –∏ –∏–º–∏—Ç–∏—Ä—É—é—â–∏–π –º—ã—à–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Ç–∏–ø–∞ (System 2). –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è OpenAI o1, –¥–∞–∂–µ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).</p>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –Ω–µ—Ö–≤–∞—Ç–∫–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.  –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–æ–º, —á—Ç–æ –¥–∞–∂–µ –µ—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω –≤–µ—Ä–Ω—ã–π –æ—Ç–≤–µ—Ç, —ç—Ç–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ —Ä–µ—à–µ–Ω–∏—è.  –û—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Å–Ω–∏–∂–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, –æ—Ü–µ–Ω–∏–≤–∞—é—â–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞, —á—Ç–æ –æ—á–µ–Ω—å —Ç—Ä—É–¥–æ–µ–º–∫–æ. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –ø–æ–∫–∞ –Ω–µ –¥–∞–µ—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑-–∑–∞ —à—É–º–∞ –≤ –æ—Ü–µ–Ω–∫–∞—Ö.  –ü–æ—ç—Ç–æ–º—É, –ø–æ–¥—Ö–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, –¥–æ—Å—Ç–∏–≥–∞—é—Ç –ø—Ä–µ–¥–µ–ª–∞ –∏ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–≤–æ–∏—Ö —É—á–∏—Ç–µ–ª–µ–π.</p>\n<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∏—Å–ø–æ–ª—å–∑—É—è –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) –∏ –ø–æ–∏—Å–∫ –ø–æ –¥–µ—Ä–µ–≤—É –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ (MCTS) –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è.  –ü—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–¥–∞:</strong>  –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –≤ —Ä–∞–º–∫–∞—Ö MCTS. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ SLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —à–∞–≥ —Ä–µ—à–µ–Ω–∏—è –≤ –≤–∏–¥–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (CoT) –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–¥–∞ –Ω–∞ Python.  –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ —à–∞–≥–∏, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –∫–æ–¥ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è.  –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –∫–∞–∂–¥–æ–º—É —à–∞–≥—É Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∫–ª–∞–¥ —à–∞–≥–∞ –≤ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –®–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—â–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è.</li>\n<li><strong>–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤ (PPM):</strong> –≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É (reward) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ä–µ—à–µ–Ω–∏—è.  –î–ª—è –æ–±—É—á–µ–Ω–∏—è PPM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ—Ç —Ñ–∞–∫—Ç, —á—Ç–æ Q-–∑–Ω–∞—á–µ–Ω–∏—è, —Ö–æ—Ç—è –∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ç–æ—á–Ω—ã–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏, –ø–æ–∑–≤–æ–ª—è—é—Ç –Ω–∞–¥–µ–∂–Ω–æ –æ—Ç–ª–∏—á–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.  –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Å–æ–∑–¥–∞—é—Ç—Å—è –ø–∞—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π, –∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ä–∞–Ω–∂–∏—Ä—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä—è–º–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ü–µ–Ω–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∑–∞—à—É–º–ª–µ–Ω—ã.</li>\n<li><strong>–ß–µ—Ç—ã—Ä–µ—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è:</strong> –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å–æ —Å–±–æ—Ä–∞ 747 —Ç—ã—Å—è—á –∑–∞–¥–∞—á –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Ä–µ—à–µ–Ω–∏—è (policy model) –∏ PPM –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è MCTS –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ó–∞—Ç–µ–º –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—Ç—Å—è –±–æ–ª–µ–µ —Å–∏–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.  –ö–∞–∂–¥—ã–π —ç—Ç–∞–ø –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —É—Å–∏–ª–µ–Ω–∏—é –º–æ–¥–µ–ª–∏, –ø–æ–≤—ã—à–µ–Ω–∏—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ PPM, —É–ª—É—á—à–µ–Ω–∏—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–µ—à–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –æ—Ö–≤–∞—Ç–∞ –∑–∞–¥–∞—á.</li>\n</ol>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ SLM (–æ—Ç 1.5 –¥–æ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –Ω–∞ —Å–µ–º–∏ –∑–∞–¥–∞—á–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å rStar-Math.  –ù–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, rStar-Math –ø–æ–≤—ã—Å–∏–ª —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B —Å 58.8% –¥–æ 89.4%, –∞ Qwen2.5-Math-1.5B —Å 51.2% –¥–æ 87.8%. –° 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ –ø–æ–∏—Å–∫–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç–∏–≥–∞—é—Ç 90% –∏ 88.4% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è o1-preview –∏ –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-minis. –ù–∞ –æ–ª–∏–º–ø–∏–∞–¥–µ AIME 2024, rStar-Math —Ä–µ—à–∏–ª 53.3% –∑–∞–¥–∞—á, —á—Ç–æ –Ω–∞ 8.7% –±–æ–ª—å—à–µ, —á–µ–º o1-preview. –ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ–¥—Ö–æ–¥–∞ rStar-Math –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤.</p>"
            },
            {
                "title": "Related Works",
                "content": "Math Data Synthesis. Advancements in LLM math reasoning have largely relied on curating high-quality CoT data, with most leading approaches being GPT-distilled, using frontier models like GPT-4 for synthesis [Wang et al., 2024b, Gou et al., 2023, Luo et al., 2023]. Notable works include NuminaMath [Jia LI and Polu, 2024a] and MetaMath [Yu et al., 2023b]. While effective, this limits reasoning to the capabilities of the teacher LLM. Hard problems that the teacher LLM cannot solve are excluded in the training set. Even solvable problems may contain error-prone intermediate steps, which are hard to detect. Although rejection sampling methods [Yuan et al., 2023, Brown et al., 2024] can improve data quality, they do not guarantee correct intermediate steps. As result, scaling up CoT data has diminishing returns, with gains nearing saturatione.g., OpenMathInstruct-2 [Toshniwal et al., 2024] only sees 3.9% boost on MATH despite an 8 increase in dataset size. Scaling Test-time Compute has introduced new scaling laws, allowing LLMs to improve performance across by generating multiple samples and using reward models for best-solution selection [Snell et al., 2024, Wu et al., 2024, Brown et al., 2024]. Various test-time search methods have been proposed [Kang et al., 2024, Wang et al., 2024a], including random sampling [Wang et al., 2023] and tree-search methods [Yao et al., 2024, Hao et al., 2023, Zhang et al., 2024b, Qi et al., 2024] like MCTS. However, open-source methods for scaling test-time computation have shown limited gains in math reasoning, often due to policy LLM or reward model limitations. rStar-Math addresses this by iteratively evolving the policy LLM and reward model, achieving System 2 mathematical reasoning performance comparable to OpenAI o1 [OpenAI, 2024]. Reward Models are crucial for effective System 2 reasoning but are challenging to obtain. Recent works include LLM-as-a-Judge for verification [Zheng et al., 2023, Qi et al., 2024] and specialized reward models like Outcome Reward Model [Yang et al., 2024, Yu et al., 2023a] and Process Reward Model (PRM) [Lightman et al., 2024]. While PRMs offer promising dense, step-level reward signals for complex reasoning [Luo et al., 2024, Wang et al., 2024c], collecting step-level annotations remains an obstacle. While Kang et al. [2024], Wang et al. [2024a] rely on costly human-annotated datasets like PRM800k [Lightman et al., 2024], recent approaches [Wang et al., 2024c, Luo et al., 2024] explore automated annotation via Monte Carlo Sampling or MCTS. However, they struggle to generate precise reward scores, which limits performance gains. rStar-Math introduces novel process preference reward (PPM) that eliminates the need for accurate step-level reward score annotation.",
                "summary": "<p><strong>–°–∏–Ω—Ç–µ–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö</strong></p>\n<p>–£—Å–ø–µ—Ö–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –≤–æ –º–Ω–æ–≥–æ–º –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–µ—Ç–æ–¥—É Chain-of-Thought (CoT). –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≤–µ–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—é –∑–Ω–∞–Ω–∏–π –∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö —Ä–∞–±–æ—Ç –≤–∫–ª—é—á–∞—é—Ç NuminaMath –∏ MetaMath. –û–¥–Ω–∞–∫–æ, —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ–±—É—á–∞—é—â–µ–π LLM. –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—â–∞—è LLM –Ω–µ –º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å, –∏—Å–∫–ª—é—á–∞—é—Ç—Å—è –∏–∑ –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –î–∞–∂–µ –≤ —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å—Å—è –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä—É–¥–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å.</p>\n<p>–ú–µ—Ç–æ–¥—ã –æ—Ç–±–æ—Ä–∞ —Å –æ—Ç–±—Ä–∞–∫–æ–≤–∫–æ–π –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö CoT –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–Ω–∏–∂–µ–Ω–∏—é –æ—Ç–¥–∞—á–∏, –∫–æ–≥–¥–∞ –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç—Å—è –∫ –Ω–∞—Å—ã—â–µ–Ω–∏—é. –ù–∞–ø—Ä–∏–º–µ—Ä, OpenMathInstruct-2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ª–∏—à—å 3.9% –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MATH, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤–æ—Å—å–º–∏–∫—Ä–∞—Ç–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (test-time compute) –≤–≤–µ–ª–æ –Ω–æ–≤—ã–µ –∑–∞–∫–æ–Ω—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª—è—è LLM —É–ª—É—á—à–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ –Ω–∞–∏–ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –ë—ã–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ –º–µ—Ç–æ–¥—ã –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ MCTS. –û–¥–Ω–∞–∫–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–µ—Ç–æ–¥—ã –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —É—Å–ø–µ—Ö–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —á–∞—Å—Ç–æ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π —Å–∞–º–æ–π LLM –∏–ª–∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>\n<p>rStar-Math —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø—É—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è LLM –∏ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —Å—Ä–∞–≤–Ω–∏–º–æ–π —Å OpenAI o1. –ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, –Ω–æ –∏—Ö —Å–ª–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å. –ù–µ–¥–∞–≤–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ \"—Å—É–¥—å–∏\" –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ Outcome Reward Model –∏ Process Reward Model (PRM). PRM –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –Ω–æ —Å–±–æ—Ä –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π.</p>\n<p>–í —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏, –¥—Ä—É–≥–∏–µ –∏—Å—Å–ª–µ–¥—É—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é Monte Carlo Sampling –∏–ª–∏ MCTS. –û–¥–Ω–∞–∫–æ, –æ–Ω–∏ –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. rStar-Math –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø—Ä–æ—Ü–µ—Å—Å–∞ (PPM), –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ü–µ–Ω–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤.</p>"
            },
            {
                "title": "Methodology",
                "content": "3.1 Design Choices MCTS for Effective System 2 Reasoning. We aim to train math policy SLM and process reward model (PRM), and integrating both within Monte Carlo Tree Search (MCTS) for System 2 deep thinking. MCTS is chosen for two key reasons. First, it breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other 3 System 2 methods like Best-of-N [Brown et al., 2024] or self-consistency [Wang et al., 2023], which require generating full solutions in one inference. Second, the step-by-step generation in MCTS naturally yields step-level training data for both models. Standard MCTS rollout automatically assign Q-value to each step based on its contribution to the final correct answer, obviating the need for human-generated step-level annotations for process reward model training. Ideally, advanced LLMs such as GPT-4 could be integrated within MCTS to generate training data. However, this approach faces two key challenges. First, even these powerful models struggle to consistently solve difficult problems, such as Olympiad-level mathematics. Consequently, the resulting training data would primarily consist of simpler solvable problems, limiting its diversity and quality. Second, annotating per-step Q-values demands extensive MCTS rollouts; insufficient tree exploration can lead to spurious Q-value assignments, such as overestimating suboptimal steps. Given that each rollout involves multiple single-step generations and these models are computationally expensive, increasing rollouts significantly raises inference costs. Overview. To this end, we explore using two 7B SLMs (a policy SLM and PRM) to generate higherquality training data, with their smaller size allowing for extensive MCTS rollouts on accessible hardware (e.g., 440GB A100 GPUs). However, self-generating data presents greater challenges for SLMs, due to their weaker capabilities. SLMs frequently fail to generate correct solutions, and even when the final answer is correct, the intermediate steps are often flawed or of poor quality. Moreover, SLMs solve fewer challenging problems compared to advanced models like GPT-4. This section introduces our methodology, as illustrated in Fig. 1. To mitigate errors and low-quality intermediate steps, we introduce code-augmented CoT synthetic method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories, annotated with Q-values. To further improve SLM performance on challenging problems, we introduce four-round self-evolution recipe. In each round, both the policy SLM and the reward model are updated to stronger versions, progressively tackling more difficult problems and generating higher-quality training data. Finally, we present novel process reward model training approach that eliminates the need for precise per-step reward annotations, yielding the more effective process preference model (PPM). 3.2 Step-by-Step Verified Reasoning Trajectory We start by introducing our method for generating step-by-step verified reasoning trajectories with per-step Q-value annotations. Given problem and policy model , we run the standard MCTS to incrementally construct search tree for step-by-step solution exploration. As shown in Fig. 1(a), the root node represents question x, while child nodes correspond to intermediate steps generated by . root-to-leaf path ending at terminal node sd forms trajectory = s1 s2 ... sd, with each step si assigned Q-value Q(si). From the search tree , we extract solution trajectories = {t1, t2, ..., tn}(n 1). Our goal is to select high-quality trajectories from to construct the training set. For this purpose, we introduce code-augmented CoT synthesis method to filter out low-quality generations and perform extensive rollouts to improve the reliability of Q-value accuracy. Code-augmented CoT Generation. Prior MCTS approaches primarily generate natural language (NL) CoTs [Qi et al., 2024, Zhang et al., 2024a]. However, LLMs often suffer from hallucination, producing incorrect or irrelevant steps yet still arrive at the correct answer by chance [Lanham et al., 2023]. These flawed steps are challenging to detect and eliminate. To address this, we propose novel code execution augmented CoT. As shown in Fig. 2, the policy model generates one-step NL CoT alongside its corresponding Python code, where the NL CoT is embedded as Python comment. Only generations with successfully executed Python code are retained as valid candidates. Figure 2: An example of Code-augmented CoT. Specifically, starting from the initial root node x, we perform multiple MCTS iterations through selection, expansion, rollout, and back-propagation. At step i, we collect the latest reasoning trajectory s1 s2 ... si1 as the current state. Based on this state, we prompt (see Appendix A.3) the policy model to generate candidates si,0, ..., si,n1 for step i. Python code execution is then employed to filter valid nodes. As shown in Fig. 2, each generation si,j is concatenated with the code from all previous steps, forming s1 s2 ... si1 si,j. Candidates that execute successfully are retained as valid nodes and scored by the PPM, which assigns Q-value q(si). Then, we use the well-known Upper Confidence bounds for Trees (UCT) [Kocsis and Szepesv√°ri, 2006] to select the best node among the candidates. This selection process is mathematically represented as: UCT(s) = Q(s) + (cid:115) ln Nparent(s) (s) ; where Q(s) = q(s) (s) (1) where (s) denotes the number of visits to node s, and Nparent(s) is the visit count of ss parent node. The predicted reward q(s) is provided by the PPM and will be updated through back-propagation. is constant that balances exploitation and exploration. Extensive Rollouts for Q-value Annotation. Accurate Q-value Q(s) annotation in Eq. 1 is crucial for guiding MCTS node selection towards correct problem-solving paths and identifying high-quality steps within trajectories. To improve Q-value reliability, we draw inspiration from Go players, who retrospectively evaluate the reward of each move based on game outcomes. Although initial estimates may be imprecise, repeated gameplay refines these evaluations over time. Similarly, in each rollout, we update the Q-value of each step based on its contribution to achieving the correct final answer. After extensive MCTS rollouts, steps consistently leading to correct answers achieve higher Q-values, occasional successes yield moderate Q-values, and consistently incorrect steps receive low Q-values. Specifically, we introduce two self-annotation methods to obtain these step-level Q-values. Fig. 1(c) shows the detailed setting in the four rounds of self-evolution. Terminal-guided annotation. During the first two rounds, when the PPM is unavailable or insufficiently accurate, we use terminal-guided annotation. Formally, let q(si)k denote the value for step si after back-propagation in the kth rollout. Following AlphaGo [Silver et al., 2017] and rStar [Qi et al., 2024], we score each intermediate node based on its contribution to the final correct answer: q(si)k = q(si)k1 + q(sd)k; where the initial value q(si)0 = 0 in the first rollout. If this step frequently leads to correct answer, its value will increase; otherwise, it decreases. Terminal nodes are scored as q(sd) = 1 for correct answers and q(sd) = 1 otherwise, as shown in Fig. 1. (2) PRM-augmented annotation. Starting from the third round, we use PPM to score each step for more effective generation. Compared to terminal-guided annotation, which requires multiple rollouts for meaningful value, PPM directly predicts non-zero initial value. PPM-augmented MCTS also helps the policy model to generate higher-quality steps, guiding solutions towards correct paths. Formally, for step si, PPM predicts an initial q(si)0 value based on the partial trajectory: q(si)0 = (x s1 s2 ... si1 si) This value will be updated based on terminal nodes q(sd) value through MCTS back-propagation in Eq. 2. For terminal node sd, we do not use PRM for scoring during training data generation. Instead, we assign more accurate score based on ground truth labels as terminal-guided rewarding. (3) 3.3 Process Preference Model Process reward models, which provide granular step-level reward signals, is highly desirable for solving challenging math problems. However, obtaining high-quality step-level training data remains an open challenge. Existing methods rely on human annotations [Lightman et al., 2023] or MCTSgenerated scores [Zhang et al., 2024a, Chen et al., 2024] to assign score for each step. These scores then serve as training targets, with methods such as MSE loss [Chen et al., 2024] or pointwise loss [Wang et al., 2024c, Luo et al., 2024, Zhang et al., 2024a] used to minimize the difference between predicted and labeled scores. As result, the precision of these annotated step-level reward scores directly determines the effectiveness of the resulting process reward model. Unfortunately, precise per-step scoring remains unsolved challenge. Although our extensive MCTS rollouts improve the reliability of Q-values, precisely evaluating fine-grained step quality presents 5 major obstacle. For instance, among set of correct steps, it is difficult to rank them as best, secondbest, or average and then assign precise scores. Similarly, among incorrect steps, differentiating the worst from moderately poor steps poses analogous challenges. Even expert human annotation struggles with consistency, particularly at scale, leading to inherent noise in training labels. We introduce novel training method that trains process preference model (PPM) by constructing step-level positive-negative preference pairs. As shown in Fig. 1(b), instead of using Q-values as direct reward labels, we use them to select steps from MCTS tree for preference pair construction. For each step, we select two candidates with the highest Q-values as positive steps and two with the lowest as negative steps. Critically, the selected positive steps must lead to correct final answer, while negative steps must lead to incorrect answers. For intermediate steps (except the final answer step), the positive and negative pairs share the same preceding steps. For the final answer step, where identical reasoning trajectories rarely yield different final answers, we relax this restriction. We select two correct trajectories with the highest average Q-values as positive examples and two incorrect trajectories with the lowest average Q-values as negative examples. Following [Ouyang et al., 2022], we define our loss function using the standard Bradley-Terry model with pairwise ranking loss: Lppm(Œ∏) = when is not final answer step, ypos E(x,ypos = s1 ... si1 spos (5) Here, rŒ∏(x, yi) denotes the output of the PPM, where is the problem and is the trajectory from the first step to the ith step. D)[log(œÉ(rŒ∏(x, ypos ,yneg ; yneg = s1 ... si1 sneg ) rŒ∏(x, yneg )))] (4) i 1 2 2 3.4 Self-Evolved Deep Thinking 3.4.1 Training with Step-by-Step Verified Reasoning Trajectory Math Problems Collection. We collect large dataset of 747k math word problems with final answer ground-truth labels, primarily from NuminaMath [Jia LI and Polu, 2024a] and MetaMath [Yu et al., 2023b]. Notably, only competition-level problems (e.g., Olympiads and AIME/AMC) from NuminaMath are included, as we observe that grade-school-level problems do not significantly improve LLM complex math reasoning. To augment the limited competition-level problems, we follow [Li et al., 2024] and use GPT-4 to synthesize new problems based on the seed problems in 7.5k MATH train set and 3.6k AMC-AIME training split. However, GPT-4 often generated unsolvable problems or incorrect solutions for challenging seed problems. To filter these, we prompt GPT-4 to generate 10 solutions per problem, retaining only those with at least 3 consistent solutions. Reasoning Trajectories Collection. Instead of using the original solutions in the 747k math dataset, we conduct extensive MCTS rollouts (Sec. 3.2) to generate higher-quality step-by-step verified reasoning trajectories. In each self-evolution round, we perform 16 rollouts per math problem, which leads to 16 reasoning trajectories. Problems are then categories by difficulty based on the correct ratio of the generated trajectories: easy (all solutions are correct), medium (a mix of correct and incorrect solutions) and hard (all solutions are incorrect). For hard problems with no correct trajectories, an additional MCTS with 16 rollouts is performed. After that, all step-by-step trajectories and their annotated Q-values are collected and filtered to train the policy SLM and process preference model. Supervised Fine-tuning the Policy SLM. Through extensive experiments, we find that selecting high-quality reasoning trajectories is the key for fine-tuning frontier math LLM. While methods such as GPT-distillation and Best-of-N can include low-quality or erroneous intermediate steps, more effective approach ensures that every step in the trajectory is of high quality. To achieve this, we use per-step Q-values to select optimal trajectories from MCTS rollouts. Specifically, for each math problem, we select the top-2 trajectories with the highest average Q-values among those leading to correct answers as SFT training data. Training PPM. The PPM is initialized from the fine-tuned policy model, with its next-token prediction head replaced by scalar-value head consisting of linear layer and tanh function to constrain outputs to the range [-1, 1]. We filter out math problems where all solution trajectories are fully correct or incorrect. For problems with mixed outcomes, we select two positive and two negative examples for each step based on Q-values, which are used as preference pairs for training data. 3.4.2 Recipe for Self-Evolution Due to the weaker capabilities of SLMs, we perform four rounds of MCTS deep thinking to progressively generate higher-quality data and expand the training set with more challenging math problems. Table 2: Percentage of the 747k math problems correctly solved in each round. Only problems have correct solutions are included in the training set. The first round uses DeepSeek-Coder-Instruct as the policy LLM, while later rounds use our fine-tuned 7B policy SLM. # models in MCTS GSM-level MATH-level Olympiad-level All Round 1 DeepSeek-Coder-V2-Instruct Round 2 Round 3 Round policy SLM-r1 policy SLM-r2, PPM-r2 policy SLM-r3, PPM-r3 96.61% 97.88% 98.15% 98.15% 67.36% 67.40% 88.69% 94.53% 20.99% 56.04% 62.16% 80.58% 60.17% 66.60% 77.86% 90.25% Table 3: Pass@1 accuracy of the resulting policy SLM in each round, showing continuous improvement until surpassing the bootstrap model. Round# MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 DeepSeek-Coder-V2-Instruct (bootstrap model) Base (Qwen2.5-Math-7B) policy SLM-r1 policy SLM-r2 policy SLM-r3 policy SLM-r4 75.3 58.8 69.6 73.6 75.8 78. 13.3 0.0 3.3 10.0 16.7 26.7 57.5 22.5 30.0 35.0 45.0 47.5 37.6 21.8 34.7 39.0 44.1 47. 46.2 41.6 44.5 45.7 49.6 52.5 94.9 91.6 88.4 89.1 89.3 89.7 64.7 51.7 57.4 59.7 62.8 65. Each round uses MCTS to generate step-by-step verified reasoning trajectories, which are then used to train the new policy SLM and PPM. The new models are then applied in next round to generate higher-quality training data. Fig. 1(c) and Table 2 detail the models used for data generation in each round, along with the identifiers of the trained policy model and PPM. Next, we outline the details and specific improvements targeted in each round. Round 1: Bootstrapping an initial strong policy SLM-r1. To enable SLMs to self-generate reasonably good training data, we perform bootstrap round to fine-tune an initial strong policy model, denoted as SLM-r1. As shown in Table 2, we run MCTS with DeepSeek-Coder-V2-Instruct (236B) to collect the SFT data. With no available reward model in this round, we use terminal-guided annotation for Q-values and limit MCTS to 8 rollouts for efficiency. For correct solutions, the top-2 trajectories with the highest average Q-values are selected as SFT data. We also train PPM-r1, but the limited rollouts yields unreliable Q-values, affecting the effectiveness of PPM-r1 ( Table 4). Round 2: Training reliable PPM-r2. In this round, with the policy model updated to the 7B SLM-r1, we conduct extensive MCTS rollouts for more reliable Q-value annotation and train the first reliable reward model, PPM-r2. Specifically, we perform 16 MCTS rollouts per problem. The resulting step-by-step verified reasoning trajectories show significant improvements in both quality and Q-value precision. As shown in Table 4, PPM-r2 is notably more effective than in the bootstrap round. Moreover, the policy SLM-r2 also continues to improve as expected  (Table 3)  . Round 3: PPM-augmented MCTS to significantly improve data quality. With the reliable PPM-r2, we perform PPM-augmented MCTS in this round to generate data, leading to significantly higher-quality trajectories that cover more math and Olympiad-level problems in the training set  (Table 2)  . The generated reasoning trajectories and self-annotated Q-values are then used to train the new policy SLM-r3 and PPM-r3, both of which show significant improvements. Round 4: Solving challenging math problems. After the third round, while grade school and MATH problems achieve high success rates, only 62.16% of Olympiad-level problems are included in the training set. This is NOT solely due to weak reasoning abilities in our SLMs, as many Olympiad problems remain unsolved by GPT-4 or o1. To improve coverage, we adopt straightforward strategy. For unsolved problems after 16 MCTS rollouts, we perform an additional 64 rollouts, and if needed, increase to 128. We also conduct multiple MCTS tree expansions with different random seeds. This boosts the success rate of Olympiad-level problems to 80.58%. After four rounds of self-evolution, 90.25% of the 747k math problems are successfully covered into the training set, as shown in Table 2. Among the remaining unsolved problems, significant portion consists of synthetic questions. We manually review random sample of 20 problems and find that 19 are incorrectly labeled with wrong answers. Based on this, we conclude that the remaining unsolved problems are of low quality and thus terminate the self-evolution at round 4. 7 Table 4: The quality of PPM consistently improves across rounds. The policy model has been fixed with policy SLM-r1 for fair comparison. Round# MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 PPM-r1 PPM-r2 PPM-r3 PPM-r4 75.2 84.1 85.2 87. 10.0 26.7 33.3 43.3 57.5 75.0 77.5 77.5 35.7 52.7 59.5 61.5 45.4 54.2 55.6 56.8 90.9 93.3 93.9 94.2 60.3 73.0 76.6 77.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 3.1-3.3 —Å—Ç–∞—Ç—å–∏ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ MCTS –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á</h2>\n<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–∞ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (MCTS) –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å –¥–≤—É–º—è –º–æ–¥–µ–ª—è–º–∏: –ø–æ–ª–∏—Ç–∏–∫–æ–π (SLM) –∏ –º–æ–¥–µ–ª—å—é –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PRM). MCTS –≤—ã–±—Ä–∞–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø–æ –¥–≤—É–º –ø—Ä–∏—á–∏–Ω–∞–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–µ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —á—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç —Ä–∞–±–æ—Ç—É –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ Best-of-N –∏–ª–∏ —Å–∞–º–æ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –ø–æ—à–∞–≥–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ MCTS –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π MCTS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–º—É —à–∞–≥—É, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –µ–≥–æ –≤–∫–ª–∞–¥–µ –≤ –∏—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, —á—Ç–æ –∏–∑–±–∞–≤–ª—è–µ—Ç –æ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>\n<p>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ç–∞–∫–∏—Ö –∫–∞–∫ GPT-4, –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö MCTS —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –¥–≤—É–º—è –ø—Ä–æ–±–ª–µ–º–∞–º–∏. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–∞–∂–µ —Ç–∞–∫–∏–µ –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –≤—Å–µ–≥–¥–∞ –º–æ–≥—É—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —Ä–µ—à–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç —Å–æ—Å—Ç–æ—è—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö, —Ä–µ—à–∞–µ–º—ã—Ö –∑–∞–¥–∞—á, —á—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç –∏—Ö —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS; –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–µ—Ä–µ–≤–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ª–æ–∂–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∫ –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–µ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤. –£—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ –∫–∞–∂–¥—ã–π –ø—Ä–æ–≥–æ–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π, –∞ LLM —è–≤–ª—è—é—Ç—Å—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ –∑–∞—Ç—Ä–∞—Ç–Ω—ã–º–∏, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–æ–Ω–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.</p>\n<p>–î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º –∞–≤—Ç–æ—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –¥–≤–µ 7B –º–æ–¥–µ–ª–∏ SLM (–æ–¥–Ω–∞ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ –æ–¥–Ω–∞ –¥–ª—è PRM) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ú–µ–Ω—å—à–∏–π —Ä–∞–∑–º–µ—Ä —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–∏. –û–¥–Ω–∞–∫–æ, —Å–∞–º–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –±–æ–ª—å—à—É—é –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è SLM –∏–∑-–∑–∞ –∏—Ö –º–µ–Ω—å—à–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. SLM —á–∞—Å—Ç–æ –Ω–µ –º–æ–≥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∏ –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç –æ—à–∏–±–æ—á–Ω—ã–º–∏ –∏–ª–∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.</p>\n<p>–î–ª—è —Å–º—è–≥—á–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –∏ –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±—à–∏—Ä–Ω—ã–µ –ø—Ä–æ–≥–æ–Ω—ã MCTS –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—à–∞–≥–æ–≤—ã—Ö –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏. –î–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ SLM –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç —Ä–µ—Ü–µ–ø—Ç —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –≤ —á–µ—Ç—ã—Ä–µ —Ä–∞—É–Ω–¥–∞. –í –∫–∞–∂–¥–æ–º —Ä–∞—É–Ω–¥–µ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Ä–µ—à–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.</p>\n<p>–ú–µ—Ç–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∑–∞–ø—É—Å–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ MCTS –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ä–µ—à–µ–Ω–∏–π. –ö–æ—Ä–Ω–µ–≤–æ–π —É–∑–µ–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–æ–ø—Ä–æ—Å, –∞ –¥–æ—á–µ—Ä–Ω–∏–µ —É–∑–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º —à–∞–≥–∞–º, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏. –ü—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –¥–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —É–∑–ª–∞ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é, –∫–∞–∂–¥–æ–º—É —à–∞–≥—É –≤ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ —Å–∏–Ω—Ç–µ–∑–∞ CoT, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –∫–æ–¥–æ–º, –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–Ω–∏—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π.</p>\n<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ CoT, –∑–¥–µ—Å—å –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–¥–Ω–æ—à–∞–≥–æ–≤—ã–π CoT –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –≤–º–µ—Å—Ç–µ —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º –Ω–∞ Python, –≥–¥–µ CoT –≤—Å—Ç—Ä–æ–µ–Ω –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è. –°–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ–¥ –∫–æ—Ç–æ—Ä—ã—Ö —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–æ—á–Ω—ã–µ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É —Å–ª—É—á–∞–π–Ω–æ. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ MCTS, –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —Ñ–∏–ª—å—Ç—Ä—É—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞. –û—Å—Ç–∞–≤—à–∏–µ—Å—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç—Å—è –º–æ–¥–µ–ª—å—é PRM, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç Q-–∑–Ω–∞—á–µ–Ω–∏–µ. –ó–∞—Ç–µ–º, —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ UCT –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∏–π —É–∑–µ–ª.</p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ Q-–∑–Ω–∞—á–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –∏–≥—Ä–æ–∫–∞–º –≤ –ì–æ, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ö–æ–¥–∞ –∏–≥—Ä—ã. –ü–æ—Å–ª–µ –æ–±—à–∏—Ä–Ω—ã—Ö –ø—Ä–æ–≥–æ–Ω–æ–≤ MCTS, —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è, –∞ —à–∞–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –Ω–µ–≤–µ—Ä–Ω—ã–º –æ—Ç–≤–µ—Ç–∞–º, –ø–æ–ª—É—á–∞—é—Ç –Ω–∏–∑–∫–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –º–µ—Ç–æ–¥–∞ —Å–∞–º–æ–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: —Ç–µ—Ä–º–∏–Ω–∞–ª—å–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–µ—Ä–≤—ã—Ö –¥–≤—É—Ö —Ä–∞—É–Ω–¥–∞—Ö, –∫–æ–≥–¥–∞ PRM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω–∞) –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è, –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω–∞—è PRM (–Ω–∞—á–∏–Ω–∞—è —Å —Ç—Ä–µ—Ç—å–µ–≥–æ —Ä–∞—É–Ω–¥–∞). –í –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ —à–∞–≥–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–≥–æ, –ø—Ä–∏–≤–µ–ª –ª–∏ –æ–Ω –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É –≤ –∫–æ–Ω—Ü–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ PRM –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –∑–∞—Ç–µ–º –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è.</p>\n<p>–ú–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ –≥—Ä–∞–Ω—É–ª—è—Ä–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤, –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á. –û–¥–Ω–∞–∫–æ, –ø–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∏—Ö –æ—Å—Ç–∞–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É –∏–ª–∏ –Ω–∞ –æ—Ü–µ–Ω–∫–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ MCTS. –¢–æ—á–Ω–æ—Å—Ç—å —ç—Ç–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (PPM), –∫–æ—Ç–æ—Ä—ã–π –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä—è–º—ã—Ö –º–µ—Ç–æ–∫ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö –¥–ª—è –≤—ã–±–æ—Ä–∞ —à–∞–≥–æ–≤ –∏–∑ –¥–µ—Ä–µ–≤–∞ MCTS –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–∞—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –≤—ã–±–∏—Ä–∞—é—Ç—Å—è –¥–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å —Å–∞–º—ã–º–∏ –≤—ã—Å–æ–∫–∏–º–∏ Q-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —à–∞–≥–æ–≤ –∏ –¥–≤–∞ —Å —Å–∞–º—ã–º–∏ –Ω–∏–∑–∫–∏–º–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –ó–∞—Ç–µ–º PPM –æ–±—É—á–∞–µ—Ç—Å—è –æ—Ç–ª–∏—á–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —à–∞–≥–∏ –æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ —Ç–æ—á–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∞—Ö –Ω–∞ —É—Ä–æ–≤–Ω–µ —à–∞–≥–æ–≤ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.</p>"
            },
            {
                "title": "Evaluation",
                "content": "4.1 Setup Evaluation Datasets. We evaluate rStar-Math on diverse mathematical benchmarks. In addition to the widely-used GSM8K [Cobbe et al., 2021], we include challenging benchmarks from multiple domains: (i) competition and Olympiad-level benchmarks, such as MATH-500 [Lightman et al., 2023], AIME 2024 [AI-MO, 2024a], AMC 2023 [AI-MO, 2024b] and Olympiad Bench [He et al., 2024]. Specifically, AIME is the exams designed to challenge the brightest high school math students in American, with the 2024 dataset comprising 30 problems from AIME and II exams; (ii) collegelevel math problems from College Math [Tang et al., 2024] and (iii) out-of-domain math benchmark: GaoKao (Chinese College Entrance Exam) En 2023 [Liao et al., 2024]. Base Models and Setup. rStar-Math is general approach applicable to various LLMs. To show its effectiveness and generalizability, we use SLMs of different sizes as the base policy models: Qwen2.5-Math-1.5B [Qwen, 2024b], Phi3-mini-Instruct (3B) [Microsoft, 2024, Abdin et al., 2024], Qwen2-Math-7B [Qwen, 2024a] and Qwen2.5-Math-7B [Qwen, 2024c]. Among these, Phi3-miniInstruct is general-purpose SLM without specialization in math reasoning. Due to limited GPU resources, we performed 4 rounds of self-evolution exclusively on Qwen2.5Math-7B, yielding 4 evolved policy SLMs  (Table 3)  and 4 PPMs  (Table 4)  . For the other 3 policy LLMs, we fine-tune them using step-by-step verified trajectories generated from Qwen2.5-Math-7Bs 4th round. The final PPM from this round is then used as the reward model for the 3 policy SLMs. Baselines. rStar-Math is System 2 method. We compare it against three strong baselines representing both System 1 and System 2 approaches: (i) Frontier LLMs, including GPT-4o, the latest Claude, OpenAI o1-preview and o1-mini. We measure their accuracy on AMC 2023, Olympiad Bench, College Math, Gaokao and GSM8K, with accuracy numbers for other benchmarks are taken from public technical reports [Team, 2024a]. (ii) Open-sourced superior reasoning models, including DeepSeek-Coder-v2-Instruct, Mathstral [Team, 2024b], NuminaMath-72B [Jia LI and Polu, 2024a], and LLaMA3.1 [Dubey et al., 2024], which represent the current mainstream System 1 approaches for improving LLM math reasoning. (iii) Both System 1 and System 2 performance of the base models trained from the original models teams, including Instruct versions (e.g., Qwen2.5-Math-7B-Instruct) and Best-of-N (e.g., Qwen2.5-Math-72B-Instruct+Qwen2.5-Math-RM-72B). Notably, the reward model used for the three Qwen base models is 72B ORM, significantly larger than our 7B PPM. Evaluation Metric. We report Pass@1 accuracy for all baselines. For System 2 baselines, we use default evaluation settings, such as default thinking time for o1-mini and o1-preview. For Qwen models with Best-of-N, we re-evaluate MATH-500, AIME/AMC accuracy; other benchmarks results are from their technical reports. For fair comparison, rStar-Math run MCTS to generate the same number of solutions as Qwen. Specifically, for AIME/AMC, we generate 16 trajectories for AIME/AMC and 8 for other benchmarks, using PPM to select the best solution. We also report performance with increased test-time computation using 64 trajectories, denoted as rStar-Math64. 4.2 Main Results Results on diverse challenging math benchmarks. Table 5 shows the results of rStar-Math with comparing to state-of-the-art reasoning models. We highlight three key observations: (1) rStar-Math significantly improves SLMs math reasoning capabilities, achieving performance comparable to or surpassing OpenAI o1 with substantially smaller model size (1.5B-7B). For example, Qwen2.5Math-7B, originally at 58.8% accuracy on MATH, improved dramatically to 90.0% with rStar-Math, outperforming o1-preview and Claude 3.5 Sonnet while matching o1-mini. On the College Math benchmark, rStar-Math exceeds o1-mini by 2.7%. On AIME 2024, rStar-Math scored 53.3%, ranking just below o1-mini, with the 7B model solving 8/15 problems in both AIME and II, placing in the top 20% of the brightest high school math students. Notably, 8 of the unsolved problems were 8 Table 5: The results of rStar-Math and other frontier LLMs on the most challenging math benchmarks. rStar-Math64 shows the Pass@1 accuracy achieved when sampling 64 trajectories. Competition and College Level Method MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K OOD Gaokao En 2023 Model Frontier LLMs GPT-4o Claude3.5-Sonnet GPT-o1-preview GPT-o1-mini Open-Sourced Reasoning LLMs System 1 DeepSeek-Coder-V2-Instruct System 1 Mathstral-7B-v0.1 System 1 NuminaMath-72B-CoT System 1 LLaMA3.1-8B-Instruct System 1 LLaMA3.1-70B-Instruct Qwen2.5-Math-72B-Instruct System 1 Qwen2.5-Math-72B-Instruct+72B ORM System System 1 System 1 - - 76.6 78.3 85.5 90.0 75.3 57.8 64.0 51.4 65.4 85.6 85.8 9.3 16.0 44.6 56.7 13.3 0.0 3.3 6.7 23.3 30.0 36.7 47.5 - 90.0 95. 57.5 37.5 70.0 25.0 50.0 70.0 72.5 43.3 - - 65.3 37.6 21.5 32.6 15.4 27.7 49.0 54.5 Phi3-mini-Instruct (base model) rStar-Math (3.8B SLM+7B PPM) rStar-Math64 (3.8B SLM+7B PPM) System 1 System 2 System 2 41.4 85.4 86. 3.33 40.0 43.3 7.5 77.5 80.0 12.3 59.3 60.3 General Base Model: Phi3-mini-Instruct (3.8B) System 1 Qwen2.5-Math-1.5B (base model) Qwen2.5-Math-1.5B-Instruct System 1 Qwen2.5-Math-1.5B-Instruct+72B ORM System 2 rStar-Math (1.5B SLM+7B PPM) System 2 rStar-Math64 (1.5B SLM+7B PPM) System 2 Math-Specialized Base Model: Qwen2.5-Math-1.5B 16.7 38.1 47.3 63.5 64. 51.2 60.0 83.4 87.8 88.6 22.5 60.0 72.5 80.0 85.0 0.0 10.0 20.0 46.7 46.7 Math-Specialized Base Model: Qwen2-Math-7B Qwen2-Math-7B (base model) Qwen2-Math-7B-Instruct Qwen2-Math-7B-Instruct+72B ORM rStar-Math (7B SLM+7B PPM) rStar-Math64 (7B SLM+7B PPM) System 1 System 1 System 2 System 2 System 53.4 73.2 83.4 88.2 88.6 3.3 13.3 23.3 43.3 46.7 25.0 62.5 62.5 80.0 85.0 17.3 38.2 47.6 63.1 63.4 System 1 Qwen2.5-Math-7B (base model) Qwen2.5-Math-7B-Instruct System 1 Qwen2.5-Math-7B-Instruct+72B ORM System 2 rStar-Math (7B SLM+7B PPM) System 2 rStar-Math64 (7B SLM+7B PPM) System 2 Math-Specialized Base Model: Qwen2.5-Math-7B 21.8 41.6 49.9 65.3 65. 58.8 82.6 88.4 89.4 90.0 22.5 62.5 75.0 87.5 87.5 0.0 6.0 26.7 50.0 53.3 48.5 - - 57.8 46.2 33.7 39.7 33.8 42.5 49.5 50.6 33.1 58.0 59. 38.4 47.7 50.2 59.0 59.3 39.4 45.9 47.9 58.4 59.3 41.6 46.8 49.6 59.0 60.5 92.9 96.4 - 94.8 94.9 84.9 90.8 76.6 94.1 95.9 96.4 85.7 94.5 94. 74.6 84.8 94.1 94.3 94.8 80.4 89.9 95.1 94.6 94.8 91.6 95.2 97.9 95.0 95.2 67.5 - - 78.4 64.7 46.0 58.4 38.4 54.0 71.9 76.9 37.1 77.1 77. 46.5 65.5 73.0 77.7 79.5 47.3 62.1 71.9 78.2 79.2 51.7 66.8 75.1 80.5 81.3 geometry-based, requiring visual understanding, capability rStar-Mathcurrently does not support. (2) Despite using smaller policy models (1.5B-7B) and reward models (7B), rStar-Math significantly outperforms state-of-the-art System 2 baselines. Compared to Qwen Best-of-N baselines, which use the same base models (Qwen2-Math-7B, Qwen2.5-Math-1.5B/7B) but 10 larger reward model (Qwen2.5-Math-RM-72B), rStar-Math consistently improves the reasoning accuracy of all base models to state-of-the-art levels. Even against Best-of-N with 10 larger Qwen2.5-Math-72BInstruct policy model, rStar-Math surpasses it on all benchmarks except GSM8K, using the same number of sampled solutions. (3) Beyond well-known benchmarks like MATH, GSM8K, and AIME, which may risk over-optimization, rStar-Math shows strong generalizability on other challenging math benchmarks, including Olympiad Bench, College Math, and the Chinese College Entrance Math Exam (Gaokao), setting new state-of-the-art scores. As discussed in Sec. 3.4, our training set is primarily sourced from public datasets, with no specific optimizations for these benchmarks. Scaling up test-time computation. rStar-Math uses MCTS to augment the policy model, searching solutions guided by the PPM. By increasing test-time computation, it explores more trajectories, potentially improving performance. In Fig. 3, we show the impact of test-time compute scaling by comparing the accuracy of the official Qwen Best-of-N across different numbers of sampled trajectories on four challenging math benchmarks. Sampling only one trajectory corresponds to the policy LLMs Pass@1 accuracy, indicating fallback to System 1 reasoning. We highlight two key Figure 3: Reasoning performance under scaling up the test-time compute. observations: (1) With only 4 trajectories, rStar-Math significantly outperforms Best-of-N baselines, exceeding o1-preview and approaching o1-mini, demonstrating its effectiveness. (2) Scaling test-time compute improves reasoning accuracy across all benchmarks, though with varying trends. On Math, AIME, and Olympiad Bench, rStar-Math shows saturation or slow improvement at 64 trajectories, while on College Math, performance continues to improve steadily. 4.3 Ablation Study and Analysis We ablate the effectiveness of our three innovations. For System 2-style inference, Pass@1 accuracy is measured with 16 trajectories for AIME and AMC, and 8 for other benchmarks. Table 6: The continuously improved math reasoning capabilities through rStar-Math self-evolved deep thinking. Starting from round 2, the 7B base model powered by rStar-Math surpasses GPT-4o. Round# GPT-4o Base 7B model rStar-Math Round 1 rStar-Math Round 2 rStar-Math Round 3 rStar-Math Round 4 MATH AIME 2024 AMC 2023 Olympiad Bench College Math GSM8K GaokaoEn 2023 76.6 58.8 75.2 86.6 87.0 89.4 9. 0.0 10.0 43.3 46.7 50.0 47.5 22.5 57.5 75.0 80.0 87.5 43.3 21.8 35.7 59.4 61.6 65.3 48. 41.6 45.4 55.6 56.5 59.0 92.9 91.6 90.9 94.0 94.2 95.0 67.5 51.7 60.3 76.4 77.1 80.5 The effectiveness of self-evolution. The impressive results in Table 5 are achieved after 4 rounds of rStar-Math self-evolved deep thinking. Table 6 shows the math reasoning performance in each round, demonstrating continuous improvement in accuracy. In round 1, the main improvement comes from applying SFT to the base model. Round 2 brings significant boost with the application of stronger PPM in MCTS, which unlocks the full potential of System 2 deep reasoning. Notably, starting from round 2, rStar-Math outperforms GPT-4o. Rounds 3 and 4 show further improvements, driven by stronger System 2 reasoning through better policy SLMs and PPMs. The effectiveness of step-by-step verified reasoning trajectory. rStar-Math generates step-by-step verified reasoning trajectories, which eliminate error intermediate steps and further expand training set with more challenging problems. To evaluate its effectiveness, we use the data generated from round 4 as SFT training data and compare it against three strong baselines: (i) GPT-distillation, which includes open-sourced CoT solutions synthesized using GPT-4, such as MetaMath [Yu et al., 2023b], NuminaMath-CoT [Jia LI and Polu, 2024b]; (ii) Random sampling from self-generation, which use the same policy model (i.e., policy SLM-r3) to randomly generate trajectories; (iii) Rejection sampling, where 32 trajectories are randomly sampled from the policy model, with high-quality solutions ranked by our trained ORM (appendix A.1). For fairness, we select two correct trajectories for each math problem in baseline (ii) and (iii). All SFT experiments use the same training recipe. Table 7 shows the math reasoning accuracy of Qwen2.5-Math-7B fine-tuned on different datasets. We highlight two observations: (i) Fine-tuning with our step-by-step verified trajectories significantly outperforms all other baselines. This is primarily due to our PPM-augmented MCTS for code-augmented CoT synthesis, which provides denser verification during math solution generation. It proves more effective than both random sampling, which lacks verification, and rejection sampling, 10 Table 7: Ablation study on the effectiveness of our step-by-step verified reasoning trajectories as the SFT dataset. We report the SFT accuracy of Qwen2.5-Math-7B fine-tuned with different datasets. Dataset MATH AIME AMC Olympiad Bench College Math GSM8K GaokaoEn GPT-4o - GPT4-distillation (Open-sourced) MetaMath NuminaMath-CoT 76.6 55.2 69. Self-generation by policy SLM-r3 Random sample Rejection sampling 72.4 73.4 Step-by-step verified (ours) 78.4 9.3 3.33 10.0 10.0 13.3 26. 47.5 32.5 50.0 45.0 47.5 47.5 43.3 19.1 37.2 41.0 44.7 47. 48.5 39.2 43.4 48.0 50.8 52.5 92.9 85.1 89.8 87.5 89.3 89. 67.5 43.6 59.5 57.1 61.7 65.7 where ORM provides only sparse verification. (ii) Even randomly sampled code-augmented CoT solutions from our SLM yields comparable or better performance than GPT-4 synthesized NuminaMath and MetaMath datasets. This indicates that our policy SLMs, after rounds of self-evolution, can generate high-quality math solutions. These results demonstrates the huge potential of our method to self-generate higher-quality reasoning data without relying on advanced LLM distillation. The effectiveness of PPM. We train both strong ORM and Q-value score-based PRM (PQM) for comparison. To ensure fair evaluation, we use the highest-quality training data: the step-by-step verified trajectories generated in round 4, with selected math problems matching those used for PPM training. Similar to PPM, we use step-level Q-values as to select positive and negative trajectories for each math problem. The ORM is trained using pairwise ranking loss [Ouyang et al., 2022], while the PQM follows [Chen et al., 2024, Zhang et al., 2024a] to use Q-values as reward labels and optimize with MSE loss. Detailed training settings are provided in Appendix A.1. Table 8: Ablation study on the reward model. Process reward models (PQM and PPM) outperform ORM, with PPM pushing the frontier of math reasoning capabilities. RM Inference MATH AIME AMC Olympiad Bench College Math GSM8K GaokaoEn o1-mini - ORM Best-of-N PQM PPM MCTS MCTS 90.0 82.6 88.2 89.4 56.7 26.7 46.7 50.0 95.0 65.0 85.0 87. 65.3 55.1 62.9 65.3 55.6 55.5 57.6 59.0 94.8 92.3 94.6 95. 78.6 72.5 79.5 80.5 Table 8 compares the performance of ORM, PQM, and PPM for System 2 reasoning using our final round policy model. ORM provides reward signals only at the end of problem solving, so we use the Best-of-N method, while PRM and PPM leverage MCTS-driven search. As shown in Table 8, both PQM and PPM outperform ORM by providing denser step-level reward signals, leading to higher accuracy on complex math reasoning tasks. However, PQM struggles on more challenging benchmarks, such as MATH and Olympiad Bench, due to the inherent imprecision of Q-values. In contrast, PPM constructs step-level preference data for training, enabling our 7B policy model to achieve comparable or superior performance to o1-mini across all benchmarks.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 4.1 \"–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö\" –∏ 4.2 \"–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\" —Å—Ç–∞—Ç—å–∏ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.</h2>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ rStar-Math, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.</p>\n<p><strong>4.1 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ rStar-Math –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, –≤–∫–ª—é—á–∞—è:</p>\n<ul>\n<li><strong>GSM8K:</strong> —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li><strong>–ó–∞–¥–∞—á–∏ —É—Ä–æ–≤–Ω—è —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –æ–ª–∏–º–ø–∏–∞–¥:</strong><ul>\n<li><strong>MATH-500:</strong> –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á.</li>\n<li><strong>AIME 2024:</strong> –∑–∞–¥–∞—á–∏, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ª—É—á—à–∏—Ö —Å—Ç–∞—Ä—à–µ–∫–ª–∞—Å—Å–Ω–∏–∫–æ–≤ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤ –°–®–ê.</li>\n<li><strong>AMC 2023:</strong> –∑–∞–¥–∞—á–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã.</li>\n<li><strong>Olympiad Bench:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è.</li>\n</ul>\n</li>\n<li><strong>–ó–∞–¥–∞—á–∏ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ —É—Ä–æ–≤–Ω—è –∫–æ–ª–ª–µ–¥–∂–∞:</strong><ul>\n<li><strong>College Math:</strong> –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á –∏–∑ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—Å–∫–∏—Ö –∫—É—Ä—Å–æ–≤.</li>\n</ul>\n</li>\n<li><strong>–ó–∞–¥–∞—á–∏ –∏–∑ –¥—Ä—É–≥–æ–π –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏:</strong><ul>\n<li><strong>GaoKao (–∫–∏—Ç–∞–π—Å–∫–∏–π –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω –≤ –∫–æ–ª–ª–µ–¥–∂) En 2023:</strong> –∑–∞–¥–∞—á–∏ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω–æ–≥–æ —ç–∫–∑–∞–º–µ–Ω–∞.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞</strong></p>\n<p>rStar-Math —è–≤–ª—è–µ—Ç—Å—è –æ–±—â–∏–º –ø–æ–¥—Ö–æ–¥–æ–º, –ø—Ä–∏–º–µ–Ω–∏–º—ã–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM). –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–∞–ª—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (SLM) —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤:</p>\n<ul>\n<li>Qwen2.5-Math-1.5B</li>\n<li>Phi3-mini-Instruct (3B) - –æ–±—â–∞—è –º–æ–¥–µ–ª—å –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li>Qwen2-Math-7B</li>\n<li>Qwen2.5-Math-7B</li>\n</ul>\n<p>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ 4 —Ä–∞—É–Ω–¥–∞ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2.5-Math-7B, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–≥–æ –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ 4 —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–≤—à–∏—Ö –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (policy SLM) –∏ 4 –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è (PPM). –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö Qwen2.5-Math-7B –Ω–∞ 4 —Ä–∞—É–Ω–¥–µ. –ò—Ç–æ–≥–æ–≤–∞—è PPM –∏–∑ —ç—Ç–æ–≥–æ —Ä–∞—É–Ω–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏ –ø–æ–æ—â—Ä–µ–Ω–∏—è –¥–ª—è —ç—Ç–∏—Ö —Ç—Ä–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏.</p>\n<p><strong>–ë–∞–∑–æ–≤—ã–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è</strong></p>\n<p>rStar-Math –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ—Ç–æ–¥–∞–º \"–°–∏—Å—Ç–µ–º—ã 2\" (—Ç—Ä–µ–±—É—é—â–∏–º –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è). –û–Ω —Å—Ä–∞–≤–Ω–∏–≤–∞–ª—Å—è —Å —Ç—Ä–µ–º—è —Å–∏–ª—å–Ω—ã–º–∏ –±–∞–∑–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–º–∏ –∫–∞–∫ \"–°–∏—Å—Ç–µ–º—É 1\" (–±—ã—Å—Ç—Ä–æ–µ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ), —Ç–∞–∫ –∏ \"–°–∏—Å—Ç–µ–º—É 2\":</p>\n<ul>\n<li><strong>–ü–µ—Ä–µ–¥–æ–≤—ã–µ LLM:</strong> GPT-4o, Claude, OpenAI o1-preview –∏ o1-mini.</li>\n<li><strong>–û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:</strong> DeepSeek-Coder-v2-Instruct, Mathstral, NuminaMath-72B –∏ LLaMA3.1.</li>\n<li><strong>–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥–∞–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤:</strong> –≤–∫–ª—é—á–∞—è –≤–µ—Ä—Å–∏–∏ Instruct (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-7B-Instruct) –∏ Best-of-N (–Ω–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5-Math-72B-Instruct+Qwen2.5-Math-RM-72B).</li>\n</ul>\n<p><strong>–ú–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏</strong></p>\n<p>–î–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–µ—Ç—Ä–∏–∫–∞ Pass@1 (–¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏). –î–ª—è –º–æ–¥–µ–ª–µ–π \"–°–∏—Å—Ç–µ–º—ã 2\" –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≤—Ä–µ–º—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –¥–ª—è o1-mini –∏ o1-preview. –î–ª—è –º–æ–¥–µ–ª–µ–π Qwen Best-of-N –±—ã–ª–∏ –ø–µ—Ä–µ–æ—Ü–µ–Ω–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ MATH-500 –∏ AIME/AMC. –î–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è rStar-Math –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Ç–∞–∫–æ–µ –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π, –∫–∞–∫ –∏ Qwen. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –¥–ª—è AIME/AMC –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å 16 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ - 8. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å PPM –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –¢–∞–∫–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏), –æ–±–æ–∑–Ω–∞—á–µ–Ω–Ω–∞—è –∫–∞–∫ rStar-Math64.</p>\n<p><strong>4.2 –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</strong></p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ 5 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã rStar-Math –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø–µ—Ä–µ–¥–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ SLM</strong>, –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–π –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π OpenAI o1, –ø—Ä–∏ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –º–µ–Ω—å—à–µ–º —Ä–∞–∑–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ (1.5B-7B). –ù–∞–ø—Ä–∏–º–µ—Ä, Qwen2.5Math-7B, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–≤—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å 58.8% –Ω–∞ MATH, —É–ª—É—á—à–∏–ª–∞—Å—å –¥–æ 90.0% —Å rStar-Math, –ø—Ä–µ–≤–∑–æ–π–¥—è o1-preview –∏ Claude 3.5 Sonnet –∏ –¥–æ—Å—Ç–∏–≥–Ω—É–≤ —É—Ä–æ–≤–Ω—è o1-mini. –ù–∞ College Math rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç o1-mini –Ω–∞ 2.7%. –ù–∞ AIME 2024 rStar-Math –Ω–∞–±—Ä–∞–ª 53.3%, —É—Å—Ç—É–ø–∏–≤ —Ç–æ–ª—å–∫–æ o1-mini.</li>\n<li><strong>rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç \"–°–∏—Å—Ç–µ–º—É 2\"</strong>, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ–Ω—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ (1.5B-7B) –∏ –º–æ–¥–µ–ª–µ–π –ø–æ–æ—â—Ä–µ–Ω–∏—è (7B). –ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen Best-of-N, rStar-Math –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤—Å–µ—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–∞–∂–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Best-of-N —Å 72B –º–æ–¥–µ–ª—å—é –ø–æ–ª–∏—Ç–∏–∫–∏ rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –µ–µ –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∫—Ä–æ–º–µ GSM8K.</li>\n<li><strong>rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –Ω–∞ –¥—Ä—É–≥–∏—Ö —Å–ª–æ–∂–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö</strong>, –≤–∫–ª—é—á–∞—è Olympiad Bench, College Math –∏ Gaokao, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—è –Ω–æ–≤—ã–µ —Ä–µ–∫–æ—Ä–¥—ã.</li>\n</ol>\n<p><strong>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</strong></p>\n<p>rStar-Math –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MCTS –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ–ª–∏—Ç–∏–∫–∏, –æ—Å—É—â–µ—Å—Ç–≤–ª—è—è –ø–æ–∏—Å–∫ —Ä–µ—à–µ–Ω–∏–π –ø–æ–¥ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–º PPM. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —É–ª—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ –≤–ª–∏—è–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:</p>\n<ol>\n<li><strong>–£–∂–µ —Å 4 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è–º–∏ rStar-Math –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç Best-of-N</strong>, –ø—Ä–∏–±–ª–∏–∂–∞—è—Å—å –∫ o1-mini.</li>\n<li><strong>–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤–æ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π</strong> –Ω–∞ –≤—Å–µ—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ö–æ—Ç—è –∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–ø–∞–º–∏. –ù–∞ Math, AIME –∏ Olympiad Bench rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—Å—ã—â–µ–Ω–∏–µ –∏–ª–∏ –º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–∏ 64 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞ College Math –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å—Å—è.</li>\n</ol>"
            },
            {
                "title": "Findings and Discussions",
                "content": "The emergence of intrinsic self-reflection capability. key breakthrough in OpenAI o1 is its intrinsic self-reflection capability. When the model makes an error, it recognizes the mistake and can self-correct with correct answer [Noam Brown and Lightman, 2024]. Yet it has consistently been found to be largely ineffective in open-sourced LLMs. The community has actively explored various approaches, including self-correction [Huang et al., 2023, Kumar et al., 2024], self-reflection [Renze and Guven, 2024, Shinn et al., 2024], to explicitly train or prompt LLMs to develop such capability. In our experiments, we unexpectedly observe that our MCTS-driven deep thinking exhibits selfreflection during problem-solving. As shown in Fig. 4, the model initially formalizes an equation using SymPy in the first three steps, which would lead to an incorrect answer (left branch). Interestingly, in the fourth step (right branch), the policy model recognizes the low quality of its earlier steps and refrains from continuing along the initial problem-solving path. Instead, it backtracks and resolves the problem using new, simpler approach, ultimately arriving at the correct answer. An additional example of self-correction is provided in AppendixA.2. Notably, no self-reflection training data or prompt was included, suggesting that advanced System 2 reasoning can foster intrinsic self-reflection. 11 Figure 4: An example of intrinsic self-reflection during rStar-Math deep thinking. Figure 5: Pass@1 accuracy of policy models and their accuracy after applying System 2 reasoning with various reward models, shows that reward models primarily determine the final performance. PPM shapes the reasoning boundary in System 2 deep thinking. Both the policy and reward models are crucial for System 2 deep reasoning. Our experiments show that once the policy model attains reasonably strong capability level, (see Appendix A.1 ), the PPM becomes the key determinant of the upper performance limit. Fig. 5 summarizes the accuracy of policy models of different sizes, as well as the improvements achieved with reward models. Despite variations in Pass@1 accuracy due to differences in training strategies, datasets, and model scales, the reward model proves to be the dominant factor in System 2 reasoning. For instance, although the SFT accuracy of rStar-Math-7B is lower than Qwen2.5-Math-72B-Instruct, pairing it with our 7B PPM allows rStar-Math to outperform the 72B policy model with Qwen 72B ORM. Moreover, despite varying Pass@1 accuracy across our three policy SLM sizes, the final reasoning accuracy converges after applying the PPM. PPM spots theorem-application steps. When solving challenging math problems, identifying and applying relevant theorems or key conclusions often form the cornerstone of successful problemsolving [Xin et al., 2024]. In our experiments, we find that during rStar-Math problem-solving, our PPM effectively identifies critical theorem-application intermediate steps within policy models deep thinking process. These steps are predicted with high reward scores, guiding the policy model to generate the correct solution. Appendix A.2 provides examples where the PPM successfully identifies key theorems such as Fermats little theorem [Weisstein, a], Vietas formulas [Weisstein, b], the AM-GM inequality [amg], the Pythagorean theorem [pyt], and the Shoelace Theorem [sho], etc. 12 Generalization discussions. rStar-Math offers general methodology for improving LLM reasoning applicable to various domains. First, rStar-Math can generalize to more challenging math tasks, such as theorem proving, though its current focus is on word problems due to dataset limitations. Nonetheless, rStar-Math demonstrates the potential to prove mathematical statements. As shown in Appendix A.2, it successfully proves an Olympiad-level problem involving Fermats Little Theorem, providing step-by-step correct proof through its deep reasoning process. Second, rStar-Mathcan generalize to other domains, such as code and commonsense reasoning. Notably, synthesizing stepby-step verified training trajectories for general reasoning requires mechanism to provide feedback on whether given trajectory reaches the desired output at the end of MCTS rollout. For instance, in code reasoning, this could involve designing extensive test cases; in general reasoning, feedback could be obtained through human labeling or mutual verification with another LLM [Qi et al., 2024].",
                "summary": "<p><strong>–í–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏</strong></p>\n<p>–ö–ª—é—á–µ–≤—ã–º –ø—Ä–æ—Ä—ã–≤–æ–º –≤ –º–æ–¥–µ–ª–∏ OpenAI o1 —è–≤–ª—è–µ—Ç—Å—è –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–æ–ø—É—Å–∫–∞–µ—Ç –æ—à–∏–±–∫—É, –æ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –µ—ë –∏ –º–æ–∂–µ—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –≤—ã–¥–∞–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –û–¥–Ω–∞–∫–æ, –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö LLM (–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö) —ç—Ç–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–∫–∞–∑–∞–ª–∞—Å—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π. –°–æ–æ–±—â–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–æ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã, –≤–∫–ª—é—á–∞—è —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—é –∏ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∏–ª–∏ –ø–æ–±—É–¥–∏—Ç—å LLM –∫ —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ç–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.</p>\n<p>–í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ, –±—ã–ª–æ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≥–ª—É–±–æ–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–µ MCTS (–º–µ—Ç–æ–¥ –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–µ—Ä–µ–≤–∞), –ø—Ä–æ—è–≤–ª—è–µ—Ç —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—é –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á. –ù–∞ –ø—Ä–∏–º–µ—Ä–µ, –ø–æ–∫–∞–∑–∞–Ω–Ω–æ–º –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 4, –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Ñ–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É—è SymPy, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–≤–µ—Ä–Ω–æ–º—É –æ—Ç–≤–µ—Ç—É (–ª–µ–≤–∞—è –≤–µ—Ç–≤—å). –û–¥–Ω–∞–∫–æ, –Ω–∞ —á–µ—Ç–≤—ë—Ä—Ç–æ–º —à–∞–≥–µ (–ø—Ä–∞–≤–∞—è –≤–µ—Ç–≤—å), –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤–æ–∏—Ö –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤ –∏ –æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è –ø–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–º—É –ø—É—Ç–∏. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ, –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –Ω–∞–∑–∞–¥ –∏ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–æ–≤—ã–π, –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥, –≤ –∏—Ç–æ–≥–µ –ø–æ–ª—É—á–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥—ë–Ω –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –Ω–∏–∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏–ª–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –°–∏—Å—Ç–µ–º—ã 2 –º–æ–∂–µ—Ç —Å–ø–æ—Å–æ–±—Å—Ç–≤–æ–≤–∞—Ç—å –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—é –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏.</p>\n<p><strong>–†–æ–ª—å –º–æ–¥–µ–ª–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏</strong></p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–∞–∫ –∏ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏–≥—Ä–∞—é—Ç —Ä–µ—à–∞—é—â—É—é —Ä–æ–ª—å –≤ –≥–ª—É–±–æ–∫–æ–º –º—ã—à–ª–µ–Ω–∏–∏ –°–∏—Å—Ç–µ–º—ã 2. –û–¥–Ω–∞–∫–æ, –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–º–µ–Ω–Ω–æ –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è (PPM) —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–∏–º –≤–µ—Ä—Ö–Ω–∏–π –ø—Ä–µ–¥–µ–ª –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–∏—Å—É–Ω–æ–∫ 5 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤, –∞ —Ç–∞–∫–∂–µ —É–ª—É—á—à–µ–Ω–∏—è, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–µ —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏) –∏–∑-–∑–∞ —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è—Ö –æ–±—É—á–µ–Ω–∏—è, –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—Å—à—Ç–∞–±–∞—Ö –º–æ–¥–µ–ª–µ–π, –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–º —Ñ–∞–∫—Ç–æ—Ä–æ–º –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –°–∏—Å—Ç–µ–º—ã 2. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ö–æ—Ç—è —Ç–æ—á–Ω–æ—Å—Ç—å SFT (–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º) –º–æ–¥–µ–ª–∏ rStar-Math-7B –Ω–∏–∂–µ, —á–µ–º —É Qwen2.5-Math-72B-Instruct, –≤ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ —Å PPM 7B, rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ 72B —Å Qwen 72B ORM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏ Pass@1 –º–µ–∂–¥—É —Ç—Ä–µ–º—è —Ä–∞–∑–º–µ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Å—Ö–æ–¥–∏—Ç—Å—è –ø–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è PPM.</p>\n<p><strong>PPM –≤—ã—è–≤–ª—è–µ—Ç —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º</strong></p>\n<p>–ü—Ä–∏ —Ä–µ—à–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, –≤—ã—è–≤–ª–µ–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–æ—Ä–µ–º –∏–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–æ–≤ —á–∞—Å—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–Ω–æ–≤–æ–π —É—Å–ø–µ—à–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ –≤–æ –≤—Ä–µ–º—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á rStar-Math, PPM —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –≤–∞–∂–Ω—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ–ª–∏—Ç–∏–∫–∏. –≠—Ç–∏ —à–∞–≥–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –Ω–∞–ø—Ä–∞–≤–ª—è—è –º–æ–¥–µ–ª—å –ø–æ–ª–∏—Ç–∏–∫–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è. –í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2 –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ PPM —É—Å–ø–µ—à–Ω–æ –≤—ã—è–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–æ—Ä–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞–ª–∞—è —Ç–µ–æ—Ä–µ–º–∞ –§–µ—Ä–º–∞, —Ñ–æ—Ä–º—É–ª—ã –í–∏–µ—Ç–∞, –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–æ –ö–æ—à–∏, —Ç–µ–æ—Ä–µ–º–∞ –ü–∏—Ñ–∞–≥–æ—Ä–∞ –∏ —Ñ–æ—Ä–º—É–ª–∞ –ø–ª–æ—â–∞–¥–∏ –ì–∞—É—Å—Å–∞.</p>\n<p><strong>–û–±—Å—É–∂–¥–µ–Ω–∏–µ –æ–±–æ–±—â–µ–Ω–∏—è</strong></p>\n<p>rStar-Math –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–±—â—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π LLM, –ø—Ä–∏–º–µ–Ω–∏–º—É—é –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ–±–ª–∞—Å—Ç—è–º. –í–æ-–ø–µ—Ä–≤—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —Ç–µ–æ—Ä–µ–º, —Ö–æ—Ç—è –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, rStar-Math –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π. –ö–∞–∫ –ø–æ–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.2, –æ–Ω —É—Å–ø–µ—à–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É —É—Ä–æ–≤–Ω—è –æ–ª–∏–º–ø–∏–∞–¥—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞–ª–æ–π —Ç–µ–æ—Ä–µ–º—ã –§–µ—Ä–º–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –ø–æ—à–∞–≥–æ–≤–æ–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–ª—É–±–æ–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í–æ-–≤—Ç–æ—Ä—ã—Ö, rStar-Math –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–∏–µ –æ–±–ª–∞—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫–æ–¥ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞. –ü—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ —Å–∏–Ω—Ç–µ–∑ –ø–æ—à–∞–≥–æ–≤–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π —Ç—Ä–µ–±—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ —Ç–æ–º, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –ª–∏ –¥–∞–Ω–Ω–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–æ–Ω—Ü–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è MCTS. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ –∫–æ–¥–µ —ç—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤; –≤ –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–ª–∏ –≤–∑–∞–∏–º–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –¥—Ä—É–≥–æ–π LLM.</p>"
            },
            {
                "title": "Conclusion",
                "content": "In this work, we present rStar-Math, self-evolved System 2 deep thinking approach that significantly boosts the math reasoning capabilities of small LLMs, achieving state-of-the-art OpenAI o1-level performance. Our approach demonstrates that SLMs can self-generate high-quality training data for frontier-level math reasoning. Extensive experiments across four different-sized SLMs and challenging math benchmarks demonstrate the superiority of rStar-Math, with achieving leading results while outperforming existing math reasoning LLMs and Best-of-N baselines. We also reveal key findings, including the emergence of self-reflection and the effectiveness of the PPM in identifying critical intermediate steps, such as theorem-application steps. Finally, rStar-Math can achieve further improvements by collecting more challenging math problems, we leave this as future work.",
                "summary": "<p>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –º–µ—Ç–æ–¥ rStar-Math, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è –ø–æ–¥—Ö–æ–¥ \"–°–∏—Å—Ç–µ–º—ã 2\" –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —É –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (SLM), –¥–æ—Å—Ç–∏–≥–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ OpenAI o1, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–¥–æ–≤—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.</p>\n<p>–ù–∞—à –ø–æ–¥—Ö–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ SLM –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –ø–µ—Ä–µ–¥–æ–≤–æ–º —É—Ä–æ–≤–Ω–µ. –û–±—à–∏—Ä–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —á–µ—Ç—ã—Ä—å–º—è SLM —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Å–ª–æ–∂–Ω—ã–º–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ—Å—Ç–∞–º–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ rStar-Math. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –±–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ \"Best-of-N\".</p>\n<p>–¢–∞–∫–∂–µ –º—ã –≤—ã—è–≤–∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤–∫–ª—é—á–∞—è –ø–æ—è–≤–ª–µ–Ω–∏–µ —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å PPM (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ, Post-Processing Method) –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —à–∞–≥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —à–∞–≥–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ç–µ–æ—Ä–µ–º. –ù–∞–∫–æ–Ω–µ—Ü, rStar-Math –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏—á—å –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π –∑–∞ —Å—á–µ—Ç —Å–±–æ—Ä–∞ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –º—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ —Å—É—Ç–∏ —Å—Ç–∞—Ç—å–∏:</strong></p>\n<ul>\n<li><strong>–°–∏—Å—Ç–µ–º–∞ 2 –º—ã—à–ª–µ–Ω–∏—è:</strong> –≠—Ç–æ –æ—Ç—Å—ã–ª–∫–∞ –∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏, –≥–¥–µ \"–°–∏—Å—Ç–µ–º–∞ 1\" - —ç—Ç–æ –±—ã—Å—Ç—Ä–æ–µ, –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –º—ã—à–ª–µ–Ω–∏–µ, –∞ \"–°–∏—Å—Ç–µ–º–∞ 2\" - –º–µ–¥–ª–µ–Ω–Ω–æ–µ, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ. rStar-Math, –≤–∏–¥–∏–º–æ, –∏–º–∏—Ç–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ –æ–±–¥—É–º–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.</li>\n<li><strong>SLM (Small Language Models):</strong>  –†–µ—á—å –∏–¥–µ—Ç –æ –Ω–µ–±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —É—Å—Ç—É–ø–∞—é—Ç –±–æ–ª—å—à–∏–º –º–æ–¥–µ–ª—è–º –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–æ rStar-Math –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –¥–æ—Å—Ç–∏–≥–∞—Ç—å –≤—ã—Å–æ–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ.</li>\n<li><strong>OpenAI o1-level performance:</strong> –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–µ—Ç–æ–¥ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ —Å –æ–¥–Ω–æ–π –∏–∑ –º–æ–¥–µ–ª–µ–π OpenAI, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —ç—Ç–∞–ª–æ–Ω–æ–º –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò.</li>\n<li><strong>–°–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏—è:</strong>  –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –º–æ–º–µ–Ω—Ç, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —É–ª—É—á—à–∞—Ç—å –∏—Ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.</li>\n<li><strong>PPM:</strong> –ú–µ—Ç–æ–¥ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã—è–≤–ª—è—Ç—å –∫–ª—é—á–µ–≤—ã–µ —à–∞–≥–∏ –≤ —Ä–µ—à–µ–Ω–∏–∏.</li>\n<li><strong>Best-of-N baselines:</strong>  –ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –∏–∑ N —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤. rStar-Math –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —ç—Ç–æ—Ç –º–µ—Ç–æ–¥.</li>\n</ul>"
            }
        ]
    },
    {
        "id": "2501.01880",
        "title": "Long Context vs. RAG for LLMs: An Evaluation and Revisits",
        "url": "https://arxiv.org/pdf/2501.01880",
        "abstract": "Extending context windows (i.e., Long Context, LC) and using retrievers to\nselectively access relevant information (i.e., Retrieval-Augmented Generation,\nRAG) are the two main strategies to enable LLMs to incorporate extremely long\nexternal contexts. This paper revisits recent studies on this topic,\nhighlighting their key insights and discrepancies. We then provide a more\ncomprehensive evaluation by filtering out questions answerable without external\ncontext, identifying the most effective retrieval methods, and expanding the\ndatasets. We show that LC generally outperforms RAG in question-answering\nbenchmarks, especially for Wikipedia-based questions. Summarization-based\nretrieval performs comparably to LC, while chunk-based retrieval lags behind.\nHowever, RAG has advantages in dialogue-based and general question queries.\nThese insights underscore the trade-offs between RAG and LC strategies,\noffering guidance for future optimization of LLMs with external knowledge\nsources. We also provide an in-depth discussion on this topic, highlighting the\noverlooked importance of context relevance in existing studies.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-27",
        "pub_date_card": {
            "ru": "27 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 27",
            "zh": "12Êúà27Êó•"
        },
        "hash": "b641dbe6c9dd585f",
        "authors": [
            "Xinze Li",
            "Yixin Cao",
            "Yubo Ma",
            "Aixin Sun"
        ],
        "affiliations": [
            "S-Lab, Nanyang Technological University",
            "School of Computer Science, Fudan University"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2501.01880.jpg",
        "data": {
            "categories": [
                "#rag",
                "#optimization",
                "#long_context",
                "#benchmark"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "–ë–∏—Ç–≤–∞ –≥–∏–≥–∞–Ω—Ç–æ–≤: LC vs RAG –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π",
                "desc": "–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ (LC) –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤ –¥–ª—è –≤—ã–±–æ—Ä–æ—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –æ—Ü–µ–Ω–∫—É —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö, —Ñ–∏–ª—å—Ç—Ä—É—è –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LC –≤ —Ü–µ–ª–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç RAG –≤ –∑–∞–¥–∞—á–∞—Ö –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –í–∏–∫–∏–ø–µ–¥–∏–∏. –û–¥–Ω–∞–∫–æ RAG –∏–º–µ–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö."
            },
            "en": {
                "title": "Balancing Long Context and Retrieval for Optimal LLM Performance",
                "desc": "This paper explores two main strategies for enhancing large language models (LLMs) with long external contexts: Long Context (LC) and Retrieval-Augmented Generation (RAG). It reviews recent research, clarifying key findings and differences between these approaches. The authors conduct a thorough evaluation, revealing that LC generally performs better than RAG in answering questions, particularly those based on Wikipedia. However, RAG shows strengths in dialogue and general queries, highlighting the need to balance these strategies for optimal performance in LLMs."
            },
            "zh": {
                "title": "Èïø‰∏ä‰∏ãÊñá‰∏éÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÁöÑÊùÉË°°ÂàÜÊûê",
                "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÊâ©Â±ï‰∏ä‰∏ãÊñáÁ™óÂè£ÔºàÈïø‰∏ä‰∏ãÊñáÔºåLCÔºâÂíå‰ΩøÁî®Ê£ÄÁ¥¢Âô®ÔºàÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºåRAGÔºâÊù•ÈÄâÊã©ÊÄßËÆøÈóÆÁõ∏ÂÖ≥‰ø°ÊÅØÁöÑ‰∏§Áßç‰∏ªË¶ÅÁ≠ñÁï•Ôºå‰ª•Â∏ÆÂä©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂ§ÑÁêÜÊûÅÈïøÁöÑÂ§ñÈÉ®‰∏ä‰∏ãÊñá„ÄÇÁ†îÁ©∂Ë°®ÊòéÔºåÂú®ÈóÆÁ≠îÂü∫ÂáÜÊµãËØï‰∏≠ÔºåÈïø‰∏ä‰∏ãÊñáÈÄöÂ∏∏‰ºò‰∫éÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºåÂ∞§ÂÖ∂ÊòØÂú®Âü∫‰∫éÁª¥Âü∫ÁôæÁßëÁöÑÈóÆÈ¢ò‰∏ä„ÄÇÊÄªÁªìÊÄßÊ£ÄÁ¥¢‰∏éÈïø‰∏ä‰∏ãÊñáÁöÑË°®Áé∞Áõ∏ÂΩìÔºåËÄåÂü∫‰∫éÂùóÁöÑÊ£ÄÁ¥¢ÂàôÁõ∏ÂØπËæÉÂº±„ÄÇÁÑ∂ËÄåÔºåÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÂú®ÂØπËØùÂíå‰∏ÄËà¨ÈóÆÈ¢òÊü•ËØ¢‰∏≠ÂÖ∑Êúâ‰ºòÂäøÔºåËøôÁ™ÅÊòæ‰∫Ü‰∏§ÁßçÁ≠ñÁï•‰πãÈó¥ÁöÑÊùÉË°°Ôºå‰∏∫Êú™Êù•‰ºòÂåñÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõ‰∫ÜÊåáÂØº„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Extending context windows (i.e., Long Context, LC) and using retrievers to selectively access relevant information (i.e., Retrieval-Augmented Generation, RAG) are the two main strategies to enable LLMs to incorporate extremely long external contexts. This paper revisits recent studies on this topic, highlighting their key insights and discrepancies. We then provide a more comprehensive evaluation by filtering out questions answerable without external context, identifying the most effective retrieval methods, and expanding the datasets. We show that LC generally outperforms RAG in question-answering benchmarks, especially for Wikipedia-based questions. Summarization-based retrieval performs comparably to LC, while chunk-based retrieval lags behind. However, RAG has advantages in dialogue-based and general question queries. These insights underscore the trade-offs between RAG and LC strategies, offering guidance for future optimization of LLMs with external knowledge sources. We also provide an in-depth discussion on this topic, highlighting the overlooked importance of context relevance in existing studies.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞, –ø–æ–∑–≤–æ–ª—è—é—â–∏—Ö –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–º –≤–Ω–µ—à–Ω–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ (Long Context, LC) –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (Retrieval-Augmented Generation, RAG).</p>\n<p><strong>–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ (LC)</strong> –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞ –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑, —á—Ç–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –¥–∞–µ—Ç –µ–π –¥–æ—Å—Ç—É–ø –∫–æ –≤—Å–µ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. <strong>–ú–µ—Ç–æ–¥ RAG</strong>, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Ç–æ–π —á–∞—Å—Ç–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ —Ç–µ–∫—É—â–µ–º—É –∑–∞–ø—Ä–æ—Å—É.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –∞–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ —ç—Ç–∏–º –¥–≤—É–º –ø–æ–¥—Ö–æ–¥–∞–º, –≤—ã–¥–µ–ª—è—é—Ç—Å—è –∏—Ö –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è. –ó–∞—Ç–µ–º –∞–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–æ–¥—è—Ç –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –æ—Ü–µ–Ω–∫—É, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—è –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ–ø—Ä–µ–¥–µ–ª—è—è –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä—è—è –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ <strong>–≤ –∑–∞–¥–∞—á–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã LC –æ–±—ã—á–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç RAG</strong>, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ –¥–µ–ª–æ –∫–∞—Å–∞–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ Wikipedia. <strong>–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏</strong> (summarization-based retrieval) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ —Å LC, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ <strong>–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤</strong> (chunk-based retrieval) –æ—Ç—Å—Ç–∞–µ—Ç. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, —É <strong>RAG –µ—Å—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –æ–±—â–∏—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö</strong>.</p>\n<p>–≠—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å—ã –º–µ–∂–¥—É —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ RAG –∏ LC –∏ –¥–∞—é—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±—É–¥—É—â–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–Ω–∞–Ω–∏–π. –í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç –≤–∞–∂–Ω–æ—Å—Ç—å <strong>—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</strong>, –∫–æ—Ç–æ—Ä–∞—è —á–∞—Å—Ç–æ —É–ø—É—Å–∫–∞–µ—Ç—Å—è –∏–∑ –≤–∏–¥—É –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö.</p>"
            },
            {
                "title": "Introduction",
                "content": "Large Language Models (LLMs) (Brown et al., 2020) have demonstrated strong zero/few-shot capabilities in open-ended question answering (Yang et al., 2019). However, they face challenges such as hallucinations (Shuster et al., 2021; Ji et al., 2023), lacking real-time information and domain-specific knowledge (Su et al., 2024; Zhang et al., 2024), among others. common solution is to enhance LLMs with external memory to provide reliable and up-to-date data sources. Yet, incorporating additional content is constrained by the limited context window of LLMs. To address this, two main approaches are adopted: (i) building models with long context windows to read in more information (LC) (Fei et al., 2024; Chen et al., 2023; Wang et al., 2024c), and (ii) employing retrievers to include text segments relevant to the query (RAG) (Jiang et al., 2023; Asai et al., 2024; Gao et al., 2023). As shown by the timeline in Figure 1a, there is clear trend toward developing models that handle longer context windows and combining LC with RAG methods. The chronological overview of related studies highlights an increasing focus on both LC and RAG since mid-2023, as evidenced by growing number of publications aimed at optimizing the efficient retrieval, and utilization of long contexts. The development of models supporting longer context windows underscores the growing importance of handling extensive inputs effectively. Despite the broad consensus regarding the importance of LC and RAG, there remain disagreements and contradictory insights from different studies, summarized in Table 1. For example, while several studies agree on the effectiveness of combining LC and RAG (Xu et al., 2024b; Jiang et al., 2024b), others suggest that combining may not be beneficial (Bai et al., 2024a; Jin et al., 2024). Moreover, conflicting conclusions are reported regarding the benefits of RAG versus LC. Some papers find RAG advantageous in certain contexts (Xu et al., 2024a; Yu et al., 2024), while others highlight superior results from LC (Li et al., 2024; Xu et al., 2024b). These divergent insights showcase the complexity and ongoing debates in the field, suggesting that optimal strategies may vary depending on specific model architectures and benchmark conditions. To explore the underlying reasons, we conduct an in-depth investigation into the conditions that lead to disagreements among existing studies. During this process, we also identify key aspects that may have been overlooked in earlier research. Specifically, we revisit the evaluation process and implement the following changes. First, we fil- (a) Related work on LC and RAG, each paper is labeled by char and one color. For instance, green and \"L\" represent \"LongRAG\". (b) Chronological progress of key LLMs from 2023 to 2024. We focus on the models that publications in 1a use. We underline the models that support context window length of 32K. (c) History of frequently used retrievers from the 1980s until 2024. We bold the retrievers that no existing publications in 1a uses. Figure 1: Chronological overview of the development of RAG and LC. The Sub-graphs respectively illustrate the timelines for (a) publications related to LC and RAG, (b) long-context models, and (c) retrievers. We label before each model and retriever with the char and color block representing the publication that uses it. ter out questions from existing datasets that can be correctly answered without external context, removing biases from the parametric knowledge of LLMs and focusing on questions requiring external knowledge. Second, we evaluate retrieval methods and baselines on smaller filtered dataset (1,000+ questions) from 12 QA datasets to identify the best retriever. Third, we expand the dataset size by approximately 10 times by collecting additional data from the original sources of the 12 datasets1. Lastly, we compare the answers produced by the two settings, i.e., LC and RAG, and conduct an in-depth analysis. Our results are based on the expanded dataset using the long-context setting and the best retrieval method identified earlier. Our key contributions in this paper are as follows: (i) Providing comprehensive survey of existing studies on LC and RAG, analyzing their implementations and key insights. (ii) Proposing fair and systematic evaluation framework, and performing detailed analyses to understand the strengths and limitations of LC and RAG. (iii) Discussing chal1The experiment code and expanded datasets are available at https://github.com/lixinze777/LC_VS_RAG lenges for comparing and combining LC and RAG, reflecting on the key points that researchers tend to overlook in this field. Evaluation results indicate that LC models generally outperform RAG when processing self-contained information like stories, while RAG excels at handling fragmented information, particularly in dialogue-based contexts. These experiments deepen our understanding of the strengths and limitations of LC and RAG, offering valuable insights into optimizing retrieval strategies and effectively integrating these approaches to enhance performance in open-domain question answering. These findings also based on systematic survey of existing studies on this topic (see 2). Additionally, we discuss key aspects of comparing LC and RAG in 6, highlighting areas that have been underexplored in prior research.",
                "summary": "<p>–ë–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –Ω–æ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏, —Ç–∞–∫–∏–º–∏ –∫–∞–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π. –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è LLM –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ LLM. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞: (i) —Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (LC) –∏ (ii) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –ø–æ–∏—Å–∫–∞ (retrievers) –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å—É (RAG). –ù–∞–±–ª—é–¥–∞–µ—Ç—Å—è —è–≤–Ω–∞—è —Ç–µ–Ω–¥–µ–Ω—Ü–∏—è –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –º–æ–¥–µ–ª–µ–π, —Å–ø–æ—Å–æ–±–Ω—ã—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ–∫–Ω–∞, –∞ —Ç–∞–∫–∂–µ –∫ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—é LC –∏ RAG.</p>\n<p>–•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Å —Å–µ—Ä–µ–¥–∏–Ω—ã 2023 –≥–æ–¥–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ä–∞—Å—Ç—É—â–∏–π –∏–Ω—Ç–µ—Ä–µ—Å –∫ LC –∏ RAG, –æ —á–µ–º —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–π, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª–µ–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ–∫–Ω–∞, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Ä–∞—Å—Ç—É—â—É—é –≤–∞–∂–Ω–æ—Å—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p>–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–±—â–µ–µ –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ LC –∏ RAG, —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è –∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –≤—ã–≤–æ–¥—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ LC –∏ RAG, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ. –¢–∞–∫–∂–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã –≤—ã–≤–æ–¥—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ RAG –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å LC. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ RAG –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ LC. –≠—Ç–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –≤—ã–≤–æ–¥—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–¥–æ–ª–∂–∞—é—â–∏–µ—Å—è –¥–µ–±–∞—Ç—ã –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–æ–≥—É—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –º–æ–¥–µ–ª–µ–π –∏ —É—Å–ª–æ–≤–∏–π —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.</p>\n<p>–î–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω —ç—Ç–∏—Ö —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏–π, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ–≤–æ–¥—è—Ç —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —É—Å–ª–æ–≤–∏–π, –ø—Ä–∏–≤–æ–¥—è—â–∏—Ö –∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è–º –º–µ–∂–¥—É —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª—è—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã—Ç—å —É–ø—É—â–µ–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –∞–≤—Ç–æ—Ä—ã –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ—Ü–µ–Ω–∫–∏ –∏ –≤–Ω–æ—Å—è—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n1.  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —Å–º–µ—â–µ–Ω–∏—è, –≤—ã–∑–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ LLM, –∏ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π.\n2.  –û—Ü–µ–Ω–∫–∞ –º–µ—Ç–æ–¥–æ–≤ –ø–æ–∏—Å–∫–∞ –∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö (1000+ –≤–æ–ø—Ä–æ—Å–æ–≤) –∏–∑ 12 –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö QA –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª—É—á—à–µ–≥–æ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞.\n3.  –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 10 —Ä–∞–∑ –ø—É—Ç–µ–º —Å–±–æ—Ä–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ 12 –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö.\n4.  –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤ –¥–≤—É—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö, —Ç.–µ. LC –∏ RAG, –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ª—É—á—à–µ–≥–æ –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–µ–µ.</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã:\n1.  –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –æ–±–∑–æ—Ä–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ LC –∏ RAG, –∞–Ω–∞–ª–∏–∑ –∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π –∏ –∫–ª—é—á–µ–≤—ã—Ö –≤—ã–≤–æ–¥–æ–≤.\n2.  –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–π –∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ü–µ–Ω–∫–∏ –∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω LC –∏ RAG.\n3.  –û–±—Å—É–∂–¥–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è LC –∏ RAG, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —á–∞—Å—Ç–æ —É–ø—É—Å–∫–∞—é—Ç –∏–∑ –≤–∏–¥—É –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª–∏ LC –æ–±—ã—á–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç RAG –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–∞–∫–æ–π –∫–∞–∫ –∏—Å—Ç–æ—Ä–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ RAG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∏–∞–ª–æ–≥–æ–≤. –≠—Ç–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —É–≥–ª—É–±–ª—è—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Å—Ç–æ—Ä–æ–Ω LC –∏ RAG, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ü–µ–Ω–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –¥–æ—Å—Ç—É–ø–µ. –≠—Ç–∏ –≤—ã–≤–æ–¥—ã —Ç–∞–∫–∂–µ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –æ–±–∑–æ—Ä–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ–±—Å—É–∂–¥–∞—é—Ç—Å—è –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è LC –∏ RAG, –≤—ã–¥–µ–ª—è—è –æ–±–ª–∞—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑—É—á–µ–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö.</p>"
            },
            {
                "title": "Related work",
                "content": "Our primary focus is to evaluate and compare LC and RAG. To this end, we review papers with similar focus, and provide detailed analysis of the retrievers and long-context settings they employ. 2.1 Retrievers Retrievers, as fundamental components of RAG pipelines, focus on identifying and extracting contextually relevant segments of documents. We categorize retrieval strategies into three main approaches: chunk-based retrieval, which splits documents into smaller segments and then retrieves those most relevant to query; index-based retrieval, which builds specialized index structures to guide efficient and context-rich lookups; and summarization-based retrieval, which leverages hierarchical summaries to capture documents key information at various levels of abstraction. Chunk-based Retrieval can be broadly categorized into sparse retrievers and dense retrievers. Sparse retrievers, such as the classic BM25 (Robertson and Zaragoza, 2009), operate on term frequency-based representations of text and rank chunks based on similarity function, leveraging exact matches and term weighting. With the advent of word embeddings, dense retrievers have gained prominence. These models encode both queries and document chunks into dense vector representations and calculate relevance using similarity metrics, such as cosine similarity. Since text similarity is often defined by measuring the distance between embeddings, the quality of these embeddings is particularly important. Contriever (Izacard et al., 2022) leverages contrastive learning for training without supervision. By generating synthetic queries and pre-training on unlabeled data, Contriever provides robust retrieval capabilities especially in cross-lingual applications. On larger scale, BGE-Large (Xiao et al., 2023) employs diverse datasets and sophisticated training methods to outperform previous models on comprehensive benchmarks such as C-MTEB. E5Mistral7b (Wang et al., 2024b) combines open-source, decoder-only LLMs with synthetic data generation pipelines. With minimal human annotations, the fine-tuning achieves SOTA performance on BEIR and MTEB. Dragon (Lin et al., 2023) also employs data augmentation, including cropping and generative queries, and integrates labels from multiple retrieval sources. This strategy ensures its effectiveness without increasing model complexity. Another method of learning high-quality embeddings is through strong generalization ability from LLMs. For instance, OpenAI embeddings draw upon the GPT-3.5/4 family while Zhipu-embedding-3 leverages the GLM family (Zeng et al., 2024). Index-based Retrieval requires pre-processing on the documents with more complicated data structures (Gupta et al., 2018). With the development of LLM, Llama-Index (Liu, 2022) was proposed to facilitate interaction between the model and documents more conveniently. The index provides flexible interface to construct various data structures, known as indices that store, organize, and facilitate quick retrieval of context. Once created, these indices can be efficiently queried, guiding the LLM to the most relevant information, improving the accuracy of responses. Some classic indexing methods include tree index which constructs hierarchical tree from nodes, and knowledge graph index, which builds knowledge graph with labeled nodes and relationships. Summarization-based Retrieval is built on top of chunkand index-based approaches. It provides comprehensive summaries for key points in document. These summaries available for retrieval. RAPTOR (Sarthi et al., 2024) improves retrieval by generating recursive summaries of text chunks organized in tree structure. Instead of retrieving short, contiguous text snippets, RAPTOR clusters text segments, summarizes them at various levels, and forms hierarchical tree that represents the documents content at different levels of abstraction. This allows retrieval models to extract context at varying levels of detail, improving the ability to handle complex questions that require synthesizing information from multiple parts of the document. Such summarization-based retrieval method enhances retrieval accuracy for tasks requiring longrange or multi-step reasoning. 2.2 Long-Context LLMs Many research efforts focus on extending input and output windows to accommodate more context (see Figure 1b), enabling applications such as extended dialogues, large document processing, and complex multimodal tasks. Thus, our analysis focuses on two dimensions: the model capabilities and the context length they can reach. Model Ability. While most of the models discussed here excel at understanding long documents, many emphasize specialized capabilities. ChatGLM2-6B-32K (Zeng et al., 2024) employs Multi-Query Attention to achieve high reasoning efficiency with low memory usage, making it suitable for tasks requiring deep reasoning. XGen-7B-8K (Nijkamp et al., 2023) enhances long-context conversational understanding and text summarization, enabling coherent and contextually rich dialogues. InternLM-7B-8k (Cai et al., 2024) is optimized for knowledge understanding, reading comprehension, and multilingual translation, supporting diverse linguistic applications. Models like DeepSeek-V2-Chat (DeepSeekAI et al., 2024), Qwen2-72B-Instruct (Yang et al., 2024), Qwen2.5-72B-Instruct (Qwen et al., 2024), Mixtral-7x8b (Jiang et al., 2024a), and DBRXInstruct excel in mathematical computations, logical reasoning, and coding, demonstrating strong performance in technical and analytical tasks. Additionally, Claude-3-Opus, Sonnet, Haiku, Gemini-1.5-flash, and Gemini-1.5-pro (Reid et al., 2024) incorporate multi-modal capabilities, effectively handling both textual and visual information. GLM-4-9B-Chat (Zeng et al., 2024), Mistral12b-Instruct, and Llama-3.1-Instruct (Dubey et al., 2024) offer robust multilingual abilities, strong instruction-following and multi-turn dialogue capabilities, increasing their utility in wide range of conversational scenarios. Finally, Claude-2 is notable for low hallucination rate when processing extra-long documents, ensuring high accuracy and reliability in information retrieval and synthesis. Context Length. As shown in Figure 1b, there is clear trend of increasing context length in newly released models. Following the categorization approach proposed by ChatQA2 (Xu et al., 2024a), we classify these models into three categories based on their supported context windows: short (up to 4K), long (up to 32K), and ultra-long (more than 32K) context models. Short context models, such as Llama2-70B and llama2-7B-chat-4k (Touvron et al., 2023), support up to 4K tokens and are typically employed as baselines for retrieval and standard conversational tasks. Long context models, including XGen-7B8K(Nijkamp et al., 2023), InternLM-7B-8k(Cai et al., 2024), Mixtral-7x8b (Jiang et al., 2024a), DBRX-Instruct and Gemma2-9B (Mesnard et al., 2024), offer context windows ranging from 8K to 32K tokens. These are ideal for extended conversations, comprehensive text analysis, and detailed summarization tasks. Ultra-long context models extend beyond 32K tokens. For example, Claude-2 provides 100K token window, while Claude-3-Opus, Sonnet, and Haiku handle up to 200K tokens. GPT-4-Turbo(OpenAI et al., 2023), GPT-4o, and GPT-o1 all support 128K tokens, as do DeepSeek-V2-Chat(DeepSeek-AI et al., 2024), Qwen2-72B-Instruct(Yang et al., 2024), Qwen2.572B-Instruct (Qwen et al., 2024), GLM-4-9BChat (Zeng et al., 2024), GLM-4-Plus, Mistral-12bInstruct, and Llama-3.1-Instruct. Notably, Gemini1.5-flahs and Gemini-1.5-pro(Reid et al., 2024) both support up to an unprecedented 10M tokens. These ultra long-context models enable the processing of exceptionally large documents, complex multimodal tasks, and extensive multi-turn dialogues. 2.3 Comparing & Combining LC and RAG Since the increase in LLMs context window lengths, some models can contain the entire document, reducing the need to retrieve on documents. Hence, more studies have begun comparing the performance of long-context LLMs and RAG, as well as investigating ways to combine them. LongBench (Bai et al., 2024a) conducts early comparison experiments on 4K model with RAG and 32K model. Xu et al. (2024b) systematically compare LC LLMs and RAG, and proposes their combination. LongRAG (Jiang et al., 2024b) introduces long retrievers and long readers, successful application of long retrieval units to RAG. ChatQA2 (Xu et al., 2024a) instruction-tunes long-context LLMs to 128K context window and tests their ability with long-context retrievers. Self-ROUTE (Li et al., 2024) enables the model to select either RAG or LC based on self-reflection to reduce costs. OPRAG (Yu et al., 2024) preserves the original order of retrieved chunks, and LC LLM meets RAG (Jin et al., 2024) investigates long-context LLMs in RAG systems, proposing retrieval reordering methods. LC RAG Performance of LLM (Leng et al., 2024) evaluates the effectiveness of RAG on longcontext LLMs across context lengths from 2K to 2M tokens. Very recently, LongBench is updated to LongBench V2 (Bai et al., 2024b), which tests LLMs on long context comprehension and reasoning with more realistic and challenging setting. We summarize the key insights from these papers into three categories: (1) general insights such as chunking strategies, (2) combining the two strategies, and (3) comparing the performance between LC and RAG (see Table 1). Some papers reach consensus on chunking strategy that, retrieval units should be longer (Jiang et al., 2024b) and the number of chunks should be kept low (Yu et al., 2024). According to (Xu et al., 2024b), selecting the top 5 to 10 chunks typically yields strong performance, while retrieving Paper Type Findings LongBench (B) (Bai et al., 2024a) Ret-LC LLM (R) (Xu et al., 2024b) LongRAG (L) (Jiang et al., 2024b) ChatQA2 (C) (Xu et al., 2024a) Self-ROUTE (S) (Li et al., 2024) OP-RAG (O) (Yu et al., 2024) LC LLM-RAG (M) (Jin et al., 2024) LC RAG Performance (P) (Leng et al., 2024) LongBench v2 (V) (Bai et al., 2024b) + + + + + + Retrieval helps 4k model, but not 16k/32k models. Models benefit from continuous training on long contexts. Splitting context into shorter and more chunks is better. LC is better for multi-hop benchmarks than 4k RAG. RAG improves on 70B/43B models on all context lengths. For LC model, best results are obtained from top-5 or top-10. Retrieval benefits from long retrieval units. For sequence lengths up to 32K, RAG outperforms LC. From 3K to 24K, greater context window benefits RAG. LC consistently outperforms RAG, but RAG has lower cost. Efficient retrieval can outperform brute-force LC. Too many chunks in RAG harms performance. Preserving the original order is better than ordering by score. Retrieve more passages first improves performance then drops. Ordering higher score information to front and back helps. Most close models RAG improves up to 100k tokens. Most open models RAG peak at 16k-32k then performance drops. GPT-4o performs better at 128k without RAG. GPT-4o performance keeps increasing to 128k RAG context. Qwen2.5 & GLM-4-Plus drop with >32k RAG contexts. Table 1: Important findings from existing studies that compare or combine LC with RAG (label in brackets). We group the insights into three categories: 1) General strategies that improve performance marked by +. 2) Combining LC and RAG, where indicates combining is good, and for combining is not helpful, and 3) Comparing LC and RAG, where indicates RAG outperforms LC, and for LC outperforms RAG. more than 20 chunks leads to diminished results. LongBench (Bai et al., 2024a) presents different finding, suggesting that splitting long context into shorter and more numerous chunks is better. However, at the time of its publication, LLMs generally exhibited weaker long-context capabilities, and the study did not incorporate very long retrieval units (>1000 tokens). Consequently, LongBenchs findings are not at odds with the broader consensus. Nonetheless, these papers present disagreement regarding performance of retrieval on long-context LLMs. For instance, LongBench (Bai et al., 2024a) finds that retrieval helps short-context models but not 7B long-context models. In contrast, Xu et al. (2024b) suggest that RAG improves 70B models across all context lengths, attributing the discrepancy to the difference between model sizes. Similarly, ChatQA2 (Xu et al., 2024a) observes that increasing the context window from 3K to 24K tokens consistently benefits RAG. Notably, LongBench V2 (Bai et al., 2024b) shows that GPT-4o continues to improve in RAG performance even at 128K input, whereas Qwen2.5 and GLM-4-Plus show performance deterioration beyond 32K input. The observations align with findings from (Leng et al., 2024) that RAG for close-source models can improve up to 100K input, whereas performance for some open-source models peaks around 16K tokens. Hence, the varying behaviors might be due to different model size and architecture. There are even greater discrepancies in the direct comparisons between the two methods. Xu et al. (2024b) claims that long-context models outperform retrieval with short-context models in multihop benchmarks. In contrast, ChatQA2 (Xu et al., 2024a) finds that RAG can outperform LC if sufficient number of top-k chunks are used. SelfROUTE (Li et al., 2024) fully supports LC, arguing that it outperforms RAG in all benchmarks. Meanwhile, OP-RAG (Yu et al., 2024) defends RAG, demonstrating that efficient retrieval strategies can outperform brute-force approach of processing extremely long contexts. The reasons for the differences among these studies are manifold. For instance, There are three categories of retrieval methods (i.e., chunk-based, index-based, and summarization-based retrieval), but current studies rely predominantly on chunkbased retrieval, leaving room for further optimization. Additionally, evaluation scores often represent weighted averages across different datasets. Because each dataset has distinct characteristics, placing more emphasis on one dataset and less on another can alter the final results. Finally, most existing studies use only few datasets with around 200 questions each. This small sample size creates greater room for variability and reduces the general reliability of these findings.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã) –∏ –º–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LC LLM), –∞ —Ç–∞–∫–∂–µ –∏—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è.</p>\n<p><strong>–†–µ—Ç—Ä–∏–≤–µ—Ä—ã</strong> ‚Äî —ç—Ç–æ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG-–ø–∞–π–ø–ª–∞–π–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç–≤–µ—á–∞—é—Ç –∑–∞ –ø–æ–∏—Å–∫ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ò—Ö –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞:</p>\n<ol>\n<li><strong>–ß–∞–Ω–∫-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã:</strong><ul>\n<li><strong>–†–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã</strong> (–Ω–∞–ø—Ä–∏–º–µ—Ä, BM25) —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ —Ä–∞–Ω–∂–∏—Ä—É—é—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤.</li>\n<li><strong>–ü–ª–æ—Ç–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã</strong> –∫–æ–¥–∏—Ä—É—é—Ç –∑–∞–ø—Ä–æ—Å—ã –∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ø–ª–æ—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ –∏–∑–º–µ—Ä—è—é—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é –º–µ—Ç—Ä–∏–∫ —Å—Ö–æ–∂–µ—Å—Ç–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ. –ö–∞—á–µ—Å—Ç–≤–æ —ç—Ç–∏—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–º–µ–µ—Ç —Ä–µ—à–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ. –ü—Ä–∏–º–µ—Ä–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è Contriever, BGE-Large, E5Mistral7b –∏ Dragon. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ OpenAI embeddings –∏ Zhipu-embedding-3, –∏—Å–ø–æ–ª—å–∑—É—é—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.</li>\n</ul>\n</li>\n<li><strong>–ò–Ω–¥–µ–∫—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã:</strong> —Ç—Ä–µ–±—É—é—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä –¥–∞–Ω–Ω—ã—Ö. Llama-Index —É–ø—Ä–æ—â–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –≥–∏–±–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤. –ò–Ω–¥–µ–∫—Å—ã –æ—Ä–≥–∞–Ω–∏–∑—É—é—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏–º–µ—Ä—ã –≤–∫–ª—é—á–∞—é—Ç –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –∏ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–æ–≤ –∑–Ω–∞–Ω–∏–π.</li>\n<li><strong>–†–µ—Ç—Ä–∏–≤–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏:</strong> —Å—Ç—Ä–æ—è—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞–Ω–∫- –∏ –∏–Ω–¥–µ–∫—Å-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤. –û–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –∫—Ä–∞—Ç–∫–∏–µ –∏–∑–ª–æ–∂–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞. RAPTOR, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ–∑–¥–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∏–∑–ª–æ–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏, —É–ª—É—á—à–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.</li>\n</ol>\n<p><strong>–ú–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (LC LLM)</strong>: –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –æ–∫–æ–Ω, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã —Ç–µ–∫—Å—Ç–∞. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –ø–æ –¥–≤—É–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º:</p>\n<ol>\n<li><strong>–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏:</strong> –ú–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏, –ø–æ–º–∏–º–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –æ–±–ª–∞–¥–∞—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏. ChatGLM2-6B-32K –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –ø—Ä–∏ –Ω–∏–∑–∫–æ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏, XGen-7B-8K —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, InternLM-7B-8k –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ –ø–µ—Ä–µ–≤–æ–¥–∞. –ú–æ–¥–µ–ª–∏ DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, Mixtral-7x8b –∏ DBRX-Instruct –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏—è—Ö, –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏. Claude-3-Opus, Sonnet, Haiku, Gemini-1.5-flash –∏ Gemini-1.5-pro –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å. GLM-4-9B-Chat, Mistral-12b-Instruct –∏ Llama-3.1-Instruct –æ–±–ª–∞–¥–∞—é—Ç –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏. Claude-2 –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.</li>\n<li><strong>–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –ú–æ–¥–µ–ª–∏ –¥–µ–ª—è—Ç—Å—è –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:<ul>\n<li><strong>–ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç</strong> (–¥–æ 4K —Ç–æ–∫–µ–Ω–æ–≤), –Ω–∞–ø—Ä–∏–º–µ—Ä, Llama2-70B –∏ llama2-7B-chat-4k.</li>\n<li><strong>–î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç</strong> (–¥–æ 32K —Ç–æ–∫–µ–Ω–æ–≤), –Ω–∞–ø—Ä–∏–º–µ—Ä, XGen-7B8K, InternLM-7B-8k, Mixtral-7x8b, DBRX-Instruct –∏ Gemma2-9B.</li>\n<li><strong>–£–ª—å—Ç—Ä–∞-–¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç</strong> (–±–æ–ª–µ–µ 32K —Ç–æ–∫–µ–Ω–æ–≤), –Ω–∞–ø—Ä–∏–º–µ—Ä, Claude-2 (100K), Claude-3-Opus, Sonnet –∏ Haiku (200K), GPT-4-Turbo, GPT-4o, GPT-o1, DeepSeek-V2-Chat, Qwen2-72B-Instruct, Qwen2.5-72B-Instruct, GLM-4-9BChat, GLM-4-Plus, Mistral-12b-Instruct, Llama-3.1-Instruct (128K), Gemini-1.5-flahs –∏ Gemini-1.5-pro (10M).</li>\n</ul>\n</li>\n</ol>\n<p><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è LC –∏ RAG:</strong> –° —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ LLM, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤–º–µ—Å—Ç–∏—Ç—å –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –Ω–∞—á–∞–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LC LLM –∏ RAG, –∞ —Ç–∞–∫–∂–µ –∏–∑—É—á–∞—Ç—å —Å–ø–æ—Å–æ–±—ã –∏—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è.</p>\n<ul>\n<li>LongBench –ø—Ä–æ–≤–æ–¥–∏—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 4K –∏ 32K —Å RAG.</li>\n<li>Xu et al. (2024b) —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç LC LLM –∏ RAG –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏—é.</li>\n<li>LongRAG –≤–≤–æ–¥–∏—Ç –¥–ª–∏–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã –∏ —Ä–∏–¥–µ—Ä—ã.</li>\n<li>ChatQA2 –æ–±—É—á–∞–µ—Ç LC LLM –¥–æ 128K –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –∏—Ö —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞–º–∏.</li>\n<li>Self-ROUTE –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –≤—ã–±–∏—Ä–∞—Ç—å RAG –∏–ª–∏ LC –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∞–º–æ–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç.</li>\n<li>OPRAG —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.</li>\n<li>LC LLM meets RAG –∏—Å—Å–ª–µ–¥—É–µ—Ç LC LLM –≤ RAG-—Å–∏—Å—Ç–µ–º–∞—Ö –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –º–µ—Ç–æ–¥—ã –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.</li>\n<li>LC RAG Performance of LLM –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å RAG –Ω–∞ LC LLM —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç 2K –¥–æ 2M —Ç–æ–∫–µ–Ω–æ–≤.</li>\n<li>LongBench V2 —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç LLM –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.</li>\n</ul>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã:</p>\n<ol>\n<li><strong>–û–±—â–∏–µ –≤—ã–≤–æ–¥—ã:</strong><ul>\n<li>–§—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–ª–∏–Ω–Ω–µ–µ.</li>\n<li>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ–±–æ–ª—å—à–∏–º.</li>\n<li>–û–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã–±–∏—Ä–∞—Ç—å 5-10 –ª—É—á—à–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.</li>\n</ul>\n</li>\n<li><strong>–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π:</strong><ul>\n<li>–ò–∑—É—á–∞—é—Ç—Å—è —Å–ø–æ—Å–æ–±—ã –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è LC –∏ RAG –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.</li>\n</ul>\n</li>\n<li><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LC –∏ RAG:</strong><ul>\n<li>–ü—Ä–æ–≤–æ–¥–∏—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LC LLM –∏ RAG –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</li>\n</ul>\n</li>\n</ol>"
            },
            {
                "title": "Question filtering and expansion",
                "content": "To ensure fair and comprehensive comparison, we curate our evaluation dataset based on existing datasets, and apply necessary filtering ( 3.1) and augmentation ( 3.2). We select 12 long-context QA datasets frequently used in studies comparing LC and RAG: Natural Questions (Kwiatkowski et al., 2019), 2WikiMultihopQA (Ho et al., 2020), HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022), MultiFieldQA (Bai et al., 2024a), NarrativeQA (KoÀácisk√Ω et al., 2018), QASPER (Dasigi et al., 2021), QuALTY (Pang et al., 2022), Coursera, TOEFL-QA, and MultiDoc2Dial (An et al., 2024). We also include the NovelQA (Wang et al., 2024a) dataset, high-quality, human-annotated resource derived from long-form novels. We present an overview of these datasets in Table 2, including their type, context type (single-doc or multi-doc), context source, average context length, and representative studies that have utilized each dataset. 3.1 Question Filtering Given the strong capabilities of modern LLMs, many questions can be directly answered based on knowledge encoded in their parameters (Basmova et al., 2024), reducing the need for external context in some cases. However, certain queries, such as those related to private conversations, will always require additional context. To determine which approach more effectively enhances an LLMs performance with long documents, we filter the datasets to include only questions that the LLM cannot answer correctly without external context. This ensures that any correct answers obtained subsequently must rely on external knowledge rather than the models built-in knowledge. For our implementation, we use GPT-4o for question filtering due to its strong capabilities. We employ strict exact-match scoring metric to ensure that the model not only provides the correct answer but also demonstrates complete understanding of the required information. 3.2 Question (and Context) Expansion RAG and LC produce identical answers for about 60% of the questions in existing evaluations (Li et al., 2024), leaving relatively few questions to help us understand the differences between the two. To ensure robust statistical significance, we expand the dataset size to approximately 20,000 questions by collecting additional samples. To maintain similar distribution as the original datasets, we follow two principles during data collection. First, we collect questions only from the original source of each dataset, avoiding artificially generated or LLM-augmented questions. Second, we add distracting passages to the original context for each question to extend the context length, following the implementation described in LongBench. For NovelQA, we use all its available questions. For Coursera, MultiFieldQA, and MultiDoc2Dial datasets, we do not further enlarge their sizes to avoid introducing artificial data. Hereafter, we refer to the expanded dataset as the full question set and the original, pre-expansion dataset as the sample question set. 3.3 Dataset Statistics After expansion, we obtain 19,188 questions, of which 13,651 require context to be answered using the filtering method from 3.1, as listed in Table 3. Notably, questions grounded in factual knowledge, such as those from Coursera, show high removal rate. Similarly, questions drawn from well-known books or requiring multi-hop reasoning often exhibit higher likelihood of being directly answered by LLMs without context. Comparing the 12 individual datasets, we observe similar filtering rate between the sample and the full question sets (see Tables 2 and 3), indicating that both sets follow similar distribution.",
                "summary": "<p>–î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —á–µ—Å—Ç–Ω–æ–≥–æ –∏ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å–æ–∑–¥–∞–ª–∏ —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–º–µ–Ω—è—è –∫ –Ω–∏–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ.</p>\n<p><strong>3.1 –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤</strong></p>\n<p>–û–Ω–∏ –≤—ã–±—Ä–∞–ª–∏ 12 –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (LC) –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (RAG): Natural Questions, 2WikiMultihopQA, HotpotQA, MuSiQue, MultiFieldQA, NarrativeQA, QASPER, QuALTY, Coursera, TOEFL-QA –∏ MultiDoc2Dial, –∞ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏–ª–∏ NovelQA, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º –∏–∑ —Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–æ–º–∞–Ω–æ–≤. –í —Ç–∞–±–ª–∏—Ü–µ 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –æ–±–∑–æ—Ä —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∏—Ö —Ç–∏–ø, —Ç–∏–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–¥–Ω–æ–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–π –∏–ª–∏ –º–Ω–æ–≥–æ–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–π), –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å.</p>\n<p>–¢–∞–∫ –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –æ–±–ª–∞–¥–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏, –º–Ω–æ–≥–∏–µ –≤–æ–ø—Ä–æ—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ç–≤–µ—á–µ–Ω—ã –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π, –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤ –∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö, —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–æ –≤–Ω–µ—à–Ω–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –û–¥–Ω–∞–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ª–∏—á–Ω—ã–º–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞–º–∏, –≤—Å–µ–≥–¥–∞ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ß—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏, –∞–≤—Ç–æ—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∏–≤ —Ç–æ–ª—å–∫–æ —Ç–µ –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ LLM –Ω–µ –º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ª—é–±—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏, –±—É–¥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π, –∞ –Ω–µ –æ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏. –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å GPT-4o –∏–∑-–∑–∞ –µ–µ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–∏–º–µ–Ω—è–ª—Å—è —Å—Ç—Ä–æ–≥–∏–π –∫—Ä–∏—Ç–µ—Ä–∏–π —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ —Ç–æ–ª—å–∫–æ –¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–æ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ–º–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</p>\n<p><strong>3.2 –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ (–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)</strong></p>\n<p>–ú–µ—Ç–æ–¥—ã RAG –∏ LC –¥–∞—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ 60% –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ—Ü–µ–Ω–∫–∞—Ö, –æ—Å—Ç–∞–≤–ª—è—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–∞–ª–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É —ç—Ç–∏–º–∏ –¥–≤—É–º—è –ø–æ–¥—Ö–æ–¥–∞–º–∏. –ß—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –Ω–∞–¥–µ–∂–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å, –∞–≤—Ç–æ—Ä—ã —Ä–∞—Å—à–∏—Ä–∏–ª–∏ —Ä–∞–∑–º–µ—Ä –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–Ω–æ –¥–æ 20 000 –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–æ–±—Ä–∞–≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—Ä–∞–∑—Ü—ã. –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ–±–ª—é–¥–∞–ª–∏—Å—å –¥–≤–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞. –í–æ-–ø–µ—Ä–≤—ã—Ö, –≤–æ–ø—Ä–æ—Å—ã —Å–æ–±–∏—Ä–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, –∏–∑–±–µ–≥–∞—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö LLM –≤–æ–ø—Ä–æ—Å–æ–≤. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –¥–æ–±–∞–≤–ª—è–ª–∏—Å—å –æ—Ç–≤–ª–µ–∫–∞—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –∫–∞–∫ —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–±–æ—Ç–µ LongBench. –î–ª—è NovelQA –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã. –î–ª—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö Coursera, MultiFieldQA –∏ MultiDoc2Dial –∏—Ö —Ä–∞–∑–º–µ—Ä—ã –Ω–µ –±—ã–ª–∏ —É–≤–µ–ª–∏—á–µ–Ω—ã, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≤–Ω–µ—Å–µ–Ω–∏—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–∞–ª–µ–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –∏—Å—Ö–æ–¥–Ω—ã–π, –¥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö - –≤—ã–±–æ—Ä–æ—á–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –≤–æ–ø—Ä–æ—Å–æ–≤.</p>\n<p><strong>3.3 –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö</strong></p>\n<p>–ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –±—ã–ª–æ –ø–æ–ª—É—á–µ–Ω–æ 19 188 –≤–æ–ø—Ä–æ—Å–æ–≤, –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö 13 651 —Ç—Ä–µ–±–æ–≤–∞–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–æ–≥–ª–∞—Å–Ω–æ –º–µ—Ç–æ–¥—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ 3.1, –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ 3. –í–æ–ø—Ä–æ—Å—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ Coursera, –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å —É–¥–∞–ª–µ–Ω–∏—è. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –≤–æ–ø—Ä–æ—Å—ã, –≤–∑—è—Ç—ã–µ –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–Ω–∏–≥ –∏–ª–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á–∞—Å—Ç–æ —Å –±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ–≥–ª–∏ –±—ã—Ç—å –æ—Ç–≤–µ—á–µ–Ω—ã LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ 12 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–ª–∞—Å—å —Å—Ö–æ–∂–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –≤—ã–±–æ—Ä–æ—á–Ω—ã–º –∏ –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–∞–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—Å–º. —Ç–∞–±–ª–∏—Ü—ã 2 –∏ 3), —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –æ–±–∞ –Ω–∞–±–æ—Ä–∞ –∏–º–µ—é—Ç —Å—Ö–æ–∂–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.</p>"
            },
            {
                "title": "Evaluation methodology",
                "content": "4.1 Evaluation Framework Our evaluation of RAG and LC is conducted in the following three phases. Phase 1: Empirical Study on Retrievers. We evaluate five retrievers: BM25, Contriever, OpenAI Embeddings, Llama-Index, and RAPTOR, on the sample question set. The retriever yielding the best performance is then selected for subsequent comparisons with LC on the full question set. Phase 2: Comparing RAG and LC. Using the best retriever, RAG is compared with LC by anDataset Doc Source Avg Len Used by Papers # # Kept % Kept Mode NQ Coursera NovelQA 2WikiMHQA HotpotQA MuSiQue MultiFieldQA NarrativeQA QASPER QuALTY TOEFL-QA MultiDoc2Dial multi multi single multi multi multi single single single single single multi Wikipedia Coursera books Wikipedia Wikipedia Wikipedia papers, reports books, films papers stories exams dialogue 18,164.7 M, 7,934.3 NIL (L-eval) 67,000.0 NIL (NovelQA) 7,191.3 10,602.7 12,974.3 5,706.1 25,274.2 5,350.3 5,089.2 109 172 210 B, S, 300 B, R, L, C, S, 200 200 B, R, C, 150 B, R, L, C, 200 B, R, 224 B, R, 202 R, 121 729.1 NIL (L-eval) 158 3,076.9 NIL (L-eval) 22 54 109 152 93 140 121 171 221 202 121 158 20 Open 32 MCQ 52 MCQ Open 51 Open 47 Open 70 Open 81 Open 86 99 Open 100 MCQ 100 MCQ Open 100 Table 2: Overview of the original datasets (i.e., the pre-expanded sample question set) and their characteristics. The column represents dataset type with values for Knowledge, for reasoning, and for reading comprehension. For each dataset, we report the existing papers (with the label) about LC & RAG that use it. If no paper has used it, we report its source like L-eval (An et al., 2024). We also report number of questions in each set (# Q), number and percentage of questions retained after filtering (# Kept and % Kept) out questions needing no context, and mode of question. Dataset # Questions # Kept % Kept 4.2 Retriever Selection Coursera NQ NovelQA 2WikiMHQA HotpotQA MuSiQue MultiFieldQA NarrativeQA QASPER QuALTY TOEFL-QA MultiDoc2Dial 172 1,109 2,283 2,300 2,200 2,200 150 2,211 2,718 2,725 962 54 373 869 1,036 1,113 1,663 121 1,880 2,674 2,725 962 158 32 34 38 45 51 78 81 85 98 100 100 100 Total 71 19,188 Table 3: Statistics of the full question set, ordered by increasing percentage of questions kept after filtering out questions needing no context. 13,628 For Figure 1 shows that existing studies primarily select one or more chunk-based retrieval methods, while indexand summarization-based retrievers are less frequently evaluated. In our study, we evaluate various retrieval methods to ensure that RAG is supported by the most effective retrievers. we use BM25 (Robertson and Zaragoza, 2009), Contriever (Izacard et al., 2022), and OpenAIs text-embedding-3-Small. BM25 serves as classic baseline, while Contriever and textembedding-3-Small represent embeddings from well-performing closed-source and open-source models, respectively. chunk-based retrieval, swering questions on the full question set. Both methods use the same underlying LLM for question answering. For RAG, relevant documents or chunks are fetched from the available context and provided to the LLM as input to generate answers. In contrast, for LC, the entire context available to the question is given to the LLM, with truncation from the back of the context applied if the context exceeds the models context window. The evaluation metrics are explained in 4.3. Phase 3: In-depth Analysis. We focus on 4 specific subsets of questions: 1) those answered correctly only by RAG, 2) those answered correctly only by LC, 3) those RAG gives better answers, and 4) those LC gives better answers. These subsets are analyzed to understand the types of questions each method excels at, providing insights into the strengths and limitations of both approaches in different scenarios. For index-based retrieval, we employ Llamaindex and leverage two indexing methods that suit long documents. Specifically, tree-index organizes documents into hierarchical tree structure, enabling efficient retrieval of context. The root node contains high-level summary, while subsequent child nodes store progressively finer-grained representations. When queried, the retrieval process navigates through this hierarchy, starting from the toplevel summary and moving down to more specific nodes as needed. Sentence Window Retriever focuses on local, sentence-level context rather than entire documents or large text chunks. It creates smaller windows of few sentences each. When query arrives, the retriever searches these windows to identify segments most semantically similar to the query. By working at finer granularity, the sentence window retriever provides more targeted and contextually accurate snippets of text, Match (EM) score strictly to all questions to determine the correctness of the answers. Excluding the overlap, the top right block indicates the questions that only LC answers correctly, and similarly, the bottom left block indicates the questions that only RAG answers correctly. The remaining gray block represents the questions that both RAG and LC answer incorrectly, as judged by Exact Match. Since many questions involve long open-ended responses, we calculate the 1 scores of the answers provided by both methods against the ground truth. If RAG achieves higher 1 score than LC, we consider RAG to have answered the question better, and vice versa for LC. detailed explanation of 1 score calculation is provided in appendix The loose evaluation setting considers all cases in which one method outperforms the other, including 1) when one method obtains the correct answer and the other is wrong under EM, and 2) when one method achieves higher 1 score. We adopt this loose evaluation because references for some datasets are long, open-ended answers, making it very unlikely to match them exactly under EM. In addition, some short answers (about 56 words) may differ slightly from the reference while still conveying the correct idea. Although these answers would be marked incorrect by EM, they might attain high 1 score. Hence, comparing 1 scores helps compensate for the strictness of EM.",
                "summary": "<p>–í —Ä–∞–∑–¥–µ–ª–µ 4.1 –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏ RAG (Retrieval-Augmented Generation) –∏ LC (Long Context). –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ —Ç—Ä–∏ —ç—Ç–∞–ø–∞:</p>\n<p><strong>–≠—Ç–∞–ø 1: –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤.</strong> –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (—Ä–µ—Ç—Ä–∏–≤–µ—Ä–æ–≤) –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –≤–æ–ø—Ä–æ—Å–æ–≤: BM25, Contriever, OpenAI Embeddings, Llama-Index –∏ RAPTOR.  –í—ã–±–∏—Ä–∞–µ—Ç—Å—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä, –ø–æ–∫–∞–∑–∞–≤—à–∏–π –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è RAG —Å LC –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –≤–æ–ø—Ä–æ—Å–æ–≤. <em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –≠—Ç–æ—Ç —ç—Ç–∞–ø –≤–∞–∂–µ–Ω –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è RAG, –ø–æ—Å–∫–æ–ª—å–∫—É –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ RAG.</em></p>\n<p><strong>–≠—Ç–∞–ø 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG –∏ LC.</strong> –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ RAG —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å LC –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—É—á—à–µ–≥–æ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞, –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ. –û–±–∞ –º–µ—Ç–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω—É –∏ —Ç—É –∂–µ –±–æ–ª—å—à—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (LLM) –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –í RAG —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤. –í LC –≤–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å –≤–æ–∑–º–æ–∂–Ω—ã–º —É—Å–µ—á–µ–Ω–∏–µ–º —Å –∫–æ–Ω—Ü–∞, –µ—Å–ª–∏ –æ–Ω –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –º–æ–¥–µ–ª–∏) –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è LLM. <em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –°—Ä–∞–≤–Ω–∏–≤–∞—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—ã, –≥–¥–µ RAG —Å–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∞ LC –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç.</em></p>\n<p><strong>–≠—Ç–∞–ø 3: –£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑.</strong> –ù–∞ —Ç—Ä–µ—Ç—å–µ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤ –≤–æ–ø—Ä–æ—Å–æ–≤: 1) –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ RAG, 2) –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ LC, 3) –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ RAG –¥–∞–µ—Ç –ª—É—á—à–∏–µ –æ—Ç–≤–µ—Ç—ã, –∏ 4) –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ LC –¥–∞–µ—Ç –ª—É—á—à–∏–µ –æ—Ç–≤–µ—Ç—ã. –≠—Ç–æ—Ç –∞–Ω–∞–ª–∏–∑ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö. <em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –≠—Ç–æ—Ç —ç—Ç–∞–ø –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–Ω—è—Ç—å, –≤ –∫–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –∫–∞–∂–¥—ã–π –º–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ, —á—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Ö —Ä–∞–∑–ª–∏—á–∏–π.</em></p>\n<p>–¢–∞–∫–∂–µ –≤ —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã. BM25 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥. Contriever –∏ text-embedding-3-Small –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –∑–∞–∫—Ä—ã—Ç–æ–π –∏ –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª–µ–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –î–ª—è –∏–Ω–¥–µ–∫—Å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Llama-Index —Å –¥–≤—É–º—è –º–µ—Ç–æ–¥–∞–º–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: tree-index (–æ—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É) –∏ Sentence Window Retriever (—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π).  <em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –û–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</em></p>\n<p>–í —Ä–∞–∑–¥–µ–ª–µ —Ç–∞–∫–∂–µ –ø–æ—è—Å–Ω—è—é—Ç—Å—è –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏. –î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ Exact Match (EM), –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º.  –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ F1 score, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–µ–ø–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Ç–æ—á–Ω–æ.  –î–ª—è –±–æ–ª–µ–µ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è \"—Å–≤–æ–±–æ–¥–Ω–∞—è\" –æ—Ü–µ–Ω–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–æ–π, –≤–∫–ª—é—á–∞—è —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ EM, –∞ –¥—Ä—É–≥–æ–π –Ω–µ—Ç, –∞ —Ç–∞–∫–∂–µ —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ –æ–¥–∏–Ω –º–µ—Ç–æ–¥ –∏–º–µ–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π F1 score. <em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –û–±—ä—è—Å–Ω—è–µ—Ç—Å—è, –ø–æ—á–µ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è \"—Å–≤–æ–±–æ–¥–Ω–∞—è\" –æ—Ü–µ–Ω–∫–∞, –∏ –∫–∞–∫ –æ–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–≥–æ—Å—Ç—å EM.</em></p>"
            },
            {
                "title": "Experiments",
                "content": "To obtain answers, we use the same prompt From the context: [context], answer the questions briefly with no explanation. for both retrieval and long context settings. For MCQ questions, we add one sentence Answer the question with the letters of the correct options (e.g. A, BC, C, ACD, etc.) without including text. These prompts ensure LLMs to directly answer the questions, which makes evaluation more convenient. Figure 2: Evaluation Matrix for In-depth Analysis. improving the models ability to answer specific questions. For summarization-based retrieval, we use RAPTOR (Sarthi et al., 2024). It constructs hierarchical tree by recursively clustering text chunks based on semantic similarity, summarizing each cluster into parent node, and continuing this process until no further clustering is possible. After constructing the tree, we apply the collapsed tree traversal approach, as previous work has demonstrated its superior performance. This approach flattens the hierarchical structure into single layer and compares the query against all nodes across every level simultaneously. The top-k most relevant nodes are then selected based on predefined token limit, ensuring that the retrieved information maintains the appropriate level of granularity. Although RAPTORs implementation appears similar to the Llama Tree Index, they differ in both construction and navigation. First, Llama Tree Index groups consecutive nodes, while RAPTOR freely clusters nodes from far positions, and even allows single node to appear in multiple clusters. Second, Llama Tree Index navigates down the hierarchy to retrieve only leaf nodes, while RAPTOR evaluates all nodes from all layers simultaneously. Hence, RAPTOR can retrieve not only original texts but also generated summaries. 4.3 Evaluation Metric 5.1 Phase 1: Retrievers We use win-lose rate system to compare LC and RAG, as illustrated in Figure 2. The horizontal yellow block represents the questions that the LLM answers correctly using LC, while the vertical blue block represents the questions that the LLM answers correctly using RAG. Their overlap in the top-left corner represents the questions that both methods answer correctly. We apply an Exact Evaluated on the sample question set, Table 5 reports the results of chunk-, index-, and summarization-based retrievers. Among them, RAPTOR performs the best with correct answer rate of 38.5%, while Index-based retrievers outperform chunk-based retrievers. Within index-based retrievers, the RAG Only score for Tree Index is much lower than that for Window Parsing (82 Dataset # Questions LC Correct RAG Correct LC Only RAG Only LC Better RAG Better Coursera 2WikiMHQA HotpotQA MultiFieldQA NQ NarrativeQA QASPER QuALITY TOEFL-QA MuiQue MultiDoc2Dial NovelQA 54 1,036 1,113 121 373 1,880 2,674 2,725 962 1,663 158 869 26 594 876 63 189 558 884 2,290 895 821 14 466 20 431 723 60 138 405 863 2,050 884 663 38 10 242 212 14 75 276 517 402 26 344 5 164 4 79 59 11 24 123 496 162 15 186 29 106 10 265 231 44 104 685 1,011 402 26 426 65 164 4 107 67 21 35 281 762 162 15 225 58 106 13,628 Overall 6,683 Table 4: Performance of LC and RAG across different datasets. We report the number of questions answered correctly by each method, as well as the breakdown of questions where: only LC answers correctly (LC Only), only RAG answers correctly (RAG Only), LC outperforms RAG (LC Better), and RAG outperforms LC (RAG Better). 1,294 2,287 3,433 1,843 Type Retriever Correct (%) RAG Only RAG Better Chunk Index BM25 Contriever Text-emb-3-small Tree Index Window Parsing 319 (20.4) 315 (20.1) 338 (21.6) 470 (30.1) 555 (35.5) 50 43 47 82 91 141 143 234 237 Summarization RAPTOR Table 5: Comparison of different retrieval methods 602 (38.5) 258 97 vs. 91), and their RAG Better scores are nearly identical (234 vs. 237). This discrepancy suggests that Tree Index may be undervalued in the RAG Only metric but still contributes in open question scenarios that require long answers. We further observe the questions and contexts that each retriever exclusively answers correctly. RAPTOR shows stronger ability than other retrievers, especially in scenarios that require an entire understanding of the document, like research papers. Chunk-based methods struggle when required information is spread across multiple chunks. Indexbased retrievers are not as strong in overall understanding as RAPTOR, but they show good ability in interpreting dialogues. Therefore, we select RAPTOR as the primary retriever for evaluation on the full question set. 5.2 Phase 2: Comparing LC and RAG We compare LC and RAG on the filtered, full question set. The results across 12 datasets are summarized in Table 4. Overall, LC correctly answers 56.3% of the questions, while RAG provides correct answers to 49.0%. LC correctly answers more than 2,000 questions that RAG misses, while RAG exclusively answers almost 1,300 questions. When looking at the loose evaluation setting, LC answers 3,433 questions better than RAG, and RAG anLooking at swers 1,843 questions better than LC. The gap further widens compared to strict setting, indicating long-context LLMs ability to answer questions with open long answers is also strong. individual datasets, in MultiDoc2Dial, RAG exhibits better performance than LC in strict evaluation (5 vs 29), but is surpassed by LC in loose evaluation (65 vs 58). In contrast, on datasets like NarrativeQA and QuaLITY, LC shows strong lead not just in overall correctness but also in the number of questions that are answered better. Collectively, the results show that both methods have unique strengths and limitations. Although LC shows better overall results than RAG, out of the 13,628 questions, almost 10% can be only answered correctly by RAG, which is not small ratio. This shows that retrievers cannot be simply replaced by long-context LLM in searching. This also motivates us to further examine what kind of questions (and context) can be only answered correctly by RAG (or LC). 5.3 Phase 3: In-Depth Analysis The overall results are influenced by the combined effects of different scenarios, so we need to separately analyze each scenario to see if more detailed results can be obtained. We analyze the performance of LC and RAG across different knowledge sources (Figure 3) and question types (Figures 4). Here, we use EM Scores only, for strict evaluation standard. We also report the results for loose evaluation standard (i.e., EM Scores and 1 Scores) in appendix B, which shows similar trends. From Figure 3, it is evident that LC excels with knowledge sources such as Wikipedia and stories. However, the Wikipedia context is collected Figure 3: Performance breakdown by knowledge source for LC Only and RAG Only. by adding extensive noise to create long context, which generally makes the context less relevant to the question, with only small portion being useful. This synthetic context formation partially simulates the RAG process and may introduce an In addiunfair bias against the RAG pipeline. tion, summarization-based retrieval methods may split Wikipedia articles unnaturally, generating less meaningful summaries. LCs strong performance demonstrates that long-context LLMs are robust to noise in such forms of context. In contrast, RAG performs better with dialoguerelated sources and achieves comparable performance with papers or reports. The information in these sources is naturally segmented, conversations have turns, and papers and reports have clearly defined sections or subsections, making the retrieval of key segments easier. Figure 4 shows that LC performs better for factbased questions such as Who, Where, and Which. These questions often benefit from having all the relevant context available in dense region close to the answer. RAG, however, is largely comparable to LC for more open-ended questions such as How, which often require synthesizing information from multiple sources and therefore benefit from retrieval-based approaches. Furthermore, RAG outperforms LC in the Other questions, which consist mainly of general questions that can be answered with Yes or No. We hypothesize that the reason could be due to the training data. Long-context LLMs are more familiar with phrasing of common type questions than general questions. Words like Who or Where act as keywords for long-context LLMs to search, while retrievers use these keywords not so well. 5.4 Word Frequency Visualization To better understand the scenarios that LC and RAG each excels at, we visualize the word frequencies by their TF-IDF scores, plotted in Figure 5. The TF-IDF scores were calculated from Figure 4: Performance breakdown by question type for LC Only and RAG Only. Figure 5: Top 15 Words based on TF-IDF Score for LC Only vs. RAG Only. questions in the datasets where either LC or RAG produced correct answers exclusively. Specifically, all questions from each dataset are concatenated and treated as single document for this analysis, meaning that the TF-IDF scores primarily reflect the term frequency within each dataset. Stopwords are removed and not shown in the plot. Figure 5 presents the top 15 words that appear most frequently combined in both LC only and RAG only questions. Words such as song, film, and novel have higher TF-IDF scores for LC, suggesting that LC performs better with narrative topics. Conversely, words like country, dataset, and model have higher scores for RAG, indicating its strength in retrieving information on technical or data-oriented topics. This analysis underscores the complementary strengths and limitations of LC and RAG in handling different types of questions. 5.5 Impact of Generation Model in RAG We now evaluate the impact of different generation models on RAGs performance. Table 6 shows the results of using GPT-4o and GPT-4-Turbo as the generator with three retrievers (BM25, Tree Index, RAPTOR), each of which represents one retriever type. The results indicate that the performance of different generation models remains largely conRetriever Model Correct (%) RAG Only RAG Better BM25 Tree-Index GPT-4o GPT-4-Turbo GPT-4o GPT-4-Turbo 319 (20.4) 310 (19.8) 470 (30.1) 458 (29.3) 50 51 82 81 141 152 234 RAPTOR 602 (38.5) 589 (37.7) Table 6: Results of using different generation models GPT-4o GPT-4-Turbo 258 295 97 99 sistent regardless of the retriever used. RAPTOR performs the best across both generation models, though there is slight decrease in performance when using GPT-4-Turbo compared to GPT-4o. While GPT-4o slightly outperforms GPT-4Turbo across all retrievers, the differences are marginal. This implies that both generation models are capable of generating high-quality responses, and the choice between them may depend more on other factors such as efficiency or resource availability. The consistency across retrievers also demonstrates that the retrieval method plays larger role in determining overall performance than the specific generation model used. We will report the results from other models and the experiment is in progress. 5.6 Case Study For deeper understanding of the difference between LC and RAG, we conduct case study to analyze the frequent errors from each method, and present them in Tables 7 and 8. We manually examine the questions that only RAG made mistakes, and those only LC made mistakes. The most frequent mistake made by RAG is its failure to retrieve the relevant context, leading to its refusal to answer the question. As shown in Table 7, the model correctly identifies that Anthony Upko was formerly involved in the government of Nigeria but fails to retrieve the debt-to-GDP ratio as part of the context. This retrieval failure can arise due to two possible reasons: the retriever might fail to locate the relevant sentences from documents, or the sentences may be split across two chunks, with the debt-to-GDP ratio lacking clear subject. Interestingly, when provided with the same prompt, LC rarely reports lack of context, suggesting its robustness in handling such cases. Another error made by RAG is misinterpreting partial context. In the second example, where RAG incorrectly answered the birthday, the model retrieved May 8, 1940, instead of the correct date, January 8, 1935. This occurred because the sen1. 11 percent Nigeria is the The context does not Question: What is the debt-to-GDP ratio of the country where Anthony Upko was formerly involved in the government? Wrong Answer: provide the debt-to-GDP ratio for Nigeria. Gold: Relevant Sents: worlds 20th largest economy ... the debt-to-GDP ratio is only 11 percent. Anthony Ukpo was Minister of 2. Information and Culture, and then Governor of Rivers State, Nigeria. Question: song Swing Down Sweet Chariot birthday? Wrong Answer: Gold: Relevant Sents: Chariot is traditional song ... recorded by Elvis Presley. 2. Elvis Aaron Presley (January 8, 1935 - August 16, 1977), also known as ... Table 7: Examples cases where RAG made mistakes When is the performer of Swing Down Sweet January 8, 1935 May 8, 1940 1. This helped us narrow Yes, the tweets come Do the tweets come from Question: specific region? Wrong Answer: from 16 different countries. Gold: No Relevant Sents: down our query space to 16 countries. Question: his wealth? Wrong Answer: Gold: Relevant Sents: aunts estate, Emily learns that Valancourt has gone to Paris and lost his wealth. Where did Valancourt lose Returning to her In Gambling. Paris Table 8: Examples representing common cases where only RAG answers correctly tence Swing Down Sweet Chariot is traditional song ... recorded by Elvis Presley spans too long, creating ambiguity in linking the birthday to the correct person. This type of retrieval failure highlights core limitation: RAG relies heavily on retrieving continuous text spans, and any fragmentation or overly long context can lead to an incomplete understanding. In contrast, LC tends to provide more holistic answers when processing longer contexts directly, as it bypasses the dependency on retrieval module. Wrong answers by LC are often caused by question misinterpretation. For instance, as shown in Table 8, when asked whether the tweets come from specific region, LC answers yes, referencing that the tweets originate from 16 countries. It fails to interpret the relationship between specific region and 16 different countries. In another example, when asked where Valancourt lost his wealth, the model identifies the correct sentence but answers how instead of where. These examples highlight that LC sometimes struggles to align its semantic understanding with the required level of specificity or perspective, resulting in answers that are related but not addressing the questions intent. In both cases, the LLMs are able to locate the related texts from the documents, but the reasoning ability might be affected by the noise.",
                "summary": "<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ –æ—Ç–≤–µ—Ç—É –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (LC) –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞ (RAG). </p>\n<p><strong>–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –æ—Ü–µ–Ω–∫–∏:</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —Å–∏—Å—Ç–µ–º–∞ \"–≤—ã–∏–≥—Ä–∞–ª-–ø—Ä–æ–∏–≥—Ä–∞–ª\". –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ \"LC Correct\", –∞ —Ç–µ, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª–∞ –º–æ–¥–µ–ª—å RAG, –∫–∞–∫ \"RAG Correct\". –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π - –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ–±–µ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—Ç–∏–ª–∏ –≤–µ—Ä–Ω–æ. \"LC Only\" - –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª–∞ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å LC, \"RAG Only\" - —Ç–æ–ª—å–∫–æ RAG. \"LC Better\" –∏ \"RAG Better\" –æ–±–æ–∑–Ω–∞—á–∞—é—Ç, –∫–∞–∫–∞—è –º–æ–¥–µ–ª—å –¥–∞–ª–∞ –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (–Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Ç–æ—á–Ω—ã–π).</p>\n<p><strong>–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (Retrievers):</strong></p>\n<p>–î–ª—è RAG –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:\n* <strong>Chunk-based:</strong> —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã.\n* <strong>Index-based:</strong> –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞. –í–∫–ª—é—á–∞–µ—Ç –º–µ—Ç–æ–¥—ã BM25, Contriever, Text-emb-3-small, Tree Index –∏ Window Parsing.\n* <strong>Summarization-based:</strong> –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ RAPTOR.</p>\n<p><strong>RAPTOR:</strong>\nRAPTOR —Å—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ, —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É—è —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏. –ö–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä —Å—É–º–º–∏—Ä—É–µ—Ç—Å—è –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —É–∑–µ–ª. –ó–∞—Ç–µ–º –¥–µ—Ä–µ–≤–æ \"—Å—Ö–ª–æ–ø—ã–≤–∞–µ—Ç—Å—è\" –≤ –æ–¥–∏–Ω —Å–ª–æ–π, –∏ –∑–∞–ø—Ä–æ—Å —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å–æ –≤—Å–µ–º–∏ —É–∑–ª–∞–º–∏ –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –í—ã–±–∏—Ä–∞—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —É–∑–ª—ã –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤.\n*   <strong>–û—Ç–ª–∏—á–∏–µ –æ—Ç Llama Tree Index:</strong> RAPTOR –º–æ–∂–µ—Ç –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞ –∏ –¥–∞–∂–µ –≤–∫–ª—é—á–∞—Ç—å –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –¢–∞–∫–∂–µ RAPTOR –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ —É–∑–ª—ã –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –ª–∏—Å—Ç–æ–≤—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –Ω–æ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ.</p>\n<p><strong>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:</strong></p>\n<ul>\n<li><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:</strong> RAPTOR –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (38.5% –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤), –æ–ø–µ—Ä–µ–¥–∏–≤ –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã. –ò–Ω–¥–µ–∫—Å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∫–∞–∑–∞–ª–∏ —Å–µ–±—è –ª—É—á—à–µ, —á–µ–º –º–µ—Ç–æ–¥—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.</li>\n<li><strong>–°—Ä–∞–≤–Ω–µ–Ω–∏–µ LC –∏ RAG:</strong><ul>\n<li>–í —Ü–µ–ª–æ–º, LC –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (56.3% –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤), —á–µ–º RAG (49.0%). </li>\n<li>–û–¥–Ω–∞–∫–æ, RAG –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª –Ω–∞ –ø–æ—á—Ç–∏ 10% –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ LC –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª.</li>\n<li>–í \"—Å–≤–æ–±–æ–¥–Ω–æ–π\" –æ—Ü–µ–Ω–∫–µ (–∫–æ–≥–¥–∞ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å), LC —Ç–∞–∫–∂–µ –ø—Ä–µ–≤–∑–æ—à—ë–ª RAG, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–∏–ª–µ LC –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã.</li>\n<li>–ù–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, MultiDoc2Dial) RAG –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –Ω–∞ –¥—Ä—É–≥–∏—Ö (NarrativeQA –∏ QuaLITY) LC –±—ã–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ —É –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞ –µ—Å—Ç—å —Å–≤–æ–∏ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã.</li>\n</ul>\n</li>\n<li><strong>–ê–Ω–∞–ª–∏–∑ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∑–Ω–∞–Ω–∏–π:</strong><ul>\n<li>LC –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–∞–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏, –∫–∞–∫ Wikipedia –∏ –∏—Å—Ç–æ—Ä–∏–∏. –ü—Ä–∏ —ç—Ç–æ–º, –∫–æ–Ω—Ç–µ–∫—Å—Ç Wikipedia –±—ã–ª —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ \"–∑–∞—à—É–º–ª–µ–Ω\", —á—Ç–æ, –≤–æ–∑–º–æ–∂–Ω–æ, –ø–æ–≤–ª–∏—è–ª–æ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç RAG.</li>\n<li>RAG –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–∏–∞–ª–æ–≥–æ–≤—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —É—Ä–æ–≤–Ω–µ LC —Å –Ω–∞—É—á–Ω—ã–º–∏ —Å—Ç–∞—Ç—å—è–º–∏ –∏ –æ—Ç—á–µ—Ç–∞–º–∏.</li>\n</ul>\n</li>\n<li><strong>–ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º –≤–æ–ø—Ä–æ—Å–æ–≤:</strong><ul>\n<li>LC –ª—É—á—à–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö—Ç–æ\", \"–ì–¥–µ\", \"–ö–∞–∫–æ–π\").</li>\n<li>RAG —Å—Ä–∞–≤–Ω–∏–º —Å LC –ø–æ –æ—Ç–∫—Ä—ã—Ç—ã–º –≤–æ–ø—Ä–æ—Å–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–ö–∞–∫\").</li>\n<li>RAG –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç LC –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–î–∞/–ù–µ—Ç\").</li>\n</ul>\n</li>\n<li><strong>–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤:</strong><ul>\n<li>–°–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ–º (–ø–µ—Å–Ω—è, —Ñ–∏–ª—å–º, —Ä–æ–º–∞–Ω), —á–∞—â–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–ª LC.</li>\n<li>–°–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏ (—Å—Ç—Ä–∞–Ω–∞, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –º–æ–¥–µ–ª—å), —á–∞—â–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–≤–µ—á–∞–ª RAG.</li>\n</ul>\n</li>\n<li><strong>–í–ª–∏—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ RAG:</strong><ul>\n<li>–í–ª–∏—è–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ RAG —Ç–∞–∫–∂–µ –æ—Ü–µ–Ω–∏–≤–∞–ª–æ—Å—å, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø–∏—Å–∞–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ.</li>\n</ul>\n</li>\n</ul>\n<p><strong>–í—ã–≤–æ–¥—ã:</strong></p>\n<p>–û–±–∞ –º–µ—Ç–æ–¥–∞ –∏–º–µ—é—Ç —Å–≤–æ–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. LC —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–ª–∏–Ω–Ω—ã–º, –¥–∞–∂–µ –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏ –ª—É—á—à–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã. RAG, –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å, —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–∏–∞–ª–æ–≥–∞–º–∏, –Ω–∞—É—á–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, –≥–¥–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–∏–Ω—Ç–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç, —á—Ç–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ –∑–∞–º–µ–Ω–µ–Ω–æ –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∏ —á—Ç–æ –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –º–æ–≥—É—Ç –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞.</p>"
            },
            {
                "title": "Discussion",
                "content": "6.1 What is Long Context? Although we have reviewed 9 studies that either directly or implicitly compare or integrate RAG and Long Context, very few studies clearly define what Long Context is. To this end, we separately interpret the two words long and context. Long. Out of the 9 studies reviewed earlier, only 2 studies, ChatQA2 and LongBench v2 explicitly define Long Context as greater than 32k and greater than 8k tokens respectively. For other studies, we can only infer their definitions of long based on the models and datasets they use. It seems that three studies consider 8k as minimum requirement for long context, and another three studies set this requirement at 16k. Lastly, OP-RAG regards 128k as long context. In short, each work defines Long Context based on its own criteria due to the lack of clear standard. Moreover, as the context windows of language models continue to expand, the terms long and short are relative. For example, 4k tokens are not considered long context in any of the reviewed studies but are extremely long for BERTbase models, which support only 512 tokens. As result, the definition of long remains ambiguous, leading to inconsistent use of this concept among researchers. In practice, the definition of long is complicated, depending on the context length of latest LLMs, and the length of the documents in targeted domain. Context In the English dictionary, context is defined as the situation within which something happens, and that can help explain it. By this definition, the context of question is expected to help explain it, implying that the context should have strong relevance to the question. However, long-context datasets are not always constructed with this principle in mind. The construction of long-context datasets can generally be categorized into two types: Realistic Long Texts: These datasets originate from sources such as novels, research papers, or other lengthy narratives, exemplified by datasets like NovelQA. Such datasets typically pose challenges that involve reading comprehension and require models to process and synthesize dense information spread across cohesive, extended text. Synthetic Long Texts: These datasets are often created by concatenating smaller, query-relevant segments of text, such as Wikipedia-sourced datasets in LongBench. This construction process may involve stitching together Wikipedia excerpts, injecting noise, or combining unrelated passages to simulate long document. critical observation is that realistic long contexts align more closely with reading comprehension tasks, where models primarily absorb and reason over information. Such datasets have high contextual relevance, since the questions are normally based on the documents that users provided. In contrast, synthetic long contexts often resemble factual reasoning tasks, where models retrieve and verify knowledge. Such datasets inherently incorporate pre-processing step like RAG pipeline. They can assess the impact of information placement on model performance, such as the lost-in-the-middle phenomenon. On the other hand, realistic and synthetic long texts can only serve as proxies to reflect context relevance to some extent. The scope of the context is question-dependent and difficult to define clearly. 6.2 How to Compare or Combine LC & RAG? The lack of clear definition for long context also indicates the absence of coherent framework for comparing or combining LC and RAG. We propose such framework by examining three key perspectives: context length, context relevance, and experiment design. Context Length. From the models perspective, context length refers to the maximum number of tokens model can process. From the datasets perspective, it denotes the amount of text provided In synthetic datasets, context with question. length is flexible, but this introduces trade-off between length and relevance. Adding irrelevant information as context may help to test models robustness to noise, but such testing may not represent real-world use cases. Therefore, any framework for comparing LC and RAG should clearly define what is considered long, while indicating whether this length criterion originates from the models capabilities, the datasets design, or both. Context Relevance. An evaluation framework must also address the relevance of the text provided as input to the model. It is crucial to distinguish between realistic long contexts and synthetic long contexts. When benchmarks include both types, separate evaluations are necessary, as synthetic contexts often have low relevance and may not accurately reflect real-world scenarios. Interestingly, the construction of synthetic long contexts often mirrors RAG pipelines. Providing an entire curated text to an LLM as context essentially represents long context RAG approach, given that such text is assembled during dataset creation. Further chunking can introduce biases against RAG by disrupting the continuity of information within each piece. Additionally, many benchmarks categorize tasks as single-doc or multi-doc based on whether the text originates from single source or multiple documents. While convenient, this categorization does not perfectly align with realistic or synthetic contexts. single document may sometimes be artificially composed of smaller fragments, while multi-sourced document might involve highly relevant sources, such as group of research papers discussing the same problem. The key issue remains determining to what extent the context provided as input to LLMs contains sufficient and relevant content to answer the question, without introducing unnecessary or unrelated information. Experiment Settings. When investigating LC and RAG, the experimental objectives can be broadly grouped into two categories: comparison and combination. Short RAG v.s. Long Single Input: one might compare short-context RAG pipeline against long-context single-input setup, analyzing both performance and computational cost. This provides insights into the trade-off between running an extra retrieval pipeline for shorter contexts versus allowing the model to process larger uninterrupted text. Long RAG v.s. Long Single Input: One may also compare long-context RAG pipeline with longcontext single-input approach. Here, the goal is to see whether chunking or filtering more relevant content through retrieval can outperform or complement fully integrated long-context approach by truncating exceptionally long documents. In the first setting, the retrieval pipeline naturally reduces the number of tokens. In the second setting, the context length remains the same for both methods, with the only difference being how the text is processed. RAG over Increasing Context: Another possible goal is understanding how RAG performance changes with increasing context lengths. In this scenario, the LC refers specifically to how many tokens model can handle. This line of work can reveal how well RAG pipelines scale when models absorb increasingly larger inputs. On the other hand, findings from evaluations often serve as guidelines for settings that address real-world problems. In this sense, RAG and LC may complement each other in real-world settings, depending on the characteristics of the data source and the types of questions to be answered. 6.3 Revisiting All Studies Based on the earlier discussion, the exploration of LC and RAG methods in LLMs highlights some critical challenges that researchers often overlook. Trade-off between Context Length and Relevance. Many studies hesitate between using flexible synthetic context with noisy concatenated contexts, or realistic context with dense information but less availability. Among the 9 studies, 6 select synthetic context as part of the datasets. Our own evaluation has also selected synthetic context datasets, but we consider the influence of synthetic long context and separately evaluate their results by context source; e.g. Wikipedia source with manually added noises represents low context relevance. Several studies have attempted to address this challenge. LongBench recently updated v2 which collects only realistic data. Despite smaller scale, LongBench v2 shows substantial improvement in context relevance compared to its first version. LongRAG retrieves from massive corpus for all questions, instead of assigning one context to each question. This method avoids retrieving from synthetic long context and is hence recommendable. Diversity in Retrieval Mechanisms. In the comparison of RAG and LC, RAG is often underrepresented due to an over-reliance on traditional retrieval strategies. Among the 9 studies, 5 experiment with different retrievers, only 2 try different chunking sizes, and none consider any retrieval method beyond chunk-based retrievers. Although we experiment with index-based and summarization-based retrievers, we cannot promise that our selected method outperforms all retrieval strategies. For investigating RAG performance over increasing context, some studies propose their own strategies for chunking and placing RAG. OP-RAG proposes preserving the original order of chunks from the context, while LC LLM-RAG proposes placing higher-scored chunks at the front and back. In addition to more advanced retrievers, certain in- (Manning et al., 2008) formation retrieval (IR) techniques like relevance feedback (Harman, 1992) or query expansion (Carpineto and Romano, 2012) might further enhance RAG performance, yet these have been overlooked in existing frameworks. Computational Cost. Most existing studies test on 6 to 8 datasets, and it becomes increasingly expensive to conduct experiments on too many models. This is especially the case when new longcontext LLMs are being released at very fast pace. Hence, any work might be questioned because the experiment results are only applicable to one or few models. Among all works, LC RAG Performance includes the largest number of models (20). While their efforts are remarkable, they only experiment on 3 datasets. FinanceBench (Islam et al., 2023) looks at finance domain, Databricks DocsQA is based on Databricks platform, and NQ as shown table 2 as very low rate of requiring external knowledge. This is not meant as criticism but rather to show the trade-off between testing many models and having comprehensive benchmark.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ 6 \"–î–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ RAG\"</h2>\n<p><strong>6.1 –ß—Ç–æ —Ç–∞–∫–æ–µ \"–¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç\"?</strong></p>\n<p>–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Å—Ä–∞–≤–Ω–∏–≤–∞—é—â–∏—Ö –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö RAG (Retrieval-Augmented Generation) –∏ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Long Context, LC), —á—ë—Ç–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ—Ç. –ü–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –ø–æ–Ω—è—Ç–∏—è \"–¥–ª–∏–Ω–Ω—ã–π\" –∏ \"–∫–æ–Ω—Ç–µ–∫—Å—Ç\" –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏.</p>\n<ul>\n<li>\n<p><strong>–î–ª–∏–Ω–Ω—ã–π:</strong> –ò–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Ç–æ–ª—å–∫–æ –¥–≤–∞ (ChatQA2 –∏ LongBench v2) —è–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∫ –±–æ–ª–µ–µ 32 —Ç—ã—Å—è—á –∏ 8 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –í –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–∂–Ω–æ –≤—ã–≤–µ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ –∏–∑ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –¢—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å—á–∏—Ç–∞—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ–º 8 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤, –µ—â—ë —Ç—Ä–∏ ‚Äî 16 —Ç—ã—Å—è—á. OP-RAG —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç 128 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∫ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–∞–∂–¥–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –µ–¥–∏–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –ø–æ –º–µ—Ä–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–Ω—è—Ç–∏—è \"–¥–ª–∏–Ω–Ω—ã–π\" –∏ \"–∫–æ—Ä–æ—Ç–∫–∏–π\" —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, 4 —Ç—ã—Å—è—á–∏ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –Ω–æ —è–≤–ª—è—é—Ç—Å—è –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–º–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π BERTbase, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö —Ç–æ–ª—å–∫–æ 512 —Ç–æ–∫–µ–Ω–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \"–¥–ª–∏–Ω–Ω–æ–≥–æ\" –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–º, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —ç—Ç–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è. –ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ \"–¥–ª–∏–Ω–Ω–æ–≥–æ\" –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö LLM –∏ –¥–ª–∏–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ü–µ–ª–µ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏.</p>\n</li>\n<li>\n<p><strong>–ö–æ–Ω—Ç–µ–∫—Å—Ç:</strong> –í —Å–ª–æ–≤–∞—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å–∏—Ç—É–∞—Ü–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–π —á—Ç–æ-—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏ –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç —ç—Ç–æ –æ–±—ä—è—Å–Ω–∏—Ç—å. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ –¥–æ–ª–∂–µ–Ω –ø–æ–º–æ–≥–∞—Ç—å –µ–≥–æ –æ–±—ä—è—Å–Ω–∏—Ç—å, —Ç–æ –µ—Å—Ç—å –∏–º–µ—Ç—å —Å–∏–ª—å–Ω—É—é —Å–≤—è–∑—å —Å –≤–æ–ø—Ä–æ—Å–æ–º. –û–¥–Ω–∞–∫–æ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞—é—Ç—Å—è —Å —É—á—ë—Ç–æ–º —ç—Ç–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞. –ò—Ö –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–∞ —Ç–∏–ø–∞:</p>\n<ul>\n<li><strong>–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã:</strong> –≠—Ç–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–æ–º–∞–Ω–æ–≤, –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ –¥—Ä—É–≥–∏—Ö –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, NovelQA). –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–µ–±—É—é—Ç –æ—Ç –º–æ–¥–µ–ª–µ–π –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ –∏ —Å–∏–Ω—Ç–µ–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –ø–æ —Å–≤—è–∑–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É.</li>\n<li><strong>–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã:</strong> –°–æ–∑–¥–∞—é—Ç—Å—è –ø—É—Ç—ë–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ–±–æ–ª—å—à–∏—Ö, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –≤ LongBench). –≠—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Ç—Ä—ã–≤–∫–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞ –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.</li>\n</ul>\n<p>–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –±–æ–ª—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∑–∞–¥–∞—á–∞–º –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—á–∏—Ç–∞–Ω–Ω–æ–≥–æ, –≥–¥–µ –º–æ–¥–µ–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ–≥–ª–æ—â–∞—é—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –¢–∞–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏–º–µ—é—Ç –≤—ã—Å–æ–∫—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –ø–æ—Å–∫–æ–ª—å–∫—É –≤–æ–ø—Ä–æ—Å—ã –æ–±—ã—á–Ω–æ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –ù–∞–ø—Ä–æ—Ç–∏–≤, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã —á–∞—Å—Ç–æ –Ω–∞–ø–æ–º–∏–Ω–∞—é—Ç –∑–∞–¥–∞—á–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –≥–¥–µ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è—é—Ç –∑–Ω–∞–Ω–∏—è. –¢–∞–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞—é—Ç —ç—Ç–∞–ø –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä—É RAG. –û–Ω–∏ –º–æ–≥—É—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —ç—Ñ—Ñ–µ–∫—Ç \"–ø–æ—Ç–µ—Ä–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ\". –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –∫–∞–∫ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ, —Ç–∞–∫ –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –º–æ–≥—É—Ç –ª–∏—à—å –≤ –Ω–µ–∫–æ—Ç–æ—Ä–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –û–±—ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–æ–ø—Ä–æ—Å–∞ –∏ –µ–≥–æ —Ç—Ä—É–¥–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ.</p>\n</li>\n</ul>\n<p><strong>6.2 –ö–∞–∫ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å LC –∏ RAG?</strong></p>\n<p>–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —á—ë—Ç–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–∞–∫–∂–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è LC –∏ RAG. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ç–∞–∫—É—é —Å–∏—Å—Ç–µ–º—É, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è —Ç—Ä–∏ –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–∞: –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –¥–∏–∑–∞–π–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.</p>\n<ul>\n<li>\n<p><strong>–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å. –° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å –≤–æ–ø—Ä–æ—Å–æ–º. –í —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –≥–∏–±–∫–æ–π, –Ω–æ —ç—Ç–æ —Å–æ–∑–¥–∞—ë—Ç –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –¥–ª–∏–Ω–æ–π –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ —à—É–º—É, –Ω–æ —Ç–∞–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ü–æ—ç—Ç–æ–º—É –ª—é–±–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è LC –∏ RAG –¥–æ–ª–∂–Ω–∞ —á—ë—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å, —á—Ç–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è –¥–ª–∏–Ω–Ω—ã–º, –∏ —É–∫–∞–∑—ã–≤–∞—Ç—å, –∏—Å—Ö–æ–¥–∏—Ç –ª–∏ —ç—Ç–æ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª–∏–Ω—ã –∏–∑ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–µ–π, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∏ —Ç–æ–≥–æ, –∏ –¥—Ä—É–≥–æ–≥–æ.</p>\n</li>\n<li>\n<p><strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–∂–Ω–∞ —Ç–∞–∫–∂–µ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º–æ–≥–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏. –ö—Ä–∞–π–Ω–µ –≤–∞–∂–Ω–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã. –ï—Å–ª–∏ —Ç–µ—Å—Ç—ã –≤–∫–ª—é—á–∞—é—Ç –æ–±–∞ —Ç–∏–ø–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –æ—Ç–¥–µ–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –Ω–∏–∑–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –º–æ–≥—É—Ç –Ω–µ—Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —á–∞—Å—Ç–æ –∏–º–∏—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω–≤–µ–π–µ—Ä—ã RAG. –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ LLM —Ü–µ–ª–æ–≥–æ –∫—É—Ä–∞—Ç–æ—Ä—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—É—Ç–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–æ–¥—Ö–æ–¥ RAG —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ —Ç–∞–∫–æ–π —Ç–µ–∫—Å—Ç —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö. –î–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ RAG, –Ω–∞—Ä—É—à–∞—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–Ω–æ–≥–∏–µ —Ç–µ—Å—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—é—Ç –∑–∞–¥–∞—á–∏ –∫–∞–∫ –æ–¥–Ω–æ–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ –∏–ª–∏ –º–Ω–æ–≥–æ–¥–æ–∫—É–º–µ–Ω—Ç–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –∏—Å—Ö–æ–¥–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–ª–∏ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –•–æ—Ç—è —ç—Ç–æ —É–¥–æ–±–Ω–æ, —ç—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º –∏–ª–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º. –û–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –∏–Ω–æ–≥–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –∏–∑ –±–æ–ª–µ–µ –º–µ–ª–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–Ω–æ–≥–æ–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –≤ —Å–µ–±—è –æ—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –≥—Ä—É–ø–ø–∞ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç, –æ–±—Å—É–∂–¥–∞—é—â–∏—Ö –æ–¥–Ω—É –∏ —Ç—É –∂–µ –ø—Ä–æ–±–ª–µ–º—É. –ö–ª—é—á–µ–≤—ã–º –≤–æ–ø—Ä–æ—Å–æ–º –æ—Å—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–≥–æ, –≤ –∫–∞–∫–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM, —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –Ω–µ –≤–≤–æ–¥—è –Ω–µ–Ω—É–∂–Ω–æ–π –∏–ª–∏ –Ω–µ—Å–≤—è–∑–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.</p>\n</li>\n<li>\n<p><strong>–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:</strong> –ü—Ä–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏ LC –∏ RAG —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ü–µ–ª–∏ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ.</p>\n<ul>\n<li><strong>–ö–æ—Ä–æ—Ç–∫–∏–π RAG –ø—Ä–æ—Ç–∏–≤ –¥–ª–∏–Ω–Ω–æ–≥–æ –µ–¥–∏–Ω–∏—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞:</strong> –ú–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä RAG —Å –∫–æ—Ä–æ—Ç–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –µ–¥–∏–Ω–∏—á–Ω—ã–º –≤–≤–æ–¥–æ–º, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Ç–∞–∫ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã. –≠—Ç–æ –¥–∞—ë—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞ –º–µ–∂–¥—É –∑–∞–ø—É—Å–∫–æ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π —Ç–µ–∫—Å—Ç.</li>\n<li><strong>–î–ª–∏–Ω–Ω—ã–π RAG –ø—Ä–æ—Ç–∏–≤ –¥–ª–∏–Ω–Ω–æ–≥–æ –µ–¥–∏–Ω–∏—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞:</strong> –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä RAG —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å –ø–æ–¥—Ö–æ–¥–æ–º —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Å –µ–¥–∏–Ω–∏—á–Ω—ã–º –≤–≤–æ–¥–æ–º. –ó–¥–µ—Å—å —Ü–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –º–æ–∂–µ—Ç –ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏ –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—É—Ç—ë–º —É—Å–µ—á–µ–Ω–∏—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í –ø–µ—Ä–≤–æ–º —Å–ª—É—á–∞–µ –∫–æ–Ω–≤–µ–π–µ—Ä –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —É–º–µ–Ω—å—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤. –í–æ –≤—Ç–æ—Ä–æ–º —Å–ª—É—á–∞–µ –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Å—Ç–∞–µ—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª—è –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤, –∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ç–ª–∏—á–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è —Å–ø–æ—Å–æ–± –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞.</li>\n<li><strong>RAG –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:</strong> –î—Ä—É–≥–∞—è –≤–æ–∑–º–æ–∂–Ω–∞—è —Ü–µ–ª—å ‚Äî –ø–æ–Ω—è—Ç—å, –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å RAG –∏–∑–º–µ–Ω—è–µ—Ç—Å—è —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –í —ç—Ç–æ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ LC –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –∫ —Ç–æ–º—É, —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–¥–µ–ª—å. –≠—Ç–∞ —Ä–∞–±–æ—Ç–∞ –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è –∫–æ–Ω–≤–µ–π–µ—Ä—ã RAG, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª–∏ –ø–æ–≥–ª–æ—â–∞—é—Ç –≤—Å—ë –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.</li>\n</ul>\n</li>\n</ul>\n<p>–° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–æ–∫ —á–∞—Å—Ç–æ —Å–ª—É–∂–∞—Ç –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–º –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫, —Ä–µ—à–∞—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã. –í —ç—Ç–æ–º —Å–º—ã—Å–ª–µ RAG –∏ LC –º–æ–≥—É—Ç –¥–æ–ø–æ–ª–Ω—è—Ç—å –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö, –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å.</p>\n<p><strong>6.3 –ü–µ—Ä–µ—Å–º–æ—Ç—Ä –≤—Å–µ—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π</strong></p>\n<p>–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–∏, –∏–∑—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ LC –∏ RAG –≤ LLM –≤—ã—è–≤–ª—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —á–∞—Å—Ç–æ —É–ø—É—Å–∫–∞—é—Ç –∏–∑ –≤–∏–¥—É.</p>\n<ul>\n<li>\n<p><strong>–ö–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é:</strong> –ú–Ω–æ–≥–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫–æ–ª–µ–±–ª—é—Ç—Å—è –º–µ–∂–¥—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏–±–∫–æ–≥–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∑–∞—à—É–º–ª–µ–Ω–Ω—ã–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏ –∏–ª–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –ø–ª–æ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –Ω–æ –º–µ–Ω—å—à–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å—é. –ò–∑ 9 –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 6 –≤—ã–±–∏—Ä–∞—é—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ —á–∞—Å—Ç–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö. –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–∞–∫–∂–µ –≤—ã–±—Ä–∞–ª–∞ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –Ω–æ –∞–≤—Ç–æ—Ä—ã —É—á–∏—Ç—ã–≤–∞—é—Ç –≤–ª–∏—è–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞; –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å—Ç–æ—á–Ω–∏–∫ –í–∏–∫–∏–ø–µ–¥–∏–∏ —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º –≤—Ä—É—á–Ω—É—é —à—É–º–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∏–∑–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ù–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø—ã—Ç–∞–ª–∏—Å—å —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É. LongBench –Ω–µ–¥–∞–≤–Ω–æ –æ–±–Ω–æ–≤–∏–ª v2, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–µ–Ω—å—à–∏–π –º–∞—Å—à—Ç–∞–±, LongBench v2 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–µ—Ä–≤–æ–π –≤–µ—Ä—Å–∏–µ–π. LongRAG –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–∞—Å—Å–∏–≤–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ –Ω–∞–∑–Ω–∞—á–∞–µ—Ç –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–º—É –≤–æ–ø—Ä–æ—Å—É. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ—ç—Ç–æ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è.</p>\n</li>\n<li>\n<p><strong>–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:</strong> –ü—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ RAG –∏ LC, RAG —á–∞—Å—Ç–æ –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –∏–∑-–∑–∞ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∏–∑–≤–ª–µ—á–µ–Ω–∏—è. –ò–∑ 9 –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 5 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É—é—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è–º–∏, —Ç–æ–ª—å–∫–æ 2 –ø—Ä–æ–±—É—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –∏ –Ω–∏ –æ–¥–Ω–æ –Ω–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫–∞–∫–æ–π-–ª–∏–±–æ –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, –≤—ã—Ö–æ–¥—è—â–∏–π –∑–∞ —Ä–∞–º–∫–∏ –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –•–æ—Ç—è –∞–≤—Ç–æ—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É—é—Ç —Å –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–Ω–∏ –Ω–µ –º–æ–≥—É—Ç –æ–±–µ—â–∞—Ç—å, —á—Ç–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∏–º–∏ –º–µ—Ç–æ–¥ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è. –î–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RAG –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —á–∞—Å—Ç–∏ –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è RAG. OP-RAG –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ LC LLM-RAG –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–º–µ—â–∞—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–º–∏ –æ—Ü–µ–Ω–∫–∞–º–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ. –í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è–º, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (IR), —Ç–∞–∫–∏–µ –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤, –º–æ–≥—É—Ç –µ—â—ë –±–æ–ª—å—à–µ –ø–æ–≤—ã—Å–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å RAG, –Ω–æ –æ–Ω–∏ –±—ã–ª–∏ —É–ø—É—â–µ–Ω—ã –∏–∑ –≤–∏–¥—É –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö.</p>\n</li>\n<li>\n<p><strong>–í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã:</strong> –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø—Ä–æ–≤–æ–¥—è—Ç —Ç–µ—Å—Ç—ã –Ω–∞ 6-8 –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â–∏–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ, –∫–æ–≥–¥–∞ –Ω–æ–≤—ã–µ LLM —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤—ã–ø—É—Å–∫–∞—é—Ç—Å—è –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ª—é–±–∞—è —Ä–∞–±–æ—Ç–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ, –ø–æ—Å–∫–æ–ª—å–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ø—Ä–∏–º–µ–Ω–∏–º—ã —Ç–æ–ª—å–∫–æ –∫ –æ–¥–Ω–æ–π –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –º–æ–¥–µ–ª—è–º. –°—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ä–∞–±–æ—Ç LC RAG Performance –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π (20). –•–æ—Ç—è –∏—Ö —É—Å–∏–ª–∏—è –∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω—ã, –æ–Ω–∏... (–¥–∞–ª–µ–µ –≤ —Ç–µ–∫—Å—Ç–µ).</p>\n</li>\n</ul>"
            },
            {
                "title": "Conclusion",
                "content": "In this paper, we survey existing studies comparing or combining LC and RAG, analyzing why different implementations may result in some conflicts among their insights. Therefore, we present thorough comparison of LC and RAG approaches by leveraging diverse set of long context QA datasets. We filtered out questions that could be answered from parametric knowledge, ensuring fair comparison by focusing on questions that required external context. Along these lines, we have developed systematic filtering and evaluation process, identified the best retrieval method, and expanded the dataset to provide statistically significant basis for analysis. The results indicate that LC generally outperforms RAG for tasks involving wellstructured, dense contextssuch as Wikipedia articles and booksand is better at answering questions requiring specific information. By contrast, RAG demonstrates advantages in handling fragmented information, particularly in dialogue-based scenarios and for more general questions. Beyond merely presenting the experimental results and findings, we delve deeper into the concept of long context and examine how LC and RAG should be compared. Our discussion aims to ensure that the insights gained are more impactful and applicable to real-world scenarios.",
                "summary": "<p>–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –ø—Ä–æ–≤–æ–¥–∏–º –æ–±–∑–æ—Ä –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Å—Ä–∞–≤–Ω–∏–≤–∞—é—â–∏—Ö –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏—Ö –º–µ—Ç–æ–¥—ã Long Context (LC) –∏ Retrieval-Augmented Generation (RAG). –ú—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏—á–∏–Ω—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ä–∞–∑–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –º–æ–≥—É—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–º –≤—ã–≤–æ–¥–∞–º. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ LC –∏ RAG –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è QA (–≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º) —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. </p>\n<p>–ß—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —á–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –º—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–ª–∏ –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ \"–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è\" (—Ç–æ –µ—Å—Ç—å –∑–Ω–∞–Ω–∏—è, —É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ—Å—è –≤ –º–æ–¥–µ–ª–∏). –ú—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–ª–∏—Å—å –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –≤–Ω–µ—à–Ω–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –î–ª—è —ç—Ç–æ–≥–æ –º—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –æ—Ü–µ–Ω–∫–∏, –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ —Ä–∞—Å—à–∏—Ä–∏–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ LC –≤ —Ü–µ–ª–æ–º –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç RAG –≤ –∑–∞–¥–∞—á–∞—Ö, –≥–¥–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω –∏ –ø–ª–æ—Ç–Ω—ã–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç–∞—Ç—å–∏ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏ –∫–Ω–∏–≥–∏). LC –ª—É—á—à–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞–ø—Ä–æ—Ç–∏–≤, RAG –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∏ –¥–ª—è –±–æ–ª–µ–µ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.</p>\n<p>–ü–æ–º–∏–º–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –º—ã —É–≥–ª—É–±–ª—è–µ–º—Å—è –≤ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º, –∫–∞–∫ —Å–ª–µ–¥—É–µ—Ç —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å LC –∏ RAG. –¶–µ–ª—å –Ω–∞—à–µ–≥–æ –æ–±—Å—É–∂–¥–µ–Ω–∏—è ‚Äì —Å–¥–µ–ª–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã–º–∏ –∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–º–∏ –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Å—Ü–µ–Ω–∞—Ä–∏—è–º.</p>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong>\n*   <strong>Long Context (LC)</strong> –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–æ–¥–µ–ª—è–º, —Å–ø–æ—Å–æ–±–Ω—ã–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞.\n*   <strong>Retrieval-Augmented Generation (RAG)</strong> ‚Äì —ç—Ç–æ –ø–æ–¥—Ö–æ–¥, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.\n*   <strong>–ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞–Ω–∏—è</strong> ‚Äì —ç—Ç–æ –∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å \"–≤—ã—É—á–∏–ª–∞\" –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –∏ —Ö—Ä–∞–Ω–∏—Ç –≤ —Å–≤–æ–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö.</p>"
            }
        ]
    },
    {
        "id": "2412.14161",
        "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks",
        "url": "https://arxiv.org/pdf/2412.14161",
        "abstract": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-18",
        "pub_date_card": {
            "ru": "18 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 18",
            "zh": "12Êúà18Êó•"
        },
        "hash": "4284432422625536",
        "authors": [
            "Frank F. Xu",
            "Yufan Song",
            "Boxuan Li",
            "Yuxuan Tang",
            "Kritanjali Jain",
            "Mengxue Bao",
            "Zora Z. Wang",
            "Xuhui Zhou",
            "Zhitong Guo",
            "Murong Cao",
            "Mingyang Yang",
            "Hao Yang Lu",
            "Amaad Martin",
            "Zhe Su",
            "Leander Maben",
            "Raj Mehta",
            "Wayne Chi",
            "Lawrence Jang",
            "Yiqing Xie",
            "Shuyan Zhou",
            "Graham Neubig"
        ],
        "affiliations": [
            "Carnegie Mellon University",
            "Duke University",
            "Independent"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.14161.jpg",
        "data": {
            "categories": [
                "#optimization",
                "#agi",
                "#agents",
                "#science",
                "#benchmark"
            ],
            "emoji": "ü§ñ",
            "ru": {
                "title": "–ò–ò-–∞–≥–µ–Ω—Ç—ã –≤ –æ—Ñ–∏—Å–µ: –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á",
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TheAgentCompany –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ, –∏–º–∏—Ç–∏—Ä—É—é—â–µ–π –Ω–µ–±–æ–ª—å—à—É—é –∫–æ–º–ø–∞–Ω–∏—é-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ —Ç–µ—Å—Ç–∏—Ä—É—é—Ç –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ø–ú) –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è –≤–µ–±-—Å–µ—Ä—Ñ–∏–Ω–≥, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ª—É—á—à–∏–µ –∞–≥–µ–Ω—Ç—ã —Å–ø–æ—Å–æ–±–Ω—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å 24% –∑–∞–¥–∞—á. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –ò–ò-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏, –Ω–æ —Å–ª–æ–∂–Ω—ã–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤—Å–µ –µ—â–µ –æ—Å—Ç–∞—é—Ç—Å—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —Å–∏—Å—Ç–µ–º."
            },
            "en": {
                "title": "Evaluating AI Agents: The Future of Work Automation",
                "desc": "This paper introduces TheAgentCompany, a benchmark designed to evaluate the performance of AI agents in completing work-related tasks similar to those of a digital worker. The study assesses how well these agents, powered by large language models (LLMs), can autonomously perform tasks such as web browsing, coding, and communication within a simulated software company environment. The results indicate that while 24% of tasks can be completed autonomously by the most effective agent, more complex tasks remain challenging for current AI systems. This research highlights the potential and limitations of AI in automating workplace functions, providing insights for industries considering AI integration."
            },
            "zh": {
                "title": "AI‰ª£ÁêÜÂä©ÂäõÂ∑•‰Ωú‰ªªÂä°Ëá™Âä®ÂåñÁöÑÊé¢Á¥¢",
                "desc": "Êú¨Êñá‰ªãÁªç‰∫Ü‰∏Ä‰∏™Âêç‰∏∫TheAgentCompanyÁöÑÂü∫ÂáÜÊµãËØïÔºåÁî®‰∫éËØÑ‰º∞‰∫∫Â∑•Êô∫ËÉΩ‰ª£ÁêÜÂú®ÊâßË°åÁúüÂÆûÂ∑•‰Ωú‰ªªÂä°‰∏≠ÁöÑË°®Áé∞„ÄÇÊàë‰ª¨ÂàõÂª∫‰∫Ü‰∏Ä‰∏™Ê®°ÊãüÂ∞èÂûãËΩØ‰ª∂ÂÖ¨Âè∏ÁöÑÁéØÂ¢ÉÔºåËÆæËÆ°‰∫ÜÂ§öÁßç‰ªªÂä°Ôºå‰ª£ÁêÜÂèØ‰ª•ÈÄöËøáÊµèËßàÁΩëÈ°µ„ÄÅÁºñÂÜô‰ª£Á†ÅÂíå‰∏éÂêå‰∫ãÊ≤üÈÄöÊù•ÂÆåÊàêËøô‰∫õ‰ªªÂä°„ÄÇÊµãËØïÁªìÊûúÊòæÁ§∫ÔºåÊúÄÂÖàËøõÁöÑ‰ª£ÁêÜËÉΩÂ§üËá™‰∏ªÂÆåÊàê24%ÁöÑ‰ªªÂä°ÔºåËøôË°®ÊòéÂú®ÁÆÄÂçï‰ªªÂä°ÁöÑËá™Âä®ÂåñÊñπÈù¢ÔºåÂΩìÂâçÁöÑËØ≠Ë®ÄÊ®°Âûã‰ª£ÁêÜË°®Áé∞ËâØÂ•Ω„ÄÇÂ∞ΩÁÆ°Â¶ÇÊ≠§ÔºåÂØπ‰∫éÊõ¥Â§çÊùÇÁöÑÈïøÊúü‰ªªÂä°ÔºåÁé∞ÊúâÁ≥ªÁªü‰ªçÁÑ∂Êó†Ê≥ïËÉú‰ªª„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω TheAgentCompany ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ AI-–∞–≥–µ–Ω—Ç–æ–≤, –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö –∑–∞–¥–∞—á–∏, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ —Ä–∞–±–æ—Ç–µ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞. </p>\n<p>–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º –º–∏—Ä–µ –º—ã –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ–º —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º–∏, –∏ –º–Ω–æ–≥–∏–µ —Ä–∞–±–æ—á–∏–µ –∑–∞–¥–∞—á–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–ø—å—é—Ç–µ—Ä–∞ –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å —ç—Ç–∏–º, –±–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–≤–∏—Ç–∏—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è AI-–∞–≥–µ–Ω—Ç—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥–æ–π –∏ –≤–ª–∏—è—Ç—å –Ω–∞ –Ω–µ–µ. –í–æ–∑–Ω–∏–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ AI-–∞–≥–µ–Ω—Ç—ã —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –∏–ª–∏ –¥–∞–∂–µ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á? –û—Ç–≤–µ—Ç –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å –≤–∞–∂–µ–Ω –∫–∞–∫ –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π, —Å—Ç—Ä–µ–º—è—â–∏—Ö—Å—è –≤–Ω–µ–¥—Ä–∏—Ç—å AI –≤ —Å–≤–æ–∏ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, —Ç–∞–∫ –∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –≤–ª–∏—è–Ω–∏–µ AI –Ω–∞ —Ä—ã–Ω–æ–∫ —Ç—Ä—É–¥–∞.</p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –∞–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫ TheAgentCompany. –û–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é —Å—Ä–µ–¥—É, –∏–º–∏—Ç–∏—Ä—É—é—â—É—é –Ω–µ–±–æ–ª—å—à—É—é IT-–∫–æ–º–ø–∞–Ω–∏—é —Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –≤–µ–±-—Å–∞–π—Ç–∞–º–∏ –∏ –¥–∞–Ω–Ω—ã–º–∏. –í —ç—Ç–æ–π —Å—Ä–µ–¥–µ —Å–æ–∑–¥–∞–Ω—ã —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ —Ç–∞–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏: –ø—Ä–æ—Å–º–æ—Ç—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞, –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º –∏ –æ–±—â–µ–Ω–∏–µ —Å –∫–æ–ª–ª–µ–≥–∞–º–∏.</p>\n<p>–ê–≤—Ç–æ—Ä—ã –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–∫ –Ω–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö API, —Ç–∞–∫ –∏ –Ω–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç —Å–ø–æ—Å–æ–±–µ–Ω –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å 24% –∑–∞–¥–∞—á. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö, –∏–º–∏—Ç–∏—Ä—É—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—á–µ–µ –º–µ—Å—Ç–æ, AI-–∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –ø—Ä–æ—Å—Ç—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –Ω–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —à–∞–≥–æ–≤, –ø–æ–∫–∞ –æ—Å—Ç–∞—é—Ç—Å—è –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.</p>"
            },
            {
                "title": "Introduction",
                "content": "We are in the midst of technological transformation. With the rapid year-by-year and month-bymonth progress brought about by large language models (LLMs), we are seeing AI-based assistance or automation become commonplace in tasks that were unthinkable only few years ago. In fact, the pace of progress is so fast that some have gone so far as to claim that the majority of human labor may be automatable within the next couple of years (Eloundou et al., 2023; Amodei & Fridman, 2024). On the other hand, others are skeptical, claiming that language models cannot truly reason (Kambhampati et al., 2024), do not generalize well to novel tasks (Chollet et al., 2024), and may only have an impact on small minority of the labor market (Wittenstein, 2024). What is the reason for this disconnect? We argue that it is, in part, due to lack of objective benchmarks that not only demonstrate the power of existing LLM-based agents to accelerate Equal contribution. 1 Preprint. It features reproducible and selfFigure 1: An overview of TheAgentCompany benchmark. hosted environment, simulated colleagues to test agent communication capabilities, checkpoint and execution-based evaluation, and set of 175 diverse, realistic and professional tasks in software engineering company setting. wide variety of repetitive tasks encountered in every-day workplaces, but also provide appropriate caveats about the tasks that agents cannot do. This is pressing issue, because the commercial and policy implications of diverse and effective acceleration or automation of work-related tasks will be broad, both positive (e.g. increase of quality of life and accelerated scientific discovery) and negative (e.g. potential displacement or loss of jobs and increase in wealth disparities). In this paper, we take some first steps towards resolving this gap and providing clearer view of where we are now with respect to acceleration or automation of consequential work-related tasks, and litmus test for future development in this direction. Concretely, we propose benchmark, TheAgentCompany (Figure 1) that estimates the ability of AI agents to perform tasks encountered in everyday workplaces. We create simulated software development company where agents must perform tasks related to software engineering, project management, financial analysis, and other typical tasks encountered in such business settings. The agents must browse the web, code, and interact with other simulated co-workers to achieve success on the provided tasks. TheAgentCompanys environment is based entirely on open-source software and self-hostable for reproducibility purposes, and we create rigorous evaluators that also assign partial credit when the agent gets the answer partially correct. We perform experiments using seven large language model backbones, including API-based models such as Anthropic Claude (Anthropic, 2023), OpenAI GPT-4o (OpenAI, 2024), Google Gemini (Team et al., 2023), Amazon Nova (Intelligence, 2024), as well as open models including Meta Llama (Dubey et al., 2024) and Alibaba Qwen (Yang et al., 2024). All models are run using the OpenHands agent framework (Wang et al., 2024b),1 which provides stable and strong agent harness for both web browsing and coding. As result of experiments, we find that the best performing model, Claude 3.5 Sonnet was able to autonomously perform 24.0% of the provided tests to completion, and achieve score of 34.4% on our metric that provides extra credit for partially completed tasks. These results present nuanced picture of the current ability of AI agents to perform tasks. Agents powered by the current gold-standard AI techniques are able to autonomously perform wide variety of tasks encountered in everyday work. However, they are not close to automating every task encountered in workspace, even on the subset of tasks presented in TheAgentCompany, which are well-scoped administrative and coding tasks encountered in software companys day-to-day work. In the rest of this paper, we explain detail comparisons to other existing benchmarks ( 2), how we set up realistic and reproducible environments ( 3), how we define tasks ( 4) and how we create them ( 5), our baseline agent ( 6), experimental results ( 7), and finally implications and future directions ( 8). 1https://github.com/All-Hands-AI/OpenHands 2 Preprint. is desktop, Table 1: Comparison of different AI agent benchmarks. Interface: the interface agent has access to; is web browser, is bash terminal. Supported Tasks: tasks in the benchmark, indicate tasks with no association with real-world occupations; SE refers to software engineering, HR is human resources, PM is project manager. Checkpoint-based evaluation: if tasks are evaluated at intermediate checkpoints and assigned partial scores. Interact with NPC Agents: If the agent can interact with other NPC agents during task-solving. is chat platform, is Python script, is API usage, Framework Diverse Real-world Work Task Categories Requires Interaction Long-Horizon w/ Checkpoints Interface Self-Hosted Environment MiniWob++ (Liu et al., 2018) Mind2Web (Deng et al., 2023) WebLINX (L√π et al., 2024) AssistantBench (Yoran et al., 2024) WebArena (Zhou et al., 2023) VisualWebArena (Koh et al., 2024) VideoWebArena (Jang et al., 2024) WorkArena (Drouin et al., 2024) OSWorld (Xie et al., 2024) Windows Agent Arena (Bonatti et al., 2024) AppWorld (Trivedi et al., 2024) Gorilla APIBench (Patil et al., 2023) œÑ -bench (Yao et al., 2024) SWE-bench (Jimenez et al., 2024) DevBench (Li et al., 2024) Smallville (Park et al., 2023) Sotopia (Zhou et al., 2024) TheAgentCompany (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) Browsing Browsing Browsing Browsing Browsing Browsing Browsing Enterprise Software Office, Coding Browsing, Office, Coding Daily Coding Retail, Airline SWE SWE Social Social SWE, HR, Admin, PM, Research, Finance (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34) (cid:34) (cid:37) (cid:34) (cid:37) (cid:34) (cid:34) (cid:34)",
                "summary": "<p>–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –º—ã –Ω–∞–±–ª—é–¥–∞–µ–º —Å—Ç—Ä–µ–º–∏—Ç–µ–ª—å–Ω—É—é —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é. –ë–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–æ–≥—Ä–µ—Å—Å—É –≤ –æ–±–ª–∞—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –º—ã –≤–∏–¥–∏–º, –∫–∞–∫ –ò–ò-–ø–æ–º–æ—â—å –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –æ–±—ã–¥–µ–Ω–Ω–æ—Å—Ç—å—é –≤ –∑–∞–¥–∞—á–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ª–µ—Ç –Ω–∞–∑–∞–¥ –∫–∞–∑–∞–ª–∏—Å—å –Ω–µ–º—ã—Å–ª–∏–º—ã–º–∏. –°–∫–æ—Ä–æ—Å—Ç—å —ç—Ç–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –Ω–∞—Å—Ç–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–∞, —á—Ç–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–∞–∂–µ —É—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —Ç—Ä—É–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –≤ –±–ª–∏–∂–∞–π—à–∏–µ –ø–∞—Ä—É –ª–µ—Ç. –û–¥–Ω–∞–∫–æ –¥—Ä—É–≥–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏, –∑–∞—è–≤–ª—è—è, —á—Ç–æ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –ø–ª–æ—Ö–æ –æ–±–æ–±—â–∞—é—Ç –∑–Ω–∞–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –∏ –º–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –ª–∏—à—å –Ω–∞ –Ω–µ–±–æ–ª—å—à—É—é —á–∞—Å—Ç—å —Ä—ã–Ω–∫–∞ —Ç—Ä—É–¥–∞.</p>\n<p>–ü—Ä–∏—á–∏–Ω–∞ —Ç–∞–∫–∏—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã—Ö –º–Ω–µ–Ω–∏–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è, –æ—Ç—á–∞—Å—Ç–∏, –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–∞–∑–µ LLM –≤ —É—Å–∫–æ—Ä–µ–Ω–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –Ω–æ –∏ –æ–±–æ–∑–Ω–∞—á–∏—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ, —É—á–∏—Ç—ã–≤–∞—è —à–∏—Ä–æ–∫–∏–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∂–∏–∑–Ω–∏ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç–∏–π), —Ç–∞–∫ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ—Ç–µ—Ä—è —Ä–∞–±–æ—á–∏—Ö –º–µ—Å—Ç –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –≤ –¥–æ—Ö–æ–¥–∞—Ö).</p>\n<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∞–≤—Ç–æ—Ä—ã –¥–µ–ª–∞—é—Ç –ø–µ—Ä–≤—ã–µ —à–∞–≥–∏ –∫ —Ä–µ—à–µ–Ω–∏—é —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–µ–¥–ª–∞–≥–∞—è –±–æ–ª–µ–µ —á–µ—Ç–∫–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–µ–∫—É—â–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ò–ò –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–∞–∂–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á. –û–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TheAgentCompany, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –î–ª—è —ç—Ç–æ–≥–æ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏–º–∏—Ç–∞—Ü–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–æ–π, —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø—Ä–æ–µ–∫—Ç–∞–º–∏, —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∏ –¥—Ä—É–≥–∏–º–∏ —Ç–∏–ø–∏—á–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–º–∏—Å—è –≤ —Ç–∞–∫–∏—Ö –±–∏–∑–Ω–µ—Å-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞—Ö. –ê–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ–±-–±—Ä–∞—É–∑–µ—Ä, –ø–∏—Å–∞—Ç—å –∫–æ–¥ –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ –¥–ª—è —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á.</p>\n<p>–°—Ä–µ–¥–∞ TheAgentCompany –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–∞ –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø—Ä–∏—Å—É–∂–¥–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π –±–∞–ª–ª, –µ—Å–ª–∏ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–∏–ª –∑–∞–¥–∞—á—É –ª–∏—à—å —á–∞—Å—Ç–∏—á–Ω–æ.</p>\n<p>–í —Ö–æ–¥–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Å–µ–º—å LLM, –≤–∫–ª—é—á–∞—è –º–æ–¥–µ–ª–∏ —Å –¥–æ—Å—Ç—É–ø–æ–º —á–µ—Ä–µ–∑ API, —Ç–∞–∫–∏–µ –∫–∞–∫ Anthropic Claude, OpenAI GPT-4o, Google Gemini, Amazon Nova, –∞ —Ç–∞–∫–∂–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ Meta Llama –∏ Alibaba Qwen. –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—Å–∫–∞–ª–∏—Å—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ OpenHands, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—É—é –∏ –Ω–∞–¥–µ–∂–Ω—É—é —Å—Ä–µ–¥—É –¥–ª—è —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤ —Å –≤–µ–±-–±—Ä–∞—É–∑–µ—Ä–æ–º –∏ –∫–æ–¥–æ–º.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å, Claude 3.5 Sonnet, —Å–º–æ–≥–ª–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å 24% –∑–∞–¥–∞–Ω–∏–π –∏ –Ω–∞–±—Ä–∞—Ç—å 34,4% –±–∞–ª–ª–æ–≤ —Å —É—á–µ—Ç–æ–º —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –≠—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ —Ç–µ–∫—É—â–∏–µ –ò–ò-–∞–≥–µ–Ω—Ç—ã —Å–ø–æ—Å–æ–±–Ω—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏—Ö—Å—è –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –û–¥–Ω–∞–∫–æ –æ–Ω–∏ –µ—â–µ –¥–∞–ª–µ–∫–∏ –æ—Ç –ø–æ–ª–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á, –¥–∞–∂–µ –≤ —Ä–∞–º–∫–∞—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –∑–∞–¥–∞—á TheAgentCompany, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ –∫–æ–¥–∏–Ω–≥–æ–≤—ã–µ –∑–∞–¥–∞—á–∏, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç–µ –∫–æ–º–ø–∞–Ω–∏–∏-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞.</p>\n<p>–í –æ—Å—Ç–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–∏ —Å—Ç–∞—Ç—å–∏ –∞–≤—Ç–æ—Ä—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—é—Ç —Å–≤–æ–π –±–µ–Ω—á–º–∞—Ä–∫ —Å –¥—Ä—É–≥–∏–º–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ (—Ä–∞–∑–¥–µ–ª 2), –æ–ø–∏—Å—ã–≤–∞—é—Ç —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–π –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–π —Å—Ä–µ–¥—ã (—Ä–∞–∑–¥–µ–ª 3), –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –∑–∞–¥–∞—á–∏ (—Ä–∞–∑–¥–µ–ª 4) –∏ –ø—Ä–æ—Ü–µ—Å—Å –∏—Ö —Å–æ–∑–¥–∞–Ω–∏—è (—Ä–∞–∑–¥–µ–ª 5), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∞–≥–µ–Ω—Ç–∞ (—Ä–∞–∑–¥–µ–ª 6), –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (—Ä–∞–∑–¥–µ–ª 7), –∞ —Ç–∞–∫–∂–µ –æ–±—Å—É–∂–¥–∞—é—Ç –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π (—Ä–∞–∑–¥–µ–ª 8).</p>"
            },
            {
                "title": "Benchmark Desiderata And Comparison To Other Benchmarks",
                "content": "In order to evaluate the ability of agents to perform tasks in complex real-world settings, we built TheAgentCompany with number of desiderata in mind. The comparison with several existing prominent agent benchmarks with respect to these desiderata is in Table 1. Coverage of Multiple Work-related Tasks: In order to make any valid statements about the potential of AI to accelerate or automate various types of real-world work, we should have tasks that are motivated by real-world work across multiple job categories. Many benchmarks are not relevant to real-world work (e.g. MiniWob++ (Liu et al., 2018)) or very relevant to real-world work, but only over limited scope of tasks (e.g. SWE-Bench (Jimenez et al., 2024)). In contrast, TheAgentCompany contains set of more diverse, realistic, and professional tasks that would typically be completed by multiple job roles in software engineering company. Requirement for Interaction: If agents are to integrate into real-world workplaces, they will need to communicate with the other human members of the workspace. Most other benchmarks do not measure communication or interactivity, with the exception of œÑ -bench (Yao et al., 2024), which only measures interaction in customer service scenarios. TheAgentCompany provides better testbed for communication as many tasks involve asking and providing information to colleagues as part of more complex task. Long-horizon Tasks with Checkpoints: In real-world settings, many tasks require taking many different steps to achieve higher-level goal. One major novel contribution of TheAgentCompany is that we both (1) contain tasks that require an agent to perform significantly more consecutive work (i.e. involving more steps and realistically taking human professionals longer to accomplish) than previous benchmarks, and (2) provide granular evaluators that measure the ability of models to perform subtasks of these larger tasks. Versatile Environment Interface: In order to handle diversity of tasks in real-world settings, we minimally should be able to interact with the tools that real-world workers use including web interfaces, programs, command-line terminals, and communication tools. TheAgentCompany covers all of these interfaces, while most previous benchmarks focus only on one or two. Preprint. Self-hosted and Reproducible: In order to allow for careful comparisons between different methods that remain constant over time, the benchmark should be fully self-hosted and reproducible. This contrasts with existing benchmarks that do not have execution environments (e.g. Mind2Web (Deng et al., 2023)) or require the usage of third-party software (e.g. WorkArena (Drouin et al., 2024)).",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–æ–≤ –≤ —É—Å–ª–æ–≤–∏—è—Ö, –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã—Ö –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Ä–∞–±–æ—á–∏–º –∑–∞–¥–∞—á–∞–º, –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º TheAgentCompany. –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –≤—ã–¥–µ–ª–∏–ª–∏ —Ä—è–¥ –∫–ª—é—á–µ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ —Ç–∞–∫–æ–º—É –±–µ–Ω—á–º–∞—Ä–∫—É, –∏ —Å—Ä–∞–≤–Ω–∏–ª–∏ TheAgentCompany —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏ (—Å–º. –¢–∞–±–ª–∏—Ü—É 1 –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ).</p>\n<p><strong>–û—Ö–≤–∞—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö –∑–∞–¥–∞—á:</strong> –î–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–µ –ò–ò –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–æ–ª—è–º. –ú–Ω–æ–≥–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ª–∏–±–æ –Ω–µ —Å–≤—è–∑–∞–Ω—ã —Å —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–æ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, MiniWob++), –ª–∏–±–æ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –ª–∏—à—å —É–∑–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á (–Ω–∞–ø—Ä–∏–º–µ—Ä, SWE-Bench). TheAgentCompany, –Ω–∞–ø—Ä–æ—Ç–∏–≤, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á, —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–æ–ª–µ–π –≤ IT-–∫–æ–º–ø–∞–Ω–∏–∏.</p>\n<p><strong>–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:</strong> –î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –æ–±—â–∞—Ç—å—Å—è —Å –ª—é–¥—å–º–∏. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –Ω–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—é –∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º œÑ-bench, –∫–æ—Ç–æ—Ä—ã–π —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –≤ —Å—Ñ–µ—Ä–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤. TheAgentCompany –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ä–µ–¥—É –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏, –ø–æ—Å–∫–æ–ª—å–∫—É –º–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É—é—Ç –æ–±–º–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π —Å –∫–æ–ª–ª–µ–≥–∞–º–∏ –≤ —Ä–∞–º–∫–∞—Ö –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.</p>\n<p><strong>–ó–∞–¥–∞—á–∏ —Å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–æ–º –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º–∏ —Ç–æ—á–∫–∞–º–∏:</strong> –í —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –º–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —à–∞–≥–æ–≤ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –æ–±—â–µ–π —Ü–µ–ª–∏. –í–∞–∂–Ω—ã–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏–µ–º TheAgentCompany —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–ª–∏—á–∏–µ –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ—Ç –∞–≥–µ–Ω—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ–≥–æ –æ–±—ä–µ–º–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã (—Ç.–µ. –±–æ–ª—å—à–µ —à–∞–≥–æ–≤ –∏ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, —Å—Ä–∞–≤–Ω–∏–º–æ–≥–æ —Å –≤—Ä–µ–º–µ–Ω–µ–º, –∑–∞—Ç—Ä–∞—á–∏–≤–∞–µ–º—ã–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∞–º–∏). –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –∏–∑–º–µ—Ä–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤—ã–ø–æ–ª–Ω—è—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏.</p>\n<p><strong>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å—Ä–µ–¥—ã:</strong> –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –∑–∞–¥–∞—á –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —É–º–µ—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ª—é–¥–∏, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã, –ø—Ä–æ–≥—Ä–∞–º–º—ã, –∫–æ–º–∞–Ω–¥–Ω—É—é —Å—Ç—Ä–æ–∫—É –∏ —Å—Ä–µ–¥—Å—Ç–≤–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏. TheAgentCompany –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —ç—Ç–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —Ñ–æ–∫—É—Å–∏—Ä—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–¥–Ω–æ–º –∏–ª–∏ –¥–≤—É—Ö.</p>\n<p><strong>–°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π –±–µ–Ω—á–º–∞—Ä–∫:</strong> –î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ—ç—Ç–æ–º—É –±–µ–Ω—á–º–∞—Ä–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º. –≠—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç TheAgentCompany –æ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏–º–µ—é—Ç —Å—Ä–µ–¥—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, Mind2Web) –∏–ª–∏ —Ç—Ä–µ–±—É—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω–Ω–µ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, WorkArena).</p>"
            },
            {
                "title": "TheAgentCompany Environment Setup",
                "content": "Our benchmark is set in an imaginary software engineering startup called TheAgentCompany, hence the benchmarks name. Within TheAgentCompany, we create tasks inspired by tasks handled by workers inside such companies. More details about the companys imaginary background, overview and employees can be found in Appendix A. The benchmark environment contains multiple components. Local Workspace The local workspace runs locally on the agents host, which is analogous to human professionals local workspace, e.g. their work laptop computer. This environment is created as sandboxed Docker environment to provide safe execution environment that will not affect other parts of the evaluation machine (Wang et al., 2024b).2 This environment is where agents work on the task, and within this environment the TheAgentCompany baseline agent ( 6) uses browser, code editor and Linux terminal with typical software preinstalled.3 Intranet This part of the environment mimics the companys internal websites that host code, documents, project management software, and communications software. To achieve our goal of reproducible, self-contained environment, we follow WebArena (Zhou et al., 2023), in using open-source, self-hostable software to host our environment. The environment mainly contains the following websites: 1. GitLab,4 an open-source alternative to source-code repositories such as GitHub. This is used for hosting TheAgentCompanys code repositories and tech-oriented wiki pages. 2. OwnCloud,5 an open-source alternative to office software such as Google Drive or Microsoft Office. This to save and share files, especially for document storage and collaborative editing. 3. Plane,6 an open-source alternative to task management software such as Jira or Linear. This is used to track issues, run sprints cycles, and manage product roadmaps. 4. RocketChat,7 an open-source alternative to communication software such as Slack. This is company-internal real-time messaging tool that facilitates collaboration between employees. All the websites hosted are reproducible and reset-able with mock data inspired by that from software engineering company. The data inside these company internal websites are populated with real-world software project data, as well as data manually curated by co-authors who have some experience in the relevant corporate roles. Simulated Colleague Communication One major aspect of working in company is communicating with other company members, and in TheAgentCompany we also test the ability of models to perform this type of communication. Specifically, we allow agents to use RocketChat to message other company members and obtain information that may not be available in the original task description. To create these simulated colleagues, we rely on the Sotopia platform (Zhou et al., 2024), which supports the creation of simulated human characters with LLMs. Each simulated colleague is equipped with detailed profile that includes their name, role, responsibilities, and project affiliations. 2https://docs.all-hands.dev/modules/usage/how-to/custom-sandbox-guide 3Other options would include using something like GUI-based desktop environment with office software (Xie et al., 2024), but we opt to build baseline solution that is entirely web-based, reflecting the recent trend of more enterprise software moving to the cloud. 4https://about.gitlab.com/install/ 5https://doc.owncloud.com/ 6https://github.com/makeplane/plane 7https://www.rocket.chat/install 4 Preprint. Domain SWE Table 2: Example task intents and checkpoints for three domains. Task Intent Checkpoints Set up JanusGraph and run it locally with an HTTP endpoint: Clone JanusGraph directory under /workspace folder (http://the-agent-company.com:8929/root/janusgraph). Build the binary file. Launch JanusGraph server locally on port 8182 with an HTTP endpoint. Finance Navigate to ownCloud at http://the-agent-company.com:8092 and complete Section BAlternative Simplified Credit of IRS Form 6765: Gather necessary information from /Documents/Financials/TAC_financials.csv and /workspace/research_wages.csv. Consult /Documents/Financials/f6765_instructions.pdf for instructions. Contact the finance director (David Wong) on Rocket.Chat (http://the-agent-company.com:3000/home) for ambiguous questions. Save the filled form as /workspace/filled_f6765.pdf. Checkpoint 1 (1pt): Check if JanusGraph repo is cloned. Checkpoint 2 (3pts): Check if the binary file is built (requires skipping Docker in pom.xml, hence higher points). Checkpoint 3 (2pts): Check the JanusGraph Server as an HTTP endpoint. Checkpoint 1 (5pts): Check if all 16 questions in Section of the form have been answered correctly. Checkpoint 2 (3pts): Check if the correct finance director (David Wong) was contacted to answer two ambiguous questions. PM Analyze The Agent Companys performance and create summary in Plane: Access Plane (http://the-agent-company.com:8091/tac/) and navigate to \"Analytics.\" Collect metrics: Open Tasks, Backlog Tasks, Unstarted Tasks, Started Tasks, Unassigned Issues, Pending Issues. Create summary and share it on Rocket.Chat (http://the-agent-company.com:3000/home) in the #kudos channel. Checkpoint 1 (1pt): Check if Plane was accessed and the agent navigated to \"Analytics\" section. Checkpoint 2 (3pts): Check if all required project metrics were collected. Checkpoint 3 (1pt): Check if the summary was shared in the #kudos channel on Rocket.Chat. (e.g., Sarah Johnson, who serves as the CTO, oversees technical strategy planning and R&D team leadership, with access to all technical channels). Agents can interact with these simulated colleagues through direct messages or in specific channels, as is standard in RocketChat and other platforms. By default, all simulated human characters are backed by the Claude-3-5-Sonnet-20241022 LLM across experiments, as we found that it provided the best results during preliminary experiments. For example conversations between the agent and the simulated colleagues drawn from empirical experiments, please refer to Appendix B.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Å—Ä–µ–¥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∏–º–∏—Ç–∏—Ä—É—é—â–∞—è —Ä–∞–±–æ—Ç—É –≤ –≤—ã–º—ã—à–ª–µ–Ω–Ω–æ–π IT-–∫–æ–º–ø–∞–Ω–∏–∏ TheAgentCompany. –≠—Ç–∞ —Å—Ä–µ–¥–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:</p>\n<p><strong>1. –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—á–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ:</strong> –≠—Ç–æ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è Docker-—Å—Ä–µ–¥–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—á–∏–π –∫–æ–º–ø—å—é—Ç–µ—Ä —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞. –í —ç—Ç–æ–π —Å—Ä–µ–¥–µ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è –±—Ä–∞—É–∑–µ—Ä, —Ä–µ–¥–∞–∫—Ç–æ—Ä –∫–æ–¥–∞ –∏ —Ç–µ—Ä–º–∏–Ω–∞–ª Linux —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è.</p>\n<p><strong>2. –ò–Ω—Ç—Ä–∞–Ω–µ—Ç:</strong> –≠—Ç–æ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –∏–º–∏—Ç–∏—Ä—É–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç—ã –∫–æ–º–ø–∞–Ω–∏–∏, –≥–¥–µ —Ä–∞–∑–º–µ—â–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:\n    * <strong>GitLab:</strong> –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.\n    * <strong>OwnCloud:</strong> –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\n    * <strong>Plane:</strong> –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∞–º–∏, —Å–ø—Ä–∏–Ω—Ç–∞–º–∏ –∏ –ø–ª–∞–Ω–∞–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.\n    * <strong>RocketChat:</strong> –¥–ª—è –æ–±—â–µ–Ω–∏—è –º–µ–∂–¥—É —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.</p>\n<p>–í—Å–µ —ç—Ç–∏ –≤–µ–±-—Å–∞–π—Ç—ã —è–≤–ª—è—é—Ç—Å—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ—Ç–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–º–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã –∏ —Ä–∞–±–æ—á–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏. –î–∞–Ω–Ω—ã–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ IT-–ø—Ä–æ–µ–∫—Ç–æ–≤, —Ç–∞–∫ –∏ –¥–∞–Ω–Ω—ã–º–∏, —Å–æ–∑–¥–∞–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–∞–º–∏ —Å—Ç–∞—Ç—å–∏, –∏–º–µ—é—â–∏–º–∏ –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö —Ä–æ–ª—è—Ö.</p>\n<p><strong>3. –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—â–µ–Ω–∏—è —Å –∫–æ–ª–ª–µ–≥–∞–º–∏:</strong> –í–∞–∂–Ω–æ–π —á–∞—Å—Ç—å—é —Ä–∞–±–æ—Ç—ã –≤ –∫–æ–º–ø–∞–Ω–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –æ–±—â–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏. –í TheAgentCompany –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ–±—â–∞—Ç—å—Å—è —Å –∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏ —á–µ—Ä–µ–∑ RocketChat. –≠—Ç–∏ \"–∫–æ–ª–ª–µ–≥–∏\" —Å–æ–∑–¥–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã Sotopia –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π LLM-–º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ –ø—Ä–æ—Ñ–∏–ª—è–º–∏, –≤–∫–ª—é—á–∞—é—â–∏–º–∏ –∏–º—è, –¥–æ–ª–∂–Ω–æ—Å—Ç—å, –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–∏—á–∞—Å—Ç–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç–∞–º. –ê–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ –Ω–∏–º –∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –æ–ø–∏—Å–∞–Ω–∏–∏ –∑–∞–¥–∞—á–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ LLM –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –∫–æ–ª–ª–µ–≥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Claude-3-5-Sonnet-20241022, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –ø–æ–∫–∞–∑–∞–ª –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö.</p>\n<p>–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã –∑–∞–¥–∞—á –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ –¥–ª—è —Ç—Ä–µ—Ö –æ–±–ª–∞—Å—Ç–µ–π: —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (SWE), —Ñ–∏–Ω–∞–Ω—Å—ã –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ (PM). –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —É–∫–∞–∑–∞–Ω—ã –¥–µ–π—Å—Ç–≤–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–≥–µ–Ω—Ç, –∞ —Ç–∞–∫–∂–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –µ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å.</p>"
            },
            {
                "title": "Task Structure",
                "content": "The tasks in TheAgentCompany include task intent, list of checkpoints that the agent must achieve, programmatic evaluator to check success on these checkpoints, and code to initialize and finalize the environment. We show some examples in Table 2, and describe each of aspect in detail below. Task Intent Each task begins with an English description, simulating how user would instruct an LLM-based agent to perform real-world task. In general, we aim for these tasks to be clear enough so that human worker would be able to complete the task without asking for further instructions directly from the user (although they may need to ask questions of their other co-workers). Checkpoints Tasks are divided into checkpoints representing intermediate milestones, each assigned point value to measure progress. Each checkpoint is awarded certain number of points based on its significance to the overall completion of the task. Checkpoints are written in English, and typically specify one or more of the following: 5 Preprint. Action Completion: Verifying whether required actions, such as using tools, navigating to URLs, or collecting data, were carried out successfully. Data Accuracy: Evaluating the correctness and completeness of the output, such as extracted data or formatted documents. Collaboration: Assessing interactions with simulated colleagues or sharing of output, such as posting messages or asking for additional information to complete the task. Evaluators Checkpoints are created in the task design phase, but for actual evaluation, each of the checkpoints must be concretely implemented through an evaluator program that checks the completion of the checkpoint. These evaluators are implemented by examining environment states, such as the local workspace, intranet status, simulated colleague interactions, or by analyzing agent trajectories, like verifying browsing history or action sequences. In most cases, these evaluators are deterministic and written as simple Python functions. For instance, in the SWE task in Table 2, the checkpoints are deterministic: verifying if the JanusGraph repository is cloned, the binary file is built, and the server is launched with an HTTP endpoint. However, for tasks with more complex and unstructured deliverables, such as in Table 2, the last checkpoint in the Finance task requires contacting the correct finance director (David Wong) to resolve ambiguous questions, which involves judgment from (simulated) human colleague, deterministic evaluation can be challenging due to subjectivity and variability. In such cases, we employ LLM-based evaluation. This involves prompting LLMs with predefined rubrics or reference outputs to assess the agents deliverables, enabling more nuanced and flexible evaluation of these tasks. Same as the NPC backbone, all LLM-based evaluators are backed by the Claude-3-5-Sonnet-20241022.",
                "summary": "<p>–í TheAgentCompany –∑–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ (task intent), —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫ (checkpoints), –∫–æ—Ç–æ—Ä—ã–µ –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –¥–æ—Å—Ç–∏—á—å, –ø—Ä–æ–≥—Ä–∞–º–º–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ (evaluator) –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —ç—Ç–∏—Ö —Ç–æ—á–µ–∫, –∞ —Ç–∞–∫–∂–µ –∫–æ–¥ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ä–µ–¥—ã.</p>\n<p><strong>–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ (Task Intent)</strong></p>\n<p>–ö–∞–∂–¥–∞—è –∑–∞–¥–∞—á–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –∫–æ—Ç–æ—Ä–æ–µ –∏–º–∏—Ç–∏—Ä—É–µ—Ç —Ç–æ, –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–≥ –±—ã –ø—Ä–æ–∏–Ω—Å—Ç—Ä—É–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞ –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM), –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–¥–∞—á–∏. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –æ–ø–∏—Å–∞–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–Ω—è—Ç–Ω—ã–º–∏ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –º–æ–≥ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É, –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —Ö–æ—Ç—è –æ–Ω –º–æ–∂–µ—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∫ –∫–æ–ª–ª–µ–≥–∞–º.</p>\n<p><strong>–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ (Checkpoints)</strong></p>\n<p>–ó–∞–¥–∞—á–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏–µ —Å–æ–±–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —ç—Ç–∞–ø—ã. –ö–∞–∂–¥–æ–π —Ç–æ—á–∫–µ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–ª–ª–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –µ–µ –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è –æ–±—â–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Ç–∞–∫–∂–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –∏ –æ–±—ã—á–Ω–æ –≤–∫–ª—é—á–∞—é—Ç –ø—Ä–æ–≤–µ—Ä–∫—É –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤:</p>\n<ul>\n<li><strong>–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π:</strong> –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≥–æ, –±—ã–ª–∏ –ª–∏ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–µ—Ä–µ—Ö–æ–¥ –ø–æ URL-–∞–¥—Ä–µ—Å–∞–º –∏–ª–∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.</li>\n<li><strong>–¢–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:</strong> –û—Ü–µ–Ω–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.</li>\n<li><strong>–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:</strong> –û—Ü–µ–Ω–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –º–æ–¥–µ–ª–∏—Ä—É–µ–º—ã–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏ –∏–ª–∏ –æ–±–º–µ–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É–±–ª–∏–∫–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –∏–ª–∏ –∑–∞–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n</ul>\n<p><strong>–û—Ü–µ–Ω—â–∏–∫–∏ (Evaluators)</strong></p>\n<p>–•–æ—Ç—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏, –¥–ª—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞–∂–¥–∞—è –∏–∑ –Ω–∏—Ö –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–≥—Ä–∞–º–º—É-–æ—Ü–µ–Ω—â–∏–∫, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ. –≠—Ç–∏ –æ—Ü–µ–Ω—â–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ä–µ–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—á–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç—Ä–∞—Å–µ—Ç–∏, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –º–æ–¥–µ–ª–∏—Ä—É–µ–º—ã–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏, –∏–ª–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–∞, –ø—Ä–æ–≤–µ—Ä—è—è –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π.</p>\n<p>–í –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ —Å–ª—É—á–∞–µ–≤ –æ—Ü–µ–Ω—â–∏–∫–∏ —è–≤–ª—è—é—Ç—Å—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ –≤–∏–¥–µ –ø—Ä–æ—Å—Ç—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ —è–∑—ã–∫–µ Python. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ SWE, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ —Ç–∞–±–ª–∏—Ü–µ 2, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è JanusGraph, —Å–±–æ—Ä–∫–∏ –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ —Å HTTP-–∫–æ–Ω–µ—á–Ω–æ–π —Ç–æ—á–∫–æ–π.</p>\n<p>–û–¥–Ω–∞–∫–æ –¥–ª—è –∑–∞–¥–∞—á —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—è—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞ –≤ –∑–∞–¥–∞—á–µ Finance (—Ç–∞–±–ª–∏—Ü–∞ 2), –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç —Å–≤—è–∑–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º (David Wong) –¥–ª—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω–∞ –∏–∑-–∑–∞ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç–∏. –í —Ç–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è LLM —Å –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –∏–ª–∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –±–æ–ª–µ–µ —Ç–æ–Ω–∫—É—é –∏ –≥–∏–±–∫—É—é –æ—Ü–µ–Ω–∫—É —ç—Ç–∏—Ö –∑–∞–¥–∞—á. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å NPC, –≤—Å–µ –æ—Ü–µ–Ω—â–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –∏—Å–ø–æ–ª—å–∑—É—é—Ç Claude-3-5-Sonnet-20241022.</p>"
            },
            {
                "title": "Result",
                "content": "+ 0.5 Sfull, where: Result: Sum of awarded points across all checkpoints (including partial credit), Total: Sum of the total points for all checkpoints, Result Total : Fractional progress toward full completion, Sfull: Binary indicator equal to 1 when the task is fully completed. This formulation ensures that agents are awarded partial credit in proportion to the points achieved, reflecting their progress toward task completion. At the same time, full task completion is strongly incentivized by incorporating an additional 50% credit, which is awarded only when all checkpoints are successfully completed. This design ensures that agents achieving partial progress receive scores scaled linearly with their performance, while those reaching 100% completion are distinctly rewarded to emphasize the importance of achieving the end goal. Preprint. Figure 2: Example TheAgentCompany workflow illustrating an agent managing sprint for the RisingWave project. The task involves identifying and moving unfinished issues to next sprint cycle, notifying assignees of those issues, running code coverage script, uploading summarized report to OwnCloud, and incorporating feedback on report from simulated project manager. Number of steps The number of steps is defined as the total number of LLM calls made during the task execution. This metric quantifies the operational effort required to perform the task. Cost per instance The cost per instance measures the monetary cost of querying the underlying LLM through its API to complete task. Assuming no prompt caching, the cost is calculated as: Cost = (Prompt token countPrompt token cost)+(Completion token countCompletion token cost). This efficiency metric reflects the computational expense of task completion based on token usage.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤, –≤—ã–ø–æ–ª–Ω—è—é—â–∏—Ö –∑–∞–¥–∞—á–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫ –∏ –ø–æ–ª–Ω–æ–µ –µ—ë –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ. </p>\n<p>–û—Ü–µ–Ω–∫–∞ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ —Ñ–æ—Ä–º—É–ª–µ: <strong>–†–µ–∑—É–ª—å—Ç–∞—Ç = –°—É–º–º–∞ –Ω–∞–±—Ä–∞–Ω–Ω—ã—Ö –±–∞–ª–ª–æ–≤ + 0.5 * Sfull</strong>, –≥–¥–µ:</p>\n<ul>\n<li><strong>–°—É–º–º–∞ –Ω–∞–±—Ä–∞–Ω–Ω—ã—Ö –±–∞–ª–ª–æ–≤</strong> ‚Äì —ç—Ç–æ —Å—É–º–º–∞ –±–∞–ª–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∑–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ (—á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤) –∑–∞–¥–∞—á–∏. –ï—Å–ª–∏ —ç—Ç–∞–ø –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é, –Ω–∞—á–∏—Å–ª—è–µ—Ç—Å—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–ª.</li>\n<li><strong>Sfull</strong> ‚Äì —ç—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä, —Ä–∞–≤–Ω—ã–π 1, –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é (–≤—Å–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã), –∏ 0 –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ.</li>\n</ul>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∞–≥–µ–Ω—Ç—ã –ø–æ–ª—É—á–∞—é—Ç —á–∞—Å—Ç–∏—á–Ω—É—é –æ—Ü–µ–Ω–∫—É, –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –∏—Ö –ø—Ä–æ–≥—Ä–µ—Å—Å—É, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–ª–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç—Å—è 50% –±–æ–Ω—É—Å–æ–º. –≠—Ç–æ –º–æ—Ç–∏–≤–∏—Ä—É–µ—Ç –∞–≥–µ–Ω—Ç–æ–≤ –Ω–µ —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤, –Ω–æ –∏ –∫ –ø–æ–ª–Ω–æ–º—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –∑–∞–¥–∞—á–∏.</p>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∞–≥–µ–Ω—Ç–∞ TheAgentCompany, —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ —Å–ø—Ä–∏–Ω—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–∞ RisingWave. –ó–∞–¥–∞—á–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:\n*   –ü–µ—Ä–µ–Ω–æ—Å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –≤ —Å–ª–µ–¥—É—é—â–∏–π —Å–ø—Ä–∏–Ω—Ç.\n*   –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞ —ç—Ç–∏ –∑–∞–¥–∞—á–∏.\n*   –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–¥–∞.\n*   –ó–∞–≥—Ä—É–∑–∫—É –æ—Ç—á–µ—Ç–∞ –≤ OwnCloud.\n*   –£—á–µ—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –∏–º–∏—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞.</p>\n<p>–¢–∞–∫–∂–µ –≤ —Å—Ç–∞—Ç—å–µ –≤–≤–æ–¥—è—Ç—Å—è –¥–≤–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:</p>\n<ul>\n<li><strong>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤</strong> ‚Äì –æ–±—â–µ–µ —á–∏—Å–ª–æ –≤—ã–∑–æ–≤–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM) –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ —É—Å–∏–ª–∏–π –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –∞–≥–µ–Ω—Ç—É –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–°—Ç–æ–∏–º–æ—Å—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä–∞</strong> ‚Äì –¥–µ–Ω–µ–∂–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM API –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –°—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —Å—É–º–º–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ –∏ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ. –≠—Ç–∞ –º–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.</li>\n</ul>\n<p><strong>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:</strong> –ú–µ—Ç—Ä–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏, –∫–æ—Ç–æ—Ä–∞—è —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –∫–∞–∫ —á–∞—Å—Ç–∏—á–Ω–æ–µ, —Ç–∞–∫ –∏ –ø–æ–ª–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏, —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º –≤ –æ–±—É—á–µ–Ω–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä–∞, –ø–æ–∑–≤–æ–ª—è—é—Ç –æ—Ü–µ–Ω–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤.</p>"
            },
            {
                "title": "Workflow",
                "content": "Each task typically follows workflow involving the following stages: 1. Initialization: The agent sets up its workspace and prepares to execute the task. 2. Execution: The agent completes subtasks, such as navigating tools, collecting data, or processing information or if required by the task, the agent interacts with simulated colleagues or shares results via communication platforms. 3. Finalization: The agent produces and submits the final output for evaluation. Example Task We consider task designed to evaluate an agents ability to perform realistic project management workflows using multiple tools and services hosted in the benchmark. The task involves managing sprint for the RisingWave project, requiring the agent to execute interdependent steps such as sprint issue management, team communication, repository operations, and report generation while incorporating feedback from simulated project manager. The workflow as illustrated in Figure 2 begins with the agent identifying unfinished issues in the current sprint on Plane and updating their sprint assignments. This step is worth 2 points and is fully completed, earning the agent the maximum score of 2/2. Next, the agent successfully notifies the relevant assignees using Rocket.Chat regarding their pending tasks and earns 1/1 point. The agent then proceeds to clone the RisingWave repository from GitLab and execute Python script in the terminal to calculate updated code coverage. This step, worth 2 points, is only partially completed, as the agent successfully clones the repository but fails to run code coverage. As result, the agent earns 1/2 points for this checkpoint. The subsequent stepsgenerating and sharing the sprint summary report on OwnCloud and incorporating feedback from simulated project managerare not completed, resulting in 0/2 and 0/1 scores, respectively. Notably, the checkpoints can also fail if the report does not meet quality standards as assessed by the LLM-based evaluator, which evaluates the report for clarity, completeness, and successful incorporation of feedback. This ensures that the assessment reflects both the generation of outputs and their qualitative relevance to the task. 7 Preprint. Finally, the overall score is calculated using the partial completion formula defined in 4.1, where the total possible points are 8, and the awarded points sum to 4. Substituting these values, the agent achieves final score of 0.25 (25%). Our scoring mechanism thus rewards incremental progress while strongly incentivizing full completion. This example represents typical task in the TheAgentCompany benchmark, where agents are required to handle complex workflows involving multiple tools and interdependent steps. By evaluating both partial progress and overall outcomes, our benchmark provides rigorous and realistic measure of agent performance, allowing us to identify their strengths and pinpoint areas for improvement in task execution.",
                "summary": "<p>–í —Ç–∏–ø–∏—á–Ω–æ–π –∑–∞–¥–∞—á–µ, –≤—ã–ø–æ–ª–Ω—è–µ–º–æ–π –∞–≥–µ–Ω—Ç–æ–º, –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç—Ç–∞–ø–∞:</p>\n<ol>\n<li><strong>–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:</strong> –ê–≥–µ–Ω—Ç –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ä–∞–±–æ—á–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ:</strong> –ê–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–¥–∑–∞–¥–∞—á–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —Å–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏, –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–ª–µ–≥–∞–º–∏ –∏–ª–∏ –¥–µ–ª–∏—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —á–µ—Ä–µ–∑ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –¥–ª—è –æ–±—â–µ–Ω–∏—è.</li>\n<li><strong>–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ:</strong> –ê–≥–µ–Ω—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏.</li>\n</ol>\n<p>–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø—Ä–∏–º–µ—Ä –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ —É–ø—Ä–∞–≤–ª—è—Ç—å –ø—Ä–æ–µ–∫—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Å–µ—Ä–≤–∏—Å—ã. –ó–∞–¥–∞—á–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ —Å–ø—Ä–∏–Ω—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–∞ RisingWave. –ê–≥–µ–Ω—Ç—É –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä—è–¥ –≤–∑–∞–∏–º–æ–∑–∞–≤–∏—Å–∏–º—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, —Ç–∞–∫–∏—Ö –∫–∞–∫: —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∞–º–∏ —Å–ø—Ä–∏–Ω—Ç–∞, –æ–±—â–µ–Ω–∏–µ —Å –∫–æ–º–∞–Ω–¥–æ–π, —Ä–∞–±–æ—Ç–∞ —Å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–º –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞. –ü—Ä–∏ —ç—Ç–æ–º –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —É—á–∏—Ç—ã–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –æ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞.</p>\n<p>–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–æ–≥–æ, —á—Ç–æ –∞–≥–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Ç–µ–∫—É—â–µ–≥–æ —Å–ø—Ä–∏–Ω—Ç–∞ –≤ Plane –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Ö –ø—Ä–∏–≤—è–∑–∫—É –∫ —Å–ø—Ä–∏–Ω—Ç—É. –≠—Ç–æ—Ç —à–∞–≥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞ –∏ –±—ã–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤—ã–ø–æ–ª–Ω–µ–Ω, –ø–æ—ç—Ç–æ–º—É –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç 2/2 –±–∞–ª–ª–∞. –î–∞–ª–µ–µ, –∞–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —É–≤–µ–¥–æ–º–ª—è–µ—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π –æ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —á–µ—Ä–µ–∑ Rocket.Chat, –ø–æ–ª—É—á–∞—è 1/1 –±–∞–ª–ª. –ó–∞—Ç–µ–º –∞–≥–µ–Ω—Ç –∫–ª–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π RisingWave –∏–∑ GitLab –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç Python-—Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–¥–∞. –≠—Ç–æ—Ç —à–∞–≥ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤ 2 –±–∞–ª–ª–∞, –Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω –ª–∏—à—å —á–∞—Å—Ç–∏—á–Ω–æ: –∞–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∫–ª–æ–Ω–∏—Ä—É–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π, –Ω–æ –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç. –í –∏—Ç–æ–≥–µ, –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç 1/2 –±–∞–ª–ª–∞ –∑–∞ —ç—Ç–æ—Ç —ç—Ç–∞–ø. –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ ‚Äì —Å–æ–∑–¥–∞–Ω–∏–µ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ —Å–ø—Ä–∏–Ω—Ç–µ –≤ OwnCloud –∏ —É—á—ë—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ ‚Äì –Ω–µ –±—ã–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã, –ø–æ—ç—Ç–æ–º—É –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç 0/2 –∏ 0/1 –±–∞–ª–ª–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —ç—Ç–∞–ø—ã –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –∑–∞—Å—á–∏—Ç–∞–Ω—ã, –µ—Å–ª–∏ –æ—Ç—á—ë—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–∞—á–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç LLM-–º–æ–¥–µ–ª—å. –û–Ω–∞ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç—á—ë—Ç –Ω–∞ —è—Å–Ω–æ—Å—Ç—å, –ø–æ–ª–Ω–æ—Ç—É –∏ —É—á—ë—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –æ—Ü–µ–Ω–∫—É –Ω–µ —Ç–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –∏ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–µ.</p>\n<p>–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ–æ—Ä–º—É–ª—ã —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ, –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –±–∞–ª–ª–æ–≤ ‚Äì 8, –∞ –Ω–∞–±—Ä–∞–Ω–Ω—ã—Ö ‚Äì 4.  –ü–æ–¥—Å—Ç–∞–≤–∏–≤ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è, –∞–≥–µ–Ω—Ç –ø–æ–ª—É—á–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª 0.25 (25%). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –ø–æ–æ—â—Ä—è–µ—Ç –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –º–æ—Ç–∏–≤–∏—Ä—É–µ—Ç –∫ –ø–æ–ª–Ω–æ–º—É –µ—ë –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é.</p>\n<p>–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä —Ç–∏–ø–∏—á–µ–Ω –¥–ª—è –∑–∞–¥–∞—á –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ TheAgentCompany, –≥–¥–µ –∞–≥–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏, –≤–∫–ª—é—á–∞—é—â–∏–º–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –≤–∑–∞–∏–º–æ–∑–∞–≤–∏—Å–∏–º—ã–µ —à–∞–≥–∏. –û—Ü–µ–Ω–∏–≤–∞—è –∫–∞–∫ —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, —Ç–∞–∫ –∏ –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –±–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—É—é –∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –º–µ—Ä—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞, –ø–æ–∑–≤–æ–ª—è—è –≤—ã—è–≤–∏—Ç—å —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è.</p>"
            },
            {
                "title": "Choosing Task Categories",
                "content": "Many previous agent benchmarks discussed in 2 were created to evaluate agents on tasks people perform in daily life (Zhou et al., 2023; L√π et al., 2024; Deng et al., 2023), or tasks that accomplish digital chores (Yoran et al., 2024; Trivedi et al., 2024). Obtaining realistic tasks for the benchmark poses challenges. Some benchmark (Xie et al., 2024; Drouin et al., 2024; Yoran et al., 2024) crowdsourced tasks based on predetermined interfaces, platforms, and services available to the agent. They also adopt strategy to first gather task templates and then instantiate more task instances by filling in the variables. Some benchmark (Zhou et al., 2023; Koh et al., 2024; Bonatti et al., 2024) took semi-systematic approach of reviewing the action history of the research team and choosing tasks that reflected the types of task that the researchers carried out in their daily life. There are several obvious issues with this if we want to evaluate agents with broader implications in the TheAgentCompany benchmark. Despite some grounding in realistic data, the process of creating tasks from these data was susceptible to heuristic, and no consideration was made for how important or time-consuming the tasks are. The tasks are biased towards those important for academics in computer science and do not reflect the tasks performed by the entire population. In TheAgentCompany, we attempt to cover wide variety of tasks motivated by real-world work. While it is highly challenging to create representative sample of tasks, fortunately we can rely on existing resources created for other purposes as reference. Specifically, we start by referencing the 29.1 release of O*NET database (O*NET, 2024; Rounds et al., 1999), which is database of jobs performed by workers in the US created by the US Department of Labor. It also contains information about tasks performed within the context of each job, abilities required to perform each task, whether the task is major or minor task for that job category, and other pieces of relevant information. Based on this data, we first identified few categories of occupation categories to focus on. First, based on statistics from O*NET, we identified job categories that have large number of people performing this job. Then, we used median salary information for each of these job categories from the US department of labor statistics, and multiplied the number of employees in that category to estimate the aggregate value of performing this job. Based on this, we identified several categories of jobs such as General and Operations Managers, Registered Nurses, Software Developers, and Financial Managers that have both high population and high average salary. Because TheAgentCompany is designed to be non-embodied benchmark in the digital domain, we excluded the categories that require extensive physical labor such as Registered Nurses, and eventually settled on the setting of software company, which would allow us to cover tasks from the other categories.",
                "summary": "<p>–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∑–∞–¥–∞—á–∏, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–µ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã–µ –¥–µ–ª–∞ –∏–ª–∏ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Ä—É—Ç–∏–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏. –û–¥–Ω–∞–∫–æ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è —Ç–∞–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤ –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–µ–π.</p>\n<p>–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∫—Ä–∞—É–¥—Å–æ—Ä—Å–∏–Ω–≥, –≥–¥–µ –∑–∞–¥–∞—á–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º. –í –¥—Ä—É–≥–∏—Ö —Å–ª—É—á–∞—è—Ö —Å–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–ª–∏ —à–∞–±–ª–æ–Ω—ã –∑–∞–¥–∞—á, –∞ –∑–∞—Ç–µ–º —Å–æ–∑–¥–∞–≤–∞–ª–∏ –∏—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä—ã, –∑–∞–ø–æ–ª–Ω—è—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ. –î—Ä—É–≥–∏–µ –ø–æ–¥—Ö–æ–¥—ã –æ—Å–Ω–æ–≤—ã–≤–∞–ª–∏—Å—å –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –≤—ã–±–∏—Ä–∞—è –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ –≤—ã–ø–æ–ª–Ω—è–ª–∏ –≤ —Å–≤–æ–µ–π –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç–µ.</p>\n<p>–û–¥–Ω–∞–∫–æ —É —ç—Ç–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –µ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. –ü—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á —á–∞—Å—Ç–æ –±—ã–ª –ø–æ–¥–≤–µ—Ä–∂–µ–Ω —ç–≤—Ä–∏—Å—Ç–∏–∫–µ, –∏ –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª–∞—Å—å –≤–∞–∂–Ω–æ—Å—Ç—å –∏–ª–∏ —Ç—Ä—É–¥–æ–µ–º–∫–æ—Å—Ç—å –∑–∞–¥–∞—á. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ç–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –±—ã–ª–∏ —Å–º–µ—â–µ–Ω—ã –≤ —Å—Ç–æ—Ä–æ–Ω—É –∑–∞–¥–∞—á, –≤–∞–∂–Ω—ã—Ö –¥–ª—è —É—á–µ–Ω—ã—Ö –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∏ –Ω–µ –æ—Ç—Ä–∞–∂–∞–ª–∏ –∑–∞–¥–∞—á–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–µ –≤—Å–µ–º –Ω–∞—Å–µ–ª–µ–Ω–∏–µ–º.</p>\n<p>–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —ç—Ç–æ–≥–æ, –≤ TheAgentCompany —Å—Ç–∞–≤–∏—Ç—Å—è —Ü–µ–ª—å –æ—Ö–≤–∞—Ç–∏—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á, –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–æ–π. –î–ª—è —ç—Ç–æ–≥–æ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö O*NET, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è—Ö –∏ –∑–∞–¥–∞—á–∞—Ö, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –≤ –°–®–ê.</p>\n<p>–°–Ω–∞—á–∞–ª–∞ –±—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–æ–∏—Ç —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è. –í—ã–±–æ—Ä –æ—Å–Ω–æ–≤—ã–≤–∞–ª—Å—è –Ω–∞ –¥–≤—É—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—é–¥–µ–π, –∑–∞–Ω—è—Ç—ã—Ö –≤ –¥–∞–Ω–Ω–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏, –∏ –æ–±—â–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—É—é –æ–Ω–∏ —Å–æ–∑–¥–∞—é—Ç (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–∞—è –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤ –Ω–∞ –º–µ–¥–∏–∞–Ω–Ω—É—é –∑–∞—Ä–ø–ª–∞—Ç—É). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –±—ã–ª–∏ –≤—ã–±—Ä–∞–Ω—ã —Ç–∞–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–∞–∫ –≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–µ –∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ–¥—Å–µ—Å—Ç—Ä—ã, —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ–Ω–µ–¥–∂–µ—Ä—ã.</p>\n<p>–ü–æ—Å–∫–æ–ª—å–∫—É TheAgentCompany –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∑–∞–¥–∞—á–∏, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä—É–¥–∞, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–µ–¥—Å–µ—Å—Ç—Ä—ã, –±—ã–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω—ã. –í –∏—Ç–æ–≥–µ –±—ã–ª–æ —Ä–µ—à–µ–Ω–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏—Ç—å—Å—è –Ω–∞ –∑–∞–¥–∞—á–∞—Ö –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, —á—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –æ—Ö–≤–∞—Ç–∏—Ç—å –∑–∞–¥–∞—á–∏ –∏–∑ –¥—Ä—É–≥–∏—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π.</p>"
            },
            {
                "title": "Choosing Tasks",
                "content": "Next, within this setting we chose tasks to implement. In this setting, we attempted to create diversity of tasks, but mostly focused on concrete tasks that have well-defined goals and success criteria. These tasks were created through combination of referencing the O*NET task list, introspection based on paper co-authors who had experience in each task category, and brainstorming lists with language models. It is important to note that in no cases have we covered an extensive list of all the tasks that are performed in particular occupational category, and therefore we caution against making any assumptions about whether particular job may be in danger of full automation based solely on 8 Preprint. Figure 3: Overview of OpenHands default CodeAct + Browsing agent architecture, the baseline agent used throughout the experiments. TheAgentCompany. Rather, it may provide insight into whether certain tasks within jobs may be accelerated or automated, and inform further analysis by labor professionals into this question.",
                "summary": "<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º –æ –∑–∞–¥–∞—á–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –≤—ã–±—Ä–∞–ª–∏ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ —Ä–∞–º–∫–∞—Ö –Ω–∞—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –ú—ã —Å—Ç—Ä–µ–º–∏–ª–∏—Å—å –∫ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—é –∑–∞–¥–∞—á, –Ω–æ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–ª–∏—Å—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Å —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏ –∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ —É—Å–ø–µ—Ö–∞. </p>\n<p>–≠—Ç–∏ –∑–∞–¥–∞—á–∏ –±—ã–ª–∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: —Å–ø–∏—Å–∫–∞ –∑–∞–¥–∞—á O*NET (–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è—Ö –∏ –Ω–∞–≤—ã–∫–∞—Ö), –ª–∏—á–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ —Å–æ–∞–≤—Ç–æ—Ä–æ–≤ —Å—Ç–∞—Ç—å–∏, –∏–º–µ—é—â–∏—Ö –æ–ø—ã—Ç –≤ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á, –∏ –º–æ–∑–≥–æ–≤–æ–≥–æ —à—Ç—É—Ä–º–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.</p>\n<p>–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –º—ã –Ω–µ —Å—Ç–∞–≤–∏–ª–∏ —Ü–µ–ª—å—é –æ—Ö–≤–∞—Ç–∏—Ç—å –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∑–∞–¥–∞—á, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö –≤ –∫–∞–∂–¥–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏. –ü–æ—ç—Ç–æ–º—É –º—ã –ø—Ä–µ–¥–æ—Å—Ç–µ—Ä–µ–≥–∞–µ–º –æ—Ç –ø–æ—Å–ø–µ—à–Ω—ã—Ö –≤—ã–≤–æ–¥–æ–≤ –æ —Ç–æ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø–æ–¥ —É–≥—Ä–æ–∑–æ–π –ø–æ–ª–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤ —Ä–∞–º–∫–∞—Ö –Ω–∞—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è.</p>\n<p>–°–∫–æ—Ä–µ–µ, –Ω–∞—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –¥–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–æ–º, –º–æ–≥—É—Ç –ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ—Ñ–µ—Å—Å–∏–π –±—ã—Ç—å —É—Å–∫–æ—Ä–µ–Ω—ã –∏–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã. –≠—Ç–æ –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –æ—Ç–ø—Ä–∞–≤–Ω–æ–π —Ç–æ—á–∫–æ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ —Ç—Ä—É–¥–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–º–æ–≥—É—Ç –≥–ª—É–±–∂–µ –∏–∑—É—á–∏—Ç—å —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –ó–¥–µ—Å—å –∞–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ –∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–π, –∞ –ª–∏—à—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö.</em></p>"
            },
            {
                "title": "Manual Task Curation",
                "content": "Once we set up the environment required for our desired jobs and task categories ( 3), we return to the curated list, and perform manual curation process for tasks. For each task, this consists of the following steps: We first create description of task intent, checkpoints, and how to evaluate each checkpoint. We then identify and import the required data for the task that are currently missing in the company Intranet services and create any necessary data. We then write scripts to configure the required initialization state in the local workspace. Finally, we implement the checkpoint evaluators that calculate the scalar scores for each checkpoint. All tasks were created by coauthors of the paper. Overall, it took 20 computer science students, software engineers, and project managers over 2 months, consuming approximately 3,000 personhours in total. Some of the more complex tasks take more than 10 hours each to design, implement, test, and verify. To ensure quality control of the task creation process, we implement several check and verification processes. For each task implementation, we require screenshot proof that the evaluator is valid and that the task is able to get full score when successfully completed. We also encourage including tests for the implemented evaluator programs. Each task contribution is also code reviewed by panel of lead authors before merging into the benchmark. After creating all tasks, final round of manual human double-check of required environment data, evaluator behavior, and checkpoint scoring for every task is performed to ensure quality. Notably, during the process person who has not curated the tasks checks all the checkpoint score assignments to make sure that the importance scoring is consistent over all the tasks and that it correlates reasonably with the relative importance of the checkpoint within the task.",
                "summary": "<p>–ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –±—ã–ª–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –æ–∫—Ä—É–∂–µ–Ω–∏–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á (–∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ —Ä–∞–∑–¥–µ–ª–µ 3), –∫–æ–º–∞–Ω–¥–∞ –≤–µ—Ä–Ω—É–ª–∞—Å—å –∫ —Å–ø–∏—Å–∫—É –∑–∞–¥–∞—á –∏ –ø—Ä–∏—Å—Ç—É–ø–∏–ª–∞ –∫ –∏—Ö —Ä—É—á–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–æ—Ü–µ—Å—Å –≤–∫–ª—é—á–∞–ª —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:</p>\n<ol>\n<li><strong>–û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏:</strong> –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–≤–∞–ª–æ—Å—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ü–µ–ª–∏ –∑–∞–¥–∞—á–∏, –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏—Å—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ (—á–µ–∫–ø–æ–∏–Ω—Ç—ã) –∏ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.</li>\n<li><strong>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:</strong> –ó–∞—Ç–µ–º –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏—Å—å –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏. –ï—Å–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å, —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.</li>\n<li><strong>–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è:</strong> –î–∞–ª–µ–µ –ø–∏—Å–∞–ª–∏—Å—å —Å–∫—Ä–∏–ø—Ç—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–æ–≤:</strong> –ù–∞–∫–æ–Ω–µ—Ü, —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏—Å—å –ø—Ä–æ–≥—Ä–∞–º–º—ã-–æ—Ü–µ–Ω—â–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—á–∏—Å–ª—è–ª–∏ —á–∏—Å–ª–æ–≤—ã–µ –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.</li>\n</ol>\n<p>–í—Å–µ –∑–∞–¥–∞—á–∏ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–æ–∞–≤—Ç–æ—Ä–∞–º–∏ —Å—Ç–∞—Ç—å–∏. –ù–∞ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å —É—à–ª–æ –±–æ–ª–µ–µ –¥–≤—É—Ö –º–µ—Å—è—Ü–µ–≤ –∏ –æ–∫–æ–ª–æ 3000 —á–µ–ª–æ–≤–µ–∫–æ-—á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã 20 —Å—Ç—É–¥–µ–Ω—Ç–æ–≤-–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–æ–≤, –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤ –∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ–≤. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á –∑–∞–Ω–∏–º–∞–ª–∏ –±–æ–ª–µ–µ 10 —á–∞—Å–æ–≤ –∫–∞–∂–¥–∞—è.</p>\n<p>–î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á –±—ã–ª–∏ –≤–Ω–µ–¥—Ä–µ–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –æ—Ü–µ–Ω—â–∏–∫–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–ª–∞ –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏. –¢–∞–∫–∂–µ –ø–æ–æ—â—Ä—è–ª–æ—Å—å –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω—â–∏–∫–æ–≤. –ö–∞–∂–¥—ã–π –≤–∫–ª–∞–¥ –≤ –≤–∏–¥–µ –Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–æ—Ö–æ–¥–∏–ª –∫–æ–¥-—Ä–µ–≤—å—é —É –≤–µ–¥—É—â–∏—Ö –∞–≤—Ç–æ—Ä–æ–≤ –ø–µ—Ä–µ–¥ –≤–∫–ª—é—á–µ–Ω–∏–µ–º –≤ –±–µ–Ω—á–º–∞—Ä–∫ (–Ω–∞–±–æ—Ä –∑–∞–¥–∞—á).</p>\n<p>–ü–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä—É—á–Ω—É—é, –≥–¥–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è –±–∞–ª–ª–æ–≤ –∑–∞ –∫–∞–∂–¥—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –∏—Ö –æ—Ü–µ–Ω–∫–∞–º. –≠—Ç–æ –¥–µ–ª–∞–ª —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —É—á–∞—Å—Ç–≤–æ–≤–∞–ª –≤ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –∑–∞–¥–∞—á, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏. –û–Ω –ø—Ä–æ–≤–µ—Ä—è–ª, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∫–∏ –∑–∞ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –±—ã–ª–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞–ª–∏ –∏—Ö –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏.</p>"
            },
            {
                "title": "Baseline Agent",
                "content": "To test the current state-of-the-art performance on the TheAgentCompany benchmark, we need agents that can at least perform tasks using browser, operate local workspace using terminal, and write and execute programs to perform most of the tasks. Throughout this paper, we experiment with OpenHands main agent (Wang et al., 2024b;a; Song et al., 2024), CodeAct Agent with Browsing.8 An overview of the agent architecture is illustrated in Figure 3. Interfaces The agent can interact with the environment through 3 interfaces. (1) bash shell that connects with the local workspace operating system environment for command execution. (2) Jupyter IPython server to handle interactive python (IPython) code execution requests and return the execution results back. (3) Chromium browser based on Playwright. The provider 8More specifically, version 0.14.2. Full details can be found in https://github.com/All-Hands-AI/OpenHands/ tree/main/openhands/agenthub/codeact_agent 9 Preprint. provides set of action primitives defined by BrowserGym (ServiceNow; Drouin et al., 2024), such as navigation, clicking, typing, and scrolling. After executing these actions, the browser runtime provides rich set of observations about the current state of the browser, including HTML, DOM, accessibility tree (Mozilla), screenshot, opened tabs, etc. These observations can be also augmented with configurable attributes that could allow agents to better understand web page observations, such as using set-of-marks on screenshot (Yang et al., 2023; He et al., 2024), visible element marking, focused element, interactable element marking, in-viewport element filtering (Zhou et al., 2023), etc. Actions The agent connects with the environment through core set of general actions. Actions IPythonRunCellAction and CmdRunAction enable the agent to execute arbitrary Python code and bash commands inside the sandbox environment (e.g., secure isolated Linux operating system used as our local workspace). BrowserInteractiveAction enables interaction with web browser with domain-specific language for browsing introduced by BrowserGym (Chezelles et al., 2024; Drouin et al., 2024). These actions provide comprehensive, yet flexible set of primitives that cover most of the tasks performed by human employees of TheAgentCompany, including navigation, click, hovering, and typing, etc. Observations Observations describe the environmental changes that the agent observes. The main types of observations used in the CodeAct agent include the execution result of bash terminal commands, Python programs, and browser actions. Specifically, the execution result of browser actions is usually browser snapshots and textual representation in the form of accessibility tree of the current browser viewport. Workflow At each step, the underlying backbone LLM will take in prompts consisting of previous agent history and the current observation of the environment, and generate response consisting of the action to execute next. On higher level, the agent can perform the task by executing code, including executing bash commands, Python code, or browser-specific programming language (defined in BrowserGym).9 This general action space allows the agent to perform various tasks, including editing files, browsing the Web, running programs, etc.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ TheAgentCompany –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∞–≥–µ–Ω—Ç—ã, —Å–ø–æ—Å–æ–±–Ω—ã–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ —Å –ø–æ–º–æ—â—å—é –±—Ä–∞—É–∑–µ—Ä–∞, —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª, –∞ —Ç–∞–∫–∂–µ –ø–∏—Å–∞—Ç—å –∏ –∑–∞–ø—É—Å–∫–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã. –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –∞–≥–µ–Ω—Ç—É OpenHands –∏ CodeAct Agent with Browsing.</p>\n<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–≥–µ–Ω—Ç–∞</strong></p>\n<p>–ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ —Ç—Ä–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞:\n1.  <strong>Bash shell:</strong> –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–≤—è–∑—å —Å –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥.\n2.  <strong>Jupyter IPython server:</strong> –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–¥ Python (IPython) –∏ –ø–æ–ª—É—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.\n3.  <strong>Chromium browser (–Ω–∞ –æ—Å–Ω–æ–≤–µ Playwright):</strong> –ê–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–±–æ—Ä –ø—Ä–∏–º–∏—Ç–∏–≤–æ–≤ –¥–µ–π—Å—Ç–≤–∏–π, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã—Ö BrowserGym, —Ç–∞–∫–∏—Ö –∫–∞–∫ –Ω–∞–≤–∏–≥–∞—Ü–∏—è, –∫–ª–∏–∫–∏, –≤–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–æ–∫—Ä—É—Ç–∫–∞. –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –±—Ä–∞—É–∑–µ—Ä –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ —Å–≤–æ–µ–º —Ç–µ–∫—É—â–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –≤–∫–ª—é—á–∞—è HTML, DOM, –¥–µ—Ä–µ–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏, —Å–∫—Ä–∏–Ω—à–æ—Ç, –æ—Ç–∫—Ä—ã—Ç—ã–µ –≤–∫–ª–∞–¥–∫–∏ –∏ —Ç.–¥. –≠—Ç–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–ø–æ–ª–Ω–µ–Ω—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–µ—Ç–∫–∞–º–∏ –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–µ, –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –≤–∏–¥–∏–º—ã—Ö –∏–ª–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –≤–∏–¥–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏ —Ç.–¥.</p>\n<p><strong>–î–µ–π—Å—Ç–≤–∏—è</strong></p>\n<p>–ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º —á–µ—Ä–µ–∑ –Ω–∞–±–æ—Ä –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π:\n*   <strong>IPythonRunCellAction:</strong> –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –∫–æ–¥ Python –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ.\n*   <strong>CmdRunAction:</strong> –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –∫–æ–º–∞–Ω–¥—ã bash –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ.\n*   <strong>BrowserInteractiveAction:</strong> –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å –≤–µ–±-–±—Ä–∞—É–∑–µ—Ä–æ–º —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤ BrowserGym. –≠—Ç–æ—Ç –Ω–∞–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–∏–±–∫–∏–µ –ø—Ä–∏–º–∏—Ç–∏–≤—ã, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–∞–¥–∞—á, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ TheAgentCompany, –≤–∫–ª—é—á–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—é, –∫–ª–∏–∫–∏, –Ω–∞–≤–µ–¥–µ–Ω–∏–µ –∫—É—Ä—Å–æ—Ä–∞ –∏ –≤–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞.</p>\n<p><strong>–ù–∞–±–ª—é–¥–µ–Ω–∏—è</strong></p>\n<p>–ù–∞–±–ª—é–¥–µ–Ω–∏—è –æ–ø–∏—Å—ã–≤–∞—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –≤ –∞–≥–µ–Ω—Ç–µ CodeAct, –≤–∫–ª—é—á–∞—é—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥ bash, –ø—Ä–æ–≥—Ä–∞–º–º Python –∏ –¥–µ–π—Å—Ç–≤–∏–π –±—Ä–∞—É–∑–µ—Ä–∞. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –¥–µ–π—Å—Ç–≤–∏–π –±—Ä–∞—É–∑–µ—Ä–∞ –æ–±—ã—á–Ω–æ —è–≤–ª—è—é—Ç—Å—è —Å–Ω–∏–º–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–∏–¥–µ –¥–µ—Ä–µ–≤–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ç–µ–∫—É—â–µ–π –≤–∏–¥–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏ –±—Ä–∞—É–∑–µ—Ä–∞.</p>\n<p><strong>–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å</strong></p>\n<p>–ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –±–∞–∑–æ–≤–∞—è –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (LLM) –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ –∏ —Ç–µ–∫—É—â–µ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –æ–∫—Ä—É–∂–µ–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–º. –ù–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –ø—É—Ç–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞, –≤–∫–ª—é—á–∞—è –∫–æ–º–∞–Ω–¥—ã bash, –∫–æ–¥ Python –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞ (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –≤ BrowserGym). –≠—Ç–æ –æ–±—â–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç—É –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ—Å–º–æ—Ç—Ä –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º –∏ —Ç.–¥.</p>"
            },
            {
                "title": "Result Overview",
                "content": "Table 3 shows the evaluation results of both closed and open foundation models on the full evaluation set of TheAgentCompany (175 tasks). We can see that the Claude-3.5-Sonnet is the clear winner across all models. However, even with the strongest frontier model, it only manages to complete 24% of the total tasks and achieves score of 34.4% taking into account partial completion credits. Note that this result comes at cost: It requires an average of almost 30 steps and more than $6 to complete each task, making it the most expensive model to run both in time and in cost. This is expected as most of the tasks in our benchmark are of long-horizon nature. The Gemini 2.0 Flash model that comes second in terms of capability requires 40 steps on average to complete the tasks, which is time consuming, yet only to achieve less than half the success rate compared to the top-performing model. Surprisingly, its cost is less than $1, making it very cost-efficient yet relatively strong model. qualitative examination demonstrated that this was due to instances where the agent got stuck in loop or aimlessly explored the environment. Among the open-weight models, Llama 3.1 (405B) achieves the highest performance, nearly on par with OpenAIs GPT-4o model, though still having big gap behind the leading Claude 3.5 Sonnet. 9https://github.com/ServiceNow/BrowserGym/blob/main/browsergym/core/src/browsergym/core/action/ functions.py 10 Preprint. Table 3: Performance comparison of various foundation models on TheAgentCompany. Model Success Score Steps Costs API-based Models Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 24.0% 34.4% 29.17 11.4% 19.0% 39.85 8.6% 16.7% 14.55 8.0% 22.10 3.4% 5.7% 19.59 1.7% Open-weights Models Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 7.4% 14.1% 22.95 6.9% 12.8% 20.93 5.7% 11.8% 23.99 6.5% 19.18 1.7% 4.2% 23.70 1.1% $6.34 $0.79 $1.29 $6.78 $1. $3.21 $0.93 $1.53 $0.83 $0.28 Table 4: Performance of the models in tasks that require different platforms in TheAgentCompany. All numbers are percentages (%). Model Success (%) Score (%) Success (%) Score (%) Success (%) Score (%) Success (%) Score (%) GitLab (71 tasks) Plane (17 tasks) RocketChat (79 tasks) ownCloud (70 tasks) Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 30.99 11.27 11.27 2.82 2. 5.63 8.45 5.63 1.41 1.41 40.25 18.21 19.46 3.88 7.22 11.84 14.26 11.33 6.09 1.94 API-based Models 41.18 17.65 23.53 5.88 5.88 50.37 29.84 33.68 14.05 16. Open-weights Models 29.41 11.76 11.76 5.88 5.88 39.12 21.65 23.56 15.35 12.45 21.52 13.92 5.06 3.80 1.27 8.86 5.06 5.06 2.53 0.00 34.68 23.34 16.08 10.97 5. 16.46 12.06 12.60 8.23 4.88 10.00 2.86 1.43 0.00 0.00 0.00 0.00 0.00 0.00 0.00 21.81 8.52 7.76 4.22 2.43 4.45 3.76 4.14 3.32 2.60 Interestingly, comparing the number of steps and costs between the open Llama 3.1 (405B) model and the closed OpenAI GPT-4o model, Llama 3.1 takes more steps and costs nearly 2x more to run, while having lower success than GPT-4o. Anecdotally, our inspection showed that GPT-4o seems to be better at giving up early, saving steps and costs if the task is clearly out of the capacity range of the agent. This suggests that open-weight models are not always the most cost-effective choice in agents given serving cost, especially with highly complex tasks. On the other hand, the newer generation of Llama model, Llama 3.3 (70B) achieves considerably high performance of 6.9% success rate, on par with the much larger (405B), older generation (Llama 3.1) model. This model also costs significantly less because of its smaller size. This suggests promising future for LLM development, as smaller and more efficient models begin to catch up in agent performance.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–∞–∫ –∑–∞–∫—Ä—ã—Ç—ã—Ö, —Ç–∞–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö, –Ω–∞ –ø–æ–ª–Ω–æ–º –Ω–∞–±–æ—Ä–µ –∏–∑ 175 –∑–∞–¥–∞—á TheAgentCompany. Claude-3.5-Sonnet –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π, –æ–¥–Ω–∞–∫–æ –¥–∞–∂–µ —ç—Ç–∞ –ø–µ—Ä–µ–¥–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–º–æ–≥–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ª–∏—à—å 24% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –∑–∞–¥–∞—á, –Ω–∞–±—Ä–∞–≤ 34.4% —Å —É—á—ë—Ç–æ–º —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –°—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π —Ü–µ–Ω–æ–π: –≤ —Å—Ä–µ–¥–Ω–µ–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–∫–æ–ª–æ 30 —à–∞–≥–æ–≤ –∏ –±–æ–ª–µ–µ 6 –¥–æ–ª–ª–∞—Ä–æ–≤ –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç—É –º–æ–¥–µ–ª—å —Å–∞–º–æ–π –¥–æ—Ä–æ–≥–æ–π –∫–∞–∫ –ø–æ –≤—Ä–µ–º–µ–Ω–∏, —Ç–∞–∫ –∏ –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, –ø–æ—Å–∫–æ–ª—å–∫—É –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–∞–¥–∞—á –≤ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ –∏–º–µ—é—Ç –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π –≥–æ—Ä–∏–∑–æ–Ω—Ç. –ú–æ–¥–µ–ª—å Gemini 2.0 Flash, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Ç—Ä–µ–±—É–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 40 —à–∞–≥–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á, —á—Ç–æ –æ—Ç–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –µ—ë –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø–æ—á—Ç–∏ –≤–¥–≤–æ–µ –Ω–∏–∂–µ, —á–µ–º —É –ª–∏–¥–µ—Ä–∞. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ –µ—ë —Å—Ç–æ–∏–º–æ—Å—Ç—å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ–Ω–µ–µ 1 –¥–æ–ª–ª–∞—Ä–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ—ë –æ—á–µ–Ω—å —ç–∫–æ–Ω–æ–º–∏—á–Ω–æ–π, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª—å—é. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Å–∏—Ç—É–∞—Ü–∏—è–º–∏, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–ª—Å—è –∏–ª–∏ –±–µ—Å—Ü–µ–ª—å–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª —Å—Ä–µ–¥—É. –°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º Llama 3.1 (405B) –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –Ω–∞–∏–≤—ã—Å—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø–æ—á—Ç–∏ –Ω–∞—Ä–∞–≤–Ω–µ —Å –º–æ–¥–µ–ª—å—é GPT-4o –æ—Ç OpenAI, —Ö–æ—Ç—è –∏ –æ—Ç—Å—Ç–∞–µ—Ç –æ—Ç –ª–∏–¥–µ—Ä–∞ Claude 3.5 Sonnet.</p>\n<p>–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –æ—Ç–∫—Ä—ã—Ç–æ–π –º–æ–¥–µ–ª—å—é Llama 3.1 (405B) –∏ –∑–∞–∫—Ä—ã—Ç–æ–π GPT-4o –æ—Ç OpenAI, Llama 3.1 —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —à–∞–≥–æ–≤ –∏ —Å—Ç–æ–∏—Ç –ø–æ—á—Ç–∏ –≤ 2 —Ä–∞–∑–∞ –¥–æ—Ä–æ–∂–µ, –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–∫–∞–∑—ã–≤–∞—è –º–µ–Ω—å—à—É—é —É—Å–ø–µ—à–Ω–æ—Å—Ç—å, —á–µ–º GPT-4o. –ö–∞–∫ –ø–æ–∫–∞–∑–∞–ª –∞–Ω–∞–ª–∏–∑, GPT-4o, –ø–æ—Ö–æ–∂–µ, –ª—É—á—à–µ —É–º–µ–µ—Ç –≤–æ–≤—Ä–µ–º—è —Å–¥–∞–≤–∞—Ç—å—Å—è, —ç–∫–æ–Ω–æ–º—è —à–∞–≥–∏ –∏ –∑–∞—Ç—Ä–∞—Ç—ã, –µ—Å–ª–∏ –∑–∞–¥–∞—á–∞ —è–≤–Ω–æ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–∞. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º –Ω–µ –≤—Å–µ–≥–¥–∞ —è–≤–ª—è—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –Ω–æ–≤–æ–µ –ø–æ–∫–æ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Llama, Llama 3.3 (70B), –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–º —É—Å–ø–µ—Ö–∞ –≤ 6.9%, —á—Ç–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π (405B) –º–æ–¥–µ–ª—å—é –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è (Llama 3.1). –≠—Ç–∞ –º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –∏–∑-–∑–∞ —Å–≤–æ–µ–≥–æ –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–µ–º –±—É–¥—É—â–µ–º –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ LLM, –ø–æ—Å–∫–æ–ª—å–∫—É –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞—á–∏–Ω–∞—é—Ç –¥–æ–≥–æ–Ω—è—Ç—å –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤.</p>\n<p>–í —Ç–∞–±–ª–∏—Ü–µ 4 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –≤ TheAgentCompany. Claude-3.5-Sonnet –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∞–º—ã–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, –Ω–æ Gemini 2.0 Flash —Ç–∞–∫–∂–µ –∏–º–µ–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ö–æ—Ä–æ—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö RocketChat –∏ Plane. –û—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ Llama 3.1 (405B), Llama 3.3 (70B) –∏ Qwen-2.5-72b, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª–µ–µ –Ω–∏–∑–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∑–∞–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –≤—Å–µ—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö.</p>"
            },
            {
                "title": "Analysis",
                "content": "How well do agents operate on different platforms? Table 4 presents performance breakdown on tasks that involve different platforms in TheAgentCompany. task is categorized under platform if one of the platforms that the task requires it. From Figure 4a, we can see that most models struggle with RocketChat and ownCloud. RocketChat platform is where all the social interaction with peers happens, and the low performance on this platform suggests that current-day LLMs still need improvements in communicating with others. ownCloud platform provides online Office suite functionality, and due to the complexity of the UI of web-based Office software, it is expected that current LLMs fail badly on the platform. This suggests that the browsing capability of the agents, especially on more complex websites, still needs improvement. These results underscore the inherent challenges and complexities of performing tasks that occur in real-world work environments, involve social interaction, or require understanding and navigating complex web interfaces. 11 Preprint. Table 5: Performance of various models in tasks with different nature in TheAgentCompany. All numbers are percentages (%). Model SDE (69 tasks) Score Success PM (28 tasks) DS (14 tasks) Success Score Success Score Admin (15 tasks) Score Success HR (29 tasks) Success Score Finance (12 tasks) Score Success Other (8 tasks) Score Success Claude-3.5-Sonnet Gemini-2.0-Flash GPT-4o Gemini-1.5-Pro Amazon-Nova-Pro-v1 Llama-3.1-405b Llama-3.3-70b Qwen-2.5-72b Llama-3.1-70b Qwen-2-72b 30.43 13.04 13.04 4.35 2.90 5.80 11.59 7.25 1.45 2.90 38.02 18.99 19.18 5.64 6.07 11.33 16.49 11.99 4.77 3. 35.71 17.86 17.86 3.57 3.57 21.43 7.14 10.71 3.57 0.00 51.31 31.71 32.27 13.19 12.54 35.62 19.83 22.90 15.16 7.44 API-based Models 14.29 0.00 0.00 0.00 0. 21.70 6.49 4.70 4.82 3.27 0.00 6.67 6.67 6.67 0.00 11.59 15.20 13.89 9.92 0.00 Open-weights Models 0.00 0.00 0.00 0.00 0.00 5.42 4.70 5.42 5.42 4. 0.00 0.00 0.00 0.00 0.00 3.33 1.67 2.14 2.42 0.56 24.14 17.24 0.00 3.45 0.00 6.90 6.90 6.90 3.45 0.00 34.49 23.08 8.28 11.42 4.27 12.56 11.38 12.36 7.19 4. 8.33 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 25.17 4.31 7.36 2.78 2.78 5.00 5.69 7.15 3.82 3.61 12.50 0.00 0.00 0.00 0.00 12.50 0.00 0.00 0.00 0. 22.40 10.05 10.78 8.07 2.86 17.45 7.03 5.99 2.86 4.95 (a) Success rate across platforms (b) Success rate across task categories Figure 4: Comparing agent success rate across platforms (left) and task categories (right). How well do agents perform on different type of tasks? Table 5 presents performance breakdown on different types of tasks in TheAgentCompany. According to the nature of the task, i.e. what kind of professionals are usually assigned with the task, the tasks in TheAgentCompany can be categorized into several job departments: Software Development Engineering (SDE), Project Management (PM), Data Science (DS), Administrative (Admin), Human Resources (HR), Financial (Finance) and all the remaining (Other). From the success rate demonstrated in Figure 4b, we can see that data science, administrative, and finance tasks are among the lowest, with many LLMs completing none of the tasks successfully, and even the strongest Claude model achieving much less than the rest of the tasks. On the other hand, software engineering tasks, which may seem like much harder tasks for many humans, result in higher success rate. This suggests that there exists gap between the perceived difficulty of the tasks for humans versus the difficulty for LLM agents. For example, some tasks in the administrative and finance category involves making spreadsheets, collecting and filling in lot of information from various people, or reading and understanding images scanned by employees. These tasks are arguably easier conceptually for humans in terms of professional skill sets than software engineering, as SDE jobs usually have higher barrier of entry and more prerequisites for certain knowledge. However, most LLMs achieve much higher score on the SDE tasks. However, LLMs fail these seemingly easier tasks due to lack of ability to understand documents, communicate with other people, navigate complex software and tedious processes, and autonomously automate repetitive tasks. We hypothesize that part of the reason lies in the fact that current LLM development is heavily based on software engineering abilities, such as coding, due to several high profile benchmarks that measure this capability (e.g. HumanEval, SWE-Bench) as well as the abundance of publicly available training data related to software. On the other hand, administrative and financial tasks, are usually private data within companies, not readily available for training LLMs. 12 Preprint.",
                "summary": "<p><strong>–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∏ —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á</strong></p>\n<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è, –∫–∞–∫ —Ö–æ—Ä–æ—à–æ –∞–≥–µ–Ω—Ç—ã —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∏ —Å –∑–∞–¥–∞—á–∞–º–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ç–∏–ø–∞.</p>\n<p><strong>–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö</strong></p>\n<p>–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ RocketChat –∏ ownCloud. RocketChat –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏, –∏ –Ω–∏–∑–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —ç—Ç–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –±–æ–ª—å—à–∏–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º (LLM) –≤—Å–µ –µ—â–µ –Ω—É–∂–Ω–æ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è –≤ –æ–±—â–µ–Ω–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏. –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ ownCloud –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ–Ω–ª–∞–π–Ω-–æ—Ñ–∏—Å–Ω–æ–≥–æ –ø–∞–∫–µ—Ç–∞, –∏ –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, LLM –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —ç—Ç–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±—Ä–∞—É–∑–∏–Ω–≥–∞ –∞–≥–µ–Ω—Ç–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—Ç —É–ª—É—á—à–µ–Ω–∏—è. –≠—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–∞–±–æ—á–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö, –≤–∫–ª—é—á–∞—é—Ç —Å–æ—Ü–∏–∞–ª—å–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∏–ª–∏ —Ç—Ä–µ–±—É—é—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ —Å–ª–æ–∂–Ω—ã–º –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º.</p>\n<p><strong>–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ —Ç–∏–ø–∞</strong></p>\n<p>–ó–∞–¥–∞—á–∏ –≤ TheAgentCompany –±—ã–ª–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—è–º–∏: —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è (SDE), —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏ (PM), –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (DS), –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ (Admin), —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–æ–º (HR), —Ñ–∏–Ω–∞–Ω—Å—ã (Finance) –∏ –¥—Ä—É–≥–∏–µ (Other).</p>\n<p>–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–∞–Ω–Ω—ã—Ö, –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∑–∞–¥–∞—á–∏, –æ–∫–∞–∑–∞–ª–∏—Å—å —Å–∞–º—ã–º–∏ —Å–ª–æ–∂–Ω—ã–º–∏ –¥–ª—è LLM. –ú–Ω–æ–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –∏–∑ —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –¥–∞–∂–µ —Å–∞–º–∞—è —Å–∏–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å Claude –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ, —á–µ–º –≤ –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö. –° –¥—Ä—É–≥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã, –∑–∞–¥–∞—á–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º–∏ –¥–ª—è –ª—é–¥–µ–π, –∏–º–µ–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –∑–∞–¥–∞—á –¥–ª—è –ª—é–¥–µ–π –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é –¥–ª—è LLM-–∞–≥–µ–Ω—Ç–æ–≤.</p>\n<p>–ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–±–æ—Ç—É —Å —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–º–∏ —Ç–∞–±–ª–∏—Ü–∞–º–∏, —Å–±–æ—Ä –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ç —Ä–∞–∑–Ω—ã—Ö –ª—é–¥–µ–π, –∞ —Ç–∞–∫–∂–µ —á—Ç–µ–Ω–∏–µ –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏. –≠—Ç–∏ –∑–∞–¥–∞—á–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ –ø—Ä–æ—â–µ –¥–ª—è –ª—é–¥–µ–π, —á–µ–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è, –ø–æ—Å–∫–æ–ª—å–∫—É –¥–ª—è SDE-–∑–∞–¥–∞—á –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –∑–Ω–∞–Ω–∏–π –∏ –Ω–∞–≤—ã–∫–æ–≤. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ LLM –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö SDE.</p>\n<p>LLM –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å \"–±–æ–ª–µ–µ –ª–µ–≥–∫–∏–º–∏\" –∑–∞–¥–∞—á–∞–º–∏ –∏–∑-–∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –ø–æ–Ω–∏–º–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã, –æ–±—â–∞—Ç—å—Å—è —Å –¥—Ä—É–≥–∏–º–∏ –ª—é–¥—å–º–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—É—Ç–∏–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç, —á—Ç–æ —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ —Ç–µ–∫—É—â–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ LLM —Å–∏–ª—å–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –Ω–∞–≤—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞–ª–∏—á–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∏–∑–º–µ—Ä—è—é—â–∏—Ö –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ –Ω–∞–≤—ã–∫–∏. –í —Ç–æ –∂–µ –≤—Ä–µ–º—è, –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, —è–≤–ª—è—é—Ç—Å—è –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–º–ø–∞–Ω–∏–π –∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM.</p>"
            },
            {
                "title": "Common Agent Failures",
                "content": "Overall, the agent performance on TheAgentCompany is still low and majority of tasks are failed. Among those, we try to find some common and interesting agent mistakes that are often surprising because they are usually not made by humans. Lack of commonsense Some tasks are failed because the agent lacks the common sense and domain background knowledge required to infer implicit assumptions. For example, one task asked the agent to Write the responses to /workspace/answer.docx but does not explicitly states that this is Microsoft Word file. human can infer this requirement from the file extension. The agent instead treats it as plain text file, writing text directly to the file, resulting in task failure. Lack of social skills Sometimes, the agent fails to understand the implications and goals in the social conversations with colleagues in TheAgentCompany. For example, one task involves asking Alex for help, and the agent first successfully asks the right question Could you tell me who should introduce myself to next on the team? Then the simulated colleague Alex replied You should introduce yourself to Chen Xinyi next. Shes on our frontend team and would be great person to connect with! At this point, human would then talk to Chen Xinyi, but instead the agent then decides to not follow up with her, and prematurely considers the task accomplished. Incompetence in browsing Often times, the biggest obstacle in tasks is the parts that require browsing the Web. This is expected as browsing is still hard for agents given the complexity of modern-day web UIs and the numerous distractions on webpage. For example, on many tasks that involve ownCloud, the closable popup that sometimes shows up and asks the user to download the mobile phone apps for better experience has become an obstacle. Humans can simply click on the to close the popup, while the agents are stuck. Similarly, when trying to download file from ownCloud, there are several popups to click through before the actual download, and each step is error prone for agents due to the complex UI. Deceiving oneself Interestingly, we find that for some tasks, when the agent is not clear what the next steps should be, it sometimes try to be clever and create fake shortcuts that omit the hard part of task. For example, during the execution of one task, the agent cannot find the right person to ask questions on RocketChat. As result, it then decides to create shortcut solution by renaming another user to the name of the intended user.",
                "summary": "<p>–í —Ö–æ–¥–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –≤ TheAgentCompany –±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ, —á—Ç–æ –æ–Ω —á–∞—Å—Ç–æ —Å–æ–≤–µ—Ä—à–∞–µ—Ç –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –¥–ª—è –ª—é–¥–µ–π. –≠—Ç–∏ –æ—à–∏–±–∫–∏ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:</p>\n<p><strong>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –∑–¥—Ä–∞–≤–æ–≥–æ —Å–º—ã—Å–ª–∞:</strong> –ê–≥–µ–Ω—Ç –∏–Ω–æ–≥–¥–∞ –Ω–µ –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å –æ—á–µ–≤–∏–¥–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∑–∞–¥–∞—á–∏ –∏–∑-–∑–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ, –≥–¥–µ —Ç—Ä–µ–±–æ–≤–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ —Ñ–∞–π–ª <code>/workspace/answer.docx</code>, –∞–≥–µ–Ω—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ Microsoft Word –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é <code>.docx</code>. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –æ–Ω –æ–±—Ä–∞–±–æ—Ç–∞–ª —Ñ–∞–π–ª –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∑–∞–ø–∏—Å–∞–ª —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é, —á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ –ø—Ä–æ–≤–∞–ª—É –∑–∞–¥–∞—á–∏. –ß–µ–ª–æ–≤–µ–∫, –æ–±–ª–∞–¥–∞—è –∑–¥—Ä–∞–≤—ã–º —Å–º—ã—Å–ª–æ–º, —Å—Ä–∞–∑—É –±—ã –ø–æ–Ω—è–ª, —á—Ç–æ —ç—Ç–æ —Ñ–∞–π–ª Word.</p>\n<p><strong>–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤:</strong> –ê–≥–µ–Ω—Ç –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Ü–µ–ª–µ–π –∏ –ø–æ–¥—Ç–µ–∫—Å—Ç–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –∫–æ–ª–ª–µ–≥–∞–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ, –≥–¥–µ –Ω—É–∂–Ω–æ –±—ã–ª–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∑–∞ –ø–æ–º–æ—â—å—é –∫ –ê–ª–µ–∫—Å—É, –∞–≥–µ–Ω—Ç —Å–Ω–∞—á–∞–ª–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å \"–ö –∫–æ–º—É –º–Ω–µ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å—Å—è —Å–ª–µ–¥—É—é—â–∏–º –≤ –∫–æ–º–∞–Ω–¥–µ?\". –ê–ª–µ–∫—Å –æ—Ç–≤–µ—Ç–∏–ª: \"–¢–µ–±–µ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å—Å—è –ß–µ–Ω –°–∏–Ω—å–∏. –û–Ω–∞ –∏–∑ –Ω–∞—à–µ–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥-–∫–æ–º–∞–Ω–¥—ã, —Å –Ω–µ–π –±—ã–ª–æ –±—ã –∑–¥–æ—Ä–æ–≤–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è!\". –ß–µ–ª–æ–≤–µ–∫ –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø–æ—à–µ–ª –±—ã –∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ß–µ–Ω –°–∏–Ω—å–∏. –û–¥–Ω–∞–∫–æ –∞–≥–µ–Ω—Ç —Ä–µ—à–∏–ª, —á—Ç–æ –∑–∞–¥–∞—á–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞, –∏ –Ω–µ —Å—Ç–∞–ª –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ.</p>\n<p><strong>–ù–µ–∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ —Ä–∞–±–æ—Ç–µ —Å –±—Ä–∞—É–∑–µ—Ä–æ–º:</strong> –°–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤–æ–∑–Ω–∏–∫–∞—é—Ç —É –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –≤–µ–±-–±—Ä–∞—É–∑–µ—Ä–æ–º. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–µ–π –∏–∑-–∑–∞ –∑–∞–ø—É—Ç–∞–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –∏ –º–Ω–æ–≥–æ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –æ—Ç–≤–ª–µ–∫–∞—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å ownCloud, –≤—Å–ø–ª—ã–≤–∞—é—â–µ–µ –æ–∫–Ω–æ —Å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º —Å–∫–∞—á–∞—Ç—å –º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —á–∞—Å—Ç–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –Ω–µ–ø—Ä–µ–æ–¥–æ–ª–∏–º—ã–º –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–µ–º –¥–ª—è –∞–≥–µ–Ω—Ç–∞. –ß–µ–ª–æ–≤–µ–∫ –ø—Ä–æ—Å—Ç–æ –∑–∞–∫—Ä—ã–ª –±—ã —ç—Ç–æ –æ–∫–Ω–æ, –∞ –∞–≥–µ–Ω—Ç –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –≤ —Ç—É–ø–∏–∫–µ. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ ownCloud –∞–≥–µ–Ω—Ç—É —Å–ª–æ–∂–Ω–æ –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Å–ø–ª—ã–≤–∞—é—â–∏—Ö –æ–∫–æ–Ω, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–∫—Ä—ã—Ç—å –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –∑–∞–≥—Ä—É–∑–∫–∏.</p>\n<p><strong>–°–∞–º–æ–æ–±–º–∞–Ω:</strong> –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö, –∫–æ–≥–¥–∞ –∞–≥–µ–Ω—Ç –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç, –∫–∞–∫ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –¥–∞–ª—å—à–µ, –æ–Ω –ø—ã—Ç–∞–µ—Ç—Å—è \"—Å—Ö–∏—Ç—Ä–∏—Ç—å\", —Å–æ–∑–¥–∞–≤–∞—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—Ö–æ–¥–Ω—ã–µ –ø—É—Ç–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–ª–æ–∂–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ –∑–∞–¥–∞—á–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –æ–¥–Ω–æ–π –∏–∑ –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ –≤ RocketChat, —á—Ç–æ–±—ã –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –æ–Ω –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª –¥—Ä—É–≥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–∏—Å–≤–æ–∏–≤ –µ–º—É –∏–º—è –Ω—É–∂–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, —Ç–µ–º —Å–∞–º—ã–º –ø—ã—Ç–∞—è—Å—å –æ–±–º–∞–Ω—É—Ç—å —Å–∞–º–æ–≥–æ —Å–µ–±—è.</p>"
            },
            {
                "title": "Implications And Future Directions",
                "content": "In this paper, we present TheAgentCompany, new benchmark that stands out because it specifically focuses on real-world tasks that would be tackled within the context of real-world work. Unsurprisingly, current state-of-the-art agents fail to solve majority of the tasks, suggesting that there is big gap for current AI agents to autonomously perform most of the jobs human worker would do, even in relatively simplified benchmarking setting. Looking at how different models perform on different types of tasks, we argue that tasks that involve social interaction with other humans, navigating through complex user interfaces designed for professionals, and tasks that are typically performed in private, without significant open and publicly available resources, are the most challenging. However, we believe that currently new LLMs are making significant progress: not only are they becoming more and more capable in terms of raw performance, but also more cost-efficient (e.g. Gemini 2.0 Flash). Open-weights models are closing the gap between proprietary frontier models too, and the newer models are getting smaller (e.g. Llama 3.3 70B) but with equivalent performance to previous huge models, also showcasing that efficiency will further improve. That said, this is just first step towards forming firmer grasp on how AI may affect the tasks performed within workspace, and it has its limitations. First, our tasks are generally on the more straightforward side due to the need to automatically evaluate with programs and test cases, and we do not cover more complex creative tasks such as brainstorming new product ideas or designing system architectures. Second, we are only using one agent scaffold as the baseline performance, and others may differ in performance. Third, while it would be interesting to know the actual performance of human professionals on these tasks to understand how LLM agents perform in 13 Preprint. comparison, due to resource limitations we were not able to perform this comparison in the current iteration of TheAgentCompany. Fourth, the topic and content of the tasks were mostly created through introspection by people familiar with these workspaces, which may result in some disconnect with actual tasks performed in enterprise settings. Based on this, there are many future directions for further improvement of TheAgentCompany or other related benchmarks in this space. These include further expanding the benchmark tasks to those encountered in other industries, or tasks that require physical labor. Benchmarking may also be expanded with tasks that have more vague intents to better simulate real-world scenarios where the goal is not immediately clear at the very beginning. Further, benchmarks could also be expanded to include higher-level longer-horizon tasks such as conceptualizing new product and carrying it to execution. We hope that TheAgentCompany provides first step, but not the only step, towards these goals, and that we or others may build upon the open source release of TheAgentCompany to further expand in these promising directions.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω TheAgentCompany ‚Äî –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–¥–µ–ª—è–µ—Ç—Å—è —Ç–µ–º, —á—Ç–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –∑–∞–¥–∞—á–∏, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—á–µ–π —Å—Ä–µ–¥–µ. –ö–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å, —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º —ç—Ç–∏—Ö –∑–∞–¥–∞—á, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —Ä–∞–∑—Ä—ã–≤–µ –º–µ–∂–¥—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –ò–ò –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–±–æ—Ç—É, –∫–æ—Ç–æ—Ä—É—é –¥–µ–ª–∞–µ—Ç –æ–±—ã—á–Ω—ã–π —á–µ–ª–æ–≤–µ–∫, –¥–∞–∂–µ –≤ —É–ø—Ä–æ—â–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.</p>\n<p>–ê–Ω–∞–ª–∏–∑–∏—Ä—É—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á, –∞–≤—Ç–æ—Ä—ã –ø—Ä–∏—à–ª–∏ –∫ –≤—ã–≤–æ–¥—É, —á—Ç–æ –Ω–∞–∏–±–æ–ª—å—à—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç –∑–∞–¥–∞—á–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ–º, –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π –ø–æ —Å–ª–æ–∂–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–º –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á–∏, –≤—ã–ø–æ–ª–Ω—è–µ–º—ã–µ –≤ —á–∞—Å—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, –±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ—Ç–∫—Ä—ã—Ç—ã–º —Ä–µ—Å—É—Ä—Å–∞–º.</p>\n<p>–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –∞–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –û–Ω–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–µ —Ç–æ–ª—å–∫–æ –±–æ–ª–µ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–º–∏, –Ω–æ –∏ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ç–∫—Ä—ã—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–æ–≥–æ–Ω—è—é—Ç –ø—Ä–æ–ø—Ä–∏–µ—Ç–∞—Ä–Ω—ã–µ, –∞ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –º–µ–Ω—å—à–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Llama 3.3 70B), —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏ —ç—Ç–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ –ø–æ–≤—ã—à–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.</p>\n<p>–û–¥–Ω–∞–∫–æ, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–æ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —à–∞–≥ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —Ç–æ–≥–æ, –∫–∞–∫ –ò–ò –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã, –∏ —É —ç—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –µ—Å—Ç—å —Å–≤–æ–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è. –í–æ-–ø–µ—Ä–≤—ã—Ö, –∑–∞–¥–∞—á–∏ –≤ TheAgentCompany –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—ã, –ø–æ—Å–∫–æ–ª—å–∫—É —Ç—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–≥—Ä–∞–º–º –∏ —Ç–µ—Å—Ç–æ–≤. –ë–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–æ–∑–≥–æ–≤–æ–π —à—Ç—É—Ä–º –Ω–æ–≤—ã—Ö –∏–¥–µ–π –∏–ª–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–∏—Å—Ç–µ–º, –Ω–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∞–≥–µ–Ω—Ç, –∏ –¥—Ä—É–≥–∏–µ –º–æ–≥—É—Ç –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –í-—Ç—Ä–µ—Ç—å–∏—Ö, –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤-–ª—é–¥–µ–π, —Ö–æ—Ç—è —Ç–∞–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã–ª–æ –±—ã –ø–æ–ª–µ–∑–Ω—ã–º. –í-—á–µ—Ç–≤–µ—Ä—Ç—ã—Ö, –∑–∞–¥–∞—á–∏ –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–∏—á–Ω–æ–≥–æ –æ–ø—ã—Ç–∞ –ª—é–¥–µ–π, –∑–Ω–∞–∫–æ–º—ã—Ö —Å —Ä–∞–±–æ—á–µ–π —Å—Ä–µ–¥–æ–π, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ–∫–æ—Ç–æ—Ä–æ–º—É —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—é —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Å—Ä–µ–¥–µ.</p>\n<p>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è TheAgentCompany –∏ –¥—Ä—É–≥–∏—Ö –ø–æ–¥–æ–±–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–∞ –∑–∞–¥–∞—á –¥–ª—è –æ—Ö–≤–∞—Ç–∞ –¥—Ä—É–≥–∏—Ö –æ—Ç—Ä–∞—Å–ª–µ–π, –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ —Ç—Ä—É–¥–∞, –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞—á —Å –±–æ–ª–µ–µ —Ä–∞—Å–ø–ª—ã–≤—á–∞—Ç—ã–º–∏ —Ü–µ–ª—è–º–∏, —á—Ç–æ–±—ã –ª—É—á—à–µ –∏–º–∏—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Ç–µ—Å—Ç—ã, –≤–∫–ª—é—á–∏–≤ –≤ –Ω–∏—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞ –∏ –µ–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è. –ê–≤—Ç–æ—Ä—ã –Ω–∞–¥–µ—é—Ç—Å—è, —á—Ç–æ TheAgentCompany —Å—Ç–∞–Ω–µ—Ç –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º –Ω–∞ –ø—É—Ç–∏ –∫ —ç—Ç–∏–º —Ü–µ–ª—è–º –∏ —á—Ç–æ –¥—Ä—É–≥–∏–µ —Å–º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã–π —Ä–µ–ª–∏–∑ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –≤ —ç—Ç–∏—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö.</p>"
            },
            {
                "title": "Author Contributions",
                "content": "This work was an open source collaborative effort between multiple institutions and many independent individuals. We used point-based system to determine contributions and award authorship. Frank Xu, Boxuan Li and Yufan Song led the project, coordinating overall development and paper writing efforts. Detailed contributions were as follows: Task Design: Frank Xu, Yufan Song, Boxuan Li, Zora Wang, Shuyan Zhou, Graham Neubig Infrastructure Development: Yufan Song, Boxuan Li Experiment: Boxuan Li, Frank Xu, Yufan Song Sotopia Integration: Yufan Song, Xuhui Zhou Task Development: Boxuan Li, Yufan Song, Frank Xu, Graham Neubig, Yuxuan Tang, Mengxue Bao, Kritanjali Jain, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Yiqing Xie, Zora Wang, Xuhui Zhou, Wayne Chi, Lawrence Jang Ideation, Discussion and Formulation: Frank Xu, Shuyan Zhou, Xuhui Zhou, Zora Wang, Wayne Chi, Yufan Song, Boxuan Li, Lawrence Jang, Graham Neubig Advising: Graham Neubig advised the project, providing guidance, resources, and substantial paper editing.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–µ–π —Å–æ–±–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π, –≤–∫–ª–∞–¥ –∫–∞–∂–¥–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª—è–ª—Å—è —Å –ø–æ–º–æ—â—å—é –±–∞–ª–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã. –ê–≤—Ç–æ—Ä–∞–º–∏ —Å—Ç–∞—Ç—å–∏ —è–≤–ª—è—é—Ç—Å—è –≤—Å–µ, –∫—Ç–æ –≤–Ω–µ—Å –∑–Ω–∞—á–∏–º—ã–π –≤–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç. –û—Å–Ω–æ–≤–Ω—É—é –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–ª–∏ –§—Ä—ç–Ω–∫ –°—é–π, –ë–æ—Å—é–∞–Ω—å –õ–∏ –∏ –Æ–π—Ñ–∞–Ω—å –°—É–Ω. </p>\n<p>–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∫–ª–∞–¥–∞ –º–µ–∂–¥—É —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –±—ã–ª–æ —Å–ª–µ–¥—É—é—â–∏–º:\n*   <strong>–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏:</strong> –§—Ä—ç–Ω–∫ –°—é–π, –Æ–π—Ñ–∞–Ω—å –°—É–Ω, –ë–æ—Å—é–∞–Ω—å –õ–∏, –ó–æ—Ä–∞ –í–∞–Ω–≥, –®—É—è–Ω—å –ß–∂–æ—É –∏ –ì—Ä—ç–º –ù—å—é–±–∏–≥.\n*   <strong>–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:</strong> –Æ–π—Ñ–∞–Ω—å –°—É–Ω –∏ –ë–æ—Å—é–∞–Ω—å –õ–∏.\n*   <strong>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:</strong> –ë–æ—Å—é–∞–Ω—å –õ–∏, –§—Ä—ç–Ω–∫ –°—é–π –∏ –Æ–π—Ñ–∞–Ω—å –°—É–Ω.\n*   <strong>–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Sotopia:</strong> –Æ–π—Ñ–∞–Ω—å –°—É–Ω –∏ –°—é–π—Ö—É—ç–π –ß–∂–æ—É.\n*   <strong>–†–∞–∑–≤–∏—Ç–∏–µ –∑–∞–¥–∞—á–∏:</strong> –ë–æ—Å—é–∞–Ω—å –õ–∏, –Æ–π—Ñ–∞–Ω—å –°—É–Ω, –§—Ä—ç–Ω–∫ –°—é–π, –ì—Ä—ç–º –ù—å—é–±–∏–≥, –Æ—Å—é–∞–Ω—å –¢–∞–Ω, –ú—ç–Ω—Å—é—ç –ë–∞–æ, –ö—Ä–∏—Ç–∞–Ω–¥–∂–∞–ª–∏ –î–∂–∞–π–Ω, –ß–∂–∏—Ç—É–Ω –ì–æ, –ú—É—Ä–æ–Ω–≥ –¶–∞–æ, –ú–∏–Ω—ä—è–Ω –Ø–Ω, –•–∞–æ –Ø–Ω –õ—É, –ê–º–∞–∞–¥ –ú–∞—Ä—Ç–∏–Ω, –ß–∂—ç –°—É, –õ–µ–∞–Ω–¥–µ—Ä –ú–∞–±–µ–Ω, –†–∞–¥–∂ –ú–µ—Ö—Ç–∞, –ò—Ü–∏–Ω –°–µ, –ó–æ—Ä–∞ –í–∞–Ω–≥, –°—é–π—Ö—É—ç–π –ß–∂–æ—É, –£—ç–π–Ω –ß–∏ –∏ –õ–æ—É—Ä–µ–Ω—Å –î–∂–∞–Ω–≥. \n*   <strong>–ò–¥–µ–π–Ω–æ–µ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏–µ, –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞:</strong> –§—Ä—ç–Ω–∫ –°—é–π, –®—É—è–Ω—å –ß–∂–æ—É, –°—é–π—Ö—É—ç–π –ß–∂–æ—É, –ó–æ—Ä–∞ –í–∞–Ω–≥, –£—ç–π–Ω –ß–∏, –Æ–π—Ñ–∞–Ω—å –°—É–Ω, –ë–æ—Å—é–∞–Ω—å –õ–∏, –õ–æ—É—Ä–µ–Ω—Å –î–∂–∞–Ω–≥ –∏ –ì—Ä—ç–º –ù—å—é–±–∏–≥.\n*   <strong>–ù–∞—É—á–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ:</strong> –ì—Ä—ç–º –ù—å—é–±–∏–≥ —Ä—É–∫–æ–≤–æ–¥–∏–ª –ø—Ä–æ–µ–∫—Ç–æ–º, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏, —Ä–µ—Å—É—Ä—Å—ã –∏ –æ—Å—É—â–µ—Å—Ç–≤–ª—è—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–∞–∂–¥—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ –≤–Ω–µ—Å —Å–≤–æ–π –≤–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç, –∞ –æ–±—â–∞—è —Ä–∞–±–æ—Ç–∞ —Å—Ç–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã—Ö —É—Å–∏–ª–∏–π.</p>"
            }
        ]
    }
]