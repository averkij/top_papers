[
    {
        "id": "2411.18279",
        "title": "Large Language Model-Brained GUI Agents: A Survey",
        "url": "https://arxiv.org/pdf/2411.18279",
        "abstract": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-27",
        "pub_date_card": {
            "ru": "27 ноября",
            "en": "November 27",
            "zh": "11月27日"
        },
        "hash": "ad1bbf8a8e8b79d9",
        "authors": [
            "Chaoyun Zhang",
            "Shilin He",
            "Jiaxu Qian",
            "Bowen Li",
            "Liqun Li",
            "Si Qin",
            "Yu Kang",
            "Minghua Ma",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "affiliations": [
            "Microsoft AI, Microsoft, China",
            "M365 Research, Microsoft, USA",
            "Shanghai Artificial Intelligence Laboratory, China",
            "Peking University, China"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.18279.jpg",
        "data": {
            "categories": [
                "#benchmark",
                "#dataset",
                "#agents",
                "#multimodal",
                "#survey"
            ],
            "emoji": "🤖",
            "ru": {
                "title": "LLM-агенты: революция в автоматизации графических интерфейсов",
                "desc": "Это обзорная статья о ГПИ-агентах на основе больших языковых моделей (LLM). В ней рассматривается эволюция, ключевые компоненты и передовые методы этих агентов, способных автоматизировать взаимодействие с графическим интерфейсом пользователя на основе естественно-языковых инструкций. Статья анализирует существующие фреймворки, методы сбора данных, разработку специализированных моделей действий и метрики оценки эффективности ГПИ-агентов. Авторы также определяют пробелы в исследованиях и намечают план дальнейшего развития этой области."
            },
            "en": {
                "title": "Revolutionizing GUI Interaction with LLM Agents",
                "desc": "This paper explores the rise of LLM-brained GUI agents, which leverage large language models to automate interactions with graphical user interfaces. These agents can understand natural language commands and perform complex tasks across various platforms, enhancing user experience significantly. The survey covers the historical development, essential components, and advanced techniques related to these agents, while also addressing key research questions and identifying gaps in the current knowledge. By providing a structured overview, the paper aims to guide future research and practical applications in this rapidly evolving field."
            },
            "zh": {
                "title": "LLM驱动的GUI代理：革新用户交互体验",
                "desc": "本论文探讨了基于大型语言模型（LLM）的图形用户界面（GUI）代理的最新进展。这些代理能够理解自然语言指令，并自动执行复杂的多步骤任务，极大地提升了用户与软件的交互体验。论文还分析了GUI代理的历史演变、核心组件和先进技术，并提出了未来研究的方向。通过对现有框架和评估指标的探讨，本文为研究人员和从业者提供了宝贵的指导。"
            }
        }
    },
    {
        "id": "2411.15124",
        "title": "TÜLU 3: Pushing Frontiers in Open Language Model Post-Training",
        "url": "https://arxiv.org/pdf/2411.15124",
        "abstract": "Language model post-training is applied to refine behaviors and unlock new\nskills across a wide range of recent language models, but open recipes for\napplying these techniques lag behind proprietary ones. The underlying training\ndata and recipes for post-training are simultaneously the most important pieces\nof the puzzle and the portion with the least transparency. To bridge this gap,\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\nmodels, alongside its data, code, and training recipes, serving as a\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\non Llama 3.1 base models, achieves results surpassing the instruct versions of\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\nClaude 3.5-Haiku. The training algorithms for our models include supervised\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\nintroduce a multi-task evaluation scheme for post-training recipes with\ndevelopment and unseen evaluations, standard benchmark implementations, and\nsubstantial decontamination of existing open datasets on said benchmarks. We\nconclude with analysis and discussion of training methods that did not reliably\nimprove performance.\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\ncuration and evaluation, the training code and infrastructure, and, most\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\n3 approach to more domains.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-22",
        "pub_date_card": {
            "ru": "22 ноября",
            "en": "November 22",
            "zh": "11月22日"
        },
        "hash": "18e2114fabdb2867",
        "authors": [
            "Nathan Lambert",
            "Jacob Morrison",
            "Valentina Pyatkin",
            "Shengyi Huang",
            "Hamish Ivison",
            "Faeze Brahman",
            "Lester James V. Miranda",
            "Alisa Liu",
            "Nouha Dziri",
            "Shane Lyu",
            "Yuling Gu",
            "Saumya Malik",
            "Victoria Graf",
            "Jena D. Hwang",
            "Jiangjiang Yang",
            "Ronan Le Bras",
            "Oyvind Tafjord",
            "Chris Wilhelm",
            "Luca Soldaini",
            "Noah A. Smith",
            "Yizhong Wang",
            "Pradeep Dasigi",
            "Hannaneh Hajishirzi"
        ],
        "affiliations": [
            "Allen Institute for AI",
            "University of Washington"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.15124.jpg",
        "data": {
            "categories": [
                "#rl",
                "#training",
                "#optimization",
                "#rlhf",
                "#open_source",
                "#benchmark",
                "#dataset",
                "#data"
            ],
            "emoji": "🧠",
            "ru": {
                "title": "Открытые рецепты для передовых языковых моделей",
                "desc": "Статья представляет T\"ULU 3 - семейство открытых языковых моделей, прошедших пост-обучение. Авторы предоставляют полный набор инструментов, включая данные, код и методики обучения, для воспроизведения и адаптации их подхода. T\"ULU 3 превосходит многие современные модели, используя методы обучения с учителем, Direct Preference Optimization и новый метод Reinforcement Learning with Verifiable Rewards. Исследование также вводит многозадачную схему оценки и обсуждает методы, не улучшившие производительность."
            },
            "en": {
                "title": "Unlocking Language Model Potential with T\"ULU 3",
                "desc": "This paper presents T\"ULU 3, a set of fully-open post-trained language models that enhance performance and capabilities beyond existing models. It addresses the lack of transparency in post-training techniques by providing open access to training data, code, and methodologies. The models utilize advanced training methods such as supervised finetuning, Direct Preference Optimization, and a new approach called Reinforcement Learning with Verifiable Rewards. T\"ULU 3 not only surpasses the performance of several proprietary models but also offers a comprehensive framework for evaluating and adapting post-training techniques across various applications."
            },
            "zh": {
                "title": "T\"ULU 3：开放的后训练模型新纪元",
                "desc": "本文介绍了T\"ULU 3，这是一个完全开放的最新后训练模型系列，旨在提高语言模型的行为和技能。我们提供了训练数据、代码和训练配方，帮助研究人员更好地理解和应用后训练技术。T\"ULU 3在多个基准测试中超越了现有的闭源模型，展示了其优越的性能。我们还提出了一种多任务评估方案，以便更全面地评估后训练配方的效果。"
            }
        }
    }
]