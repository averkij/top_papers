[
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ —Å–∏—Ö –ø–æ—Ä –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –≠—Ç–∏ —É—Å–∏–ª–∏—è –ø—Ä–∏–≤–µ–ª–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –í —Ö–æ–¥–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ \"–ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–¥–∞—é—â–µ–π—Å—è —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ- –∏ –Ω–∏–∑–∫–æ-—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏. –°–æ–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∏ —É—Å–∏–ª–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Large Language Models, LLMs), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3, GPT-4, PaLM –∏ LLaMA, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Natural Language Processing, NLP), –ø–æ–∫–∞–∑—ã–≤–∞—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –±–ª–∞–≥–æ–¥–∞—Ä—è —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤, –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∞–Ω–≥–ª–∏–π—Å–∫–æ–º. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. </p>\n<p>–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–æ–≤ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –∏—Ö –Ω–æ—Å–∏—Ç–µ–ª–µ–π –≤–Ω–µ –≤—ã–≥–æ–¥ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –ß—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–æ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å, –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>Marco-LLM —Å—Ç—Ä–µ–º–∏—Ç—Å—è –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP. –î–ª—è —ç—Ç–æ–≥–æ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2. –≠—Ç–∞ –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, —á—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Marco-LLM, –∫–æ—Ç–æ—Ä–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMMLU, Flores, Belebele, AGIEval, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–î–∞–ª—å–Ω–µ–π—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–±–æ—Ä–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —ç—Ç–∞–ø—ã –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Cross-Lingual Enhancement",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Å –æ–±–∑–æ—Ä–æ–º –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —É–∫–∞–∑–∞–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å, F1-–æ—Ü–µ–Ω–∫–∞ –∏–ª–∏ BLEU). –í —Ç–∞–±–ª–∏—Ü–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —è–∑—ã–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Ç–µ—Å—Ç—ã, –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:</p>\n<ol>\n<li>\n<p><strong>ARC (AI2 Reasoning Challenge)</strong> ‚Äì –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞—É–∫–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ—Ç –º–æ–¥–µ–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π. –í–æ–ø—Ä–æ—Å—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ –¥–≤–∞ —Ç–∏–ø–∞: –ø—Ä–æ—Å—Ç—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏ —Å–ª–æ–∂–Ω—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</p>\n</li>\n<li>\n<p><strong>Belebele</strong> ‚Äì —ç—Ç–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á—Ç–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞ 122 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ —Ç–∏–ø—ã. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ—Ç—Ä—ã–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞, –∑–∞–ø—Ä–æ—Å–∞ –∏ —á–µ—Ç—ã—Ä—ë—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞.</p>\n</li>\n<li>\n<p><strong>CEVAL</strong> ‚Äì —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏–º–µ–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>Flores</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –í–∫–ª—é—á–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö.</p>\n</li>\n<li>\n<p><strong>HellaSwag</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –≤—ã–≤–æ–¥—É. –°–æ–¥–µ—Ä–∂–∏—Ç 70000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≥–¥–µ –æ–¥–∏–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏ —Ç—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>MMLU (Massive Multitask Language Understanding)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–Ω–∞–Ω–∏–π –∏ –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —É –ò–ò-–º–æ–¥–µ–ª–µ–π. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 57 –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 57000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>TyDiQA (Typologically Diverse Question Answering)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –°–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 200000 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –≤–∑—è—Ç—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏.</p>\n</li>\n</ol>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ç–µ—Å—Ç–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ —Ç–µ—Å—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 7B –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ª—É—á—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–≥–æ –º–æ–¥–µ–ª—å—é Qwen2-7B (81.8). –ù–∞ Belebele, –æ—Ü–µ–Ω–∏–≤–∞—é—â–µ–º –∑–Ω–∞–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, Marco-Chat-7B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, —Ç–∞–∫–∂–µ –æ–ø–µ—Ä–µ–∂–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 70B –ª–∏–¥–µ—Ä–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Marco-72B, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 76.1 –Ω–∞ MMMLU, —á—Ç–æ –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –≤—ã—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ Llama3.1-70B. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –æ–±—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å–∫—Ä–∏–ø—Ç–æ–≤, Marco-72B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –ª—É—á—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–≥–æ –º–µ—Å—Ç–∞. –¢–∞–∫–∂–µ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –Ω–∞ CEval (94.5) –∏ Belebele (89.6), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</p>\n<p>–¢–∞–±–ª–∏—Ü–∞ 9 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MMMLU –¥–ª—è 7B –∏ 70B –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å Marco-Chat-7B –ª–∏–¥–∏—Ä—É–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ –≤—Å–µ–º —è–∑—ã–∫–∞–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 7B, –∞ –º–æ–¥–µ–ª—å Marco-72B ‚Äî –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 70B.</p>"
            }
        ]
    },
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–æ–≤–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É—Å–∏–ª–∏–π –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –ë–ª–∞–≥–æ–¥–∞—Ä—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–º –æ—Ü–µ–Ω–∫–∞–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –ª—é–±—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö \"–ª—é–±–æ–π-–∫-–ª—é–±–æ–º—É\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Ç–µ–º —Å–∞–º—ã–º —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ —Å –≤—ã—Å–æ–∫–∏–º–∏ –∏ –Ω–∏–∑–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –û–±—ä–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∞ —Ä–∞–±–æ—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ –≤ –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024] –∏ LLaMA [Touvron et al., 2023b], –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –≤—ã—Å–æ–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –≠—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ —Ä–∞–∑—Ä—ã–≤—É –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –∏–∑-–∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –Ø–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≥–æ–≤–æ—Ä—è—â–∏–µ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≤ –≤—ã–≥–æ–¥–∞—Ö, –ø—Ä–∏–Ω–æ—Å–∏–º—ã—Ö –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP.</p>\n<p>–î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. Marco-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2 [Bai et al., 2023].</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ú–∞—Å—Å–æ–≤–æ–µ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ Marco-LLM.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<p>–†–∞–∑–¥–µ–ª 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö LLM –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è LLM.\n–†–∞–∑–¥–µ–ª 3 –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –º–æ–Ω–æ- –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏.\n–†–∞–∑–¥–µ–ª 4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Marco-LLM, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ —Ä–∞–∑–¥–µ–ª–µ 3.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Marco-LLM",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö —Å –º–æ–¥–µ–ª—è–º–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy), –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å F1 –∏–ª–∏ BLEU. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –æ—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.</p>\n<p>–í —Ä–∞–∑–¥–µ–ª–µ –æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ:</p>\n<ol>\n<li><strong>CEVAL</strong>: –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —è–∑—ã–∫–æ–≤—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞.</li>\n<li><strong>AGIEval</strong>: –í–æ–ø—Ä–æ—Å—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ GRE, GMAT, LSAT –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–∫–∑–∞–º–µ–Ω—ã.</li>\n<li><strong>ARC</strong> (AI2 Reasoning Challenge): –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –ø–æ—á—Ç–∏ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –Ω–∞—É—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —É—Ä–æ–≤–Ω—è —Å—Ä–µ–¥–Ω–µ–π —à–∫–æ–ª—ã. –í–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: –ø—Ä–æ—Å—Ç–æ–π –∏ —Å–ª–æ–∂–Ω—ã–π, —Ç—Ä–µ–±—É—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π.</li>\n</ol>\n<p>–î–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ:</p>\n<ol>\n<li><strong>X-MMLU</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XCOPA</strong>: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ã–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XStoryCloze</strong>: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –∏—Å—Ç–æ—Ä–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n</ol>\n<p>–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li><strong>TyDiQA</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ó–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.</li>\n</ol>\n<p>–ù–∞–∫–æ–Ω–µ—Ü, –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:</p>\n<ol>\n<li><strong>Flores</strong>: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∏ –Ω–∏–∑–∫–æ —Ä–µ—Å—É—Ä—Å–Ω—ã–µ —è–∑—ã–∫–∏.</li>\n</ol>\n<p>–≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ò–ò –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —è–∑—ã–∫–∞—Ö, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>\n<p>–ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–∞–∑–º–µ—Ä–æ–º –≤ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å Marco-Chat-7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Äî Qwen2-7B (81.8). –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–∏–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –Ω–∞ Belebele, –≥–¥–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤–ª–∞–¥–µ–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏, Marco-Chat-7B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—á—Ç–∏ –Ω–∞ 9 –ø—É–Ω–∫—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º 70 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤, –º–æ–¥–µ–ª—å Marco-72B —Ç–∞–∫–∂–µ –ª–∏–¥–∏—Ä—É–µ—Ç –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É 76.1 –Ω–∞ MMMLU, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –≠—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –ª—É—á—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏, Llama3.1-70B. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, Marco-72B –Ω–∞–±–∏—Ä–∞–µ—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–∏–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ —è–∑—ã–∫–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –∏ –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-72B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 94.5 –Ω–∞ CEval, –ø–æ–∫–∞–∑—ã–≤–∞—è –æ—Ç–ª–∏—á–Ω—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –ù–∞ Belebele –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ 89.6, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —è–∑—ã–∫–∞—Ö –ê—Ñ—Ä–∏–∫–∏.</p>"
            }
        ]
    },
    {
        "id": "2412.04363",
        "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
        "url": "https://huggingface.co/papers/2412.04363",
        "abstract": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "4fa16cf75a2af540",
        "authors": [
            "Wenting Zhao",
            "Alexander M. Rush",
            "Tanya Goyal"
        ],
        "affiliations": [],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04363.jpg",
        "data": {
            "categories": [
                "#rlhf",
                "#data",
                "#security",
                "#benchmark",
                "#ethics"
            ],
            "emoji": "üé≠",
            "ru": {
                "title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–≥—É—Ç –∏—Å–∫–∞–∑–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –ò–ò!",
                "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–∞–∂–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–ª–æ—Ö–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
            },
            "en": {
                "title": "Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings",
                "desc": "This paper examines the reliability of user-generated annotations on community-driven platforms like Chatbot Arena, which are used to evaluate the performance of large language models (LLMs). It identifies three main sources of poor-quality annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other malicious behaviors. The authors demonstrate that even a small percentage of these low-quality votes can significantly impact the rankings of models, potentially shifting them by several places. The paper concludes by highlighting the ongoing challenges in maintaining high-quality human annotations to ensure trustworthy leaderboard results."
            },
            "zh": {
                "title": "Á°Æ‰øùÈ´òË¥®ÈáèÊ≥®ÈáäÔºåÊèêÂçáÊ®°ÂûãÊéíÂêçÁöÑÂèØÈù†ÊÄß",
                "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂºÄÊîæÁ§æÂå∫Âπ≥Âè∞ÔºàÂ¶ÇChatbot ArenaÔºâÂú®Êî∂ÈõÜÁî®Êà∑ÂÅèÂ•ΩÊï∞ÊçÆÊó∂Èù¢‰∏¥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÊù•Ëá™Êó†Âä®Êú∫ÊàñÊÅ∂ÊÑèÁî®Êà∑ÁöÑ‰ΩéË¥®ÈáèÊ≥®Èáä‰ºö‰∏•ÈáçÂΩ±ÂìçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊéíÂêç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ªÖ10%ÁöÑ‰ΩéË¥®ÈáèÊäïÁ•®Â∞±ÂèØËÉΩÂØºËá¥Ê®°ÂûãÂú®ÊéíË°åÊ¶ú‰∏äÊéíÂêçÂèòÂåñÂ§öËææ5‰Ωç„ÄÇÊúÄÂêéÔºåÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁ°Æ‰øùÈ´òË¥®Èáè‰∫∫Á±ªÊ≥®ÈáäÁöÑÂºÄÊîæÊÄßÊåëÊàò„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.",
                "summary": "<p>–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å–∞–π—Ç–∞, –∑–∞–≤–æ–µ–≤–∞–ª–∏ —Ä–µ–ø—É—Ç–∞—Ü–∏—é –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM. –•–æ—Ç—è —Å–µ–π—á–∞—Å —ç—Ç–æ —Å—Ç–∞–ª–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º, —Å–ª–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –æ—Ç –ª—é–¥–µ–π. –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º, —á—Ç–æ —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –∫–∞–∫ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –¥—Ä—É–≥–∏—Ö, –º–æ–≥—É—Ç –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –ª–∏–¥–µ—Ä–æ–≤. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö (–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏ —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –¥–∞–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞) –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö (–∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏, —Å—Ç—Ä–µ–º—è—â–∏–µ—Å—è –∫ —Ä–∞–∑–¥—É–≤–∞–Ω–∏—é —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏) –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –¥–æ 5 –º–µ—Å—Ç –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ù–∞–∫–æ–Ω–µ—Ü, –º—ã –æ–±—Å—É–∂–¥–∞–µ–º –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.</p>"
            },
            {
                "title": "Introduction",
                "content": "Reliable evaluation of free-form text generation quality is long-standing challenge in NLP (Gehrmann et al., 2023; Celikyilmaz et al., 2020; Goyal et al., 2022a). Despite limitations, human annotation is widely accepted as the gold standard, especially for open-ended text generation tasks without an objective notion of correctness. As result, platforms such as Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024b) and WildVision Arena (Lu et al., 2024) that allow users to interact with available large language models (LLMs) and submit preference judgments for model pairs, have become extremely valuable resource in the NLP evaluation landscape. By providing free and easy access to available LLMs, these community-driven platforms are able to incentivize millions of user interactions1 and collect large-scale and diverse dataset of user queries and preferences. Deservedly, 1As of October 6, 2024, Chatbot Arena has collected 2,011,939 pairwise preference judgments. these peer production and community-driven platforms have emerged as one of the most trusted benchmarks in NLP today.2 Moreover, such benchmarks play crucial role in auditing automatic evaluators by providing the necessary ground truth rankings that any evaluator can be validated against. In fact, the most popular automatic evaluation benchmarks today, including AlpacaEval (Li et al., 2023), WildBench (Lin et al., 2024), MixEval (Ni et al., 2024) and Arena-Hard (Li et al., 2024), validate their metric by reporting high correlation with Chatbot Arena judgments. Given its far-reaching impact, both on human and automatic benchmarking of LLMs, and consequently on LLM research more broadly, it is crucial to ensure that the model rankings on these open community leaderboards are trustworthy. However, challenges with obtaining high-quality human judgments from non-expert crowdworkers like Chatbot Arena users are widely discussed in literature (Karpinska et al., 2021; Clark et al., 2021; Hosking et al., 2024). Moreover, these platforms typically implement minimal quality controls for verifying annotation quality such as attention checks, user verification, etc. This sits in direct opposition to the goals of trustworthiness. In this paper, we play devils advocate and ask: is it even possible to ensure the reliability of community-driven open platform, like Chatbot Arena, without sacrificing user scale? We approach this thought experiment from two angles. First, using Chatbot Arena as case study, we consider three different sources of poor quality preference judgments or votes in the collected dataset: un-incentivized or apathetic users providing random judgments (Section 3.1), malicious actors launching adversarial attacks to detect and artificially inflate target models ranking (Sec2As an example, Googles Chief Scientist used high performance on Chatbot Arena to declare the success of their recent model release: https://tinyurl.com/55xs2pz4. trol measures employed by Chatbot Arena. Notation Assume there are different models = {m1, m2, ..., mk} that need to be ranked on the leaderboard. Each new user on the platform submits query and receives outputs from two different models yi mi(x) and yj mj(x).3 The user has the option to submit preference label {i, j, tie}. In order to ensure that this annotation is unbiased, the names of the models that the two outputs are sampled from is only revealed to end users after they have submitted their preference annotation. This arena logs data points of the form: (x, yi, yj, mi, mj, l). These preferences are then used to estimate the pairwise win matrix between model pairs, i.e. p(mi > mj). Next, they estimate the coefficients of the Bradley-Terry model (Bradley and Terry, 1952) to obtain scores si for each model mi M. Models are sorted by si to obtain the final ranking. Quality control measures The arena employs list of filtering strategies: detecting malicious users according to certain distribution (Section 5.1; Chiang et al. (2024a)), bot detection by Cloudflare and Google reCAPTCHA v3, automatic categorization pipelines to filter out low-quality data45, placing limits on the number of votes each IP can provide in day, and deduplicating top 0.1% occurring prompts. However, these filtering strategies focus more on filtering bots than differentiating user votes with varying qualities. Therefore, we present results and discussions in this paper assuming minimal quality control checks in the backend to filter out bad quality user annotations6. Released Artifacts We conduct our experiments using the largest publicly released dataset by Chatbot Arena. It consists of 55k preference annotations7; it includes response pairs sampled from two of 64 unique models and the corresponding pairwise preference annotation. 3The arena employs an adaptive sampling strategy that favors model pairs with higher uncertainty in relative performance, and also newly introduced models. However, exact details are not publicly shared, possibly to mitigate gaming. 4https://blog.lmarena.ai/blog/2024/ hard-prompts/ 5https://blog.lmarena.ai/blog/2024/ arena-category/ 6https://github.com/lm-sys/FastChat/ 7https://huggingface.co/datasets/lmsys/ lmsys-arena-human-preference-55k Figure 1: Our characterization of sources of poor-quality votes on open data annotation platforms: (1) Apathetic: Users who lack intrinsic motivation may submit random votes. (2) Adversarial: Malicious users aim to manipulate rankings by upvoting target model. (3) Arbitrary: Users voting based on subjective preferences in response to open-ended questions. tion 3.2), and the inherent arbitrariness of preference votes for open-ended and subjective queries (Section 3.3). For the former two sources of votes, we show that small fractions of poor-quality judgments (either apathetic or adversarial) can have non-trivial impact on the target models rankings. Concerningly, poor annotations from either apathetic or adversarial voting are not easy to detect in post-hoc manner. Moreover, even carefully recruited and onboarded human annotators exhibit low inter-annotator agreement on subjective queries, making inter-annotator-based techniques to filter out low-quality annotations ineffective. Finally, we discuss open challenges in ensuring the reliability and human annotation quality in open-source community-driven benchmarks (Section 4). We strongly believe that open data collection platforms offer an invaluable resource for the academic community and have facilitated essential work in developing new automatic evaluators (Li et al., 2023; Lin et al., 2024; Ni et al., 2024), training and evaluating reward models (Lambert et al., 2024), etc. However, critical questions exist about their reliability, especially against adversarial attacks. We hope that our work will spur future research on quality control mechanisms for open platforms that power LLM evaluations.",
                "summary": "<p>–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –¥–∞–≤–Ω–µ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena –∏ WildVision Arena, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π, —Å—Ç–∞–ª–∏ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –≠—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–≤–æ–±–æ–¥–Ω—ã–π –∏ –ª–µ–≥–∫–∏–π –¥–æ—Å—Ç—É–ø –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –º–æ–¥–µ–ª—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¢–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å–µ–≥–æ–¥–Ω—è —è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–∏–º–∏ –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–æ–≤ –≤ NLP –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –ø—É—Ç–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö –æ—Ü–µ–Ω–æ–∫. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç –Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ Chatbot Arena. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –æ–±—ã—á–Ω–æ —Ä–µ–∞–ª–∏–∑—É—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ä—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —á—Ç–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ü–µ–ª—è–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–∞–∫–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –∫–∞–∫ Chatbot Arena, –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</p>"
            },
            {
                "title": "Background",
                "content": "In this paper, we run experiments with Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024a) as case study, although our insights are broadly applicable to other similar community-driven preference collection platforms. Below, we describe the preference collection pipeline and quality conModel Leaderboard Ranking Orig. r=1 r= r=10 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 36 21 39 382 201 412 382 21 345 41 existing studies characterizing the incentives or behaviors of an average user on open platforms like Chatbot Arena. Therefore, we have no way of estimating the fraction of apathetic. Table 1: Change in leaderboard rankings for 3 test models based on different percentages (r) of arbitrary votes. The subscripts denote gain () or loss () in rankings. We find that only 10% poor quality annotations can change the rank of 2/3 systems by 5 places.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π Chatbot Arena –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞, —Ö–æ—Ç—è –Ω–∞—à–∏ –≤—ã–≤–æ–¥—ã –ø—Ä–∏–º–µ–Ω–∏–º—ã –∏ –∫ –¥—Ä—É–≥–∏–º –ø–æ–¥–æ–±–Ω—ã–º –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º. –ù–∏–∂–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ú—ã –æ—Ç–º–µ—á–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–∏—Ö —Å—Ç–∏–º—É–ª—ã –∏–ª–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ü–æ—ç—Ç–æ–º—É —É –Ω–∞—Å –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∏—Ç—å –¥–æ–ª—é –±–µ–∑—Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –¢–∞–±–ª–∏—Ü–µ 1 –ø–æ–∫–∞–∑–∞–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ –¥–ª—è —Ç—Ä–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (r) –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ü–æ–¥—Å—Ç—Ä–æ—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –æ–±–æ–∑–Ω–∞—á–∞—é—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ (+) –∏–ª–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ (-) –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ú—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ 2/3 —Å–∏—Å—Ç–µ–º –Ω–∞ 5 –ø–æ–∑–∏—Ü–∏–π.</p>"
            },
            {
                "title": "Votes and Their Impact",
                "content": "For our thought experiment, we hypothesize that there exist three potential sources of poor quality votes on open platforms: (a) apathetic votes by users that are un-incentivized, (b) adversarial votes that aim to inflate the ranking of target model, and (c) arbitrary votes on difficult to meaningfully distinguish response pairs. For each of these, we study their impact on model rankings and the challenges in mitigating them. 3.1 Apathetic Voting The main attraction of open community platforms for end users is that they expose free and easy-touse API endpoint for LLMs. This incentivizes diverse users to interact with the platform and submit queries to explore their use cases. However, these platforms do not explicitly incentivize high-quality preference annotation. We hypothesize that at least r% of users on the arena are apathetic and provide random or low-quality votes on the platform. Setup We run experiments on Chatbot Arenas dataset of 55k preferences (discussed in Section 2). We assume that this dataset reflects true rankings of models based on gold human preferences. We study the change in model rankings for 3 arbitrarily selected models: Llama-2-7b-chat, Mistral-7binstruct-v0.2, and Llama-2-13b-chat, assuming r% of these preferences were instead assigned random labels by apathetic users during data collection. Results Table 1 summarizes our results. We find that only 10% of apathetic votes in the dataset can change the leaderboard rankings of 2/3 models by 5 places (namely Llama-2-13b-chat and Mistral-7b-instruct-v0.2).8 Note that there are no 8Note that model frequency also impacts its susceptibility to ranking changes. All three models we inspect collectively occur in less than 10% of all data samples. Discussion: Can we detect and remove apathetic votes? major challenge in detecting apathetic votes is that they are often indistinguishable from arbitrary votes. Multiple past studies have found that output-level comparisons using single label is ill-defined as an annotation task (Krishna et al., 2023; Goyal et al., 2022a) as users often rely on different criteria and disagree with each other. This ambiguity makes it hard to ascertain whether observed disagreements are due to personal variations in quality assessment (arbitrary voting, discussed further in Section 3.3) or due to apathetic or low-quality annotations by certain annotators. Despite challenges with detecting individual apathetic votes, detecting apathetic users may be viable by computing agreements between model rankings by individual users. This strategy is based on the intuition that while annotators might disagree on specific examples, their aggregate systemlevel judgments tend to be more aligned (Goyal et al., 2022a). Finally, requesting additional justifications for votes, such as free-text rationales, can also help discourage apathetic votes. We discuss this more in Section 4. 3.2 Adversarial Voting We assume there exists malicious developer who seeks to inflate the rankings of their own target model mT on the arena leaderboard A. We argue that due to the lack of quality controls (e.g. user verification, attention checks, etc.), it is straightforward to inject preference votes for mT using simple attack methodology. Our main component is target model attribution algorithm which, given query-output pair (q, y), predicts whether is sampled from the target model mT (q). Given such an algorithm, we can inflate the ranking of the target model mT using the following strategy: (1) Enter prompt on the arena, (2) Detect if any of the two shown outputs y1, y2 are sampled from mT , (3) If yes, vote for the target model mT , (4) Repeat. Target model attribution algorithm We assume that the attribution algorithm has access to the target model logits. This is reasonable assumption for our setting where model developer seeks to Model Leaderboard Ranking Orig. r=1 r=5 r=10 r=100 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 232 174 21 363 325 289 342 342 297 121 139 234 Table 2: Change in leaderboard rankings for 3 test models based on different percentages (r) of adversarial votes (upvoting the target model). We find that only 10% adversarial annotations can change the rank of all systems by more than 4 places. Model TPR TNR #Tokens Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0. 91.13 88.46 100.00 89.93 91.28 86.69 328.06 326.53 319.46 Table 3: Intrinsic eval. of model attribution algorithm inflate rankings. Our simple attribution algorithm is outlined in Algorithm 1 in Appendix A. Essentially, we use teacher-forcing to determine the probability distribution over the vocabulary for all tokens at time step t, i.e. PmT (.x, y1, ...yt1). We sort the tokens in descending order of probability to identify the smallest subset of tokens that cover cumulative probability mass of at least p. We compute the fraction of generation time steps for which the actual generated token yt falls within this top-p probability subset. We compare this against threshold to classify generations = y1...yN as being sampled from mT or not. Intrinsic Evaluation of Detector Algorithm For all three test models, we report the true positive rate (TPR) and true negative rate (TNR) on the arena dataset in Table 3. We find that our detector algorithm reports very high performance (e.g. TPR=91.13%, and TNR=88.46% for Llama-2-7bchat). We also find positive correlation between the number of tokens and TPRs, which can be leveraged in the attack. Note that malicious actors can always improve the detector accuracy further using watermarking techniques (Kirchenbauer et al., 2023). Next, we use these highly performant models to cast adversarial votes. Can we influence voting on the live Chatbot Arena platform? We also implement proofof-concept of real attack on Chatbot Arena to demonstrate that current guardrails, such as bot detection, can be bypassed easily. On October 13, 2024, we programmatically launched 100 queries into Chatbot Arena, extracted the two model responses, and successfully submitted preference vote. To avoid contaminating the dataset, we only cast tie votes but note that it would be trivial to instead use the vote from the attribution algorithm. Interestingly, post-hoc analysis of this data revealed that yi-lightning family of models, released just 2 days later, were the most common (20% of the responses) in this set.9 We assume that Chatbot Arena had early access to these models and sampled them more frequently than others in order to collect enough votes. However, this knowledge of when particular models will be up-sampled can be easily exploited by adversaries to log large fraction of upvotes for their model. Impact of adversarial voting on leaderboard rankings Similar to Section 3.1, we run experiments on the 55k preference dataset from Chatbot Arena, assumed to reflect \"true\" votes. For 3 target models, we report the change in leaderboard rankings if adversarial voting was conducted on r% of the data samples during data collection. Table 2 summarizes our results. Across all models, we show that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.10. Note that, in this work, we only report results using the most simplistic version of this attack. We can further boost these numbers by not only upvoting the target model but also downvoting open-source competitor models or those ranked higher than the target model in the leaderboard. Discussion: Can we detect and remove adversarial votes? Open platforms can employ two types of mitigation strategies to address this issue: recognizing bot-like behavior to prevent votes from being cast, or detecting abnormal users post-hoc to filter out their votes. Platforms like Chatbot Arena already implement measures from both categories. For example, Chatbot Arena uses Cloudflare and Google reCAPTCHA to detect bots on their platform; however, we were able to bypass both programmatically. We did not find public information indicating that similar measures have been incorporated into the Wildvision Arena platform. There are also opportunities to detect anomalous users post-hoc based on behaviors across multiple 9Evenly distributed between yi-lightning and yi-lightning-lite. 10We assume that adversaries can get 10% votes towards their own model because newly released models will be sampled more frequently. Th Org Re Per WS GPT-3.5 vs GPT-4o 5.51 Llama-3-8b vs Llama-3-70b 10.15 -10.78 Llama-3-8b vs GPT-3.5 Llama-3-70b vs GPT-4o 9.91 17.18 20.06 8.50 5.78 27.16 7.45 3.53 7.19 -12.15 -4.66 2.75 4.56 -1. 11.34 3.15 -0.36 Table 4: Fleiss Kappa between four annotators on different evaluation axis: Th(esis), Org(anization), Re(asoning), Per(spectives), WS (Writing Style). sessions or votes. Chatbot Arena implements version of this strategy by comparing the distribution of ratings from user (uniquely identified by IP address) against historical distributions to identify anomalies. Because committed adversaries may bypass these checks using IP rotation or similar techniques, we encourage further exploration of these approaches to make them more robust. 3.3 Arbitrary Voting We assume an idealized scenario where all users genuinely make their best effort to rank model outputs. However, we argue that holistically rating response to an open-ended and inherently subjective query is ill-defined and liable to always be arbitrary. To demonstrate this, we conduct small-scale annotation study for outputs of subjective Researchy questions prompts (Rosset et al., 2024). Setup We use these prompts and generate generate responses from four language models: Llama3-8B, Llama-3-70B, GPT-4o, and GPT-3.5. We recruit four undergraduate CS students who are passionate about NLP and committed to providing thoughtful annotations. They evaluate responses on four dimensions: thesis, organization, reasoning, perspectives, and writing style. We offer them unlimited time and allow them to seek clarification from the authors when needed. Note that this dimension-wise rating is different from Chatbot Arenas setup of pairwise preferences. However, there already exist multiple prior works that argue that the task is under-defined in this latter setting and report low agreement between annotators (Goyal et al., 2022a,b; Krishna et al., 2023). Therefore, we opt to run this study using more welldefined task description. Results Table 4 shows the inter-annotator agreement between the annotators. Overall, we find very 11Representative question: How can the education system be improved?. low agreement between these well-intentioned annotators with clear guidelines, irrespective of the performance difference between the model pairs. More concerningly, the results highlight that traditional approaches like filtering out low-quality users/annotations using inter-annotator agreement may not be viable strategy for open-ended queries as it is difficult to disentangle between of low interannotator agreement due to bad annotation (apathetic votes) or inherent subjectivity. Adversarial users can also hide their votes from similar scrutiny by using open-ended prompts for which vote choice is expected to be ambiguous. Discussion We argue that arbitrary votes are not noise and provide useful signals about models relative performance. If most frontier models perform similarly well on substantial fraction of real-world queries, this information should not be discarded but inform leaderboard Elo scores. Arbitrary votes become problematic when the majority of the leaderboard is dominated by openended queries that fail to meaningfully distinguish models, despite the existence of legitimate topics or skills along where models exhibit distinct behaviors. Identifying which test examples (or type of test examples) are most informative and up-weighting them when deriving aggregate scores are potential ways of addressing this (Rodriguez et al., 2021).",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ —Ç—Ä–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Å—Ç–∏–º—É–ª–æ–≤, –≤—Ä–∞–∂–¥–µ–±–Ω—ã–µ –≥–æ–ª–æ—Å–∞, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤—ã—à–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –≤ —Å–ª—É—á–∞–µ —Ç—Ä—É–¥–Ω–æ—Ä–∞–∑–ª–∏—á–∏–º—ã—Ö –ø–∞—Ä –æ—Ç–≤–µ—Ç–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Å–º—è–≥—á–µ–Ω–∏–∏ —ç—Ç–æ–≥–æ –≤–ª–∏—è–Ω–∏—è.</p>\n<p><strong>–ê–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –∫–æ–Ω–µ—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–¥–Ω–∞–∫–æ —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —è–≤–Ω–æ –Ω–µ —Å—Ç–∏–º—É–ª–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ r% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –∞–ø–∞—Ç–∏—á–Ω—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∏–ª–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Chatbot Arena, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 55 —Ç—ã—Å—è—á –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ –ª–∏—à—å 10% –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é –¥–≤—É—Ö –∏–∑ —Ç—Ä–µ—Ö –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Llama-2-13b-chat –∏ Mistral-7b-instruct-v0.2) –Ω–∞ –ø—è—Ç—å –º–µ—Å—Ç.</p>\n<p>–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ —Å–ª–æ–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —á–∞—Å—Ç–æ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ –º–æ–¥–µ–ª–µ–π –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –ó–∞–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π –¥–ª—è –≥–æ–ª–æ—Å–æ–≤ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ.</p>\n<p><strong>–í—Ä–∞–∂–¥–µ–±–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫, —Å—Ç—Ä–µ–º—è—â–∏–π—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ —Å–≤–æ–µ–π —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ. –ò–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –º–µ—Ä –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –¥—Ä.) –ª–µ–≥–∫–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≥–æ–ª–æ—Å—É—é—â–∏—Ö –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å.</p>\n<p>–î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –±—ã–ª –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏. –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º:</p>\n<ol>\n<li>–í–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞—Ä–µ–Ω–µ,</li>\n<li>–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–æ–π –∏–∑ –¥–≤—É—Ö –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏,</li>\n<li>–ï—Å–ª–∏ –¥–∞, –ø—Ä–æ–≥–æ–ª–æ—Å–æ–≤–∞—Ç—å –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å,</li>\n<li>–ü–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å.</li>\n</ol>\n<p>–ê–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –ª–æ–≥–∏—Ç–∞–º —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º, –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –º–æ–¥–µ–ª–∏ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–≤—ã—Å–∏—Ç—å –µ–µ —Ä–µ–π—Ç–∏–Ω–≥.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ 10% –≤—Ä–∞–∂–¥–µ–±–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –±–æ–ª–µ–µ —á–µ–º –Ω–∞ —á–µ—Ç—ã—Ä–µ –º–µ—Å—Ç–∞.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–æ –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º.</p>"
            },
            {
                "title": "Conclusion & Future Directions",
                "content": "Our experiments in Section 3 lay convincing case for the need for stronger guardrails in open community-driven platforms. Although these are broadly accepted as the ground truth rankings of LLMs, we are concerned that it is easy to intentionally (adversarial) or unintentionally (apathetic, arbitrary settings) corrupt these leaderboards. The key challenge in mitigating the issue of poor quality annotations is: how can community-driven platforms strike the right balance between implementing necessary quality controls while also providing the right incentives and experience to users to continue to use these platforms. Richer feedback We encourage the community to explore ideas from past research, such as soliciting fine-grained annotations (Krishna et al., 2023; Goyal et al., 2022b) or rationales (McDonnell et al., 2016) in addition to the binary preference feedback. Rationales can be useful in encouraging apathetic users to think more critically about their votes (or abstain) and also for filtering out low-quality annotations from both apathetic and adversarial users. Past work in generation evaluation has discussed how binary preference, or even single Likert rating, for the whole output, cannot meaningfully capture the nuances of human preferences (Gehrmann et al., 2023). Instead, fine-grained preference annotation is recommended, both along multiple dimensions or quality (Gehrmann et al.) or for smaller units within the whole output (Krishna et al., 2023; Goyal et al., 2022b). More recent work proposes providing added context during evaluation to encourage higher agreement between annotators (Malaviya et al., 2024). Future work must explore how these strategies can be incorporated into open platforms without inordinately increasing the annotation burden on users. Stronger Guardrails Other guardrails could include reputation-based systems (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), machine learning based anomaly detection (Kumar et al., 2014; Wu et al., 2016) and techniques that use annotator behavior traces on the platform to estimate quality (Goyal et al., 2018). Open access to collected dataset Public release of the collected data on open platforms will spur research to address the annotation issues we discuss It would provide more detailed in this work. overview into which types of queries are most wellequipped to distinguish between models, and what are the limitations of different families of models.",
                "summary": "<p>–í —Ä–∞–∑–¥–µ–ª–µ 3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏—Ö –º–µ—Ä –∑–∞—â–∏—Ç—ã –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —Å —É—á–∞—Å—Ç–∏–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ —á—Ç–æ —Ç–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω—ã –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–ø–∞—Å–µ–Ω–∏–µ, —á—Ç–æ –∏—Ö –ª–µ–≥–∫–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏–ª–∏ –Ω–µ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, –∫–∞–∫ —Å–æ–æ–±—â–µ—Å—Ç–≤—É –¥–æ—Å—Ç–∏—á—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º –º–æ—Ç–∏–≤–∞—Ü–∏–∏ –∏ —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. </p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–¥–µ–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (Krishna et al., 2023; Goyal et al., 2022b) –∏–ª–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π (McDonnell et al., 2016) –≤–º–µ—Å—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–∏–Ω–∏–º–∞—Ç—å –±–æ–ª–µ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏ –∏ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –ø—Ä–æ—à–ª–æ–º –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–ª–∏ –µ–¥–∏–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –õ–∞–π–∫–µ—Ä—Ç–∞ –¥–ª—è –≤—Å–µ–≥–æ –≤—ã–≤–æ–¥–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç—Ä–∞–∑–∏—Ç—å –Ω—é–∞–Ω—Å—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (Gehrmann et al., 2023). –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞ (Gehrmann et al.) –∏–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π –≤—ã–≤–æ–¥–∞ (Krishna et al., 2023; Goyal et al., 2022b).</p>\n<p>–¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (Kumar et al., 2014; Wu et al., 2016) –∏ —Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (Goyal et al., 2018). –û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –ø–æ–∑–≤–æ–ª–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç –º–æ–¥–µ–ª–∏, –∏ –≤—ã—è–≤–∏—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤ –º–æ–¥–µ–ª–µ–π.</p>"
            }
        ]
    }
]