[
    {
        "id": "2411.10109",
        "title": "Generative Agent Simulations of 1,000 People",
        "url": "https://arxiv.org/abs/2411.10109",
        "abstract": "The promise of human behavioral simulation--general-purpose computational\nagents that replicate human behavior across domains--could enable broad\napplications in policymaking and social science. We present a novel agent\narchitecture that simulates the attitudes and behaviors of 1,052 real\nindividuals--applying large language models to qualitative interviews about\ntheir lives, then measuring how well these agents replicate the attitudes and\nbehaviors of the individuals that they represent. The generative agents\nreplicate participants' responses on the General Social Survey 85% as\naccurately as participants replicate their own answers two weeks later, and\nperform comparably in predicting personality traits and outcomes in\nexperimental replications. Our architecture reduces accuracy biases across\nracial and ideological groups compared to agents given demographic\ndescriptions. This work provides a foundation for new tools that can help\ninvestigate individual and collective behavior.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-15",
        "pub_date_card": {
            "ru": "15 ноября",
            "en": "November 15",
            "zh": "11月15日"
        },
        "hash": "295021638bf121fb",
        "authors": [
            "Joon Sung Park",
            "Carolyn Q. Zou",
            "Aaron Shaw",
            "Benjamin Mako Hill",
            "Carrie Cai",
            "Meredith Ringel Morris",
            "Robb Willer",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "affiliations": [
            "Computer Science Department, Stanford University",
            "Department of Communication Studies, Northwestern University",
            "Department of Communication, University of Washington",
            "Department of Sociology, Stanford University",
            "Google DeepMind"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.10109.jpg",
        "data": {
            "categories": [
                "#agi",
                "#agents",
                "#ethics",
                "#science"
            ],
            "emoji": "🤖",
            "ru": {
                "title": "Виртуальные двойники: моделирование человеческого поведения с помощью ИИ",
                "desc": "Статья представляет новую архитектуру агентов, имитирующих поведение реальных людей на основе анализа их интервью с помощью больших языковых моделей. Агенты способны воспроизводить ответы участников в социологических опросах с точностью, сравнимой с повторными ответами самих людей. Архитектура демонстрирует меньшую предвзятость по расовым и идеологическим признакам по сравнению с агентами, основанными только на демографических данных. Это исследование закладывает основу для новых инструментов изучения индивидуального и коллективного поведения."
            },
            "en": {
                "title": "Simulating Human Behavior with Generative Agents",
                "desc": "This paper introduces a new type of computational agent designed to simulate human behavior by using large language models. The agents are based on qualitative interviews from 1,052 individuals, allowing them to replicate real human attitudes and behaviors. The study shows that these generative agents can accurately mimic responses from the General Social Survey, achieving an 85% accuracy rate compared to individuals' self-reports. Additionally, the architecture minimizes biases related to race and ideology, paving the way for innovative tools in social science and policymaking."
            },
            "zh": {
                "title": "模拟人类行为的新工具",
                "desc": "本文介绍了一种新型的代理架构，能够模拟1052个真实个体的态度和行为。通过对他们生活的定性访谈应用大型语言模型，研究者测量了这些代理在多大程度上能够复制所代表个体的态度和行为。结果显示，这些生成代理在社会调查中的回答准确率达到85%，与参与者在两周后自我回答的准确率相当。该架构在减少不同种族和意识形态群体之间的准确性偏差方面表现良好，为研究个体和集体行为提供了新的工具基础。"
            }
        }
    },
    {
        "id": "2411.15594",
        "title": "A Survey on LLM-as-a-Judge",
        "url": "https://arxiv.org/abs/2411.15594",
        "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-23",
        "pub_date_card": {
            "ru": "23 ноября",
            "en": "November 23",
            "zh": "11月23日"
        },
        "hash": "5b29ec51248581eb",
        "authors": [
            "Jiawei Gu",
            "Xuhui Jiang",
            "Zhichao Shi",
            "Hexiang Tan",
            "Xuehao Zhai",
            "Chengjin Xu",
            "Wei Li",
            "Yinghan Shen",
            "Shengjie Ma",
            "Honghao Liu",
            "Yuanzhuo Wang",
            "Jian Guo"
        ],
        "affiliations": [
            "Department of Civil and Environmental Engineering, Imperial College London",
            "Gaoling School of Artificial Intelligence, Renmin University of China",
            "IDEA Research, International Digital Economy Academy",
            "Institute of Computing Technology, Chinese Academy of Sciences"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.15594.jpg",
        "data": {
            "categories": [
                "#ethics",
                "#benchmark",
                "#data",
                "#survey"
            ],
            "emoji": "⚖️",
            "ru": {
                "title": "LLM как беспристрастный судья: путь к надежной оценке",
                "desc": "Эта статья представляет собой обзор использования больших языковых моделей (LLM) в качестве оценщиков для сложных задач. Авторы исследуют стратегии повышения надежности таких систем, включая улучшение согласованности оценок и снижение предвзятости. В работе предлагаются методологии для оценки надежности LLM-as-a-Judge систем и представлен новый бенчмарк для этой цели. Статья также рассматривает практические применения, проблемы и будущие направления развития этой быстро развивающейся области."
            },
            "en": {
                "title": "Building Reliable LLM-as-a-Judge Systems for Consistent Evaluations",
                "desc": "This paper discusses the use of Large Language Models (LLMs) as evaluators, termed 'LLM-as-a-Judge', which can provide consistent and scalable assessments across various tasks. It highlights the challenges of ensuring the reliability of these systems, including issues of bias and variability in evaluations. The authors propose strategies to enhance the reliability of LLM-as-a-Judge systems and introduce a novel benchmark for evaluating their performance. This comprehensive survey aims to guide researchers and practitioners in developing effective and trustworthy LLM-based evaluation systems."
            },
            "zh": {
                "title": "LLM：评估的未来选择",
                "desc": "本文探讨了大型语言模型（LLM）作为评估工具的应用，称为“LLM作为评判者”。LLM能够处理多种数据类型，提供可扩展、经济且一致的评估，成为传统专家评估的有力替代方案。尽管如此，确保LLM作为评判者系统的可靠性仍然是一个重大挑战，需要精心设计和标准化。本文还提出了提高可靠性的策略，并介绍了一种新颖的基准测试方法，以评估这些系统的可靠性。"
            }
        }
    },
    {
        "id": "2411.16489",
        "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
        "url": "https://arxiv.org/abs/2411.16489",
        "abstract": "This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-25",
        "pub_date_card": {
            "ru": "25 ноября",
            "en": "November 25",
            "zh": "11月25日"
        },
        "hash": "3b22987c1e49b378",
        "authors": [
            "Zhen Huang",
            "Haoyang Zou",
            "Xuefeng Li",
            "Yixiu Liu",
            "Yuxiang Zheng",
            "Ethan Chern",
            "Shijie Xia",
            "Yiwei Qin",
            "Weizhe Yuan",
            "Pengfei Liu"
        ],
        "affiliations": [
            "Generative AI Research Lab (GAIR)",
            "NYU",
            "SII",
            "Shanghai Jiao Tong University"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.16489.jpg",
        "data": {
            "categories": [
                "#interpretability",
                "#data",
                "#benchmark",
                "#open_source",
                "#math",
                "#reasoning",
                "#training",
                "#hallucinations"
            ],
            "emoji": "🧠",
            "ru": {
                "title": "Дистилляция знаний: скрытый ключ к репликации передовых языковых моделей",
                "desc": "Статья представляет критический анализ текущих подходов к репликации возможностей модели OpenAI O1, уделяя особое внимание широко распространенному, но часто нераскрываемому использованию методов дистилляции знаний. Авторы демонстрируют, как простая дистилляция из API O1 в сочетании с обучением с учителем может превзойти производительность O1-preview на сложных математических задачах. Исследование также показывает способность моделей, обученных на дистиллированных данных O1, к обобщению на различные задачи, включая открытые вопросно-ответные системы. Авторы подчеркивают важность прозрачности в исследованиях ИИ и необходимость развития исследователей, мыслящих фундаментальными принципами."
            },
            "en": {
                "title": "Unlocking O1: Transparency and Distillation in AI Replication",
                "desc": "This paper critically analyzes how researchers replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. It demonstrates that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks. The study also shows that models trained on mathematical data can generalize well to other tasks, such as open-domain question answering, while reducing issues like sycophancy. The authors advocate for transparency in AI research and provide a framework for evaluating replication efforts, highlighting the importance of foundational understanding in AI development."
            },
            "zh": {
                "title": "知识蒸馏：提升AI模型性能的关键",
                "desc": "本文对当前复制OpenAI的O1模型能力的方法进行了深入分析，特别关注知识蒸馏技术的广泛使用。研究表明，通过从O1的API进行简单的蒸馏，并结合监督微调，可以在复杂的数学推理任务中实现更优的性能。我们的实验显示，经过微调的基础模型在美国邀请数学考试（AIME）中超越了O1预览，且技术复杂性较低。此外，尽管模型仅在数学问题解决数据上进行训练，但在开放式问答任务中表现出强大的泛化能力，且在微调后显著减少了对谄媚的敏感性。"
            }
        }
    },
    {
        "id": "2411.00640",
        "title": "Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations",
        "url": "https://arxiv.org/abs/2411.00640",
        "abstract": "Evaluations are critical for understanding the capabilities of large language\nmodels (LLMs). Fundamentally, evaluations are experiments; but the literature\non evaluations has largely ignored the literature from other sciences on\nexperiment analysis and planning. This article shows researchers with some\ntraining in statistics how to think about and analyze data from language model\nevaluations. Conceptualizing evaluation questions as having been drawn from an\nunseen super-population, we present formulas for analyzing evaluation data,\nmeasuring differences between two models, and planning an evaluation\nexperiment. We make a number of specific recommendations for running language\nmodel evaluations and reporting experiment results in a way that minimizes\nstatistical noise and maximizes informativeness.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-01",
        "pub_date_card": {
            "ru": "1 ноября",
            "en": "November 1",
            "zh": "11月1日"
        },
        "hash": "0bd5a7e6c3e4a303",
        "authors": [
            "Evan Miller"
        ],
        "affiliations": [
            "Anthropic"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.00640.jpg",
        "data": {
            "categories": [
                "#data",
                "#science",
                "#benchmark"
            ],
            "emoji": "📊",
            "ru": {
                "title": "Статистика на страже оценки языковых моделей",
                "desc": "Статья рассматривает важность корректного проведения экспериментов при оценке больших языковых моделей (LLM). Авторы подчеркивают необходимость применения статистических методов анализа данных в оценках LLM. Предлагаются формулы для анализа результатов оценки, измерения различий между моделями и планирования экспериментов. Даются конкретные рекомендации по проведению оценок языковых моделей и представлению результатов для минимизации статистического шума."
            },
            "en": {
                "title": "Enhancing Language Model Evaluations with Statistical Rigor",
                "desc": "This paper emphasizes the importance of proper evaluation methods for large language models (LLMs) by drawing parallels with experimental analysis in other scientific fields. It introduces statistical concepts to help researchers design and analyze their evaluation experiments effectively. The authors propose formulas for comparing model performance and offer guidelines to enhance the clarity and reliability of evaluation results. By minimizing statistical noise and maximizing the informativeness of reports, the paper aims to improve the overall understanding of LLM capabilities."
            },
            "zh": {
                "title": "优化语言模型评估的实验方法",
                "desc": "这篇论文强调了评估大型语言模型（LLMs）能力的重要性。作者指出，评估实际上是实验，但现有文献往往忽视了其他科学领域的实验分析和规划。文章为具备一定统计学基础的研究人员提供了如何思考和分析语言模型评估数据的方法。最后，作者提出了一些具体建议，以减少统计噪声并提高评估结果的信息量。"
            }
        }
    },
    {
        "id": "2411.11910",
        "title": "AIGS: Generating Science from AI-Powered Automated Falsification",
        "url": "https://arxiv.org/abs/2411.11910v2",
        "abstract": "Rapid development of artificial intelligence has drastically accelerated the\ndevelopment of scientific discovery. Trained with large-scale observation data,\ndeep neural networks extract the underlying patterns in an end-to-end manner\nand assist human researchers with highly-precised predictions in unseen\nscenarios. The recent rise of Large Language Models (LLMs) and the empowered\nautonomous agents enable scientists to gain help through interaction in\ndifferent stages of their research, including but not limited to literature\nreview, research ideation, idea implementation, and academic writing. However,\nAI researchers instantiated by foundation model empowered agents with\nfull-process autonomy are still in their infancy. In this paper, we study\n$\\textbf{AI-Generated Science}$ (AIGS), where agents independently and\nautonomously complete the entire research process and discover scientific laws.\nBy revisiting the definition of scientific research, we argue that\n$\\textit{falsification}$ is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsification, prior systems\nattempting towards AI-Generated Science either lack the part in their design,\nor rely heavily on existing verification engines that narrow the use in\nspecialized domains. In this work, we propose Baby-AIGS as a baby-step\ndemonstration of a full-process AIGS system, which is a multi-agent system with\nagents in roles representing key research process. By introducing\nFalsificationAgent, which identify and then verify possible scientific\ndiscoveries, we empower the system with explicit falsification. Experiments on\nthree tasks preliminarily show that Baby-AIGS could produce meaningful\nscientific discoveries, though not on par with experienced human researchers.\nFinally, we discuss on the limitations of current Baby-AIGS, actionable\ninsights, and related ethical issues in detail.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-17",
        "pub_date_card": {
            "ru": "17 ноября",
            "en": "November 17",
            "zh": "11月17日"
        },
        "hash": "d10fc69f7972862b",
        "authors": [
            "Zijun Liu",
            "Kaiming Liu",
            "Yiqi Zhu",
            "Xuanyu Lei",
            "Zonghan Yang",
            "Zhenhe Zhang",
            "Peng Li",
            "Yang Liu"
        ],
        "affiliations": [
            "Department of Computer Science & Technology, Tsinghua University",
            "Institute for AI Industry Research (AIR), Tsinghua University"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.11910.jpg",
        "data": {
            "categories": [
                "#agents",
                "#healthcare",
                "#science",
                "#multimodal",
                "#ethics"
            ],
            "emoji": "🤖",
            "ru": {
                "title": "Автономное научное открытие с помощью ИИ: первые шаги к полностью автоматизированной науке",
                "desc": "Эта статья представляет Baby-AIGS - систему искусственного интеллекта для автономного научного исследования. Система использует мультиагентный подход, где разные агенты выполняют ключевые этапы исследовательского процесса. Ключевым элементом является FalsificationAgent, который выявляет и проверяет потенциальные научные открытия. Эксперименты показали, что Baby-AIGS способна производить осмысленные научные результаты, хотя и уступает опытным исследователям-людям."
            },
            "en": {
                "title": "Empowering AI to Independently Discover Science",
                "desc": "This paper explores the concept of AI-Generated Science (AIGS), where autonomous agents conduct the entire research process independently. It emphasizes the importance of falsification, a key aspect of scientific inquiry, in the design of AIGS systems. The authors introduce Baby-AIGS, a multi-agent system that includes a FalsificationAgent to identify and verify potential scientific discoveries. Initial experiments indicate that Baby-AIGS can generate meaningful findings, although it still falls short compared to experienced human researchers."
            },
            "zh": {
                "title": "人工智能助力科学发现的未来",
                "desc": "这篇论文探讨了人工智能生成科学（AIGS），即自主完成整个研究过程的智能体。研究者们提出了一个名为Baby-AIGS的系统，旨在通过引入一个专门的验证智能体来实现科学发现的验证。论文强调了在科学研究中，反驳（falsification）是核心要素，并指出现有的AIGS系统在设计上存在不足。通过实验，Baby-AIGS展示了其在科学发现方面的潜力，尽管与经验丰富的人类研究者相比仍有差距。"
            }
        }
    }
]