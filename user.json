[
    {
        "id": "2411.10109",
        "title": "Generative Agent Simulations of 1,000 People",
        "url": "https://arxiv.org/abs/2411.10109",
        "abstract": "The promise of human behavioral simulation--general-purpose computational\nagents that replicate human behavior across domains--could enable broad\napplications in policymaking and social science. We present a novel agent\narchitecture that simulates the attitudes and behaviors of 1,052 real\nindividuals--applying large language models to qualitative interviews about\ntheir lives, then measuring how well these agents replicate the attitudes and\nbehaviors of the individuals that they represent. The generative agents\nreplicate participants' responses on the General Social Survey 85% as\naccurately as participants replicate their own answers two weeks later, and\nperform comparably in predicting personality traits and outcomes in\nexperimental replications. Our architecture reduces accuracy biases across\nracial and ideological groups compared to agents given demographic\ndescriptions. This work provides a foundation for new tools that can help\ninvestigate individual and collective behavior.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-15",
        "pub_date_card": {
            "ru": "15 ноября",
            "en": "November 15",
            "zh": "11月15日"
        },
        "hash": "295021638bf121fb",
        "authors": [
            "Joon Sung Park",
            "Carolyn Q. Zou",
            "Aaron Shaw",
            "Benjamin Mako Hill",
            "Carrie Cai",
            "Meredith Ringel Morris",
            "Robb Willer",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "affiliations": [
            "Computer Science Department, Stanford University",
            "Department of Communication Studies, Northwestern University",
            "Department of Communication, University of Washington",
            "Department of Sociology, Stanford University",
            "Google DeepMind"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.10109.jpg",
        "data": {
            "categories": [
                "#agi",
                "#agents",
                "#ethics",
                "#science"
            ],
            "emoji": "🤖",
            "ru": {
                "title": "Виртуальные двойники: моделирование человеческого поведения с помощью ИИ",
                "desc": "Статья представляет новую архитектуру агентов, имитирующих поведение реальных людей на основе анализа их интервью с помощью больших языковых моделей. Агенты способны воспроизводить ответы участников в социологических опросах с точностью, сравнимой с повторными ответами самих людей. Архитектура демонстрирует меньшую предвзятость по расовым и идеологическим признакам по сравнению с агентами, основанными только на демографических данных. Это исследование закладывает основу для новых инструментов изучения индивидуального и коллективного поведения."
            },
            "en": {
                "title": "Simulating Human Behavior with Generative Agents",
                "desc": "This paper introduces a new type of computational agent designed to simulate human behavior by using large language models. The agents are based on qualitative interviews from 1,052 individuals, allowing them to replicate real human attitudes and behaviors. The study shows that these generative agents can accurately mimic responses from the General Social Survey, achieving an 85% accuracy rate compared to individuals' self-reports. Additionally, the architecture minimizes biases related to race and ideology, paving the way for innovative tools in social science and policymaking."
            },
            "zh": {
                "title": "模拟人类行为的新工具",
                "desc": "本文介绍了一种新型的代理架构，能够模拟1052个真实个体的态度和行为。通过对他们生活的定性访谈应用大型语言模型，研究者测量了这些代理在多大程度上能够复制所代表个体的态度和行为。结果显示，这些生成代理在社会调查中的回答准确率达到85%，与参与者在两周后自我回答的准确率相当。该架构在减少不同种族和意识形态群体之间的准确性偏差方面表现良好，为研究个体和集体行为提供了新的工具基础。"
            }
        }
    },
    {
        "id": "2411.15594",
        "title": "A Survey on LLM-as-a-Judge",
        "url": "https://arxiv.org/abs/2411.15594",
        "abstract": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-23",
        "pub_date_card": {
            "ru": "23 ноября",
            "en": "November 23",
            "zh": "11月23日"
        },
        "hash": "5b29ec51248581eb",
        "authors": [
            "Jiawei Gu",
            "Xuhui Jiang",
            "Zhichao Shi",
            "Hexiang Tan",
            "Xuehao Zhai",
            "Chengjin Xu",
            "Wei Li",
            "Yinghan Shen",
            "Shengjie Ma",
            "Honghao Liu",
            "Yuanzhuo Wang",
            "Jian Guo"
        ],
        "affiliations": [
            "Department of Civil and Environmental Engineering, Imperial College London",
            "Gaoling School of Artificial Intelligence, Renmin University of China",
            "IDEA Research, International Digital Economy Academy",
            "Institute of Computing Technology, Chinese Academy of Sciences"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.15594.jpg",
        "data": {
            "categories": [
                "#ethics",
                "#benchmark",
                "#data",
                "#survey"
            ],
            "emoji": "⚖️",
            "ru": {
                "title": "LLM как беспристрастный судья: путь к надежной оценке",
                "desc": "Эта статья представляет собой обзор использования больших языковых моделей (LLM) в качестве оценщиков для сложных задач. Авторы исследуют стратегии повышения надежности таких систем, включая улучшение согласованности оценок и снижение предвзятости. В работе предлагаются методологии для оценки надежности LLM-as-a-Judge систем и представлен новый бенчмарк для этой цели. Статья также рассматривает практические применения, проблемы и будущие направления развития этой быстро развивающейся области."
            },
            "en": {
                "title": "Building Reliable LLM-as-a-Judge Systems for Consistent Evaluations",
                "desc": "This paper discusses the use of Large Language Models (LLMs) as evaluators, termed 'LLM-as-a-Judge', which can provide consistent and scalable assessments across various tasks. It highlights the challenges of ensuring the reliability of these systems, including issues of bias and variability in evaluations. The authors propose strategies to enhance the reliability of LLM-as-a-Judge systems and introduce a novel benchmark for evaluating their performance. This comprehensive survey aims to guide researchers and practitioners in developing effective and trustworthy LLM-based evaluation systems."
            },
            "zh": {
                "title": "LLM：评估的未来选择",
                "desc": "本文探讨了大型语言模型（LLM）作为评估工具的应用，称为“LLM作为评判者”。LLM能够处理多种数据类型，提供可扩展、经济且一致的评估，成为传统专家评估的有力替代方案。尽管如此，确保LLM作为评判者系统的可靠性仍然是一个重大挑战，需要精心设计和标准化。本文还提出了提高可靠性的策略，并介绍了一种新颖的基准测试方法，以评估这些系统的可靠性。"
            }
        }
    },
    {
        "id": "2411.16489",
        "title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
        "url": "https://arxiv.org/abs/2411.16489",
        "abstract": "This paper presents a critical examination of current approaches to\nreplicating OpenAI's O1 model capabilities, with particular focus on the\nwidespread but often undisclosed use of knowledge distillation techniques.\nWhile our previous work explored the fundamental technical path to O1\nreplication, this study reveals how simple distillation from O1's API, combined\nwith supervised fine-tuning, can achieve superior performance on complex\nmathematical reasoning tasks. Through extensive experiments, we show that a\nbase model fine-tuned on simply tens of thousands of samples O1-distilled\nlong-thought chains outperforms O1-preview on the American Invitational\nMathematics Examination (AIME) with minimal technical complexity. Moreover, our\ninvestigation extends beyond mathematical reasoning to explore the\ngeneralization capabilities of O1-distilled models across diverse tasks:\nhallucination, safety and open-domain QA. Notably, despite training only on\nmathematical problem-solving data, our models demonstrated strong\ngeneralization to open-ended QA tasks and became significantly less susceptible\nto sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of\nobscured technical claims in the field. Our work includes: (1) A detailed\ntechnical exposition of the distillation process and its effectiveness, (2) A\ncomprehensive benchmark framework for evaluating and categorizing O1\nreplication attempts based on their technical transparency and reproducibility,\n(3) A critical discussion of the limitations and potential risks of\nover-relying on distillation approaches, our analysis culminates in a crucial\nbitter lesson: while the pursuit of more capable AI systems is important, the\ndevelopment of researchers grounded in first-principles thinking is paramount.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-11-25",
        "pub_date_card": {
            "ru": "25 ноября",
            "en": "November 25",
            "zh": "11月25日"
        },
        "hash": "3b22987c1e49b378",
        "authors": [
            "Zhen Huang",
            "Haoyang Zou",
            "Xuefeng Li",
            "Yixiu Liu",
            "Yuxiang Zheng",
            "Ethan Chern",
            "Shijie Xia",
            "Yiwei Qin",
            "Weizhe Yuan",
            "Pengfei Liu"
        ],
        "affiliations": [
            "Generative AI Research Lab (GAIR)",
            "NYU",
            "SII",
            "Shanghai Jiao Tong University"
        ],
        "pdf_title_img": "assets/pdf/title_img/2411.16489.jpg",
        "data": {
            "categories": [
                "#interpretability",
                "#data",
                "#benchmark",
                "#open_source",
                "#math",
                "#reasoning",
                "#training",
                "#hallucinations"
            ],
            "emoji": "🧠",
            "ru": {
                "title": "Дистилляция знаний: скрытый ключ к репликации передовых языковых моделей",
                "desc": "Статья представляет критический анализ текущих подходов к репликации возможностей модели OpenAI O1, уделяя особое внимание широко распространенному, но часто нераскрываемому использованию методов дистилляции знаний. Авторы демонстрируют, как простая дистилляция из API O1 в сочетании с обучением с учителем может превзойти производительность O1-preview на сложных математических задачах. Исследование также показывает способность моделей, обученных на дистиллированных данных O1, к обобщению на различные задачи, включая открытые вопросно-ответные системы. Авторы подчеркивают важность прозрачности в исследованиях ИИ и необходимость развития исследователей, мыслящих фундаментальными принципами."
            },
            "en": {
                "title": "Unlocking O1: Transparency and Distillation in AI Replication",
                "desc": "This paper critically analyzes how researchers replicate the capabilities of OpenAI's O1 model, emphasizing the often hidden use of knowledge distillation techniques. It demonstrates that fine-tuning a base model with data distilled from O1's API can lead to better performance on complex mathematical reasoning tasks. The study also shows that models trained on mathematical data can generalize well to other tasks, such as open-domain question answering, while reducing issues like sycophancy. The authors advocate for transparency in AI research and provide a framework for evaluating replication efforts, highlighting the importance of foundational understanding in AI development."
            },
            "zh": {
                "title": "知识蒸馏：提升AI模型性能的关键",
                "desc": "本文对当前复制OpenAI的O1模型能力的方法进行了深入分析，特别关注知识蒸馏技术的广泛使用。研究表明，通过从O1的API进行简单的蒸馏，并结合监督微调，可以在复杂的数学推理任务中实现更优的性能。我们的实验显示，经过微调的基础模型在美国邀请数学考试（AIME）中超越了O1预览，且技术复杂性较低。此外，尽管模型仅在数学问题解决数据上进行训练，但在开放式问答任务中表现出强大的泛化能力，且在微调后显著减少了对谄媚的敏感性。"
            }
        }
    }
]