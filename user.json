[
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–æ —Å–∏—Ö –ø–æ—Ä –≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–∏–≤–Ω–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –≠—Ç–∏ —É—Å–∏–ª–∏—è –ø—Ä–∏–≤–µ–ª–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –í —Ö–æ–¥–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ª—É—á—à–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –∑–∞–¥–∞—á–∞—Ö –ø–µ—Ä–µ–≤–æ–¥–∞ \"–ª—é–±–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–¥–∞—é—â–µ–π—Å—è —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –≤—ã—Å–æ–∫–æ- –∏ –Ω–∏–∑–∫–æ-—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏. –°–æ–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∏ —É—Å–∏–ª–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Large Language Models, LLMs), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3, GPT-4, PaLM –∏ LLaMA, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (Natural Language Processing, NLP), –ø–æ–∫–∞–∑—ã–≤–∞—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∑–∞–¥–∞—á–∞—Ö –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –±–ª–∞–≥–æ–¥–∞—Ä—è —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–æ–≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤, –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∞–Ω–≥–ª–∏–π—Å–∫–æ–º. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. </p>\n<p>–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ—Ö–≤–∞—Ç–∫–∏ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫—Ä—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –ø—Ä–µ–ø—è—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–æ–≤ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –∏—Ö –Ω–æ—Å–∏—Ç–µ–ª–µ–π –≤–Ω–µ –≤—ã–≥–æ–¥ –æ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –ß—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —ç—Ç–æ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å, –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>Marco-LLM —Å—Ç—Ä–µ–º–∏—Ç—Å—è –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å —Ä–∞–∑—Ä—ã–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö NLP. –î–ª—è —ç—Ç–æ–≥–æ –æ–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2. –≠—Ç–∞ –º–æ–¥–µ–ª—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, —á—Ç–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–∞–∑–µ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Marco-LLM, –∫–æ—Ç–æ—Ä–∞—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∑–∞–¥–∞—á–∞–º–∏ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ MMMLU, Flores, Belebele, AGIEval, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∞—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–î–∞–ª—å–Ω–µ–π—à–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã –≤–∫–ª—é—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Å–±–æ—Ä–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–Ω–æ–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —ç—Ç–∞–ø—ã –ø–æ—Å—Ç–æ–±—É—á–µ–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Cross-Lingual Enhancement",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ —Å –æ–±–∑–æ—Ä–æ–º –æ—Ü–µ–Ω–æ—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –∑–∞–¥–∞—á–∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —É–∫–∞–∑–∞–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ (—Ç–æ—á–Ω–æ—Å—Ç—å, F1-–æ—Ü–µ–Ω–∫–∞ –∏–ª–∏ BLEU). –í —Ç–∞–±–ª–∏—Ü–µ —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —è–∑—ã–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è —Ç–µ—Å—Ç—ã, –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —è–∑—ã–∫–∞.</p>\n<p>–î–∞–ª–µ–µ –≤ —Ç–µ–∫—Å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ —ç—Ç–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö:</p>\n<ol>\n<li>\n<p><strong>ARC (AI2 Reasoning Challenge)</strong> ‚Äì –≤–∫–ª—é—á–∞–µ—Ç –æ–∫–æ–ª–æ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∞—É–∫–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –æ—Ç –º–æ–¥–µ–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π. –í–æ–ø—Ä–æ—Å—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ –¥–≤–∞ —Ç–∏–ø–∞: –ø—Ä–æ—Å—Ç—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∏ —Å–ª–æ–∂–Ω—ã–µ, —Ç—Ä–µ–±—É—é—â–∏–µ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</p>\n</li>\n<li>\n<p><strong>Belebele</strong> ‚Äì —ç—Ç–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —á—Ç–µ–Ω–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—é—â–∏–π –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞ 122 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ —Ç–∏–ø—ã. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ—Ç—Ä—ã–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞, –∑–∞–ø—Ä–æ—Å–∞ –∏ —á–µ—Ç—ã—Ä—ë—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞.</p>\n</li>\n<li>\n<p><strong>CEVAL</strong> ‚Äì —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–æ. –ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏–º–µ–µ—Ç —á–µ—Ç—ã—Ä–µ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>Flores</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –í–∫–ª—é—á–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö.</p>\n</li>\n<li>\n<p><strong>HellaSwag</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫ –∑–¥—Ä–∞–≤–æ–º—É —Å–º—ã—Å–ª—É –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –≤—ã–≤–æ–¥—É. –°–æ–¥–µ—Ä–∂–∏—Ç 70000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –≥–¥–µ –æ–¥–∏–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏ —Ç—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö.</p>\n</li>\n<li>\n<p><strong>MMLU (Massive Multitask Language Understanding)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∑–Ω–∞–Ω–∏–π –∏ –Ω–∞–≤—ã–∫–æ–≤ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —É –ò–ò-–º–æ–¥–µ–ª–µ–π. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç 57 –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 57000 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —á–µ—Ç—ã—Ä—å–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤.</p>\n</li>\n<li>\n<p><strong>TyDiQA (Typologically Diverse Question Answering)</strong> ‚Äì –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –°–æ–¥–µ—Ä–∂–∏—Ç –±–æ–ª–µ–µ 200000 –ø–∞—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –≤–∑—è—Ç—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏.</p>\n</li>\n</ol>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ç–µ—Å—Ç–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ —Ç–µ—Å—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º –≤ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö –∏ –∑–∞–¥–∞—á–∞—Ö. –ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 7B –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–æ–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ª—É—á—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ–≥–æ –º–æ–¥–µ–ª—å—é Qwen2-7B (81.8). –ù–∞ Belebele, –æ—Ü–µ–Ω–∏–≤–∞—é—â–µ–º –∑–Ω–∞–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤, Marco-Chat-7B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, —Ç–∞–∫–∂–µ –æ–ø–µ—Ä–µ–∂–∞—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ 70B –ª–∏–¥–µ—Ä–æ–º —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥–µ–ª—å Marco-72B, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 76.1 –Ω–∞ MMMLU, —á—Ç–æ –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –≤—ã—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ Llama3.1-70B. –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—É—é –æ–±—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é —è–∑—ã–∫–∞. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å–∫—Ä–∏–ø—Ç–æ–≤, Marco-72B –ø–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –ª—É—á—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–≥–æ –º–µ—Å—Ç–∞. –¢–∞–∫–∂–µ –≤–ø–µ—á–∞—Ç–ª—è—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã –Ω–∞ CEval (94.5) –∏ Belebele (89.6), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</p>\n<p>–¢–∞–±–ª–∏—Ü–∞ 9 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö MMMLU –¥–ª—è 7B –∏ 70B –º–æ–¥–µ–ª–µ–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å Marco-Chat-7B –ª–∏–¥–∏—Ä—É–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º –ø–æ –≤—Å–µ–º —è–∑—ã–∫–∞–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 7B, –∞ –º–æ–¥–µ–ª—å Marco-72B ‚Äî –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 70B.</p>"
            }
        ]
    },
    {
        "id": "2412.04003",
        "title": "Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement",
        "url": "https://huggingface.co/papers/2412.04003",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress in recent\nyears; however, their excellent performance is still largely limited to major\nworld languages, primarily English. Many LLMs continue to face challenges with\nmultilingual tasks, especially when it comes to low-resource languages. To\naddress this issue, we introduced Marco-LLM: Massive multilingual training for\ncross-lingual enhancement LLM. We have collected a substantial amount of\nmultilingual data for several low-resource languages and conducted extensive\ncontinual pre-training using the Qwen2 models. This effort has resulted in a\nmultilingual LLM named Marco-LLM. Through comprehensive evaluations on various\nmultilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA\nand many others, Marco-LLM has demonstrated substantial improvements over\nstate-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements\nin any-to-any machine translation tasks, showing the effectiveness of our\nmultilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not\nonly perform exceptionally well in multilingual tasks, including low-resource\nlanguages, but also maintain strong performance in English and other major\nlanguages, closing the performance gap between high- and low-resource language\ncapabilities. By bridging languages, this effort demonstrates our dedication to\nensuring LLMs work accurately across various languages.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "435ec5749dcb2e12",
        "authors": [
            "Lingfeng Ming",
            "Bo Zeng",
            "Chenyang Lyu",
            "Tianqi Shi",
            "Yu Zhao",
            "Xue Yang",
            "Yefeng Liu",
            "Yiyu Wang",
            "Linlong Xu",
            "Yangyang Liu",
            "Xiaohu Zhao",
            "Hao Wang",
            "Heng Liu",
            "Hao Zhou",
            "Huifeng Yin",
            "Zifu Shang",
            "Haijun Li",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "affiliations": [
            "MarcoPolo Team, Alibaba International Digital Commerce"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04003.jpg",
        "data": {
            "categories": [
                "#training",
                "#machine_translation",
                "#dataset",
                "#low_resource",
                "#multilingual"
            ],
            "emoji": "üåç",
            "ru": {
                "title": "Marco-LLM: –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤–æ–≥–æ –±–∞—Ä—å–µ—Ä–∞ –≤ –º–∏—Ä–µ –ò–ò",
                "desc": "Marco-LLM - —ç—Ç–æ –Ω–æ–≤–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–∏–∑–∫–æ—Ä–µ—Å—É—Ä—Å–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–æ–º –æ–±—ä–µ–º–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Qwen2. Marco-LLM –ø–æ–∫–∞–∑–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele –∏ –¥—Ä—É–≥–∏–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –∑–∞–¥–∞—á–∞—Ö –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –º–µ–∂–¥—É –ª—é–±—ã–º–∏ —è–∑—ã–∫–∞–º–∏."
            },
            "en": {
                "title": "Bridging Language Gaps with Marco-LLM",
                "desc": "This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds."
            },
            "zh": {
                "title": "Marco-LLMÔºöÊâìÁ†¥ËØ≠Ë®ÄÂ£ÅÂûíÁöÑÂ§öËØ≠Ë®ÄÊ®°Âûã",
                "desc": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ËøëÂπ¥Êù•ÂèñÂæó‰∫ÜÊòæËëóËøõÂ±ïÔºå‰ΩÜÂÖ∂‰ºòÁßÄË°®Áé∞‰∏ªË¶ÅÈõÜ‰∏≠Âú®‰∏ªË¶Å‰∏ñÁïåËØ≠Ë®Ä‰∏äÔºåÂ∞§ÂÖ∂ÊòØËã±ËØ≠„ÄÇ‰∏∫‰∫ÜÊîπÂñÑ‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®Ä‰ªªÂä°Ë°®Áé∞ÔºåÊàë‰ª¨ÊèêÂá∫‰∫ÜMarco-LLMÔºåËøôÊòØ‰∏Ä‰∏™ÈíàÂØπË∑®ËØ≠Ë®ÄÂ¢ûÂº∫ÁöÑÂ§ßËßÑÊ®°Â§öËØ≠Ë®ÄËÆ≠ÁªÉÊ®°Âûã„ÄÇÊàë‰ª¨Êî∂ÈõÜ‰∫ÜÂ§ßÈáè‰ΩéËµÑÊ∫êËØ≠Ë®ÄÁöÑÂ§öËØ≠Ë®ÄÊï∞ÊçÆÔºåÂπ∂‰ΩøÁî®Qwen2Ê®°ÂûãËøõË°å‰∫ÜÂπøÊ≥õÁöÑÊåÅÁª≠È¢ÑËÆ≠ÁªÉ„ÄÇMarco-LLMÂú®Â§öÈ°πÂ§öËØ≠Ë®ÄÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊîπËøõÔºåÂ∞§ÂÖ∂Âú®‰ªªÊÑèÂØπ‰ªªÊÑèÁöÑÊú∫Âô®ÁøªËØë‰ªªÂä°‰∏≠ÔºåÂ±ïÁ§∫‰∫ÜÂÖ∂Â§öËØ≠Ë®ÄÊ®°ÂûãÁöÑÊúâÊïàÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.",
                "summary": "<p>–í –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≥–æ–¥—ã –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –¥–æ—Å—Ç–∏–≥–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —É—Å–ø–µ—Ö–æ–≤, –æ–¥–Ω–∞–∫–æ –∏—Ö –≤—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–∏—Ä–æ–≤—ã–º–∏ —è–∑—ã–∫–∞–º–∏, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º. –ú–Ω–æ–≥–∏–µ LLM –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è —Å —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ –∫–æ–≥–¥–∞ —Ä–µ—á—å –∏–¥–µ—Ç –æ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Marco-LLM: –ú–∞—Å—Å–æ–≤–æ–µ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –∫—Ä–æ—Å—Å-–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è LLM. –ú—ã —Å–æ–±—Ä–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –∏ –ø—Ä–æ–≤–µ–ª–∏ –æ–±—à–∏—Ä–Ω—É—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π Qwen2. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç—Ç–∏—Ö —É—Å–∏–ª–∏–π –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Marco-LLM. –ë–ª–∞–≥–æ–¥–∞—Ä—è –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏–º –æ—Ü–µ–Ω–∫–∞–º –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–∞—Ö, –≤–∫–ª—é—á–∞—è MMMLU, AGIEval, Belebele, Flores-200, XCOPA –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ, Marco-LLM –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ LLM. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-LLM –¥–æ—Å—Ç–∏–≥–ª–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π –≤ –ª—é–±—ã—Ö –ø–µ—Ä–µ–≤–æ–¥—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö \"–ª—é–±–æ–π-–∫-–ª—é–±–æ–º—É\", —á—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π LLM. Marco-LLM ‚Äî —ç—Ç–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–∞—è LLM, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–∞—è –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –≤–∫–ª—é—á–∞—è —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –Ω–æ —Ç–∞–∫–∂–µ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –¥—Ä—É–≥–∏—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, —Ç–µ–º —Å–∞–º—ã–º —Å–æ–∫—Ä–∞—â–∞—è —Ä–∞–∑—Ä—ã–≤ –≤ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ —Å –≤—ã—Å–æ–∫–∏–º–∏ –∏ –Ω–∏–∑–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. –û–±—ä–µ–¥–∏–Ω—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏, —ç—Ç–∞ —Ä–∞–±–æ—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞—à—É –ø—Ä–∏–≤–µ—Ä–∂–µ–Ω–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã LLM –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</p>"
            },
            {
                "title": "Marco-LLM: Enhancing Cross-Lingual Capabilities Through Multilingual Training",
                "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 5 Conclusion and Future Work Appendix 38 A.1 Dataset Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.1 Translation Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 A.1.2 Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 A.2 More Evaluation Results for Instruct LLMs . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1. Introduction Large Language Models (LLMs) [Brown et al., 2020, OpenAI, 2023, Touvron et al., 2023b,c, Dubey and et al, 2024, Guo et al., 2024] have transformed the field of Natural Language Processing (NLP) by achieving impressive results across variety of tasks such as language understanding, generation, and translation [Bang et al., 2023, Wang et al., 2023, Jiao et al., 2023, Bai et al., 2023, Hui et al., 2024, Yao et al., 2024]. Models like GPT-4 [OpenAI, 2023], GPT-4o [Hurst et al., 2024], PaLM[Chowdhery et al., 2024], and LLaMA [Dubey and et al, 2024] have demonstrated that scaling up model size and training data leads to significant performance gains. However, the majority of these advances have been centered around high-resource languages, predominantly English [Bang et al., 2023, Jiao et al., 2023, Lai et al., 2023]. This focus has resulted in performance gap when these models are applied to multilingual tasks, especially involving low-resource languages. Multilingual NLP faces unique challenges due to the diversity and imbalance of linguistic resources [Pires et al., 2019, Conneau and Lample, 2019]. Low-resource languages often lack the extensive textual data required for training large models, which hinders the development of effective language technologies for these languages. As result, speakers of low-resource languages are underrepresented in the benefits brought by recent advancements in NLP. To address this disparity, we have developed Marco-LLM, multilingual language model trained with focus on low-resource languages. An overview of the framework of our Marco-LLM is shown in Figure 2. By collecting substantial amount of multilingual data covering series of underrepresented languages, and through massive multilingual continual pre-training and extensive multilingual posttraining (including multilingual supervised finetuning and multilingual preference alignment) using the Qwen2 model [Bai et al., 2023] as foundation, Marco-LLM aims to bridge the performance gap in multilingual NLP tasks. Our main contributions can be summarized as follows: We compile and curate large-scale multilingual dataset tailored for low-resource languages, enhancing the diversity and richness of training data. We perform massive multilingual continual pre-training and post-training on the Qwen2 model to develop Marco-LLM, multilingual LLM that substantially improves performance on low-resource language tasks. We conduct comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, AGIEval, multilingual MT-bench, etc, demonstrating that Marco-LLM outperforms state-of-the-art models in multilingual settings. The rest of this paper is structured as: In Section 2, we give an overview of relevant literature regarding the development trajectory of LLMs especially multilingual LLMs and continual pre-training for LLMs. We present the details of our continual pre-training experiments in Section 3 including the monolingual and multilingual data collection and curation, training setup and evaluation results. Furthermore, in Section 4 we demonstrate how we conduct the post-training (including supervised finetuning and preference alignment)for the Marco-LLM that has been continual pre-trained shown in Section 3, including how we construct our multilingual supervised finetuning data, how to formulate the task format, supervised training details as well as corresponding evaluation results on multilingual benchmark datasets. 4 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 2 An overview of the training and evaluation paradigm of our Marco-LLM, we conducted massive multilingual continual pre-training, multilingual supervised finetuning and preference alignment. We further perform extensive evaluation on multilingual benchmarks to validate the efficacy of our Marco-LLM. 2. Related Work In recent years, Large Language Models (LLMs) such as GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024], and LLaMA [Touvron et al., 2023b] have revolutionized the research paradigm of Natural Language Processing (NLP). These transformer-based [Vaswani et al., 2017] models have demonstrated exceptional capabilities in generating and understanding text, achieving state-of-the-art results in tasks like language translation, summarization, and questionanswering [Bang et al., 2023, Wang et al., 2023]. However, their primary focus has been on highresource languages, leaving gap in multilingual performance [Jiao et al., 2023, Bang et al., 2023, Lai et al., 2023]. To bridge the gap in multilingual applications, multilingual models such as mBERT [Pires et al., 2019], XLM-R [Conneau and Lample, 2019], mT5 [Xue et al., 2021], and PolyLM[Wei et al., 2023b] have been developed. These models aim to provide cross-lingual capabilities by being trained on diverse language datasets. Despite their promise, they often struggle with low-resource languages due to insufficient training data and the inherent difficulty of balancing multiple languages within one model [Shi et al., 2023, Dac Lai et al., 2023, Singh et al., 2024a, Lovenia et al., 2024]. Efforts in this area have included data augmentation, transfer learning, and specialized models for specific languages or tasks [Conneau et al., 2018, Artetxe et al., 2018, Conneau and Lample, 2019, Goyal et al., 2021, Team et al., 2022]. These approaches, while beneficial, have not fully harnessed the potential of large-scale language models [√úst√ºn et al., 2024]. Continual pre-training offers viable solution to enhance the adaptability of LLMs by allowing models to incorporate new information without starting from scratch [Ke et al., 2023, √áaƒüatay Yƒ±ldƒ±z et al., 2024]. This method enables models to improve performance in underrepresented areas, particularly for low-resource languages. By leveraging existing strengths and optimizing computational resources, continual pre-training presents scalable approach to overcoming current limitations in multilingual and low-resource Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement language processing. Building on these insights, we introduce Marco-LLM, which leverages continual pre-training of the Qwen2 model with focus on low-resource languages. Our approach integrates large-scale multilingual data collection and advanced training techniques to enhance the models capabilities in multilingual NLP tasks. 3. Massive Multilingual Continual Pretraining for Large Language Models In this section, we present the technical details of how we conduct continual pretraining on LLMs. Specifically, the process of continual pretraining for multilingual LLMs encompasses the following: (1) the curation and filtering of large-scale training corpus, which will be introduced in Section 3.1; (2) simple and scalable strategies to efficiently conduct continual pretraining at large scale for LLMs, detailed in Section 3.2; (3) comprehensive presentation of evaluation results across multiple benchmarks, along with an analysis, in Section 3.3. 3.1. Data Collection and Filtering We create our dataset for Marco-LLM training from variety of data sources containing knowledge until the end of 2024.4. We apply several data cleaning mechanisms and de-duplication methods on each data source to obtain high-quality tokens. 3.1.1. Multilingual Web Data Curation To produce high-quality multilingual training data, we meticulously designed cascaded dataprocessing pipeline. Similar to prior processing pipelines (e.g., CCNet[Wenzek et al., 2020], RefineWeb[Penedo et al., 2023], Llama[Touvron et al., 2023a], etc.) focusing on English, our multilingual pipeline features series of data-cleaning strategies targeting text quality and information distribution. Document Preparation. Our pipeline starts from the target document preparation to keep information distribution. First of all, we extract the main contents from the raw Common Crawl (WARC) files. Meanwhile, we perform URL filter and language identification to avoid subsequent computationally expensive processing. The former targets fraudulent and/or adult websites (e.g., predominantly pornographic, violent, related to gambling, etc.), while the latter focus on the target languages. Specifically, we classify documents according to their primary languages and remove those with low confidence in classification, leveraging inexpensive n-gram models (e.g, fast-Text [Joulin et al., 2016]). Quality Filtering. The filters in this part aim for removing text with low quality. We filter out document based on some heuristic rule filters: (1). word blocklists and garbled text filters; (2). document length, the ratio of special symbols, the ratio of stop words, and the ratio of short, consecutive, or incomplete lines; (3). repeated words, n-grams. Inspired by CulturaX [Nguyen et al., 2024], the filtering thresholds are based on statistical analysis of large document samples. To enhance our filtering process, we utilize the KenLM library [Wenzek et al., 2020] to evaluate vast array of web documents. Documents with perplexity scores largely above average are subsequently removed. Deduplication. After filtering, we implement comprehensive deduplication pipeline following the procedure in RefineWeb [Penedo et al., 2023]. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 1 Overview of corpus utilization rates across various languages and categories of our corpus, we show the total number of tokens (in Billion Tokens) available and used for high-resource and low-resource languages, as well as other data sources such as synthetic data. Category Language Total Tokens (B) Used Tokens (B) Utilization Rate (%) High-Resource Languages English (en) Chinese (zh) Arabic (ar) German (de) Spanish (es) French (fr) Korean (ko) Japanese (ja) Portuguese (pt) Turkish (tr) Low-Resource Languages Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Malay (ms) Dutch (nl) Polish (pl) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Romanian (ro) Azerbaijani (az) Nepali (ne) Other Data Sources Parallel Data High-quality Knowledge Data Synthetic Data 1,459.7 214.7 45.8 442.8 397.8 320.8 41.8 224.2 145.3 80. 6.5 11.3 23.8 56.3 2.9 31.3 45.3 251.0 8.3 18.4 8.7 24.4 270.2 376.8 214.4 16.8 160.0 19.4 22.6 103.0 65.0 6.0 Total 5,115.9 90.4 48.2 10.6 10.6 10.6 10.6 10.6 10.6 10.6 10.6 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1.9 1. 20.8 16.4 4.4 300.0 6.2 22.4 23.0 2.4 2.7 3.3 25.2 4.7 7.3 13.1 28.7 16.4 7.8 3.3 64.4 6.0 4.1 0.7 22.4 10.1 21.4 7.6 0.7 0.5 0.9 11.1 1.2 9.6 8.2 20.2 25.2 73.3 5. Its worth noting that we perform quality filtering and deduplications within data for each language. At this stage, we obtain many high-quality monolingual data in low-resource languages, generally called multilingual data, the statistics is detailed in Table 1. We determine the amount of multilingual tokens used in continual pretraining experimentally (shown in Section 3.2), balancing model performance on Chinese, English, and multilingual benchmarks. 3.1.2. Parallel Data Follow prior works such as PaLM2 [Anil et al., 2023], Skywork [Wei et al., 2023a], and PolyLM [Wei et al., 2023b], we employ parallel data into our continual pretraining dataset to further improve the cross-lingual and multilingual ability of Marco. This data is meticulously structured to pair complete source-language paragraph with its corresponding target-language counterpart, ensuring seamless alignment of linguistic capabilities between the two languages. We mainly focus on open-source parallel data, OPUS [Tiedemann, 2012, Zhang et al., 2020] and 7 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 3 The amount of tokens per category in our multilingual continual pretraining corpus for Marco-LLM. CCAligned [Chaudhary et al., 2019, El-Kishky et al., 2020]. The former covers 100 languages, is English-centric, meaning that all training pairs include English on either the source or target side, while the latter consists of parallel or comparable web-document pairs in 137 languages aligned with English. There are many bad cases, such as translation errors, in these open-source data. We utilize the following pipeline to process them. Heuristic Filtering. To obtain high-quality parallel corpora, we develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include: We filter the sentence pairs using the ratio of special symbols, the ratio of stop words, the ratio of digits and the ratio of repeated words in source sentence; We use similarity scores of LASER embeddings for sentence pairs to filter out sentence pairs with low scores. Diverse Translation Templates. After filtering, the translation templates are employed to concatenate the parallel corpora. Prior work [√úst√ºn et al., 2024] has shown the importance of diverse wording, templates, and task types to aid generalization to different natural inputs. Therefore, we utilize diverse translation templates to make the input diversity to enhance semantic alignment between multilingual parallel sentences, the details can be found in Appendix A.1.1. Empirically, the parallel data has shown the importance of enhancing cross-lingual and multilingual ability of Marco, specific NLP downstream tasks such as machine translation. More experimental details are presented in Section 3.5. https://github.com/facebookresearch/LASER 8 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 3.1.3. High-quality Knowledge Data Prior works, such as MiniCPM [Hu et al., 2024], Skywork [Wei et al., 2023a], and Llama3 [Dubey and et al, 2024], have shown that introducing the high-quality knowledge data into the pretraining stage enhances model capabilities. Similarly, we mix some high-quality knowledge data into continual pretraining. Our high-quality knowledge data consists of mQA (multilingual Question Answer), STEM, and some ability-oriented SFT data, like math and coding. All of them are collected from the open-source website, Huggingface. Similar to our pipelines for parallel multilingual data described above, we implement filters to remove data with low quality and the data from common benchmarks. In addition to the n-grams deduplications, we employ semantic deduplications to process them. Note that, we do not include any training sets from commonly used benchmarks in our high-quality knowledge data. 3.1.4. Synthetic Data Books and papers are high-quality knowledge data, but they are scarce. phi models [Gunasekar et al., 2023, Li et al., 2023] are trained by synthetic textbooks to enhance performance. Recent work [Whitehouse et al., 2023] suggests that multilingual synthetic data can also enhance cross-lingual transfer. Also, we mix some synthetic data into our continual pretraining. Our synthetic data mainly consists of two parts: keywords-based explanation and story data, and ability-oriented SFT data. We describe them below. Keywords-based Explanation/Story Data. To synthetic Wikipedia and book data, we first use GPT-4 to generate large number of keywords covering diverse topics such as mathematics, history, physics, etc. That results in 100k keyword pools. Then, we sample several target keywords from the keyword pool and employ other LLMs with the designed prompts to generate explanations of the target words and generate textbook stories, respectively. Similar to AttrPrompt [Yu et al., 2023], the prompts contains several attributes, we vary the value of attribute to generate diverse samples. An example of such prompts can be found in Appendix A.1.2, where the LLMs are instructed to generate training data based on attributes such as key words and subject. Ability-Oriented SFT Data. WizardLM [Xu et al., 2024], WizardMath [Luo et al., 2023], and WizardCoder [Luo et al., 2024] are designed to synthetic diverse instructions on target ability. We follow them to enhance Marco-LLM capacity of math and coding. To further improve the cross-lingual and multilingual ability, we use in-context learning method and translation technology to generate multilingual data. The former is that we employ the few-shot prompt to generate new sample in the similar domain, the prompts are listed in Appendix A.1.2. The latter is that we randomly translate the original instruction, question, or answer to other language, resulting in cross-lingual QA. To enhance the diversity of synthetic data, we employ several superb models (like GPT-4, Deepseekv2 [DeepSeek-AI et al., 2024], DBRX, Command-R Plus [Cohere For AI, 2024], etc.) to act as the generator, as its generation is of higher quality. 3.1.5. Data Statistics The composition of the continual pretraining dataset used for Marco-LLM is detailed in Table 1. The multilingual data mainly covers the following languages: English (en), Chinese (zh), Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), Turkish (tr), Azerbaijani (az), Bengali (bn), Czech (cs), Greek (el), Hebrew (he), Hungarian (hu), Indonesian (id), https://huggingface.co/ ttps://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm 9 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Italian (it), Kazakh (kk), Malay (ms), Nepali (ne), Dutch (nl), Polish (pl), Romanian (ro), Russian (ru), Thai (th), Ukrainian (uk), Urdu (ur), Vietnamese (vi). The data distribution across different languages is extremely uneven, shown in Figure 3. We group them into rough taxonomy of lower-resourced (LR) and higher-resourced (HR). This yield split of the 29 languages in our training mixture into 10 HR and 19 LR languages. Its worth noting that our dataset contains 5.1T tokens in total, but only about 300B tokens are used to develop our Marco. We will discuss the utilization of data in Section 3.1.6. 3.1.6. Data Utilization Although we gathered total of 5.1T training data, the actual amount used for continual pretraining is 300B. The overall data utilization rate is only 5.9%. Table 1 presents detailed breakdown of the utilization of each language and data source. We have the following findings: It is evident that, overall, the data utilization rates for both high-resource and low-resource languages are low. The more data we collect, the less effectively we utilize it. The corpus of Malay is only 2.9B, resulting in its utilization rate is up to 64.4%. The parallel data, high-quality knowledge data, and synthetic data have higher utilization rate. These data are of high quality and focused on specific tasks; moreover, the cost of acquisition is substantial. Therefore, these data are well used. Specifically, the parallel data is used to enhance the down-stream machine translation, while the others are employed to enhance oriented ability of LLM. The quality and diversity of synthetic data enhances Marco-LLM performance, thus its utilization rate is up to 73.3%. The multilingual capacity relies on the tokens utilized. Intuitively, to improve performance in specific languages, we should immerse the LLM in more diverse corpus. The number of tokens used in each high-resource languages is 10.6B, compared to 1.9B in each low-resource languages. According to Table 6 and Table 7, the improvement in low-resource languages is greater than that in high-resource languages. It indicates that open-source models (e.g., Qwen2/2.5 and llama3/3.1) primarily focus on high-resource languages, with the continuous pretraining on only 1.9B corpus for each low-resource languages producing significant effects. Based on the above findings, we outlines the following directions for further exploration: Further improving quality and diversity of multilingual data. The overall data utilization rate is only 5.9%. It means that we should further improve quality of our training data for the next model iterations. Moreover, take the diversity multilingual data into consideration, we will pay more attention to collecting multilingual data with local features. Synthetic data scaling and improving self-improvement capability. We conclude that training with synthetic data is effective. Therefore, we will continuously focus on developing advanced techniques that can control and manipulate specific attributes of the generated data, enabling the creation of diverse and customizable synthetic datasets. In addition, we will explore methods that integrate domain-specific knowledge to ensure that the generated data adheres to the underlying constraints and patterns present in the target domain. Furthermore, We aim to unlock the potential of emerging self-improvement capabilities by utilizing Marco-LLM to generate synthetic data, as we believe it can potentially bootstrap its own performance by iteratively learning from the enhanced synthetic data. 10 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 2 The proportion of high-quality and multilingual sources. Data Stage-I Stage-II English (en) Chinese (zh) High-Resourced (Others) Low-Resourced Parallel Multilingual Data High-quality Knowledge Data Synthetic Data 32% 17% 30% 9% 6% 5% 1% 28% 15% 26% 15% 8% 6% 2% 3.2. Two-Stage Continual Pretraining Strategy We develop Marco-LLM based on Qwen2, which is naturally designed to be multilingual-friendly and has an extensive vocabulary of 150k, ensuring high compression rate for multilingual data. Nonetheless, the primary challenge of continual pretraining lies in balancing the adaptation (which can lead to suboptimal performance on the new dataset) and catastrophic forgetting (resulting in significant capability loss on the previous dataset). In this paper, we propose an advanced two-stage continual pretraining learning approach designed to facilitate the transfer of commonsense knowledge, primarily acquired in English and Chinese, to variety of low-resource languages, as well as specific NLP downstream tasks such as machine translation. when it comes to continual pretraining, the data mixture and the learning rate are two crucial hyper-parameters to optimize Marco. In our practice, we employ different hyper-parameters in the two-stage training. Specifically, we employ data mixing to balance the adaptation of multilingual capabilities and prevent catastrophic forgetting in Stage-I, while the goal of Stage-II is to further strengthen Marco-LLMs multilingual capabilities via lower maximum learning rate. We describe these stages below. Data Mixture. Optimizing LLMs to learn knowledge encoded in multiple languages simultaneously is significant challenge. We concretely formulate this problem as transferring general knowledge to low-resource languages while maintaining the advantage of original languages in the base model. To address this issue, we keep the 32% English and 17% Chinese corpus in Stage-I training to avoid catastrophic forgetting, as shown in Table 2. Meanwhile, the proportion of other high-resourced (HR, apart from Chinese and English) and low-resourced (LR) are about 30% and 9%, respectively, to develop Marco-LLM with multilingual capabilities. The intuition is that HR has higher priority and is more commonly used. In Stage-II, we raise greater proportion of LR data from 9% to 15%, to further strength Marco-LLM multilingual capabilities in low-resourced. Moreover, parallel data, high-quality knowledge data, and synthetic data are increased, while dropping down English and Chinese corpus. Notably, the proportion of diverse corpus is determined by large number of experiments in Marco-1.5B with constant learning rate, not by manual experience. Lower Maximal Learning Rate. Learning rate is crucial hyper-parameter in neural network models that controls the magnitude of parameter updates [Ibrahim et al., 2024]. However, most performant open-source LLMs [Yang et al., 2024a, Dubey and et al, 2024] decay their learning rate to small value (i.e. 1e-7) by the end of training. In our practice, we employ the learning rate to be re-warned and re-decayed to improve adaptation per compute spent when continual pretraining on new distribution. The key challenge is to optimize the maximum learning rate. Empirically, decreasing the schedules maximum learning rate can help reduce forgetting, whereas increasing it can improve adaptation. We refer to Section 3.6 for learning rate tuning. After conducting fine-grained learning rate experiments when the data mixture is determined, we set maximal learning rate to 11 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement 1ùëí 5 in Stage-I to strike balance between the acquisition of multilingual languages and catastrophic forgetting. Based on the HR multilingual capacities acquired from the Stage-I training, we proceed to train the LR corpus with smaller learning rate of 6ùëí 6. As described in Table 3, the training tokens in Stage-I is about 160B, while Stage-II introduces an additional 140B tokens. Note that, an attempt to apply the Warmup-Stable-Decay (WSD) learning rate scheduler [Hu et al., 2024] resulted in subpar performance. It is suspected that the stable stage does not necessarily benefit model continual pretraining. Training. Marco-LLM were trained using Pai-Megatron-LM on cluster of 512 A100 GPU (64x80G) servers. To accelerate multi-node training for large model, tensor parallel and pipeline parallel were set to 8 to maximize the model flops utilization (MFU) of NVIDIA GPUs. Specifically, we employ 512 GPU cards for Marco-72B, and 256 GPU cards for Marco-1.5B/7B. To ensure the model learns the distribution akin, we conduct experiments in Marco-1.5B to optimize the mixing of data and learning rate. Then, we scale them up to Marco-7B and Marco-72B. We adopt most of the pretraining settings and model architectures from Qwen2. During the training, all Marco-LLM were continuously pre-trained over 300B tokens, using the Adam (ùõΩ1 = 0.9, ùõΩ2 = 0.95 ) optimizer. We utilize context window length of 32768 for Marco-1.5B and 7B, while the sequence length is 8192 in Marco-72B. At each stage, we warm-up the learning rate from 0 to the maximum learning rate over the first 10B tokens, and then decay it to 10% of the maximal learning rate using cosine schedule. We use weight decay of 0.1 and gradient clipping of 1.0. After the two-stage continual pretraining, we have developed our multilingual large model, Marco. Our two-stage continual pretraining is both effective and efficient to extend to the incoming rarely low-resourced languages, we leave it further exploration for future model iterations. Table 3 The training corpus tokens and learning rate in two Stage Continual Pretraining. Stage Training Tokens (B) LR Stage-I Stage-II 160 140 1ùëí 5 6ùëí 6 3.3. Evaluation Results We aim to assess the capabilities of Marco-LLM from various perspectives: 1) the ability of LLMs to understand and generate natural language, as well as the ability to grasp world knowledge; 2) the performance of LLMs across different languages; and 3) their capacity to handle cross-lingual tasks such as machine translation. Following the experiment design of previous work [Wei et al., 2023b], we gather subset of datasets from previous NLP tasks to construct multilingual benchmark. Table 4 summarizes the evaluation tasks and datasets, together with their language coverage. 3.3.1. Benchmarks and Evaluation Protocol All the datasets in the above multilingual benchmark can be divided into four groups: Natural Language Understanding, Knowledge, Question answering, and Machine Translation. The details of each dataset that we use for benchmarking are given below. AGIEval: AGIEval [Zhong et al., 2023] is benchmark dataset designed to evaluate the reasoning and problem-solving abilities of artificial intelligence models on tasks that mimic human examinations. https://github.com/",
                "summary": "<p>–í–≤–µ–¥–µ–Ω–∏–µ –≤ –∫—Ä—É–ø–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏</p>\n<p>–ö—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM), —Ç–∞–∫–∏–µ –∫–∞–∫ GPT-3 [Brown et al., 2020], GPT-4 [OpenAI, 2023], PaLM [Chowdhery et al., 2024] –∏ LLaMA [Touvron et al., 2023b], –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∏ –æ–±–ª–∞—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã–¥–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –ø–µ—Ä–µ–≤–æ–¥. –≠—Ç–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –±—ã–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –≥–ª–∞–≤–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –∑–∞ —Å—á–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –∏ –æ–±—ä–µ–º–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç—Ç–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –≤—ã—Å–æ–∫–∏–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –≠—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫ —Ä–∞–∑—Ä—ã–≤—É –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á, –æ—Å–æ–±–µ–Ω–Ω–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —è–∑—ã–∫–∞–º–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</p>\n<p>–ú—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏ –∏–∑-–∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ—Å—É—Ä—Å–æ–≤. –Ø–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ —á–∞—Å—Ç–æ —Å—Ç—Ä–∞–¥–∞—é—Ç –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∞ –æ–±—à–∏—Ä–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–∑—ã–∫–∞ –¥–ª—è —Ç–∞–∫–∏—Ö —è–∑—ã–∫–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≥–æ–≤–æ—Ä—è—â–∏–µ –Ω–∞ —è–∑—ã–∫–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≤ –≤—ã–≥–æ–¥–∞—Ö, –ø—Ä–∏–Ω–æ—Å–∏–º—ã—Ö –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP.</p>\n<p>–î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–æ–≥–æ –Ω–µ—Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –º–æ–¥–µ–ª—å Marco-LLM ‚Äî –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ —è–∑—ã–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏. Marco-LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å–µ—Ä–∏–∏ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç –º–∞—Å—Å–æ–≤–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏–µ (–≤–∫–ª—é—á–∞—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ Qwen2 [Bai et al., 2023].</p>\n<p>–û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li>–°–±–æ—Ä –∏ –∫—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è —è–∑—ã–∫–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.</li>\n<li>–ú–∞—Å—Å–æ–≤–æ–µ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥ –Ω–∞ –º–æ–¥–µ–ª–∏ Qwen2 –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ Marco-LLM.</li>\n<li>–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø–æ–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ Marco-LLM –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ.</li>\n</ol>\n<p>–û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<p>–†–∞–∑–¥–µ–ª 2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±–∑–æ—Ä –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ LLM, –æ—Å–æ–±–µ–Ω–Ω–æ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö LLM –∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–π –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –¥–ª—è LLM.\n–†–∞–∑–¥–µ–ª 3 –æ–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—é, –≤–∫–ª—é—á–∞—è —Å–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –º–æ–Ω–æ- –∏ –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏.\n–†–∞–∑–¥–µ–ª 4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ (–≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—É—é –¥–æ–æ–±—É—á–∞—é—â—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π) –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Marco-LLM, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ —Ä–∞–∑–¥–µ–ª–µ 3.</p>"
            },
            {
                "title": "Evaluation Benchmarks For Marco-LLM",
                "content": "alibaba/Pai-Megatron-Patch/ Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 4 Evaluation benchmarks overview. The table presents the comprehensive evaluation suite used in our experiments, spanning four major task categories: general knowledge, multilingual understanding, question answering, and machine translation. Each dataset is evaluated using either accuracy (Acc.), F1 score, or BLEU metric, covering diverse range of languages from single-language (en, zh) to multilingual scenarios. Task General Knowledge Dataset CEVAL AGIEval ARC MMLU Multilingual Understanding X-MMLU XCOPA Val Val XStoryCloze Val Question Answering Machine Translation TyDiQA Belebele Flores WMT-16 Split Metric #Languages #-shots Val Test Test Test Val Test Acc. Acc. Acc. Acc. Acc. Acc. Acc. F1 Acc. One One One One Six Thirteen Six Six Twenty-Eight Devtest BLEU BLEU Test Twenty-Eight Three 5-shot 5-shot 25-shot 5-shot 5-shot 5-shot 5-shot 1-shot 1-shot 1-shot 1-shot It includes thousands of multiple-choice questions sourced from real-world standardized tests such as the GRE, GMAT, LSAT, and other professional certification exams. The questions cover wide range of subjects, including mathematics, logical reasoning, reading comprehension, and analytical writing. ARC (AI2 Reasoning Challenge): The ARC dataset [Clark et al., 2018] consists of 7,787 natural language questions designed to assess an AI models ability to answer grade-school-level science questions. It is divided into two subsets: Easy Set, containing 5,217 questions that can often be answered with surface-level information, and Challenge Set, comprising 2,570 more difficult questions that require reasoning and background knowledge beyond simple retrieval. Questions are multiple-choice, with four options each, covering topics like biology, physics, chemistry, and earth science. Belebele: Belebele [Bandarkar et al., 2024] is multilingual multiple-choice reading comprehension dataset spanning 122 language variants, including low-resource and typologically diverse languages. It provides benchmark for evaluating machine reading comprehension across wide linguistic spectrum. Each question includes passage, query, and four answer choices. CEVAL: CEVAL [Huang et al., 2023] is comprehensive evaluation suite designed to assess the capabilities of Chinese language models. It includes over 13,000 multiple-choice questions sourced from real Chinese national college entrance examinations and professional qualification tests. The dataset covers 52 subjects across disciplines such as mathematics, physics, law, medicine, and language arts. Each question has four options. Flores: The Flores (Facebook Low Resource Languages for Emergent Situations) dataset [Team et al., 2022] is multilingual machine translation benchmark focused on low-resource languages. It includes parallel sentences in over 100 languages, with particular emphasis on underrepresented Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and low-resource languages. The dataset provides high-quality, professionally translated sentences, allowing for accurate evaluation of machine translation models. HellaSwag: HellaSwag [Zellers et al., 2019] is benchmark dataset designed to test models ability to perform commonsense reasoning and natural language inference. It contains 70,000 multiple-choice questions generated from descriptions of everyday activities. Each question provides context and four possible endings, with one correct continuation and three distractors that are misleading and adversarially constructed. For example: Context: \"A person is cooking onions on stove. They begin to cry because...\" Options: 1. \"the onions release gas that irritates the eyes.\" (Correct) 2. \"they are listening to sad music.\" 3. \"the stove is very hot.\" 4. \"they forgot to buy garlic.\" MMLU (Massive Multitask Language Understanding): The MMLU benchmark [Hendrycks et al., 2021] is designed to evaluate the broad knowledge and problem-solving abilities of AI models across 57 subjects. It includes over 57,000 multiple-choice questions from high school, undergraduate, and professional levels. Subjects span various domains, including history, mathematics, law, medicine, computer science, and more. Each question has four options, and the tasks often require reasoning, calculation, or application of specialized knowledge. TyDiQA: TyDiQA (Typologically Diverse Question Answering) is benchmark dataset [Clark et al., 2020] for information-seeking question answering in 11 typologically diverse languages. It includes over 200,000 question-answer pairs with passages from Wikipedia in languages such as Arabic, Bengali, Finnish, Japanese, Korean, Russian, Swahili, Telugu, and others. The dataset is designed to test models ability to understand and generate answers in different languages without relying on English translations. TyDiQA has two primary tasks: Gold Passage Task, where models are provided with the correct passage containing the answer, and Minimal Answer Task, where models must find the minimal span in the passage that answers the question. WMT16: The WMT16 [Bojar et al., 2016] datasets are part of the Conference on Machine Translations annual shared tasks, which provide standard benchmark datasets for evaluating machine translation systems. WMT16 includes parallel corpora for language pairs such as English-German, English-French, English-Russian and expands based on previous years by adding languages like Romanian and incorporating more challenging test sets. XCOPA: XCOPA [Ponti et al., 2020] is multilingual dataset for evaluating causal commonsense reasoning in AI systems across multiple languages. It extends the original COPA (Choice of Plausible Alternatives) dataset to 11 languages, including languages like Haitian Creole, Quechua, and Yoruba. Each question consists of premise and two alternative causes or effects, and the task is to select the more plausible one. For example: Premise: \"The ground is wet.\" Options: 1. \"It rained last night.\" (Cause) 2. \"The sun is shining.\" X-MMLU: X-MMLU [Dac Lai et al., 2023] is an extension of the MMLU benchmark for evaluating multilingual models. It includes translated versions of the original MMLU tasks into multiple languages. The dataset aims to assess models ability to perform multitask language understanding across diverse languages, testing both its knowledge and reasoning skills in non-English contexts. X-MMLU covers subjects like mathematics, science, and humanities, requiring models to demonstrate proficiency comparable to educated human speakers in various languages. 14 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XStoryCloze: XStoryCloze [Lin et al., 2021] is multilingual version of the Story Cloze Test, designed to evaluate models ability to understand and reason about narratives in different languages. The dataset provides short, four-sentence stories followed by two possible endings, and the task is to choose the coherent conclusion. It includes translations of the stories into multiple languages, testing narrative understanding and commonsense reasoning. For example: Story: 1. \"Maria woke up early on Saturday.\" 2. \"She was excited about the trip.\" 3. \"She packed her bags quickly.\" 4. \"She grabbed her keys and left the house.\" Endings: A. \"She arrived at the airport just in time for her flight.\" (Correct) B. \"She decided to go back to sleep because it was raining.\" 3.3.2. Baseline LLMs Llama3 and Llama3.1 The Llama 3 series includes the Llama3-8B/70B and Llama3.1-8B/70B model. These models have been pretrained on over 15 trillion tokens from publicly available sources, achieving superior performance on various benchmarks [Dubey and et al, 2024]. Qwen2 and Qwen2.5 We compare with the Qwen2 [Yang et al., 2024b] series LLMs, including the Qwen2-7B/72B and Qwen2.5-7B/72B models. Qwen2 LLM is pre-trained on 7 trillion tokens and Qwen2.5 is further optimized version of Qwen2, which is pretrained on 18 trillion tokens and achieved state-of-the-art performance on multiple evaluation benchmarks. 3.3.3. Experimental Results Table 5 Performance comparison of LLMs across various benchmarks: Results for LLMs with parameters of both 7B and 70B. The best performance in each benchmark is in bold. 7B Models Model Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Marco-7B 70B+ Models Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-72B AGIEval Belebele CEval Flores MMLU TyDiQA WMT16 XCOPA XMMLU XStoryCloze 64.6 66.5 24.3 44.9 68.8 78.2 80.8 60.6 61.7 84.4 73.4 72.3 55.3 63.3 78. 86.5 87.6 85.5 86.2 90.0 83.0 81.4 37.5 52.8 83.5 90.4 90.6 66.8 67.3 93.7 27.1 27.2 33.1 33.4 35.0 38.7 35.0 37.4 36.9 45.0 71.9 75.4 53.6 66.2 74. 83.8 86.3 79.2 78.8 86.3 52.3 59.9 50.5 57.0 60.8 58.7 63.7 64.3 62.8 62.7 18.1 18.2 24.6 25.8 29.0 30.2 31.0 34.3 35.0 35.1 70.6 73.6 71.7 71.6 76. 80.9 84.7 81.1 83.0 85.7 60.2 62.6 49.7 49.2 61.2 78.5 79.9 72.0 71.4 81.2 70.6 70.3 66.5 71.7 71.9 77.1 76.3 76.9 75.4 78.7 Results divided by benchmarks Our proposed models, Marco-7B and Marco-72B, demonstrate superior multilingual capabilities across variety of benchmarks compared to baseline models of similar sizes (see Tables 5). On multilingual understanding and reasoning tasks such as Belebele, CEVAL, MMLU, XMMLU, XCOPA, and XStoryCloze, the Marco-LLM consistently outperform the other strong LLMs, indicating enhanced proficiency in comprehending and common sense reasoning across diverse languages. For the 7B models, Marco-7B achieves the best performance across several benchmarks, obtaining the highest scores in AGIEval (68.8), Belebele (78.8), CEval (83.5), Flores (35.0), and TyDiQA (60.8). These results highlight its proficiency in handling diverse tasks and datasets, outperforming the strongest competitor, Qwen2.5-7B, by 2.3, 6.5, 2.1, 7.8, and 0.9 points, respectively. The 72B models further amplify these strengths. Marco-72B achieves the highest scores in multiple benchmarks, including AGIEval (84.4), Belebele (90.0), CEval (93.7), Flores (45.0), and 15 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement XMMLU (81.2), showcasing its exceptional capacity to handle complex linguistic tasks. Compared to Qwen2.5-72B, Marco-72B surpasses by 3.6 points in AGIEval, 2.4 points in Belebele, and 10.0 points in Flores. Notably, Marco-72B achieves the highest scores in benchmarks like CEVAL (93.7) and XMMLU (81.4), underscoring its advanced multilingual understanding and reasoning abilities. In addition, the Marco-LLM exhibit strong performance in multilingual translation and generation tasks, as evidenced by competitive scores on the Flores and WMT16 benchmarks. Their superior results in TyDiQA further highlight their capacity for multilingual question answering and knowledge retrieval. Overall, these findings supported that our focus on enhancing the multilingual abilities of LLMs has led to models that are highly effective in understanding, reasoning, and generating text across multiple languages, thereby validating the efficacy of our approach. Table 6 Average performance on multilingual benchmarks shown in Table 4 for five 7B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-7B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 55.1 69.6 40.5 47.3 57.0 56.0 63.4 43.2 56.8 51.1 39.1 45.9 50.2 64.0 68.1 62.2 56.8 61.0 65.6 69.2 54.8 60.9 50.0 67.6 59.2 68.1 59.8 41.3 37.0 55.9 63.5 74.2 52.6 59.8 64.9 63.5 74.9 63.8 64.7 62.9 48.2 49.8 56.6 66.1 69.4 60.7 57.7 65.2 67.4 70.7 53.3 60.4 56.7 70.3 63.6 67.7 61.0 44.0 41.56 61. 75.8 77.4 61.3 69.5 70.4 69.8 76.7 76.0 70.7 66.0 52.2 63.9 69.8 77.3 80.3 77.2 73.2 80.2 77.3 81.2 69.1 72.7 63.9 76.1 76.9 65.0 63.3 43.1 36.33 69.4 75.5 78.0 64.5 69.0 71.9 71.2 76.4 79.7 72.3 66.6 53.2 62.3 68.6 77.8 79.4 76.3 73.9 71.6 72.6 77.1 73.7 70.4 59.9 79.0 70.0 67.2 57.9 45.9 42.1 69.1 76.0 77.9 66.0 72.9 72.6 72.5 77.3 78.3 72.3 73.4 69.4 68.9 77.3 82.3 83.4 80.9 80.2 82.1 80.8 83.2 72.9 79.9 71.3 81.2 78.2 77.1 69.7 66.1 65.8 75. Results divided by languages The experiments evaluate the performance of our Marco-LLM: both 7B and 72B variants against various strong open-source LLMs across diverse set of languages on the benchmarks shown in Section 3.3.1. Both Marco-LLM are built on continual pretraining on Qwen2 with massive multilingual data. The results divided by languages shown in Table 6 reveals that Marco-7B achieves leading average score of 75.5 among the 7B parameter models. In low-resource languages such as Nepali and Kazakh, Marco-7B obtained strong performance with scores of 65.8 and 66.1, respectively, outperforming Qwen2-7B by substantial margins of 29.47 and 23.0 points. This improvement underscores the benefits of our continual pretraining approach using vast multilingual 16 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 7 Average performance on multilingual benchmarks shown in Table 4 for five 70B-parameter LLMs divided by 29 languages. The best performance for each language is highlighted in bold. Language Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B Chinese (zh) English (en) Arabic (ar) German (de) Spanish (es) French (fr) Japanese (ja) Korean (ko) Portuguese (pt) Turkish (tr) Azerbaijani (az) Bengali (bn) Hebrew (he) Indonesian (id) Italian (it) Polish (pl) Malay (ms) Dutch (nl) Romanian (ro) Russian (ru) Thai (th) Ukrainian (uk) Urdu (ur) Vietnamese (vi) Czech (cs) Greek (el) Hungarian (hu) Kazakh (kk) Nepali (ne) Avg. Scores 75.2 82.7 73.0 80.8 80.7 78.9 82.5 87.1 81.1 80.8 77.2 79.7 80.1 87.7 87.1 87.2 85.2 88.9 88.4 87.9 80.2 89.0 80.6 87.1 87.1 88.3 83.8 74.7 70.1 82.5 75.0 83.2 73.4 80.9 79.4 80.7 84.5 87.0 80.8 81.3 77.9 79.7 82.3 88.0 87.9 87.1 87.2 88.8 88.2 88.3 80.4 88.6 81.2 89.6 87.6 89.6 83.0 77.9 73.8 83. 83.7 84.6 76.2 84.0 82.5 80.4 86.6 88.8 83.6 79.0 78.8 83.2 83.9 87.6 88.1 88.6 83.4 89.0 88.2 89.9 85.7 88.2 81.4 88.7 87.8 89.4 80.1 70.3 67.1 83.8 84.8 85.1 77.4 85.0 83.1 82.7 86.1 88.9 83.8 81.5 79.1 82.9 84.1 88.4 89.9 88.8 87.9 90.3 87.3 91.0 87.6 90.2 82.1 90.0 90.0 89.0 88.1 72.2 74.1 85.2 86.4 86.0 79.7 87.0 84.8 82.8 86.2 90.6 85.5 84.4 85.8 86.7 85.1 93.0 91.0 88.2 90.7 93.0 90.4 90.3 88.3 91.7 87.9 90.8 91.4 91.1 88.3 84.8 86.7 87. corpus, which enhances the models ability to generalize across languages with limited resources. Marco-7B also shows competitive performance in high-resource languages like Arabic and German, surpassing Qwen2.5-7B by 1.5 and 3.9 points, respectively. These results highlight the effectiveness of our continual pretraining approach, which improves the models adaptability to different linguistic structures and complexities. Similarly, Marco-72B exhibits remarkable performance (shown in Table 7), achieving the highest average score of 87.9 among the LLMs with 70B+ parameters. The Marco-72B model further extends these capabilities with an average score of 87.9 across 29 languages. It demonstrates remarkable performance in low-resource languages, achieving 86.7 in Nepali and 84.8 in Kazakh, reflecting improvements over Qwen2.5-72B by 12.6 and 12.6 points, respectively. This indicates the efficacy of scaling model size alongside multilingual training data to achieve superior performance in challenging linguistic contexts. Additionally, Marco-72B remains highly competitive in high-resource languages, achieving scores of 79.7 in Arabic and 87.0 in German, which are improvements of 2.3 and 2.0 points over Qwen2.5-72B. The consistent outperformance of Marco-LLM across both high-resource and low-resource languages (especially compared with Qwen2 - the base LLM we conduct continual pretraining, our Marco-LLM achieved substantial improvements over the languages shown in Table 1) underscores the pivotal role of leveraging vast multilingual corpus during pretraining as shown in Figure 3. This approach not only enhances the models ability to generalize to low-resource languages, but also maintains strong performance in high-resource languages such as English and Chinese. These 17 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement findings suggest that the comprehensive training methodologies employed in the Marco-LLM are key factors contributing to their leading performance in multilingual settings. (a) (b) (c) (d) Figure 4 Evolution of performance on question answering and machine translation during continual pretraining in Marco-1.5B. 3.4. Evolution of performance during continual pretraining During continual pretraining, we track the performance of our models on few question answering and machine translation benchmarks. we report the performance of Marco-1.5B in Figure 4. On question answering benchmarks, the performance for multilingual languages such as Arabic (ar), German (de), Spanish (es), French (fr), Korean (ko), Japanese (ja), Portuguese (pt), and Turkish (tr) improve steadily, and while capabilities in Chinese (zh) and English (en) are maintained, and even slightly increased. On Flores, we observe the performance of all languages shows consistent improvement. Notably, Figure 4(d) illustrates the averages for both English-to-multilingual (ENXX) and multilingual-to-English (XXEN) directions. These indicators are consistent with our goals aiming at enhancing multilingual capabilities. 3.5. Data ablations on Parallel Data Empirically, introducing parallel corpora can enhance semantic alignment between cross-languages. In this section, we will explore the impact of parallel corpora on downstream specific NLP tasks, such as machine translation. We mainly focus on the quality of parallel data. The open-source parallel data has many bad cases, what happens if we ignore them on continual pretraining? we report our findings in Figure 5, where Marco-w/o-parallel-data-filtering indicates that Marco-LLM were continuously pre-trained on Qwen2 without any filtering on parallel data. We have the following findings: Phenomena vary across different model scales. Compared to base model Qwen2, the smaller models, specifically the 1.5B and 7B models, Marco-w/o-parallel-data-filtering shows improvements, whereas the 72B model has performance degradation. We will elaborate on this phenomenon. Firstly, Marcos machine translation capability benefits from both monolingual data and parallel data, thus resulting in gradient conflicts utilizing low-quality parallel data during continual pretraining in smaller models. The monolingual data, due to its dominant proportion, plays crucially positive role, which explains the improvements observed with Marco-w/o-parallel-data-filtering. Secondly, due to parameter redundancy, the 72B model contains higher number of monosemantic neurons[Gurnee et al., 2023]. The low-quality parallel data negatively interferes with the machine translation task. This meaningful discovery reveals an important insights that some conclusions obtained on small models are not necessarily scaled to larger models. Furthermore, beyond the issue of parallel 18 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 5 The performance of different model size on Flores benchmark. Marco-w/o-parallel-datafiltering denotes that we continuously pre-trained Marco-LLM based on Qwen2 without applying any filtering to parallel data. data, we suspect that cross-language gradient conflicts may also exist during multilingual training, which we will explore in future research. High-quality data enhances the performance of model. Compared to base model Qwen2, our Marco-LLM that has been continuously pre-trained with the processed parallel data, shows significant improvements across the 1.5B, 7B, and 72B models. We attribute it primarily to its data quality resulting from our data-engineering efforts. Furthermore, we will enhance the quality of our training data for the upcoming model iterations. 3.6. Effect of Learning Rate In this section, we explore the effect of the crucial hyper-parameter during continual pretraining. We select learning rate (lr) from {1ùëí5, 2ùëí5, 3ùëí5} and conduct continual pretraining on 200B corpus under determined data mixture. Figure 6 demonstrates the training dynamic between the average score on question answering benchmarks and different learning rate. Specifically, Figure 6(a) presents that the forgetting of Chinese and English ability is aggravated with the increase of learning rate, and multilingual ability is generally enhanced in Figure 6(b). Interestingly, the average accuracy in multilingual languages goes up firstly and then down when learning rate is 3ùëí5. We believe that this is due to the loss of primary language ability resulting in reducing natural language understanding. Notably, the machine translation ability gets better as the learning rate increases, which shows consistently in Figure 4(d). Therefore, we set the peak learning rate to 1ùëí5 in our experiments, as it plays pivotal role in striking balance between the acquisition of multilingual languages and the forgetting of English and Chinese. 4. Extensive Multilingual Post-training for Large Language Models After conducting continuous pre-training on up to 29 languages, we proceeded with post pre-training of the Macro model. This phase of training primarily comprises two stages: Supervised Fine-Tuning (SFT) [Wei et al., 2022, Ouyang et al., 2022] and Direct Preference Optimization (DPO) [Rafailov et al., 2023]. The main objective of the Supervised Fine-Tuning (SFT) stage is to activate and enhance the models multilingual capabilities across various domains, including commonsense reasoning, dialogue-based question answering, precise instruction following, mathematical and logical reasoning, multilingual comprehension and translation, and coding. During this stage, our research particularly focuses on (1) the automatic generation and cost-effective collection of high-quality multilingual data, and (2) the transfer of extensive domain knowledge from high-resource languages, such as English, 19 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement (a) The average accuracy in English and Chinese with different learning rate. (b) The average accuracy in multilingual languages with different learning rate. Figure 6 The average performance on question answering benchmarks with different learning rate during continual pretraining in Marco-1.5B. Chinese, and French, to low-resource languages. The DPO stage aims to ensure that the content generated by the model aligns with specified preference. 4.1. Multilingual Supervised Fine-tuning 4.1.1. Data Construction The Supervised Fine-Tuning (SFT) dataset is composed of several components: limited set of detoxification data annotated by experts, along with multilingual self-cognition enhancement data. Synthetic data and parallel corpora, which include precise instruction enhancement data, multilingual Alpaca dialogue data, as well as synthetic data for specific tasks such as comprehension, generation, and translation. Open-source instruction data, including Chain-of-Thought (CoT) enhancement data and general instruction data like Aya collection [Singh et al., 2024b]. This includes multi-turn dialogues, coding, mathematics, and more, with examples such as UltraChat, Glaive-Code, MetaMathQA, MathInstruct, Belle, and Orca. We utilize parallel data for multilingual machine translation tasks, enhancing language diversity and quality within the dataset. Specifically, we use the dev sets from WMT-14 to WMT-20 [Barrault et al., 2020] and the WikiMatrix [Schwenk et al., 2021]. Data Collection and Processing Our data collection endeavors encompass two primary facets. On one hand, we engage in the aggregation and cleansing of open-source data. On the other hand, we employ data synthesis and the translation of parallel corpora to augment the dataset. Building upon these two approaches, we have developed both multilingual Supervised Fine-Tuning (SFT) datasets and multilingual Direct Preference Optimization (DPO) datasets. The following sections provide detailed examination of the composition of these datasets and the experiments conducted to assess data distribution proportions. Data Cleaning Given that the majority of our dataset is derived from open-source, synthetic, and parallel corpus data, we implemented comprehensive data processing strategy to ensure quality. Initially, we applied regular expression filtering to remove inconsistencies such as merged 20 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement multi-turn dialogues, incorrect numbering in segmented outputs, HTML format outputs, emoji data, and hyperlinks or URL references. Additionally, regular expressions and Python calculations were employed to capture and validate mathematical equations, with incorrect mathematical results being discarded. Data Filtering To enhance the overall performance of the model, we employed comprehensive pipeline based on seminal works for data quality filtering, effectively removing low-quality training samples: Quality Scoring: For English-language data, we utilized the Deita model in conjunction to score the raw data within range of 1 to 6. High-scoring data were selected to construct parallel corpora, with GPT-4 further filtering the translated data. QA similarity: In assessing QA relevance, we evaluated the semantic similarity between input and output fields. Data with similarity below certain threshold were considered irrelevant and subsequently removed. Mathematics Grading: For mathematical data, we deployed models from the open-source project Open-Web-Math to score the data, retaining only those with higher scores for training purposes. Multilingual difficulty scoring: For extensive multilingual parallel datasets, assessing translation quality through open-source models poses challenges. We employed the Instruct-FollowingDifficulty (IFD) method. By examining the ratio of Conditioned Answer Score (CA) to Direct Answer Score (DA) during the initial iterations, we filtered data that significantly benefited the model. Semantic deduplication: Initially, we traversed the dataset to remove duplicate instructions. We then applied MinHash and SimHash techniques for further deduplication. Lastly, embeddings were extracted using open-source models, retaining data with embedding similarity below 0.7 threshold. 4.1.2. Training Setup In our instruction fine-tuning, neat packing was employed to train CT model with context length of 16,384 on our supervised finetuning data of totally 5.7 milliion examples. The training utilized the Adam optimizer with cosine schedule learning rate. It was observed that setting large learning rate during full model fine-tuning could severely affect the general knowledge acquired during the pre-training and CT phases, leading to performance degradation. The optimal instruction fine-tuning learning rate was determined by adjusting the minimum pre-training learning rate in accordance with the batch size, with the maximum and minimum fine-tuning learning rates identified as 6e-6 and 6e-7, respectively. 4.1.3. Evaluation Benchmarks In the evaluation experiments, we employ TyDiQA, AGIEval, CEVAL, Belebele and multilingual version of the original MMLU: Multilingual MMLU (MMMLU) The Multilingual Massive Multitask Language Understanding (MMMLU) dataset is an extension of the MMLU benchmark [Hendrycks et al., 2021] into multiple languages. MMMLU includes translations of the original 57 subjects into 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Brazilian Portuguese, Swahili, Yoruba, and Simplified Chinese, covering areas such as STEM, humanities, social sciences, https://huggingface.co/datasets/openai/MMMLU 21 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement and more. Each language version contains approximately 15,908 multiple-choice questions, mirroring the structure of the original MMLU dataset. 4.1.4. Baseline LLMs The LLMs we compared in this section are all Instruct models by default. We employ Llama3 and Llama3.1 as well as Qwen2 and Qwen2.5. Additionally, we also use Aya-23 and Aya-expanse: Aya-23 and Aya-expanse The Aya-23 and Aya-expanse LLMs [Aryabumi et al., 2024] that includes the Aya-23-8B, Aya-23-35",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö —Å –º–æ–¥–µ–ª—è–º–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á: –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –î–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–æ—á–Ω–æ—Å—Ç—å (Accuracy), –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å F1 –∏–ª–∏ BLEU. –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤, –æ—Ç –æ–¥–Ω–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤.</p>\n<p>–í —Ä–∞–∑–¥–µ–ª–µ –æ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ:</p>\n<ol>\n<li><strong>CEVAL</strong>: –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 13000 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —ç–∫–∑–∞–º–µ–Ω–æ–≤ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø—Ä–µ–¥–º–µ—Ç–∞–º, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∏–∑–∏–∫–∞, –ø—Ä–∞–≤–æ, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —è–∑—ã–∫–æ–≤—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞.</li>\n<li><strong>AGIEval</strong>: –í–æ–ø—Ä–æ—Å—ã –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∏—Ä–æ–≤—ã—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ GRE, GMAT, LSAT –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–∫–∑–∞–º–µ–Ω—ã.</li>\n<li><strong>ARC</strong> (AI2 Reasoning Challenge): –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –ø–æ—á—Ç–∏ 7800 –≤–æ–ø—Ä–æ—Å–æ–≤, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –Ω–∞—É—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —É—Ä–æ–≤–Ω—è —Å—Ä–µ–¥–Ω–µ–π —à–∫–æ–ª—ã. –í–æ–ø—Ä–æ—Å—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ –¥–≤–∞ –Ω–∞–±–æ—Ä–∞: –ø—Ä–æ—Å—Ç–æ–π –∏ —Å–ª–æ–∂–Ω—ã–π, —Ç—Ä–µ–±—É—é—â–∏–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π.</li>\n</ol>\n<p>–î–ª—è –º—É–ª—å—Ç–∏–ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ:</p>\n<ol>\n<li><strong>X-MMLU</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XCOPA</strong>: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ã–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è–∑—ã–∫–∞—Ö.</li>\n<li><strong>XStoryCloze</strong>: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –∏—Å—Ç–æ—Ä–∏–π –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö.</li>\n</ol>\n<p>–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –≤–∫–ª—é—á–∞—é—Ç:</p>\n<ol>\n<li><strong>TyDiQA</strong>: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏ –Ω–∞ 11 —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –ó–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –±–µ–∑ –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.</li>\n</ol>\n<p>–ù–∞–∫–æ–Ω–µ—Ü, –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:</p>\n<ol>\n<li><strong>Flores</strong>: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –±–æ–ª–µ–µ —á–µ–º 100 —è–∑—ã–∫–∞—Ö, –≤–∫–ª—é—á–∞—è –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∏ –Ω–∏–∑–∫–æ —Ä–µ—Å—É—Ä—Å–Ω—ã–µ —è–∑—ã–∫–∏.</li>\n</ol>\n<p>–≠—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –ò–ò –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.</p>"
            },
            {
                "title": "Performance Evaluation Of Multilingual Language Models",
                "content": "B, Aya-expanse-8B and Aya-expanse-35B models, which are open-source multilingual language models supporting 23 languages. Aya-23/Aya-expanse are based on CommandR with sophisticated multilingual supervised finetuning. Table 8 Performance comparison of LLMs across multiple major benchmarks. Best performance in each benchmark is marked in bold. Model MMMLU TydiQA AGIEval CEval Belebele 7B Models Aya-23-8B Aya-expanse-8B Llama3-8B Llama3.1-8B Qwen2-7B Qwen2.5-7B Marco-Chat-7B 70B Models Aya-23-35B Aya-expanse-32B Llama3-70B Llama3.1-70B Qwen2-72B Qwen2.5-72B Marco-72B 41.0 48.2 46.6 49.2 52.2 56.0 60.1 50.1 58.9 64.3 71.7 69.2 69.0 76.1 47.2 28.3 39.7 53.0 29.2 39. 57.7 50.2 30.0 52.0 53.1 40.3 48.4 61.0 37.1 36.7 43.4 41.8 57.1 59.0 43.9 48.5 50.8 55.6 81.8 77.9 61. 86.4 44.4 45.7 57.1 55.0 66.0 67.5 53.6 56.9 66.7 71.6 90.6 88.2 72.7 94.5 52.5 64.3 50.7 63.9 69.4 70. 79.3 66.3 72.7 76.2 84.4 85.3 88.9 89.6 4.1.5. Results and Discussion Evaluation Results divided by benchmarks Table 8 presents the average scores of our MarcoChat-7B and Marco-72B, in comparison with several baseline models across five major benchmarks: MMMLU, TydiQA, AGIEval, CEval, and Belebele. These benchmarks are designed to evaluate language models on diverse range of tasks and languages, highlighting their multilingual comprehension and reasoning abilities. Our Marco-Chat-7B model consistently achieves the highest scores among the 7B parameter models across all benchmarks. Specifically, it significantly outperforms the baselines on CEval and Belebele, which focus on Chinese educational subjects and variety of African languages, respectively. On CEval, Marco-Chat-7B attains score of 86.4, surpassing the next best model, Qwen2-7B, by substantial margin of 4.6 points. This indicates our models strong capability in understanding and processing Chinese language content. Similarly, on Belebele, which evaluates proficiency in underrepresented African languages, Marco-Chat-7B achieves score of 79.3, outperforming others https://cohere.com/blog/aya-expanse-connecting-our-world https://cohere.com/command 22 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 9 MMMLU Results: Performance of various LLMs across different languages in the MMMLU dataset for both 7B and 70B models. The best performance in each language is highlighted in bold. Language Qwen2 Qwen2.5 Llama3 Llama3.1 Aya-23 Aya-expanse Marco-Chat GPT7B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 70B Models Arabic Bengali German Spanish French Hindi Indonesian Italian Japanese Korean Chinese Portuguese Swahili Yoruba Avg. Score 50.9 42.6 57.3 60.3 61.1 44.5 56.6 60.2 56.3 54.1 62.0 59.9 34.9 30.5 52. 72.0 68.3 74.4 77.0 75.6 69.9 73.1 75.3 74.1 72.3 77.5 76.8 47.3 34.6 69.2 56.6 45.3 62.3 65.3 65.0 46.6 61.4 64.7 61.0 59.1 64.3 64.4 35.7 32.8 56.0 74.3 67.2 72.5 77.5 76.0 69.1 73.3 72.5 74.7 71.8 76.7 76.9 48.8 35.5 69. 40.5 36.4 53.5 55.8 55.8 41.4 51.0 53.3 42.3 46.5 51.4 55.5 37.5 31.0 46.6 60.6 53.8 71.4 74.3 73.1 65.0 70.6 73.3 65.6 64.5 69.5 73.7 51.1 33.6 64.3 42.2 39.8 55.6 59.1 58.9 45.8 54.3 56.3 52.1 50.8 55.5 59.0 40.3 31.4 50. 71.1 66.5 77.0 79.3 77.9 72.7 75.7 77.8 73.8 72.7 74.8 78.9 64.0 41.2 71.7 42.1 27.4 43.3 47.9 46.9 38.9 46.7 47.1 45.6 43.6 45.7 46.9 26.2 26.4 41.0 51.8 32.9 55.5 58.0 58.1 47.6 55.5 57.8 54.5 53.7 54.1 58.3 33.6 30.4 50. 48.8 33.4 53.9 56.1 55.5 46.2 53.3 55.3 51.5 50.7 52.5 55.8 32.0 29.9 48.2 61.6 43.9 64.7 67.5 67.5 58.8 65.4 66.5 64.3 62.4 63.4 67.2 38.4 33.4 58.9 60.6 54.4 65.9 67.7 67.6 54.3 62.3 65.4 64.2 63.0 66.5 67.6 43.9 37.2 60. 79.3 76.6 80.7 82.6 80.7 76.9 79.0 81.6 81.6 78.8 82.0 81.7 63.7 44.0 76.1 62.7 60.0 68.0 68.2 67.5 62.2 66.1 68.3 64.4 63.6 65.9 68.8 53.1 38.0 62.6 71.1 64.8 75.7 76.8 75.8 70.1 73.7 75.8 71.6 71.3 72.5 76.2 68.1 47.3 70. by nearly 9 points. This demonstrates the effectiveness of our multilingual training approach in capturing linguistic nuances across diverse languages, including those with limited available data. In the 70B size LLMs, Marco-72B leads the group by large margin on all benchmarks. It achieves score of 76.1 on MMMLU, which assesses multitask language understanding across various subjects and languages. This result is 4.4 points higher than the next best model, Llama3.1-70B, highlighting our models superior general language understanding capabilities. On TydiQA, benchmark for typologically diverse question answering, Marco-72B attains score of 61.0, outperforming the second-best model by 7.9 points. This suggests that our model shows strong performance at various tasks across wide range of languages with different grammatical structures and scripts. Additionally, Marco-72B achieves an impressive score of 94.5 on CEval, indicating excellent proficiency in Chinese across various academic subjects. On Belebele, it reaches 89.6, showcasing strong performance in languages that are often underrepresented in training data. These results highlight the effectiveness of our approach in building truly multilingual LLM. By consistently achieving top performance across benchmarks that evaluate different languages and tasks, our models demonstrate robust multilingual proficiency and adaptability. This underscores the importance of incorporating diverse and comprehensive multilingual dataset during training 23 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 10 Performance comparison of language models on Belebele benchmark [Bandarkar et al., 2024] across different languages. Language Qwen2 Qwen2.5 Llama-3 Llama3.1 Aya-23 Aya-expanse Marco-Chat 7B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 70B Models Azerbaijani Bengali Czech Greek Hebrew Hungarian Indonesian Italian Japanese Kazakh Malay Dutch Nepali Polish Romanian Russian Ukrainian Urdu Thai Vietnamese Avg. Scores 59.9 64.9 75.4 68.7 74.2 57.3 77.2 78.2 78.0 48.0 78.9 58.7 44.3 78.4 72.4 79.7 76.1 64.9 72.3 80. 69.4 79.9 84.9 89.7 89.2 85.0 72.8 88.9 89.8 87.8 73.6 88.7 90.9 70.6 86.4 88.4 90.0 90.9 83.1 86.2 88.8 85.3 60.4 64.2 75.9 75.2 72.7 63.0 78.4 81.9 69.8 51.2 77.1 74.6 49.4 70.3 74.0 73.3 73.2 64.4 74.5 77.0 70.0 81.6 87.3 91.9 92.6 86.9 89.3 91.7 90.4 90.2 76.0 91.2 93.2 80.1 89.7 92.1 94.1 93.4 86.4 87.1 92. 88.9 41.3 46.8 54.3 59.0 45.9 45.1 61.9 60.6 49.9 36.0 57.9 56.3 38.9 55.0 55.2 52.7 51.1 49.0 43.0 53.8 50.7 63.3 75.3 79.0 87.0 75.9 58.0 82.6 83.8 82.2 54.1 87.7 80.4 55.7 75.0 73.4 86.1 84.1 78.9 79.7 81.7 76.2 57.7 57.2 73.4 74.3 59.3 52.4 75.0 71.8 64.7 49.1 67.4 70.1 46.7 65.0 68.7 70.2 69.8 59.2 57.2 68. 63.9 79.7 81.4 86.8 89.4 78.9 74.1 87.3 87.9 86.9 78.2 88.7 87.1 76.0 88.7 86.3 87.2 90.2 90.2 82.7 83.9 84.4 34.1 28.7 61.6 65.2 61.7 35.0 64.7 67.0 64.2 28.3 53.2 66.2 32.9 61.1 65.9 64.1 61.8 35.6 37.6 61.4 52.5 51.9 42.1 79.6 80.1 77.0 53.6 77.8 81.1 73.7 40.7 74.8 80.8 39.9 73.9 75.7 73.2 74.3 47.2 53.6 75. 66.3 49.9 41.6 76.9 80.3 77.9 44.0 77.6 75.7 74.1 38.0 73.3 72.1 40.8 73.9 74.8 74.3 76.2 50.2 41.8 73.2 64.3 58.3 64.8 85.8 83.6 83.1 50.8 81.1 77.7 78.2 55.1 76.4 85.7 50.1 83.7 77.7 84.6 79.8 63.7 63.8 69.2 72.7 72.3 75.1 84.4 81.4 82.1 68.0 82.8 86.8 83.1 73.1 83.4 85.3 70.0 73.3 73.2 87.9 83.4 76.2 76.9 86. 79.3 85.6 89.2 91.8 91.9 86.0 87.0 93.1 91.1 90.1 81.7 92.1 94.4 84.4 90.6 90.6 92.7 93.0 88.2 87.6 92.2 89.6 to enhance the language understanding abilities of large language models. Our observations also reveal that models with higher number of parameters, like Marco-72B, substantially benefit from our multilingual training strategy, achieving significant improvements over other large models. Furthermore, the consistent gains across both high-resource languages (like English and Chinese) and low-resource languages (as represented in Belebele) indicate that our models do not merely rely on the abundance of data in certain languages but truly learn to generalize across linguistic boundaries. 24 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 11 Performance comparison of various LLMs and MT systems on the Flores benchmark. The best performance in each column is highlighted in bold. GPT4 DeepL Google Aya-32B Aya-35B Qwen2-72B Qwen2.5-72B Llama3-70B Llama3.1-70B Marco-7B Marco-72B EnXX En2Ar 40.4 48.1 50. En2De 45.9 48.7 49.3 En2Es 33. 32.9 34.6 En2Fr 54.4 59.1 57. En2It 37.2 41.5 39.1 En2Ja 34. 36.8 41.1 En2Ko 28.5 32.9 33. En2Nl 34.8 37.0 36.3 En2Pl En2Pt 30.3 33.4 33.7 54.8 45.7 En2Ru 36.8 40.5 40.9 En2Tr 36. 45.0 44.2 En2Uk 37.0 42.9 41. En2Zh 44.2 48.6 50.6 31.5 32. 28.4 50.5 32.4 31.0 27.2 25. 24.8 44.0 32.3 33.1 33.5 26. 24.4 33.1 20.6 34.5 25.3 9. 12.3 24.2 16.5 41.7 23.8 26. 17.0 15.6 Avg. Scores 39.2 42.4 43. 32.3 23.2 XXEn Ar2En 42.7 47.7 46. De2En 47.7 51.0 51.3 Es2En 34. 36.9 36.3 Fr2En 48.9 50.8 52. It2En 36.7 40.2 40.2 Ja2En 30. 37.0 36.7 Ko2En 33.3 39.3 38. Nl2En 36.0 37.7 38.7 Pl2En Pt2En 33.5 35.8 37.0 53.1 55.8 56. Ru2En 38.7 43.3 42.9 Tr2En 42. 48.5 47.7 Uk2En 43.4 47.2 47. Zh2En 31.3 36.8 37.7 33.3 30. 24.8 27.7 28.6 20.5 21.9 23. 19.6 37.9 23.6 31.1 27.4 24. 41.1 40.9 33.8 45.1 37.5 22. 25.9 32.8 27.6 50.7 36.2 36. 38.8 26.7 Avg. Scores 36.8 43.4 43. 26.8 35.4 17.1 37.7 32.0 52. 34.2 29.6 19.2 28.4 20.4 50. 33.6 22.8 25.2 28.0 30.8 41. 46.9 34.5 48.8 36.7 29.8 32. 35.9 33.7 53.0 39.0 39.9 41. 31.0 38.9 29.9 40.4 31.7 53. 34.8 33.0 24.8 30.9 26.0 52. 36.1 30.4 30.0 33.9 34.8 44. 48.4 35.3 49.5 38.2 31.9 34. 36.3 34.7 53.5 40.2 42.3 43. 35.4 40.6 29.2 43.0 31.5 52. 34.8 14.3 0.1 32.0 28.6 52. 35.6 32.7 36.5 13.3 31.2 37. 46.3 33.7 47.5 36.5 26.2 28. 34.8 32.1 51.8 37.9 37.9 40. 29.0 37.2 33.5 44.7 32.5 55. 36.6 33.0 27.7 34.7 29.5 54. 37.9 36.8 36.8 31.3 41.5 41. 33.1 54.9 36.2 36.9 29.0 33. 28.3 54.5 37.7 35.8 36.3 45. 37.5 38.9 46.1 49.4 35.0 50. 38.4 32.3 33.8 37.0 35.3 54. 41.0 43.1 44.9 34.7 48.0 50. 40.2 51.5 42.9 36.3 37.0 39. 38.4 54.5 43.8 43.4 46.3 38. 41.2 43.6 61.2 47.7 37.2 58. 40.6 37.7 31.6 35.9 30.6 57. 43.4 37.7 44.3 48.6 43.8 58. 54.7 47.2 56.8 49.2 49.5 49. 46.4 45.9 60.3 49.2 52.0 58. 45.5 51.6 MMMLU Results We also highlight the results on the recently released multilingual version of MMLU from OpenAI , which are shown in Table 9. The MMMLU dataset evaluates language models across diverse set of languages and tasks. This benchmark is crucial for assessing the multilingual capabilities of language models, particularly their ability to process and understand languages beyond https://huggingface.co/datasets/openai/MMMLU 25 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement English. In the 7B LLMs, Marco-7B consistently outperforms baseline models across wide range of languages. It achieves the highest scores in languages such as Arabic (60.6), Bengali (54.4), German (65.9), and Spanish (67.7), demonstrating significant advantage over other models. This performance is particularly noteworthy in Bengali, where Marco-7B surpasses the second best model (Qwen2.5-7B) by substantial margin of 4.1, highlighting its ability to handle languages with less training data effectively. The models strong results in languages like Hindi (+7.7 compared to the second best model) and Korean further emphasize its robust multilingual capabilities, making it versatile tool for diverse linguistic contexts. In the 70B LLMs, Marco-72B achieves the highest average score across all languages, showcasing its superior multilingual understanding. It demonstrated competitive performance in high-resource languages such as German (80.7), Spanish (82.6), and Chinese (82.0), while also delivering strong performance in lower-resource languages like Swahili (63.7) and Yoruba (44.0). These results underscore the models extensive language processing abilities, positioning it as leading solution for multilingual applications. It is worth noting that our Marco-72B outperformed GPT-4 (for 7B LLMs we compare with GPT-4o-mini and for 70B LLMs we compare GPT-4) [Hurst et al., 2024] in many languages. truction (existing preference dataset) Belebele Results The results on the Belebele [Bandarkar et al., 2024], presented in Table 10, illustrate the strong multilingual capabilities of the Marco-Chat models across both 7B and 70B parameter scales. For the 7B models, Marco-Chat consistently achieves the highest scores across most languages, with an average score of 79.3, significantly outperforming other models such as Qwen2 (69.4) and Qwen2.5 (70.0). This performance is particularly impressive in low-resource languages like Kazakh and Nepali, where Marco-Chat scores 73.1 and 70.0, respectively, showcasing improvements of 25.1 and 25.7 points over Qwen2. These substantial gains underscore the success of our continual pretraining approach, which effectively leverages vast multilingual corpus to enhance generalization capabilities across languages with limited resources. Additionally, Marco-Chat remains highly competitive in highresource languages such as Italian (86.8) and Japanese (83.1), further demonstrating its versatility and robustness. The 70B models exhibit similar patterns, with Marco-Chat achieving an average score of 89.6, leading the performance across nearly all languages. Notable improvements are observed in Bengali (89.2) and Indonesian (93.1), surpassing Qwen2.5 by 1.9 and 1.4 points, respectively. The models ability to handle complex linguistic tasks is further exemplified in high-resource languages such as Dutch (94.4) and Russian (92.7), where it achieves top scores. These results highlight the strategic advantage of our approach, which effectively captures linguistic nuances and complexities, benefiting from extensive multilingual pretraining. Overall, the results on the Belebele benchmark validate our focus on enhancing multilingual capabilities. English-pivot Translation Results The English-pivot translation results on Flores benchmark [Goyal et al., 2021, Team et al., 2022], as detailed in Table 11, highlight the strengths of the Marco-Chat LLMs in translation tasks across variety of languages. This benchmark is designed to evaluate translation quality from English to multiple target languages (ENXX) and vice versa (XXEN), offering insights into the multilingual capabilities of different models. In the ENXX translation tasks, the Marco-72B model achieves an average score of 43.8, surpassing the second-best model, Google Translate, by margin of 0.3 points. Notably, Marco-72B shows strong performance in translating English into Arabic (En2Ar) with BLEU score of 61.2, outperforming Google by 11.2 points, and in Portuguese (En2Pt) with BLEU score of 57.9, leading by 1.9 points. These results highlight the models ability to handle both high-resource languages, such as French 26 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Table 12 Any2Any translation performance on Flores benchmark across various language pairs for LLMs. The table compares results from the Instruct models of Qwen2, Qwen2.5, Llama3, Llama3.1, and Marco-LLM, with the best performance highlighted in bold for each translation direction. Trans. Dir. Qwen2-7B Qwen2.5-7B Llama3-8B Llama3.1-8B Aya-expanse-8B Aya-23-8B Marco-7B Ar2Ja Es2Ja Fr2Ja Hu2Ja Hu2Ko Ja2Ar Ja2Es Ja2Ko Ja2Th Ja2Zh Kk2Ar Kk2Fr Kk2Ja Kk2Ko Kk2Pt Kk2Th Kk2Zh Ko2Ja Ko2Th Ko2Zh Th2Ar Th2Es Th2Fr Th2Ja Th2Kk Th2Ko Th2Zh Tr2Ja Uk2Fr Uk2Ja Uk2Kk Uk2Ko Uk2Th Uk2Zh Ur2Ar Ur2Ko Zh2Ar Zh2Fr Zh2Ja Zh2Ko Zh2Pt Zh2Th Avg. Score 16.2 17.2 19.9 15.2 10.5 9.8 16.1 16.3 11.7 19.5 7.3 13.8 11.7 8.3 12.7 6.7 11.9 22.4 11.9 20.0 11.3 16.4 22.2 16.5 2.2 12.2 18.8 17.7 27.5 17.3 3.4 13.2 12.4 20.7 8.0 8.7 11.9 24.5 19.2 14.2 22.6 13.3 14.6 14.3 10.7 14.0 9.6 6.9 7.4 15.0 12.1 11.4 17.6 5.5 8.9 6.0 4.7 8.8 7.1 10.8 21.2 9.7 20.3 9.1 15.1 20.3 15.4 1.7 9.4 18.0 7.3 24.3 13.0 2.7 8.9 11.2 18.4 7.4 6.8 9.8 21.7 15.0 10.4 20.9 10.8 11.9 13.1 11.2 14.1 12.3 9.4 8.6 15.9 12.2 11.1 6.9 7.1 17.9 10.4 9.6 15.2 8.9 10.2 17.5 11.2 10.2 8.7 16.4 19.3 12.6 4.4 8.3 9.5 11.9 31.2 14.1 7.9 10.7 13.7 11.6 5.2 8.5 10.6 21.8 10.9 9.7 19.5 12.8 12. 17.5 18.1 21.6 16.0 10.2 11.1 16.7 17.3 11.6 10.2 8.6 14.1 12.2 9.3 10.2 10.4 13.1 22.3 12.2 16.2 5.5 9.1 12.4 14.5 5.4 7.6 9.2 20.0 33.0 20.1 8.3 15.8 15.2 9.8 4.8 9.3 13.5 25.7 18.4 14.8 20.5 13.9 13.9 16.9 18.3 17.6 10.7 9.2 12.3 12.9 18.7 0.8 14.9 2.3 5.6 4.1 4.5 4.2 0.4 3.0 23.9 0.8 16.6 1.7 1.5 5.1 0.0 0.3 0.8 0.0 19.6 31.5 16.9 0.7 16.0 1.1 16.1 3.4 4.5 14.9 24.5 14.4 15.7 21.0 1.0 9.7 17.1 19.9 22.3 12.6 11.0 9.6 16.2 17.2 2.0 12.7 5.7 10.6 9.6 7.5 9.7 1.2 7.6 19.3 2.0 14.7 6.8 10.0 14.2 5.5 0.8 5.1 6.4 21.4 33.0 21.8 1.8 17.3 2.9 17.8 6.4 9.0 13.6 21.3 17.3 15.4 21.5 2.5 11. 21.8 22.4 25.8 20.3 14.0 15.5 19.1 22.6 16.4 22.9 11.6 20.5 16.8 13.1 16.9 12.7 18.5 26.2 14.7 22.7 15.4 18.6 25.2 22.7 9.3 16.5 22.3 23.2 34.6 26.6 12.4 18.9 19.9 25.6 10.7 12.8 17.0 28.9 27.7 21.2 25.4 19.8 19.7 (En2Fr, 58.8), and low-resource languages, such as Ukrainian (En2Uk, 44.3), demonstrating its strong capability and robustness. Besides, for the language pairs where Marco-LLM underperformed competitive commercial MT systems such as En2De, En2Ja and En2Tr, it still achieved the best translation performance compared to the other open-sourced LLMs. For XXEN translations, Marco-72B achieves an impressive average score of 51.6, leading the performance with significant margin of 8.0 points over the second-best model, Google. The model shows outstanding performance in translating from Italian (It2En, 49.2) and Korean (Ko2En, 49.0), reflecting its advanced capacity to capture nuanced linguistic features and deliver high-quality translations. The consistent outperformance 27 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 7 Accuracy trends across training checkpoints for different languages on MMMLU. The model shows rapid initial learning (0-230 steps) followed by performance stabilization. High-resource languages (ZH-CN, IT-IT) consistently outperform low-resource ones (YO-NG), with persistent performance gap of 29%. across both high-resource languages (e.g., French to English, 56.8) and low-resource languages (e.g., Ukrainian to English, 58.8) underscores the effectiveness of our multilingual approach. The results on the Flores benchmark [Team et al., 2022] validate the effectiveness of focusing on enhancing multilingual capabilities. Non-English-pivot Translation Results The Non-English-pivot translation results (Any2Any translation - translation from any languages to any languages) the Flores benchmark, as shown in Table ??, highlight the superior performance of the Marco-7B model in Any2Any translation tasks. The Marco-7B model achieves the highest average score of 19.5, which is substantial margin above the second-best model, Qwen2-7B, with an average score of 14.4. This represents notable improvement of 5.1 points. In some specific language directions, Marco-7B exhibits remarkable strong performance. For example, Marco-7B outperformed Qwen2-7B in Chinese to Japanese (Zh2Ja) translation by 8.5 points. Similarly, in the Arabic to Japanese (Ar2Ja) translation, Marco-7B achieves score of 21.8, outperforming Llama3.1-8B by 4.3 points. Moreover, in the French to Japanese (Fr2Ja) translation, Marco-7B scores 25.8, which is 4.2 points higher than Llama3.1-8B. These results underscore the models capability to manage complex linguistic structures, particularly in non-English-pivot language pairs. The Marco-7B models superior performance is evident across both high-resource and low-resource languages, demonstrating its versatility and robustness. This is particularly important for enhancing the multilingual/cross-lingual capabilities of large language models, as it ensures consistent translation quality across wide range of languages beyond traditional English-centric translation [Fan et al., 2020]. Performance Across Different Training Steps Our analysis of the performance of Marco-7B model across different training steps (0-1150) on the MMMLU benchmark is shown in Figure 7. The most significant improvements occur during the early training phase (0-230 steps), where the overall 28 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement average accuracy increases dramatically from 40.81% to 59.29%. After 460 step, the performance stabilizes across most languages, with the final overall accuracy reaching 60.05%. High-resource languages like Chinese (ZH-CN) and Italian (IT-IT) achieve and maintain higher accuracy levels (>65%) compared to low-resource languages, with Yoruba (YO-NG) plateauing around 37%. This persistent performance gap of approximately 29% between high and low-resource languages suggests that while the model effectively captures multilingual knowledge early in training, achieving better performance across languages remains challenge that may require more monolingual data during the pretraining phase rather than simply extending training steps for SFT. 4.2. Multilingual Preference Alignment Preference alignment is critical for ensuring that an LLMs outputs are consistent with human expectations and values. However, most LLMs are predominantly aligned on preference in English data, leading to disparity in performance when applied to other languages. In multilingual setting, this alignment becomes even more essential due to the variations in language structures [She et al., 2024], idiomatic expressions, and cultural references. By focusing on multilingual preference alignment, we aim to enhance Marcos ability to generate responses that are not only grammatically correct but also culturally appropriate and contextually relevant in multiple languages. Moreover, multilingual preference alignment helps mitigate biases that may arise from training on datasets that lack linguistic diversity. It promotes fairness and inclusivity, enabling the model to cater to broader user base. By aligning the models preferences across different languages, we ensure that Marco-LLM can effectively understand and respond to users worldwide, fostering better communication and understanding across linguistic boundaries. 4.2.1. Dataset Construction from Existing Preference Data To construct comprehensive multilingual preference dataset, we began with the LMSYS Arena Human Preference dataset [Chiang et al., 2024] . This dataset comprises 57.5k high-quality human preference annotations for various prompts and responses in English. We selected subset of highquality examples based on criteria such as clarity, relevance, and diversity of topics to ensure robust foundation for multilingual preference alignment. The selected examples were then translated into the 28 target languages. This translation step was critical to extend the language coverage of the original data. By leveraging existing English preference data and extending it to multiple languages, we aim to improve the performance of preference alignment of Marco-LLM under various languages beyond English. 4.2.2. Multilingual Preference Data Generation and Translation In addition to the translated data, we expanded our preference dataset by incorporating prompts from the UltraFeedback dataset [Cui et al., 2023], which are also translated into 28 languages. For each prompt, we utilized Marco-LLM to generate at least two distinct responses with different generation configuration. This approach allowed us to capture the models inherent variability in generating responses across different languages. To establish preferences between the generated responses, we employed another LLM to evaluate and select the better response based on predefined criteria such as relevance, coherence, and adherence to the prompt. This process effectively created set of preference pairs that reflect the models capabilities and the desired outcomes in various languages. By generating and evaluating responses within the target languages, we ensured that the preference data was culturally and linguistically appropriate. https://huggingface.co/datasets/lmsys/lmsys-arena-human-preference-55k 29 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement Figure 8 Performance comparison of Marco-LLM against baseline models across 28 languages on multilingual MT-bench. Each subplot shows the win rate (blue), loss rate (green), and tie rate (red) for specific language. Win rates indicate Marco-LLMs superior responses, loss rates represent baseline models better performance, and tie rates show equivalent quality responses. 4.2.3. Evaluation Results To evaluate Marcos multilingual capabilities for capturing preference in different languages, we translated the original English MT-Bench benchmark [Chiang et al., 2024] into the 28 target languages. We then compare the generated responses from LLMs in pairwise manner using GPT-4o-mini, specifically we compare the responses of Marco-LLM (7B) with the responses from the other six baseline LLMs including Qwen2, Qwen2.5, Llama3, Llama3.1, Aya-23, Aya-expanse (all in 7/8B size). The results from the multilingual MT-bench, as illustrated in Figure 8, reveal that Marco-chat-7B model outperforms baseline models in 25 out of 28 languages. We evaluate model responses using GPT-4o-mini where win rates, loss rates, and tie rates (Marco-LLM vs baseline) were averaged for the baseline models mentioned in Section 4.1.4 across each language. Marco-chat-7B achieved better generation quality in low-resource languages. For instance, in Azerbaijani (az), the model achieves win rate of 50.52% compared to loss rate of 25%, while in Bengali (bn), the win rate is 51.56% against loss rate of 23.44% as well as Kazakh(he) where Marco-LLM obtained win rate of 66.04% . These results indicate clear advantage over the baseline models. Hebrew (he) also demonstrates strong performance with win rate of 41.56%, surpassing the loss rate of 31.56%. In certain high-resource languages, Marco-chat-7B maintains competitive performance. For example, in French (fr), our model achieves win rate of 35.98% against loss rate of 29.27%, and in Chinese (zh), it achieves win rate of 37.29% compared to loss rate of 27.40%. These results reflect the models effective handling of languages with extensive linguistic data. The model also performs consistently well across various language families, maintaining win rates around 35% for Indo-European languages such as Italian (it) with win rate of 34.17%, and Dutch (nl) with win rate of 37.81%. These results suggest balanced performance across diverse linguistic structures. While Marco-LLM achieved strong performance in 25 languages out of 28 languages, it still underperformed in languages including Czech, Greek and Indonesian. This suggests the need for further refinement in handling complex grammatical structures. Overall, the experimental results highlight Marco-chat-7Bs strong multilingual performance, particularly in languages where it achieves higher win rate than loss rate. The model effectively addresses the challenges posed by both high-resource and low-resource 30 Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement languages, demonstrating its capability and potential for deployment in wide range of linguistic environments. 5. Conclusion and Future Work In this paper, we introduced Marco-LLM, multilingual LLM specifically designed to address the challenges posed by low-resource languages. By leveraging large and diverse multilingual dataset, we conducted extensive multilingual continual pre-training and post-training, including supervised finetuning and preference alignment, based on the Qwen2 model. Our comprehensive evaluations on benchmarks such as MMMLU, Flores, Belebele, CEVAL, TydiQA, and multilingual MT-bench validated that Marco-LLM obtained excellent performance in multilingual tasks. The results demonstrate that focusing on low-resource languages can bridge existing performance gaps and extend the benefits of LLMs to wider range of linguistic communities. Our work highlights the importance of data diversity and targeted training strategies in enhancing model performance across diverse languages. For future work, there are several directions for future research. One promising direction is to extend Marco-LLMs capabilities to include more languages, further enriching the linguistic diversity it can handle. Additionally, exploring the integration of multilingual reasoning capabilities could enhance the models ability to understand and generate more complex language structures. Furthermore, improving model efficiency and scalability will be essential for deploying these systems in real-world applications, particularly in resource-constrained environments.",
                "summary": "<p>–í —Ç–∞–±–ª–∏—Ü–µ 8 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –æ—Å–Ω–æ–≤–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º: MMMLU, TydiQA, AGIEval, CEval –∏ Belebele. –≠—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ —è–∑—ã–∫–∞—Ö, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—è –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é.</p>\n<p>–ú–æ–¥–µ–ª—å Marco-Chat-7B –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–∞–∑–º–µ—Ä–æ–º –≤ 7 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –Ω–∞ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–∞—Ö CEval –∏ Belebele, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–∞—Ö –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –ù–∞ CEval –º–æ–¥–µ–ª—å Marco-Chat-7B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 86.4, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤—ã—à–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Äî Qwen2-7B (81.8). –≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å–∏–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –Ω–∞ Belebele, –≥–¥–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –≤–ª–∞–¥–µ–Ω–∏–µ –º–∞–ª–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –∞—Ñ—Ä–∏–∫–∞–Ω—Å–∫–∏–º–∏ —è–∑—ã–∫–∞–º–∏, Marco-Chat-7B –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç 79.3, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—á—Ç–∏ –Ω–∞ 9 –ø—É–Ω–∫—Ç–æ–≤.</p>\n<p>–°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º 70 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤, –º–æ–¥–µ–ª—å Marco-72B —Ç–∞–∫–∂–µ –ª–∏–¥–∏—Ä—É–µ—Ç –≤–æ –≤—Å–µ—Ö —Ç–µ—Å—Ç–∞—Ö. –û–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ü–µ–Ω–∫—É 76.1 –Ω–∞ MMMLU, –∫–æ—Ç–æ—Ä–∞—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ —è–∑—ã–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö. –≠—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 4.4 –ø—É–Ω–∫—Ç–∞ –ª—É—á—à–µ, —á–µ–º —É —Å–ª–µ–¥—É—é—â–µ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏, Llama3.1-70B. –ù–∞ TydiQA, —Ç–µ—Å—Ç–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö, Marco-72B –Ω–∞–±–∏—Ä–∞–µ—Ç 61.0, —á—Ç–æ –Ω–∞ 7.9 –ø—É–Ω–∫—Ç–æ–≤ –±–æ–ª—å—à–µ, —á–µ–º —É –≤—Ç–æ—Ä–æ–π –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏. –≠—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–∏–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ —è–∑—ã–∫–æ–≤ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞–º–∏ –∏ –ø–∏—Å—å–º–µ–Ω–Ω–æ—Å—Ç—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, Marco-72B –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –≤–ø–µ—á–∞—Ç–ª—è—é—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 94.5 –Ω–∞ CEval, –ø–æ–∫–∞–∑—ã–≤–∞—è –æ—Ç–ª–∏—á–Ω—É—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –∫–∏—Ç–∞–π—Å–∫–æ–º —è–∑—ã–∫–µ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞–º. –ù–∞ Belebele –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ 89.6, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —è–∑—ã–∫–∞—Ö –ê—Ñ—Ä–∏–∫–∏.</p>"
            }
        ]
    },
    {
        "id": "2412.04363",
        "title": "Challenges in Trustworthy Human Evaluation of Chatbots",
        "url": "https://huggingface.co/papers/2412.04363",
        "abstract": "Open community-driven platforms like Chatbot Arena that collect user\npreference data from site visitors have gained a reputation as one of the most\ntrustworthy publicly available benchmarks for LLM performance. While now\nstandard, it is tricky to implement effective guardrails to collect\nhigh-quality annotations from humans. In this paper, we demonstrate that three\nsources of bad annotations, both malicious and otherwise, can corrupt the\nreliability of open leaderboard rankings. In particular, we show that only 10\\%\nof poor quality votes by apathetic (site visitors not appropriately\nincentivized to give correct votes) or adversarial (bad actors seeking to\ninflate the ranking of a target model) annotators can change the rankings of\nmodels by up to 5 places on the leaderboard. Finally, we discuss open\nchallenges in ensuring high-quality human annotations.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-05",
        "pub_date_card": {
            "ru": "5 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 5",
            "zh": "12Êúà5Êó•"
        },
        "hash": "4fa16cf75a2af540",
        "authors": [
            "Wenting Zhao",
            "Alexander M. Rush",
            "Tanya Goyal"
        ],
        "affiliations": [],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.04363.jpg",
        "data": {
            "categories": [
                "#rlhf",
                "#data",
                "#security",
                "#benchmark",
                "#ethics"
            ],
            "emoji": "üé≠",
            "ru": {
                "title": "–û—Å—Ç–æ—Ä–æ–∂–Ω–æ: –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–≥—É—Ç –∏—Å–∫–∞–∑–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –ò–ò!",
                "desc": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç, —á—Ç–æ –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏—Å–∫–∞–∂–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–ª–æ—Ö–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏ –∏ –¥—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã. –°—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π."
            },
            "en": {
                "title": "Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings",
                "desc": "This paper examines the reliability of user-generated annotations on community-driven platforms like Chatbot Arena, which are used to evaluate the performance of large language models (LLMs). It identifies three main sources of poor-quality annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other malicious behaviors. The authors demonstrate that even a small percentage of these low-quality votes can significantly impact the rankings of models, potentially shifting them by several places. The paper concludes by highlighting the ongoing challenges in maintaining high-quality human annotations to ensure trustworthy leaderboard results."
            },
            "zh": {
                "title": "Á°Æ‰øùÈ´òË¥®ÈáèÊ≥®ÈáäÔºåÊèêÂçáÊ®°ÂûãÊéíÂêçÁöÑÂèØÈù†ÊÄß",
                "desc": "Êú¨ÊñáÊé¢ËÆ®‰∫ÜÂºÄÊîæÁ§æÂå∫Âπ≥Âè∞ÔºàÂ¶ÇChatbot ArenaÔºâÂú®Êî∂ÈõÜÁî®Êà∑ÂÅèÂ•ΩÊï∞ÊçÆÊó∂Èù¢‰∏¥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ÂèëÁé∞ÔºåÊù•Ëá™Êó†Âä®Êú∫ÊàñÊÅ∂ÊÑèÁî®Êà∑ÁöÑ‰ΩéË¥®ÈáèÊ≥®Èáä‰ºö‰∏•ÈáçÂΩ±ÂìçÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑÊéíÂêç„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ªÖ10%ÁöÑ‰ΩéË¥®ÈáèÊäïÁ•®Â∞±ÂèØËÉΩÂØºËá¥Ê®°ÂûãÂú®ÊéíË°åÊ¶ú‰∏äÊéíÂêçÂèòÂåñÂ§öËææ5‰Ωç„ÄÇÊúÄÂêéÔºåÊñáÁ´†ËÆ®ËÆ∫‰∫ÜÁ°Æ‰øùÈ´òË¥®Èáè‰∫∫Á±ªÊ≥®ÈáäÁöÑÂºÄÊîæÊÄßÊåëÊàò„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.",
                "summary": "<p>–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å –æ—Ç–∫—Ä—ã—Ç—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π —Å–∞–π—Ç–∞, –∑–∞–≤–æ–µ–≤–∞–ª–∏ —Ä–µ–ø—É—Ç–∞—Ü–∏—é –æ–¥–Ω–æ–π –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ LLM. –•–æ—Ç—è —Å–µ–π—á–∞—Å —ç—Ç–æ —Å—Ç–∞–ª–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º, —Å–ª–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –æ—Ç –ª—é–¥–µ–π. –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –º—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º, —á—Ç–æ —Ç—Ä–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, –∫–∞–∫ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –¥—Ä—É–≥–∏—Ö, –º–æ–≥—É—Ç –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö —Å–ø–∏—Å–∫–æ–≤ –ª–∏–¥–µ—Ä–æ–≤. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö (–ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏ —Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã –¥–∞–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞) –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö (–∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫–∏, —Å—Ç—Ä–µ–º—è—â–∏–µ—Å—è –∫ —Ä–∞–∑–¥—É–≤–∞–Ω–∏—é —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏) –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –¥–æ 5 –º–µ—Å—Ç –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ù–∞–∫–æ–Ω–µ—Ü, –º—ã –æ–±—Å—É–∂–¥–∞–µ–º –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π.</p>"
            },
            {
                "title": "Introduction",
                "content": "Reliable evaluation of free-form text generation quality is long-standing challenge in NLP (Gehrmann et al., 2023; Celikyilmaz et al., 2020; Goyal et al., 2022a). Despite limitations, human annotation is widely accepted as the gold standard, especially for open-ended text generation tasks without an objective notion of correctness. As result, platforms such as Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024b) and WildVision Arena (Lu et al., 2024) that allow users to interact with available large language models (LLMs) and submit preference judgments for model pairs, have become extremely valuable resource in the NLP evaluation landscape. By providing free and easy access to available LLMs, these community-driven platforms are able to incentivize millions of user interactions1 and collect large-scale and diverse dataset of user queries and preferences. Deservedly, 1As of October 6, 2024, Chatbot Arena has collected 2,011,939 pairwise preference judgments. these peer production and community-driven platforms have emerged as one of the most trusted benchmarks in NLP today.2 Moreover, such benchmarks play crucial role in auditing automatic evaluators by providing the necessary ground truth rankings that any evaluator can be validated against. In fact, the most popular automatic evaluation benchmarks today, including AlpacaEval (Li et al., 2023), WildBench (Lin et al., 2024), MixEval (Ni et al., 2024) and Arena-Hard (Li et al., 2024), validate their metric by reporting high correlation with Chatbot Arena judgments. Given its far-reaching impact, both on human and automatic benchmarking of LLMs, and consequently on LLM research more broadly, it is crucial to ensure that the model rankings on these open community leaderboards are trustworthy. However, challenges with obtaining high-quality human judgments from non-expert crowdworkers like Chatbot Arena users are widely discussed in literature (Karpinska et al., 2021; Clark et al., 2021; Hosking et al., 2024). Moreover, these platforms typically implement minimal quality controls for verifying annotation quality such as attention checks, user verification, etc. This sits in direct opposition to the goals of trustworthiness. In this paper, we play devils advocate and ask: is it even possible to ensure the reliability of community-driven open platform, like Chatbot Arena, without sacrificing user scale? We approach this thought experiment from two angles. First, using Chatbot Arena as case study, we consider three different sources of poor quality preference judgments or votes in the collected dataset: un-incentivized or apathetic users providing random judgments (Section 3.1), malicious actors launching adversarial attacks to detect and artificially inflate target models ranking (Sec2As an example, Googles Chief Scientist used high performance on Chatbot Arena to declare the success of their recent model release: https://tinyurl.com/55xs2pz4. trol measures employed by Chatbot Arena. Notation Assume there are different models = {m1, m2, ..., mk} that need to be ranked on the leaderboard. Each new user on the platform submits query and receives outputs from two different models yi mi(x) and yj mj(x).3 The user has the option to submit preference label {i, j, tie}. In order to ensure that this annotation is unbiased, the names of the models that the two outputs are sampled from is only revealed to end users after they have submitted their preference annotation. This arena logs data points of the form: (x, yi, yj, mi, mj, l). These preferences are then used to estimate the pairwise win matrix between model pairs, i.e. p(mi > mj). Next, they estimate the coefficients of the Bradley-Terry model (Bradley and Terry, 1952) to obtain scores si for each model mi M. Models are sorted by si to obtain the final ranking. Quality control measures The arena employs list of filtering strategies: detecting malicious users according to certain distribution (Section 5.1; Chiang et al. (2024a)), bot detection by Cloudflare and Google reCAPTCHA v3, automatic categorization pipelines to filter out low-quality data45, placing limits on the number of votes each IP can provide in day, and deduplicating top 0.1% occurring prompts. However, these filtering strategies focus more on filtering bots than differentiating user votes with varying qualities. Therefore, we present results and discussions in this paper assuming minimal quality control checks in the backend to filter out bad quality user annotations6. Released Artifacts We conduct our experiments using the largest publicly released dataset by Chatbot Arena. It consists of 55k preference annotations7; it includes response pairs sampled from two of 64 unique models and the corresponding pairwise preference annotation. 3The arena employs an adaptive sampling strategy that favors model pairs with higher uncertainty in relative performance, and also newly introduced models. However, exact details are not publicly shared, possibly to mitigate gaming. 4https://blog.lmarena.ai/blog/2024/ hard-prompts/ 5https://blog.lmarena.ai/blog/2024/ arena-category/ 6https://github.com/lm-sys/FastChat/ 7https://huggingface.co/datasets/lmsys/ lmsys-arena-human-preference-55k Figure 1: Our characterization of sources of poor-quality votes on open data annotation platforms: (1) Apathetic: Users who lack intrinsic motivation may submit random votes. (2) Adversarial: Malicious users aim to manipulate rankings by upvoting target model. (3) Arbitrary: Users voting based on subjective preferences in response to open-ended questions. tion 3.2), and the inherent arbitrariness of preference votes for open-ended and subjective queries (Section 3.3). For the former two sources of votes, we show that small fractions of poor-quality judgments (either apathetic or adversarial) can have non-trivial impact on the target models rankings. Concerningly, poor annotations from either apathetic or adversarial voting are not easy to detect in post-hoc manner. Moreover, even carefully recruited and onboarded human annotators exhibit low inter-annotator agreement on subjective queries, making inter-annotator-based techniques to filter out low-quality annotations ineffective. Finally, we discuss open challenges in ensuring the reliability and human annotation quality in open-source community-driven benchmarks (Section 4). We strongly believe that open data collection platforms offer an invaluable resource for the academic community and have facilitated essential work in developing new automatic evaluators (Li et al., 2023; Lin et al., 2024; Ni et al., 2024), training and evaluating reward models (Lambert et al., 2024), etc. However, critical questions exist about their reliability, especially against adversarial attacks. We hope that our work will spur future research on quality control mechanisms for open platforms that power LLM evaluations.",
                "summary": "<p>–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è –¥–∞–≤–Ω–µ–π –ø—Ä–æ–±–ª–µ–º–æ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –∑–æ–ª–æ—Ç—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–Ω—è—Ç–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Chatbot Arena –∏ WildVision Arena, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM) –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π, —Å—Ç–∞–ª–∏ —Ü–µ–Ω–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ NLP. –≠—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–≤–æ–±–æ–¥–Ω—ã–π –∏ –ª–µ–≥–∫–∏–π –¥–æ—Å—Ç—É–ø –∫ –¥–æ—Å—Ç—É–ø–Ω—ã–º –º–æ–¥–µ–ª—è–º, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–±–∏—Ä–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –¢–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å–µ–≥–æ–¥–Ω—è —è–≤–ª—è—é—Ç—Å—è –æ–¥–Ω–∏–º–∏ –∏–∑ —Å–∞–º—ã—Ö –Ω–∞–¥–µ–∂–Ω—ã—Ö —ç—Ç–∞–ª–æ–Ω–æ–≤ –≤ NLP –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–∏–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω—â–∏–∫–æ–≤ –ø—É—Ç–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—ã—Ö –æ—Ü–µ–Ω–æ–∫. –û–¥–Ω–∞–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ –æ—Ç –Ω–µ–∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ Chatbot Arena. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –æ–±—ã—á–Ω–æ —Ä–µ–∞–ª–∏–∑—É—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ä—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —á—Ç–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç —Ü–µ–ª—è–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ç–∞–∫–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –∫–∞–∫ Chatbot Arena, –±–µ–∑ —É—â–µ—Ä–±–∞ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.</p>"
            },
            {
                "title": "Background",
                "content": "In this paper, we run experiments with Chatbot Arena (Zheng et al., 2023; Chiang et al., 2024a) as case study, although our insights are broadly applicable to other similar community-driven preference collection platforms. Below, we describe the preference collection pipeline and quality conModel Leaderboard Ranking Orig. r=1 r= r=10 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 36 21 39 382 201 412 382 21 345 41 existing studies characterizing the incentives or behaviors of an average user on open platforms like Chatbot Arena. Therefore, we have no way of estimating the fraction of apathetic. Table 1: Change in leaderboard rankings for 3 test models based on different percentages (r) of arbitrary votes. The subscripts denote gain () or loss () in rankings. We find that only 10% poor quality annotations can change the rank of 2/3 systems by 5 places.",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –º—ã –ø—Ä–æ–≤–æ–¥–∏–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –ø–ª–∞—Ç—Ñ–æ—Ä–º–æ–π –¥–ª—è —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π Chatbot Arena –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–º–µ—Ä–∞, —Ö–æ—Ç—è –Ω–∞—à–∏ –≤—ã–≤–æ–¥—ã –ø—Ä–∏–º–µ–Ω–∏–º—ã –∏ –∫ –¥—Ä—É–≥–∏–º –ø–æ–¥–æ–±–Ω—ã–º –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º, —É–ø—Ä–∞–≤–ª—è–µ–º—ã–º —Å–æ–æ–±—â–µ—Å—Ç–≤–æ–º. –ù–∏–∂–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏ –º–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞. –ú—ã –æ—Ç–º–µ—á–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–∏—Ö —Å—Ç–∏–º—É–ª—ã –∏–ª–∏ –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ Chatbot Arena. –ü–æ—ç—Ç–æ–º—É —É –Ω–∞—Å –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∏—Ç—å –¥–æ–ª—é –±–µ–∑—Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –¢–∞–±–ª–∏—Ü–µ 1 –ø–æ–∫–∞–∑–∞–Ω–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ –¥–ª—è —Ç—Ä–µ—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (r) –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ü–æ–¥—Å—Ç—Ä–æ—á–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –æ–±–æ–∑–Ω–∞—á–∞—é—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ (+) –∏–ª–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ (-) –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ. –ú—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ 10% –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ 2/3 —Å–∏—Å—Ç–µ–º –Ω–∞ 5 –ø–æ–∑–∏—Ü–∏–π.</p>"
            },
            {
                "title": "Votes and Their Impact",
                "content": "For our thought experiment, we hypothesize that there exist three potential sources of poor quality votes on open platforms: (a) apathetic votes by users that are un-incentivized, (b) adversarial votes that aim to inflate the ranking of target model, and (c) arbitrary votes on difficult to meaningfully distinguish response pairs. For each of these, we study their impact on model rankings and the challenges in mitigating them. 3.1 Apathetic Voting The main attraction of open community platforms for end users is that they expose free and easy-touse API endpoint for LLMs. This incentivizes diverse users to interact with the platform and submit queries to explore their use cases. However, these platforms do not explicitly incentivize high-quality preference annotation. We hypothesize that at least r% of users on the arena are apathetic and provide random or low-quality votes on the platform. Setup We run experiments on Chatbot Arenas dataset of 55k preferences (discussed in Section 2). We assume that this dataset reflects true rankings of models based on gold human preferences. We study the change in model rankings for 3 arbitrarily selected models: Llama-2-7b-chat, Mistral-7binstruct-v0.2, and Llama-2-13b-chat, assuming r% of these preferences were instead assigned random labels by apathetic users during data collection. Results Table 1 summarizes our results. We find that only 10% of apathetic votes in the dataset can change the leaderboard rankings of 2/3 models by 5 places (namely Llama-2-13b-chat and Mistral-7b-instruct-v0.2).8 Note that there are no 8Note that model frequency also impacts its susceptibility to ranking changes. All three models we inspect collectively occur in less than 10% of all data samples. Discussion: Can we detect and remove apathetic votes? major challenge in detecting apathetic votes is that they are often indistinguishable from arbitrary votes. Multiple past studies have found that output-level comparisons using single label is ill-defined as an annotation task (Krishna et al., 2023; Goyal et al., 2022a) as users often rely on different criteria and disagree with each other. This ambiguity makes it hard to ascertain whether observed disagreements are due to personal variations in quality assessment (arbitrary voting, discussed further in Section 3.3) or due to apathetic or low-quality annotations by certain annotators. Despite challenges with detecting individual apathetic votes, detecting apathetic users may be viable by computing agreements between model rankings by individual users. This strategy is based on the intuition that while annotators might disagree on specific examples, their aggregate systemlevel judgments tend to be more aligned (Goyal et al., 2022a). Finally, requesting additional justifications for votes, such as free-text rationales, can also help discourage apathetic votes. We discuss this more in Section 4. 3.2 Adversarial Voting We assume there exists malicious developer who seeks to inflate the rankings of their own target model mT on the arena leaderboard A. We argue that due to the lack of quality controls (e.g. user verification, attention checks, etc.), it is straightforward to inject preference votes for mT using simple attack methodology. Our main component is target model attribution algorithm which, given query-output pair (q, y), predicts whether is sampled from the target model mT (q). Given such an algorithm, we can inflate the ranking of the target model mT using the following strategy: (1) Enter prompt on the arena, (2) Detect if any of the two shown outputs y1, y2 are sampled from mT , (3) If yes, vote for the target model mT , (4) Repeat. Target model attribution algorithm We assume that the attribution algorithm has access to the target model logits. This is reasonable assumption for our setting where model developer seeks to Model Leaderboard Ranking Orig. r=1 r=5 r=10 r=100 Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0.2 21 39 232 174 21 363 325 289 342 342 297 121 139 234 Table 2: Change in leaderboard rankings for 3 test models based on different percentages (r) of adversarial votes (upvoting the target model). We find that only 10% adversarial annotations can change the rank of all systems by more than 4 places. Model TPR TNR #Tokens Llama-2-7b-chat Llama-2-13b-chat Mistral-7b-instruct-v0. 91.13 88.46 100.00 89.93 91.28 86.69 328.06 326.53 319.46 Table 3: Intrinsic eval. of model attribution algorithm inflate rankings. Our simple attribution algorithm is outlined in Algorithm 1 in Appendix A. Essentially, we use teacher-forcing to determine the probability distribution over the vocabulary for all tokens at time step t, i.e. PmT (.x, y1, ...yt1). We sort the tokens in descending order of probability to identify the smallest subset of tokens that cover cumulative probability mass of at least p. We compute the fraction of generation time steps for which the actual generated token yt falls within this top-p probability subset. We compare this against threshold to classify generations = y1...yN as being sampled from mT or not. Intrinsic Evaluation of Detector Algorithm For all three test models, we report the true positive rate (TPR) and true negative rate (TNR) on the arena dataset in Table 3. We find that our detector algorithm reports very high performance (e.g. TPR=91.13%, and TNR=88.46% for Llama-2-7bchat). We also find positive correlation between the number of tokens and TPRs, which can be leveraged in the attack. Note that malicious actors can always improve the detector accuracy further using watermarking techniques (Kirchenbauer et al., 2023). Next, we use these highly performant models to cast adversarial votes. Can we influence voting on the live Chatbot Arena platform? We also implement proofof-concept of real attack on Chatbot Arena to demonstrate that current guardrails, such as bot detection, can be bypassed easily. On October 13, 2024, we programmatically launched 100 queries into Chatbot Arena, extracted the two model responses, and successfully submitted preference vote. To avoid contaminating the dataset, we only cast tie votes but note that it would be trivial to instead use the vote from the attribution algorithm. Interestingly, post-hoc analysis of this data revealed that yi-lightning family of models, released just 2 days later, were the most common (20% of the responses) in this set.9 We assume that Chatbot Arena had early access to these models and sampled them more frequently than others in order to collect enough votes. However, this knowledge of when particular models will be up-sampled can be easily exploited by adversaries to log large fraction of upvotes for their model. Impact of adversarial voting on leaderboard rankings Similar to Section 3.1, we run experiments on the 55k preference dataset from Chatbot Arena, assumed to reflect \"true\" votes. For 3 target models, we report the change in leaderboard rankings if adversarial voting was conducted on r% of the data samples during data collection. Table 2 summarizes our results. Across all models, we show that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.10. Note that, in this work, we only report results using the most simplistic version of this attack. We can further boost these numbers by not only upvoting the target model but also downvoting open-source competitor models or those ranked higher than the target model in the leaderboard. Discussion: Can we detect and remove adversarial votes? Open platforms can employ two types of mitigation strategies to address this issue: recognizing bot-like behavior to prevent votes from being cast, or detecting abnormal users post-hoc to filter out their votes. Platforms like Chatbot Arena already implement measures from both categories. For example, Chatbot Arena uses Cloudflare and Google reCAPTCHA to detect bots on their platform; however, we were able to bypass both programmatically. We did not find public information indicating that similar measures have been incorporated into the Wildvision Arena platform. There are also opportunities to detect anomalous users post-hoc based on behaviors across multiple 9Evenly distributed between yi-lightning and yi-lightning-lite. 10We assume that adversaries can get 10% votes towards their own model because newly released models will be sampled more frequently. Th Org Re Per WS GPT-3.5 vs GPT-4o 5.51 Llama-3-8b vs Llama-3-70b 10.15 -10.78 Llama-3-8b vs GPT-3.5 Llama-3-70b vs GPT-4o 9.91 17.18 20.06 8.50 5.78 27.16 7.45 3.53 7.19 -12.15 -4.66 2.75 4.56 -1. 11.34 3.15 -0.36 Table 4: Fleiss Kappa between four annotators on different evaluation axis: Th(esis), Org(anization), Re(asoning), Per(spectives), WS (Writing Style). sessions or votes. Chatbot Arena implements version of this strategy by comparing the distribution of ratings from user (uniquely identified by IP address) against historical distributions to identify anomalies. Because committed adversaries may bypass these checks using IP rotation or similar techniques, we encourage further exploration of these approaches to make them more robust. 3.3 Arbitrary Voting We assume an idealized scenario where all users genuinely make their best effort to rank model outputs. However, we argue that holistically rating response to an open-ended and inherently subjective query is ill-defined and liable to always be arbitrary. To demonstrate this, we conduct small-scale annotation study for outputs of subjective Researchy questions prompts (Rosset et al., 2024). Setup We use these prompts and generate generate responses from four language models: Llama3-8B, Llama-3-70B, GPT-4o, and GPT-3.5. We recruit four undergraduate CS students who are passionate about NLP and committed to providing thoughtful annotations. They evaluate responses on four dimensions: thesis, organization, reasoning, perspectives, and writing style. We offer them unlimited time and allow them to seek clarification from the authors when needed. Note that this dimension-wise rating is different from Chatbot Arenas setup of pairwise preferences. However, there already exist multiple prior works that argue that the task is under-defined in this latter setting and report low agreement between annotators (Goyal et al., 2022a,b; Krishna et al., 2023). Therefore, we opt to run this study using more welldefined task description. Results Table 4 shows the inter-annotator agreement between the annotators. Overall, we find very 11Representative question: How can the education system be improved?. low agreement between these well-intentioned annotators with clear guidelines, irrespective of the performance difference between the model pairs. More concerningly, the results highlight that traditional approaches like filtering out low-quality users/annotations using inter-annotator agreement may not be viable strategy for open-ended queries as it is difficult to disentangle between of low interannotator agreement due to bad annotation (apathetic votes) or inherent subjectivity. Adversarial users can also hide their votes from similar scrutiny by using open-ended prompts for which vote choice is expected to be ambiguous. Discussion We argue that arbitrary votes are not noise and provide useful signals about models relative performance. If most frontier models perform similarly well on substantial fraction of real-world queries, this information should not be discarded but inform leaderboard Elo scores. Arbitrary votes become problematic when the majority of the leaderboard is dominated by openended queries that fail to meaningfully distinguish models, despite the existence of legitimate topics or skills along where models exhibit distinct behaviors. Identifying which test examples (or type of test examples) are most informative and up-weighting them when deriving aggregate scores are potential ways of addressing this (Rodriguez et al., 2021).",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≥–∏–ø–æ—Ç–µ–∑–∞ –æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–∏ —Ç—Ä–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö: –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –±–µ–∑ —Å—Ç–∏–º—É–ª–æ–≤, –≤—Ä–∞–∂–¥–µ–±–Ω—ã–µ –≥–æ–ª–æ—Å–∞, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–∞–≤—ã—à–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∞ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≥–æ–ª–æ—Å–∞ –≤ —Å–ª—É—á–∞–µ —Ç—Ä—É–¥–Ω–æ—Ä–∞–∑–ª–∏—á–∏–º—ã—Ö –ø–∞—Ä –æ—Ç–≤–µ—Ç–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑ —ç—Ç–∏—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å—Å–ª–µ–¥—É–µ—Ç—Å—è –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–π—Ç–∏–Ω–≥–∏ –º–æ–¥–µ–ª–µ–π –∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –≤ —Å–º—è–≥—á–µ–Ω–∏–∏ —ç—Ç–æ–≥–æ –≤–ª–∏—è–Ω–∏—è.</p>\n<p><strong>–ê–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –¥–ª—è –∫–æ–Ω–µ—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –æ–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∏ –ø—Ä–æ—Å—Ç–æ–π –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (LLM). –û–¥–Ω–∞–∫–æ —ç—Ç–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —è–≤–Ω–æ –Ω–µ —Å—Ç–∏–º—É–ª–∏—Ä—É—é—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –ø–æ –∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ r% –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –∞–ø–∞—Ç–∏—á–Ω—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∏–ª–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥–æ–ª–æ—Å–∞.</p>\n<p>–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö Chatbot Arena, —Å–æ–¥–µ—Ä–∂–∞—â–µ–º 55 —Ç—ã—Å—è—á –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –≤—Å–µ–≥–æ –ª–∏—à—å 10% –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∏–∑–º–µ–Ω–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥–æ–≤—É—é –ø–æ–∑–∏—Ü–∏—é –¥–≤—É—Ö –∏–∑ —Ç—Ä–µ—Ö –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Llama-2-13b-chat –∏ Mistral-7b-instruct-v0.2) –Ω–∞ –ø—è—Ç—å –º–µ—Å—Ç.</p>\n<p>–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã–µ –≥–æ–ª–æ—Å–∞ —Å–ª–æ–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —á–∞—Å—Ç–æ –Ω–µ–æ—Ç–ª–∏—á–∏–º—ã –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤, –≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ä–µ–π—Ç–∏–Ω–≥–∞–º–∏ –º–æ–¥–µ–ª–µ–π –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. –ó–∞–ø—Ä–æ—Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π –¥–ª—è –≥–æ–ª–æ—Å–æ–≤ —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –∞–ø–∞—Ç–∏—á–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ.</p>\n<p><strong>–í—Ä–∞–∂–¥–µ–±–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ</strong></p>\n<p>–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∑–ª–æ—É–º—ã—à–ª–µ–Ω–Ω–∏–∫, —Å—Ç—Ä–µ–º—è—â–∏–π—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ —Å–≤–æ–µ–π —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ. –ò–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –º–µ—Ä –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –ø—Ä–æ–≤–µ—Ä–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –¥—Ä.) –ª–µ–≥–∫–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –≥–æ–ª–æ—Å—É—é—â–∏—Ö –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å.</p>\n<p>–î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç, –±—ã–ª –ª–∏ –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏. –°—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–æ—Å—Ç–æ–∏—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º:</p>\n<ol>\n<li>–í–≤–µ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞—Ä–µ–Ω–µ,</li>\n<li>–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫–∞–∫–æ–π –∏–∑ –¥–≤—É—Ö –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏,</li>\n<li>–ï—Å–ª–∏ –¥–∞, –ø—Ä–æ–≥–æ–ª–æ—Å–æ–≤–∞—Ç—å –∑–∞ —Ü–µ–ª–µ–≤—É—é –º–æ–¥–µ–ª—å,</li>\n<li>–ü–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ—Ü–µ—Å—Å.</li>\n</ol>\n<p>–ê–ª–≥–æ—Ä–∏—Ç–º –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –∏–º–µ–µ—Ç –¥–æ—Å—Ç—É–ø –∫ –ª–æ–≥–∏—Ç–∞–º —Ü–µ–ª–µ–≤–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑—É–º–Ω—ã–º –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–µ–º, –∫–æ–≥–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –º–æ–¥–µ–ª–∏ –ø—ã—Ç–∞–µ—Ç—Å—è –ø–æ–≤—ã—Å–∏—Ç—å –µ–µ —Ä–µ–π—Ç–∏–Ω–≥.</p>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ 10% –≤—Ä–∞–∂–¥–µ–±–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤ –º–æ–≥—É—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –±–æ–ª–µ–µ —á–µ–º –Ω–∞ —á–µ—Ç—ã—Ä–µ –º–µ—Å—Ç–∞.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–æ –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º.</p>"
            },
            {
                "title": "Conclusion & Future Directions",
                "content": "Our experiments in Section 3 lay convincing case for the need for stronger guardrails in open community-driven platforms. Although these are broadly accepted as the ground truth rankings of LLMs, we are concerned that it is easy to intentionally (adversarial) or unintentionally (apathetic, arbitrary settings) corrupt these leaderboards. The key challenge in mitigating the issue of poor quality annotations is: how can community-driven platforms strike the right balance between implementing necessary quality controls while also providing the right incentives and experience to users to continue to use these platforms. Richer feedback We encourage the community to explore ideas from past research, such as soliciting fine-grained annotations (Krishna et al., 2023; Goyal et al., 2022b) or rationales (McDonnell et al., 2016) in addition to the binary preference feedback. Rationales can be useful in encouraging apathetic users to think more critically about their votes (or abstain) and also for filtering out low-quality annotations from both apathetic and adversarial users. Past work in generation evaluation has discussed how binary preference, or even single Likert rating, for the whole output, cannot meaningfully capture the nuances of human preferences (Gehrmann et al., 2023). Instead, fine-grained preference annotation is recommended, both along multiple dimensions or quality (Gehrmann et al.) or for smaller units within the whole output (Krishna et al., 2023; Goyal et al., 2022b). More recent work proposes providing added context during evaluation to encourage higher agreement between annotators (Malaviya et al., 2024). Future work must explore how these strategies can be incorporated into open platforms without inordinately increasing the annotation burden on users. Stronger Guardrails Other guardrails could include reputation-based systems (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), machine learning based anomaly detection (Kumar et al., 2014; Wu et al., 2016) and techniques that use annotator behavior traces on the platform to estimate quality (Goyal et al., 2018). Open access to collected dataset Public release of the collected data on open platforms will spur research to address the annotation issues we discuss It would provide more detailed in this work. overview into which types of queries are most wellequipped to distinguish between models, and what are the limitations of different families of models.",
                "summary": "<p>–í —Ä–∞–∑–¥–µ–ª–µ 3 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–±–µ–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏—Ö –º–µ—Ä –∑–∞—â–∏—Ç—ã –≤ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö —Å —É—á–∞—Å—Ç–∏–µ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–∞. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ —á—Ç–æ —Ç–∞–∫–∏–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —à–∏—Ä–æ–∫–æ –ø—Ä–∏–∑–Ω–∞–Ω—ã –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–ø–∞—Å–µ–Ω–∏–µ, —á—Ç–æ –∏—Ö –ª–µ–≥–∫–æ –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏–ª–∏ –Ω–µ–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å. –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, –∫–∞–∫ —Å–æ–æ–±—â–µ—Å—Ç–≤—É –¥–æ—Å—Ç–∏—á—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º –º–æ—Ç–∏–≤–∞—Ü–∏–∏ –∏ —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏. </p>\n<p>–î–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–¥–µ–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (Krishna et al., 2023; Goyal et al., 2022b) –∏–ª–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π (McDonnell et al., 2016) –≤–º–µ—Å—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –ø—Ä–∏–Ω–∏–º–∞—Ç—å –±–æ–ª–µ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–∏ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–∏ –∏ –æ—Ç—Å–µ–∏–≤–∞—Ç—å –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ç –∞–ø–∞—Ç–∏—á–Ω—ã—Ö –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í –ø—Ä–æ—à–ª–æ–º –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–ª–∏ –µ–¥–∏–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –õ–∞–π–∫–µ—Ä—Ç–∞ –¥–ª—è –≤—Å–µ–≥–æ –≤—ã–≤–æ–¥–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ç—Ä–∞–∑–∏—Ç—å –Ω—é–∞–Ω—Å—ã —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π (Gehrmann et al., 2023). –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞ (Gehrmann et al.) –∏–ª–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π –≤—ã–≤–æ–¥–∞ (Krishna et al., 2023; Goyal et al., 2022b).</p>\n<p>–¢–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç—Å—è –¥—Ä—É–≥–∏–µ –º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ (Adler and de Alfaro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), –º–µ—Ç–æ–¥—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (Kumar et al., 2014; Wu et al., 2016) –∏ —Ç–µ—Ö–Ω–∏–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (Goyal et al., 2018). –û—Ç–∫—Ä—ã—Ç—ã–π –¥–æ—Å—Ç—É–ø –∫ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º –ø–æ–∑–≤–æ–ª–∏—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–ª–∏—á–∞—é—Ç –º–æ–¥–µ–ª–∏, –∏ –≤—ã—è–≤–∏—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–µ–º–µ–π—Å—Ç–≤ –º–æ–¥–µ–ª–µ–π.</p>"
            }
        ]
    },
    {
        "id": "2412.08905",
        "title": "Phi-4 Technical Report",
        "url": "https://huggingface.co/papers/2412.08905",
        "abstract": "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "5b5d18f4e7e9fad9",
        "authors": [
            "Marah Abdin",
            "Jyoti Aneja",
            "Harkirat Behl",
            "S√©bastien Bubeck",
            "Ronen Eldan",
            "Suriya Gunasekar",
            "Michael Harrison",
            "Russell J. Hewett",
            "Mojan Javaheripi",
            "Piero Kauffmann",
            "James R. Lee",
            "Yin Tat Lee",
            "Yuanzhi Li",
            "Weishung Liu",
            "Caio C. T. Mendes",
            "Anh Nguyen",
            "Eric Price",
            "Gustavo de Rosa",
            "Olli Saarikivi",
            "Adil Salim",
            "Shital Shah",
            "Xin Wang",
            "Rachel Ward",
            "Yue Wu",
            "Dingli Yu",
            "Cyril Zhang",
            "Yi Zhang"
        ],
        "affiliations": [
            "Microsoft Research"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.08905.jpg",
        "data": {
            "categories": [
                "#data",
                "#reasoning",
                "#synthetic",
                "#training",
                "#benchmark",
                "#architecture"
            ],
            "emoji": "üß†",
            "ru": {
                "title": "–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö - –∫–ª—é—á –∫ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤—É —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏",
                "desc": ""
            },
            "en": {
                "title": "Elevating Language Models with Quality Data",
                "desc": ""
            },
            "zh": {
                "title": "Êï∞ÊçÆË¥®ÈáèÈ©±Âä®ÁöÑËØ≠Ë®ÄÊ®°Âûãphi-4",
                "desc": ""
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",
                "summary": "–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ phi-4, —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ì–ª–∞–≤–Ω–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å phi-4 ‚Äì —ç—Ç–æ –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –∏–≥—Ä–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö\" –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ –∫–æ–¥, phi-4 –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è.\n\n–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –º–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤–∞ Phi –≤ –æ—Å–Ω–æ–≤–Ω–æ–º \"–ø–µ—Ä–µ–Ω–∏–º–∞–ª–∏\" —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —É –º–æ–¥–µ–ª–∏-—É—á–∏—Ç–µ–ª—è (GPT-4). –û–¥–Ω–∞–∫–æ phi-4 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–µ–≥–æ —É—á–∏—Ç–µ–ª—è –≤ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ç–æ—á–Ω—ã–º–∏ –Ω–∞—É–∫–∞–º–∏ (STEM), —á—Ç–æ —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É–µ—Ç –æ —Ç–æ–º, —á—Ç–æ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ —Ä–∞–º–∫–∏ –ø—Ä–æ—Å—Ç–æ–≥–æ \"–ø–µ—Ä–µ–Ω–∏–º–∞–Ω–∏—è\" –∑–Ω–∞–Ω–∏–π.\n\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å phi-3, phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–≤–æ–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –∑–∞–¥–∞—á–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –±–ª–∞–≥–æ–¥–∞—Ä—è —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö, —É—á–µ–±–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏—è–º –≤ —Å—Ö–µ–º–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏.\n\n*–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –ê–≤—Ç–æ—Ä—ã –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞—é—Ç, —á—Ç–æ phi-4 –Ω–µ –ø—Ä–æ—Å—Ç–æ \"—Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª\" —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏, –∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª –∏—Ö –∑–∞ —Å—á–µ—Ç –±–æ–ª–µ–µ –ø—Ä–æ–¥—É–º–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –¥–∞–Ω–Ω—ã–º –∏ –æ–±—É—á–µ–Ω–∏—é.*\n"
            },
            {
                "title": "Introduction",
                "content": "Recent advancements in Large Language Models (LLMs) have shown that significant improvements in data quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling compute with model and dataset size. Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoningfocused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training. Synthetic data constitutes the bulk of the training data for phi-4 and is generated using diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal. These methods enable the construction of datasets that induce stronger reasoning and problem-solving abilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic data in phi-4 also plays crucial role in post-training, where techniques such as rejection sampling and novel approach to Direct Preference Optimization (DPO) are employed to refine the models outputs. The development of phi-4 is guided by three core pillars: 1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are designed to prioritize reasoning and problem-solving, carefully generated to ensure diversity and 1 Small models Large models phi-4 14b phi-3 14b Qwen 2.5 14b instruct GPT 4o-mini Llama-3.3 70b instruct Qwen 2.5 72b instruct GPT 4o v - m MMLU GPQA MATH HumanEval MGSM SimpleQA DROP MMLUPro HumanEval+ ArenaHard LiveBench IFEval PhiBench (internal) 84.8 56.1 80.4 82.6 80.6 3.0 75.5 70.4 82.8 75.4 47.6 63.0 77.9 31.2 44.6 67.8 53.5 7.6 68.3 51.3 69.2 45.8 28.1 57.9 56. 43.9 79.9 42.9 75.6 72.1 79.6 5.4 85.5 63.2 79.1 70.2 46.6 78.7 49.8 81.8 40.9 73.0 86.2 86.5 9.9 79.3 63.4 82.0 76.2 48.1 80. 58.7 86.3 49.1 66.31 78.91 89.1 20.9 90.2 64.4 77.9 65.5 57.6 89.3 57.1 85.3 49.0 80.0 80.4 87.3 10.2 76.7 69.6 78.4 78.4 55.3 85. 64.6 88.1 50.6 74.6 90.6 90.4 39.4 80.9 73.0 88.0 75.6 57.6 84.8 72.4 Table 1: Performance of phi-4 on set of standard benchmarks. The first set of benchmarks uses OpenAIs simple-evals framework [Ope24b], specifying the prompts/extraction/temperature=0.5. We compare to small models of similar inference cost, as well as to larger models. relevance. We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of phi. 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model). These seeds form the foundation of the synthetic generation pipeline. To complement these synthetic datasets, we also filter the web for high-quality data (in terms of knowledge and reasoning) to use directly in pretraining. 3. Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing new technique to create DPO pairs, based on pivotal token search. With these innovations, the performance of phi-4 on reasoning-related tasks is comparable to or surpasses much larger models. For example, its performance on many widely used reasoning-related benchmarks meets or exceeds that of Llama-3.1-405B. In Table 1 we compare the performance of our model on academic benchmarks to several contemporary foundation models. We find that phi4 significantly exceeds its teacher GPT-4o on the GPQA (graduate-level STEM Q&A) and MATH (math competition) benchmarks. 1These scores are lower than those reported by Meta, perhaps because simple-evals has strict formatting requirement that Llama models have particular trouble following. We use the simple-evals framework because it is reproducible, but Meta reports 77 for MATH and 88 for HumanEval on Llama-3.3. 2We use organic to refer to human-generated or otherwise non-synthetic data. Figure 1: Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature = 0.5. We chose = 0.5 to follow simple-evals [Ope24b]. Error bars are 2œÉ of the estimate. On competition math, phi-4 scores well above its weight-class even compared to nonopen-weight models.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å phi-4, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑–≤–∏—Ç–∏–µ–º —Å–µ–º–µ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π Phi –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–∞–ª—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ö–ª—é—á–µ–≤—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —É—Å–ø–µ—Ö–∞ phi-4 —è–≤–ª—è–µ—Ç—Å—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–æ–∑–¥–∞–Ω–∏—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞.\n\n**–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ phi-4:**\n\n1.  **–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:** –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö phi-4 —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –º–Ω–æ–≥–æ–∞–≥–µ–Ω—Ç–Ω–æ–µ –ø–æ–¥—Å–∫–∞–∑—ã–≤–∞–Ω–∏–µ (multi-agent prompting), —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Å–∞–º–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (self-revision workflows) –∏ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (instruction reversal). –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —Ä–∞–∑–≤–∏—Ç–∏—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –¥–æ–ª—é —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –ø–æ–∫–æ–ª–µ–Ω–∏—è–º–∏ Phi.\n\n2.  **–û—Ç–±–æ—Ä –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö:** –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Å–Ω–æ–≤—ã –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, –≤–∫–ª—é—á–∞—è –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∫–æ–¥–∞. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Å–ª—É–∂–∞—Ç \"–∑–∞—Ç—Ä–∞–≤–∫–æ–π\" –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –≥–ª—É–±–æ–∫–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è –∏ –æ–±–ª–∞–¥–∞—é—Ç –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é –¥–ª—è –º–æ–¥–µ–ª–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç —Ç–∞–∫–∂–µ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç—Å—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π), –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏.\n\n3.  **–ü–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥:** –í phi-4 —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –ø–æ—Å—Ç-—Ç—Ä–µ–Ω–∏–Ω–≥–∞ –ø—É—Ç–µ–º —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π SFT (Supervised Fine-Tuning) –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –∞ —Ç–∞–∫–∂–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä –¥–ª—è DPO (Direct Preference Optimization), –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–∏—Å–∫–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏.\n\n–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–∏–º –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è–º phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —Å—Ä–∞–≤–Ω–∏–º—É—é –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â—É—é –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ –º–Ω–æ–≥–∏—Ö —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Å—Ç–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Llama-3.1-405B. –í —Ç–∞–±–ª–∏—Ü–µ 1 –ø–æ–∫–∞–∑–∞–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ phi-4 —Å –¥—Ä—É–≥–∏–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Å—Ç–∞—Ö. –ü—Ä–∏ —ç—Ç–æ–º phi-4 –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ–µ–≥–æ \"—É—á–∏—Ç–µ–ª—è\" GPT-4o –Ω–∞ —Ç–µ—Å—Ç–∞—Ö GPQA (–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –ø–æ STEM –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤) –∏ MATH (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è).\n\n–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, —Å—Ç–∞—Ç—å—è –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç, —á—Ç–æ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏ —á—Ç–æ –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º —É–ª—É—á—à–µ–Ω–∏—è–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ä–∞–≤–Ω–∏–º—ã–º —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π.\n"
            },
            {
                "title": "Addressing Overfitting and Data Contamination",
                "content": "Decontamination: One pitfall of foundation models is overfitting to benchmarks, such as through the leakage of benchmark test sets via the web corpus. We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results. More details of the decontamination method are given in Appendix B. AMC Benchmark: The surest way to guard against overfitting to the test set is to test on fresh data. We tested our model on the November 2024 AMC-10 and AMC-12 math competitions [Com24], which occurred after all our training data was collected, and we only measured our performance after choosing all the hyperparameters in training our final model. These contests are the entry points to the Math Olympiad track in the United States and over 150,000 students take the tests each year. In Figure 1 we plot the average score over the four versions of the test, all of which have maximum score of 150. phi-4 outperforms not only similar-size or open-weight models but also much larger frontier models. Such strong performance on fresh test set suggests that phi-4s top-tier performance on the MATH benchmark is not due to overfitting or contamination. We provide further details in Appendix C. Relying on Contamination-Proof Benchmarks: We give significant weight to benchmarks which were designed in such way that the questions are original and do not appear on the web, such as GPQA [RHS+23]. While optimizing our model, we relied on an internal benchmark composed primarily of original prompts written by the team (see Section 5 for further details). Long Chain-of-Thought Models: style of LLM that scales inference-time compute by generating long chains of thought has emerged over the past few months, as pioneered by OpenAI O1 [Ope24a] and followed by DeepSeek-R1-Lite-Preview [Dee24] and Qwen/QwQ-32B-Preview [Tea24]. These models perform well on reasoning benchmarks, where QwQ, the only such model with open weights, averages 124.5 points in the AMC-10/12 setting of Figure 1. However, QwQ also uses 4X more tokens on this task than phi-4 and has more than twice as many parameters. Thus, the inference cost of QwQ is an order of magnitude higher than phi-4. Consequently, these models are not in the same class as phi-4 with respect to cost or latency.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤–∞–∂–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∞ –∏–º–µ–Ω–Ω–æ:\n\n**1. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" (Decontamination):**\n\n–û–¥–Ω–æ–π –∏–∑ –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ LLM —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö. –≠—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ —Å–ª—É—á–∞–π–Ω–æ \"–ø—Ä–æ—Å–∞—á–∏–≤–∞—é—Ç—Å—è\" –≤ –æ–±—É—á–∞—é—â–∏–π –∫–æ—Ä–ø—É—Å, –Ω–∞–ø—Ä–∏–º–µ—Ä, —á–µ—Ä–µ–∑ –≤–µ–±-–∫–æ—Ä–ø—É—Å. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —ç—Ç–æ–≥–æ, –¥–ª—è –º–æ–¥–µ–ª–∏ phi-4 –±—ã–ª —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω –ø—Ä–æ—Ü–µ—Å—Å –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ Phi. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –±—É–¥—É—Ç –∏—Å–∫–∞–∂–µ–Ω—ã \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ–º\" –¥–∞–Ω–Ω—ã–º–∏. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–∞ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ B.\n\n**2. –ë–µ–Ω—á–º–∞—Ä–∫ AMC (American Mathematics Competitions):**\n\n–ß—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è, –º–æ–¥–µ–ª—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –∏–º–µ–Ω–Ω–æ –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω–∫—É—Ä—Å–∞—Ö AMC-10 –∏ AMC-12, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–æ–¥–∏–ª–∏—Å—å –≤ –Ω–æ—è–±—Ä–µ 2024 –≥–æ–¥–∞, –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å–±–æ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏–∑–º–µ—Ä—è–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –≤—Å–µ—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –≠—Ç–∏ –∫–æ–Ω–∫—É—Ä—Å—ã —è–≤–ª—è—é—Ç—Å—è –æ—Ç–±–æ—Ä–æ—á–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ª–∏–º–ø–∏–∞–¥—ã –≤ –°–®–ê, –∏ –≤ –Ω–∏—Ö –µ–∂–µ–≥–æ–¥–Ω–æ —É—á–∞—Å—Ç–≤—É—é—Ç –±–æ–ª–µ–µ 150 000 —à–∫–æ–ª—å–Ω–∏–∫–æ–≤. –ù–∞ –≥—Ä–∞—Ñ–∏–∫–µ (–†–∏—Å—É–Ω–æ–∫ 1) –ø–æ–∫–∞–∑–∞–Ω —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ —á–µ—Ç—ã—Ä–µ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º —Ç–µ—Å—Ç–∞, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª –≤ –∫–∞–∂–¥–æ–º –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–≤–µ–Ω 150. phi-4 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–ª–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏, –Ω–æ –∏ –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏. –¢–∞–∫–∏–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –Ω–æ–≤—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≥–æ–≤–æ—Ä—è—Ç –æ —Ç–æ–º, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ MATH –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" –¥–∞–Ω–Ω—ã–º–∏. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ C.\n\n**3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∑–∞—â–∏—â–µ–Ω–Ω—ã—Ö –æ—Ç \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\":**\n\n–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –≤–æ–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º–∏ –∏ –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, GPQA. –ü—Ä–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫, —Å–æ—Å—Ç–æ—è—â–∏–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ (–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ –†–∞–∑–¥–µ–ª–µ 5).\n\n**4. –ú–æ–¥–µ–ª–∏ —Å –¥–ª–∏–Ω–Ω–æ–π —Ü–µ–ø–æ—á–∫–æ–π —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Long Chain-of-Thought Models):**\n\n–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø–æ—è–≤–∏–ª–∏—Å—å LLM, –∫–æ—Ç–æ—Ä—ã–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –¥–ª–∏–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –≠—Ç–∏ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ OpenAI O1, DeepSeek-R1-Lite-Preview –∏ Qwen/QwQ-32B-Preview, –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –ø—Ä–æ–≤–µ—Ä—è—é—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é. QwQ, –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –≤–µ—Å–∞–º–∏ –≤ —ç—Ç–æ–π –≥—Ä—É–ø–ø–µ, –Ω–∞–±–∏—Ä–∞–µ—Ç –≤ —Å—Ä–µ–¥–Ω–µ–º 124,5 –±–∞–ª–ª–∞ –≤ —Ç–µ—Å—Ç–µ AMC-10/12. –û–¥–Ω–∞–∫–æ QwQ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏–º–µ–µ—Ç –±–æ–ª–µ–µ —á–µ–º –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á–µ–º phi-4. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –≤—ã–≤–æ–¥ QwQ –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –≤—ã—à–µ, —á–µ–º —É phi-4. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —ç—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Ç–æ–º—É –∂–µ –∫–ª–∞—Å—Å—É, —á—Ç–æ –∏ phi-4, —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –∑–∞–¥–µ—Ä–∂–∫–∏.\n"
            },
            {
                "title": "Purpose of Synthetic Data",
                "content": "Synthetic data as substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by language model is by definition predicted by the preceding tokens, making it easier for model to follow the resulting reasoning patterns. In this way, synthetic data may act as form of spoonfeeding, presenting challenges in digestible and progression-oriented manner. simple example to illustrate this is that human-written solution to math problem might start with the final answer. This answer is much too hard to output immediately, for either human or an LLMthe human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce it linearly. Synthetic solutions to math problems will not have such roadblocks. Alignment with Inference Contexts. Synthetic data is typically closer to the format of outputs we expect our models to generate. Training on such data helps align the models pretraining experience with the scenarios it encounters during inference. This alignment ensures that the context seen during generation remains in-distribution with respect to the data the model was pretrained on. For example, web forums are very different in style from LLM interactions. If fact only appears in web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces. Rewriting facts from the web forum into the language style of an LLM makes the facts more accessible during the LLM chat context of inference. Principles. Our approach to generating synthetic data for phi-4 is guided by the following principles: 1. Diversity: The data should comprehensively cover subtopics and skills within each domain. This requires curating diverse seeds from organic sources. 2. Nuance and Complexity: Effective training requires nuanced, non-trivial examples that reflect the complexity and the richness of the domain. Data must go beyond basics to include edge cases and advanced examples. 3. Accuracy: Code should execute correctly, proofs should be valid, and explanations should adhere to established knowledge, etc. 4. Chain-of-Thought: Data should encourage systematic reasoning, teaching the model various approaches to the problems in step-by-step manner. This fosters coherent outputs for complex tasks.",
                "summary": "## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n\n–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤—Å—ë –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π Phi –Ω–∞–≥–ª—è–¥–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç—Ç–æ. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—Å—Ç–æ –¥–µ—à—ë–≤–∞—è –∑–∞–º–µ–Ω–∞ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–º\" –¥–∞–Ω–Ω—ã–º, –æ–Ω–∏ –æ–±–ª–∞–¥–∞—é—Ç —Ä—è–¥–æ–º –ø—Ä—è–º—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤.\n\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ.** –í –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏ —á–∞—Å—Ç–æ —Å–ª–æ–∂–Ω—ã –∏ –Ω–µ–ø—Ä—è–º—ã. –ú–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —à–∞–≥–æ–≤, —á—Ç–æ–±—ã —Å–≤—è–∑–∞—Ç—å —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω —Å–æ —Å–ª–µ–¥—É—é—â–∏–º, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —ç—Ç–æ–≥–æ, –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏. –≠—Ç–æ –æ–±–ª–µ–≥—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–∞–µ–º—ã—Ö –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ü–µ–ø–æ—á–µ–∫. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ —Å–≤–æ–µ–≥–æ —Ä–æ–¥–∞ \"–∫–æ—Ä–º–ª–µ–Ω–∏–µ —Å –ª–æ–∂–µ—á–∫–∏\", –∫–æ–≥–¥–∞ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –≤ –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å —Ñ–æ—Ä–º–µ. –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä: —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–∏, –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ–µ —á–µ–ª–æ–≤–µ–∫–æ–º, –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –≠—Ç–æ—Ç –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–µ–Ω –¥–ª—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –∫–∞–∫ —á–µ–ª–æ–≤–µ–∫–æ–º, —Ç–∞–∫ –∏ LLM. –ß–µ–ª–æ–≤–µ–∫ –ø–æ–ª—É—á–∏–ª –µ–≥–æ –ø—É—Ç–µ–º –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–≥–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ LLM –Ω–∞—É—á–∏—Ç—Å—è –≤—ã–¥–∞–≤–∞—Ç—å –µ–≥–æ –ª–∏–Ω–µ–π–Ω–æ. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –Ω–µ –±—É–¥—É—Ç –∏–º–µ—Ç—å —Ç–∞–∫–∏—Ö –ø—Ä–µ–ø—è—Ç—Å—Ç–≤–∏–π.\n\n**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º –≤—ã–≤–æ–¥–∞.** –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –æ–±—ã—á–Ω–æ –±–ª–∏–∂–µ –∫ —Ñ–æ—Ä–º–∞—Ç—É –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –æ–∂–∏–¥–∞–µ–º –æ—Ç –Ω–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –æ–ø—ã—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–æ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–Ω–∞ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–∞–±–ª—é–¥–∞–µ–º—ã–π –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –æ—Å—Ç–∞—ë—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤–µ–±-—Ñ–æ—Ä—É–º—ã —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –ø–æ —Å—Ç–∏–ª—é –æ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å LLM. –ï—Å–ª–∏ –∫–∞–∫–æ–π-–ª–∏–±–æ —Ñ–∞–∫—Ç –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –¥–∞–Ω–Ω—ã—Ö –≤–µ–±-—Ñ–æ—Ä—É–º–æ–≤, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å—á–∏—Ç–∞—Ç—å, —á—Ç–æ –æ–Ω –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–µ–Ω –≤ —á–∞—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç. –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ —Å –≤–µ–±-—Ñ–æ—Ä—É–º–∞ –Ω–∞ —è–∑—ã–∫–æ–≤–æ–π —Å—Ç–∏–ª—å LLM –¥–µ–ª–∞–µ—Ç —Ñ–∞–∫—Ç—ã –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —á–∞—Ç–∞ LLM –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞.\n\n**–ü—Ä–∏–Ω—Ü–∏–ø—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è phi-4.** –ù–∞—à –ø–æ–¥—Ö–æ–¥ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è phi-4 –æ—Å–Ω–æ–≤—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö:\n\n1.  **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ:** –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω–µ –æ—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –ø–æ–¥—Ç–µ–º—ã –∏ –Ω–∞–≤—ã–∫–∏ –≤ –∫–∞–∂–¥–æ–π –æ–±–ª–∞—Å—Ç–∏. –≠—Ç–æ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–±–æ—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö \"—Å–µ–º—è–Ω\" –∏–∑ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n2.  **–ù—é–∞–Ω—Å—ã –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å:** –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –Ω—é–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö, –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∏ –±–æ–≥–∞—Ç—Å—Ç–≤–æ –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –≤—ã—Ö–æ–¥–∏—Ç—å –∑–∞ —Ä–∞–º–∫–∏ –æ—Å–Ω–æ–≤ –∏ –≤–∫–ª—é—á–∞—Ç—å –ø–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã.\n3.  **–¢–æ—á–Ω–æ—Å—Ç—å:** –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏, –∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–±—â–µ–ø—Ä–∏–Ω—è—Ç—ã–º –∑–Ω–∞–Ω–∏—è–º –∏ —Ç.–¥.\n4.  **–¶–µ–ø–æ—á–∫–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π:** –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –ø–æ–æ—â—Ä—è—Ç—å —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –æ–±—É—á–∞—è –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–Ω—ã–º –ø–æ–¥—Ö–æ–¥–∞–º –∫ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º —à–∞–≥ –∑–∞ —à–∞–≥–æ–º. –≠—Ç–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—é —Å–≤—è–∑–Ω—ã—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.\n"
            },
            {
                "title": "Synthetic Data for Pretraining and Midtraining",
                "content": "We created 50 broad types of synthetic datasets, each one relying on different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to total of about 400B unweighted tokens. In Appendix D, we give few examples of transcripts taken from our synthetic generations. Here, we highlight novel methodologies used in generating synthetic datasets for phi-4: Seed Curation: The synthetic dataset generation begins with high-quality seeds sourced from multiple domains. These curated seeds provide the foundation for synthetic data generation, enabling the creation of exercises, discussions, and reasoning tasks tailored to the models training objectives. 1. Web and Code-based Seeds: Excerpts and snippets are extracted from web pages, books, and code repositories with focus on content that demonstrates high complexity, reasoning depth, and educational value. To ensure quality, we employ two-stage filtering process: first, identifying pages with strong educational potential, and second, segmenting the selected pages into passages, scoring each for its factual and reasoning content. 2. Question Datasets: large set of questions was collected from websites, forums, and Q&A platforms. These questions were then filtered using plurality-based technique to balance difficulty. Specifically, we generated multiple independent answers for each question and applied majority voting to assess the consistency of responses. We discarded questions where all answers agreed (indicating the question was too easy) or where answers were entirely inconsistent (indicating the question was too difficult or ambiguous). This filtering process produces dataset of questions that challenge the models reasoning and problem-solving abilities while remaining approachable. The plurality answers were used in place of the ground truth in our rejection-sampling based generations. 3. Creating Question-Answer pairs from Diverse Sources: Another technique we use for seed curation involves leveraging language models to extract question-answer pairs from organic sources such as books, scientific papers, and code. This approach does not rely on merely identifying explicit Q&A pairs within the text. Instead, it involves pipeline designed to detect deduction chains or logical progressions in the text. The language model identifies key steps in reasoning or problem-solving processes and reformulates them into questions and corresponding answers. Our experiments show that, if done correctly, training on the resulting content can be far more effective (in terms of improvement on academic and internal benchmarks) than training on the original content. Rewrite and Augment: Seeds are transformed into synthetic data through multi-step prompting workflows. This includes rewriting most of the useful content in given passages into exercises, discussions, or structured reasoning tasks. Self-revision: The initial responses are then iteratively refined through feedback loop where model critiques and subsequently improves its own outputs, guided by the rubrics focused on reasoning and factual accuracy. 5 Instruction Reversal for Code and Other Tasks: To enhance the models ability to generate outputs from instructions, we used an instruction reversal technique. For example, we take existing code snippets from the code data corpus and use it to generate corresponding instructions that include the problem description or task prompt. The resulting synthetic data pairs were structured with the instruction appearing before the code. Only data with high fidelity between the original and regenerated code are retained, ensuring alignment between the instructions and the outputs. This method can be generalized to other targeted use cases. Validation of Code and Other Scientific Data: When appropriate, we incorporate tests for validating our reasoning-heavy synthetic datasets. The synthetic code data is validated through execution loops and tests. For scientific datasets, the questions are extracted from scientific materials using method designed to ensure high relevance, groundedness, and difficulty balance.",
                "summary": "–í —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4. –ë—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 50 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ç–µ–º, –Ω–∞–≤—ã–∫–æ–≤ –∏ —Ç–∏–ø–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è, –æ–±—â–∏–º –æ–±—ä–µ–º–æ–º –æ–∫–æ–ª–æ 400 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤.\n\n**–ö—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:** –ü—Ä–æ—Ü–µ—Å—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ—Ç–±–æ—Ä–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Å–ª—É–∂–∞—Ç –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –¥–∏—Å–∫—É—Å—Å–∏–∏ –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª—è–º –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n\n1.  **–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ –∫–æ–¥–∞:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü, –∫–Ω–∏–≥ –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –∫–æ–¥–∞, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—â–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å, –≥–ª—É–±–∏–Ω—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å. –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ–Ω—è–ª—Å—è –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: —Å–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–ª–∏—Å—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤—ã—Å–æ–∫–∏–º –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–æ–º, –∞ –∑–∞—Ç–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–∞–∑–¥–µ–ª—è–ª–∏—Å—å –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ—Ü–µ–Ω–∏–≤–∞–ª—Å—è –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ –≥–ª—É–±–∏–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.\n2.  **–ù–∞–±–æ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:** –ë—ã–ª–∞ —Å–æ–±—Ä–∞–Ω–∞ –±–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤, —Ñ–æ—Ä—É–º–æ–≤ –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤. –≠—Ç–∏ –≤–æ–ø—Ä–æ—Å—ã –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∏ –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤. –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –±—ã–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ (—Å–ª–∏—à–∫–æ–º –ª–µ–≥–∫–∏–µ) –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–º–∏ (—Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–µ), –æ—Ç–±—Ä–∞—Å—ã–≤–∞–ª–∏—Å—å. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∏–ª—Å—è –Ω–∞–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—Ä–æ—Å–∞—é—Ç –≤—ã–∑–æ–≤ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º –º–æ–¥–µ–ª–∏ –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ —Ä–µ—à–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º, –æ—Å—Ç–∞–≤–∞—è—Å—å –ø—Ä–∏ —ç—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –≤–º–µ—Å—Ç–æ \"–∏—Å—Ç–∏–Ω–Ω—ã—Ö\" –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è.\n3.  **–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** –î–ª—è –æ—Ç–±–æ—Ä–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏–∑ –∫–Ω–∏–≥, –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ –∫–æ–¥–∞. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏—â–µ—Ç —è–≤–Ω—ã–µ –ø–∞—Ä—ã \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –≤ —Ç–µ–∫—Å—Ç–µ. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤—ã—è–≤–ª—è–µ—Ç —Ü–µ–ø–æ—á–∫–∏ —É–º–æ–∑–∞–∫–ª—é—á–µ–Ω–∏–π –∏–ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ. –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —à–∞–≥–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–ª–∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∏—Ö –≤ –≤–æ–ø—Ä–æ—Å—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –æ—Ç–≤–µ—Ç—ã. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç–∞–∫–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º (—Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ç–µ—Å—Ç–∞—Ö), —á–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ.\n\n**–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ:** –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Å—Ç—É–ø–µ–Ω—á–∞—Ç—ã—Ö –ø–æ–¥—Å–∫–∞–∑–æ–∫. –≠—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –±–æ–ª—å—à–µ–π —á–∞—Å—Ç–∏ –ø–æ–ª–µ–∑–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è, –¥–∏—Å–∫—É—Å—Å–∏–∏ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ.\n\n**–°–∞–º–æ—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:** –ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∑–∞—Ç–µ–º –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ü–∏–∫–ª–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏, –≤ –∫–æ—Ç–æ—Ä–æ–º –º–æ–¥–µ–ª—å –∫—Ä–∏—Ç–∏–∫—É–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç —Å–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∫—Ä–∏—Ç–µ—Ä–∏–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ —Ç–æ—á–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–æ–≤.\n\n**–û–±—Ä–∞—â–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –∫–æ–¥–∞ –∏ –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á:** –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ –æ–±—Ä–∞—â–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞ –∏–∑ –∫–æ—Ä–ø—É—Å–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –≤–∫–ª—é—á–∞—é—â–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–¥–∞—á–∏. –ü–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ç–∞–∫, —á—Ç–æ–±—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—Ä–µ–¥—à–µ—Å—Ç–≤–æ–≤–∞–ª–∞ –∫–æ–¥—É. –°–æ—Ö—Ä–∞–Ω—è–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∏—Å—Ö–æ–¥–Ω—ã–º –∏ —Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–¥–æ–º, —á—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–ª–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–æ–±—â–µ–Ω –Ω–∞ –¥—Ä—É–≥–∏–µ —Ü–µ–ª–µ–≤—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.\n\n**–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∞ –∏ –¥—Ä—É–≥–∏—Ö –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:** –ö–æ–≥–¥–∞ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ, –≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞—é—Ç—Å—è —Ç–µ—Å—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∞ –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ü–∏–∫–ª–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–æ–≤. –î–ª—è –Ω–∞—É—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å—ã –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–µ–≥–æ –≤—ã—Å–æ–∫—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å, –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å.\n"
            },
            {
                "title": "Curation and Filtering of Web and Q&A Data",
                "content": "Q&A datasets. We collected tens-of-millions high-quality organic problems and solutions by reviewing public websites, relying on existing datasets, and acquiring external datasets. Our experience from previous models showed that question-answer data contributed significantly to various capabilities, such as mathematical reasoning and academic performance. Our ablation studies showed that organic questions are substantially more effective than synthetic questions. We used several ways to synthetically augment the dataset of organic questions to obtain larger dataset. While these rewritten questions improved the models capabilities, the gains were not as pronounced. significant portion of the collected questions lacked accurate solutions. To address this, we replaced the answers with synthetically generated ones and used majority-voting to increase accuracy. All collected questions and solutions underwent thorough decontamination process to ensure there is no overlap with test sets3. Targeting High-quality Web Data. We collected wide variety of high-quality organic data sources for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums, and programming tutorials). In addition to directly training on this text, we used various web sources as seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to be absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations for derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web data. We discuss the main techniques and considerations below: Targeted Acquisitions: We included major repositories of reasoning-dense documents that are publicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g., licensed books) aiming for level of comprehensiveness, recency, and cleanliness above the typical standard of externally available corpora. Filtering Web Dumps: To capture the long tail of information-rich web sources (e.g., forums, blogs, course material, domain-specific wikis), we took the approach of selecting small fraction of highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on 106 LLM-generated annotations. This approach tends to over-index on STEM-related keywords, so we created specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history, travel, culture, and entertainment). These topic classifications were also obtained by distilling an 3This step is crucial to the reliability of some of the academic benchmarks: for instance, some test benchmark variants can be found on platforms like Hugging Face. Moreover, benchmarks such as MMLU are frequently compiled from websourced questions. LLM annotator. Finally, we removed corrupted text and binary files by detecting outliers according to n-gram statistics and compression ratios. Multilingual Data: We incorporated multilingual datasets to ensure that our model could handle wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi and Japanese. This involved sourcing and processing high-quality multilingual documents from CommonCrawl and Wikipedia. Our multilingual processing pipeline consists of language identification model, based on fastText used to categorize documents into 176 languages, then uses the same classifiers for filtering web dumps to filter for quality. Note that the classifiers were trained on multilingual LLM-generated annotations. Custom Extraction and Cleaning Pipelines: To ensure sufficient cleanliness and uniformity between heterogeneous organic data sources, we needed collection of customized heuristics and parsers. For each targeted data source, we built custom pipelines to ingest variety of file formats (e.g., multi-file TeX source, ePub and other XML-like formats, Microsoft Word documents, and PDFs). For general web data, we built custom HTML-to-text extractor, taking significant care to preserve fragile content that is frequently corrupted by naƒ±ve parsers (e.g., TeX/MathML equations, code blocks, tables, and forum thread structure). This extractor prunes and normalizes the DOM tree, using variety of signals (e.g., HTML tag names, CSS classes, content length, and tree depth) to distinguish elements such as boilerplate, advertisements, equations, and syntaxhighlighter artifacts.",
                "summary": "**–ù–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ**\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ —Å–æ–±—Ä–∞–Ω—ã –¥–µ—Å—è—Ç–∫–∏ –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –≤–µ–±-—Å–∞–π—Ç–æ–≤, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≤–Ω–µ—à–Ω–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –û–ø—ã—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ç–∏–ø–∞ \"–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç\" –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∞—è —É—Å–ø–µ–≤–∞–µ–º–æ—Å—Ç—å. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ \"–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ\" –≤–æ–ø—Ä–æ—Å—ã (—Ç–æ –µ—Å—Ç—å, –≤–æ–ø—Ä–æ—Å—ã, —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –ª—é–¥—å–º–∏, –∞ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ) –≥–æ—Ä–∞–∑–¥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö. –î–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –Ω–∞–±–æ—Ä–∞ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –•–æ—Ç—è —ç—Ç–∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —É–ª—É—á—à–∏–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, –ø—Ä–∏—Ä–æ—Å—Ç –±—ã–ª –Ω–µ —Ç–∞–∫–∏–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–º. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —á–∞—Å—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –∏–º–µ–ª–∞ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç—ã –±—ã–ª–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ–º. –í—Å–µ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ—à–ª–∏ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É, —á—Ç–æ–±—ã –∏—Å–∫–ª—é—á–∏—Ç—å –ª—é–±–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏.\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –±—ã–ª —Å–æ–±—Ä–∞–Ω —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏—á–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–ª—Å—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º, —Ç—Ä–µ–±—É—é—â–∏–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º –Ω—é–∞–Ω—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏, –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä—É–º—ã –∏ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é). –í–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –Ω–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–æ –∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω–≤–µ–π–µ—Ä–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö. –ë—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ, —á—Ç–æ —á–∏—Å—Ç—ã–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫—Ä–∞–π–Ω–µ –≤–∞–∂–Ω—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: –¥–∞–∂–µ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ –º–æ–≥—É—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ —Å–µ—Ä—å–µ–∑–Ω–æ–º—É —É—Ö—É–¥—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ—ç—Ç–æ–º—É –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–∏–ª–∏—è –±—ã–ª–∏ –≤–ª–æ–∂–µ–Ω—ã –≤ —Ç—â–∞—Ç–µ–ª—å–Ω—É—é –∫—É—Ä–∞—Ü–∏—é –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö.\n\n–ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–ª —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:\n\n*   **–¶–µ–ª–µ–≤—ã–µ –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏—è:** –ë—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –∫—Ä—É–ø–Ω—ã–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã –¥–ª—è –ø—É–±–ª–∏—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, arXiv, PubMed Central, GitHub) –∏–ª–∏ –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏). –¶–µ–ª—å—é –±—ã–ª–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ø–æ–ª–Ω–æ—Ç—ã, –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —á–∏—Å—Ç–æ—Ç—ã –≤—ã—à–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤.\n*   **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–µ–±-–¥–∞–º–ø–æ–≤:** –î–ª—è –æ—Ö–≤–∞—Ç–∞ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ –Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ–æ—Ä—É–º–æ–≤, –±–ª–æ–≥–æ–≤, —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∏–∫–∏) –±—ã–ª –ø—Ä–∏–º–µ–Ω–µ–Ω –ø–æ–¥—Ö–æ–¥ –≤—ã–±–æ—Ä–∞ –Ω–µ–±–æ–ª—å—à–æ–π —á–∞—Å—Ç–∏ —Å–∞–º—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –º–∞—Å—Å–æ–≤—ã—Ö –≤–µ–±-–¥–∞–º–ø–æ–≤. –î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –Ω–µ–±–æ–ª—å—à–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM), –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö LLM. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç STEM-—Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –ø–æ—ç—Ç–æ–º—É –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ-STEM —Ç–µ–º–∞—Ç–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å–∫—É—Å—Å—Ç–≤–∞, –∏—Å—Ç–æ—Ä–∏–∏, –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–π, –∫—É–ª—å—Ç—É—Ä—ã –∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–π). –≠—Ç–∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∞–∫–∂–µ –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã –ø—É—Ç–µ–º –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏–∏ LLM-–∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–º. –ù–∞–∫–æ–Ω–µ—Ü, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –¥–≤–æ–∏—á–Ω—ã–µ —Ñ–∞–π–ª—ã –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã –ø—É—Ç–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ n-–≥—Ä–∞–º–º –∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º —Å–∂–∞—Ç–∏—è.\n*   **–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:** –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —è–∑—ã–∫–æ–≤, –±—ã–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –Ω–µ–º–µ—Ü–∫–∏–π, –∏—Å–ø–∞–Ω—Å–∫–∏–π, —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π, –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π, –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π, —Ö–∏–Ω–¥–∏ –∏ —è–ø–æ–Ω—Å–∫–∏–π. –≠—Ç–æ –≤–∫–ª—é—á–∞–ª–æ –ø–æ–∏—Å–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ CommonCrawl –∏ Wikipedia. –ö–æ–Ω–≤–µ–π–µ—Ä –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –º–æ–¥–µ–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —è–∑—ã–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ fastText, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ 176 —è–∑—ã–∫–æ–≤, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤–µ–±-–¥–∞–º–ø–æ–≤ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã—Ö LLM-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö.\n*   **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω–≤–µ–π–µ—Ä—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏:** –î–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —á–∏—Å—Ç–æ—Ç—ã –∏ –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è –º–µ–∂–¥—É —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª—Å—è –Ω–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫ –∏ –ø–∞—Ä—Å–µ—Ä–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–µ–ª–µ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –∫–æ–Ω–≤–µ–π–µ—Ä—ã –¥–ª—è –ø—Ä–∏–µ–º–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ TeX, ePub –∏ –¥—Ä—É–≥–∏–µ XML-–ø–æ–¥–æ–±–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã, –¥–æ–∫—É–º–µ–Ω—Ç—ã Microsoft Word –∏ PDF). –î–ª—è –æ–±—â–∏—Ö –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Å–æ–∑–¥–∞–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä HTML-–≤-—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —É–¥–µ–ª—è–µ—Ç –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é \"—Ö—Ä—É–ø–∫–æ–≥–æ\" –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —á–∞—Å—Ç–æ –ø–æ–≤—Ä–µ–∂–¥–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ä—Å–µ—Ä–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É—Ä–∞–≤–Ω–µ–Ω–∏—è TeX/MathML, –±–ª–æ–∫–∏ –∫–æ–¥–∞, —Ç–∞–±–ª–∏—Ü—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Ç–æ–∫–∞ —Ñ–æ—Ä—É–º–∞). –≠—Ç–æ—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –æ–±—Ä–µ–∑–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–µ—Ä–µ–≤–æ DOM, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–º–µ–Ω–∞ HTML-—Ç–µ–≥–æ–≤, –∫–ª–∞—Å—Å—ã CSS, –¥–ª–∏–Ω—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏ –≥–ª—É–±–∏–Ω—É –¥–µ—Ä–µ–≤–∞) –¥–ª—è —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∫–∞–∫ —à–∞–±–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —Ä–µ–∫–ª–∞–º–∞, —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥—Å–≤–µ—Ç–∫–∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞.\n"
            },
            {
                "title": "Pretraining Details",
                "content": "The phi-4 model is based on decoder-only transformer architecture [VSP+17] with 14B parameters and default context length of 4096. This is later extended to 16K context length during midtraining. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than 2K sliding window used in phi-3-medium. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. The training hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted by stress testing the learning rate warm-up stage for stability. Pretraining is followed by shorter midtraining stage to increase the original context length of 4k to 16k. Since pre-trained models are not good at instruction following, it is not very informative to use 0-shot evaluations that require the answer to be in specific format, for example simple-evals. We 7 MMLU MMLU pro GSM8k Human-Eval ARCC MBPP MATH TQA phi-4 (4k) phi-4 (16k) +3.0 +2.7 +10.3 +8.9 +2.2 +1.2 +7.8 +9.0 +1.1 +0.9 +6.8 +9. +8.9 +8.4 -0.7 -1.5 Table 2: Pretraining benchmarks for phi-4 compared to its predecessor, phi-3-medium after pretraining. therefore use an internal implementation of benchmarks for pretraining which uses mixture of loglikelihood and/or few-shot prompts for various tasks. Specifically, we used log-likelihood evaluations for MMLU (5-shot), MMLU-pro, and ARCC (1-shot). We used 1, 3, 4, and 8 few-shot examples for TriviaQA (TQA), MBPP, MATH, and GSM8k to help the model adhere to the answer format for easier extraction of the solution. We use this evaluation method throughout Section 3. Table 2 summarizes the performance boost of pretrained phi-4 compared with its predecessor phi-3-medium.",
                "summary": "–ú–æ–¥–µ–ª—å phi-4 –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ transformer —Ç–æ–ª—å–∫–æ —Å –¥–µ–∫–æ–¥–µ—Ä–æ–º, –æ–Ω–∞ –∏–º–µ–µ—Ç 14 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 4096 —Ç–æ–∫–µ–Ω–æ–≤. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ 16 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∞ –Ω–∞ phi-3-medium, –∑–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π. –í–æ-–ø–µ—Ä–≤—ã—Ö, –≤ phi-4 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä tiktoken, –∫–æ—Ç–æ—Ä—ã–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 100 352, –≤–∫–ª—é—á–∞—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–æ–∫–µ–Ω—ã. –í–æ-–≤—Ç–æ—Ä—ã—Ö, –≤ phi-4 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (full attention) –∫–æ –≤—Å–µ–π –¥–ª–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ 4096 —Ç–æ–∫–µ–Ω–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –≤ phi-3-medium –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ —Ä–∞–∑–º–µ—Ä–æ–º 2048 —Ç–æ–∫–µ–Ω–æ–≤.\n\n–ú–æ–¥–µ–ª—å phi-4 –±—ã–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 10 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–∞—Ä–∞—Å—Ç–∞–Ω–∏—è –∏ —Å–ø–∞–¥–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è, —Å –ø–∏–∫–æ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 0.0003, –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –≤–µ—Å–æ–≤—ã–º —Ä–∞—Å–ø–∞–¥–æ–º 0.1 –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º –±–∞—Ç—á–∞ 5760. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø—É—Ç–µ–º —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–∞–ø–∞ –Ω–∞—Ä–∞—Å—Ç–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏. –ü–æ—Å–ª–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±—ã–ª –ø—Ä–æ–≤–µ–¥–µ–Ω –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π —ç—Ç–∞–ø –¥–æ–æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å 4 —Ç—ã—Å—è—á –¥–æ 16 —Ç—ã—Å—è—á —Ç–æ–∫–µ–Ω–æ–≤.\n\n–ü–æ—Å–∫–æ–ª—å–∫—É –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ \"0-shot\", —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ç–≤–µ—Ç–∞ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, simple-evals, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–º–µ—Å—å –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è –∏/–∏–ª–∏ few-shot –ø–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ, –¥–ª—è MMLU (5-shot), MMLU-pro –∏ ARCC (1-shot) –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ—Ü–µ–Ω–∫–∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –î–ª—è TriviaQA (TQA), MBPP, MATH –∏ GSM8k –ø—Ä–∏–º–µ–Ω—è–ª–∏—Å—å 1, 3, 4 –∏ 8 few-shot –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è —Ñ–æ—Ä–º–∞—Ç–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ—à–µ–Ω–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ 3. –í —Ç–∞–±–ª–∏—Ü–µ 2 –ø–æ–∫–∞–∑–∞–Ω–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ phi-4 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –µ—ë –ø—Ä–µ–¥—à–µ—Å—Ç–≤–µ–Ω–Ω–∏—Ü–µ–π, phi-3-medium.\n"
            },
            {
                "title": "Data Composition in Pretraining",
                "content": "The phi-3 model family were trained using two-phase strategy. Most of the training tokens were used in phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with data mixture consisting primarily of synthetic tokens and much smaller allocation for ultra-filtered and reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed marginal drop in the benefit from using non-synthetic tokens for the phi-3 family of model sizes. We note two key observations. Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs over our synthetic data led to better performance with respect to adding fresh web tokens. Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and demonstrated increased hallucinations. Figure 2 demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In this example, we conduct two training runs per model scale, using the same number of training tokens on top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed (a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and 12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen, performing more iterations on the synthetic data is more beneficial than supplying more web tokens. Inspired by this scaling behavior of our synthetic data, we trained 13B parameter model solely on synthetic4 data, for ablation purposes only the model sees over 20 repetitions of each data source. For the sake of ablations, we partitioned our synthetic data into web rewrites, which includes more direct rewrites of our filtered web content relative to all other types of synthetic data. Table 3 compares the previous phi-3-medium model with the new model trained entirely on the synthetic data. Throughout training, all benchmarks consistently improved, despite the increase in epochs, and the majority of the benchmarks showed improvements over phi-3. However, knowledge-related benchmarks, like 1-shot triviaqa (TQA), show large gap where synthetic models are subpar. These observations led us to rethink the role of web data in our data mixture. 4This is an updated mixture of synthetic data that contains new sources compared to phi-3. 8 Figure 2: 5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are trained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens. We see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch models perform better than those that have seen more unique web tokens. MMLU MMLU pro GSM8k Human-Eval ARCC MBPP MATH TQA Synthetic Synthetic + Web Rewrites +0.8 +0.3 +4.0 +4.1 +2.2 +1.8 +12.1 +13.3 0.0 +3.0 +5.0 +7. +4.9 +8.1 -14.8 -7.7 Table 3: Benchmark performance of 13B models (used for ablations only) trained on data mixtures containing no web data. The respective training tokens are either from synthetic sources, or an equal share of synthetic data and web rewrites. All numbers are reported relative to the performance of phi-3-medium, which has seen combination of web and synthetic data.",
                "summary": "–°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π phi-3 –æ–±—É—á–∞–ª–æ—Å—å –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞. –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –±–æ–ª—å—à–∞—è —á–∞—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª–∏ —Å–æ–±–æ–π –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ. –í—Ç–æ—Ä–æ–π —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è –ø—Ä–æ—Ö–æ–¥–∏–ª –Ω–∞ —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö, —Å–æ—Å—Ç–æ—è—â–µ–π –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, —Å –≥–æ—Ä–∞–∑–¥–æ –º–µ–Ω—å—à–µ–π –¥–æ–ª–µ–π —É–ª—å—Ç—Ä–∞-–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö.\n\n–ü–æ –º–µ—Ä–µ —Ä–æ—Å—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞—à–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º—ã –Ω–∞–±–ª—é–¥–∞–ª–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ–ª—å–∑—ã –æ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π phi-3. –ë—ã–ª–∏ —Å–¥–µ–ª–∞–Ω—ã –¥–≤–∞ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è: –≤–µ–±-–¥–∞–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞–ª–∏ –Ω–µ–±–æ–ª—å—à—É—é –ø–æ–ª—å–∑—É –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–∞–≤–∞–ª–∞ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–±-—Ç–æ–∫–µ–Ω–æ–≤.\n\n–ú–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π, –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∏ —É—Å–∏–ª–µ–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π. –†–∏—Å—É–Ω–æ–∫ 2 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–≤–æ–µ —è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –Ω–µ–±–æ–ª—å—à–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ. –í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å –¥–≤–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–≤–µ—Ä—Ö –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ. –î–ª—è –≤—Å–µ—Ö –∑–∞–ø—É—Å–∫–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –±—ã–ª–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º (–ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö), –Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—è–ª–æ—Å—å, –∞ –∏–º–µ–Ω–Ω–æ 4 –∏ 12 —ç–ø–æ—Ö. –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –±—ã–ª–∏ –Ω–æ–≤—ã–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–º–∏ –∏–∑ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –ö–∞–∫ –≤–∏–¥–Ω–æ, –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ, —á–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–µ–±-—Ç–æ–∫–µ–Ω–æ–≤.\n\n–í–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –Ω–∞—à–∏—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –º—ã –æ–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å —Å 13 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏ —ç—Ç–æ–º –¥–ª—è —Ü–µ–ª–µ–π –∞–±–ª—è—Ü–∏–∏ –º–æ–¥–µ–ª—å –≤–∏–¥–µ–ª–∞ –±–æ–ª–µ–µ 20 –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è —Ü–µ–ª–µ–π –∞–±–ª—è—Ü–∏–∏ –º—ã —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –Ω–∞—à–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ \"–ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ\", –∫–æ—Ç–æ—Ä—ã–µ –≤–∫–ª—é—á–∞—é—Ç –±–æ–ª–µ–µ –ø—Ä—è–º—ã–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –Ω–∞—à–µ–≥–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–∞, –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –≤—Å–µ—Ö –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n\n–í —Ç–∞–±–ª–∏—Ü–µ 3 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∞—è –º–æ–¥–µ–ª—å phi-3-medium —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é, –æ–±—É—á–µ–Ω–Ω–æ–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤—Å–µ —Ç–µ—Å—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–ª–∏—Å—å, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö, –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –ø–æ–∫–∞–∑–∞–ª–∏ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å phi-3. –û–¥–Ω–∞–∫–æ —Ç–µ—Å—Ç—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ 1-shot triviaqa (TQA), –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –±–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤, –≥–¥–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç —Ö—É–∂–µ. –≠—Ç–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∑–∞—Å—Ç–∞–≤–∏–ª–∏ –Ω–∞—Å –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–∏—Ç—å —Ä–æ–ª—å –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –≤ –Ω–∞—à–µ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö.\n\n*–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ê–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ –≤—ã—è—Å–Ω–∏–ª–∏, —á—Ç–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π phi-3 –±–æ–ª—å—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω–æ, —á–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö. –û–¥–Ω–∞–∫–æ, –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ–∫–∞–∑–∞–ª–∏ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π.*\n"
            },
            {
                "title": "Data Mixture",
                "content": "To design our pretraining data mixture for given training token budget, we search over different allocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites5, 3) filtered web (divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g., academic data, books, and forums), and 5) code data. We conducted ablations using shorter token horizon of 1T tokens to derive the data mixture. These ablations rely on our established result on the high-rank correlation of short training with longer training, up to the over-fitting saturation threshold of data sources. In addition we observe high rank correlation between the performance of the 7B and 14B models on different data mixtures, given large enough distance between the data mixtures. This allowed us to conduct the experiments at 7B scale and transfer the findings to phi-4. Among the numerous ablations, we highlight few that show best insights on our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions and code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters. Table 4 summarizes the results for the hand-picked ablations, as compared with the data mixture that was used for the final training run. uniform allocation of tokens among the three categories is suboptimal due to the higher quality of synthetic data and the only benchmark that shows clear benefit from web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally better than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy filtered web data sources to improve knowledge benchmarks (see Section 3.1) to balance all model 5Web rewrites is sub-category of synthetic data that is substantially large and contains direct rewrites of web content. 9 MMLU MATH GSM8k Human-Eval ARCC MBPP TQA MMLU pro Average Uniform + WR + -3.3 +3.3 +0.6 -0.6 -5.4 +4.0 +1.2 -0.7 -5.8 +2.1 +1.5 -0.7 -1.2 -6.1 -1.2 -4.3 +0.6 +1.9 +1.6 +0. -2.0 +0.4 +1.6 -2.0 +3.3 -3.0 -3.7 +6.9 -3.6 +3.7 +1.2 +0.9 -2.2 +0.8 +0.4 0.0 Table 4: Ablations on the allocation of 75% of training tokens to synthetic (S), filtered web (W), and web rewrite (WR) categories, while other data sources are held constant in the remaining 25% token budget. All benchmark numbers are measured relative to the final data mixture used for training phi-4. capabilities. We also note that we observed the gap between the chosen data mixture and the synthetic heavy runs largely closes as the model goes through the post-training stage. An end-to-end optimization of pretraining data mixture that also takes into account the effects of post-training is an interesting future area of investigation. Data Source Fraction of Training Unique Token Count Number of Epochs Web Web rewrites Synthetic Code data Acquired sources 15% 15% 40% 20% 10% 1.3T 290B 290B 820B 580B 1.2 5.2 13.8 2.4 1.7 Table 5: Data mixture for pretraining. The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data (mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books. In terms of total number of unique tokens in each data mixture cluster, filtered web data is the largest cluster with 1.3T tokens. Code and targeted acquisitions are the second and third largest clusters with 820B and 580B tokens, respectively. Finally, web rewrites and synthetic data have similar token count of 290B tokens. The total number of epochs on each data source is determined using the ratio of allocated tokens in the mixture and the number of unique tokens in that source.",
                "summary": "–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –º—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. –≠—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤–∫–ª—é—á–∞—é—Ç: 1) —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ, 2) –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç, 3) –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–± (—Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–π –Ω–∞ —á–∞—Å—Ç–∏, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∑–Ω–∞–Ω–∏—è), 4) —Ü–µ–ª–µ–≤—ã–µ –∏ –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã, –∫–Ω–∏–≥–∏ –∏ —Ñ–æ—Ä—É–º—ã) –∏ 5) –¥–∞–Ω–Ω—ã–µ –∫–æ–¥–∞.\n\n–î–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –º—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–µ–Ω—å—à–∏–º –æ–±—ä–µ–º–æ–º —Ç–æ–∫–µ–Ω–æ–≤ (1 —Ç—Ä–∏–ª–ª–∏–æ–Ω). –≠—Ç–æ –±—ã–ª–æ –≤–æ–∑–º–æ–∂–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è —Ç–æ–º—É, —á—Ç–æ –º—ã —Ä–∞–Ω–µ–µ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö, –≤–ø–ª–æ—Ç—å –¥–æ –Ω–∞—Å—ã—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã –∑–∞–º–µ—Ç–∏–ª–∏ –≤—ã—Å–æ–∫—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –º–µ–∂–¥—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –º–æ–¥–µ–ª–µ–π —Å 7 –∏ 14 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —ç—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –Ω–∞–º –ø—Ä–æ–≤–æ–¥–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–æ–¥–µ–ª–∏ —Å 7 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –º–æ–¥–µ–ª—å phi-4.\n\n–°—Ä–µ–¥–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤, –º—ã –≤—ã–¥–µ–ª–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ, –∫–æ—Ç–æ—Ä—ã–µ –¥–∞–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ü–µ–Ω–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–æ—Å—Ç–∞–≤–µ –¥–∞–Ω–Ω—ã—Ö. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –º—ã –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–ª–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Ü–µ–ª–µ–≤—ã—Ö –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–∏–π –∏ –∫–æ–¥–∞, –∞ –∑–∞—Ç–µ–º –∏–∑–º–µ–Ω—è–ª–∏ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–µ–±-–¥–∞–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n\n–¢–∞–±–ª–∏—Ü–∞ 4 –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É —Ç—Ä–µ–º—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –æ–∫–∞–∑–∞–ª–æ—Å—å –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∏–∑-–∑–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç, –≥–¥–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞–ª–∏ —è–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ, ‚Äî —ç—Ç–æ TQA. –•–æ—Ç—è –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø—Ä–µ–æ–±–ª–∞–¥–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Å—Ç—Ä–æ–∫–∏ 2 –∏ 3 —Ç–∞–±–ª–∏—Ü—ã) –±—ã–ª–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ, —á–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä, –º—ã —Ä–µ—à–∏–ª–∏ –≤–∫–ª—é—á–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ –∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∑–Ω–∞–Ω–∏—è –≤–µ–±-–¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–æ –∑–Ω–∞–Ω–∏—è–º–∏, –∏ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –ø–æ –º–µ—Ä–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —ç—Ç–∞–ø–∞ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —è–≤–ª—è–µ—Ç—Å—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–π —Ç–µ–º–æ–π –¥–ª—è –±—É–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.\n\n–§–∏–Ω–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç 30% —Ç–æ–∫–µ–Ω–æ–≤ –º–µ–∂–¥—É –≤–µ–±-–¥–∞–Ω–Ω—ã–º–∏ –∏ –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–º –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–ø–æ—Ä–æ–≤–Ω—É). –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Å—Ç—É–ø–∞—é—Ç –∏–∑ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (40%). –¢–∞–∫–∂–µ 20% —Ç–æ–∫–µ–Ω–æ–≤ –≤—ã–¥–µ–ª–µ–Ω–æ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∫–æ–¥–∞ (—Å–º–µ—Å—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞) –∏ 10% –¥–ª—è —Ü–µ–ª–µ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã –∏ –∫–Ω–∏–≥–∏).\n\n–ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–±-–¥–∞–Ω–Ω—ã–µ —è–≤–ª—è—é—Ç—Å—è —Å–∞–º—ã–º –±–æ–ª—å—à–∏–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º (1.3 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤). –ö–æ–¥ –∏ —Ü–µ–ª–µ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–Ω–∏–º–∞—é—Ç –≤—Ç–æ—Ä–æ–µ –∏ —Ç—Ä–µ—Ç—å–µ –º–µ—Å—Ç–∞ (820 –∏ 580 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ). –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –≤–µ–±-–∫–æ–Ω—Ç–µ–Ω—Ç –∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (290 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤). –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –≤—ã–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ.\n"
            },
            {
                "title": "Midtraining Details",
                "content": "phi-4 includes midtraining stage where the context length is increased from the original 4K to 16K. We conduct several ablations to study the role of data on long-context performance. Specifically, we try data sources that are inherently long context, and compare them with artificially created long context data where samples are padded together to fill the sequence. We observe the former to perform better in longer context tasks. Inspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and code data) to separate samples above 8K context. We then up-weight the data subsets that are 16K or higher in length. We also create new synthetic datasets that satisfy the > 4K sequence requirement. The final data mixture includes 30% of the newly curated longer context data and 70% portion of recall 10 Model Max Length Recall RAG ICL Re-rank QA Summ phi-4 Qwen-2.5-14B Llama-3.3-70B GPT-4o-mini GPT-4o phi-4 Qwen-2.5-14B Llama-3.3-70B GPT-4o-mini GPT-4o 8K 8K 8K 8K 8K 16K 16K 16K 16K 16K 100.0 100.0 92.0 99.2 100.0 99.0 100.0 92.0 100.0 100.0 58.1 62.2 65.3 65.8 66. 57.1 59.1 62.2 63.6 66.7 68.0 67.8 69.4 74.4 83.0 77.0 67.6 70.0 78.4 85.6 65.3 58.2 64.4 69.4 75.1 54.4 50.3 63.3 63.9 73.8 26.7 24.7 30.0 31.3 37. 36.0 29.7 36.7 36.0 43.7 38.3 37.2 37.8 38.5 43.0 40.5 42.3 41.9 45.2 46.3 Table 6: Evaluation results on the long-context benchmark HELMET [YGH+24]. tokens from the pretraining stage. To accommodate longer context, we increase the base frequency of rope position encoding to 250K following [AI23b]. We drop the maximum learning rate by factor of 10 compared to the pretraining stage and train for total of 250B tokens. To effectively evaluate the long-context capability of our model, it is essential to have comprehensive evaluation framework with practical scenarios. While synthetic benchmarks like needle-in-a-haystack and RULER are preferred for their simplicity and control, our emphasis is on diverse range of tasks that reflect real-world applications, such as reasoning across entire documents. We report the performance of phi-4 and other models on the tasks we selected from the HELMET [YGH+24] evaluation suite in Table 6 and outline our evaluation methods below. Note that results are average across 5 runs for each categories. Recall: The task involves retrieving the corresponding value from randomly-generated long JSON file given specific key (Metric: SubEM) RAG: Answer questions based on many retrieved and shuffled Wikipedia documents. The datasets used for this task are NaturalQuestions, HotpotQA, and PopQA. Final results are average of all datasets (Metric: SubEM) Re-rank: The task is to re-rank the top-10 documents given query and many retrieved and shuffled documents. This uses the MSMARCO dataset (Metric: nDCG@10) ICL: The task involves many-shot in-context learning with datasets such as TREC coarse, TREC fine, Banking77, NLU and CLINC150. Final results are average of all datasets (Metric: F1) QA: Answer questions given lengthy document. The dataset associated with this task is NarrativeQAv2 (Metric: GPT-4o scoring) Summ: The task involves summarizing lengthy legal document, and the dataset used is MultiLexSum (Metric: GPT-4o scoring) Dataset Name Sample Count unknown + safety data generic multiple-choice Q&A math data python data cpp, go, java, js, rust data 3,000 132,859 76,552 16,080 21,806 Table 7: Data Mixture for Pivotal Token DPO",
                "summary": "**–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ phi-4**\n\n–í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω —ç—Ç–∞–ø, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã—Ö 4K —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ 16K —Ç–æ–∫–µ–Ω–æ–≤. –ß—Ç–æ–±—ã –∏–∑—É—á–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –±—ã–ª–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–º–µ—é—â–∏–µ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ç–∞–∫ –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –≥–¥–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –±—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ, –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏–º–µ—é—â–∏–µ –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.\n\n–û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —ç—Ç–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏—è—Ö, –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–µ—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö (–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã, –∫–Ω–∏–≥–∏ –∏ –∫–æ–¥) –±—ã–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã, —á—Ç–æ–±—ã –≤—ã–¥–µ–ª–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–ª–∏–Ω–æ–π –±–æ–ª–µ–µ 8K —Ç–æ–∫–µ–Ω–æ–≤. –ó–∞—Ç–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª–∏–Ω–æ–π 16K –∏ –±–æ–ª–µ–µ –±—ã–ª–∏ —É—Å–∏–ª–µ–Ω—ã. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –±—ã–ª–∏ —Å–æ–∑–¥–∞–Ω—ã –Ω–æ–≤—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ > 4K. –í –∏—Ç–æ–≥–æ–≤–æ–π —Å–º–µ—Å–∏ –¥–∞–Ω–Ω—ã—Ö 30% –∑–∞–Ω–∏–º–∞–ª–∏ –Ω–µ–¥–∞–≤–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –∞ 70% - –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–≤—à–∏–µ—Å—è –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n\n–î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è RoPE –±—ã–ª–∞ —É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ 250K. –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∞ —Å–Ω–∏–∂–µ–Ω–∞ –≤ 10 —Ä–∞–∑ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ø–æ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Å—Ç–∞–≤–∏–ª–æ 250 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤.\n\n**–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –¥–ª–∏–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ**\n\n–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–ª–∏–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –Ω–∞–±–æ—Ä –∑–∞–¥–∞—á, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Ä–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –•–æ—Ç—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ \"–∏–≥–æ–ª–∫–∞ –≤ —Å—Ç–æ–≥–µ —Å–µ–Ω–∞\", –ø—Ä–æ—Å—Ç—ã –∏ —É–¥–æ–±–Ω—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è, –æ—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–ª–æ—Å—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –∏–º–∏—Ç–∏—Ä—É—é—â–∏–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ª—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –Ω–∞ –∑–∞–¥–∞—á–∞—Ö, –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–∞–±–æ—Ä–∞ HELMET.\n\n–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–¥–∞—á–∞–º:\n\n*   **Recall:** –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ JSON-—Ñ–∞–π–ª–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∫–ª—é—á—É.\n*   **RAG (Retrieval-Augmented Generation):** –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏ –ø–µ—Ä–µ–º–µ—à–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Wikipedia.\n*   **Re-rank:** –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ 10 –ª—É—á—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É.\n*   **ICL (In-Context Learning):** –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö TREC, Banking77, NLU –∏ CLINC150.\n*   **QA (Question Answering):** –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–ª–∏–Ω–Ω–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É.\n*   **Summ (Summarization):** –æ–±–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ 6 –∏ —è–≤–ª—è—é—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ 5 –∑–∞–ø—É—Å–∫–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.\n\n**–°–æ—Å—Ç–∞–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO**\n\n–í —Ç–∞–±–ª–∏—Ü–µ 7 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Å–æ—Å—Ç–∞–≤ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è DPO (Direct Preference Optimization) –º–æ–¥–µ–ª–∏. –î–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Ç–∞–∫–∏–µ –∫–∞–∫:\n\n*   –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–∞–Ω–Ω—ã–µ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n*   –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º\n*   –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n*   –î–∞–Ω–Ω—ã–µ –Ω–∞ —è–∑—ã–∫–µ Python\n*   –î–∞–Ω–Ω—ã–µ –Ω–∞ —è–∑—ã–∫–∞—Ö C++, Go, Java, JavaScript –∏ Rust\n\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–∞–∫–∂–µ —É–∫–∞–∑–∞–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ.\n"
            },
            {
                "title": "Direct Preference Optimization",
                "content": "We use DPO [RSM+23] to align the model with human preferences, and also to steer the model away from unwanted behavior through pairs of desired and undesired outputs. DPO data covers chat format data, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning, robustness, and safety. We do two rounds of DPO on the SFT model. We introduce technique, Pivotal Token Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture for first round are provided in Table 7. For the second round, which we call judge-guided DPO, we gather approximately 850k pairs of desired and undesired outputs. The prompts are sourced from various publicly available instruction tuning datasets and also include prompts related to safety and Responsible AI (RAI). Next, for each of these prompts, we generate responses from GPT-4o, GPT-4t and our model. From these responses, we create various combinations of DPO pairs and use GPT-4o as judge to label positive or negative for given pair. For given pair of responses, each assistant response is given score based on accuracy, style, and detail. We label the response with higher accuracy or overall (average of accuracy, style, and detail) score as the positive response. We provide the prompt we used in Appendix A. The data mixture for this round is provided in Table 8. Both stages also include small amount of data for safety and mitigating hallucinations. 12 Suppose that the roots of x3 + 3x2 + 4x 11 = 0 are a, b, and c, and that the roots User: of x3 + rx2 + sx + = 0 are + b, + c, and + a. AI: To solve the problem, we need to dealdealdealdealdeal dealdealdealdealdeal with the roots of the two cubic equations deal deal deal deal deal deal deal First, consider the cubic polynomial polynomial polynomial polynomial polynomial polynomial polynomial given. polynomial ( xÀÜ3 + 3xÀÜ2 + 4x - 11 = 0 ), polynomial polynomial polynomial polynomial polynomial polynomial polynomial polynomial polynomial By Vietas formulas, we have: [ + with roots ( ), ( ), and ( ). + = -3, ] [ ab + bc + ca = 4, ] [ abc = 11. ] Now, consider the other other other other other other other other other other other other other other other other other polynomial ( xÀÜ3 + rxÀÜ2 + sx + = 0 ) with roots ( a+b ), ( b+c ), and ( c+a ). We need to find the value of ( ), which corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds to the negative corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds corresponds Find t. of 0.42 0.93 according to Vietas formulas::::::::::::::::: [ the product of the roots according according according according according according according according according according according according according according according according (a 0.95 0.71 +b)(b+c)(c+a). ] First notenotenotenotenote the important First, notenotenotenotenote important important First First important important First First important important First First important identity: [ (a+b)(b+c)(c+a) = (a+b+c)(ab+bc+ca) - important important First First note important important note First First note important important note First First note important important note First First important note First abc. ] PlugPlugPlugPlugPlug known values from ourourourourourourourourourourourourourourourourour Vietas results forforforforforforforforforforforforforforforforfor ( ), ( ), and PlugPlugPlugPlugPlugging in known known known known known known known known known known known Plug Plug known known Plug Plug known known Plug Plug known Plug ( ), we have: Substitute these into the identity: [ (a+b)(b+c)(c+a) = (-3)(4) - 11 = -12 - 11 = -23. ] Thus, ( = -(a+b)(b+c)(c+a) = 23 ). Therefore, the value of ( ) is (boxed23). [ +++++++++++++++++ + = -3, ] [ ab + bc + ca = 4, ] [ abc = 11. ] ) c ( 1 0 Figure 3: Illustration of pivotal tokens for GPT-4o at temperature 1 on problem from the MATH benchmark [HBK+21], where the initial success probability is 0.31. Each token is colorized by the probability of success for an independent completion (N = 529) continued from after the token, with red for p(success) = 0 and blue for p(success) = 1. The line plot shows the same probabilities. The tokens that changes p(success) by 0.2 are shown boxed , with subscripts showing the change in probability. Tokens with probability 0.1 are underlined to illustrate that pivotal tokens are distinct from low-probability tokens. The token probabilities of negative and (a were 0.31 and 0.12, respectively. The greedy tokens for the same prefixes are product with 0.66 probability and with 0.88 probability. procedure PivotalTokenSearch(Q, Tfull, pgap) procedure Subdivide(Tprefix, ) if 1 or p(success Tprefix) p(success Tprefix + ) < pgap then Base cases. return [T ] Tleft, Tright Split(T ) return Subdivide(Tprefix, Tleft) Subdivide(Tprefix + Tleft, Tright) We split at the cumulative midpoint of token log probabilities. Tprefix œµ for all Subdivide(œµ, Tfull) do if = 1 and p(success Tprefix) p(success Tprefix + ) pgap then yield (Q, Tprefix, ) Tprefix Tprefix + Output pivotal tokens and context for postprocessing. Figure 4: Pseudocode for Pivotal Token Search (PTS). Note that estimating p(success . . . ) involves sampling the language model and invoking the oracle. In an efficient implementation p(success . . . ) should be memoized.",
                "summary": "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ DPO (Direct Preference Optimization). –î–∞–Ω–Ω—ã–µ –¥–ª—è DPO –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Ñ–æ—Ä–º–∞—Ç—ã —á–∞—Ç–æ–≤, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ –¥–∞–Ω–Ω—ã–µ, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –ò–ò (RAI). –≠—Ç–æ —É–ª—É—á—à–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è—Ö, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n\n–û–±—É—á–µ–Ω–∏–µ DPO –ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞ –ø–æ–≤–µ—Ä—Ö –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π —Å —É—á–∏—Ç–µ–ª–µ–º (SFT). –ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ DPO –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ—Ö–Ω–∏–∫–∞ Pivotal Token Search (PTS) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ä –∂–µ–ª–∞–µ–º—ã—Ö –∏ –Ω–µ–∂–µ–ª–∞–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è DPO —Å –æ—Ü–µ–Ω–∫–∞–º–∏, —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è –æ–∫–æ–ª–æ 850 —Ç—ã—Å—è—á –ø–∞—Ä –∂–µ–ª–∞–µ–º—ã—Ö –∏ –Ω–µ–∂–µ–ª–∞–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ü–æ–¥—Å–∫–∞–∑–∫–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏, –∞ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞—é—Ç –ø–æ–¥—Å–∫–∞–∑–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –ò–ò (RAI).\n\n–ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–¥—Å–∫–∞–∑–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –æ—Ç–≤–µ—Ç—ã –æ—Ç GPT-4o, GPT-4t –∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –ò–∑ —ç—Ç–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ–∑–¥–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –ø–∞—Ä –¥–ª—è DPO, –∏ GPT-4o –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å—É–¥—å–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π. –ö–∞–∂–¥–æ–º—É –æ—Ç–≤–µ—Ç—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Å—Ç–∏–ª—è –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏. –û—Ç–≤–µ—Ç —Å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é –∏–ª–∏ –æ–±—â–µ–π (—Å—Ä–µ–¥–Ω–µ–π) –æ—Ü–µ–Ω–∫–æ–π —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º. –û–±–∞ —ç—Ç–∞–ø–∞ —Ç–∞–∫–∂–µ –≤–∫–ª—é—á–∞—é—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\n\n–í —Å—Ç–∞—Ç—å–µ —Ç–∞–∫–∂–µ –ø—Ä–∏–≤–µ–¥—ë–Ω –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è PTS, –≥–¥–µ –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç \"–∫–ª—é—á–µ–≤—ã–µ\" —Ç–æ–∫–µ–Ω—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏. –ù–∞ —Ä–∏—Å—É–Ω–∫–µ 3 –ø–æ–∫–∞–∑–∞–Ω–æ, –∫–∞–∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –æ—Ç–≤–µ—Ç–∞ –º–µ–Ω—è–µ—Ç—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–∫–µ–Ω–∞, –∞ —Ç–∞–∫–∂–µ –≤—ã–¥–µ–ª–µ–Ω—ã —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ —ç—Ç—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å. –†–∏—Å—É–Ω–æ–∫ 4 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Å–µ–≤–¥–æ–∫–æ–¥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ PTS.\n"
            },
            {
                "title": "Pivotal Token Search",
                "content": "Consider generative model producing token-by-token response to given prompt. For each token produced, which corresponds to prefix of the model response, one can consider the conditional probability of the models answer being correct given that prefix, as well as the increment in this probability with respect to that token (in other words, the difference in the probability of being correct before and after producing that token). It is often the case that the overall correctness is highly dependent on successful generation of small number of key tokens. For example, we can see in Figure 3 where the model outputs math solution and fortunate sampling of crucial token negative shifts the solution from possible failure to likely success, while sampling of the token (a subsequently risks failure again. We refer to these tokens as pivotal tokens as they have an outsized effect on the course of the solution. Now, consider how the solution from Figure 3 would be used in DPO as full-length accepted response. As the figure shows, there are many tokens with probabilities much lower than the 0.31 of negative , which would contribute to noise in the gradients diluting the signal from the pivotal token. Even worse, the token (a that contributed to the lack of robustness would receive strong positive learning signal thanks to its low probability of 0.12. Moreover, intuition suggests that when two texts substantially deviate from each other, comparison of their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it makes more sense that the signal should come from the first tokens after the two texts starts diverging from each other. To alleviate these effects, we employ method we call Pivotal Token Search (PTS) for generating preference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the preference optimization takes effect with respect to single token. PTS identifies points of completion token sequence Tfull = t1, t2, . . . for some user query where the next token ti has significant impact on the probability of success p(success t1, . . . , ti). PTS estimates these probabilities by sampling completions starting from + t1, . . . , ti, which are checked for correctness with an oracle6 for Q. Figure 4 shows basic instantiation of the algorithm. The procedure Subdivide recursively splits the sequence into segments ti, . . . , tj until the change in probability p(success t1, . . . , ti1) p(success t1, . . . , tj) for each segment is below threshold pgap or the segment is just single token. Tokens with sharp change in success probability are kept as pivotal. We turn pivotal tokens into preference data by taking + t1, . . . , ti1 as the query, and single tokens tacc and trej that increase/decrease p(success t1, . . . , ti1, tacc/rej) as the accepted and rejected completions, respectively.7 The binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but it only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the course of the solution. We used PTS to generate preference data for tasks where ground-truth is readily available, such as mathematics, various forms of question answering and coding. To improve sample efficiency, we filter the target questions to only include those with 0.2 p(success) 0.8, as pivotal tokens are rare for tasks that are very easy or hard. See Figure 5 for examples of preference data we generated using PTS. The math question answering example shows how pivotal tokens often are not actual mistakes, but choices that drive the model down less advantageous path. Here, multiplying both sides by the denominators separately is equally valid to directly cross-multiplying, but for the model doing the latter here is more robust. By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially 6For coding, comprehensive test suite can be used. For math problems, answers can be compared to the ground truth. 7We find drawing tacc and trej from rollouts PTS already used to estimate p(success t1, . . . , ti1) to be effective. 14 Find the product of all real values of for which 1 User: solution. AI: To solve the problem, we need to determine the values of such that the equation 2x = rx has exactly one real 7 has exactly one real solution for x. First, clear the fractions by Good: cross-multiplying Bad: multiplying both sides by 1 2x = 7 Adopt the axial channeling approximation to model the interaction Consider 1 keV helium atom impinging at one-degree incidence angle on LiF User: crystal surface. between the helium atom and the crystal as 2D potential energy landscape. Newtons equations to compute the helium atoms trajectory and determine the subsequent momentum transfers from binary collisions with surface atoms. energy transferred across all collisions and compute the probability, Pe, that the helium atom remains elastically scattered assuming vibrational frequency for surface atoms of œâ = 1 1014Hz. 1. Pe = exp( Eloss AI: To determine the probability Pe that the helium atom remains elastically scattered, we need to calculate the total virtual energy transferred, Eloss, during the helium atoms interaction with the LiF crystal surface. Given Good: What is Pe? 2hœâ ), 2. Pe = exp( Eloss hœâ ), or 4. Pe = 1 exp( Eloss hœâ ) Bad: that the potential interactions hœâ ), 3. Pe = exp( Eloss the vibrational frequency Sum the total virtual Apply User: function described. Read the following function signature and docstring, and fully implement the Your response should only contain the code for this function. def reorder_sequence(n: int, sequence: list[int]) -> list[int]: \"\"\" Given sequence of integer numbers that Polycarp wrote down on the board, this function rearranges (reorders) elements of this sequence to match the order Polycarp could have possibly written them. Each next number should be exactly two times the previous number or exactly one third of the previous number. :param n: The number of elements in the sequence (2 <= <= 100). :param sequence: list of integer numbers in arbitrary order (1 <= elements <= 3 * 10ÀÜ18). :return: reordered list of integer numbers representing valid sequence Polycarp could have written. \"\"\" AI: ... from collections import defaultdict # Create adjacency list where each number can point to its possible predecessor and successors graph = defaultdict(list) for num in sequence: for other in sequence: if num != other: Good: other % if num == other * 2 or ( Bad: num * 2 Figure 5: Preference data generated by Pivotal Token Search in answering math and physics questions, and implementing function in Python. The tokens that form the actual pair for DPO are underlined. 15 Figure 6: The post-training process described in Appendix A.1 decreases hallucinations. One measure is that the problems in SimpleQAwhich the model very rarely gets correctare increasingly not attempted during the course of post-training. We believe the final result is better behavior, even though the simple-evals score for SimpleQA (the F1 score) actually gives our base model higher score than our final model. stronger. In [LLX+24] contrastive estimation approach involving model trained on incorrect Related Work: trajectories is used to score which tokens likely contributed to failure, which is further employed to weigh rejected responses in DPO. In comparison, our PTS avoids complications from learned proxies by directly estimating p(success). They also report difficulties applying their method to accepted responses in DPO, while our method generates both positive and negative preference data directly targeting pivotal tokens. Automated process supervision methods [WLS+24, LLL+24] have applied search and rollouts to generate data for training process reward models. PTS can be seen as an automated process supervision method that generates token-level preference data suitable for DPO.",
                "summary": "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –º–µ—Ç–æ–¥ Pivotal Token Search (PTS), –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—É—Ç–µ–º –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.\n\n**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–µ —Ç–µ–∫—Å—Ç –ø–æ —Ç–æ–∫–µ–Ω–∞–º, —á–∞—Å—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –∏—Ö –æ–±—â–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–µ–±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ \"–∫–ª—é—á–µ–≤—ã—Ö\" —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã, –Ω–∞–∑–≤–∞–Ω–Ω—ã–µ \"–æ–ø–æ—Ä–Ω—ã–º–∏\" (pivotal), –º–æ–≥—É—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ —Ö–æ–¥ —Ä–µ—à–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –∑–∞–¥–∞—á–µ –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –≤—ã–±–æ—Ä –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–æ–π –≤—ã–±–æ—Ä –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –æ—à–∏–±–∫–µ. –ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–∞–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é DPO (Direct Preference Optimization), –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ–≥—É—Ç —Å–æ–∑–¥–∞—Ç—å \"—à—É–º\" –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö, —Ä–∞–∑–º—ã–≤–∞—è —Å–∏–≥–Ω–∞–ª –æ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, —Ç–æ–∫–µ–Ω—ã, –ø—Ä–∏–≤–æ–¥—è—â–∏–µ –∫ –æ—à–∏–±–∫–µ, –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π —Å–∏–≥–Ω–∞–ª –∏–∑-–∑–∞ —Å–≤–æ–µ–π –Ω–∏–∑–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n\n**–†–µ—à–µ–Ω–∏–µ: Pivotal Token Search (PTS)**. PTS ‚Äì —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—Ü–µ–ª–µ–Ω –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∏–º–µ–Ω–Ω–æ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤. –û–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n\n1.  **–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:** –ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é –≤ –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å. –û–Ω —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã, –ø–æ–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞ (p(success)) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–µ —Å—Ç–∞–Ω–µ—Ç –º–µ–Ω—å—à–µ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –Ω–µ –±—É–¥–µ—Ç —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. –¢–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–∑—ã–≤–∞—é—Ç —Ä–µ–∑–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞, –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∫–∞–∫ –æ–ø–æ—Ä–Ω—ã–µ. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—É—Ç–µ–º –≤—ã–±–æ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–π, –Ω–∞—á–∏–Ω–∞—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é \"–æ—Ä–∞–∫—É–ª–∞\" (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∑–∞–¥–∞—á –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, –∞ –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á ‚Äî —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º).\n2.  **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π:** –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø–æ—Ä–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ PTS —Å–æ–∑–¥–∞–µ—Ç –ø–∞—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è DPO. –ó–∞–ø—Ä–æ—Å ‚Äî —ç—Ç–æ –ø—Ä–µ—Ñ–∏–∫—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ –æ–ø–æ—Ä–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞. \"–ü—Ä–∏–Ω—è—Ç–æ–µ\" –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –æ–ø–æ—Ä–Ω—ã–π —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞, –∞ \"–æ—Ç–≤–µ—Ä–≥–Ω—É—Ç–æ–µ\" –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞. –≠—Ç–∏ —Ç–æ–∫–µ–Ω—ã –±–µ—Ä—É—Ç—Å—è –∏–∑ —Ç–µ—Ö –∂–µ –≤—ã–±–æ—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ PTS –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —É—Å–ø–µ—Ö–∞.\n\n**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ PTS:**\n\n*   **–¶–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:** PTS —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n*   **–£–º–µ–Ω—å—à–µ–Ω–∏–µ —à—É–º–∞:** PTS —É–º–µ–Ω—å—à–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ \"—à—É–º–∞\" –æ—Ç –Ω–µ–∫–ª—é—á–µ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é.\n*   **–£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤:** –ú–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ —Ç–æ–∫–µ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –æ—à–∏–±–∫–∞–º.\n*   **–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å:** PTS –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á, –≥–¥–µ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ).\n\n**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**\n\n*   –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –≤—Å–µ–≥–¥–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–ø–æ—Ä–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∏–∑ –Ω–∏—Ö, –µ—Å–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ —è–≤–ª—è–µ—Ç—Å—è –ø–æ—á—Ç–∏ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–π –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ —Ä–µ—à–µ–Ω–∏—è.\n\n**–£–ª—É—á—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:** –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏ PTS —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –≤–∫–ª—é—á–∞—è —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0.2 –¥–æ 0.8, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ä–µ–¥–∫–∏ –¥–ª—è –æ—á–µ–Ω—å –ª–µ–≥–∫–∏—Ö –∏–ª–∏ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.\n\n**–ü—Ä–∏–º–µ—Ä—ã:** –í —Å—Ç–∞—Ç—å–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é PTS –¥–ª—è –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ, —Ñ–∏–∑–∏–∫–µ –∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é. –≠—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —á–∞—Å—Ç–æ —è–≤–ª—è—é—Ç—Å—è –Ω–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏, –∞ –≤—ã–±–æ—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç –º–æ–¥–µ–ª—å –ø–æ –º–µ–Ω–µ–µ –≤—ã–≥–æ–¥–Ω–æ–º—É –ø—É—Ç–∏.\n\n**–°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏:** –ú–µ—Ç–æ–¥ PTS —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç—Å—è —Å –¥—Ä—É–≥–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏, –≤ —Ç–æ–º —á–∏—Å–ª–µ —Å –ø–æ–¥—Ö–æ–¥–æ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –∞ —Ç–∞–∫–∂–µ —Å –º–µ—Ç–æ–¥–∞–º–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —ç—Ç–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤, PTS –Ω–∞–ø—Ä—è–º—É—é –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π, –Ω–∞—Ü–µ–ª–µ–Ω–Ω—ã–µ –Ω–∞ –æ–ø–æ—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã.\n"
            },
            {
                "title": "Benchmarking Considerations",
                "content": "While academic benchmarks are widely used to measure the progress in LLM advancement, they suffer from several limitations that can fail to reveal models true capabilities and weaknesses. These limitations include: Data Contamination: Many benchmarks rely on datasets that overlap with pretraining corpora, creating risk of data contamination. Although we took extensive measures to deduplicate and 16 SFT DPO stage 1 DPO stage 2 only phi-4 (stage 1 + 2) v - m MMLU GPQA MATH HumanEval MGSM SimpleQA DROP 82.8 47.3 77.1 79.5 80.8 3.7 82.8 MMLUPro 61.9 HumanEval+ 77.9 56.7 66.2 ArenaHard IFEval 84.8 53.6 80.5 81.6 80.8 2.9 86.1 70.0 81.9 66.5 63.0 PhiBench (internal) 48. 54.5 84.2 52.4 77.6 81.5 81.5 2.9 71.8 67.2 81.4 69.8 63.0 53.0 84.8 56.1 80.4 82.6 80.6 3.0 75.5 70.4 82.8 75.4 63. 56.2 Table 9: Performance through the post-training process. DPO stage 1 is pivotal token DPO, and DPO stage 2 is more standard judge-guided DPO. Each also has 1-5% hallucination and safety data mixed in. decontaminate our training data, including standard n-gram deduplication and decontamination, these methods are not effective against all scenarios, including rephrasing, which leaves some uncertainty about the true extent of generalization. Limited Skill Scope: Most benchmarks evaluate models on narrowly defined skills, such as solving specific style of math problems at certain grade level or implementing isolated Python functions. This narrow scope can fail to capture models broader capabilities and weaknesses. Bias in Generation-Based Benchmarks: Some benchmarks use LLM-as-judge for evaluating generated outputs. These judgments sometimes may prioritize style, fluency, or surface-level qualities over accuracy and validity of the reasoning chain, leading to potential biases in scoring. Limitations of Multiple-Choice Tasks: Benchmarks that rely on multiple-choice questions often test models ability to make clever guesses that can be achieved by pattern matching rather than effectively utilizing the underlying concepts through reasoning. To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4s development. This benchmark was designed with the following goals: 1. Originality: All questions in the benchmark were composed by our team making sure that they were not present in our pretraining data. Our goal for the internal benchmark is to reveal models generalization ability in various domains. 2. Skill Diversity: Our benchmark includes wide range of tasks to assess multiple dimensions of model performance. For instance, in coding, it goes beyond isolated function implementation to include debugging, extending incomplete code, and explaining code snippets. Similarly, in 17 mathematics, it incorporates tasks like identifying the errors in proofs or generating related problems, rather than simply solving equations. This ensures that the benchmark captures broader spectrum of skills and reasoning processes. 3. Rigorous Scoring for Generation Tasks: For tasks requiring judgment of model-generated outputs, we addressed the common pitfalls of LLM-based scoring by carefully curating detailed judge instructions (or judge notes). These rubrics specify exactly how to evaluate responses, focusing on achieving accuracy, logical structure, and adherence to task requirements, while minimizing tendencies towards stylistic biases. We observed significantly improved consistency and reduction of adverse impact due to subjective preferences in the scoring outcomes. PhiBench played central role in optimizing phi-4. We used it to guide decisions about dataset mixtures and hyperparameter choices for more effective post-training techniques. PhiBench was also used to perform high-signal studies that identify weaknesses in the model and provide feedback for new incoming data sources.",
                "summary": "**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ PhiBench**\n\n–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏, —Ö–æ—Ç—è –∏ —à–∏—Ä–æ–∫–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM), –∏–º–µ—é—Ç —Ä—è–¥ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–µ –≤—ã—è–≤–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π. –≠—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤–∫–ª—é—á–∞—é—Ç:\n\n1.  **–ó–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** –ú–Ω–æ–≥–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è —Å –∫–æ—Ä–ø—É—Å–∞–º–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —Ä–∏—Å–∫ \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è\" –¥–∞–Ω–Ω—ã—Ö, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç \"–ø–æ–º–Ω–∏—Ç—å\" –æ—Ç–≤–µ—Ç—ã –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –∞ –Ω–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–∏–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ. –•–æ—Ç—è –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –º–µ—Ä—ã –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é n-–≥—Ä–∞–º–º, —ç—Ç–∏ –º–µ—Ç–æ–¥—ã –Ω–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã –ø—Ä–æ—Ç–∏–≤ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è, —á—Ç–æ –æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—É—é –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–∏–Ω–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é.\n\n2.  **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞–≤—ã–∫–æ–≤:** –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –º–æ–¥–µ–ª–∏ –ø–æ —É–∑–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º, —Ç–∞–∫–∏–º –∫–∞–∫ —Ä–µ—à–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ Python. –¢–∞–∫–æ–π —É–∑–∫–∏–π –æ—Ö–≤–∞—Ç –º–æ–∂–µ—Ç –Ω–µ –≤—ã—è–≤–∏—Ç—å –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –º–æ–¥–µ–ª–µ–π.\n\n3.  **–ü—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç LLM –≤ –∫–∞—á–µ—Å—Ç–≤–µ \"—Å—É–¥—å–∏\" –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠—Ç–∏ –æ—Ü–µ–Ω–∫–∏ –∏–Ω–æ–≥–¥–∞ –º–æ–≥—É—Ç –æ—Ç–¥–∞–≤–∞—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç–∏–ª—é, –±–µ–≥–ª–æ—Å—Ç–∏ —Ä–µ—á–∏ –∏–ª–∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–∞–º, –∞ –Ω–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ü–µ–ø–æ—á–∫–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º –≤ –æ—Ü–µ–Ω–∫–µ.\n\n4.  **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∑–∞–¥–∞—á —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º:** –ë–µ–Ω—á–º–∞—Ä–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –≤—ã–±–æ—Ä–æ–º, —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –¥–µ–ª–∞—Ç—å \"—É–º–Ω—ã–µ\" –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã –ø—É—Ç–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å –æ–±—Ä–∞–∑—Ü–æ–º, –∞ –Ω–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.\n\n–ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã, –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º PhiBench, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤ –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä—ã–µ –æ–∫–∞–∑–∞–ª–∏—Å—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º–æ–¥–µ–ª–∏ phi-4. –≠—Ç–æ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ü–µ–ª—è–º–∏:\n\n1.  **–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** –í—Å–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ –±—ã–ª–∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω—ã –∫–æ–º–∞–Ω–¥–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –¶–µ–ª—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ ‚Äî –≤—ã—è–≤–∏—Ç—å —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –∫ –æ–±–æ–±—â–µ–Ω–∏—é –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö.\n\n2.  **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –Ω–∞–≤—ã–∫–æ–≤:** –ë–µ–Ω—á–º–∞—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–Ω –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –≤–∫–ª—é—á–∞—è –æ—Ç–ª–∞–¥–∫—É, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –∫–æ–¥–∞ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞. –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –æ–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ç–∞–∫–∏–µ –∑–∞–¥–∞—á–∏, –∫–∞–∫ –≤—ã—è–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö –∏–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ä–µ—à–µ–Ω–∏–µ —É—Ä–∞–≤–Ω–µ–Ω–∏–π. –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫ –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –Ω–∞–≤—ã–∫–æ–≤ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.\n\n3.  **–°—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–¥–∞—á –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:** –î–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ—Ü–µ–Ω–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –±—ã–ª–∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ LLM –ø—É—Ç–µ–º —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è —Å—É–¥–µ–π. –≠—Ç–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç, –∫–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã, —É–¥–µ–ª—è—è –æ—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏, –ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∑–∞–¥–∞–Ω–∏—è, —Å–≤–æ–¥—è –∫ –º–∏–Ω–∏–º—É–º—É —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∫ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–º –∏—Å–∫–∞–∂–µ–Ω–∏—è–º. –ù–∞–±–ª—é–¥–∞–ª–æ—Å—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—ã—à–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –æ—Ü–µ–Ω–∫–∏.\n\nPhiBench —Å—ã–≥—Ä–∞–ª —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—É—é —Ä–æ–ª—å –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ phi-4. –û–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –æ —Å–æ—á–µ—Ç–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–±–æ—Ä–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏. PhiBench —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, –≤—ã—è–≤–ª—è—é—â–∏—Ö —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏—Ö –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –¥–ª—è –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö.\n"
            },
            {
                "title": "Performance on Key Benchmarks",
                "content": "Our benchmark results were presented in Table 1, along with comparisons to other models. We first report the values from OpenAIs simple-evals benchmark, which is framework (including prompts, temperature, and extraction) for evaluating MMLU [HBB+20], GPQA diamond [RHS+23], MATH [HBK+21], HumanEval [CTJ+21], MGSM [SSF+22], and the SimpleQA [WKC+24] F1-score. We also consider MMLU-pro [WMZ+24], HumanEval+ [LXWZ23], ArenaHard [CZS+24], and IFEval [ZLM+23], for which we use an internal framework and prompting and extraction. Finally, we use PhiBench, our internal collection of evaluations (see Section 5). phi-4 outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12 benchmarks. While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers for SimpleQA, DROP, and IFEval, we consider phi-4s behavior on SimpleQA to actually be better than Qwens. In fact, our base model gets higher benchmark score than Qwen-2.5-14B-Instruct on SimpleQA, and we intentionally modified the models behavior in post-training to optimize for better user experience rather than higher benchmark score. See Figure 6 and Appendix A.1 for details. Our model excels at STEM Q&A tasks. For example, on GPQA (graduate-level STEM questions) and MATH (math competitions), it even outscores its teacher model, GPT-4o. It also scores higher at coding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark against, including much larger Llama models. phi-4s weakest benchmark scores are on SimpleQA, DROP, and IFEval. We believe for the first two that the number reported by simple-evals is reductive and does not accurately reflect model performance on the benchmark problems. However, IFEval reveals real weakness of our model it has trouble strictly following instructions. While strict instruction following was not an emphasis of our synthetic data generations for this model, we are confident that phi-4s instruction-following performance could be significantly improved with targeted synthetic data.",
                "summary": "–í —Ç–∞–±–ª–∏—Ü–µ 1 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ phi-4 –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, –∞ —Ç–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å:\n\n*   **OpenAI simple-evals:** –≠—Ç–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∑–∞–¥–∞—á–∞—Ö MMLU, GPQA, MATH, HumanEval, MGSM –∏ SimpleQA (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º F1-–º–µ—Ä—ã). –§—Ä–µ–π–º–≤–æ—Ä–∫ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏ –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤.\n*   **MMLU-pro, HumanEval+, ArenaHard –∏ IFEval:** –î–ª—è —ç—Ç–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –º–µ—Ç–æ–¥–∞–º–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.\n*   **PhiBench:** –≠—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –Ω–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∞–≤—Ç–æ—Ä–∞–º–∏ —Å—Ç–∞—Ç—å–∏.\n\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ phi-4 –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à—É—é –ø–æ —Ä–∞–∑–º–µ—Ä—É –º–æ–¥–µ–ª—å Qwen-2.5-14B-Instruct –Ω–∞ 9 –∏–∑ 12 –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ phi-4 –ø–æ–∫–∞–∑–∞–ª–∞ –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ SimpleQA, DROP –∏ IFEval, –∞–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ phi-4 –Ω–∞ SimpleQA —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ª—É—á—à–µ, —á–µ–º —É Qwen. –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å phi-4 –¥–∞–∂–µ –ø–æ–∫–∞–∑–∞–ª–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ SimpleQA, —á–µ–º Qwen-2.5-14B-Instruct, –Ω–æ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞, –∞ –Ω–µ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 6 –∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ A.1.\n\n–ú–æ–¥–µ–ª—å phi-4 –æ—Å–æ–±–µ–Ω–Ω–æ —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –∑–∞–¥–∞—á–∞–º–∏ STEM Q&A (–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—É–∫–∏, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏). –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ GPQA (–≤–æ–ø—Ä–æ—Å—ã STEM —É—Ä–æ–≤–Ω—è –º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä—ã) –∏ MATH (–º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è) –æ–Ω–∞ –¥–∞–∂–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–≤–æ—é –æ–±—É—á–∞—é—â—É—é –º–æ–¥–µ–ª—å GPT-4o. Phi-4 —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏ (HumanEval –∏ HumanEval+) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤–∫–ª—é—á–∞—è –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ Llama.\n\n–°–∞–º—ã–µ —Å–ª–∞–±—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã phi-4 –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö SimpleQA, DROP –∏ IFEval. –ê–≤—Ç–æ—Ä—ã —Å—á–∏—Ç–∞—é—Ç, —á—Ç–æ –æ—Ü–µ–Ω–∫–∞ SimpleQA, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é simple-evals, —è–≤–ª—è–µ—Ç—Å—è –∑–∞–Ω–∏–∂–µ–Ω–Ω–æ–π –∏ –Ω–µ—Ç–æ—á–Ω–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –û–¥–Ω–∞–∫–æ IFEval –≤—ã—è–≤–∏–ª —Ä–µ–∞–ª—å–Ω—É—é —Å–ª–∞–±–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ‚Äì –ø—Ä–æ–±–ª–µ–º—ã —Å–æ —Å—Ç—Ä–æ–≥–∏–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –•–æ—Ç—è —Å—Ç—Ä–æ–≥–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –Ω–µ –±—ã–ª–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, –∞–≤—Ç–æ—Ä—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å phi-4 –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–∞ —Å –ø–æ–º–æ—â—å—é —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
            },
            {
                "title": "Safety",
                "content": "We developed phi-4 in accordance with Microsofts Responsible AI principles. Our overall approach to RAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations across dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets 18 phi-3 (3B-4K) phi-3 (7B-8K) phi-3 (14B-4K) Mistral (7B-v0.1) Mistral (7B-v0.2) Llama-3 (8B) Gemma (7B) phi-4 Grounding 4. 4.701 4.787 4.065 4.692 4.672 4. 4.619 3P Content Harms (DR1) Books, News, Recipes, Songs 0.251 0.253 0.26 0.562 0. 0.373 0.383 0.121 Harmful Content Continuation (DR3) Harmful Content Summarization (DR3) Jailbreak(DR1) Hate/Fairness, Self-Harm, Sexual, Violence 0.007 0.003 0.01 0.026 0. 0.013 0.013 0.036 Hate/Fairness, Self-Harm, Sexual, Violence 0.105 0. 0.112 0.223 0.16 0.082 0.103 0. See text for covered topics 0.117 0.107 0.111 0.156 0. 0.13 0.114 0.073 Table 10: Performance comparison across models. Lower scores are better, except for Grounding, where higher score is better. phi-4 values are bold for readability. [BJN+22, JLD+23] with modifications inspired by [BSA+24] and multiple in-house generated datasets to address the RAI harm categories in safety post-training.",
                "summary": "–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª–∏ phi-4 –º—ã –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–ª–∏—Å—å –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ (RAI) Microsoft. –ù–∞—à –ø–æ–¥—Ö–æ–¥ –∫ RAI –≤–∫–ª—é—á–∞–ª –≤ —Å–µ–±—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ \"–∫—Ä–∞—Å–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π\" (–∏–º–∏—Ç–∞—Ü–∏—è –∞—Ç–∞–∫) –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –ø–æ –¥–µ—Å—è—Ç–∫–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤—Ä–µ–¥–∞ RAI.\n\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ø–æ–ª–µ–∑–Ω–æ—Å—Ç–∏ –∏ –±–µ–∑–≤—Ä–µ–¥–Ω–æ—Å—Ç–∏, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ –º–æ–¥–µ–ª—è—Ö phi-3 (3B-4K, 7B-8K, 14B-4K). –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –º–æ–¥–µ–ª–µ–π Mistral (7B-v0.1, 7B-v0.2), Llama-3 (8B), Gemma (7B). –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç–æ–π [BSA+24], –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –≤—Ä–µ–¥–∞ RAI, –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏.\n\n–í —Ç–∞–±–ª–∏—Ü–µ 10 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è phi-4. –û—Ü–µ–Ω–∫–∞ \"Grounding\" (—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã) —è–≤–ª—è–µ—Ç—Å—è –∏—Å–∫–ª—é—á–µ–Ω–∏–µ–º, –≥–¥–µ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ª—É—á—à–µ, –∞ –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –±–∞–ª–ª —Å—á–∏—Ç–∞–µ—Ç—Å—è –ª—É—á—à–µ. –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ phi-4 –≤—ã–¥–µ–ª–µ–Ω—ã –∂–∏—Ä–Ω—ã–º —à—Ä–∏—Ñ—Ç–æ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏.\n\n–í —Ç–∞–±–ª–∏—Ü–µ –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\n*   **Grounding:** –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã.\n*   **3P Content Harms (DR1):** –í—Ä–µ–¥, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —Ç—Ä–µ—Ç—å–∏—Ö —Å—Ç–æ—Ä–æ–Ω, –≤–∫–ª—é—á–∞—è –∫–Ω–∏–≥–∏, –Ω–æ–≤–æ—Å—Ç–∏, —Ä–µ—Ü–µ–ø—Ç—ã –∏ –ø–µ—Å–Ω–∏.\n*   **Harmful Content Continuation (DR3):** –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n*   **Harmful Content Summarization (DR3):** –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã—Ö —Ä–µ–∑—é–º–µ.\n*   **Jailbreak (DR1):** –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –æ–±—Ö–æ–¥—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.\n\n–í—Ä–µ–¥ –æ—Ü–µ–Ω–∏–≤–∞–ª—Å—è –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: –Ω–µ–Ω–∞–≤–∏—Å—Ç—å/—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å, –ø—Ä–∏—á–∏–Ω–µ–Ω–∏–µ –≤—Ä–µ–¥–∞ —Å–µ–±–µ, —Å–µ–∫—Å—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏ –Ω–∞—Å–∏–ª–∏–µ. –í —Ç–∞–±–ª–∏—Ü–µ –≤–∏–¥–Ω–æ, —á—Ç–æ phi-4 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –ø–ª–∞–Ω–µ —Å–Ω–∏–∂–µ–Ω–∏—è –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.\n"
            },
            {
                "title": "RAI Benchmarks",
                "content": "Table 10 shows the results of in-house RAI benchmarks [MHJ+23] for phi-4 compared to the phi-3 models [AAA+24], Mistral-7b-v0.1 [JSM+23], Mistral-7b-v0.2, Gemma 7b [TMH+24], and Llama-3-instruct8b [AI23b]. This benchmark utilized GPT-4o to simulate multi-turn conversations in five different categories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5 (fully grounded), and measures if the information in response is based on given prompt. In other categories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm) to 7 (severe harm) and the defect rates (DR-x) were computed as the percentage of samples with the severity score being greater than or equal to x. The Jailbreak (DR1) benchmark consists of simulated conversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular conspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people. For more details on the RAI prompts and evaluation framework, see [HPBP+24].",
                "summary": "–í —Ç–∞–±–ª–∏—Ü–µ 10 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π phi-4 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –º–æ–¥–µ–ª—è–º–∏ phi-3, Mistral-7b-v0.1, Mistral-7b-v0.2, Gemma 7b –∏ Llama-3-instruct8b. –î–ª—è —ç—Ç–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è GPT-4o, –∫–æ—Ç–æ—Ä—ã–π –∏–º–∏—Ç–∏—Ä–æ–≤–∞–ª –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏ –≤ –ø—è—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –∏ –æ—Ü–µ–Ω–∏–≤–∞–ª –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π. \n\n–û—Ü–µ–Ω–∫–∞ \"–∑–∞–∑–µ–º–ª–µ–Ω–∏—è\" (grounding) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –æ—Ç–≤–µ—Ç–µ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ. –û–Ω–∞ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç 0 (–Ω–µ –∑–∞–∑–µ–º–ª–µ–Ω–æ) –¥–æ 5 (–ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–∑–µ–º–ª–µ–Ω–æ). –í –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –æ—Ç–≤–µ—Ç—ã –æ—Ü–µ–Ω–∏–≤–∞–ª–∏—Å—å –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ—Å—Ç–∏ (–æ—Ç 0 - –Ω–µ—Ç –≤—Ä–µ–¥–∞, –¥–æ 7 - —Å–µ—Ä—å–µ–∑–Ω—ã–π –≤—Ä–µ–¥). –¢–∞–∫–∂–µ –±—ã–ª–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–µ—Ñ–µ–∫—Ç–Ω–æ—Å—Ç–∏ (DR-x), –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ–±—Ä–∞–∑—Ü–æ–≤, –≥–¥–µ –æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–∞ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ x.\n\n–¢–µ—Å—Ç \"Jailbreak\" (DR1) –≤–∫–ª—é—á–∞–ª –∏–º–∏—Ç–∞—Ü–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –Ω–∞ —Ç–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Å–æ–≤—Ä–∞—â–µ–Ω–∏–µ–º –¥–µ—Ç–µ–π, –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º —É–±–µ–∂–¥–µ–Ω–∏–µ–º, —É—Ç–µ—á–∫–æ–π 100 —Å–ª–æ–≤ –∏–∑ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏ —Ç–µ–æ—Ä–∏—è–º–∏ –∑–∞–≥–æ–≤–æ—Ä–∞, –ø—Ä–µ–¥—Ä–∞—Å—Å—É–¥–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π, –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –Ω–µ–∑–∞–∫–æ–Ω–Ω—ã–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏ –∏ –Ω–∞—Å–∏–ª–∏–µ–º –ø—Ä–æ—Ç–∏–≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π. –ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ —Å–∏—Å—Ç–µ–º–µ –æ—Ü–µ–Ω–∫–∏ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Ä–∞–±–æ—Ç–µ [HPBP+24].\n"
            },
            {
                "title": "Red Teaming",
                "content": "In addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an independent group tasked with identifying safety and security vulnerabilities in Microsofts GenAI products. AIRT conducted two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both average and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior of phi-4 was similar to that of the phi-3 family, but identified several risky behaviors that were addressed by further rounds of safety post-training. In addition, the adversarial user scenario tested wide range of techniques aimed at intentionally subverting the models safety training including jailbreaks, prompt encodings, and multi-turn attacks. phi-4 showed strong defenses against these techniques. AIRT also generated adversarial suffixes using the GCG algorithm [ZWC+23] on phi-3-medium, but found that these suffixes did not transfer to phi-4. Further red teaming is required to identify possible risks across broader range of scenarios and harm categories.",
                "summary": "–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É RAI (–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞), –º—ã —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–ª–∏ —Å Microsoft AI Red Team (AIRT), –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π –≥—Ä—É–ø–ø–æ–π, –∑–∞–¥–∞—á–µ–π –∫–æ—Ç–æ—Ä–æ–π —è–≤–ª—è–µ—Ç—Å—è –≤—ã—è–≤–ª–µ–Ω–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –∑–∞—â–∏—Ç–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ Microsoft GenAI. AIRT –ø—Ä–æ–≤–µ–ª–∞ –¥–≤—É—Ö–Ω–µ–¥–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ phi-4, –∏–º–∏—Ç–∏—Ä—É—è –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∫–∞–∫ –æ–±—ã—á–Ω—ã—Ö, —Ç–∞–∫ –∏ –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã—Ö –∏ –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ. –í —Ü–µ–ª–æ–º, AIRT –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞, —á—Ç–æ –ø–æ–≤–µ–¥–µ–Ω–∏–µ phi-4 –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–æ–≤–µ–¥–µ–Ω–∏—é —Å–µ–º–µ–π—Å—Ç–≤–∞ phi-3, –Ω–æ –≤—ã—è–≤–∏–ª–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω—ã –≤ —Ö–æ–¥–µ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –µ—ë —Å–æ–∑–¥–∞–Ω–∏—è.\n\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å—Ü–µ–Ω–∞—Ä–∏–π —Å –∑–ª–æ–Ω–∞–º–µ—Ä–µ–Ω–Ω—ã–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ç–µ—Ö–Ω–∏–∫, –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –Ω–∞ –ø—Ä–µ–¥–Ω–∞–º–µ—Ä–µ–Ω–Ω–æ–µ –æ–±—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤–∫–ª—é—á–∞—è \"–ø–æ–±–µ–≥–∏ –∏–∑ —Ç—é—Ä—å–º—ã\" (jailbreaks), –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –º–Ω–æ–≥–æ—Ö–æ–¥–æ–≤—ã–µ –∞—Ç–∞–∫–∏. Phi-4 –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —Å–∏–ª—å–Ω—É—é –∑–∞—â–∏—Ç—É –æ—Ç —ç—Ç–∏—Ö —Ç–µ—Ö–Ω–∏–∫. AIRT —Ç–∞–∫–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–µ —Å—É—Ñ—Ñ–∏–∫—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞ GCG –Ω–∞ phi-3-medium, –Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞, —á—Ç–æ —ç—Ç–∏ —Å—É—Ñ—Ñ–∏–∫—Å—ã –Ω–µ –ø–µ—Ä–µ–Ω–æ—Å—è—Ç—Å—è –Ω–∞ phi-4. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –≤—ã—è–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏ –≤ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —É—â–µ—Ä–±–∞.\n\n*–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: \"–ü–æ–±–µ–≥ –∏–∑ —Ç—é—Ä—å–º—ã\" (jailbreak) - —ç—Ç–æ —Ç–µ—Ö–Ω–∏–∫–∞, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞—Å—Ç–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–Ω–∞ –æ–±—ã—á–Ω–æ –Ω–µ –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä, –≤—Ä–µ–¥–æ–Ω–æ—Å–Ω—ã–π –∏–ª–∏ –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.*\n"
            },
            {
                "title": "Weaknesses",
                "content": "While phi-4 achieves similar level of language understanding and reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual knowledge. For example, if is plausible human name, the model sometimes responds to prompts of the form Who is X? with hallucinated biography of the person X. This limitation would be improved by augmenting the model with search engine, but factual hallucinations cannot be eliminated completely. While phi-4 demonstrates relatively strong performance in answering questions and performing reasoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving specific formatting requirements. For instance, when tasked with generating outputs in strict tabular formats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model may produce outputs that deviate from the specified guidelines. This limitation arises in part from the models training focus, which prioritized synthetic datasets tailored toward Q&A and reasoning tasks over instruction-following scenarios. Even on reasoning tasks, phi-4 can make mistakes. For example, when asked which number is smaller, 9.9 or 9.11?, the model can conclude incorrectly that 9.9 is smaller than 9.11. Moreover, as our data contains lot of chain-of-thought examples, phi-4 sometimes gives long elaborate answers even for simple problemsthis might make user interactions tedious. We also note that while phi-4 can function as chat bot, it has been fine-tuned to maximize performance on single-turn queries. Despite diligent RAI efforts, we acknowledge challenges around reproduction or amplification of biases, inappropriate content generation, and safety issues. The use of carefully curated training data, as well as targeted post-training, and improvements from red-teaming insights, have resulted in mitigating these issues across all dimensions, but have not resolved the issues completely.",
                "summary": "–ú–æ–¥–µ–ª—å phi-4, —Ö–æ—Ç—è –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Ä–æ–≤–µ–Ω—å –ø–æ–Ω–∏–º–∞–Ω–∏—è —è–∑—ã–∫–∞ –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, —Å—Ä–∞–≤–Ω–∏–º—ã–π —Å –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏, –≤—Å–µ –∂–µ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏–∑-–∑–∞ —Å–≤–æ–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–æ—è–≤–ª—è–µ—Ç—Å—è –≤ —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç–∏ –∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è–º, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –∫–∞—Å–∞–µ—Ç—Å—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –∏–º—è, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤—ã–¥–∞—Ç—å –≤—ã–º—ã—à–ª–µ–Ω–Ω—É—é –±–∏–æ–≥—Ä–∞—Ñ–∏—é —ç—Ç–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞. –£–ª—É—á—à–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –º–æ–∂–Ω–æ, –ø–æ–¥–∫–ª—é—á–∏–≤ –∫ –º–æ–¥–µ–ª–∏ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É, –Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–∏—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –Ω–µ —É–¥–∞—Å—Ç—Å—è.\n\n–¢–∞–∫–∂–µ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ —Ä–µ—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, phi-4 —Ö—É–∂–µ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —á–µ—Ç–∫–∏–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, –æ—Å–æ–±–µ–Ω–Ω–æ –µ—Å–ª–∏ –æ–Ω–∏ –∫–∞—Å–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü, —Å–ø–∏—Å–∫–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ –∏–ª–∏ —Å–æ–±–ª—é–¥–µ–Ω–∏–∏ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ—Ç–∫–ª–æ–Ω—è—Ç—å—Å—è –æ—Ç –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–ª—Å—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º –¥–∞–Ω–Ω—ã–º, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –∏ –∑–∞–¥–∞—á–∏ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ, –∞ –Ω–µ –Ω–∞ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º.\n\n–î–∞–∂–µ –≤ –∑–∞–¥–∞—á–∞—Ö –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ phi-4 –º–æ–∂–µ—Ç –¥–æ–ø—É—Å–∫–∞—Ç—å –æ—à–∏–±–∫–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —á–∏—Å–µ–ª 9.9 –∏ 9.11 –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ—à–∏–±–æ—á–Ω–æ –∑–∞–∫–ª—é—á–∏—Ç—å, —á—Ç–æ 9.9 –º–µ–Ω—å—à–µ, —á–µ–º 9.11. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, phi-4 –∏–Ω–æ–≥–¥–∞ –¥–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–∞–∂–µ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —á—Ç–æ –º–æ–∂–µ—Ç —É—Ç–æ–º–ª—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.\n\n–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ö–æ—Ç—è phi-4 –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ —á–∞—Ç-–±–æ—Ç, –æ–Ω–∞ –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –æ–¥–Ω–æ—Ö–æ–¥–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.\n\n–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —É—Å–∏–ª–∏—è –ø–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –º—ã –ø—Ä–∏–∑–Ω–∞–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ–º –∏–ª–∏ —É—Å–∏–ª–µ–Ω–∏–µ–º –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–µ–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –Ω–µ–ø—Ä–∏–µ–º–ª–µ–º–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö, —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∏ —É–ª—É—á—à–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –ø–æ–∑–≤–æ–ª–∏–ª–∏ —Å–Ω–∏–∑–∏—Ç—å –æ—Å—Ç—Ä–æ—Ç—É —ç—Ç–∏—Ö –ø—Ä–æ–±–ª–µ–º, –Ω–æ –Ω–µ —É—Å—Ç—Ä–∞–Ω–∏–ª–∏ –∏—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é.\n"
            }
        ]
    },
    {
        "id": "2412.09605",
        "title": "AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials",
        "url": "https://huggingface.co/papers/2412.09605",
        "abstract": "Graphical User Interface (GUI) agents hold great potential for automating\ncomplex tasks across diverse digital environments, from web applications to\ndesktop software. However, the development of such agents is hindered by the\nlack of high-quality, multi-step trajectory data required for effective\ntraining. Existing approaches rely on expensive and labor-intensive human\nannotation, making them unsustainable at scale. To address this challenge, we\npropose AgentTrek, a scalable data synthesis pipeline that generates\nhigh-quality GUI agent trajectories by leveraging web tutorials. Our method\nautomatically gathers tutorial-like texts from the internet, transforms them\ninto task goals with step-by-step instructions, and employs a visual-language\nmodel agent to simulate their execution in a real digital environment. A\nVLM-based evaluator ensures the correctness of the generated trajectories. We\ndemonstrate that training GUI agents with these synthesized trajectories\nsignificantly improves their grounding and planning performance over the\ncurrent models. Moreover, our approach is more cost-efficient compared to\ntraditional human annotation methods. This work underscores the potential of\nguided replay with web tutorials as a viable strategy for large-scale GUI agent\ntraining, paving the way for more capable and autonomous digital agents.",
        "score": 1,
        "issue_id": 1,
        "pub_date": "2024-12-12",
        "pub_date_card": {
            "ru": "12 –¥–µ–∫–∞–±—Ä—è",
            "en": "December 12",
            "zh": "12Êúà12Êó•"
        },
        "hash": "298173f67e05442e",
        "authors": [
            "Yiheng Xu",
            "Dunjie Lu",
            "Zhennan Shen",
            "Junli Wang",
            "Zekun Wang",
            "Yuchen Mao",
            "Caiming Xiong",
            "Tao Yu"
        ],
        "affiliations": [
            "Salesforce Research",
            "University of Hong Kong"
        ],
        "pdf_title_img": "assets\\pdf\\title_img\\2412.09605.jpg",
        "data": {
            "categories": [
                "#data",
                "#dataset",
                "#training",
                "#optimization",
                "#agents",
                "#synthetic"
            ],
            "emoji": "ü§ñ",
            "ru": {
                "title": "AgentTrek: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤",
                "desc": "–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç AgentTrek - –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUI) —Å –ø–æ–º–æ—â—å—é –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤. –ú–µ—Ç–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —É—á–µ–±–Ω—ã–µ –ø–æ—Å–æ–±–∏—è, –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Ö –≤ —Ü–µ–ª–∏ –∑–∞–¥–∞—á —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ –∏—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ. –û–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç–µ–∫—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º."
            },
            "en": {
                "title": "Automating GUI Agent Training with Web Tutorials",
                "desc": "This paper introduces AgentTrek, a novel approach for generating high-quality training data for Graphical User Interface (GUI) agents. It addresses the challenge of obtaining multi-step trajectory data by automatically synthesizing it from web tutorials, thus eliminating the need for costly human annotation. The method involves transforming tutorial texts into actionable task goals and using a visual-language model to simulate the execution of these tasks. The results show that training GUI agents with these synthesized trajectories enhances their performance in grounding and planning, making the process more efficient and scalable."
            },
            "zh": {
                "title": "Âà©Áî®ÁΩëÁªúÊïôÁ®ãÊèêÂçáGUI‰ª£ÁêÜËÆ≠ÁªÉÊïàÁéá",
                "desc": "Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AgentTrekÁöÑÊï∞ÊçÆÂêàÊàêÁÆ°ÈÅìÔºåÁî®‰∫éÁîüÊàêÈ´òË¥®ÈáèÁöÑÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÔºàGUIÔºâ‰ª£ÁêÜËΩ®Ëøπ„ÄÇËØ•ÊñπÊ≥ïÈÄöËøáËá™Âä®Êî∂ÈõÜÁΩëÁªúÊïôÁ®ãÊñáÊú¨ÔºåÂ∞ÜÂÖ∂ËΩ¨Âåñ‰∏∫‰ªªÂä°ÁõÆÊ†áÂíåÈÄêÊ≠•Êåá‰ª§ÔºåÂπ∂Âà©Áî®ËßÜËßâËØ≠Ë®ÄÊ®°Âûã‰ª£ÁêÜÂú®ÁúüÂÆûÊï∞Â≠óÁéØÂ¢É‰∏≠Ê®°ÊãüÊâßË°å„ÄÇ‰∏é‰º†ÁªüÁöÑ‰∫∫Á±ªÊ†áÊ≥®ÊñπÊ≥ïÁõ∏ÊØîÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®ÊàêÊú¨‰∏äÊõ¥ÂÖ∑ÊïàÁéáÔºåÂêåÊó∂ÊòæËëóÊèêÈ´ò‰∫ÜGUI‰ª£ÁêÜÁöÑÂü∫Á°ÄÂíåËßÑÂàíÊÄßËÉΩ„ÄÇÊ≠§Á†îÁ©∂Â±ïÁ§∫‰∫ÜÂà©Áî®ÁΩëÁªúÊïôÁ®ãËøõË°åÂºïÂØºÈáçÊîæÁöÑÊΩúÂäõÔºå‰∏∫Â§ßËßÑÊ®°GUI‰ª£ÁêÜËÆ≠ÁªÉÂºÄËæü‰∫ÜÊñ∞ÁöÑÂèØËÉΩÊÄß„ÄÇ"
            }
        },
        "clean_sections": [
            {
                "title": "Abstract",
                "content": "Graphical User Interface (GUI) agents hold great potential for automating complex tasks across diverse digital environments, from web applications to desktop software. However, the development of such agents is hindered by the lack of high-quality, multi-step trajectory data required for effective training. Existing approaches rely on expensive and labor-intensive human annotation, making them unsustainable at scale. To address this challenge, we propose AgentTrek, a scalable data synthesis pipeline that generates high-quality GUI agent trajectories by leveraging web tutorials. Our method automatically gathers tutorial-like texts from the internet, transforms them into task goals with step-by-step instructions, and employs a visual-language model agent to simulate their execution in a real digital environment. A VLM-based evaluator ensures the correctness of the generated trajectories. We demonstrate that training GUI agents with these synthesized trajectories significantly improves their grounding and planning performance over the current models. Moreover, our approach is more cost-efficient compared to traditional human annotation methods. This work underscores the potential of guided replay with web tutorials as a viable strategy for large-scale GUI agent training, paving the way for more capable and autonomous digital agents.",
                "summary": "<p>–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (GUI). –¢–∞–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Ä–µ–¥–∞—Ö, –Ω–æ –∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∑–∞—Ç—Ä—É–¥–Ω–µ–Ω–∞ –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –¥–µ–π—Å—Ç–≤–∏–π (—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π). –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã —Ä–∞–∑–º–µ—Ç–∫–∏ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–∏—Ö –∑–∞—Ç—Ä–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —Ä—É—á–Ω—É—é —Ä–∞–±–æ—Ç—É.</p>\n<p>–ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –∞–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç AgentTrek ‚Äì –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ–±-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:</p>\n<ol>\n<li><strong>–°–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π:</strong> AgentTrek –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.</li>\n<li><strong>–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–¥–∞—á–∏:</strong> –≠—Ç–∏ —Ç–µ–∫—Å—Ç—ã –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∏ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.</li>\n<li><strong>–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:</strong> –ê–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å (VLM), –∏–º–∏—Ç–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ü–∏—Ñ—Ä–æ–≤–æ–π —Å—Ä–µ–¥–µ.</li>\n<li><strong>–û—Ü–µ–Ω–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏:</strong> VLM-–º–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.</li>\n</ol>\n<p>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω, —á–µ–º —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —Å—Ç–∞—Ç—å—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–µ–±-–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª–µ–µ –º–æ—â–Ω—ã—Ö –∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤.</p>"
            },
            {
                "title": "Introduction",
                "content": "Graphical User Interfaces (GUIs) are fundamental medium for human-computer interaction, enabling users to perform tasks across various digital pl atforms. Automating GUI operations through agentic automation has the potential to significantly enhance productivity by enabling autonomous task completion using human-centric tools. Additionally, this approach can foster the development of advanced AI systems capable of learning from rich digital environments. Recent advancements in large language models (LLMs) have endowed the models with powerful abilities in understanding, reasoning, and decisionmaking, which are essential for the evolution of GUI agents in diverse contexts such as web (Zheng et al., 2024), desktop (Xie et al., 2024), and mobile applications (Zhang et al., 2023). Despite these advancements, the performance of GUI agents remains suboptimal. Contemporary Large Language Models (LLMs) are primarily engineered and trained on datasets optimized for generating informative responses (Ouyang et al., 2022; OpenAI, 2024). Their architecture and training paradigms are Figure 1: Expected Agent Trajectories Equal contribution 1 Figure 2: Overview of the AgentTrek Pipeline: (1) Automatic Tutorial Collection from the Internet: Tutorial-related data is extracted and filtered from internet sources using heuristic methods and FastText model. An LLM processes the filtered textual data, transforming it into structured tutorials. (2) Trajectory data collection via guided replay: VLM agent interacts with the real digital environment guided by tutorials, while high-quality trajectory data, including observations, actions, and reasoning, is collected. Another VLM evaluator acts as judger to further improve the effectiveness of the synthetic dataset. (3) Training and fine-tuning with replay data: The collected trajectory data is used to train and fine-tune GUI agent models, which are evaluated on standard agent benchmarks, demonstrating significant improvements. not inherently designed to make complex, sequential action decisions that require long-term observation and historical context. Consequently, training GUI agents with multi-step trajectory data is crucial to improving their capabilities. High-quality agent trajectories contain several key components: high-level goal, sequence of interleaved observations, natural language reasoning, and grounded actions (as shown in Figure 1). Unfortunately, such data is not readily available on the internet like textual or image data, as it involves complex situational reasoning and multimodal interactivity. Existing approaches typically rely on human annotation to collect these trajectories (Deng et al., 2024; Rawles et al., 2023; Li et al., 2024), process that is both expensive and not scalable. To address this data scarcity, data synthesis has emerged as vital approach in AI system development. Synthesizing agent trajectories presents significant challenges due to the need for interwoven natural language instructions, visual observations, and context-specific actions that must be accurately grounded in the GUI environment. Although there have been some successful applications of LLMs in data synthesis pipelines (Ye et al., 2022; Peng et al., 2023; Qin et al., 2023), these complexities still make GUI trajectory synthesis particularly demanding. In this work, we present AgentTrek, scalable data synthesis pipeline specifically designed for training GUI agents. We begin by automatically gathering and filtering tutorial-like text from the web, which describes GUI tasks and workflows in web environments. These tutorials are then transformed into agent tasks with high-level objectives and detailed step-by-step instructions. Using visual-language model (VLM) agent, we simulate the execution of these tasks, guided by the synthesized tutorials. An evaluator model is also employed to subsequently verify whether the goal was successfully achieved. Through this comprehensive pipeline, we efficiently generated large volume of high-quality web agent trajectories. Our experimental results demonstrate that training GUI agent models with these synthesized web agent trajectories not only improves their performance but also enables them to surpass the capabilities of their initial teacher models, which is the replay model GPT-4 in our case. Compared to traditional human-annotated data pipelines, our method is significantly more cost-effective, emphasizing the scalability and economic viability of the AgentTrek pipeline. We introduce AgentTrek, novel pipeline that leverages web tutorials to synthesize highquality web agent trajectory data at scale, effectively bridging the gap between LLM capabilities and the demanding need for multi-step, context-rich training data for GUI agents. 2 Extensive experiments demonstrate that agents trained with our synthesized data outperform those trained on existing datasets in both grounding and planning capabilities, validating the effectiveness of AgentTrek. Our pipeline significantly reduces the cost and scalability obstacles of human-annotated data collection, providing practical approach for large-scale GUI agent training through data synthesis. Table 1: Comparison of AgentTrek with other trajectory datasets for training. For the calculation of dataset size and average steps, see Appendix A. Datasets RUSS ScreenAgent WebLINX MM-Mind2Web GUIAct Size 80 203 969 1009 2482 AgentTrek (Ours) 10398 Average Steps HTML AxTree Intermediate Reasoning Video Matching Screenshot Website Task Inst. Level 5.4 4.3 18.8 7.3 6.7 12. Yes No Yes Yes No Yes No No No No No Yes No Yes No No No Yes No No No No No Yes No Yes Yes No Yes Yes 22 - 155 137 121 Low High & Low High & Low High High 127 High & Low",
                "summary": "<p>–í –¥–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤, –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (GUI). –¢–∞–∫–∏–µ –∞–≥–µ–Ω—Ç—ã –º–æ–≥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö, —á—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (LLM) –æ–±–ª–∞–¥–∞—é—Ç –º–æ—â–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏ –∫ –ø–æ–Ω–∏–º–∞–Ω–∏—é, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—é –∏ –ø—Ä–∏–Ω—è—Ç–∏—é —Ä–µ—à–µ–Ω–∏–π, –Ω–æ –æ–Ω–∏ –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π, —Ç—Ä–µ–±—É—é—â–∏—Ö –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ —É—á–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ—ç—Ç–æ–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ü–µ–ª—å, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ –¥–µ–π—Å—Ç–≤–∏—è, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ GUI.</p>\n<p>–ü—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ —Ç–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤ —Ç—Ä–µ–±—É—é—Ç —Å–ª–æ–∂–Ω–æ–≥–æ —Å–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è. –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –¥–æ—Ä–æ–≥–æ—Å—Ç–æ—è—â—É—é —Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –≤ —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä —Å–∏–Ω—Ç–µ–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º AgentTrek.</p>\n<p>AgentTrek –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –æ–±—É—á–∞—é—â–∏–π —Ç–µ–∫—Å—Ç –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∏–π –∑–∞–¥–∞—á–∏ –∏ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –≤ –≤–µ–±-—Å—Ä–µ–¥–µ. –≠—Ç–∏ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∏ –∞–≥–µ–Ω—Ç–∞ —Å —Ü–µ–ª—è–º–∏ –∏ –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏. –ó–∞—Ç–µ–º –∞–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–æ–¥–µ–ª—å VLM (–≤–∏–∑—É–∞–ª—å–Ω–æ-—è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å), –∏–º–∏—Ç–∏—Ä—É–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–∏—Ö –∑–∞–¥–∞—á, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å-–æ—Ü–µ–Ω—â–∏–∫, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏.</p>\n<p>–° –ø–æ–º–æ—â—å—é —ç—Ç–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å–º–æ–≥–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —ç—Ç–∏—Ö —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ç–æ–ª—å–∫–æ —É–ª—É—á—à–∞–µ—Ç –∏—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–º –ø—Ä–µ–≤–∑–æ–π—Ç–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–≤–æ–∏—Ö –∏—Å—Ö–æ–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π-—É—á–∏—Ç–µ–ª–µ–π (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ GPT-4). –í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∫–æ–Ω–≤–µ–π–µ—Ä–∞–º–∏ —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π, –º–µ—Ç–æ–¥ AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ —ç–∫–æ–Ω–æ–º–∏—á–µ–Ω –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º.</p>\n<p>–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, AgentTrek –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–∏–Ω—Ç–µ–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GUI-–∞–≥–µ–Ω—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è –≤–µ–±-—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –Ω–µ—Ö–≤–∞—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ —Å–Ω–∏–∑–∏—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–º –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ GUI-–∞–≥–µ–Ω—Ç–æ–≤.</p>"
            },
            {
                "title": "Method",
                "content": "We introduce pipeline to collect and process GUI tutorials from the internet for training visual language models (VLMs) in web automation tasks. The method comprises three main steps: 1. Collecting Tutorials: We extract web interaction tutorials from large datasets using keyword filtering and language models to identify and standardize relevant content. 2. Guided Replay: An agent uses these tutorials to perform tasks in web environment, interacting with real websites while we record its actions and thoughts. 3. Model Training: We train visual agent model that relies on screenshots and standard GUI actions, enhancing its web navigation capabilities with the collected data. This approach enables efficient training of VLMs without extensive manual annotation, offering scalable solution for automating web tasks. 2.1 AUTOMATIC TUTORIALS COLLECTION FROM INTERNET We first extract web interaction tutorials from Redpajama dataset (Computer, 2023). rule-based heuristic filter is applied to create preliminary dataset, subset of which is annotated by an advanced LLM to generate labeled samples for training effective FastText classification model (Joulin et al., 2017), the tutorial classifier. This classifier further enhances the data quality through filtering. Finally, LLMs are employed to tag and paraphrase the raw text of tutorials into standardized format, preparing them for the replay phase in Section 2.2. 2.1.1 PREFILTER FUNCTION Although GUI tutorials are abundant online, they constitute only small fraction of web content, making pre-filter essential for identifying relevant content. Similar patterns often appear in tutorials, such as distinctive keywords like click and type, as well as platform-specific terms like macOS and Windows. We compiled rule-based filter using keyword lists sourced from official websites and forums. Leveraging RedPajama data with over 20 billion URLs, our pre-filter applies Keyword Matching in the first 38k words, evaluates samples based on Length, and filters them by URL Format for relevance. Validated using 180 positive and 105 negative ground-truth samples, the prefilter achieved 92.69% recall rate on positive samples, ensuring both diversity and quantity. After filtering, the dataset size is reduced from 20.8 billion to 68.8 million entries (Figure 4). 3 Figure 3: Overview of the tutorial filtering and classification pipeline. Starting with Redpajama, the data is prefiltered, annotated by an advanced LLM, and used to train tutorial classifier. The classifier further filters the raw text, which is then paraphrased into structured tutorials with task descriptions, prerequisites, and step-by-step instructions. 2.1.2 LLM LABELER While initial rule-based filtering narrows the context, the proportion of true positive tutorial content remains low. To improve the quality and relevance of selected tutorials, we leverage an advanced LLM, GPT-4O MINI, for automated labeling, due to its strong ability to comprehend and analyze complex, information-dense text. Prior to full implementation, we tested GPT-4O MINI on manually annotated ground-truth validation set, where it achieved an F1 score nearly 90%. In cases where human and LLM annotations conflicted, the LLM demonstrated the ability to identify tutorial-related content in lengthy texts that humans might overlook. This, along with the validation set result, suggests that GPT-4O MINI may surpass human performance in webpage labeling, enabling efficient generation of large labeled dataset for training in the following section. 2.1.3 FASTTEXT FILTER Following the automated labeling process, we employed FastText, an n-gram-based deep learning model, as our classifier. FastText classifies tutorial text segments as tutorial or non-tutorial, with binary output and confidence score to enhance the accuracy of tutorial selection. To train the model, we combined LLM-labeled data with human-labeled samples, creating dataset of approximately 90,000 examples. The train-test split is 95:5, with the model demonstrating strong classification performance. Using this classifier, we further curated the initial filtered dataset, collecting approximately 18.8 million tutorial-like web text samples. Prefilter LLM FastText 0.69 0.885 0.895 Metric Precision Recall F1 0.60 0.89 0. 0.61 0.885 0.895 Table 2: Performance of Filters. 2.1.4 TAG & PARAPHRASE After filtering the raw tutorial content using the FastText model, we then tag and paraphrase the content for further processing, including extracting meta-information and formatting the tutorials according to standardized template. To handle the length and noise of the raw tutorial data, we employed GPT-4O MINI, streamlining the tagging and paraphrasing while ensuring the output aligned with the comprehensive template and gold-standard examples. The key components of the template include specifying the Platform and Target (e.g., macOS, Windows, browser, or app), providing concise Task Description, listing Prerequisites needed Figure 4: The data flow during the early stages of our pipeline. before starting, outlining Step-by-Step Instructions for completing the task, and detailing the Expected Outcome. The cost for tagging and paraphrasing 1,000 entries is approximately 0.89 dollars.",
                "summary": "<p>–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (—Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤) –ø–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—é —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º (GUI) –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º —à–∞–≥–æ–º –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (VLMs) –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á.</p>\n<p><strong>2.1 –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞</strong></p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤:</p>\n<ol>\n<li>\n<p><strong>–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (Pre-filter):</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞–±–æ—Ä —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"–∫–ª–∏–∫–Ω—É—Ç—å\", \"–≤–≤–µ—Å—Ç–∏\"), —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö –¥–ª—è —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤, –∞ —Ç–∞–∫–∂–µ –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, \"macOS\", \"Windows\").</li>\n<li>–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –±–æ–ª—å—à–æ–º—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö Redpajama (–±–æ–ª–µ–µ 20 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ URL).</li>\n<li>–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞ URL.</li>\n<li>–ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∞—è –ø–æ–ª–Ω–æ—Ç–∞ (recall) –≤ 92.69% –ø—Ä–∏ –æ—Ç–±–æ—Ä–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–µ—è—Ç—å –±–æ–ª—å—à—É—é —á–∞—Å—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∫—Ä–∞—Ç–∏–≤ –∏—Ö –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å 20.8 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ 68.8 –º–∏–ª–ª–∏–æ–Ω–∞ –∑–∞–ø–∏—Å–µ–π.</li>\n</ul>\n</li>\n<li>\n<p><strong>–†–∞–∑–º–µ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (LLM Labeler):</strong></p>\n<ul>\n<li>–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å GPT-4O MINI –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–Ω —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–º.</li>\n<li>–ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –≤—Ä—É—á–Ω—É—é —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –≥–¥–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç F1-–º–µ—Ä—É –æ–∫–æ–ª–æ 90%.</li>\n<li>–í —Å–ª—É—á–∞—è—Ö —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–∞–∑–º–µ—Ç–∫–æ–π —á–µ–ª–æ–≤–µ–∫–∞ –∏ LLM, –º–æ–¥–µ–ª—å –∏–Ω–æ–≥–¥–∞ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–ª–∞ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –≤ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ —á–µ–ª–æ–≤–µ–∫ –º–æ–≥ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å. –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏–ª–æ –ø–æ–ª—É—á–∏—Ç—å –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.</li>\n</ul>\n</li>\n<li>\n<p><strong>–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é FastText:</strong></p>\n<ul>\n<li>–û–±—É—á–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å FastText (–º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ n-–≥—Ä–∞–º–º–∞—Ö) –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ —Ç—É—Ç–æ—Ä–∏–∞–ª –∏–ª–∏ –Ω–µ —Ç—É—Ç–æ—Ä–∏–∞–ª.</li>\n<li>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–º–µ—Å—å –¥–∞–Ω–Ω—ã—Ö, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö LLM –∏ –≤—Ä—É—á–Ω—É—é, –≤—Å–µ–≥–æ –æ–∫–æ–ª–æ 90 000 –ø—Ä–∏–º–µ—Ä–æ–≤.</li>\n<li>–ú–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –æ—Å—Ç–∞–≤–∏–≤ –æ–∫–æ–ª–æ 18.8 –º–∏–ª–ª–∏–æ–Ω–∞ –ø—Ä–∏–º–µ—Ä–æ–≤, –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã.</li>\n</ul>\n</li>\n<li>\n<p><strong>–¢–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ (Tag &amp; Paraphrase):</strong></p>\n<ul>\n<li>–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é FastText, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã —Ç–µ–≥–∏—Ä—É—é—Ç—Å—è –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—é—Ç—Å—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.</li>\n<li>–î–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPT-4O MINI, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —à–∞–±–ª–æ–Ω–æ–º.</li>\n<li>–®–∞–±–ª–æ–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:<ul>\n<li><strong>–ü–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ —Ü–µ–ª—å:</strong> (–Ω–∞–ø—Ä–∏–º–µ—Ä, macOS, Windows, –±—Ä–∞—É–∑–µ—Ä –∏–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ).</li>\n<li><strong>–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏.</strong></li>\n<li><strong>–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —É—Å–ª–æ–≤–∏—è</strong> –¥–ª—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–ü–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</strong> –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.</strong></li>\n</ul>\n</li>\n<li>–≠—Ç–æ—Ç —ç—Ç–∞–ø –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—É—Ç–æ—Ä–∏–∞–ª—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ 1000 –∑–∞–ø–∏—Å–µ–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 0.89 –¥–æ–ª–ª–∞—Ä–∞.</li>\n</ul>\n</li>\n</ol>\n<p>–í –∏—Ç–æ–≥–µ, —ç—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–±–∏—Ä–∞—Ç—å –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—É—Ç–æ—Ä–∏–∞–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω—ã–º —à–∞–≥–æ–º –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á.</p>"
            },
            {
                "title": "Trajectory Data Collection via Guided Replay",
                "content": "Figure 5: Overview of Guided Replay data collection and evaluation pipeline. VLM agent is provided with the filtered and formatted tutorials, then observes and interacts with the real environment during execution, while all the actions and intermediate thoughts are recorded as data trajectory. The final result is evaluated by an advanced VLM to ensure the correctness. 2.2.1 TRAJECTORY DATA DEFINITION The trajectory data generated by our pipeline is designed to enhance an agents web navigation capabilities by integrating high-level planning, low-level instructions, and grounded operations. Each data instance includes the following components: Task Information. Detailed task metadata, including platform, task description, prerequisites, instructions, and expected outcomes, which support both planning and execution. Post-processed Textual Trajectory. Refined after replay, highlighting key elements for model finetuning. This includes Task Metadata, summarizing the task to encourage adaptive decision-making, Observations to provide visual context, Intermediate Reasoning offering insights into the agents decision-making process, and Action Sequence to capture detailed element information for web interactions. Screenshots and Video Recordings. Visual records of the entire process for comprehensive documentation. Reproducible Native Trace. Captured via Playwright, including DOM snapshots, HTML, network flow, and action sequences, allowing full reconstruction and detailed analysis of agent-environment interactions.",
                "summary": "<p>–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –∏ –æ—Ü–µ–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–º–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–ü—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (Guided Replay):</strong></p>\n<ol>\n<li><strong>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞:</strong> –ê–≥–µ–Ω—Ç—É (VLM agent) –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (tutorials).</li>\n<li><strong>–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ:</strong> –ê–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å —Ä–µ–∞–ª—å–Ω–æ–π –≤–µ–±-—Å—Ä–µ–¥–æ–π, –≤—ã–ø–æ–ª–Ω—è—è –∑–∞–¥–∞–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.</li>\n<li><strong>–ó–∞–ø–∏—Å—å:</strong> –í –ø—Ä–æ—Ü–µ—Å—Å–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∑–∞–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞, –µ–≥–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠—Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û—Ü–µ–Ω–∫–∞:</strong> –ü–æ–ª—É—á–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π VLM (–±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏), —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n</ol>\n<p><strong>–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏:</strong></p>\n<p>–î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –∞–≥–µ–Ω—Ç–∞ –∫ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∑–∞ —Å—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è:</p>\n<ul>\n<li><strong>–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–¥–∞—á–µ (Task Information):</strong> –ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –∑–∞–¥–∞—á–µ, —Ç–∞–∫–∏–µ –∫–∞–∫ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞, –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏, –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —É—Å–ª–æ–≤–∏—è, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –æ–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç—É –∫–∞–∫ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏, —Ç–∞–∫ –∏ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Ç–µ–∫—Å—Ç–æ–≤–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è (Post-processed Textual Trajectory):</strong> –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è, –¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è (replay). –û–Ω–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:<ul>\n<li><strong>–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (Task Metadata):</strong> –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.</li>\n<li><strong>–ù–∞–±–ª—é–¥–µ–Ω–∏—è (Observations):</strong> –í–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç–æ–º –≤–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏.</li>\n<li><strong>–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (Intermediate Reasoning):</strong> –ó–∞–ø–∏—Å–∏ –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –∞–≥–µ–Ω—Ç–æ–º.</li>\n<li><strong>–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π (Action Sequence):</strong> –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –∞–≥–µ–Ω—Ç–∞ —Å –≤–µ–±-—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏.</li>\n</ul>\n</li>\n<li><strong>–°–∫—Ä–∏–Ω—à–æ—Ç—ã –∏ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏ (Screenshots and Video Recordings):</strong> –í–∏–∑—É–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.</li>\n<li><strong>–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–π –Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ª–µ–¥ (Reproducible Native Trace):</strong> –ó–∞–ø–∏—Å—å, –ø–æ–ª—É—á–µ–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é Playwright, –≤–∫–ª—é—á–∞—é—â–∞—è –≤ —Å–µ–±—è —Å–Ω–∏–º–∫–∏ DOM, HTML, —Å–µ—Ç–µ–≤–æ–π —Ç—Ä–∞—Ñ–∏–∫ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å–æ —Å—Ä–µ–¥–æ–π –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑.</li>\n</ul>\n<p><strong>–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:</strong> –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ Guided Replay, —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, –∫–∞–∫ –∞–≥–µ–Ω—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å –≤–µ–±-—Å—Ä–µ–¥–æ–π. –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è, —Ç–∞–∫ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—É—á–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.</p>"
            },
            {
                "title": "Guided Replay with Tutorials",
                "content": "Although we have collected and processed high-quality tutorials, significant gap remains in acquiring the grounding data crucial for training more effective agent model. To address this, we leverage BrowserGym (Drouin et al., 2024) to enable the model to replay tasks under the guidance of the generated tutorials. Figure 6: Guided replay example. This example demonstrates an agents execution of finding the return policy for mens football apparel, showcasing its actions alongside the corresponding inner thoughts. BrowserGym is versatile environment for web task automation in the Chromium browser, enabling Visual Language Model (VLM) agents to execute web-based operations (Drouin et al., 2024). Agents are provided with tagged and paraphrased tutorials and target web url, allowing them to navigate directly to the task scene. Step-by-step instructions guide the agent through the task, with the expected outcome determining task completion. The agents initial observations include the webpages viewport screenshot and accessibility tree (AXTree), but the HTML file is excluded due to its size and irrelevance to visual agents. Actions are executed using Playwright (Microsoft, 2023) functions such as click, select option, and clear, while Playwright also records detailed traces, including target elements, coordinates, screenshots, and DOM snapshots at the same time, along with agents internal thoughts between actions. Token consumption is about 8,027 per step and 86,114 per task. With GPT-4O-08-06, replaying 1,000 tasks costs approximately 215 dollars. Cost detail see 2.2.3 EVALUATION OF TRAJECTORY Although large amount of guided replay data has been recorded, it is crucial to extract the effective segments that can truly contribute to enhancing the agents performance. Recent work by (Pan et al., 2024) highlights the potential of Visual Language Models (VLMs) in evaluating trajectory data using recorded images and interaction processes as input. VLMs are highly scalable, capable of processing large datasets concurrently at low cost, and provide transparent evaluations. Therefore, we implemented VLM Evaluator to further improve our data quality. VLM Evaluator Design. To ensure trajectory data quality, we define effectiveness based on two criteria: adherence to task instructions and successful completion of core components. We employ GPT-4O as the backbone of our VLM evaluator, using structured prompt to assess recorded trajectories. The evaluator receives the task description d, the agents action history a, and inner thoughts for each step. The sequential format is: {task description; inner thought 1; action 1; inner thought 2; action 2; ...}, as illustrated in Figure 5. The VLM provides trajectory-level assessment and performs stepwise analysis, offering justifications for any ineffective trajectory and identifying the earliest point of failure. Validation on Human-Annotated Set. Although the capabilities of Vision Language Models (VLMs) are well-recognized, validation is essential. To assess the automatic evaluators perfor6 Table 3: Evaluator Accuracy Comparison Table 4: Cost Breakdown Trajectory Evaluator Replayed Web Tutorials GPT-4o WebArena Results GPT-4V Cap. + GPT-4 Cap. + Mixtral Acc. 84.0% 80.6% 82.1% 74.4% Phase Cost/1k ($) Model T&P Replay Eval Total 0.89 215.36 3.10 219.35 gpt-4o-mini gpt-4o gpt-4o mance, we manually reviewed 1,081 trajectories and created validation set of 558 samples with human-annotated justifications. As shown in the Table 3, despite handling different input formats across various task scenarios, the evaluator achieved high performance metrics. And according to the observation shown in Appendix D, evaluator often applys stricter standards than human evaluators. This demonstrates its robustness in accurately identifying effective trajectories. 2.3 TRAIN AND FINE-TUNE THE MODEL WITH TRAJECTORY DATA We chose purely visual agent model that relies exclusively on screenshot-based observations, rather than incorporating accessibility trees or textual representations, for several reasons. First, GUIs are inherently visual, and mapping instructions to visual elements aligns more closely with human cognitive processes. Second, Textual representations, such as HTML or accessibility trees, are often verbose, which leads to heavy overhead for computation. Second, Different websites can have varying structures for their textual representation while image-based representations allow the model to unify its observations across diverse platforms with varying resolutions, improving generalization. 2.3.1 PURE VISION & GUI ACTION FRAMEWORK In this work, we propose to unify observation and action space via pure vision and standard pyautogui commands with pluggable action system. Using pure vision as input eliminates the need for the model to understand different UI source codes across platforms, even when visually similar elements are written differently in HTML. Additionally, HTML input typically costs an average of 4,000 tokens per step. In contrast, recent VLMs with high-resolution multimodal understanding, such as Qwen2-VL, require only 1,200 tokens for 720p image. This significantly lowers the computational cost while maintaining sufficient visual information for the task. For action, we hoose the widely used standard pyautogui action space with pluggable action system. Most web agent leverage playwright action sapce. But playwright actions incline to interact with html selector element instead of visual ui element. Therefore, we use pyautogui commands to unify basic GUI operations on web. Since we collect the data from website by playwright, we need to map the playwright actions to pyautogui actions as shown in the Figure 9. In addition, we utilize pluggable action system to cover specific playwright action like select option. 2.3.2 MODEL ARCHITECTURE AND TRAINING Unlike agents that rely on structured UI representations like accessibility trees, vision-based grounding requires models to map intents directly to visual observations. For this, we chose Qwen2VL (Wang et al., 2024), which uses NaViT as an image encoder with dynamic resolution support Dehghani et al. (2023). By removing absolute position embeddings and incorporating 2D-RoPE Su et al. (2024), Qwen2-VL can process images of any resolution, efficiently converting them into variable visual tokens. This makes Qwen2-VL ideal for GUI agents, as it can encode high-resolution images with fewer token costs, making it well-suited for our tasks. Our training process, starting with VLM capable of high-resolution image understanding, consists of one tunnig stage. We use data from AgentTrek Data to enhance VLM capabilities in grounding and planning.",
                "summary": "<h2>–ò–∑–ª–æ–∂–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Å—Ç–∞—Ç—å–∏: –£–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞</h2>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∞–≥–µ–Ω—Ç–æ–≤, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –ø—Ä–æ–±–ª–µ–º–æ–π –Ω–µ—Ö–≤–∞—Ç–∫–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ß—Ç–æ–±—ã —Ä–µ—à–∏—Ç—å —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É, –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Å—Ä–µ–¥—É BrowserGym, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ \"–ø–µ—Ä–µ–∏–≥—Ä—ã–≤–∞—Ç—å\" –∑–∞–¥–∞—á–∏, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–Ω–µ–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã.</p>\n<p><strong>BrowserGym</strong> - —ç—Ç–æ —Å—Ä–µ–¥–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –≤–µ–±-–∑–∞–¥–∞—á –≤ –±—Ä–∞—É–∑–µ—Ä–µ Chromium. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–º –º–æ–¥–µ–ª–∏ VLM (Visual Language Model), –≤—ã–ø–æ–ª–Ω—è—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö. –ê–≥–µ–Ω—Ç–∞–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ URL —Ü–µ–ª–µ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã. –ü–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–µ–¥—É—Ç –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á—É, –∞ –µ–µ —É—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.</p>\n<p>–ù–∞–±–ª—é–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –≤–∫–ª—é—á–∞—é—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç –≤–∏–¥–∏–º–æ–π –æ–±–ª–∞—Å—Ç–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–µ—Ä–µ–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (AXTree). HTML-—Ñ–∞–π–ª –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è –∏–∑-–∑–∞ –µ–≥–æ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –î–µ–π—Å—Ç–≤–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–π Playwright, —Ç–∞–∫–∏—Ö –∫–∞–∫ –∫–ª–∏–∫, –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∞ –ø–æ–ª—è. Playwright —Ç–∞–∫–∂–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏, –≤–∫–ª—é—á–∞—è —Ü–µ–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –∏ DOM-—Å–Ω–∏–º–∫–∏, –∞ —Ç–∞–∫–∂–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –º–µ–∂–¥—É –¥–µ–π—Å—Ç–≤–∏—è–º–∏.</p>\n<p>–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∞: –æ–∫–æ–ª–æ 8027 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —à–∞–≥ –∏ 86114 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –∑–∞–¥–∞—á—É. –ü–µ—Ä–µ–∏–≥—Ä—ã–≤–∞–Ω–∏–µ 1000 –∑–∞–¥–∞—á —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT-4O-08-06 –æ–±—Ö–æ–¥–∏—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 215 –¥–æ–ª–ª–∞—Ä–æ–≤.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π</strong></p>\n<p>–ü–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏ –±–æ–ª—å—à–æ–≥–æ –æ–±—ä–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–µ—Ä–µ–∏–≥—Ä–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö, –∞–≤—Ç–æ—Ä—ã —Å—Ç–∞—Ç—å–∏ —Å—Ç–æ–ª–∫–Ω—É–ª–∏—Å—å —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –≤—ã–¥–µ–ª–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –º–æ–≥—É—Ç —É–ª—É—á—à–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞. –î–ª—è —ç—Ç–æ–≥–æ –æ–Ω–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ <strong>VLM Evaluator</strong> (–æ—Ü–µ–Ω—â–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ VLM), –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. VLM-–º–æ–¥–µ–ª–∏ —Ö–æ—Ä–æ—à–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É—é—Ç—Å—è, —Å–ø–æ—Å–æ–±–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω—É—é –æ—Ü–µ–Ω–∫—É.</p>\n<p><strong>–î–∏–∑–∞–π–Ω VLM Evaluator</strong></p>\n<p>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–≤—É–º—è –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏: —Å–æ–±–ª—é–¥–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —É—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∑–∞–¥–∞—á–∏. –í –∫–∞—á–µ—Å—Ç–≤–µ –æ—Å–Ω–æ–≤—ã –æ—Ü–µ–Ω—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å GPT-4O. –û—Ü–µ–Ω—â–∏–∫ –ø–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏, –∏—Å—Ç–æ—Ä–∏—é –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ –∏ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ. –î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ: {–æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏; —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ 1; –¥–µ–π—Å—Ç–≤–∏–µ 1; —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ 2; –¥–µ–π—Å—Ç–≤–∏–µ 2; ...}. VLM –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –ø–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑, –æ–±–æ—Å–Ω–æ–≤—ã–≤–∞—è –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è—è –º–æ–º–µ–Ω—Ç –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–∏.</p>\n<p><strong>–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π</strong></p>\n<p>–î–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞ –∞–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ 558 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–π –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ 1081 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω—â–∏–∫–∞, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ä–∞–∑–ª–∏—á–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –∑–∞–¥–∞—á. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω—â–∏–∫ —á–∞—Å—Ç–æ –ø—Ä–∏–º–µ–Ω—è–ª –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã, —á–µ–º –ª—é–¥–∏-–æ—Ü–µ–Ω—â–∏–∫–∏, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ –µ–≥–æ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –≤ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π.</p>\n<p><strong>–û–±—É—á–µ–Ω–∏–µ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∞–≤—Ç–æ—Ä—ã –≤—ã–±—Ä–∞–ª–∏ –º–æ–¥–µ–ª—å, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–∫—Ä–∏–Ω—à–æ—Ç–∞—Ö), –∞ –Ω–µ –Ω–∞ –¥–µ—Ä–µ–≤—å—è—Ö –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö. –≠—Ç–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–æ —Ç–µ–º, —á—Ç–æ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –ø–æ —Å–≤–æ–µ–π –ø—Ä–∏—Ä–æ–¥–µ –≤–∏–∑—É–∞–ª—å–Ω—ã, –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –±–æ–ª—å—à–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—é. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–º–∏ –∏ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –±–æ–ª—å—à–∏–º –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –∑–∞—Ç—Ä–∞—Ç–∞–º.</p>\n<p><strong>–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –¥–µ–π—Å—Ç–≤–∏–π –≤ GUI</strong></p>\n<p>–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏ –¥–µ–π—Å—Ç–≤–∏–π —Å –ø–æ–º–æ—â—å—é –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –∫–æ–º–∞–Ω–¥ pyautogui —Å –ø–æ–¥–∫–ª—é—á–∞–µ–º–æ–π —Å–∏—Å—Ç–µ–º–æ–π –¥–µ–π—Å—Ç–≤–∏–π. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏ –ø–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±—É—é—Ç –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å HTML.</p>\n<p>–í –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–µ–π—Å—Ç–≤–∏–π –≤—ã–±—Ä–∞–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã pyautogui, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å GUI. –ü–æ—Å–∫–æ–ª—å–∫—É –¥–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Playwright, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è Playwright –≤ –¥–µ–π—Å—Ç–≤–∏—è pyautogui. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∞–µ–º–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –æ—Ö–≤–∞—Ç–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π Playwright, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤—ã–±–æ—Ä –æ–ø—Ü–∏–∏.</p>\n<p><strong>–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏–µ</strong></p>\n<p>–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è —Å–ø–æ—Å–æ–±–Ω–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è–º–∏, –±—ã–ª–∞ –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å Qwen2-VL, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è NaViT –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. Qwen2-VL —Å–ø–æ—Å–æ–±–Ω–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª—é–±–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –∏—Ö –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç Qwen2-VL –∏–¥–µ–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ GUI, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ –º–æ–∂–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å –º–µ–Ω—å—à–∏–º–∏ –∑–∞—Ç—Ä–∞—Ç–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.</p>\n<p>–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –∏–∑ AgentTrek Data –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π VLM –≤ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–≤—è–∑–∫–∏ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.</p>"
            },
            {
                "title": "Experimental Setup",
                "content": "Agent Training. For the text-based web agent, we utilize 6,000 agent trajectories, included accessibility tree as observations and playwright actions as the agents action space, from the AgentTrek dataset to fine-tune the Qwen2 series LLMs of various sizes, including 7B, 32B, and 72B. For the vision-only Web agent, we fine-tune Qwen2-VL Wang et al. (2024) using 10,000 selected agent trajectories from the dataset. Evaluation For Text-based Web Agent. To demostrate the capability of the text-based agent, we select WebArena Zhou et al. (2023) as evaluation benchmark. WebArena, based on real websites, creates multiple virtual environments and uses various evaluation methods to assess the task completion rate, making it more suitable for real-world task completion evaluation. Evaluation For Vision-based Web Agent. To validate the effectiveness of our dataset, it is essential to demonstrate its impact on improving both the grounding and planning capabilities of the model. Therefore, we will evaluate the models performance on two benchmarks that assess these abilities. (1) ScreenSpot (Cheng et al., 2024), which is GUI visual grounding benchmark comprising 1.2K single-step instructions and target element bounding boxes. It spans mobile, desktop, and web environments, categorizing elements into text and icons. Given that our data originates exclusively from the web, we focus solely on web-based performance. (2) Multimodal-Mind2Web (Deng et al., 2024), the multimodal extension of the web agent benchmark Mind2Web (Deng et al., 2024). It consists of three categories, namely, cross-task, cross-website and cross-domain, ranked by their degree of deviation from the training data. 3.2 MAIN RESULTS WebArena. Through the experimental results from table 5, we can obtain that: (1) AgentTreks textual trajectories significantly boost performance on WebArena, surpassing open-source baselines and GPT-4o. (2) Considering that WebArena is an OOD web agent benchmark featuring self-hosted websites, strong results on WebArena confirm that AgentTreks data generalizes well to unseen domains. Table 5: Comparison of task success rate on WebArena Model WebArena CodeLlama-7B-Instruct (Ou et al., 2024) LLaMa3-chat-8B (Ou et al., 2024) Qwen2.5-7B-Instruct LLama3-chat-70B (Ou et al., 2024) GPT-4o(Zhou et al., 2023) GPT-4(Ou et al., 2024) Synatra-CodeLlama-7B (Ou et al., 2024) AutoWebGLM (OOD SFT) (Lai et al., 2024) AutoWebGLM (In-domain RFT) (Lai et al., 2024) Qwen2.5-7B-Instruct w/ AgentTrek Qwen2.5-32B-Instruct w/ AgentTrek 0.00 3.32 3.80 7.02 13.10 14.41 6.28 8.50 18.20 10.46 16. ScreenSpot. Fine-tuning with the AgentTrek dataset significantly improved Qwen2-VLs grounding ability for both text and icon-based tasks, more than doubling its baseline performance and surpass8 ing several models on the ScreenSpot benchmark. This demonstrates the strong impact of AgentTrek in enhancing the models grounding capabilities for web-based GUI tasks. Table 6: Comparison of grounding performance on ScreenSpot Web Grounding Model Text Icon/Widget Average GPT-4 (Cheng et al., 2024) GPT-4o (Cheng et al., 2024) Qwen2-VL-7B SeeClick (Cheng et al., 2024) CogAgent (Cheng et al., 2024) GPT-4 + OmniParser (Lu et al., 2024) Qwen2-VL-7B w/ AgentTrek 9.2 12.2 35.2 55.7 70.4 81.3 81.7 8.8 7.8 25.7 32.5 28.6 51. 51.5 9.0 10.1 30.7 44.7 50.7 67.0 67.4 Mind2Web. The baseline Qwen2-VL-7B model is excluded due to its poor ability to locate target elements, critical requirement for web-based tasks. As result, only the fine-tuned models are included in the table. Fine-tuning with the AgentTrek dataset greatly enhanced Qwen2-VLs performance, particularly in the Operation F1 metric, where it surpassed both GPT-3.5 and GPT-4 in all settings. The combination of AgentTrek and Mind2Web datasets delivers the best results across all metrics and settings. While fine-tuning with Mind2Web alone yields strong performance, the model still benefits from the addition of AgentTrek data. These findings highlight the complementary strengths of the two datasets: AgentTrek provides precise, grounded data, while Mind2Web contributes valuable resources for tackling complex web-based tasks. Additional details on result sources are provided in Appendix J.2. Table 7: Performance comparison across different methods and evaluation settings. H, I, AT, M2W stand for HTML, Image, AgentTrek, Mind2Web Obs Model Method Cross-Task Cross-Website Cross-Domain Ele.Acc Op.F Step SR Ele.Acc Op.F1 Step SR Ele.Acc Op.F1 Step SR HTML + Image GPT-3.5 GPT-4 GPT-4 GPT-4 Choice Choice Choice SoM Qwen2-VL Vision + AT + M2W Vision + AT + M2W Vision 19.4 40. 46.4 29.6 45.5 54.8 60.8 59.2 63.1 73.4 - 84.9 89.5 88.9 16.8 32. 40.2 20.3 40.9 50.9 55.7 14.9 30.2 38.0 20.1 40.8 52.9 57.6 56.5 61. 67.8 - 82.8 83.9 88.1 14.1 27.0 32.4 13.9 35.1 44.9 51.4 25.2 35. 42.4 27.0 48.6 51.8 56.0 57.9 61.9 69.3 - 84.1 86.8 87.5 24.1 29. 36.8 23.7 42.1 47.7 52.",
                "summary": "<p>–í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏, –∫–∞–∫ —Å —Ç–µ–∫—Å—Ç–æ–≤–æ–π, —Ç–∞–∫ –∏ —Å –≤–∏–∑—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.</p>\n<p><strong>–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤.</strong> –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å 6000 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–∞ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö AgentTrek. –í –∫–∞—á–µ—Å—Ç–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å –¥–µ—Ä–µ–≤–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (accessibility tree), –∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞ ‚Äî –¥–µ–π—Å—Ç–≤–∏—è Playwright. –ú–æ–¥–µ–ª–∏ Qwen2 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (7B, 32B –∏ 72B) –±—ã–ª–∏ –¥–æ–æ–±—É—á–µ–Ω—ã –Ω–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞ –±—ã–ª–∞ –¥–æ–æ–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å Qwen2-VL, –∏—Å–ø–æ–ª—å–∑—É—è 10000 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–∞ –∏–∑ —Ç–æ–≥–æ –∂–µ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞.</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –±—ã–ª –≤—ã–±—Ä–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ WebArena, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–∞—Ö –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ü–µ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö.</p>\n<p><strong>–û—Ü–µ–Ω–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–µ–±-–∞–≥–µ–Ω—Ç–∞.</strong> –î–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–≤–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä—è—é—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–∏–≤—è–∑–∫–µ (grounding) –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.\n*   <strong>ScreenSpot</strong> ‚Äî –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π 1200 –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏—Ö —Ä–∞–º–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –º–æ–±–∏–ª—å–Ω—ã–µ, –¥–µ—Å–∫—Ç–æ–ø–Ω—ã–µ –∏ –≤–µ–±-—Å—Ä–µ–¥—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É—è —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —Ç–µ–∫—Å—Ç –∏ –∏–∫–æ–Ω–∫–∏. –¢–∞–∫ –∫–∞–∫ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã —Ç–æ–ª—å–∫–æ –∏–∑ –≤–µ–±-—Å—Ä–µ–¥—ã, –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–≤–æ–¥–∏–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –≤ –≤–µ–±-—Å—Ä–µ–¥–µ.\n*   <strong>Multimodal-Mind2Web</strong> ‚Äî –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞ Mind2Web, —Å–æ—Å—Ç–æ—è—â–µ–µ –∏–∑ —Ç—Ä–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π (cross-task, cross-website –∏ cross-domain), —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –æ—Ç –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.</p>\n<p><strong>–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.</strong></p>\n<p><strong>WebArena.</strong> –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ WebArena, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –æ—Ç–∫—Ä—ã—Ç—ã–µ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ GPT-4o. –°–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ WebArena, –∫–æ—Ç–æ—Ä—ã–π —è–≤–ª—è–µ—Ç—Å—è –±–µ–Ω—á–º–∞—Ä–∫–æ–º –¥–ª—è –≤–µ–±-–∞–≥–µ–Ω—Ç–æ–≤, —Ä–∞–±–æ—Ç–∞—é—â–∏—Ö —Å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —Ä–∞–∑–º–µ—â–µ–Ω–Ω—ã–º–∏ —Å–∞–π—Ç–∞–º–∏, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ AgentTrek —Ö–æ—Ä–æ—à–æ –æ–±–æ–±—â–∞—é—Ç—Å—è –Ω–∞ –Ω–µ–≤–∏–¥–∏–º—ã–µ –¥–æ–º–µ–Ω—ã.</p>\n<p><strong>ScreenSpot.</strong> –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ Qwen2-VL –∫ –ø—Ä–∏–≤—è–∑–∫–µ –∫–∞–∫ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, —Ç–∞–∫ –∏ –¥–ª—è –∏–∫–æ–Ω–æ—á–Ω—ã—Ö –∑–∞–¥–∞—á, –±–æ–ª–µ–µ —á–µ–º —É–¥–≤–æ–∏–≤ –±–∞–∑–æ–≤—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –ø—Ä–µ–≤–∑–æ–π–¥—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ ScreenSpot. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ AgentTrek –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∫ –ø—Ä–∏–≤—è–∑–∫–µ –¥–ª—è –≤–µ–±-–∑–∞–¥–∞—á —Å –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.</p>\n<p><strong>Mind2Web.</strong> –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Qwen2-VL-7B –±—ã–ª–∞ –∏—Å–∫–ª—é—á–µ–Ω–∞ –∏–∑ –æ—Ü–µ–Ω–∫–∏ –∏–∑-–∑–∞ –µ–µ –Ω–∏–∑–∫–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞—Ö–æ–¥–∏—Ç—å —Ü–µ–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AgentTrek –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏–ª–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å Qwen2-VL, –æ—Å–æ–±–µ–Ω–Ω–æ –≤ –º–µ—Ç—Ä–∏–∫–µ Operation F1, –≥–¥–µ –æ–Ω–∞ –ø—Ä–µ–≤–∑–æ—à–ª–∞ GPT-3.5 –∏ GPT-4 –≤–æ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö. –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö AgentTrek –∏ Mind2Web –¥–∞–µ—Ç –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º. –•–æ—Ç—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö Mind2Web —Ç–∞–∫–∂–µ –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö AgentTrek –≤—Å–µ —Ä–∞–≤–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –≠—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∑–∞–∏–º–æ–¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –¥–≤—É—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö: AgentTrek –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–∞–Ω–Ω—ã–µ, –∞ Mind2Web –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ü–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤–µ–±-–∑–∞–¥–∞—á.</p>"
            },
            {
                "title": "Analysis",
                "content": "With our AgentTrek pipeline, we generate large-scale trajectory data that excels in three areas. First, the dataset offers extensive diversity, covering multiple domains and task types, and benefiting from internet-sourced tutorials that enhance task execution. Our experiment showed 230% performance increase when agents followed detailed instructions. Second, the data is gathered from real-world web environments, avoiding the limitations of simulations. Starting with RedPajama, we filtered and processed 23,430 tutorials, producing 10,398 successful trajectories from 127 websites. Third, the data is comprehensive, capturing highand low-level task details, including DOM/HTML structures, AXTree snapshots, video recordings, and screenshots. This rich data improves the agents performance on long-horizon tasks, and with per-trajectory cost of just $0.551, our pipeline offers an efficient, scalable solution for data generation. 4.1 IMPORTANCE OF TUTORIALS Tutorials extracted from the internet play crucial role in guiding the replay process. First, they ensure diversity in the generated trajectories. Tutorials often have distinct task goals, and even when they target the same objective, they may offer different execution methods, enriching the trajectory 9 data. Second, tutorials significantly improve the agents execution. We tested the agent on 400 tasks, replaying them twice: once with tutorials and once using only high-level goals. The results show that step-by-step instructions greatly enhanced performance. Without tutorials, only 63 effective trajectories were generated (15.78% of the total). With tutorials, the agent produced 208 effective trajectories (52%), marking an increase of over 230%, demonstrating the importance of detailed instructions in improving reliability and effectiveness. For further analysis, please see Appendix B",
                "summary": "<p><strong>4.1 –í–∞–∂–Ω–æ—Å—Ç—å –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤</strong></p>\n<p>–û–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞, –∏–≥—Ä–∞—é—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –¥–µ–π—Å—Ç–≤–∏–π –∞–≥–µ–Ω—Ç–∞. –í–æ-–ø–µ—Ä–≤—ã—Ö, –æ–Ω–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π. –†–∞–∑–ª–∏—á–Ω—ã–µ –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —á–∞—Å—Ç–æ –∏–º–µ—é—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ü–µ–ª–∏, –∞ –¥–∞–∂–µ –ø—Ä–∏ —Å—Ö–æ–∂–∏—Ö —Ü–µ–ª—è—Ö –º–æ–≥—É—Ç –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ–±–æ–≥–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö.</p>\n<p>–í–æ-–≤—Ç–æ—Ä—ã—Ö, –æ–±—É—á–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞–º–µ—Ç–Ω–æ —É–ª—É—á—à–∞—é—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á –∞–≥–µ–Ω—Ç–æ–º. –í —Ö–æ–¥–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∞–≥–µ–Ω—Ç—É –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å 400 –∑–∞–¥–∞—á –¥–≤–∞–∂–¥—ã: –æ–¥–∏–Ω —Ä–∞–∑, —Å–ª–µ–¥—É—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∏ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑, –æ–ø–∏—Ä–∞—è—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ü–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ë–µ–∑ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –≤—Å–µ–≥–æ 63 —É—Å–ø–µ—à–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (15.78% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞). –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –≤–æ–∑—Ä–æ—Å–ª–æ –¥–æ 208 (52%), —á—Ç–æ –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 230% –±–æ–ª—å—à–µ. –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞. (–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ B).</p>\n<p><em>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç—Å—è, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞ –∑–∞ —Å—á–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –ø–æ—à–∞–≥–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π.</em></p>"
            },
            {
                "title": "Data Composition",
                "content": "Figure 7: The distribution of website with domains involved in our dataset To summarize the data flow through our pipeline: First, we filter tutorial data from the RedPajama web snapshot. Next, the filtered data is paraphrased for clarity and classification. We then gather up-to-date data from mainstream websites for replay and, finally, collect effective trajectory data from the replays. After filtering RedPajamas vast dataset, we retained over 18.8 million entries. By applying criteria such as recency and popularity, 23,430 tutorials were prepared for replay. With success rate of 44.4%, we generated 10,398 trajectories, covering 127 websites across 11 distinct categories. 4.3 COMPARISON WITH EXISTING WORK AND RESEARCH CHALLENGES AgentTrek generates comprehensive, large-scale trajectory data, excelling in several key areas as shown in Table 1 (Niu et al., 2024; L`u et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024). First, with nearly 5k verified trajectories and an average of 12.1 steps per trajectory, our dataset provides strong foundation for training and evaluating agents on longhorizon web tasks. Second, it is the most complete dataset to date, incorporating DOM/HTML structures, AXTree data, intermediate reasoning steps, full video recordings, and corresponding screenshots for each action. Moreover, despite full automation without human intervention, our dataset maintains diversity across 120 websites and 12 distinct task categories. By leveraging modern large language models (LLMs), we can extract both high-level task objectives and detailed step-by-step instructions, offering flexibility for future use. Finally, our pipeline significantly reduces the cost and scalability challenges of human-annotated data collection. With success rate factored in, the cost per trajectory is just $0.551, making our approach both efficient and scalable for large-scale data generation. Cost detail see C",
                "summary": "<p><strong>–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç–æ–≤ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏</strong></p>\n<p>–í —Ä–∞–º–∫–∞—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ –≤–µ–±-—Å–Ω–∏–º–∫–∞ RedPajama. –≠—Ç–∏ –¥–∞–Ω–Ω—ã–µ –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É—é—Ç—Å—è –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –î–∞–ª–µ–µ, —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–æ–≤ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è (replay), –∏, –Ω–∞–∫–æ–Ω–µ—Ü, –∏–∑ —ç—Ç–∏—Ö –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑ –æ–≥—Ä–æ–º–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö RedPajama –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –±–æ–ª–µ–µ 18.8 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –∑–∞–ø–∏—Å–µ–π. –ü—Ä–∏–º–µ–Ω–∏–≤ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏, –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –±—ã–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ 23 430 –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤. –° –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ 44.4% –±—ã–ª–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 10 398 —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏—Ö 127 –≤–µ–±-—Å–∞–π—Ç–æ–≤ –≤ 11 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ–±-—Å–∞–π—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω–∞–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –Ω–∞ —Ä–∏—Å—É–Ω–∫–µ 7 (–Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è).</p>\n<p>–í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–∞–±–æ—Ç–∞–º–∏ (Niu et al., 2024; Lu et al., 2024; Deng et al., 2024; Yao et al., 2022; Song et al., 2024; Wornow et al., 2024), AgentTrek –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–µ, –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∏—Ö –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª—é—á–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (—Å–º. –¢–∞–±–ª–∏—Ü—É 1, –Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è). –í–æ-–ø–µ—Ä–≤—ã—Ö, —Å –ø–æ—á—Ç–∏ 5 —Ç—ã—Å—è—á–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∏ –≤ —Å—Ä–µ–¥–Ω–µ–º 12.1 —à–∞–≥–∞ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–æ—á–Ω—É—é –æ—Å–Ω–æ–≤—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ –∑–∞–¥–∞—á–∞—Ö –≤–µ–±-–Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Å –¥–ª–∏–Ω–Ω—ã–º–∏ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º–∏. –í–æ-–≤—Ç–æ—Ä—ã—Ö, —ç—Ç–æ —Å–∞–º—ã–π –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–π –¥–µ–Ω—å, –≤–∫–ª—é—á–∞—é—â–∏–π DOM/HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –¥–∞–Ω–Ω—ã–µ AXTree, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —ç—Ç–∞–ø—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –ø–æ–ª–Ω—ã–µ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è. –ë–æ–ª–µ–µ —Ç–æ–≥–æ, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø–æ–ª–Ω—É—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é –±–µ–∑ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞, –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ 120 –≤–µ–±-—Å–∞–π—Ç–∞–º –∏ 12 —Ä–∞–∑–ª–∏—á–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∑–∞–¥–∞—á. –ë–ª–∞–≥–æ–¥–∞—Ä—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) –º–æ–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∫–∞–∫ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Ü–µ–ª–∏ –∑–∞–¥–∞—á, —Ç–∞–∫ –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è. –ù–∞–∫–æ–Ω–µ—Ü, –∫–æ–Ω–≤–µ–π–µ—Ä –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –∑–∞—Ç—Ä–∞—Ç—ã –∏ –ø—Ä–æ–±–ª–µ–º—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–æ —Å–±–æ—Ä–æ–º –¥–∞–Ω–Ω—ã—Ö, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª—é–¥—å–º–∏. –° —É—á–µ—Ç–æ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏, —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–¥–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—Å–µ–≥–æ 0.551 –¥–æ–ª–ª–∞—Ä–∞, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö. –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ C (–Ω–µ –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è).</p>"
            },
            {
                "title": "Related Work",
                "content": "LLM-based Agents. LLM-based agents are autonomous systems that leverage large language models (Brown et al., 2020) to interact with real-world websites and os environments. These agents can 10 understand natural language instructions and perform wide range of complex tasks across various domains, such as e-commerce, online assistance, and knowledge navigation (Nakano et al., 2021; Cheng et al., 2024). Recent efforts in this space include models like SeeAct (Zheng et al., 2024) and WebVoyager (He et al., 2024), which aim to generalize agent behavior to real-world websites. While LLM-based agents have shown promise, challenges remain in the need for agent specified data. Our work extends this line of research by introducing cost-effective pipeline to generate comprehensive agent trajectory data, advancing the state-of-the-art in data synthesis for agent-based applications. Agent Data. As agents gain increasing popularity, the demand for efficient and scalable data is becoming both larger and more urgent. However, most existing data primarily serve as supplements to various benchmarks (Zhou et al., 2023; Li et al., 2023; Deng et al., 2024), with few datasets specifically designed for agent trajectory analysis. Furthermore, these datasets are often limited by the need for human annotation, which hampers scalability. In our work, our pipeline managed to automatically generate comprehensive agent trajectory data in cost-efficient manner, paving the way for new direction in data synthesis within the field of agents. Automatic Evaluation for Digital Agents. Recently, there has been growing interest in automating the evaluation of digital agents using Vision-Language Models (VLMs) and Large Language Models (LLMs). These methods leverage models to assess agent performance in real-world tasks. Research in this area spans several dimensions: some works focus on trajectory-level success (Pan et al., 2024), while others evaluate stepwise success based on adherence to instructions (Wornow et al., 2024). Additionally, evaluations are conducted across various task environments, such as web-based platforms and mobile operating systems like Android and iOS (Pan et al., 2024). In our work, we prompt VLM, GPT-4o, as an autonomous evaluator, using agents interacton process as inputs to assess whether the agent has successfully completed tasks at the trajectory level.",
                "summary": "<p><strong>–ê–≥–µ–Ω—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM)</strong></p>\n<p>–ê–≥–µ–Ω—Ç—ã, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM), –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç LLM –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–±-—Å–∞–π—Ç–∞–º–∏ –∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏. –≠—Ç–∏ –∞–≥–µ–Ω—Ç—ã —Å–ø–æ—Å–æ–±–Ω—ã –ø–æ–Ω–∏–º–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ –∏ –≤—ã–ø–æ–ª–Ω—è—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö, —Ç–∞–∫–∏—Ö –∫–∞–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–æ–º–º–µ—Ä—Ü–∏—è, –æ–Ω–ª–∞–π–Ω-–ø–æ–º–æ—â—å –∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –∑–Ω–∞–Ω–∏—è–º. –ü—Ä–∏–º–µ—Ä—ã —Ç–∞–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤–∫–ª—é—á–∞—é—Ç SeeAct –∏ WebVoyager, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–µ–º—è—Ç—Å—è –æ–±–æ–±—â–∏—Ç—å –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –≤–µ–±-—Å–∞–π—Ç–∞—Ö.</p>\n<p>–ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å–ª—É–∂–∞—Ç –ª–∏—à—å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º, –∏ –ª–∏—à—å –Ω–µ–º–Ω–æ–≥–∏–µ –∏–∑ –Ω–∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π –∞–≥–µ–Ω—Ç–æ–≤. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, —ç—Ç–∏ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å—é —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è—Ö –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–∏–Ω—Ç–µ–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π.</p>\n<p><strong>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤</strong></p>\n<p>–í –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Ä–∞—Å—Ç–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å –∫ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π \"–∑—Ä–µ–Ω–∏–µ-—è–∑—ã–∫\" (VLM) –∏ LLM. –≠—Ç–∏ –º–µ—Ç–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π: –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–±–æ—Ç—ã —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á–µ–Ω—ã –Ω–∞ —É—Å–ø–µ—Ö–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –¥—Ä—É–≥–∏–µ –æ—Ü–µ–Ω–∏–≤–∞—é—Ç —É—Å–ø–µ—Ö –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–≤–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö –∑–∞–¥–∞—á, —Ç–∞–∫–∏—Ö –∫–∞–∫ –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∏ –º–æ–±–∏–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ Android –∏ iOS. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ü–µ–Ω—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è VLM (GPT-4o), –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —É—Å–ø–µ—à–Ω–æ –ª–∏ –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω–∏–ª –∑–∞–¥–∞—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏.</p>"
            }
        ]
    }
]