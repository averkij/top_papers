[
    {
        "id": "https://huggingface.co/papers/2505.04620",
        "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
        "url": "https://huggingface.co/papers/2505.04620",
        "abstract": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/",
        "score": 2,
        "issue_id": 3661,
        "pub_date": "2025-05-07",
        "pub_date_card": {
            "ru": "7 Ğ¼Ğ°Ñ",
            "en": "May 7",
            "zh": "5æœˆ7æ—¥"
        },
        "hash": "57991e528141671e",
        "authors": [
            "Hao Fei",
            "Yuan Zhou",
            "Juncheng Li",
            "Xiangtai Li",
            "Qingshan Xu",
            "Bobo Li",
            "Shengqiong Wu",
            "Yaoting Wang",
            "Junbao Zhou",
            "Jiahao Meng",
            "Qingyu Shi",
            "Zhiyuan Zhou",
            "Liangtao Shi",
            "Minghe Gao",
            "Daoan Zhang",
            "Zhiqi Ge",
            "Weiming Wu",
            "Siliang Tang",
            "Kaihang Pan",
            "Yaobo Ye",
            "Haobo Yuan",
            "Tao Zhang",
            "Tianjie Ju",
            "Zixiang Meng",
            "Shilin Xu",
            "Liyu Jia",
            "Wentao Hu",
            "Meng Luo",
            "Jiebo Luo",
            "Tat-Seng Chua",
            "Shuicheng Yan",
            "Hanwang Zhang"
        ],
        "affiliations": [
            "HFUT",
            "KAUST",
            "NJU",
            "NTU",
            "NUS",
            "PKU",
            "SJTU",
            "UR",
            "WHU",
            "ZJU"
        ],
        "pdf_title_img": "assets/pdf/title_img/2505.04620.jpg",
        "data": {
            "categories": [
                "#benchmark",
                "#agi",
                "#multimodal"
            ],
            "emoji": "ğŸ¤–",
            "ru": {
                "title": "ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¿ÑƒÑ‚Ğ¸ Ğº AGI",
                "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ¸ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº General-Level Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ MLLM Ğ¿Ğ¾ 5-Ğ±Ğ°Ğ»Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑˆĞºĞ°Ğ»Ğµ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸ĞµĞ¹ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¡Ğ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ, Ğ¸Ğ·Ğ¼ĞµÑ€ÑÑÑ‰Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº General-Bench, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 700 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 325,800 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²."
            },
            "en": {
                "title": "Towards Multimodal Generalists: Evaluating MLLM Performance and Progress",
                "desc": "The paper discusses the evolution of Multimodal Large Language Models (MLLMs) towards a Multimodal Generalist paradigm, which allows these models to not only understand but also generate content across various modalities. It introduces a new evaluation framework called General-Level, which categorizes MLLM performance into five levels, helping to assess their capabilities and generality. The framework emphasizes the concept of Synergy, which evaluates how consistently models perform across different tasks and modalities. Additionally, the paper presents General-Bench, a comprehensive benchmark with over 700 tasks to facilitate the comparison of MLLMs and track their progress towards achieving artificial general intelligence (AGI)."
            },
            "zh": {
                "title": "è¿ˆå‘é€šç”¨äººå·¥æ™ºèƒ½çš„å¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶",
                "desc": "å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…ˆè¿›èƒ½åŠ›ã€‚ç°æœ‰çš„MLLMæ­£æœç€å¤šæ¨¡æ€é€šç”¨ä¸»ä¹‰è€…çš„æ–¹å‘æ¼”å˜ï¼Œä¸ä»…èƒ½å¤Ÿç†è§£å¤šç§æ¨¡æ€ï¼Œè¿˜èƒ½åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´ç”Ÿæˆå†…å®¹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°æ¡†æ¶â€”â€”General-Levelï¼Œå®šä¹‰äº†MLLMæ€§èƒ½å’Œé€šç”¨æ€§çš„äº”ä¸ªç­‰çº§ï¼Œä»¥æ¯”è¾ƒä¸åŒæ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡General-Benchï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ›´å¹¿æ³›çš„æŠ€èƒ½å’Œä»»åŠ¡è¯„ä¼°ï¼Œæ­ç¤ºäº†é€šç”¨æ¨¡å‹åœ¨å®ç°çœŸæ­£äººå·¥æ™ºèƒ½æ–¹é¢çš„æŒ‘æˆ˜ã€‚"
            }
        }
    },
    {
        "id": "https://huggingface.co/papers/2512.13281",
        "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?",
        "url": "https://huggingface.co/papers/2512.13281",
        "abstract": "The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.",
        "score": 2,
        "issue_id": 74,
        "pub_date": "2025-12-15",
        "pub_date_card": {
            "ru": "15 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
            "en": "December 15",
            "zh": "12æœˆ15æ—¥"
        },
        "hash": "2d4888772001806a",
        "authors": [
            "Jiaqi Wang",
            "Weijia Wu",
            "Yi Zhan",
            "Rui Zhao",
            "Ming Hu",
            "James Cheng",
            "Wei Liu",
            "Philip Torr",
            "Kevin Qinghong Lin"
        ],
        "affiliations": [
            "CUHK",
            "NUS",
            "University of Oxford",
            "Video Rebirth"
        ],
        "pdf_title_img": "assets/pdf/title_img/2512.13281.jpg",
        "data": {
            "categories": [
                "#audio",
                "#video",
                "#benchmark",
                "#multimodal"
            ],
            "emoji": "ğŸ¬",
            "ru": {
                "title": "Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ°: ĞºĞ°Ğº AI-Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¾Ğ±Ğ¼Ğ°Ğ½Ñ‹Ğ²Ğ°ÑÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚",
                "desc": "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Video Reality Test Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ ASMR ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚ÑŒ ĞºĞ°Ğº Ğ»ÑĞ´ĞµĞ¹, Ñ‚Ğ°Ğº Ğ¸ Vision Language Models (VLMs), Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ 56% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµĞ»Ğ¾Ğº Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² 81% Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸-VLMs Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… VLMs Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ‚Ğ¾Ğ½ĞºĞ¸Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¸ Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°."
            },
            "en": {
                "title": "Testing the Limits of AI-Generated Video Realism",
                "desc": "The Video Reality Test benchmark assesses how realistic AI-generated ASMR videos are and how well they can be detected by both humans and Vision-Language Models (VLMs). It highlights that even advanced video generation models can trick these detection systems, revealing weaknesses in their ability to judge perceptual fidelity and audio-visual coherence. The benchmark focuses on immersive ASMR videos, emphasizing the importance of audio paired with video for accurate detection. Findings show that while audio can enhance discrimination between real and fake videos, certain misleading cues still pose challenges for detection models."
            },
            "zh": {
                "title": "è§†é¢‘ç”Ÿæˆçš„çœŸå®æ„ŸæŒ‘æˆ˜",
                "desc": "æœ¬è®ºæ–‡ä»‹ç»äº†è§†é¢‘ç°å®æµ‹è¯•åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°AIç”Ÿæˆçš„ASMRè§†é¢‘çš„çœŸå®æ„Ÿå’Œæ£€æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ä¹Ÿèƒ½æ¬ºéª—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œäººç±»ï¼Œæ˜¾ç¤ºå‡ºåœ¨æ„ŸçŸ¥çœŸå®æ„Ÿå’ŒéŸ³è§†é¢‘ä¸€è‡´æ€§æ–¹é¢çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„çœŸå®ASMRè§†é¢‘ï¼Œæµ‹è¯•éŸ³è§†é¢‘ç´§å¯†ç»“åˆä¸‹çš„æ„ŸçŸ¥çœŸå®æ„Ÿã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæœ€ä½³ç”Ÿæˆæ¨¡å‹Veo3.1-Fastèƒ½å¤Ÿæ¬ºéª—å¤§å¤šæ•°VLMsï¼Œä¸”å…¶æ£€æµ‹å‡†ç¡®ç‡è¿œä½äºäººç±»ä¸“å®¶ã€‚"
            }
        }
    }
]