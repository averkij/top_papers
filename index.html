
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 17 papers. September 11.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">11 сентября</span> | <span id="title-articles-count">17 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-10.html">⬅️ <span id="prev-date">10.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-12.html">➡️ <span id="next-date">12.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '11 сентября', 'en': 'September 11', 'zh': '9月11日'};
        let feedDateNext = {'ru': '12.09', 'en': '09/12', 'zh': '9月12日'};
        let feedDatePrev = {'ru': '10.09', 'en': '09/10', 'zh': '9月10日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.07980', 'title': 'Parallel-R1: Towards Parallel Thinking via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.07980', 'abstract': "Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.", 'score': 66, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '41def489cc53d3e0', 'authors': ['Tong Zheng', 'Hongming Zhang', 'Wenhao Yu', 'Xiaoyang Wang', 'Xinyu Yang', 'Runpeng Dai', 'Rui Liu', 'Huiwen Bao', 'Chengsong Huang', 'Heng Huang', 'Dong Yu'], 'affiliations': ['Carnegie Mellon University', 'City University of Hong Kong', 'Tencent AI Lab Seattle', 'University of Maryland, College Park', 'University of North Carolina at Chapel Hill', 'Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2509.07980.jpg', 'data': {'categories': ['#math', '#open_source', '#rl', '#optimization', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Параллельное мышление для ИИ: новый уровень рассуждений', 'desc': 'Parallel-R1 - это фреймворк обучения с подкреплением, который улучшает способности больших языковых моделей к рассуждениям путем параллельного мышления. Он использует прогрессивную учебную программу, начиная с обучения на более простых задачах и переходя к более сложным. Эксперименты показали значительное улучшение точности на математических бенчмарках по сравнению с последовательным мышлением. Анализ выявил, что модель использует параллельное мышление сначала как стратегию исследования, а затем для многоаспектной проверки.'}, 'en': {'title': 'Unlocking Reasoning Power with Parallel Thinking', 'desc': 'The paper introduces Parallel-R1, a reinforcement learning framework designed to improve the reasoning abilities of large language models (LLMs) by facilitating parallel thinking. This approach allows the model to explore multiple reasoning paths simultaneously, which enhances its performance on complex tasks. Unlike traditional methods that rely on supervised fine-tuning, Parallel-R1 employs a progressive curriculum that first trains the model on simpler tasks before transitioning to more challenging ones using reinforcement learning. The results demonstrate significant accuracy improvements on various math benchmarks, showcasing the effectiveness of parallel thinking in enhancing model performance.'}, 'zh': {'title': '并行思维：提升推理能力的新方法', 'desc': 'Parallel-R1是一个强化学习框架，旨在通过并行思维来增强大型语言模型的推理能力。该框架采用渐进式课程，解决了在训练并行思维时的冷启动问题。通过在简单任务上进行监督微调，模型学习并行思维能力，然后转向强化学习以应对更复杂的问题。实验结果表明，Parallel-R1在数学基准测试中显著提高了模型的准确性，展示了并行思维在推理任务中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2509.07979', 'title': 'Visual Representation Alignment for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2509.07979', 'abstract': "VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.", 'score': 53, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': 'f229168a41923a52', 'authors': ['Heeji Yoon', 'Jaewoo Jung', 'Junwan Kim', 'Hyungyu Choi', 'Heeseong Shin', 'Sangbeom Lim', 'Honggyu An', 'Chaehyun Kim', 'Jisang Han', 'Donghyun Kim', 'Chanho Eom', 'Sunghwan Hong', 'Seungryong Kim'], 'affiliations': ['Chung-Ang University', 'ETH Zurich', 'KAIST AI', 'Korea University', 'NYU'], 'pdf_title_img': 'assets/pdf/title_img/2509.07979.jpg', 'data': {'categories': ['#benchmark', '#cv', '#alignment', '#training', '#multimodal', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление визуального понимания языковых моделей через выравнивание представлений', 'desc': 'Статья представляет VIRAL - стратегию регуляризации для мультимодальных больших языковых моделей (MLLM). VIRAL выравнивает внутренние визуальные представления MLLM с предобученными визуальными фундаментальными моделями (VFM). Это позволяет MLLM сохранять важные визуальные детали и дополнять их знаниями из VFM. Эксперименты показывают улучшение результатов на различных мультимодальных задачах.'}, 'en': {'title': 'Aligning Visual Representations for Better Multimodal Learning', 'desc': "This paper introduces VIRAL, a regularization technique that improves the performance of multimodal large language models (MLLMs) on vision-related tasks. The authors identify that MLLMs struggle with tasks like object counting due to a lack of direct visual supervision, which leads to the loss of important visual details. VIRAL addresses this issue by aligning the visual representations of MLLMs with those from pre-trained vision foundation models (VFMs), ensuring that the models retain critical visual information. The results show that this alignment significantly enhances the models' reasoning capabilities on complex visual inputs across various benchmarks."}, 'zh': {'title': 'VIRAL：提升视觉任务表现的对齐策略', 'desc': '本文提出了一种名为VIRAL的正则化策略，旨在将多模态大语言模型（MLLMs）的视觉表示与预训练的视觉基础模型（VFMs）对齐，从而提升在视觉任务上的表现。我们发现，传统的文本监督方法对视觉路径的指导有限，导致模型在训练过程中忽视了细致的视觉信息。通过强制对齐内部视觉表示，VIRAL不仅帮助模型保留输入视觉编码器中的重要细节，还能补充来自VFMs的额外视觉知识。实验结果表明，VIRAL在多项广泛采用的多模态基准测试中均取得了一致的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2509.07969', 'title': 'Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search', 'url': 'https://huggingface.co/papers/2509.07969', 'abstract': 'Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.', 'score': 45, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '4d29daad3dc9ac61', 'authors': ['Xin Lai', 'Junyi Li', 'Wei Li', 'Tao Liu', 'Tianjian Li', 'Hengshuang Zhao'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.07969.jpg', 'data': {'categories': ['#data', '#open_source', '#cv', '#rl', '#training', '#multimodal', '#dataset', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Глубокое мышление в визуальном поиске: Mini-o3 раздвигает границы рассуждений', 'desc': 'Система Mini-o3 представляет собой подход к глубокому многоступенчатому рассуждению в задачах визуального поиска. Она использует итеративный конвейер сбора данных и стратегию маскирования избыточных шагов для достижения наилучших результатов с богатыми паттернами рассуждений. Несмотря на обучение с ограничением в шесть шагов взаимодействия, модель генерирует траектории, естественно масштабируемые до десятков шагов при выводе. Эксперименты показывают, что Mini-o3 эффективно решает сложные задачи визуального поиска, демонстрируя глубокие пути рассуждений.'}, 'en': {'title': 'Unlocking Deep Reasoning in Visual Search with Mini-o3', 'desc': 'Mini-o3 is a novel system designed for deep reasoning in visual search tasks, enabling multi-turn interactions that enhance problem-solving capabilities. It utilizes an iterative data collection pipeline and an over-turn masking strategy to improve performance and reasoning diversity. By constructing the Visual Probe Dataset, Mini-o3 addresses the limitations of existing models that struggle with complex tasks requiring extensive exploration. The system achieves state-of-the-art results by allowing for a greater number of reasoning steps during inference, leading to more accurate and nuanced solutions.'}, 'zh': {'title': 'Mini-o3：深度多轮推理的视觉搜索新突破', 'desc': 'Mini-o3是一个用于视觉搜索任务的深度多轮推理系统，采用迭代数据收集管道和超轮掩蔽策略，达到了最先进的性能。该系统通过构建视觉探测数据集，设计了数千个具有挑战性的视觉搜索问题，以支持探索性推理。Mini-o3能够执行深度的多轮推理，处理数十个步骤，克服了现有方法在交互轮次和推理模式上的局限性。实验结果表明，Mini-o3能够生成丰富的推理模式和深度思考路径，有效解决复杂的视觉搜索问题。'}}}, {'id': 'https://huggingface.co/papers/2509.07295', 'title': 'Reconstruction Alignment Improves Unified Multimodal Models', 'url': 'https://huggingface.co/papers/2509.07295', 'abstract': 'Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense "text prompts," providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs', 'score': 31, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'b638c4100f242a73', 'authors': ['Ji Xie', 'Trevor Darrell', 'Luke Zettlemoyer', 'XuDong Wang'], 'affiliations': ['UC Berkeley', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2509.07295.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#open_source', '#multimodal', '#optimization', '#diffusion', '#training'], 'emoji': '🔄', 'ru': {'title': 'RecA: Эффективное выравнивание мультимодальных моделей для улучшения генерации изображений', 'desc': 'Reconstruction Alignment (RecA) - это метод пост-обучения, который улучшает мультимодальные модели, используя визуальные эмбеддинги в качестве плотных промптов. RecA оптимизирует единую мультимодальную модель (UMM) для реконструкции входного изображения с помощью самоконтролируемой функции потерь, тем самым переориентируя понимание и генерацию. Этот метод применим к авторегрессионным моделям, моделям с маскированной авторегрессией и диффузионным UMM, последовательно улучшая точность генерации и редактирования изображений. Несмотря на простоту, RecA значительно повышает производительность генерации изображений на различных бенчмарках, превосходя более крупные модели с открытым исходным кодом.'}, 'en': {'title': 'Enhancing Multimodal Models with Reconstruction Alignment', 'desc': "Reconstruction Alignment (RecA) is a novel post-training technique designed to enhance unified multimodal models (UMMs) by utilizing visual embeddings as dense prompts. This method addresses the limitations of traditional training, which often relies on sparse image-text pairs that fail to capture detailed visual information. By conditioning UMMs on their own visual understanding embeddings and optimizing for self-supervised reconstruction, RecA effectively realigns the model's understanding and generation capabilities. The results demonstrate significant improvements in image generation and editing fidelity across various UMM architectures, making RecA a resource-efficient and broadly applicable strategy."}, 'zh': {'title': '重建对齐：提升多模态模型的图像生成与编辑精度', 'desc': '重建对齐（RecA）是一种后训练方法，通过使用视觉嵌入作为密集提示，增强多模态模型的图像生成和编辑精度。传统的训练方法依赖于图像-文本对，但这些文本通常缺乏细致的视觉细节。RecA利用视觉理解编码器的嵌入作为丰富的“文本提示”，在没有文本描述的情况下提供监督。该方法在多种多模态模型中表现出色，显著提高了图像生成和编辑的性能。'}}}, {'id': 'https://huggingface.co/papers/2509.06818', 'title': 'UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward', 'url': 'https://huggingface.co/papers/2509.06818', 'abstract': 'UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With "multi-to-multi matching" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO', 'score': 23, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '42f9165098d4a2b8', 'authors': ['Yufeng Cheng', 'Wenxu Wu', 'Shaojin Wu', 'Mengqi Huang', 'Fei Ding', 'Qian He'], 'affiliations': ['UXO Team, Intelligent Creation Lab, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2509.06818.jpg', 'data': {'categories': ['#dataset', '#rl', '#open_source', '#optimization', '#diffusion', '#training'], 'emoji': '🎭', 'ru': {'title': 'UMO: Улучшение идентичности в кастомизации изображений', 'desc': "UMO - это новая система оптимизации для улучшения согласованности идентичности и уменьшения путаницы в кастомизации изображений с несколькими референсами. Она использует обучение с подкреплением на диффузионных моделях для решения проблемы глобального назначения. UMO применяет парадигму 'многие-ко-многим' для сопоставления идентичностей и работает с различными методами кастомизации изображений. Исследователи также создали масштабируемый набор данных и новую метрику для оценки путаницы идентичностей."}, 'en': {'title': 'UMO: Enhancing Identity Consistency in Image Customization', 'desc': 'The paper introduces UMO, a framework that improves how images are customized while keeping identities consistent across multiple references. It uses reinforcement learning on diffusion models to tackle the challenge of identity confusion, which is crucial since humans are particularly sensitive to faces. UMO reformulates the problem of generating multiple identities as a global assignment optimization task, enhancing the scalability of identity preservation. The authors also create a new dataset and metric to support their framework and demonstrate that UMO significantly outperforms existing methods in maintaining identity consistency.'}, 'zh': {'title': '统一多身份优化，提升图像定制一致性', 'desc': 'UMO（统一多身份优化框架）通过在扩散模型上应用强化学习，增强了多参考图像定制中的身份一致性，减少了身份混淆。该框架解决了在多参考图像中保持一致身份的挑战，提升了定制模型的身份可扩展性。UMO将多身份生成重新定义为全局分配优化问题，利用“多对多匹配”范式实现身份一致性。通过构建一个包含合成和真实部分的可扩展定制数据集，UMO在多个图像定制方法上显著提高了身份一致性，并减少了身份混淆。'}}}, {'id': 'https://huggingface.co/papers/2509.07414', 'title': 'Language Self-Play For Data-Free Training', 'url': 'https://huggingface.co/papers/2509.07414', 'abstract': "Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines.", 'score': 19, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '438447513c4c481f', 'authors': ['Jakub Grudzien Kuba', 'Mengting Gu', 'Qi Ma', 'Yuandong Tian', 'Vijai Mohan'], 'affiliations': ['Meta Superintelligence Labs', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.07414.jpg', 'data': {'categories': ['#games', '#rl', '#optimization', '#training', '#rlhf'], 'emoji': '🎮', 'ru': {'title': 'Самоигра языковых моделей: путь к улучшению без новых данных', 'desc': 'Статья представляет новый метод улучшения больших языковых моделей (LLM) под названием Language Self-Play (LSP). LSP использует принципы теории игр и самообучения, позволяя модели соревноваться с самой собой для улучшения своих навыков. Эксперименты показали, что LSP превосходит традиционные методы, основанные на дополнительных данных. Этот подход может помочь преодолеть ограничения, связанные с необходимостью постоянного увеличения объема обучающих данных для LLM.'}, 'en': {'title': 'Empowering Models Through Self-Play: No Data Needed!', 'desc': "Language Self-Play (LSP) is a novel approach that improves large language models' (LLMs) ability to follow instructions without needing more training data. By using a game-theoretic framework, LSP allows models to play against themselves, which helps them develop stronger skills over time. This self-play method leads to better performance on instruction-following tasks compared to traditional data-driven methods. Experiments show that pretrained models can significantly enhance their capabilities through this self-play mechanism."}, 'zh': {'title': '语言自我对弈：无数据提升模型性能的创新方法', 'desc': '语言自我对弈（LSP）是一种增强大型语言模型在遵循指令任务上表现的方法。通过自我对弈，模型能够在没有额外数据的情况下提升自身能力。该方法利用博弈论框架，将模型的表现视为在竞争游戏中的表现，从而促使更强的策略产生。实验结果表明，预训练模型通过自我对弈可以有效提高在挑战性任务上的表现，超越了基于数据的方法。'}}}, {'id': 'https://huggingface.co/papers/2509.06951', 'title': 'F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions', 'url': 'https://huggingface.co/papers/2509.06951', 'abstract': 'F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability.', 'score': 19, 'issue_id': 5807, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'c287d68dc6b0f03d', 'authors': ['Qi Lv', 'Weijie Kong', 'Hao Li', 'Jia Zeng', 'Zherui Qiu', 'Delin Qu', 'Haoming Song', 'Qizhi Chen', 'Xiang Deng', 'Jiangmiao Pang'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2509.06951.jpg', 'data': {'categories': ['#agi', '#cv', '#reasoning', '#benchmark', '#agents', '#transfer_learning', '#training'], 'emoji': '🔮', 'ru': {'title': 'Визуальное предвидение для улучшения принятия решений ИИ', 'desc': 'F1 - это предварительно обученная система для выполнения задач в динамических визуальных средах. Она использует архитектуру Mixture-of-Transformer с модулями для восприятия, генерации предвидения и контроля. F1 применяет механизм предсказания следующего масштаба для синтеза визуального предвидения, обусловленного целью. Система обучается по трехэтапной схеме на обширном наборе данных, что улучшает модульное рассуждение и переносимое визуальное предвидение.'}, 'en': {'title': 'F1: Enhancing Decision-Making with Visual Foresight in Dynamic Environments', 'desc': 'The paper presents F1, a pretrained Vision-Language-Action (VLA) framework designed to enhance performance in dynamic environments. Unlike traditional models that react to immediate states, F1 incorporates visual foresight generation to improve decision-making and robustness. It utilizes a Mixture-of-Transformer architecture that integrates perception, foresight, and control, allowing for better planning and action generation. Through a comprehensive training process on a large dataset, F1 demonstrates superior task success and generalization compared to existing methods.'}, 'zh': {'title': 'F1：动态环境中的前瞻性决策框架', 'desc': 'F1是一个预训练的视觉-语言-行动（VLA）框架，旨在通过视觉前瞻生成来提高在动态环境中的任务成功率和泛化能力。它采用混合变换器架构，结合感知、前瞻生成和控制模块，增强了理解、生成和行动之间的联系。F1的核心是下一尺度预测机制，通过预测未来的视觉状态，将行动生成重新定义为一个以前瞻为指导的逆动力学问题。经过在超过33万条轨迹和136个多样化任务上的三阶段训练，F1在真实世界任务和模拟基准测试中表现优异，显著提高了任务成功率和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2509.06923', 'title': 'Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding', 'url': 'https://huggingface.co/papers/2509.06923', 'abstract': "SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable success in enhancing the reasoning capabilities of large language models (LLMs). However, existing RLVR methods often suffer from exploration inefficiency due to mismatches between the training data's difficulty and the model's capability. LLMs fail to discover viable reasoning paths when problems are overly difficult, while learning little new capability when problems are too simple. In this work, we formalize the impact of problem difficulty by quantifying the relationship between loss descent speed and rollout accuracy. Building on this analysis, we propose SEELE, a novel supervision-aided RLVR framework that dynamically adjusts problem difficulty to stay within the high-efficiency region. SEELE augments each training sample by appending a hint (part of a full solution) after the original problem. Unlike previous hint-based approaches, SEELE deliberately and adaptively adjusts the hint length for each problem to achieve an optimal difficulty. To determine the optimal hint length, SEELE employs a multi-round rollout sampling strategy. In each round, it fits an item response theory model to the accuracy-hint pairs collected in preceding rounds to predict the required hint length for the next round. This instance-level, real-time difficulty adjustment aligns problem difficulty with the evolving model capability, thereby improving exploration efficiency. Experimental results show that SEELE outperforms Group Relative Policy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5 points, respectively, and surpasses the best previous supervision-aided approach by +3.6 points on average across six math reasoning benchmarks.", 'score': 17, 'issue_id': 5806, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'da5bcd38585e0d46', 'authors': ['Ziheng Li', 'Zexu Sun', 'Jinman Zhao', 'Erxue Min', 'Yongcheng Zeng', 'Hui Wu', 'Hengyi Cai', 'Shuaiqiang Wang', 'Dawei Yin', 'Xu Chen', 'Zhi-Hong Deng'], 'affiliations': ['Aerospace Information Research Institute, Chinese Academy of Sciences', 'Baidu Inc.', 'Department of Computer Science, University of Toronto', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Institute of Automation, Chinese Academy of Sciences', 'School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06923.jpg', 'data': {'categories': ['#math', '#rl', '#optimization', '#training', '#rlhf', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Динамическая настройка сложности для эффективного обучения ИИ математическому мышлению', 'desc': 'SEELE - это новая система обучения с подкреплением с проверяемыми наградами (RLVR), которая динамически регулирует сложность задач для улучшения исследовательской эффективности в задачах математического рассуждения. Система использует адаптивную длину подсказок, чтобы поддерживать оптимальный уровень сложности для обучающейся языковой модели. SEELE применяет многораундовую стратегию сэмплирования и теорию ответов на вопросы для определения оптимальной длины подсказки. Эксперименты показывают, что SEELE превосходит существующие методы RLVR и обучения с учителем на нескольких эталонных тестах по математическому рассуждению.'}, 'en': {'title': 'Dynamic Difficulty for Enhanced Learning Efficiency', 'desc': "SEELE is a new framework in reinforcement learning with verifiable rewards (RLVR) that improves how models learn to solve math problems by adjusting the difficulty of tasks dynamically. It does this by adding hints to problems, where the length of the hint is tailored to match the model's current abilities, ensuring that the challenges are neither too hard nor too easy. This adaptive hinting strategy helps the model explore more effectively and learn better by staying within an optimal difficulty range. Experiments show that SEELE significantly outperforms existing methods, leading to better performance in math reasoning tasks."}, 'zh': {'title': 'SEELE：动态调整难度，提升推理效率', 'desc': 'SEELE是一种新颖的强化学习可验证奖励（RLVR）框架，旨在通过动态调整问题难度来提高数学推理任务中的探索效率。该框架通过在原始问题后附加提示（部分完整解决方案）来增强每个训练样本。SEELE与以往的提示方法不同，它根据每个问题的特点，灵活地调整提示长度，以实现最佳难度。实验结果表明，SEELE在六个数学推理基准测试中，表现优于其他方法，显著提高了模型的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2509.06830', 'title': 'Curia: A Multi-Modal Foundation Model for Radiology', 'url': 'https://huggingface.co/papers/2509.06830', 'abstract': "Curia, a foundation model trained on extensive cross-sectional imaging data, demonstrates superior performance across multiple radiological tasks and shows emergent properties in cross-modality and low-data settings.  \t\t\t\t\tAI-generated summary \t\t\t\t AI-assisted radiological interpretation is based on predominantly narrow, single-task models. This approach is impractical for covering the vast spectrum of imaging modalities, diseases, and radiological findings. Foundation models (FMs) hold the promise of broad generalization across modalities and in low-data settings. However, this potential has remained largely unrealized in radiology. We introduce Curia, a foundation model trained on the entire cross-sectional imaging output of a major hospital over several years, which to our knowledge is the largest such corpus of real-world data-encompassing 150,000 exams (130 TB). On a newly curated 19-task external validation benchmark, Curia accurately identifies organs, detects conditions like brain hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging. Curia meets or surpasses the performance of radiologists and recent foundation models, and exhibits clinically significant emergent properties in cross-modality, and low-data regimes. To accelerate progress, we release our base model's weights at https://huggingface.co/raidium/curia.", 'score': 17, 'issue_id': 5812, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '5a09abd9043552bb', 'authors': ['Corentin Dancette', 'Julien Khlaut', 'Antoine Saporta', 'Helene Philippe', 'Elodie Ferreres', 'Baptiste Callard', 'Théo Danielou', 'Léo Alberge', 'Léo Machado', 'Daniel Tordjman', 'Julie Dupuis', 'Korentin Le Floch', 'Jean Du Terrail', 'Mariam Moshiri', 'Laurent Dercle', 'Tom Boeken', 'Jules Gregory', 'Maxime Ronot', 'François Legou', 'Pascal Roux', 'Marc Sapoval', 'Pierre Manceron', 'Paul Hérent'], 'affiliations': ['.omics, Paris, France', 'Centre Cardiologique du Nord, Saint-Denis, 93200, France', 'Department of Radiology and Radiological Science, Medical University of South Carolina, Charleston, SC, USA', 'Department of Radiology, Columbia University Irving Medical Center, New York, NY, 10032, USA', 'Department of Radiology, FHU MOSAIC, Beaujon Hospital, APHP.Nord, Clichy, France', 'Department of Vascular and Oncological Interventional Radiology, Hˆopital Europeen Georges Pompidou, AP-HP, Paris, France', 'Faculte de Sante, Universite Paris-Cite, Paris, France', 'HEKA, INRIA, Paris, France', 'PARCC 970, INSERM, Paris, France', 'Raidium, 27 rue du faubourg Saint-Jacques, Paris, 75014, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.06830.jpg', 'data': {'categories': ['#dataset', '#healthcare', '#benchmark', '#data', '#low_resource', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Curia: универсальная модель для радиологической интерпретации', 'desc': 'Модель Curia - это фундаментальная модель машинного обучения, обученная на обширном наборе данных медицинской визуализации. Она демонстрирует превосходную производительность в различных радиологических задачах, включая идентификацию органов, обнаружение заболеваний и прогнозирование исходов при стадировании опухолей. Curia показывает эмерджентные свойства в кросс-модальных и низкоресурсных сценариях. Модель обучена на 150 000 исследований (130 ТБ данных) и превосходит как радиологов, так и другие современные фундаментальные модели по точности.'}, 'en': {'title': 'Curia: Revolutionizing Radiology with a Foundation Model', 'desc': 'Curia is a foundation model designed for radiology, trained on a vast dataset of cross-sectional imaging from a major hospital. It excels in various radiological tasks, outperforming traditional narrow models by demonstrating strong generalization across different imaging modalities and in scenarios with limited data. The model has been validated on a comprehensive benchmark, showing its ability to accurately identify organs and detect critical conditions. By releasing its weights, Curia aims to foster further advancements in AI-assisted radiological interpretation.'}, 'zh': {'title': 'Curia：放射学的基础模型新突破', 'desc': 'Curia是一个基础模型，经过大量横断面影像数据的训练，能够在多个放射学任务中表现出色。它在跨模态和低数据环境下展现出新兴特性，超越了传统的单任务模型。Curia使用了来自一家大型医院的150,000个检查数据，成为现实世界数据中最大的训练集之一。通过在19个任务的外部验证基准上测试，Curia的表现与放射科医生相当，甚至更优，显示出其广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2509.03646', 'title': 'Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.03646', 'abstract': 'Reinforcement Learning enhances LLM reasoning through a two-phase process involving procedural correctness and strategic planning, with HICRA algorithm focusing on high-impact planning tokens to improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning (RL) has proven highly effective at enhancing the complex reasoning abilities of Large Language Models (LLMs), yet underlying mechanisms driving this success remain largely opaque. Our analysis reveals that puzzling phenomena like ``aha moments", ``length-scaling\'\' and entropy dynamics are not disparate occurrences but hallmarks of an emergent reasoning hierarchy, akin to the separation of high-level strategic planning from low-level procedural execution in human cognition. We uncover a compelling two-phase dynamic: initially, a model is constrained by procedural correctness and must improve its low-level skills. The learning bottleneck then decisively shifts, with performance gains being driven by the exploration and mastery of high-level strategic planning. This insight exposes a core inefficiency in prevailing RL algorithms like GRPO, which apply optimization pressure agnostically and dilute the learning signal across all tokens. To address this, we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that concentrates optimization efforts on high-impact planning tokens. HICRA significantly outperforms strong baselines, demonstrating that focusing on this strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we validate semantic entropy as a superior compass for measuring strategic exploration over misleading metrics such as token-level entropy.', 'score': 14, 'issue_id': 5819, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 сентября', 'en': 'September 3', 'zh': '9月3日'}, 'hash': '298f6c3e803a9fce', 'authors': ['Haozhe Wang', 'Qixin Xu', 'Che Liu', 'Junhong Wu', 'Fangzhen Lin', 'Wenhu Chen'], 'affiliations': ['Hong Kong University of Science and Technology', 'Imperial College London', 'M-A-P, Tsinghua University', 'UCAS', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.03646.jpg', 'data': {'categories': ['#rlhf', '#training', '#optimization', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Иерархическое обучение с подкреплением для улучшения рассуждений ИИ', 'desc': 'Статья описывает двухфазный процесс улучшения рассуждений больших языковых моделей с помощью обучения с подкреплением. Первая фаза фокусируется на процедурной корректности, а вторая - на стратегическом планировании. Авторы предлагают алгоритм HICRA, который концентрирует оптимизацию на токенах высокого уровня планирования. Исследование показывает, что такой подход значительно превосходит базовые методы в задачах сложных рассуждений.'}, 'en': {'title': 'Unlocking Advanced Reasoning in LLMs with HICRA', 'desc': 'This paper discusses how Reinforcement Learning (RL) can improve the reasoning capabilities of Large Language Models (LLMs) through a two-phase learning process. Initially, the model focuses on procedural correctness, enhancing its low-level skills before shifting to high-level strategic planning. The authors introduce the HICRA algorithm, which optimizes learning by concentrating on high-impact planning tokens, thus addressing inefficiencies in traditional RL methods. Their findings suggest that measuring semantic entropy is more effective for guiding strategic exploration than conventional metrics like token-level entropy.'}, 'zh': {'title': '聚焦高影响规划，提升推理能力', 'desc': '强化学习（RL）在提升大型语言模型（LLM）的复杂推理能力方面表现出色，但其成功的机制仍不清晰。我们的分析表明，诸如“恍然大悟时刻”、“长度缩放”和熵动态等现象并不是孤立的，而是新兴推理层次的标志，类似于人类认知中高层战略规划与低层程序执行的分离。我们发现了一种引人注目的两阶段动态：最初，模型受到程序正确性的限制，必须提高其低层技能。然后，学习瓶颈转移，性能提升主要依赖于高层战略规划的探索和掌握。'}}}, {'id': 'https://huggingface.co/papers/2509.07301', 'title': 'Causal Attention with Lookahead Keys', 'url': 'https://huggingface.co/papers/2509.07301', 'abstract': "CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.  \t\t\t\t\tAI-generated summary \t\t\t\t In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.", 'score': 10, 'issue_id': 5807, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '2a5370a17853db77', 'authors': ['Zhuoqing Song', 'Peng Sun', 'Huizhuo Yuan', 'Quanquan Gu'], 'affiliations': ['ByteDance Seed', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07301.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#architecture', '#optimization', '#training'], 'emoji': '🏰', 'ru': {'title': 'CASTLE: Взгляд в будущее для улучшения языкового моделирования', 'desc': 'Статья представляет новый механизм внимания под названием CASTLE для языкового моделирования. В отличие от стандартного причинного внимания, CASTLE обновляет ключи токенов с учетом будущего контекста, сохраняя при этом авторегрессивные свойства. Авторы вывели математическую эквивалентность, позволяющую эффективно обучать модель параллельно. CASTLE превосходит стандартное причинное внимание в задачах языкового моделирования на различных масштабах моделей.'}, 'en': {'title': 'CASTLE: Future Context for Smarter Attention in Language Models', 'desc': 'CASTLE is a novel attention mechanism designed for language modeling that enhances the standard causal attention by updating keys with future context. Unlike traditional methods where keys are static and only consider past tokens, CASTLE introduces lookahead keys that incorporate information from future tokens while maintaining the autoregressive nature of the model. This allows for a more dynamic representation of context, leading to improved performance on various language tasks. The mechanism is efficient, enabling parallel training without the need to explicitly compute lookahead keys at each position, resulting in lower validation perplexity and better overall results.'}, 'zh': {'title': 'CASTLE：未来上下文的自回归注意力机制', 'desc': 'CASTLE是一种注意力机制，它在保持自回归特性的同时，使用未来上下文更新键值。与标准的因果注意力不同，CASTLE的每个令牌的键会随着上下文的发展而不断更新。我们称这些更新后的键为前瞻键，因为它们来自于较早的位置，但整合了相对这些位置后面出现的令牌的信息。实验结果表明，CASTLE在语言建模基准测试中表现优于标准因果注意力，降低了验证困惑度，并在多种下游任务中提升了性能。'}}}, {'id': 'https://huggingface.co/papers/2509.07968', 'title': 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge', 'url': 'https://huggingface.co/papers/2509.07968', 'abstract': "SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.", 'score': 7, 'issue_id': 5806, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': '7fb6599f8657bb35', 'authors': ['Lukas Haas', 'Gal Yona', "Giovanni D'Antonio", 'Sasha Goldshtein', 'Dipanjan Das'], 'affiliations': ['Google DeepMind, Google Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.07968.jpg', 'data': {'categories': ['#dataset', '#hallucinations', '#interpretability', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Точный бенчмарк для оценки фактической достоверности языковых моделей', 'desc': 'SimpleQA Verified - это усовершенствованный бенчмарк для оценки фактической точности больших языковых моделей (LLM). Он устраняет недостатки предыдущих бенчмарков, включая шумные и неправильные метки, тематические смещения и избыточность вопросов. Бенчмарк был создан с помощью многоступенчатого процесса фильтрации, включающего дедупликацию, балансировку тем и сверку источников. На этом новом бенчмарке модель Gemini 2.5 Pro достигла наилучшего F1-показателя в 55.6, превзойдя другие передовые модели.'}, 'en': {'title': 'Elevating Factuality Evaluation for Language Models', 'desc': 'SimpleQA Verified is a new benchmark designed to assess the factual accuracy of Large Language Models (LLMs) more effectively. It improves upon previous benchmarks by eliminating issues like incorrect labels and biases, ensuring a more reliable evaluation process. The benchmark consists of 1,000 carefully curated prompts that have undergone rigorous filtering to enhance their quality and challenge. With this tool, researchers can better measure the factual performance of models like Gemini 2.5 Pro, which has achieved a leading F1-score, thus helping to reduce the occurrence of hallucinations in AI-generated content.'}, 'zh': {'title': '提升大型语言模型事实性评估的基准工具', 'desc': 'SimpleQA Verified 是一个用于评估大型语言模型（LLM）短文本事实性的基准，包含1000个提示。它解决了OpenAI基准中的一些关键问题，如标签噪声、主题偏见和问题冗余。通过严格的多阶段过滤过程，SimpleQA Verified 提供了一个更可靠和具有挑战性的评估集，并改进了自动评分提示。该基准使研究社区能够更准确地跟踪参数模型的事实性进展，并减少幻觉现象。'}}}, {'id': 'https://huggingface.co/papers/2509.06942', 'title': 'Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference', 'url': 'https://huggingface.co/papers/2509.06942', 'abstract': "Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.", 'score': 6, 'issue_id': 5815, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '666a0df353939fc4', 'authors': ['Xiangwei Shen', 'Zhimin Li', 'Zhantao Yang', 'Shiyi Zhang', 'Yingfang Zhang', 'Donghao Li', 'Chunyu Wang', 'Qinglin Lu', 'Yansong Tang'], 'affiliations': ['Hunyuan, Tencent', 'School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen', 'Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06942.jpg', 'data': {'categories': ['#diffusion', '#alignment', '#rlhf', '#training', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Эффективное улучшение диффузионных моделей с учетом человеческих предпочтений', 'desc': 'В статье представлены два метода улучшения диффузионных моделей: Direct-Align и Semantic Relative Preference Optimization (SRPO). Direct-Align использует предопределенный шумовой приор для эффективного восстановления изображений, избегая чрезмерной оптимизации на поздних этапах. SRPO формулирует награды как текстово-обусловленные сигналы, позволяя корректировать их онлайн в ответ на аугментацию промптов. Эти методы снижают вычислительные затраты и минимизируют необходимость офлайн-адаптации модели вознаграждений. Применение этих подходов к модели FLUX.1.dev улучшило реалистичность и эстетическое качество генерируемых изображений более чем в 3 раза по оценкам людей.'}, 'en': {'title': 'Enhancing Diffusion Models with Direct-Align and SRPO', 'desc': "This paper presents two innovative methods, Direct-Align and Semantic Relative Preference Optimization (SRPO), to enhance diffusion models' alignment with human preferences. Direct-Align simplifies the process of recovering original images by using a predefined noise prior, which reduces the need for costly multistep denoising. SRPO allows for real-time adjustments of rewards based on text prompts, minimizing the need for extensive offline reward model adaptations. Together, these methods significantly improve the aesthetic quality and realism of generated images while lowering computational costs."}, 'zh': {'title': '提升扩散模型与人类偏好的对齐', 'desc': '本文提出了Direct-Align和语义相对偏好优化（SRPO）两种方法，以提高扩散模型与人类偏好的对齐度，同时降低计算成本。Direct-Align通过预定义噪声来有效恢复原始图像，避免了在后期时间步的过度优化。SRPO则将奖励信号与文本条件相结合，实现了在线调整奖励，从而减少了对离线奖励微调的依赖。通过优化去噪和在线奖励调整，我们显著提高了FLUX.1.dev模型在真实感和美学质量上的表现。'}}}, {'id': 'https://huggingface.co/papers/2509.01624', 'title': 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with\n  Quantization-Aware Scheduling', 'url': 'https://huggingface.co/papers/2509.01624', 'abstract': "Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image diffusion models are computationally intensive, often requiring dozens of forward passes through large transformer backbones. For instance, Stable Diffusion XL generates high-quality images with 50 evaluations of a 2.6B-parameter model, an expensive process even for a single batch. Few-step diffusion models reduce this cost to 2-8 denoising steps but still depend on large, uncompressed U-Net or diffusion transformer backbones, which are often too costly for full-precision inference without datacenter GPUs. These requirements also limit existing post-training quantization methods that rely on full-precision calibration. We introduce Q-Sched, a new paradigm for post-training quantization that modifies the diffusion model scheduler rather than model weights. By adjusting the few-step sampling trajectory, Q-Sched achieves full-precision accuracy with a 4x reduction in model size. To learn quantization-aware pre-conditioning coefficients, we propose the JAQ loss, which combines text-image compatibility with an image quality metric for fine-grained optimization. JAQ is reference-free and requires only a handful of calibration prompts, avoiding full-precision inference during calibration. Q-Sched delivers substantial gains: a 15.5% FID improvement over the FP16 4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step Phased Consistency Model, showing that quantization and few-step distillation are complementary for high-fidelity generation. A large-scale user study with more than 80,000 annotations further confirms Q-Sched's effectiveness on both FLUX.1[schnell] and SDXL-Turbo.", 'score': 5, 'issue_id': 5809, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 сентября', 'en': 'September 1', 'zh': '9月1日'}, 'hash': '377f1ad33c67cc37', 'authors': ['Natalia Frumkin', 'Diana Marculescu'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2509.01624.jpg', 'data': {'categories': ['#training', '#diffusion', '#inference', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Q-Sched: Эффективная квантизация диффузионных моделей без потери качества', 'desc': 'Q-Sched - это новый метод пост-тренировочной квантизации для диффузионных моделей. Он уменьшает размер модели в 4 раза, сохраняя точность полной точности и улучшая метрики качества изображений. Q-Sched модифицирует планировщик диффузионной модели, а не веса модели, и использует JAQ-функцию потерь для оптимизации. Метод показывает значительные улучшения FID по сравнению с моделями полной точности и подтвержден масштабным пользовательским исследованием.'}, 'en': {'title': 'Q-Sched: Efficient Quantization for High-Quality Diffusion Models', 'desc': 'Q-Sched is a new method for post-training quantization specifically designed for diffusion models, which helps to significantly reduce the model size by 4 times while keeping the accuracy intact. This method modifies the diffusion model scheduler instead of changing the model weights, allowing for efficient few-step sampling that maintains full-precision performance. It introduces the JAQ loss, which optimizes quantization-aware pre-conditioning coefficients by focusing on text-image compatibility and image quality without needing full-precision calibration. The results show that Q-Sched not only improves image quality metrics but also demonstrates that quantization and few-step distillation can work together effectively for high-quality image generation.'}, 'zh': {'title': 'Q-Sched：量化与高保真生成的完美结合', 'desc': 'Q-Sched是一种新颖的后训练量化方法，专为扩散模型设计。它通过调整扩散模型的调度器，而不是直接修改模型权重，实现了模型大小减少4倍，同时保持全精度的准确性。该方法引入了JAQ损失函数，结合文本-图像兼容性和图像质量指标，进行精细优化。Q-Sched在多个实验中显示出显著的性能提升，证明了量化和少步蒸馏在高保真生成中的互补性。'}}}, {'id': 'https://huggingface.co/papers/2509.07558', 'title': 'ΔL Normalization: Rethink Loss Aggregation in RLVR', 'url': 'https://huggingface.co/papers/2509.07558', 'abstract': 'ΔL Normalization addresses gradient variance in Reinforcement Learning with Verifiable Rewards by providing an unbiased policy loss estimate with minimal variance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Delta L Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed Delta L Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.', 'score': 4, 'issue_id': 5812, 'pub_date': '2025-09-09', 'pub_date_card': {'ru': '9 сентября', 'en': 'September 9', 'zh': '9月9日'}, 'hash': 'd915dc7f29ac41a8', 'authors': ['Zhiyuan He', 'Xufang Luo', 'Yike Zhang', 'Yuqing Yang', 'Lili Qiu'], 'affiliations': ['Microsoft Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.07558.jpg', 'data': {'categories': ['#rl', '#optimization', '#training', '#reasoning'], 'emoji': '📊', 'ru': {'title': 'Стабилизация обучения языковых моделей с помощью Delta L Normalization', 'desc': 'Статья представляет метод Delta L Normalization для обучения с подкреплением с проверяемыми наградами (RLVR). Этот подход решает проблему высокой дисперсии градиентов, вызванную переменной длиной генерируемых ответов. Delta L Normalization обеспечивает несмещенную оценку функции потерь с минимальной дисперсией. Эксперименты показывают превосходство метода для различных размеров моделей, максимальных длин и задач.'}, 'en': {'title': 'Minimizing Variance for Unbiased Policy Loss in RL', 'desc': 'Delta L Normalization is a novel method designed to reduce gradient variance in Reinforcement Learning with Verifiable Rewards (RLVR). It addresses the challenge of high variability in response lengths during training, which can lead to unstable optimization. Unlike previous methods that either introduce bias or fail to minimize variance, Delta L Normalization provides an unbiased estimate of policy loss while effectively reducing gradient variance. Experimental results demonstrate its effectiveness across various model sizes and tasks, showcasing its potential to enhance the performance of large language models.'}, 'zh': {'title': 'ΔL归一化：稳定强化学习的无偏损失估计', 'desc': 'ΔL归一化是一种针对可验证奖励的强化学习中梯度方差问题的解决方案。它通过提供无偏的策略损失估计，显著降低了梯度方差，从而实现更稳定的优化。该方法特别适用于动态生成长度的情况，能够有效改善大语言模型的推理能力。实验结果表明，ΔL归一化在不同模型规模、最大长度和任务上均表现出色。'}}}, {'id': 'https://huggingface.co/papers/2509.07253', 'title': 'Benchmarking Information Retrieval Models on Complex Retrieval Tasks', 'url': 'https://huggingface.co/papers/2509.07253', 'abstract': 'A benchmark of complex retrieval tasks reveals that even state-of-the-art models struggle with high-quality retrieval, and LLM-based query expansion does not consistently improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are incredible and versatile tools for text-based tasks that have enabled countless, previously unimaginable, applications. Retrieval models, in contrast, have not yet seen such capable general-purpose models emerge. To achieve this goal, retrieval models must be able to perform complex retrieval tasks, where queries contain multiple parts, constraints, or requirements in natural language. These tasks represent a natural progression from the simple, single-aspect queries that are used in the vast majority of existing, commonly used evaluation sets. Complex queries naturally arise as people expect search systems to handle more specific and often ambitious information requests, as is demonstrated by how people use LLM-based information systems. Despite the growing desire for retrieval models to expand their capabilities in complex retrieval tasks, there exist limited resources to assess the ability of retrieval models on a comprehensive set of diverse complex tasks. The few resources that do exist feature a limited scope and often lack realistic settings making it hard to know the true capabilities of retrieval models on complex real-world retrieval tasks. To address this shortcoming and spur innovation in next-generation retrieval models, we construct a diverse and realistic set of complex retrieval tasks and benchmark a representative set of state-of-the-art retrieval models. Additionally, we explore the impact of LLM-based query expansion and rewriting on retrieval quality. Our results show that even the best models struggle to produce high-quality retrieval results with the highest average nDCG@10 of only 0.346 and R@100 of only 0.587 across all tasks. Although LLM augmentation can help weaker models, the strongest model has decreased performance across all metrics with all rewriting techniques.', 'score': 3, 'issue_id': 5819, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': '8b55377fd1797b1f', 'authors': ['Julian Killingback', 'Hamed Zamani'], 'affiliations': ['University of Massachusetts Amherst, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.07253.jpg', 'data': {'categories': ['#survey', '#rag', '#benchmark', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Сложный поиск: современные модели не справляются', 'desc': 'Исследование показало, что современные модели поиска испытывают трудности с выполнением сложных задач извлечения информации. Даже самые продвинутые системы достигают лишь средних показателей качества при работе с многоаспектными запросами. Использование языковых моделей (LLM) для расширения запросов не приводит к стабильному улучшению результатов. Авторы создали набор разнообразных и реалистичных сложных задач поиска для оценки возможностей существующих моделей и стимулирования разработки более совершенных систем.'}, 'en': {'title': 'Benchmarking Complex Retrieval: Challenges and Opportunities', 'desc': 'This paper evaluates the performance of state-of-the-art retrieval models on complex retrieval tasks, which involve multi-part queries and specific constraints. It highlights that even advanced models struggle to deliver high-quality results, with the best achieving only moderate scores in retrieval metrics. The study also examines the effectiveness of using large language models (LLMs) for query expansion and rewriting, finding that while they can assist weaker models, they may hinder the performance of stronger ones. To foster improvement in retrieval systems, the authors propose a new benchmark of diverse and realistic complex retrieval tasks.'}, 'zh': {'title': '推动复杂检索任务的创新', 'desc': '这篇论文探讨了复杂检索任务的基准测试，发现即使是最先进的模型在高质量检索方面也面临挑战。研究表明，基于大型语言模型（LLM）的查询扩展并不总能提高检索性能。为了推动检索模型的创新，作者构建了一套多样化且现实的复杂检索任务，并对一组代表性的先进检索模型进行了基准测试。结果显示，尽管LLM增强可以帮助较弱的模型，但最强模型在所有重写技术下的性能均有所下降。'}}}, {'id': 'https://huggingface.co/papers/2509.06938', 'title': 'From Noise to Narrative: Tracing the Origins of Hallucinations in\n  Transformers', 'url': 'https://huggingface.co/papers/2509.06938', 'abstract': "Transformer models tend to activate input-insensitive semantic features under uncertainty, leading to hallucinations that can be predicted from their internal activations.  \t\t\t\t\tAI-generated summary \t\t\t\t As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.", 'score': 1, 'issue_id': 5822, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 сентября', 'en': 'September 8', 'zh': '9月8日'}, 'hash': 'dab8cc466304604a', 'authors': ['Praneet Suresh', 'Jack Stanley', 'Sonia Joseph', 'Luca Scimeca', 'Danilo Bzdok'], 'affiliations': ['Meta AI', 'Mila - Quebec AI Institute'], 'pdf_title_img': 'assets/pdf/title_img/2509.06938.jpg', 'data': {'categories': ['#architecture', '#security', '#alignment', '#training', '#rlhf', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие механизмов галлюцинаций в трансформерных моделях', 'desc': 'Исследование показывает, что трансформерные модели активируют семантические признаки, нечувствительные к входным данным, в условиях неопределенности, что приводит к галлюцинациям. Эти галлюцинации можно предсказать по внутренним активациям модели. Авторы обнаружили, что количество семантических концепций, используемых моделью, растет по мере увеличения неструктурированности входных данных. Результаты исследования имеют важное значение для безопасности ИИ, выявления потенциальных уязвимостей и количественной оценки риска галлюцинаций модели.'}, 'en': {'title': 'Understanding Hallucinations in Transformer Models', 'desc': 'This paper investigates how transformer models generate hallucinations, which are incorrect outputs, particularly under uncertain input conditions. It shows that as the input becomes less structured, the model activates more semantic concepts that are not directly related to the input, leading to these hallucinations. The authors use sparse autoencoders to analyze the internal activations of the models and find that they can predict hallucinations based on these activations. The findings highlight the importance of understanding transformer behavior to improve AI safety and align models with human values.'}, 'zh': {'title': '揭示变换器模型幻觉的秘密', 'desc': '本研究探讨了变换器模型在输入不确定性下如何产生幻觉现象。我们发现，当输入信息变得更加无结构时，模型激活的语义概念数量会增加。特别是在纯噪声输入的情况下，模型会激活一些与输入无关的语义特征，导致输出幻觉。通过分析模型内部激活，我们能够预测这些幻觉的出现，从而为提高AI模型的安全性和可靠性提供了重要依据。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment (4)', '#architecture (2)', '#audio', '#benchmark (7)', '#cv (3)', '#data (2)', '#dataset (4)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (3)', '#open_source (5)', '#optimization (10)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (7)', '#rlhf (5)', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (14)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-09-11 00:51',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-11 00:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-11 00:51')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    