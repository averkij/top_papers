
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. January 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 января</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-07.html">⬅️ <span id="prev-date">07.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-09.html">➡️ <span id="next-date">09.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 января', 'en': 'January 8', 'zh': '1月8日'};
        let feedDateNext = {'ru': '09.01', 'en': '01/09', 'zh': '1月9日'};
        let feedDatePrev = {'ru': '07.01', 'en': '01/07', 'zh': '1月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.03262', 'title': 'REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models', 'url': 'https://huggingface.co/papers/2501.03262', 'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical approach for aligning large language models with human preferences, witnessing rapid algorithmic evolution through methods such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), REINFORCE Leave One-Out (RLOO), ReMax, and Group Relative Policy Optimization (GRPO). We present REINFORCE++, an enhanced variant of the classical REINFORCE algorithm that incorporates key optimization techniques from PPO while eliminating the need for a critic network. REINFORCE++ achieves three primary objectives: (1) simplicity (2) enhanced training stability, and (3) reduced computational overhead. Through extensive empirical evaluation, we demonstrate that REINFORCE++ exhibits superior stability compared to GRPO and achieves greater computational efficiency than PPO while maintaining comparable performance. The implementation is available at https://github.com/OpenRLHF/OpenRLHF.', 'score': 41, 'issue_id': 1553, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': 'a05acf5aab0c07dd', 'authors': ['Jian Hu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2501.03262.jpg', 'data': {'categories': ['#training', '#rlhf', '#optimization', '#alignment'], 'emoji': '🤖', 'ru': {'title': 'REINFORCE++: Простой и эффективный алгоритм для RLHF', 'desc': 'В статье представлен REINFORCE++, улучшенная версия алгоритма REINFORCE для обучения с подкреплением на основе обратной связи от человека (RLHF). REINFORCE++ сочетает ключевые техники оптимизации из PPO, но не требует использования критической нейронной сети. Алгоритм отличается простотой, повышенной стабильностью обучения и сниженными вычислительными затратами. Эмпирические исследования показывают, что REINFORCE++ демонстрирует лучшую стабильность по сравнению с GRPO и большую вычислительную эффективность, чем PPO, при сохранении сопоставимой производительности.'}, 'en': {'title': 'REINFORCE++: Simplifying Reinforcement Learning with Human Feedback', 'desc': 'This paper introduces REINFORCE++, a new version of the REINFORCE algorithm designed to improve the training of reinforcement learning models using human feedback. It combines the strengths of Proximal Policy Optimization (PPO) while removing the need for a critic network, making it simpler and more efficient. The authors highlight that REINFORCE++ offers better training stability and lower computational costs compared to existing methods like GRPO and PPO. Their experiments show that REINFORCE++ performs well while being easier to use and faster to train.'}, 'zh': {'title': 'REINFORCE++：简化与高效的强化学习新选择', 'desc': '强化学习中的人类反馈（RLHF）是一种重要的方法，用于使大型语言模型更符合人类的偏好。本文提出了REINFORCE++，这是经典REINFORCE算法的增强版本，结合了PPO的优化技术，并且不再需要评论网络。REINFORCE++的主要目标是实现简单性、提高训练稳定性和减少计算开销。通过大量实证评估，我们证明了REINFORCE++在稳定性上优于GRPO，并且在计算效率上超过PPO，同时保持了相似的性能。'}}}, {'id': 'https://huggingface.co/papers/2501.02955', 'title': 'MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models', 'url': 'https://huggingface.co/papers/2501.02955', 'abstract': "In recent years, vision language models (VLMs) have made significant advancements in video understanding. However, a crucial capability - fine-grained motion comprehension - remains under-explored in current benchmarks. To address this gap, we propose MotionBench, a comprehensive evaluation benchmark designed to assess the fine-grained motion comprehension of video understanding models. MotionBench evaluates models' motion-level perception through six primary categories of motion-oriented question types and includes data collected from diverse sources, ensuring a broad representation of real-world video content. Experimental results reveal that existing VLMs perform poorly in understanding fine-grained motions. To enhance VLM's ability to perceive fine-grained motion within a limited sequence length of LLM, we conduct extensive experiments reviewing VLM architectures optimized for video feature compression and propose a novel and efficient Through-Encoder (TE) Fusion method. Experiments show that higher frame rate inputs and TE Fusion yield improvements in motion understanding, yet there is still substantial room for enhancement. Our benchmark aims to guide and motivate the development of more capable video understanding models, emphasizing the importance of fine-grained motion comprehension. Project page: https://motion-bench.github.io .", 'score': 30, 'issue_id': 1551, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'a7051c2d239484b4', 'authors': ['Wenyi Hong', 'Yean Cheng', 'Zhuoyi Yang', 'Weihan Wang', 'Lefan Wang', 'Xiaotao Gu', 'Shiyu Huang', 'Yuxiao Dong', 'Jie Tang'], 'affiliations': ['Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2501.02955.jpg', 'data': {'categories': ['#architecture', '#optimization', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'MotionBench: новый рубеж в понимании движения для моделей компьютерного зрения', 'desc': 'Статья представляет новый бенчмарк MotionBench для оценки способности моделей компьютерного зрения понимать детальные движения в видео. Авторы обнаружили, что существующие модели плохо справляются с этой задачей. Для улучшения результатов предложен новый метод Through-Encoder Fusion, а также использование видео с более высокой частотой кадров. Бенчмарк призван стимулировать развитие более совершенных моделей понимания видео.'}, 'en': {'title': 'Enhancing Video Understanding with Fine-Grained Motion Comprehension', 'desc': "This paper introduces MotionBench, a new benchmark for evaluating how well vision language models (VLMs) understand fine-grained motion in videos. It identifies a gap in current models' abilities to comprehend detailed motion, which is crucial for accurate video analysis. The benchmark includes various motion-oriented question types and diverse video data to ensure comprehensive testing. The authors also propose a Through-Encoder Fusion method to improve VLM performance, highlighting the need for further advancements in fine-grained motion comprehension."}, 'zh': {'title': '提升视频理解的细粒度运动能力', 'desc': '近年来，视觉语言模型（VLMs）在视频理解方面取得了显著进展。然而，细粒度运动理解这一关键能力在当前基准测试中仍未得到充分探索。为了解决这一问题，我们提出了MotionBench，这是一个全面的评估基准，旨在评估视频理解模型的细粒度运动理解能力。实验结果表明，现有的VLM在理解细粒度运动方面表现不佳，因此我们提出了一种新颖的Through-Encoder（TE）融合方法，以提高模型的运动理解能力。'}}}, {'id': 'https://huggingface.co/papers/2501.03575', 'title': 'Cosmos World Foundation Model Platform for Physical AI', 'url': 'https://huggingface.co/papers/2501.03575', 'abstract': 'Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via https://github.com/NVIDIA/Cosmos.', 'score': 23, 'issue_id': 1552, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': 'f4b2044cbc1076a8', 'authors': ['NVIDIA', ':', 'Niket Agarwal', 'Arslan Ali', 'Maciej Bala', 'Yogesh Balaji', 'Erik Barker', 'Tiffany Cai', 'Prithvijit Chattopadhyay', 'Yongxin Chen', 'Yin Cui', 'Yifan Ding', 'Daniel Dworakowski', 'Jiaojiao Fan', 'Michele Fenzi', 'Francesco Ferroni', 'Sanja Fidler', 'Dieter Fox', 'Songwei Ge', 'Yunhao Ge', 'Jinwei Gu', 'Siddharth Gururani', 'Ethan He', 'Jiahui Huang', 'Jacob Huffman', 'Pooya Jannaty', 'Jingyi Jin', 'Seung Wook Kim', 'Gergely Klár', 'Grace Lam', 'Shiyi Lan', 'Laura Leal-Taixe', 'Anqi Li', 'Zhaoshuo Li', 'Chen-Hsuan Lin', 'Tsung-Yi Lin', 'Huan Ling', 'Ming-Yu Liu', 'Xian Liu', 'Alice Luo', 'Qianli Ma', 'Hanzi Mao', 'Kaichun Mo', 'Arsalan Mousavian', 'Seungjun Nah', 'Sriharsha Niverty', 'David Page', 'Despoina Paschalidou', 'Zeeshan Patel', 'Lindsey Pavao', 'Morteza Ramezanali', 'Fitsum Reda', 'Xiaowei Ren', 'Vasanth Rao Naik Sabavat', 'Ed Schmerling', 'Stella Shi', 'Bartosz Stefaniak', 'Shitao Tang', 'Lyne Tchapmi', 'Przemek Tredak', 'Wei-Cheng Tseng', 'Jibin Varghese', 'Hao Wang', 'Haoxiang Wang', 'Heng Wang', 'Ting-Chun Wang', 'Fangyin Wei', 'Xinyue Wei', 'Jay Zhangjie Wu', 'Jiashu Xu', 'Wei Yang', 'Lin Yen-Chen', 'Xiaohui Zeng', 'Yu Zeng', 'Jing Zhang', 'Qinsheng Zhang', 'Yuxuan Zhang', 'Qingqing Zhao', 'Artur Zolkowski'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2501.03575.jpg', 'data': {'categories': ['#open_source', '#data', '#benchmark', '#architecture', '#video', '#multimodal', '#dataset', '#training'], 'emoji': '🌍', 'ru': {'title': 'Цифровой двойник мира для обучения физического ИИ', 'desc': 'Статья представляет платформу Cosmos World Foundation Model для разработки моделей мира в физическом ИИ. Авторы предлагают концепцию базовой модели мира, которую можно дообучать для конкретных приложений. Платформа включает конвейер курации видео, предобученные базовые модели мира, примеры дообучения и токенизаторы видео. Проект открытый и доступен на GitHub для помощи разработчикам физического ИИ в решении важных проблем общества.'}, 'en': {'title': 'Empowering Physical AI with Customizable World Models', 'desc': 'This paper introduces the Cosmos World Foundation Model Platform, designed to assist developers in creating tailored world models for Physical AI systems. It emphasizes the necessity of having a digital twin of both the AI and its environment to enable effective training. The platform includes a comprehensive video curation pipeline, pre-trained models, and tools for fine-tuning these models for specific applications. By making the platform and models open-source, the authors aim to empower developers to address significant societal challenges using Physical AI.'}, 'zh': {'title': '构建物理AI的数字双胞胎与世界模型', 'desc': '这篇论文介绍了物理人工智能（Physical AI）在数字训练中的重要性。为了实现这一目标，需要构建一个数字双胞胎（digital twin）和一个世界模型（world model）。我们提出了Cosmos世界基础模型平台，帮助开发者为物理人工智能定制世界模型。该平台提供了视频策划管道、预训练的世界基础模型以及后训练示例，旨在解决社会中的关键问题，并且是开源的。'}}}, {'id': 'https://huggingface.co/papers/2501.03895', 'title': 'LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token', 'url': 'https://huggingface.co/papers/2501.03895', 'abstract': 'The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.', 'score': 19, 'issue_id': 1550, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '925d2f81d6fcbb0b', 'authors': ['Shaolei Zhang', 'Qingkai Fang', 'Zhe Yang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.03895.jpg', 'data': {'categories': ['#agi', '#video', '#multimodal', '#architecture', '#optimization', '#cv', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективность через минимизацию: революция в мультимодальных моделях', 'desc': 'Статья представляет LLaVA-Mini - эффективную мультимодальную модель с минимальным количеством визуальных токенов. Авторы обнаружили, что большинство визуальных токенов играют ключевую роль только в ранних слоях языковой модели. LLaVA-Mini вводит предварительное слияние модальностей, чтобы объединить визуальную информацию с текстовыми токенами заранее. Эксперименты показывают, что LLaVA-Mini превосходит LLaVA-v1.5, используя всего 1 визуальный токен вместо 576, что значительно повышает эффективность обработки.'}, 'en': {'title': 'Maximizing Efficiency with Minimal Vision Tokens in LMMs', 'desc': 'This paper presents LLaVA-Mini, an efficient large multimodal model (LMM) designed to reduce the number of vision tokens while maintaining visual information integrity. The authors identify that most vision tokens are primarily important in the early layers of the language model, where they integrate visual data with text. By implementing a technique called modality pre-fusion, LLaVA-Mini compresses the input from 576 vision tokens to just one, significantly enhancing efficiency. Experimental results show that LLaVA-Mini not only outperforms its predecessor but also achieves a 77% reduction in computational load and rapid processing times for high-resolution images and videos.'}, 'zh': {'title': '高效多模态模型LLaVA-Mini的创新之路', 'desc': '本文介绍了一种高效的多模态模型LLaVA-Mini，该模型通过减少视觉标记的数量来提高效率。研究发现，大多数视觉标记在大型语言模型的早期层中起着关键作用，因此可以在此之前将视觉信息与文本标记融合。LLaVA-Mini采用了模态预融合的方法，将视觉信息提前融合，从而将输入到语言模型的视觉标记压缩为一个标记。实验结果表明，LLaVA-Mini在多个基准测试中表现优于之前的模型，且显著降低了计算复杂度和延迟。'}}}, {'id': 'https://huggingface.co/papers/2501.04001', 'title': 'Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos', 'url': 'https://huggingface.co/papers/2501.04001', 'abstract': 'This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications.', 'score': 16, 'issue_id': 1555, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': 'd079946bf74858cd', 'authors': ['Haobo Yuan', 'Xiangtai Li', 'Tao Zhang', 'Zilong Huang', 'Shilin Xu', 'Shunping Ji', 'Yunhai Tong', 'Lu Qi', 'Jiashi Feng', 'Ming-Hsuan Yang'], 'affiliations': ['Bytedance Seed', 'Peking University', 'UC Merced', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2501.04001.jpg', 'data': {'categories': ['#dataset', '#multimodal', '#benchmark', '#cv'], 'emoji': '🎥', 'ru': {'title': 'Sa2VA: Единая модель для понимания изображений и видео', 'desc': 'Sa2VA - это первая унифицированная модель для плотного заземленного понимания изображений и видео. Она объединяет SAM-2 (модель сегментации видео) с LLaVA (продвинутой моделью компьютерного зрения и языка) в едином пространстве токенов большой языковой модели. Sa2VA генерирует токены инструкций, направляющие SAM-2 в создании точных масок, что позволяет осуществлять заземленное мультимодальное понимание как статического, так и динамического визуального контента. Модель достигает передовых результатов в различных задачах, особенно в сегментации объектов по ссылкам в видео.'}, 'en': {'title': 'Sa2VA: Unifying Image and Video Understanding for Enhanced Multi-Modal Tasks', 'desc': 'Sa2VA is a groundbreaking model that integrates image and video understanding into a single framework. It combines the strengths of SAM-2 for video segmentation and LLaVA for vision-language tasks, allowing it to handle various multi-modal tasks with minimal tuning. By creating a shared token space for text, images, and videos, Sa2VA can generate specific instruction tokens that help in accurately segmenting objects in both images and videos. The introduction of the Ref-SAV dataset further enhances its capabilities, enabling it to achieve top performance in complex visual environments.'}, 'zh': {'title': 'Sa2VA：图像与视频的统一理解模型', 'desc': '本研究提出了Sa2VA，这是第一个统一的模型，能够对图像和视频进行密集的基础理解。与现有的多模态大型语言模型不同，Sa2VA支持多种图像和视频任务，包括引用分割和对话，且只需最少的一次性指令调优。Sa2VA结合了基础视频分割模型SAM-2和先进的视觉语言模型LLaVA，将文本、图像和视频统一到共享的LLM令牌空间中。实验表明，Sa2VA在多个任务上达到了最先进的水平，特别是在引用视频对象分割方面，展示了其在复杂现实应用中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.03847', 'title': 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control', 'url': 'https://huggingface.co/papers/2501.03847', 'abstract': 'Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.', 'score': 10, 'issue_id': 1552, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '975d5fa9d59bde28', 'authors': ['Zekai Gu', 'Rui Yan', 'Jiahao Lu', 'Peng Li', 'Zhiyang Dou', 'Chenyang Si', 'Zhen Dong', 'Qifeng Liu', 'Cheng Lin', 'Ziwei Liu', 'Wenping Wang', 'Yuan Liu'], 'affiliations': ['Hong Kong University of Science and Technology, China', 'Nanyang Technological University, Singapore', 'Texas A&M University, U.S.A', 'The University of Hong Kong, China', 'Wuhan University, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.03847.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d'], 'emoji': '🎬', 'ru': {'title': 'DaS: Универсальный контроль над генерацией видео через 3D-сигналы', 'desc': 'Авторы представляют новый подход под названием Diffusion as Shader (DaS) для контролируемой генерации видео с помощью диффузионных моделей. В отличие от существующих методов, ограниченных одним типом контроля, DaS поддерживает множество задач управления видео в единой архитектуре. Ключевая идея заключается в использовании 3D-сигналов управления, что делает процесс диффузии видео изначально 3D-ориентированным. DaS демонстрирует сильные возможности управления в различных задачах, включая генерацию видео из 3D-моделей, контроль камеры, перенос движения и манипуляции с объектами.'}, 'en': {'title': 'Empowering Video Generation with 3D Control Signals', 'desc': 'This paper presents Diffusion as Shader (DaS), a new method for generating videos that allows for precise control over various aspects of video creation. Unlike previous models that only used 2D control signals, DaS utilizes 3D tracking videos, which helps in managing the dynamic nature of video content. This approach enables users to manipulate video elements like camera angles and object movements more effectively. The results show that DaS can maintain high-quality video generation while ensuring temporal consistency across frames, even with limited training data.'}, 'zh': {'title': '多样化视频控制的新方法：扩散作为着色器', 'desc': '扩散模型在从文本提示或图像生成高质量视频方面表现出色。然而，精确控制视频生成过程，如相机操作或内容编辑，仍然是一个重大挑战。现有的受控视频生成方法通常仅限于单一控制类型，缺乏处理多样化控制需求的灵活性。本文提出了一种新方法——扩散作为着色器（DaS），它在统一架构中支持多种视频控制任务，利用3D控制信号来实现更灵活的视频控制。'}}}, {'id': 'https://huggingface.co/papers/2501.03936', 'title': 'PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides', 'url': 'https://huggingface.co/papers/2501.03936', 'abstract': 'Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, often overlooking visual design and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to understand their structural patterns and content schemas, then drafts outlines and generates slides through code actions to ensure consistency and alignment. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Experiments show that PPTAgent significantly outperforms traditional automatic presentation generation methods across all three dimensions. The code and data are available at https://github.com/icip-cas/PPTAgent.', 'score': 7, 'issue_id': 1557, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '57bb4703056c9e20', 'authors': ['Hao Zheng', 'Xinyan Guan', 'Hao Kong', 'Jia Zheng', 'Hongyu Lin', 'Yaojie Lu', 'Ben He', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'Shanghai Jiexin Technology', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2501.03936.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#dataset'], 'emoji': '🎭', 'ru': {'title': 'PPTAgent: ИИ-помощник для создания презентаций нового уровня', 'desc': 'Исследователи представили PPTAgent - систему для автоматического создания презентаций из документов. В отличие от существующих методов, PPTAgent улучшает не только качество контента, но и визуальный дизайн и структурную согласованность. Система использует двухэтапный подход, вдохновленный рабочим процессом человека: сначала анализирует образцы презентаций, затем создает слайды с помощью программных действий. Авторы также разработали фреймворк PPTEval для комплексной оценки генерируемых презентаций.'}, 'en': {'title': 'PPTAgent: Elevating Presentation Generation with Content, Design, and Coherence', 'desc': 'This paper presents PPTAgent, a novel approach for automatically generating presentations from documents. Unlike existing methods that focus solely on content quality, PPTAgent enhances the overall presentation by considering visual design and structural coherence as well. It employs a two-stage, edit-based process that first analyzes reference presentations to extract patterns and then generates slides through code actions. Additionally, the authors introduce PPTEval, a framework for evaluating presentations based on content, design, and coherence, demonstrating that PPTAgent outperforms traditional methods in all areas.'}, 'zh': {'title': '智能生成高质量演示文稿的解决方案', 'desc': '本文提出了一种名为PPTAgent的自动生成演示文稿的方法。该方法通过两阶段的编辑式流程，综合考虑内容质量、视觉设计和结构一致性。PPTAgent首先分析参考演示文稿，以理解其结构模式和内容框架，然后通过代码操作草拟大纲并生成幻灯片。为了全面评估生成演示文稿的质量，本文还引入了PPTEval评估框架，从内容、设计和一致性三个维度进行评估。'}}}, {'id': 'https://huggingface.co/papers/2501.03714', 'title': 'MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2501.03714', 'abstract': '3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and representing complex real-world motions. To tackle these issues, we propose MoDecGS, a memory-efficient Gaussian splatting framework designed for reconstructing novel views in challenging scenarios with complex motions. We introduce GlobaltoLocal Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarsetofine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), extending static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions, by directly deforming the implicit Scaffold attributes which are anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, allowing MoDecGS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDecGS achieves an average 70% reduction in model size over stateoftheart methods for dynamic 3D Gaussians from realworld dynamic videos while maintaining or even improving rendering quality.', 'score': 5, 'issue_id': 1556, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': 'c6cfa761edc047da', 'authors': ['Sangwoon Kwak', 'Joonsoo Kim', 'Jun Young Jeong', 'Won-Sik Cheong', 'Jihyong Oh', 'Munchurl Kim'], 'affiliations': ['Chung-Ang University', 'Electronics and Telecommunications Research Institute', 'Korea Advanced Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2501.03714.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎭', 'ru': {'title': 'Эффективное представление сложных движений в динамических сценах', 'desc': 'MoDecGS - это новый фреймворк для эффективной реконструкции динамических сцен с использованием 3D Gaussian Splatting. Он вводит метод GlobaltoLocal Motion Decomposition (GLMD) для захвата сложных движений, используя Global Canonical Scaffolds и Local Canonical Scaffolds. Фреймворк также включает Global Anchor Deformation (GAD) для представления глобальной динамики и Local Gaussian Deformation (LGD) для точной настройки локальных движений. MoDecGS демонстрирует значительное сокращение размера модели при сохранении или улучшении качества рендеринга по сравнению с существующими методами.'}, 'en': {'title': 'Efficient Dynamic Scene Rendering with MoDecGS', 'desc': 'The paper presents MoDecGS, a new framework for 3D Gaussian Splatting that efficiently handles dynamic scenes in neural rendering. It introduces GlobaltoLocal Motion Decomposition (GLMD) to capture complex motions using both Global and Local Canonical Scaffolds. The method employs Global Anchor Deformation (GAD) for global dynamics and Local Gaussian Deformation (LGD) for fine-tuning local motions. MoDecGS significantly reduces model size by 70% compared to existing methods while enhancing rendering quality, making it suitable for real-world dynamic video reconstruction.'}, 'zh': {'title': '高效动态场景重建的新方法', 'desc': '3D高斯点云（3DGS）在场景表示和神经渲染方面取得了显著进展，但在处理动态场景时仍面临存储需求和复杂运动表示的挑战。为了解决这些问题，我们提出了MoDecGS，一个内存高效的高斯点云框架，旨在重建具有复杂运动的新视角。我们引入了全局到局部运动分解（GLMD），以粗到细的方式有效捕捉动态运动，并扩展了静态支架表示以适应动态视频重建。通过全局锚点变形（GAD）和局部高斯变形（LGD），MoDecGS在保持或提高渲染质量的同时，平均减少了70%的模型大小。'}}}, {'id': 'https://huggingface.co/papers/2501.03931', 'title': 'Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2501.03931', 'abstract': 'We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/', 'score': 4, 'issue_id': 1550, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '1c9696a99b57f781', 'authors': ['Yuechen Zhang', 'Yaoyang Liu', 'Bin Xia', 'Bohao Peng', 'Zexin Yan', 'Eric Lo', 'Jiaya Jia'], 'affiliations': ['CMU', 'CUHK', 'HKUST', 'SmartMore'], 'pdf_title_img': 'assets/pdf/title_img/2501.03931.jpg', 'data': {'categories': ['#training', '#video', '#multimodal', '#open_source', '#synthetic', '#architecture', '#diffusion'], 'emoji': '🪞', 'ru': {'title': 'Магическое зеркало: видео с сохранением личности и естественным движением', 'desc': 'Magic Mirror - это новая система для создания видео с сохранением идентичности и кинематографическим качеством. Она использует модель видеодиффузии и вводит три ключевых компонента: двойной экстрактор лицевых признаков, легкий кросс-модальный адаптер и двухэтапную стратегию обучения. Система эффективно сочетает сохранение идентичности с естественным движением, превосходя существующие методы по нескольким метрикам. Magic Mirror требует минимального добавления параметров и будет доступна в открытом доступе.'}, 'en': {'title': 'Magic Mirror: Identity-Preserved Video Generation with Cinematic Quality', 'desc': 'Magic Mirror is a new framework designed to create high-quality videos that maintain the identity of individuals while showcasing dynamic motion. It addresses the challenges faced by previous video generation methods, which often struggled to keep a consistent identity or required extensive fine-tuning for specific individuals. The framework utilizes Video Diffusion Transformers and introduces innovative components like a dual-branch facial feature extractor and a cross-modal adapter to enhance identity integration. Through a two-stage training approach, Magic Mirror achieves a remarkable balance between identity preservation and natural motion, outperforming existing techniques with fewer additional parameters.'}, 'zh': {'title': 'Magic Mirror：保持身份一致的动态视频生成', 'desc': '本文介绍了Magic Mirror，一个用于生成保持身份一致的视频框架，具有电影级质量和动态运动。尽管最近的视频扩散模型在文本到视频生成方面取得了显著进展，但在生成自然运动的同时保持一致的身份仍然具有挑战性。我们的方法基于视频扩散变换器，提出了三个关键组件，以有效整合身份信息并保持运动多样性。实验结果表明，Magic Mirror在多个指标上超越了现有方法，同时增加的参数极少。'}}}, {'id': 'https://huggingface.co/papers/2501.03916', 'title': 'Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback', 'url': 'https://huggingface.co/papers/2501.03916', 'abstract': 'The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose Dolphin, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. Dolphin can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, Dolphin first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that Dolphin can generate novel ideas continuously and complete the experiment in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 2D image classification and 3D point classification.', 'score': 3, 'issue_id': 1555, 'pub_date': '2025-01-07', 'pub_date_card': {'ru': '7 января', 'en': 'January 7', 'zh': '1月7日'}, 'hash': '9a18a60e788b7840', 'authors': ['Jiakang Yuan', 'Xiangchao Yan', 'Botian Shi', 'Tao Chen', 'Wanli Ouyang', 'Bo Zhang', 'Lei Bai', 'Yu Qiao', 'Bowen Zhou'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2501.03916.jpg', 'data': {'categories': ['#open_source', '#agents', '#science', '#3d', '#cv', '#benchmark', '#dataset'], 'emoji': '🐬', 'ru': {'title': 'Dolphin: ИИ-ассистент для полного цикла научных исследований', 'desc': 'Статья представляет Dolphin - первую замкнутую систему для автоматического проведения научных исследований. Dolphin генерирует идеи на основе релевантных статей, автоматически создает и отлаживает код для экспериментов, а затем анализирует результаты. Система способна непрерывно генерировать новые идеи и проводить эксперименты в цикле. Эксперименты показали, что Dolphin может предлагать методы, сопоставимые с современными подходами в некоторых задачах машинного обучения.'}, 'en': {'title': 'Dolphin: Automating Scientific Research with AI', 'desc': 'This paper introduces Dolphin, an innovative framework designed to automate the scientific research process. Dolphin operates in a closed-loop system, generating research ideas, conducting experiments, and analyzing results to refine future ideas. It utilizes AI to rank relevant literature and automatically generate and debug code, enhancing research efficiency. The framework has been tested on various benchmark datasets, demonstrating its ability to produce novel ideas and achieve results comparable to leading methods in tasks like image classification.'}, 'zh': {'title': 'Dolphin：自动化科学研究的新纪元', 'desc': '这篇论文介绍了一个名为Dolphin的闭环开放式自动研究框架，旨在提升科学研究的效率。Dolphin能够生成研究想法、进行实验，并根据实验结果反馈生成更高质量的想法。具体来说，Dolphin首先根据相关论文生成新想法，然后自动生成和调试代码，最后分析每个想法的结果并反馈到下一轮生成中。实验结果表明，Dolphin能够持续生成新想法，并在循环中完成实验，且在某些任务上与最先进的方法相当。'}}}, {'id': 'https://huggingface.co/papers/2501.02260', 'title': 'MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control', 'url': 'https://huggingface.co/papers/2501.02260', 'abstract': "We address the problem of facial expression editing by controling the relative variation of facial action-unit (AU) from the same person. This enables us to edit this specific person's expression in a fine-grained, continuous and interpretable manner, while preserving their identity, pose, background and detailed facial attributes. Key to our model, which we dub MagicFace, is a diffusion model conditioned on AU variations and an ID encoder to preserve facial details of high consistency. Specifically, to preserve the facial details with the input identity, we leverage the power of pretrained Stable-Diffusion models and design an ID encoder to merge appearance features through self-attention. To keep background and pose consistency, we introduce an efficient Attribute Controller by explicitly informing the model of current background and pose of the target. By injecting AU variations into a denoising UNet, our model can animate arbitrary identities with various AU combinations, yielding superior results in high-fidelity expression editing compared to other facial expression editing works. Code is publicly available at https://github.com/weimengting/MagicFace.", 'score': 3, 'issue_id': 1550, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': '9eeeb5b132839793', 'authors': ['Mengting Wei', 'Tuomas Varanka', 'Xingxun Jiang', 'Huai-Qian Khor', 'Guoying Zhao'], 'affiliations': ['Center for Machine Vision and Signal Analysis, Faculty of Information Technology and Electrical Engineering, University of Oulu, Oulu, FI-90014, Finland', 'Key Laboratory of Child Development and Learning Science of Ministry of Education, School of Biological Sciences and Medical Engineering, Southeast University, Nanjing 210096, China'], 'pdf_title_img': 'assets/pdf/title_img/2501.02260.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#open_source', '#cv'], 'emoji': '🎭', 'ru': {'title': 'Точное редактирование мимики с сохранением личности', 'desc': 'Статья представляет новый подход к редактированию мимики лица с использованием диффузионной модели, названной MagicFace. Модель позволяет точно и интерпретируемо изменять выражение лица конкретного человека, сохраняя его идентичность, позу и фоновые детали. Ключевым элементом является условная генерация на основе вариаций лицевых единиц действия (AU) и использование ID-энкодера для сохранения деталей лица. MagicFace демонстрирует превосходные результаты в высококачественном редактировании выражений лица по сравнению с другими методами.'}, 'en': {'title': 'MagicFace: Fine-Grained Facial Expression Editing with Consistent Identity', 'desc': 'This paper presents a method for editing facial expressions while maintaining the identity and other attributes of the person. The proposed model, named MagicFace, utilizes a diffusion model that is conditioned on facial action unit (AU) variations, allowing for fine-grained control over expressions. It incorporates a pretrained Stable-Diffusion model and an ID encoder to ensure high consistency in facial details. Additionally, an Attribute Controller is introduced to maintain background and pose consistency during the editing process, resulting in high-fidelity expression animations.'}, 'zh': {'title': '魔法面孔：高保真面部表情编辑的创新之路', 'desc': '我们提出了一种面部表情编辑的方法，通过控制同一人的面部动作单元（AU）的相对变化来实现。这种方法可以细致、连续且可解释地编辑特定人的表情，同时保持他们的身份、姿势、背景和面部细节。我们的模型称为MagicFace，核心是一个基于AU变化的扩散模型和一个ID编码器，以保持面部细节的一致性。通过将AU变化注入去噪UNet，我们的模型能够以高保真度编辑面部表情，效果优于其他相关工作。'}}}, {'id': 'https://huggingface.co/papers/2501.02790', 'title': 'Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model', 'url': 'https://huggingface.co/papers/2501.02790', 'abstract': 'Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method.', 'score': 2, 'issue_id': 1562, 'pub_date': '2025-01-06', 'pub_date_card': {'ru': '6 января', 'en': 'January 6', 'zh': '1月6日'}, 'hash': 'bd19e4a3e48539d4', 'authors': ['Yueqin Yin', 'Shentao Yang', 'Yujia Xie', 'Ziyi Yang', 'Yuting Sun', 'Hany Awadalla', 'Weizhu Chen', 'Mingyuan Zhou'], 'affiliations': ['Microsoft', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2501.02790.jpg', 'data': {'categories': ['#training', '#reasoning', '#alignment', '#rlhf', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Сегментарный RLHF: золотая середина между токенами и бандитами', 'desc': 'Данная статья представляет новый подход к обучению языковых моделей с подкреплением на основе обратной связи от человека (RLHF). Авторы предлагают использовать сегментарную модель вознаграждения, которая присваивает награду семантически завершенным текстовым сегментам. Метод позволяет динамическую сегментацию текста и совместим со стандартными наборами данных последовательных предпочтений. Для эффективного RL-обучения языковой модели авторы обобщают классические нормализаторы скалярного бандитного вознаграждения в локально-зависимые функции нормализации.'}, 'en': {'title': 'Enhancing Language Models with Segment-Level Rewards in RLHF', 'desc': 'This paper discusses a new approach to Reinforcement Learning from Human Feedback (RLHF) for language models (LMs). It critiques previous methods that treat the task as a bandit problem, which can overlook the sequential nature of text generation and lead to sparse rewards. The authors propose a segment-level reward model that assigns rewards to complete text segments, improving reward assignment. Their method incorporates dynamic text segmentation and enhances training efficiency by using location-aware normalizer functions, showing competitive results on established RLHF benchmarks.'}, 'zh': {'title': '段落级奖励模型：强化学习的新突破', 'desc': '本论文探讨了如何通过人类反馈进行强化学习（RLHF），以使语言模型（LM）更符合人类偏好。以往的RLHF研究通常采用赌博机模型，但这种方法忽视了语言模型生成的序列特性，并可能面临稀疏奖励的问题。我们提出了一种基于段落级奖励模型的方法，为每个语义完整的文本段落分配奖励，从而克服了以往方法的不足。通过动态文本分割和与标准序列偏好数据集的兼容性，我们的方法在多个RLHF基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2501.02393', 'title': 'Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers', 'url': 'https://huggingface.co/papers/2501.02393', 'abstract': "We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.", 'score': 1, 'issue_id': 1563, 'pub_date': '2025-01-04', 'pub_date_card': {'ru': '4 января', 'en': 'January 4', 'zh': '1月4日'}, 'hash': 'a200448c9795e159', 'authors': ['Markus J. Buehler'], 'affiliations': ['Laboratory for Atomistic and Molecular Mechanics (LAMM) MIT Cambridge, MA 02139, USA'], 'pdf_title_img': 'assets/pdf/title_img/2501.02393.jpg', 'data': {'categories': ['#graphs', '#architecture', '#interpretability', '#training'], 'emoji': '🕸️', 'ru': {'title': 'Трансформеры эволюционируют в графовые модели для реляционного рассуждения', 'desc': 'Статья представляет новый подход к модификации архитектуры Трансформеров путем интеграции графового реляционного рассуждения в механизм внимания. Авторы переформулируют механизм внимания Трансформера как графовую операцию и предлагают Graph-Aware Isomorphic Attention, используя стратегии моделирования графов, такие как Graph Isomorphism Networks (GIN) и Principal Neighborhood Aggregation (PNA). Метод позволяет улучшить представление реляционных структур, уменьшить разрыв в обобщении и повысить производительность обучения. Также предложен метод тонкой настройки Sparse GIN-Attention, который интерпретирует матрицы внимания как разреженные графы смежности, улучшая адаптивность предобученных моделей.'}, 'en': {'title': 'Transforming Attention: Merging Graphs and Transformers for Enhanced Learning', 'desc': 'This paper introduces a new way to enhance Transformer models by incorporating graph-based reasoning into their attention mechanisms. By treating attention as a graph operation, the authors propose a method called Graph-Aware Isomorphic Attention, which utilizes advanced graph techniques to better capture relationships in data. They also present Sparse GIN-Attention, a fine-tuning method that interprets attention matrices as sparse graphs, improving the adaptability of pre-trained models with less computational cost. Overall, this approach not only improves learning performance but also opens up new possibilities for applying Transformers in various fields like bioinformatics and language modeling.'}, 'zh': {'title': '图感知注意力：Transformer的新视角', 'desc': '本文提出了一种通过将图感知关系推理整合到注意力机制中来修改Transformer架构的方法。这种方法将Transformer的注意力机制重新表述为图操作，并提出了图感知同构注意力（Graph-Aware Isomorphic Attention）。该方法利用图同构网络（GIN）和主邻域聚合（PNA）等先进的图建模策略，增强了关系结构的表示能力。通过引入稀疏GIN注意力（Sparse GIN-Attention），我们展示了如何在保持计算效率的同时，提升预训练模型的适应性和泛化能力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (1)', '#agi (1)', '#alignment (2)', '#architecture (5)', '#audio', '#benchmark (7)', '#cv (4)', '#data (1)', '#dataset (4)', '#diffusion (3)', '#ethics', '#games', '#graphs (1)', '#hallucinations', '#healthcare', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (3)', '#plp', '#rag', '#reasoning (1)', '#rl', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (5)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-08 19:07',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-08 19:07')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-08 19:07')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    