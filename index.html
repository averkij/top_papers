
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. November 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 0;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            margin-top: 10px;
            margin-bottom: 10px;
            display: block;
            border-radius: 5px;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 ноября</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-21.html">⬅️ <span id="prev-date">21.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-11-25.html">➡️ <span id="next-date">25.11</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-11.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 ноября', 'en': 'November 22', 'zh': '11月22日'};
        let feedDateNext = {'ru': '25.11', 'en': '11/25', 'zh': '11月25日'};
        let feedDatePrev = {'ru': '21.11', 'en': '11/21', 'zh': '11月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'Статья от ', 'en': 'Published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.10442', 'title': 'Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization', 'url': 'https://huggingface.co/papers/2411.10442', 'abstract': 'Existing open-source multimodal large language models (MLLMs) generally follow a training process involving pre-training and supervised fine-tuning. However, these models suffer from distribution shifts, which limit their multimodal reasoning, particularly in the Chain-of-Thought (CoT) performance. To address this, we introduce a preference optimization (PO) process to enhance the multimodal reasoning capabilities of MLLMs. Specifically, (1) on the data side, we design an automated preference data construction pipeline to create MMPR, a high-quality, large-scale multimodal reasoning preference dataset. and (2) on the model side, we explore integrating PO with MLLMs, developing a simple yet effective method, termed Mixed Preference Optimization (MPO), which boosts multimodal CoT performance. Our approach demonstrates improved performance across multiple benchmarks, particularly in multimodal reasoning tasks. Notably, our model, InternVL2-8B-MPO, achieves an accuracy of 67.0 on MathVista, outperforming InternVL2-8B by 8.7 points and achieving performance comparable to the 10x larger InternVL2-76B. We hope this study could inspire further advancements in MLLMs. Code, data, and model shall be publicly released.', 'score': 49, 'issue_id': 722, 'pub_date': '2024-11-15', 'pub_date_card': {'ru': '15 ноября', 'en': 'November 15', 'zh': '11月15日'}, 'hash': '3cc3675b352b1634', 'authors': ['Weiyun Wang', 'Zhe Chen', 'Wenhai Wang', 'Yue Cao', 'Yangzhou Liu', 'Zhangwei Gao', 'Jinguo Zhu', 'Xizhou Zhu', 'Lewei Lu', 'Yu Qiao', 'Jifeng Dai'], 'affiliations': ['Fudan University', 'Nanjing University', 'OpenGVLab, Shanghai AI Laboratory', 'SenseTime Research', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.10442.jpg', 'data': {'categories': ['#training', '#benchmark', '#reasoning', '#open_source', '#dataset', '#multimodal', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Усиление мультимодальных рассуждений ИИ через оптимизацию предпочтений', 'desc': 'Исследователи представили новый метод улучшения мультимодальных языковых моделей (MLLM) с помощью оптимизации предпочтений (PO). Они создали крупномасштабный набор данных MMPR для мультимодальных рассуждений и разработали метод смешанной оптимизации предпочтений (MPO). Их модель InternVL2-8B-MPO показала значительное улучшение производительности в задачах мультимодальных рассуждений, особенно в тестах типа Chain-of-Thought. Результаты демонстрируют потенциал этого подхода для повышения способностей MLLM к мультимодальным рассуждениям.'}, 'en': {'title': 'Boosting Multimodal Reasoning with Preference Optimization', 'desc': 'This paper addresses the limitations of existing multimodal large language models (MLLMs) in reasoning tasks due to distribution shifts. It introduces a preference optimization (PO) process to enhance the Chain-of-Thought (CoT) performance of these models. The authors create a high-quality dataset called MMPR through an automated preference data construction pipeline and develop a method called Mixed Preference Optimization (MPO) to integrate PO with MLLMs. Their model, InternVL2-8B-MPO, shows significant improvements in multimodal reasoning tasks, achieving higher accuracy than previous models and demonstrating the potential for further advancements in this field.'}, 'zh': {'title': '提升多模态推理能力的新方法', 'desc': '本文介绍了一种新的方法来提高多模态大语言模型（MLLMs）的推理能力。我们提出了一种偏好优化（PO）过程，旨在解决现有模型在多模态推理中的分布偏移问题。通过构建高质量的大规模多模态推理偏好数据集MMPR，并将PO与MLLMs结合，我们开发了混合偏好优化（MPO）方法，显著提升了模型在多模态链式思维（CoT）任务中的表现。实验结果表明，我们的模型在多个基准测试中表现优异，特别是在多模态推理任务上。'}}}, {'id': 'https://huggingface.co/papers/2411.14402', 'title': 'Multimodal Autoregressive Pre-training of Large Vision Encoders', 'url': 'https://huggingface.co/papers/2411.14402', 'abstract': 'We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings.', 'score': 34, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '95826a974f0f9bb2', 'authors': ['Enrico Fini', 'Mustafa Shukor', 'Xiujun Li', 'Philipp Dufter', 'Michal Klein', 'David Haldimann', 'Sai Aitharaju', 'Victor Guilherme Turrisi da Costa', 'Louis Béthune', 'Zhe Gan', 'Alexander T Toshev', 'Marcin Eichner', 'Moin Nabi', 'Yinfei Yang', 'Joshua M. Susskind', 'Alaaeldin El-Nouby'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2411.14402.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'AIMV2: универсальный энкодер изображений с мультимодальным предобучением', 'desc': 'Представлен новый метод предобучения крупномасштабных энкодеров изображений под названием AIMV2. Он расширяет авторегрессионное предобучение на мультимодальный контекст, включая изображения и текст. AIMV2 использует мультимодальный декодер для генерации фрагментов изображений и текстовых токенов. Модель демонстрирует высокую производительность в различных задачах компьютерного зрения и мультимодального понимания, превосходя современные контрастивные модели.'}, 'en': {'title': 'AIMV2: Unifying Vision and Text for Superior Multimodal Understanding', 'desc': 'This paper presents AIMV2, a new method for pre-training vision encoders that can handle both images and text. It builds on recent techniques in autoregressive pre-training, allowing the model to generate image patches and text tokens effectively. AIMV2 is designed to be scalable and performs exceptionally well on various tasks, including localization and classification. The results show that AIMV2 outperforms existing models like CLIP in multimodal image understanding, achieving high accuracy on benchmarks like ImageNet-1k.'}, 'zh': {'title': 'AIMV2：多模态视觉编码的创新之路', 'desc': '我们提出了一种新颖的大规模视觉编码器预训练方法。该方法基于最近在自回归视觉模型预训练方面的进展，扩展到多模态设置，即图像和文本。我们介绍了AIMV2，这是一系列通用的视觉编码器，具有简单的预训练过程、可扩展性和在多种下游任务中的卓越表现。我们的编码器在多模态评估和视觉基准测试（如定位、基础和分类）中表现出色，AIMV2-3B编码器在ImageNet-1k上实现了89.5%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2411.13676', 'title': 'Hymba: A Hybrid-head Architecture for Small Language Models', 'url': 'https://huggingface.co/papers/2411.13676', 'abstract': 'We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the "forced-to-attend" burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size reduction, and 3.49x throughput.', 'score': 32, 'issue_id': 721, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': '24009a4acf67d4c7', 'authors': ['Xin Dong', 'Yonggan Fu', 'Shizhe Diao', 'Wonmin Byeon', 'Zijia Chen', 'Ameya Sunil Mahabaleshwarkar', 'Shih-Yang Liu', 'Matthijs Van Keirsbilck', 'Min-Hung Chen', 'Yoshi Suhara', 'Yingyan Lin', 'Jan Kautz', 'Pavlo Molchanov'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.13676.jpg', 'data': {'categories': ['#small_models', '#training', '#architecture', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Гибридная архитектура Hymba: эффективность малых языковых моделей на новом уровне', 'desc': 'Авторы представляют Hymba - семейство малых языковых моделей с гибридной параллельной архитектурой, сочетающей механизмы внимания трансформеров и модели пространства состояний (SSM) для повышения эффективности. В модель добавлены обучаемые мета-токены, хранящие важную информацию и снижающие нагрузку на механизмы внимания. Оптимизация включает межслойное разделение ключей и значений, а также частичное скользящее окно внимания. Модель Hymba-1.5B-Base превосходит все публичные модели до 2 млрд параметров и даже Llama-3.2-3B по точности, при значительном уменьшении размера кэша и увеличении пропускной способности.'}, 'en': {'title': 'Hymba: Efficient Language Models with Hybrid Architecture', 'desc': 'Hymba is a new family of small language models that combines transformer attention with state space models (SSMs) to improve efficiency. The model uses attention heads for detailed recall and SSM heads for summarizing context effectively. It also features learnable meta tokens that help retain important information without overloading the attention mechanism. Our experiments show that Hymba outperforms existing small language models, achieving better accuracy while using significantly less cache and providing faster processing speeds.'}, 'zh': {'title': 'Hymba：高效的小型语言模型新选择', 'desc': '我们提出了Hymba，这是一种小型语言模型，采用混合头并行架构，将变换器注意机制与状态空间模型（SSMs）结合，以提高效率。注意头提供高分辨率的回忆，而SSM头则实现高效的上下文总结。此外，我们引入了可学习的元标记，这些标记被添加到提示前，存储关键信息，减轻了与注意机制相关的“强制关注”负担。我们的模型通过跨层键值（KV）共享和部分滑动窗口注意机制进一步优化，结果是缓存大小紧凑。'}}}, {'id': 'https://huggingface.co/papers/2411.14405', 'title': 'Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions', 'url': 'https://huggingface.co/papers/2411.14405', 'abstract': 'Currently OpenAI o1 has sparked a surge of interest in the study of large reasoning models (LRM). Building on this momentum, Marco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding -- which are well-suited for reinforcement learning (RL) -- but also places greater emphasis on open-ended resolutions. We aim to address the question: "Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?" Marco-o1 is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies -- optimized for complex real-world problem-solving tasks.', 'score': 32, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'ef4a95abeea69237', 'authors': ['Yu Zhao', 'Huifeng Yin', 'Bo Zeng', 'Hao Wang', 'Tianqi Shi', 'Chenyang Lyu', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2411.14405.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Расширение границ искусственного интеллекта: от стандартных задач к открытым проблемам', 'desc': 'Статья описывает разработку модели Marco-o1, которая расширяет возможности OpenAI o1 в области рассуждений. Модель нацелена на решение задач с открытым концом, где отсутствуют четкие стандарты и сложно количественно оценить результаты. Marco-o1 использует усовершенствованные методы, такие как обучение с подкреплением, цепочки размышлений и метод Монте-Карло. Основная цель - создать модель, способную эффективно обобщать знания и решать сложные задачи реального мира.'}, 'en': {'title': 'Unlocking Generalization in Large Reasoning Models', 'desc': "This paper explores the capabilities of the Marco-o1 model in handling large reasoning tasks across various domains. It emphasizes the model's ability to generalize beyond traditional areas like mathematics and coding, where answers are clear-cut. The research investigates how Marco-o1 can tackle open-ended problems where standard solutions and quantifiable rewards are not readily available. Key techniques employed include Chain-of-Thought fine-tuning and Monte Carlo Tree Search, which enhance the model's reasoning and problem-solving abilities in complex scenarios."}, 'zh': {'title': '推动推理模型的广泛应用', 'desc': '这篇论文探讨了大型推理模型（LRM）的研究，特别是OpenAI的o1模型。Marco-o1不仅关注数学、物理和编程等有标准答案的学科，还强调开放式问题的解决能力。研究的核心问题是o1模型是否能够有效地推广到缺乏明确标准和难以量化奖励的更广泛领域。Marco-o1结合了链式思维（CoT）微调、蒙特卡洛树搜索（MCTS）、反思机制和创新推理策略，以优化复杂的现实问题解决任务。'}}}, {'id': 'https://huggingface.co/papers/2411.14199', 'title': 'OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs', 'url': 'https://huggingface.co/papers/2411.14199', 'abstract': "Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.", 'score': 21, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'f429efe07ec308f2', 'authors': ['Akari Asai', 'Jacqueline He', 'Rulin Shao', 'Weijia Shi', 'Amanpreet Singh', 'Joseph Chee Chang', 'Kyle Lo', 'Luca Soldaini', 'Sergey Feldman', "Mike D'arcy", 'David Wadden', 'Matt Latzke', 'Minyang Tian', 'Pan Ji', 'Shengyan Liu', 'Hao Tong', 'Bohao Wu', 'Yanyu Xiong', 'Luke Zettlemoyer', 'Graham Neubig', 'Dan Weld', 'Doug Downey', 'Wen-tau Yih', 'Pang Wei Koh', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for AI', 'Carnegie Mellon University', 'Meta', 'Stanford University', 'University of Illinois, Urbana-Champaign', 'University of North Carolina, Chapel Hill', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2411.14199.jpg', 'data': {'categories': ['#science', '#rag', '#open_source', '#multimodal', '#benchmark', '#hallucinations'], 'emoji': '🔬', 'ru': {'title': 'OpenScholar: ИИ-помощник для синтеза научной литературы', 'desc': 'Статья представляет OpenScholar - специализированную языковую модель с расширенным поиском, которая отвечает на научные запросы, используя 45 миллионов научных статей. Для оценки модели авторы разработали ScholarQABench - первый масштабный мультидоменный бенчмарк для поиска литературы. OpenScholar-8B превосходит GPT-4 и PaperQA2 по точности ответов, несмотря на меньший размер модели. Эксперты предпочли ответы OpenScholar-8B и OpenScholar-GPT4o экспертным ответам в 51% и 70% случаев соответственно.'}, 'en': {'title': 'Empowering Scientific Research with OpenScholar: Accurate, Citation-Backed Insights', 'desc': 'This paper presents OpenScholar, a retrieval-augmented language model designed to assist researchers in synthesizing scientific literature. OpenScholar effectively identifies relevant passages from a vast collection of 45 million open-access papers and generates citation-backed responses to scientific queries. The model is evaluated using ScholarQABench, a benchmark that includes expert-written queries and answers across multiple domains, demonstrating superior performance in correctness compared to other models like GPT-4o. Additionally, OpenScholar shows a significant reduction in citation hallucination, achieving accuracy comparable to human experts, and enhances the performance of existing models through its innovative architecture.'}, 'zh': {'title': 'OpenScholar：提升科学文献检索的智能助手', 'desc': '本论文介绍了一种名为OpenScholar的专用检索增强语言模型，旨在帮助科学家从4500万篇开放获取论文中提取相关信息并生成基于引用的回答。我们开发了ScholarQABench，这是第一个大规模的多领域文献搜索基准，包含2967个专家编写的查询和208个长答案，涵盖计算机科学、物理学、神经科学和生物医学等领域。OpenScholar在准确性上超越了GPT-4o和PaperQA2，尽管其模型规模较小，且在引用准确性方面与人类专家相当。我们还开源了所有代码、模型和数据，提供了公共演示，促进了科学文献的检索和理解。'}}}, {'id': 'https://huggingface.co/papers/2411.14251', 'title': 'Natural Language Reinforcement Learning', 'url': 'https://huggingface.co/papers/2411.14251', 'abstract': 'Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases. Our code will be released at https://github.com/waterhorse1/Natural-language-RL.', 'score': 21, 'issue_id': 719, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '351fc2a705b34aff', 'authors': ['Xidong Feng', 'Ziyu Wan', 'Haotian Fu', 'Bo Liu', 'Mengyue Yang', 'Girish A. Koushik', 'Zhiyuan Hu', 'Ying Wen', 'Jun Wang'], 'affiliations': ['Brown University', 'National University of Singapore', 'Shanghai Jiao Tong University', 'University College London', 'University of Bristol', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2411.14251.jpg', 'data': {'categories': ['#interpretability', '#rl', '#rlhf', '#open_source', '#games'], 'emoji': '🗣️', 'ru': {'title': 'Обучение с подкреплением заговорило на естественном языке', 'desc': 'Эта статья представляет новую концепцию - обучение с подкреплением на естественном языке (NLRL). NLRL расширяет традиционные марковские процессы принятия решений, переопределяя основные принципы RL в языковом пространстве. Используя достижения в области больших языковых моделей, NLRL может быть реализовано с помощью промптов или градиентного обучения. Эксперименты на играх Maze, Breakthrough и крестики-нолики демонстрируют эффективность и интерпретируемость этого подхода.'}, 'en': {'title': 'Revolutionizing Decision-Making with Language: Natural Language Reinforcement Learning', 'desc': "This paper introduces Natural Language Reinforcement Learning (NLRL), which adapts traditional Reinforcement Learning (RL) methods to work with natural language representations. By extending the Markov Decision Process (MDP) framework, NLRL redefines key RL concepts such as task objectives, policies, and value functions in the context of language. The approach leverages advancements in large language models (LLMs) to enhance policy and value updates through prompting or gradient-based training. Experimental results on games like Maze, Breakthrough, and Tic-Tac-Toe showcase NLRL's effectiveness and interpretability across various applications."}, 'zh': {'title': '自然语言强化学习：决策的新视角', 'desc': '强化学习（RL）通过马尔可夫决策过程（MDP）来数学化决策制定。本文提出了一种新的可能性，即自然语言强化学习（NLRL），通过将传统的MDP扩展到基于自然语言的表示空间。NLRL创新性地重新定义了强化学习的原则，包括任务目标、策略、价值函数、贝尔曼方程和策略迭代，使其适应语言的对应关系。通过在迷宫、突破和井字棋等游戏中的实验，验证了NLRL框架在多种应用场景中的有效性、效率和可解释性。'}}}, {'id': 'https://huggingface.co/papers/2411.12364', 'title': 'Ultra-Sparse Memory Network', 'url': 'https://huggingface.co/papers/2411.12364', 'abstract': 'It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget.', 'score': 16, 'issue_id': 721, 'pub_date': '2024-11-19', 'pub_date_card': {'ru': '19 ноября', 'en': 'November 19', 'zh': '11月19日'}, 'hash': '090bf8a39ee13838', 'authors': ['Zihao Huang', 'Qiyang Min', 'Hongzhi Huang', 'Defa Zhu', 'Yutao Zeng', 'Ran Guo', 'Xun Zhou'], 'affiliations': ['Seed-Foundation-Model Team, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2411.12364.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'UltraMem: сверхбыстрые трансформеры с разреженной памятью', 'desc': 'В этой статье представлен новый подход UltraMem, который использует сверхразреженные слои памяти для улучшения производительности трансформеров. Метод позволяет значительно снизить задержку при выводе, сохраняя качество модели. Исследованы законы масштабирования новой архитектуры, показывающие её преимущества перед традиционными моделями. Эксперименты с сетями, содержащими до 20 миллионов ячеек памяти, демонстрируют state-of-the-art скорость вывода и качество модели при заданном вычислительном бюджете.'}, 'en': {'title': 'UltraMem: Speeding Up Transformers with Sparse Memory!', 'desc': 'This paper presents UltraMem, a novel architecture designed to enhance the efficiency of Transformer models by integrating a large-scale, ultra-sparse memory layer. By decoupling the number of parameters from computational complexity, UltraMem addresses the high memory access costs that hinder inference speed in existing models. The authors demonstrate that their approach not only reduces inference latency but also maintains competitive model performance, even with networks containing up to 20 million memory slots. Additionally, the study explores the scaling laws of UltraMem, showing that it outperforms traditional models while adhering to computational constraints.'}, 'zh': {'title': 'UltraMem：提升推理速度的新架构', 'desc': '本论文提出了一种名为UltraMem的新架构，旨在解决Transformer模型在推理时的高内存访问成本问题。通过引入大规模的超稀疏内存层，UltraMem能够在保持模型性能的同时显著降低推理延迟。我们还研究了这种新架构的扩展规律，结果表明其具有良好的扩展性，并且在性能上优于传统模型。实验表明，我们的方法在给定的计算预算内实现了最先进的推理速度和模型性能。'}}}, {'id': 'https://huggingface.co/papers/2411.14432', 'title': 'Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.14432', 'abstract': "Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.", 'score': 15, 'issue_id': 720, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '0af1bb82d8021d3b', 'authors': ['Yuhao Dong', 'Zuyan Liu', 'Hai-Long Sun', 'Jingkang Yang', 'Winston Hu', 'Yongming Rao', 'Ziwei Liu'], 'affiliations': ['Nanjing University', 'S-Lab, NTU', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14432.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#multimodal', '#data', '#dataset', '#benchmark', '#agents', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение визуальных рассуждений ИИ через длинные цепочки и мультиагентное обучение', 'desc': 'Статья представляет Insight-V - подход к улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждениям. Авторы предлагают двухэтапный конвейер для создания длинных и структурированных данных для обучения без участия человека. Они также разрабатывают мультиагентную систему, состоящую из агента рассуждений и агента-резюме, для эффективного обучения MLLM. Результаты показывают значительное улучшение производительности на сложных мультимодальных задачах, требующих визуального рассуждения.'}, 'en': {'title': 'Empowering Multi-Modal Reasoning with Insight-V', 'desc': 'This paper introduces Insight-V, a novel approach to enhance the reasoning capabilities of multi-modal large language models (MLLMs) by generating high-quality long-chain reasoning data. The authors propose a two-step pipeline that creates structured reasoning paths without human intervention and employs a multi-granularity assessment method to ensure the quality of the generated data. They also develop a multi-agent system that includes a reasoning agent for long-chain reasoning and a summary agent to evaluate and condense the reasoning outputs. The results show that Insight-V significantly improves performance on complex multi-modal tasks, particularly those requiring visual reasoning, while maintaining effectiveness on perception-focused tasks.'}, 'zh': {'title': '提升多模态推理能力的创新方法', 'desc': '本文介绍了一种名为Insight-V的系统，旨在提高多模态大语言模型（MLLMs）的推理能力。我们设计了一个两步生成管道，以无人工干预的方式创建长且结构化的推理数据，并采用多粒度评估方法确保数据质量。研究表明，直接用复杂推理数据监督MLLMs并不能达到理想效果，因此我们构建了一个多代理系统，包括专注于长链推理的推理代理和负责评估和总结推理结果的总结代理。通过这种方法，Insight-V在视觉推理等多模态基准测试中表现出显著的性能提升。'}}}, {'id': 'https://huggingface.co/papers/2411.14430', 'title': 'Stable Flow: Vital Layers for Training-Free Image Editing', 'url': 'https://huggingface.co/papers/2411.14430', 'abstract': 'Diffusion models have revolutionized the field of content synthesis and editing. Recent models have replaced the traditional UNet architecture with the Diffusion Transformer (DiT), and employed flow-matching for improved training and sampling. However, they exhibit limited generation diversity. In this work, we leverage this limitation to perform consistent image edits via selective injection of attention features. The main challenge is that, unlike the UNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it unclear in which layers to perform the injection. Therefore, we propose an automatic method to identify "vital layers" within DiT, crucial for image formation, and demonstrate how these layers facilitate a range of controlled stable edits, from non-rigid modifications to object addition, using the same mechanism. Next, to enable real-image editing, we introduce an improved image inversion method for flow models. Finally, we evaluate our approach through qualitative and quantitative comparisons, along with a user study, and demonstrate its effectiveness across multiple applications. The project page is available at https://omriavrahami.com/stable-flow', 'score': 9, 'issue_id': 723, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '4d5707c1fdd2e4f9', 'authors': ['Omri Avrahami', 'Or Patashnik', 'Ohad Fried', 'Egor Nemchinov', 'Kfir Aberman', 'Dani Lischinski', 'Daniel Cohen-Or'], 'affiliations': ['Reichman University', 'Snap Research', 'Tel Aviv University', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2411.14430.jpg', 'data': {'categories': ['#cv', '#training', '#diffusion', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Стабильное редактирование изображений через выборочное внедрение признаков в DiT', 'desc': 'Эта статья представляет новый подход к редактированию изображений с использованием диффузионных моделей. Авторы предлагают метод выборочного внедрения признаков внимания в ключевые слои Диффузионного Трансформера (DiT) для выполнения стабильных и контролируемых изменений изображений. Они также разрабатывают улучшенный метод инверсии изображений для потоковых моделей, что позволяет редактировать реальные изображения. Эффективность подхода демонстрируется через качественные и количественные сравнения, а также пользовательское исследование.'}, 'en': {'title': 'Enhancing Image Editing with Vital Layer Injection in Diffusion Transformers', 'desc': "This paper discusses advancements in diffusion models for content synthesis and editing, specifically focusing on the Diffusion Transformer (DiT) architecture. The authors address the challenge of limited generation diversity in DiT by proposing a method to selectively inject attention features into vital layers of the model. This approach allows for consistent and controlled image edits, such as non-rigid modifications and object additions, despite DiT's lack of a traditional coarse-to-fine synthesis structure. Additionally, the paper introduces an improved image inversion method for flow models and validates the effectiveness of their technique through various evaluations and user studies."}, 'zh': {'title': '利用扩散变换器实现稳定的图像编辑', 'desc': '扩散模型在内容合成和编辑领域取得了革命性进展。最近的模型用扩散变换器（DiT）替代了传统的UNet架构，并采用流匹配技术以改善训练和采样。然而，这些模型的生成多样性有限。我们利用这一限制，通过选择性注入注意力特征来实现一致的图像编辑，并提出了一种自动识别DiT中关键层的方法，以便进行稳定的图像修改。'}}}, {'id': 'https://huggingface.co/papers/2411.14257', 'title': 'Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models', 'url': 'https://huggingface.co/papers/2411.14257', 'abstract': "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This suggests that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token.", 'score': 8, 'issue_id': 732, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '765c1d51aaa40d67', 'authors': ['Javier Ferrando', 'Oscar Obeso', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['ETH Zürich', 'UPC'], 'pdf_title_img': 'assets/pdf/title_img/2411.14257.jpg', 'data': {'categories': ['#rlhf', '#architecture', '#interpretability', '#training', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Самопознание ИИ: ключ к контролю галлюцинаций', 'desc': 'Исследователи обнаружили, что ключевым механизмом галлюцинаций в больших языковых моделях является распознавание сущностей. Используя разреженные автоэнкодеры, они выявили значимые направления в пространстве представлений, которые определяют, распознает ли модель сущность. Эти направления имеют причинно-следственную связь и могут влиять на поведение модели, заставляя ее отказываться отвечать на вопросы о известных сущностях или галлюцинировать атрибуты неизвестных. Исследование также показало, что эти механизмы сохраняются после файнтюнинга чат-моделей.'}, 'en': {'title': 'Understanding Hallucinations: Entity Recognition in Language Models', 'desc': "This paper investigates the issue of hallucinations in large language models, which occur when these models generate incorrect or fabricated information. The authors utilize sparse autoencoders to analyze how these models recognize entities and determine their knowledge about them. They find that the model's internal representations can indicate whether it knows about an entity, influencing its responses to questions. The study reveals that these representations can affect the model's behavior, such as leading it to refuse to answer questions about known entities or to invent details about unknown ones."}, 'zh': {'title': '揭示大型语言模型的自我知识与幻觉机制', 'desc': '在大型语言模型中，幻觉现象普遍存在，但其背后的机制尚不清楚，这限制了我们解决这一问题的能力。通过使用稀疏自编码器作为可解释性工具，我们发现实体识别是这些机制的关键部分，模型能够识别出自己能否回忆起某个实体的事实。稀疏自编码器揭示了表示空间中的有意义方向，这些方向可以检测模型是否认识某个实体，例如识别出它对某个运动员或电影并不了解。这表明模型具有自我知识：关于自身能力的内部表示，这些方向在模型的拒绝回答已知实体问题或对未知实体进行幻觉时具有因果相关性。'}}}, {'id': 'https://huggingface.co/papers/2411.13807', 'title': 'MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control', 'url': 'https://huggingface.co/papers/2411.13807', 'abstract': 'The rapid advancement of diffusion models has greatly improved video synthesis, especially in controllable video generation, which is essential for applications like autonomous driving. However, existing methods are limited by scalability and how control conditions are integrated, failing to meet the needs for high-resolution and long videos for autonomous driving applications. In this paper, we introduce MagicDriveDiT, a novel approach based on the DiT architecture, and tackle these challenges. Our method enhances scalability through flow matching and employs a progressive training strategy to manage complex scenarios. By incorporating spatial-temporal conditional encoding, MagicDriveDiT achieves precise control over spatial-temporal latents. Comprehensive experiments show its superior performance in generating realistic street scene videos with higher resolution and more frames. MagicDriveDiT significantly improves video generation quality and spatial-temporal controls, expanding its potential applications across various tasks in autonomous driving.', 'score': 6, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '95ef9fbe239921f8', 'authors': ['Ruiyuan Gao', 'Kai Chen', 'Bo Xiao', 'Lanqing Hong', 'Zhenguo Li', 'Qiang Xu'], 'affiliations': ['CUHK', 'HKUST', 'Huawei Cloud', 'Huawei Noahs Ark Lab'], 'pdf_title_img': 'assets/pdf/title_img/2411.13807.jpg', 'data': {'categories': ['#architecture', '#diffusion', '#training', '#video'], 'emoji': '🚗', 'ru': {'title': 'Революция в синтезе видео для беспилотных автомобилей', 'desc': 'Статья представляет MagicDriveDiT - новый подход к генерации видео для автономного вождения, основанный на архитектуре DiT. Метод улучшает масштабируемость с помощью flow matching и прогрессивного обучения для сложных сценариев. MagicDriveDiT использует пространственно-временное кодирование условий для точного контроля над латентными переменными. Эксперименты показывают превосходную производительность в генерации реалистичных уличных сцен с более высоким разрешением и большим количеством кадров.'}, 'en': {'title': 'MagicDriveDiT: Revolutionizing Video Generation for Autonomous Driving', 'desc': 'This paper presents MagicDriveDiT, a new method for generating high-quality videos using diffusion models, specifically designed for applications in autonomous driving. It addresses the limitations of existing techniques by enhancing scalability and integrating control conditions more effectively. The approach utilizes flow matching and a progressive training strategy to handle complex video scenarios, ensuring better performance in generating long, high-resolution videos. By incorporating spatial-temporal conditional encoding, MagicDriveDiT allows for precise control over the generated video content, making it suitable for various autonomous driving tasks.'}, 'zh': {'title': 'MagicDriveDiT：提升视频生成质量与控制能力的创新方法', 'desc': '本论文介绍了一种名为MagicDriveDiT的新方法，基于DiT架构，旨在解决视频合成中的可扩展性和控制条件集成问题。该方法通过流匹配技术增强了可扩展性，并采用渐进式训练策略来处理复杂场景。通过引入时空条件编码，MagicDriveDiT能够精确控制时空潜变量。实验结果表明，该方法在生成高分辨率和更多帧的真实街景视频方面表现优越，显著提升了视频生成质量和时空控制能力。'}}}, {'id': 'https://huggingface.co/papers/2411.14343', 'title': 'UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages', 'url': 'https://huggingface.co/papers/2411.14343', 'abstract': 'Large language models (LLMs) under-perform on low-resource languages due to limited training data. We present a method to efficiently collect text data for low-resource languages from the entire Common Crawl corpus. Our approach, UnifiedCrawl, filters and extracts common crawl using minimal compute resources, yielding mono-lingual datasets much larger than previously available sources. We demonstrate that leveraging this data to fine-tuning multilingual LLMs via efficient adapter methods (QLoRA) significantly boosts performance on the low-resource language, while minimizing VRAM usage. Our experiments show large improvements in language modeling perplexity and an increase in few-shot prompting scores. Our work and released source code provide an affordable approach to improve LLMs for low-resource languages using consumer hardware. Our source code is available here at https://github.com/bethelmelesse/unifiedcrawl.', 'score': 4, 'issue_id': 728, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': 'e3be7df0af13931a', 'authors': ['Bethel Melesse Tessema', 'Akhil Kedia', 'Tae-Sun Chung'], 'affiliations': ['Ajou University Suwon, South Korea', 'Independent Researcher Seoul, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.14343.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#open_source', '#training', '#multilingual', '#data'], 'emoji': '🌍', 'ru': {'title': 'Улучшение языковых моделей для малоресурсных языков с помощью UnifiedCrawl', 'desc': 'Статья представляет метод UnifiedCrawl для эффективного сбора текстовых данных для малоресурсных языков из корпуса Common Crawl. Авторы демонстрируют, что использование этих данных для дообучения многоязычных языковых моделей с помощью эффективных методов адаптации (QLoRA) значительно повышает производительность на малоресурсных языках. Эксперименты показывают улучшение перплексии языкового моделирования и повышение оценок при few-shot промптинге. Предложенный подход позволяет улучшать большие языковые модели для малоресурсных языков, используя доступное оборудование.'}, 'en': {'title': 'Boosting Low-Resource Languages with UnifiedCrawl', 'desc': 'This paper addresses the challenge of large language models (LLMs) performing poorly on low-resource languages due to a lack of training data. The authors introduce a method called UnifiedCrawl, which efficiently collects and filters text data from the Common Crawl corpus, creating larger mono-lingual datasets. They demonstrate that fine-tuning multilingual LLMs with this data using efficient adapter methods like QLoRA leads to significant improvements in language modeling and few-shot prompting scores. The approach is designed to be accessible, allowing enhancements to LLMs for low-resource languages using standard consumer hardware.'}, 'zh': {'title': '提升低资源语言模型性能的新方法', 'desc': '本论文提出了一种名为UnifiedCrawl的方法，用于从Common Crawl数据集中高效收集低资源语言的文本数据。该方法通过最小化计算资源的使用，过滤和提取数据，从而生成比以往更大的单语数据集。我们展示了利用这些数据通过高效的适配器方法（QLoRA）对多语言大语言模型进行微调，可以显著提高低资源语言的性能，同时减少显存使用。实验结果表明，语言建模的困惑度有了显著改善，少量示例提示的得分也有所提高。'}}}, {'id': 'https://huggingface.co/papers/2411.13082', 'title': 'Patience Is The Key to Large Language Model Reasoning', 'url': 'https://huggingface.co/papers/2411.13082', 'abstract': 'Recent advancements in the field of large language models, particularly through the Chain of Thought (CoT) approach, have demonstrated significant improvements in solving complex problems. However, existing models either tend to sacrifice detailed reasoning for brevity due to user preferences, or require extensive and expensive training data to learn complicated reasoning ability, limiting their potential in solving complex tasks. To bridge this gap, following the concept of scaling test-time, we propose a simple method by encouraging models to adopt a more patient reasoning style without the need of introducing new knowledge or skills. To employ a preference optimization approach, we generate detailed reasoning processes as positive examples and simple answers as negative examples, thereby training the model to favor thoroughness in its responses. Our results demonstrate a performance increase of up to 6.7% on GSM8k with training just on a lightweight dataset.', 'score': 4, 'issue_id': 726, 'pub_date': '2024-11-20', 'pub_date_card': {'ru': '20 ноября', 'en': 'November 20', 'zh': '11月20日'}, 'hash': 'b18aa77451c249b7', 'authors': ['Yijiong Yu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.13082.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Терпеливые рассуждения: новый путь к улучшению ИИ', 'desc': 'Статья описывает новый метод улучшения рассуждений больших языковых моделей без необходимости обширного обучения. Авторы предлагают подход, основанный на оптимизации предпочтений, где модель учится выдавать более подробные рассуждения вместо кратких ответов. Метод использует детальные рассуждения как положительные примеры и простые ответы как отрицательные при обучении. Результаты показывают улучшение производительности до 6.7% на наборе данных GSM8k при обучении на небольшом датасете.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with Simple Training Techniques', 'desc': 'This paper discusses improvements in large language models using the Chain of Thought (CoT) method, which enhances their ability to solve complex problems. It identifies a challenge where models often prioritize brevity over detailed reasoning, or require large amounts of training data to develop reasoning skills. The authors propose a new method that encourages models to adopt a more thorough reasoning style without needing additional knowledge. By using preference optimization, they train models with detailed reasoning as positive examples and simple answers as negative examples, achieving a notable performance increase on the GSM8k dataset with minimal training data.'}, 'zh': {'title': '提升推理能力，简化训练过程', 'desc': '本文探讨了大型语言模型在解决复杂问题时的进展，特别是通过思维链（CoT）方法的应用。现有模型往往为了简洁而牺牲详细推理，或者需要大量昂贵的训练数据来学习复杂的推理能力。为了解决这个问题，我们提出了一种简单的方法，鼓励模型采用更耐心的推理风格，而无需引入新知识或技能。通过生成详细的推理过程作为正例和简单答案作为负例，我们训练模型更倾向于全面的回答，结果显示在GSM8k上性能提高了6.7%。'}}}, {'id': 'https://huggingface.co/papers/2411.14347', 'title': 'DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding', 'url': 'https://huggingface.co/papers/2411.14347', 'abstract': "In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.", 'score': 3, 'issue_id': 739, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '5805ca9f7ff81a78', 'authors': ['Tianhe Ren', 'Yihao Chen', 'Qing Jiang', 'Zhaoyang Zeng', 'Yuda Xiong', 'Wenlong Liu', 'Zhengyu Ma', 'Junyi Shen', 'Yuan Gao', 'Xiaoke Jiang', 'Xingyu Chen', 'Zhuheng Song', 'Yuhong Zhang', 'Hongjie Huang', 'Han Gao', 'Shilong Liu', 'Hao Zhang', 'Feng Li', 'Kent Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)'], 'pdf_title_img': 'assets/pdf/title_img/2411.14347.jpg', 'data': {'categories': ['#optimization', '#long_context', '#cv', '#benchmark', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'DINO-X: Универсальное обнаружение объектов без подсказок', 'desc': 'DINO-X - это унифицированная объектно-ориентированная модель компьютерного зрения, разработанная IDEA Research, которая демонстрирует лучшую на сегодняшний день производительность в обнаружении объектов в открытом мире. Модель использует архитектуру трансформера типа кодировщик-декодировщик для создания объектного представления и поддерживает различные типы подсказок, включая текстовые, визуальные и пользовательские. DINO-X обучается на крупномасштабном наборе данных Grounding-100M, содержащем более 100 миллионов высококачественных образцов для заземления. Экспериментальные результаты показывают превосходную производительность DINO-X, особенно в задачах обнаружения объектов с длинным хвостом распределения.'}, 'en': {'title': 'DINO-X: Revolutionizing Open-World Object Detection', 'desc': 'DINO-X is a cutting-edge object-centric vision model that excels in open-world object detection. It utilizes a Transformer-based encoder-decoder architecture to create detailed object representations, allowing for flexible input through text, visual, or customized prompts. The model is trained on a massive dataset of over 100 million samples, enhancing its ability to detect a wide range of objects without needing specific prompts. Experimental results show that DINO-X outperforms previous models, especially in recognizing rare objects, achieving significant improvements in detection accuracy across various benchmarks.'}, 'zh': {'title': 'DINO-X：开放世界物体检测的新标杆', 'desc': '本文介绍了DINO-X，这是由IDEA研究团队开发的统一对象中心视觉模型，具有当前最佳的开放世界物体检测性能。DINO-X采用与Grounding DINO 1.5相同的基于Transformer的编码器-解码器架构，旨在实现开放世界物体理解的对象级表示。为了简化长尾物体检测，DINO-X扩展了输入选项，支持文本提示、视觉提示和自定义提示，甚至可以实现无提示的开放世界检测。通过在超过1亿个高质量样本的Grounding-100M数据集上进行预训练，DINO-X能够同时支持多种物体感知和理解任务，实验结果显示其在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2411.14384', 'title': 'Baking Gaussian Splatting into Diffusion Denoiser for Fast and Scalable Single-stage Image-to-3D Generation', 'url': 'https://huggingface.co/papers/2411.14384', 'abstract': 'Existing feed-forward image-to-3D methods mainly rely on 2D multi-view diffusion models that cannot guarantee 3D consistency. These methods easily collapse when changing the prompt view direction and mainly handle object-centric prompt images. In this paper, we propose a novel single-stage 3D diffusion model, DiffusionGS, for object and scene generation from a single view. DiffusionGS directly outputs 3D Gaussian point clouds at each timestep to enforce view consistency and allow the model to generate robustly given prompt views of any directions, beyond object-centric inputs. Plus, to improve the capability and generalization ability of DiffusionGS, we scale up 3D training data by developing a scene-object mixed training strategy. Experiments show that our method enjoys better generation quality (2.20 dB higher in PSNR and 23.25 lower in FID) and over 5x faster speed (~6s on an A100 GPU) than SOTA methods. The user study and text-to-3D applications also reveals the practical values of our method. Our Project page at https://caiyuanhao1998.github.io/project/DiffusionGS/ shows the video and interactive generation results.', 'score': 2, 'issue_id': 733, 'pub_date': '2024-11-21', 'pub_date_card': {'ru': '21 ноября', 'en': 'November 21', 'zh': '11月21日'}, 'hash': '1f0661cca4948898', 'authors': ['Yuanhao Cai', 'He Zhang', 'Kai Zhang', 'Yixun Liang', 'Mengwei Ren', 'Fujun Luan', 'Qing Liu', 'Soo Ye Kim', 'Jianming Zhang', 'Zhifei Zhang', 'Yuqian Zhou', 'Zhe Lin', 'Alan Yuille'], 'affiliations': ['Adobe Research', 'Hong Kong University of Science and Technology', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2411.14384.jpg', 'data': {'categories': ['#3d', '#diffusion'], 'emoji': '🌟', 'ru': {'title': 'Революция в 3D-генерации: от 2D к реалистичным трехмерным сценам', 'desc': 'DiffusionGS - это новая модель генерации трехмерных объектов и сцен по одному изображению. В отличие от существующих методов, она напрямую генерирует 3D облака гауссовых точек, обеспечивая согласованность с разных ракурсов. Модель обучается на смешанном наборе данных объектов и сцен, что улучшает ее обобщающую способность. Эксперименты показывают, что DiffusionGS превосходит существующие методы по качеству и скорости генерации.'}, 'en': {'title': 'Revolutionizing 3D Generation with DiffusionGS', 'desc': 'This paper introduces DiffusionGS, a new single-stage 3D diffusion model designed to generate 3D representations from a single 2D view. Unlike existing methods that struggle with 3D consistency and are limited to object-centric images, DiffusionGS produces 3D Gaussian point clouds that maintain view consistency across various prompt directions. The model enhances its performance by utilizing a scene-object mixed training strategy, which increases the diversity of the training data. Experimental results demonstrate that DiffusionGS achieves superior generation quality and speed compared to state-of-the-art methods, making it a valuable tool for text-to-3D applications.'}, 'zh': {'title': 'DiffusionGS：从单视角生成一致的3D场景', 'desc': '现有的前馈图像到3D方法主要依赖于2D多视角扩散模型，这些模型无法保证3D一致性。我们提出了一种新颖的单阶段3D扩散模型DiffusionGS，可以从单一视角生成物体和场景。DiffusionGS在每个时间步直接输出3D高斯点云，以确保视角一致性，并能够根据任何方向的提示视图进行稳健生成。通过开发场景-物体混合训练策略，我们扩大了3D训练数据，提高了DiffusionGS的能力和泛化能力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment', '#architecture (5)', '#audio', '#benchmark (5)', '#cv (3)', '#data (2)', '#dataset (5)', '#diffusion (3)', '#ethics', '#games (1)', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (4)', '#open_source (4)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (10)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${title}</p>
                            <p class="pub-date">📝 ${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <img class="article-pdf-title-img" src="https://hfday.ru/${item['pdf_title_img']}" />
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-11-24 12:40',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-11-24 12:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-11-24 12:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    