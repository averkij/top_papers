
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. December 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 декабря</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-16.html">⬅️ <span id="prev-date">16.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-18.html">➡️ <span id="next-date">18.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 декабря', 'en': 'December 17', 'zh': '12月17日'};
        let feedDateNext = {'ru': '18.12', 'en': '12/18', 'zh': '12月18日'};
        let feedDatePrev = {'ru': '16.12', 'en': '12/16', 'zh': '12月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.09871', 'title': 'Byte Latent Transformer: Patches Scale Better Than Tokens', 'url': 'https://huggingface.co/papers/2412.09871', 'abstract': 'We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.', 'score': 24, 'issue_id': 1164, 'pub_date': '2024-12-13', 'pub_date_card': {'ru': '13 декабря', 'en': 'December 13', 'zh': '12月13日'}, 'hash': '1239257bd35bfa00', 'authors': ['Artidoro Pagnoni', 'Ram Pasunuru', 'Pedro Rodriguez', 'John Nguyen', 'Benjamin Muller', 'Margaret Li', 'Chunting Zhou', 'Lili Yu', 'Jason Weston', 'Luke Zettlemoyer', 'Gargi Ghosh', 'Mike Lewis', 'Ari Holtzman', 'Srinivasan Iyer'], 'affiliations': ['FAIR at Meta', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2412.09871.jpg', 'data': {'categories': ['#training', '#optimization', '#long_context', '#reasoning', '#inference', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективные языковые модели на уровне байтов', 'desc': 'В статье представлена новая архитектура языковой модели на уровне байтов - Byte Latent Transformer (BLT). BLT кодирует байты в динамически изменяемые патчи, которые служат основными единицами вычислений. Сегментация патчей основана на энтропии следующего байта, что позволяет выделять больше вычислительных ресурсов там, где это необходимо. Исследование показало, что BLT может масштабироваться до 8 миллиардов параметров и 4 триллионов обучающих байтов, демонстрируя эффективность и улучшенную генерализацию по сравнению с моделями на основе токенизации.'}, 'en': {'title': 'Revolutionizing Efficiency with Byte-Level Transformers', 'desc': 'The Byte Latent Transformer (BLT) is a novel architecture for large language models (LLMs) that operates at the byte level, achieving performance comparable to traditional tokenization methods while enhancing efficiency and robustness. It utilizes dynamically sized patches to encode bytes, adjusting the size based on the complexity of the data, which allows for more effective use of computational resources. The study showcases the scalability of byte-level models, demonstrating significant improvements in both training and inference efficiency, particularly in reasoning and generalization tasks. Overall, BLT outperforms tokenization-based models by optimizing patch and model size simultaneously, leading to better performance at fixed inference costs.'}, 'zh': {'title': '字节潜在变换器：高效扩展的新选择', 'desc': '本文介绍了一种新的字节级大语言模型架构，称为字节潜在变换器（BLT）。BLT通过动态大小的补丁编码字节，作为计算的主要单位，从而在推理效率和鲁棒性上显著提升。补丁的分割基于下一个字节的熵，能够在数据复杂性增加时分配更多的计算资源。研究结果表明，BLT在固定推理成本下，能够比基于标记化的模型实现更好的扩展性，同时提高了训练和推理的效率。'}}}, {'id': 'https://huggingface.co/papers/2412.09645', 'title': 'Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models', 'url': 'https://huggingface.co/papers/2412.09645', 'abstract': "Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.", 'score': 23, 'issue_id': 1161, 'pub_date': '2024-12-10', 'pub_date_card': {'ru': '10 декабря', 'en': 'December 10', 'zh': '12月10日'}, 'hash': '69f7aa2abe9671ed', 'authors': ['Fan Zhang', 'Shulin Tian', 'Ziqi Huang', 'Yu Qiao', 'Ziwei Liu'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2412.09645.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#open_source', '#interpretability'], 'emoji': '🔍', 'ru': {'title': 'Эффективная оценка генеративных моделей: человекоподобный подход', 'desc': 'Статья представляет новый подход к оценке генеративных визуальных моделей, называемый Evaluation Agent. Этот метод имитирует человеческий подход, анализируя небольшое количество образцов за несколько раундов, что значительно ускоряет процесс оценки. Evaluation Agent предлагает эффективность, настраиваемость под нужды пользователя, объяснимость результатов и масштабируемость для различных моделей. Эксперименты показывают, что этот метод сокращает время оценки до 10% от традиционных подходов, сохраняя сопоставимую точность.'}, 'en': {'title': 'Efficient and Tailored Evaluation for Visual Generative Models', 'desc': 'This paper introduces the Evaluation Agent framework, designed to improve the evaluation of visual generative models like image and video generators. Traditional evaluation methods are slow and often fail to meet specific user needs, requiring extensive sampling that is computationally expensive. The Evaluation Agent mimics human evaluation by using fewer samples and providing detailed, tailored analyses, making the process more efficient and explainable. Experiments demonstrate that this framework can reduce evaluation time significantly while maintaining comparable results to existing methods.'}, 'zh': {'title': '高效评估生成模型的新方法', 'desc': '最近，视觉生成模型的进步使得高质量的图像和视频生成成为可能，应用范围广泛。然而，评估这些模型通常需要采样数百或数千张图像或视频，这使得计算过程非常耗时，尤其是对于基于扩散的模型。现有的评估方法依赖于固定的流程，忽视了用户的特定需求，并且提供的数值结果缺乏清晰的解释。为此，我们提出了评估代理框架，采用类人策略进行高效、动态的多轮评估，仅需少量样本，并提供详细的用户定制分析。'}}}, {'id': 'https://huggingface.co/papers/2412.11919', 'title': 'RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation', 'url': 'https://huggingface.co/papers/2412.11919', 'abstract': "Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose RetroLLM, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at https://github.com/sunnynexus/RetroLLM.", 'score': 23, 'issue_id': 1158, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '35265a6474f53410', 'authors': ['Xiaoxi Li', 'Jiajie Jin', 'Yujia Zhou', 'Yongkang Wu', 'Zhonghua Li', 'Qi Ye', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.11919.jpg', 'data': {'categories': ['#optimization', '#hallucinations', '#rag'], 'emoji': '🔍', 'ru': {'title': 'RetroLLM: Единая модель для точного поиска и генерации', 'desc': 'RetroLLM - это новая унифицированная модель, объединяющая поиск и генерацию в единый процесс для больших языковых моделей. Она использует ограниченное декодирование для генерации доказательств непосредственно из корпуса текстов. Модель включает иерархические ограничения FM-индекса и опережающую стратегию декодирования для повышения точности. Эксперименты показали превосходную производительность RetroLLM на задачах вопросно-ответных систем как в рамках предметной области, так и вне ее.'}, 'en': {'title': 'RetroLLM: Unifying Retrieval and Generation for Accurate Evidence Generation', 'desc': 'This paper introduces RetroLLM, a novel framework that combines retrieval and generation in large language models (LLMs) to enhance their performance and reduce hallucinations. By integrating these processes, RetroLLM allows LLMs to generate precise evidence directly from a knowledge corpus while minimizing unnecessary input tokens. The framework employs hierarchical FM-Index constraints to filter relevant documents and a forward-looking constrained decoding strategy to ensure the accuracy of generated evidence. Experimental results show that RetroLLM outperforms existing methods on various open-domain question-answering datasets, demonstrating its effectiveness in both in-domain and out-of-domain scenarios.'}, 'zh': {'title': '整合检索与生成，提升语言模型的准确性', 'desc': '大型语言模型（LLMs）在生成能力上表现出色，但常常出现幻觉现象。检索增强生成（RAG）通过引入外部知识提供了有效的解决方案，但现有方法仍存在一些局限性，如额外的检索器部署成本、从检索文本块中产生的冗余输入标记，以及检索与生成缺乏联合优化。为了解决这些问题，我们提出了RetroLLM，一个将检索与生成整合为一个统一过程的框架，使LLMs能够直接从语料库中生成细粒度证据，并进行受限解码。此外，我们引入了层次FM-Index约束和前瞻性受限解码策略，以提高证据生成的准确性。'}}}, {'id': 'https://huggingface.co/papers/2412.10316', 'title': 'BrushEdit: All-In-One Image Inpainting and Editing', 'url': 'https://huggingface.co/papers/2412.10316', 'abstract': 'Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.', 'score': 20, 'issue_id': 1162, 'pub_date': '2024-12-13', 'pub_date_card': {'ru': '13 декабря', 'en': 'December 13', 'zh': '12月13日'}, 'hash': 'd8789f3e683b7c6b', 'authors': ['Yaowei Li', 'Yuxuan Bian', 'Xuan Ju', 'Zhaoyang Zhang', 'Ying Shan', 'Yuexian Zou', 'Qiang Xu'], 'affiliations': ['ARC Lab, Tencent PCG', 'Peking University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.10316.jpg', 'data': {'categories': ['#interpretability', '#cv', '#diffusion', '#multimodal', '#agents'], 'emoji': '🖌️', 'ru': {'title': 'BrushEdit: Интеллектуальное редактирование изображений с помощью инструкций и инпейнтинга', 'desc': 'Статья представляет BrushEdit - новую парадигму редактирования изображений на основе инструкций и инпейнтинга. Она использует мультимодальные большие языковые модели (MLLM) и модели инпейнтинга изображений для автономного и интерактивного редактирования. Система включает классификацию категорий редактирования, идентификацию основных объектов, получение масок и инпейнтинг области редактирования. Эксперименты показывают превосходную производительность по семи метрикам, включая сохранение области маски и согласованность эффекта редактирования.'}, 'en': {'title': 'Empowering Image Editing with Interactive Instruction and Inpainting', 'desc': 'This paper introduces BrushEdit, a new method for image editing that combines instruction-based and inpainting techniques. It addresses the limitations of current methods by allowing users to interactively specify editing regions and intensity without being constrained to black-box operations. The system uses multimodal large language models (MLLMs) alongside a dual-branch image inpainting model to classify editing categories, identify main objects, and create masks for editing. Experimental results demonstrate that BrushEdit outperforms existing methods in preserving mask regions and maintaining coherent editing effects.'}, 'zh': {'title': '自由形式的智能图像编辑新方法', 'desc': '本论文提出了一种新的图像编辑方法，称为BrushEdit，旨在解决现有图像编辑技术的局限性。通过结合多模态大语言模型（MLLMs）和图像修复模型，该方法实现了用户友好的自由形式指令编辑。BrushEdit能够自动识别编辑类别、主要对象，并获取编辑区域的掩码，从而支持更大幅度的图像修改。实验结果表明，该框架在多个指标上表现优越，能够有效保持掩码区域和编辑效果的一致性。'}}}, {'id': 'https://huggingface.co/papers/2412.11815', 'title': 'ColorFlow: Retrieval-Augmented Image Sequence Colorization', 'url': 'https://huggingface.co/papers/2412.11815', 'abstract': 'Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/.', 'score': 18, 'issue_id': 1161, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': 'de381dc70d0db48f', 'authors': ['Junhao Zhuang', 'Xuan Ju', 'Zhaoyang Zhang', 'Yong Liu', 'Shiyi Zhang', 'Chun Yuan', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.11815.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#rag', '#cv', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'ColorFlow: Революция в автоматической колоризации изображений с сохранением идентичности', 'desc': 'ColorFlow - это новая трёхэтапная система на основе диффузионных моделей для автоматической колоризации последовательностей чёрно-белых изображений. Она использует механизм самовнимания для извлечения цветовой идентичности и её сохранения при колоризации. Система превосходит существующие модели по нескольким метрикам, устанавливая новый стандарт в последовательной колоризации изображений. ColorFlow потенциально может принести пользу индустрии искусства, особенно в области колоризации мультфильмов и комиксов.'}, 'en': {'title': 'Revolutionizing Image Sequence Colorization with ColorFlow', 'desc': 'This paper presents ColorFlow, a novel framework designed for colorizing black-and-white image sequences while maintaining the identity of characters and objects. It utilizes a three-stage diffusion model that enhances controllability and consistency, addressing limitations found in previous methods. The framework features a dual-branch architecture that separates color identity extraction from the colorization process, allowing for effective use of color references. The authors also introduce ColorFlow-Bench, a benchmark for evaluating colorization performance, demonstrating that their approach significantly outperforms existing techniques in the field.'}, 'zh': {'title': 'ColorFlow：图像序列上色的新标准', 'desc': '本文提出了一种名为ColorFlow的三阶段扩散模型框架，旨在自动为黑白图像序列上色，同时保持角色和物体的身份一致性。该方法通过检索增强的上色管道，利用相关的颜色参考进行图像上色，避免了现有方法中需要逐个身份微调的复杂性。ColorFlow采用双分支设计，一方面提取颜色身份，另一方面进行上色，充分利用了扩散模型的优势。通过ColorFlow-Bench基准测试，结果表明该模型在多个指标上优于现有模型，为图像序列上色设定了新标准，可能对艺术行业带来积极影响。'}}}, {'id': 'https://huggingface.co/papers/2412.12095', 'title': 'Causal Diffusion Transformers for Generative Modeling', 'url': 'https://huggingface.co/papers/2412.12095', 'abstract': "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.", 'score': 15, 'issue_id': 1161, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': 'e5107a05a397194f', 'authors': ['Chaorui Deng', 'Deyao Zh', 'Kunchang Li', 'Shi Guan', 'Haoqi Fan'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.12095.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#dataset', '#benchmark', '#training', '#multimodal'], 'emoji': '🔮', 'ru': {'title': 'CausalFusion: Объединение авторегрессии и диффузии для мультимодального генеративного ИИ', 'desc': 'Статья представляет Causal Diffusion - авторегрессионный аналог моделей диффузии для прогнозирования следующих токенов. Предложен CausalFusion - декодер-трансформер, который факторизует данные по токенам и уровням шума диффузии. Модель достигает передовых результатов в генерации изображений на ImageNet, сохраняя преимущества авторегрессии. CausalFusion демонстрирует мультимодальные возможности в совместной генерации изображений и подписей, а также способность к zero-shot манипуляциям изображениями.'}, 'en': {'title': 'CausalFusion: Bridging Autoregressive and Diffusion Models for Enhanced Multimodal Generation', 'desc': 'Causal Diffusion is a new approach that combines autoregressive (AR) models with diffusion models for predicting the next token in sequences. This method allows for better performance by introducing sequential factorization, which helps in transitioning smoothly between AR and diffusion generation modes. The proposed model, CausalFusion, is a transformer that effectively handles both discrete and continuous data, achieving top results in image generation tasks. Additionally, it demonstrates the ability to generate images and captions together, as well as perform image manipulations without prior training on specific tasks.'}, 'zh': {'title': '因果扩散：自回归与扩散模型的完美结合', 'desc': '我们提出了因果扩散（Causal Diffusion），作为扩散模型的自回归（AR）对应物。它是一种友好于离散和连续模式的下一个标记预测框架，并与现有的下一个标记预测模型（如LLaMA和GPT）兼容。通过在扩散模型中引入序列因子化，我们显著提高了性能，并实现了自回归和扩散生成模式之间的平滑过渡。我们还展示了因果融合（CausalFusion）在多模态能力方面的应用，包括联合图像生成和标题生成，以及零-shot上下文图像操作的能力。'}}}, {'id': 'https://huggingface.co/papers/2412.11231', 'title': 'Smaller Language Models Are Better Instruction Evolvers', 'url': 'https://huggingface.co/papers/2412.11231', 'abstract': 'Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess a broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction Complex-Aware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: https://github.com/HypherX/Evolution-Analysis{https://github.com/HypherX/Evolution-Analysis}', 'score': 14, 'issue_id': 1159, 'pub_date': '2024-12-15', 'pub_date_card': {'ru': '15 декабря', 'en': 'December 15', 'zh': '12月15日'}, 'hash': '0fd693d18eb484a1', 'authors': ['Tingfeng Hui', 'Lulu Zhao', 'Guanting Dong', 'Yaqi Zhang', 'Hua Zhou', 'Sen Su'], 'affiliations': ['Beijing Academy of Artificial Intelligence, BAAI, Beijing, China', 'Beijing University of Posts and Telecommunications, Beijing, China', 'Renmin University of China, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.11231.jpg', 'data': {'categories': ['#small_models', '#training', '#open_source', '#alignment', '#optimization'], 'emoji': '🔬', 'ru': {'title': 'Малые модели превосходят гигантов в создании инструкций', 'desc': 'Статья исследует потенциал малых языковых моделей (SLM) в эволюции инструкций для обучения больших языковых моделей. Авторы обнаружили, что SLM могут создавать более эффективные инструкции, чем крупные модели (LLM), благодаря более широкому пространству выходных данных. Они предложили новую метрику IC-IFD для более точной оценки эффективности инструкций, учитывающую их сложность. Результаты ставят под сомнение распространенное предположение о превосходстве LLM в этой задаче.'}, 'en': {'title': 'Unlocking the Power of Smaller Models in Instruction Tuning', 'desc': 'This paper investigates the effectiveness of smaller language models (SLMs) in instruction tuning, challenging the belief that larger models like GPT-4 are always superior. The authors conduct experiments showing that SLMs can generate more effective and diverse instructions compared to their larger counterparts. They highlight that SLMs have a wider output space, allowing for richer instruction variants during the evolution process. Additionally, the paper introduces a new metric, Instruction Complex-Aware IFD (IC-IFD), to better assess the impact of instruction complexity on model performance.'}, 'zh': {'title': '小型语言模型的指令调优潜力', 'desc': '本研究探讨了指令调优在小型语言模型（SLMs）中的潜力，挑战了大型语言模型（LLMs）在指令演变中的主导地位。实验表明，SLMs能够生成比LLMs更有效的指令，且在指令演变过程中具有更广泛的输出空间。我们还发现现有的评估指标未能充分考虑指令的影响，因此提出了指令复杂度感知的IFD（IC-IFD）方法，以更准确地评估指令数据的有效性。通过这项研究，我们希望推动对小型语言模型的理解和应用。'}}}, {'id': 'https://huggingface.co/papers/2412.12083', 'title': 'IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations', 'url': 'https://huggingface.co/papers/2412.12083', 'abstract': 'Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation.', 'score': 10, 'issue_id': 1158, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '333271d63ddd2102', 'authors': ['Zhibing Li', 'Tong Wu', 'Jing Tan', 'Mengchen Zhang', 'Jiaqi Wang', 'Dahua Lin'], 'affiliations': ['Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.12083.jpg', 'data': {'categories': ['#optimization', '#3d', '#cv', '#dataset', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Декомпозиция изображений на геометрию и материалы с помощью диффузионной модели', 'desc': 'IDArb - это модель на основе диффузии для декомпозиции изображений на внутренние свойства объектов. Она позволяет точно оценивать нормали поверхности и свойства материалов по нескольким изображениям с разным освещением. Модель использует новый модуль межвидового и междоменного внимания, а также стратегию обучения с аугментацией освещения. Авторы также представили новый набор данных ARB-Objaverse для обучения модели.'}, 'en': {'title': 'Revolutionizing 3D Content Creation with IDArb', 'desc': "This paper presents IDArb, a diffusion-based model that addresses the challenge of intrinsic decomposition from multiple images with varying lighting conditions. Traditional methods are slow and struggle with ambiguities, while learning-based approaches often lack multi-view consistency. IDArb utilizes a novel cross-view, cross-domain attention mechanism and an illumination-augmented training strategy to achieve accurate estimations of surface normals and material properties. The introduction of the ARB-Objaverse dataset further enhances the model's training, leading to superior performance in various applications such as relighting and 3D reconstruction."}, 'zh': {'title': 'IDArb：多视角一致的内在分解新方法', 'desc': '本论文提出了一种名为IDArb的扩散模型，旨在从多视角图像中进行内在分解，捕捉几何和材料信息。与传统的优化方法相比，IDArb能够在不同光照条件下实现准确且多视角一致的表面法线和材料属性估计。我们还引入了一个新的数据集ARB-Objaverse，提供了大规模的多视角内在数据，支持模型的稳健训练。实验结果表明，IDArb在定性和定量上均优于现有的最先进方法，具有广泛的应用潜力。'}}}, {'id': 'https://huggingface.co/papers/2412.11605', 'title': 'SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models', 'url': 'https://huggingface.co/papers/2412.11605', 'abstract': 'Instruction-following is a fundamental capability of language models, requiring the model to recognize even the most subtle requirements in the instructions and accurately reflect them in its output. Such an ability is well-suited for and often optimized by preference learning. However, existing methods often directly sample multiple independent responses from the model when creating preference pairs. Such practice can introduce content variations irrelevant to whether the instruction is precisely followed (e.g., different expressions about the same semantic), interfering with the goal of teaching models to recognize the key differences that lead to improved instruction following. In light of this, we introduce SPaR, a self-play framework integrating tree-search self-refinement to yield valid and comparable preference pairs free from distractions. By playing against itself, an LLM employs a tree-search strategy to refine its previous responses with respect to the instruction while minimizing unnecessary variations. Our experiments show that a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses GPT-4-Turbo on the IFEval benchmark without losing general capabilities. Furthermore, SPaR demonstrates promising scalability and transferability, greatly enhancing models like GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree search would impact model performance. Our code and data are publicly available at https://github.com/thu-coai/SPaR.', 'score': 9, 'issue_id': 1159, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '6e4d876c9f198e44', 'authors': ['Jiale Cheng', 'Xiao Liu', 'Cunxiang Wang', 'Xiaotao Gu', 'Yida Lu', 'Dan Zhang', 'Yuxiao Dong', 'Jie Tang', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University', 'The Knowledge Engineering Group (KEG), Tsinghua University', 'Zhipu AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.11605.jpg', 'data': {'categories': ['#transfer_learning', '#benchmark', '#optimization', '#training', '#open_source', '#rlhf', '#alignment', '#architecture'], 'emoji': '🎯', 'ru': {'title': 'SPaR: точное следование инструкциям через самоигру', 'desc': 'Статья описывает новый метод SPaR для улучшения способности языковых моделей следовать инструкциям. В отличие от существующих подходов, SPaR использует самоигру и древовидный поиск для создания более релевантных пар предпочтений. Эксперименты показывают, что модель LLaMA3-8B, обученная с помощью SPaR, превосходит GPT-4-Turbo на бенчмарке IFEval. Метод также демонстрирует хорошую масштабируемость и переносимость на другие модели.'}, 'en': {'title': 'Enhancing Instruction Following with SPaR: A Self-Play Approach', 'desc': 'This paper presents SPaR, a self-play framework designed to improve instruction-following capabilities in language models. It utilizes a tree-search strategy to refine responses, ensuring that preference pairs are valid and comparable without introducing irrelevant content variations. By training a LLaMA3-8B model with SPaR, the authors demonstrate that it outperforms GPT-4-Turbo on the IFEval benchmark while maintaining general performance. The framework also shows potential for scalability and transferability to larger models like GLM-4-9B and LLaMA3-70B, with insights on how tree search impacts inference performance.'}, 'zh': {'title': '自我对弈提升指令遵循能力', 'desc': '本文提出了一种名为SPaR的自我对弈框架，旨在提高语言模型对指令的遵循能力。通过树搜索自我精炼，SPaR能够生成有效且可比较的偏好对，避免了无关内容的干扰。实验表明，经过SPaR训练的LLaMA3-8B模型在IFEval基准测试中超越了GPT-4-Turbo，同时保持了其通用能力。SPaR还展示了良好的可扩展性和迁移性，显著提升了GLM-4-9B和LLaMA3-70B等模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2412.12091', 'title': 'Wonderland: Navigating 3D Scenes from a Single Image', 'url': 'https://huggingface.co/papers/2412.12091', 'abstract': 'This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.', 'score': 8, 'issue_id': 1161, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '56eac228e6d5c48b', 'authors': ['Hanwen Liang', 'Junli Cao', 'Vidit Goel', 'Guocheng Qian', 'Sergei Korolev', 'Demetri Terzopoulos', 'Konstantinos N. Plataniotis', 'Sergey Tulyakov', 'Jian Ren'], 'affiliations': ['Snap Inc.', 'University of California, Los Angeles', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2412.12091.jpg', 'data': {'categories': ['#3d', '#diffusion', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'От 2D к 3D: революция в реконструкции сцен с помощью латентных пространств', 'desc': 'Статья представляет новый подход к созданию качественных 3D-сцен из одного изображения. Авторы предлагают использовать латентное пространство видео-диффузионной модели для предсказания 3D Gaussian Splatting. Модель обучается на латентных представлениях видео, что позволяет генерировать согласованные многоракурсные данные. Результаты показывают значительное улучшение качества 3D-реконструкции по сравнению с существующими методами, особенно для изображений вне обучающей выборки.'}, 'en': {'title': 'Transforming Single Images into Rich 3D Worlds Efficiently!', 'desc': 'This paper presents a new approach to generating high-quality 3D scenes from a single image, addressing limitations of existing methods that often require multiple views and extensive optimization. The authors introduce a large-scale reconstruction model that leverages latents from a video diffusion model, enabling the prediction of 3D Gaussian Splattings in a fast, feed-forward manner. By utilizing a video diffusion model that generates videos along specific camera paths, the method captures multi-view information while ensuring 3D consistency. The proposed model shows significant improvements in generating 3D scenes, especially for images not seen during training, marking a breakthrough in single-view 3D reconstruction.'}, 'zh': {'title': '高效生成高质量3D场景的新方法', 'desc': '本文探讨了如何从单张任意图像高效创建高质量、广范围的3D场景。现有方法面临多视图数据需求、每个场景优化耗时、背景视觉质量低以及未见区域重建失真等限制。我们提出了一种新颖的管道，利用视频扩散模型的潜在特征预测3D高斯点云，从而克服这些限制。通过在视频潜在空间上训练3D重建模型，我们实现了高效生成高质量、广范围的通用3D场景。'}}}, {'id': 'https://huggingface.co/papers/2412.11279', 'title': 'VividFace: A Diffusion-Based Hybrid Framework for High-Fidelity Video Face Swapping', 'url': 'https://huggingface.co/papers/2412.11279', 'abstract': 'Video face swapping is becoming increasingly popular across various applications, yet existing methods primarily focus on static images and struggle with video face swapping because of temporal consistency and complex scenarios. In this paper, we present the first diffusion-based framework specifically designed for video face swapping. Our approach introduces a novel image-video hybrid training framework that leverages both abundant static image data and temporal video sequences, addressing the inherent limitations of video-only training. The framework incorporates a specially designed diffusion model coupled with a VidFaceVAE that effectively processes both types of data to better maintain temporal coherence of the generated videos. To further disentangle identity and pose features, we construct the Attribute-Identity Disentanglement Triplet (AIDT) Dataset, where each triplet has three face images, with two images sharing the same pose and two sharing the same identity. Enhanced with a comprehensive occlusion augmentation, this dataset also improves robustness against occlusions. Additionally, we integrate 3D reconstruction techniques as input conditioning to our network for handling large pose variations. Extensive experiments demonstrate that our framework achieves superior performance in identity preservation, temporal consistency, and visual quality compared to existing methods, while requiring fewer inference steps. Our approach effectively mitigates key challenges in video face swapping, including temporal flickering, identity preservation, and robustness to occlusions and pose variations.', 'score': 7, 'issue_id': 1171, 'pub_date': '2024-12-15', 'pub_date_card': {'ru': '15 декабря', 'en': 'December 15', 'zh': '12月15日'}, 'hash': 'a26672151ad54307', 'authors': ['Hao Shao', 'Shulun Wang', 'Yang Zhou', 'Guanglu Song', 'Dailan He', 'Shuo Qin', 'Zhuofan Zong', 'Bingqi Ma', 'Yu Liu', 'Hongsheng Li'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'SenseTime Research'], 'pdf_title_img': 'assets/pdf/title_img/2412.11279.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#synthetic', '#video', '#cv', '#3d'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная замена лиц в видео с помощью диффузионных моделей', 'desc': 'Авторы представляют первую систему на основе диффузионных моделей для замены лиц в видео. Они используют гибридный подход обучения на статичных изображениях и видеопоследовательностях, что позволяет улучшить временную согласованность генерируемых видео. Для улучшения разделения признаков идентичности и позы создан специальный набор данных AIDT. Система также использует методы 3D-реконструкции для обработки больших вариаций поз и демонстрирует превосходные результаты по сравнению с существующими подходами.'}, 'en': {'title': 'Revolutionizing Video Face Swapping with Diffusion Models', 'desc': 'This paper introduces a new method for video face swapping that uses a diffusion-based framework, which is a first in this area. The authors combine static image data with video sequences to improve the quality and consistency of the swapped faces over time. They also create a unique dataset called the Attribute-Identity Disentanglement Triplet (AIDT) to help the model learn to separate identity and pose features effectively. The results show that their approach outperforms existing methods in maintaining identity, reducing flickering, and handling occlusions and pose changes.'}, 'zh': {'title': '基于扩散模型的视频换脸新方法', 'desc': '本论文提出了一种基于扩散模型的视频换脸框架，专门解决视频换脸中的时间一致性和复杂场景问题。我们引入了一种图像-视频混合训练框架，利用静态图像数据和时间序列视频，克服了仅使用视频训练的局限性。通过构建属性-身份解耦三元组数据集（AIDT），我们有效地分离了身份和姿态特征，并增强了对遮挡的鲁棒性。实验结果表明，我们的方法在身份保留、时间一致性和视觉质量方面优于现有方法，同时推理步骤更少。'}}}, {'id': 'https://huggingface.co/papers/2412.12094', 'title': 'SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator', 'url': 'https://huggingface.co/papers/2412.12094', 'abstract': "Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.", 'score': 6, 'issue_id': 1167, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '19a5b778582814f2', 'authors': ['Guoxuan Chen', 'Han Shi', 'Jiawei Li', 'Yihang Gao', 'Xiaozhe Ren', 'Yimeng Chen', 'Xin Jiang', 'Zhenguo Li', 'Weiyang Liu', 'Chao Huang'], 'affiliations': ['Center of Excellence for Generative AI, KAUST', 'Huawei Noahs Ark Lab', 'Max Planck Institute for Intelligent Systems, Tubingen', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.12094.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#benchmark', '#inference', '#long_context'], 'emoji': '🚀', 'ru': {'title': 'SepLLM: Ускорение больших языковых моделей без потери качества', 'desc': 'Эта статья представляет SepLLM - новый фреймворк для ускорения работы больших языковых моделей (LLM). Авторы обнаружили, что специальные токены-разделители играют непропорционально большую роль в формировании внимания модели. Основываясь на этом наблюдении, SepLLM сжимает сегменты между разделителями, что позволяет значительно уменьшить размер KV-кэша без существенной потери качества. Эксперименты показали, что SepLLM может обрабатывать последовательности длиной до 4 миллионов токенов, сохраняя при этом высокое качество языкового моделирования.'}, 'en': {'title': 'Accelerating LLMs by Compressing Attention with SepLLM', 'desc': 'This paper presents SepLLM, a framework designed to enhance the efficiency of Large Language Models (LLMs) by addressing their computational challenges. The authors discovered that certain special tokens, which appear to be meaningless, actually hold significant weight in the attention mechanism, allowing for the compression of information between these tokens. By condensing this information into the separator tokens, SepLLM reduces the number of tokens processed, leading to faster inference times without losing important data. Experimental results show that SepLLM can significantly decrease the key-value (KV) cache size while still performing well on various benchmarks, even in scenarios with extremely long sequences.'}, 'zh': {'title': 'SepLLM：加速推理的高效框架', 'desc': '大型语言模型（LLMs）在自然语言处理任务中表现出色，但其庞大的规模带来了计算需求和推理速度的挑战。我们发现某些看似无意义的特殊标记（如分隔符）在注意力分数中占据了不成比例的比重。基于这一观察，我们提出了SepLLM框架，通过压缩分隔符之间的信息并消除冗余标记，从而加速推理过程。实验结果表明，SepLLM在多个设置下均表现出色，尤其是在流式处理时能够有效处理超过400万标记的序列。'}}}, {'id': 'https://huggingface.co/papers/2412.11258', 'title': 'GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs', 'url': 'https://huggingface.co/papers/2412.11258', 'abstract': 'Estimating physical properties for visual data is a crucial task in computer vision, graphics, and robotics, underpinning applications such as augmented reality, physical simulation, and robotic grasping. However, this area remains under-explored due to the inherent ambiguities in physical property estimation. To address these challenges, we introduce GaussianProperty, a training-free framework that assigns physical properties of materials to 3D Gaussians. Specifically, we integrate the segmentation capability of SAM with the recognition capability of GPT-4V(ision) to formulate a global-local physical property reasoning module for 2D images. Then we project the physical properties from multi-view 2D images to 3D Gaussians using a voting strategy. We demonstrate that 3D Gaussians with physical property annotations enable applications in physics-based dynamic simulation and robotic grasping. For physics-based dynamic simulation, we leverage the Material Point Method (MPM) for realistic dynamic simulation. For robot grasping, we develop a grasping force prediction strategy that estimates a safe force range required for object grasping based on the estimated physical properties. Extensive experiments on material segmentation, physics-based dynamic simulation, and robotic grasping validate the effectiveness of our proposed method, highlighting its crucial role in understanding physical properties from visual data. Online demo, code, more cases and annotated datasets are available on https://Gaussian-Property.github.io{this https URL}.', 'score': 6, 'issue_id': 1158, 'pub_date': '2024-12-15', 'pub_date_card': {'ru': '15 декабря', 'en': 'December 15', 'zh': '12月15日'}, 'hash': 'b8115a0ffb05a0df', 'authors': ['Xinli Xu', 'Wenhang Ge', 'Dicong Qiu', 'ZhiFei Chen', 'Dongyu Yan', 'Zhuoyun Liu', 'Haoyu Zhao', 'Hanfeng Zhao', 'Shunsi Zhang', 'Junwei Liang', 'Ying-Cong Chen'], 'affiliations': ['HKUST', 'HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2412.11258.jpg', 'data': {'categories': ['#3d', '#cv', '#robotics'], 'emoji': '🧪', 'ru': {'title': 'GaussianProperty: Физические свойства в 3D без обучения', 'desc': 'GaussianProperty - это безтренировочный фреймворк для присвоения физических свойств материалов 3D гауссианам. Он использует сегментационные возможности SAM и распознавание GPT-4V для анализа физических свойств на 2D изображениях, а затем проецирует их на 3D гауссианы. Фреймворк применяется для физического моделирования с использованием метода материальной точки (MPM) и прогнозирования силы захвата в робототехнике. Эксперименты подтверждают эффективность метода в понимании физических свойств из визуальных данных.'}, 'en': {'title': 'Revolutionizing Physical Property Estimation with GaussianProperty', 'desc': 'This paper presents GaussianProperty, a novel framework designed to estimate physical properties of materials from visual data without the need for extensive training. It combines the segmentation capabilities of SAM with the recognition abilities of GPT-4V(ision) to create a reasoning module that works on 2D images. The framework projects these properties into 3D Gaussians using a voting strategy, facilitating applications in dynamic simulation and robotic grasping. The authors demonstrate the effectiveness of their approach through experiments, showcasing its potential in enhancing physics-based simulations and improving robotic interactions with objects.'}, 'zh': {'title': '从视觉数据中提取物理属性的创新方法', 'desc': '本文介绍了一种名为GaussianProperty的框架，用于从视觉数据中估计物理属性。该方法结合了SAM的分割能力和GPT-4V(ision)的识别能力，形成了一个针对2D图像的全局-局部物理属性推理模块。通过投票策略，我们将多视角2D图像的物理属性投影到3D高斯分布上。实验结果表明，该方法在物理基础动态仿真和机器人抓取等应用中具有显著效果。'}}}, {'id': 'https://huggingface.co/papers/2412.11834', 'title': 'Wonderful Matrices: Combining for a More Efficient and Effective Foundation Model Architecture', 'url': 'https://huggingface.co/papers/2412.11834', 'abstract': 'In order to make the foundation model more efficient and effective, our idea is combining sequence transformation and state transformation. First, we prove the availability of rotary position embedding in the state space duality algorithm, which reduces the perplexity of the hybrid quadratic causal self-attention and state space duality by more than 4%, to ensure that the combining sequence transformation unifies position encoding. Second, we propose dynamic mask attention, which maintains 100% accuracy in the more challenging multi-query associative recall task, improving by more than 150% compared to quadratic causal self-attention and state space duality, to ensure that the combining sequence transformation selectively filters relevant information. Third, we design cross domain mixture of experts, which makes the computational speed of expert retrieval with more than 1024 experts 8 to 10 times faster than the mixture of experts, to ensure that the combining state transformation quickly retrieval mixture. Finally, we summarize these matrix algorithms that can form the foundation model: Wonderful Matrices, which can be a competitor to popular model architectures.', 'score': 4, 'issue_id': 1159, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '7090d4cb4588f236', 'authors': ['Jingze Shi', 'Bingheng Wu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.11834.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Чудесные матрицы: новый подход к архитектуре фундаментальных моделей', 'desc': 'Статья представляет новый подход к улучшению фундаментальных моделей машинного обучения, объединяя преобразования последовательностей и состояний. Авторы доказывают эффективность ротационного позиционного кодирования в алгоритме дуальности пространства состояний, что снижает перплексию модели. Они предлагают динамическое маскирующее внимание, значительно улучшающее точность в задачах ассоциативного поиска. Кроме того, разработан метод кросс-доменной смеси экспертов, ускоряющий вычисления при большом количестве экспертов.'}, 'en': {'title': 'Enhancing Foundation Models with Efficient Transformations', 'desc': 'This paper introduces a novel approach to enhance foundation models by integrating sequence transformation and state transformation techniques. It demonstrates the effectiveness of rotary position embedding in reducing perplexity in hybrid attention mechanisms, leading to improved performance. The authors also present dynamic mask attention, which significantly boosts accuracy in complex recall tasks while filtering relevant information efficiently. Additionally, they propose a cross-domain mixture of experts that accelerates expert retrieval, making it a competitive alternative to existing model architectures.'}, 'zh': {'title': '结合序列与状态变换，提升基础模型效率', 'desc': '本文提出了一种结合序列变换和状态变换的方法，以提高基础模型的效率和效果。我们证明了旋转位置嵌入在状态空间对偶算法中的有效性，显著降低了混合二次因果自注意力和状态空间对偶的困惑度。我们还提出了动态掩码注意力，在多查询关联回忆任务中保持100%的准确率，提升了150%以上。最后，我们设计了跨域专家混合，使得专家检索的计算速度比传统方法快8到10倍，形成了具有竞争力的基础模型：奇妙矩阵。'}}}, {'id': 'https://huggingface.co/papers/2412.11586', 'title': 'StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors', 'url': 'https://huggingface.co/papers/2412.11586', 'abstract': 'While haircut indicates distinct personality, existing avatar generation methods fail to model practical hair due to the general or entangled representation. We propose StrandHead, a novel text to 3D head avatar generation method capable of generating disentangled 3D hair with strand representation. Without using 3D data for supervision, we demonstrate that realistic hair strands can be generated from prompts by distilling 2D generative diffusion models. To this end, we propose a series of reliable priors on shape initialization, geometric primitives, and statistical haircut features, leading to a stable optimization and text-aligned performance. Extensive experiments show that StrandHead achieves the state-of-the-art reality and diversity of generated 3D head and hair. The generated 3D hair can also be easily implemented in the Unreal Engine for physical simulation and other applications. The code will be available at https://xiaokunsun.github.io/StrandHead.github.io.', 'score': 4, 'issue_id': 1159, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '9787d6a0befef45d', 'authors': ['Xiaokun Sun', 'Zeyu Cai', 'Zhenyu Zhang', 'Ying Tai', 'Jian Yang'], 'affiliations': ['Nanjing University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2412.11586.jpg', 'data': {'categories': ['#3d', '#open_source', '#diffusion', '#optimization'], 'emoji': '💇', 'ru': {'title': 'Реалистичные 3D-прически из текста: новый уровень генерации аватаров', 'desc': 'Статья представляет StrandHead - новый метод генерации 3D-аватаров головы с детализированными волосами на основе текстового описания. Авторы предлагают использовать дистилляцию 2D генеративных диффузионных моделей для создания реалистичных прядей волос без использования 3D-данных для обучения. Метод включает ряд надежных приоров для инициализации формы, геометрических примитивов и статистических характеристик причесок, что обеспечивает стабильную оптимизацию и соответствие текстовому описанию. Эксперименты показывают, что StrandHead достигает лучших результатов по реалистичности и разнообразию сгенерированных 3D-голов и причесок.'}, 'en': {'title': 'StrandHead: Realistic 3D Hair Generation from Text Prompts', 'desc': 'This paper introduces StrandHead, a new method for creating 3D head avatars from text descriptions, focusing on generating realistic hair. Unlike previous methods that struggle with hair representation, StrandHead uses a unique approach to model hair as individual strands, allowing for more detailed and diverse outputs. The method leverages 2D generative diffusion models without needing 3D data, employing reliable priors to ensure stable optimization and alignment with text prompts. The results show that StrandHead outperforms existing techniques in generating lifelike 3D heads and hair, making it suitable for applications like physical simulation in gaming engines.'}, 'zh': {'title': 'StrandHead：生成独特3D发型的创新方法', 'desc': '本论文提出了一种新的3D头像生成方法StrandHead，能够生成具有独立发丝表示的3D头发。该方法不依赖于3D数据进行监督，而是通过提炼2D生成扩散模型来生成逼真的发丝。我们提出了一系列可靠的先验知识，包括形状初始化、几何原语和统计发型特征，从而实现稳定的优化和与文本对齐的性能。实验结果表明，StrandHead在生成3D头部和头发的真实感和多样性方面达到了最先进的水平。'}}}, {'id': 'https://huggingface.co/papers/2412.12004', 'title': 'The Open Source Advantage in Large Language Models (LLMs)', 'url': 'https://huggingface.co/papers/2412.12004', 'abstract': 'Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their "black box" nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment.', 'score': 3, 'issue_id': 1170, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '0abc91185b16b301', 'authors': ['Jiya Manchanda', 'Laura Boettcher', 'Matheus Westphalen', 'Jasser Jasser'], 'affiliations': ['Rollins College, Winter Park'], 'pdf_title_img': 'assets/pdf/title_img/2412.12004.jpg', 'data': {'categories': ['#ethics', '#machine_translation', '#data', '#open_source', '#low_resource', '#reasoning', '#multilingual', '#architecture', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Открытость против закрытости: балансируя инновации и доступность в мире больших языковых моделей', 'desc': 'В статье рассматривается сравнение закрытых и открытых больших языковых моделей (LLM) в контексте обработки естественного языка. Закрытые модели, такие как GPT-4, лидируют по производительности, но критикуются за непрозрачность и ограничение доступа. Открытые инициативы, например LLaMA и BLOOM, фокусируются на демократизации ИИ и эффективности, сокращая разрыв в производительности. Статья подчеркивает важность гибридных подходов для будущего развития LLM, обеспечивающих доступность, конкурентоспособную производительность и этичное применение.'}, 'en': {'title': 'Bridging the Gap: Open vs Closed-Source LLMs for Ethical AI', 'desc': 'This paper discusses the evolution of large language models (LLMs) in natural language processing, highlighting the contrast between closed-source models like GPT-4 and open-source alternatives such as LLaMA and BLOOM. Closed-source models achieve high performance through proprietary datasets and extensive resources but are criticized for their lack of transparency and accessibility. In contrast, open-source models focus on democratization and community-driven development, successfully addressing performance gaps in diverse languages and domains. The paper emphasizes the need for hybrid approaches that combine the strengths of both paradigms to ensure ethical AI development and accessibility for all.'}, 'zh': {'title': '开源与封闭源：AI发展的未来之路', 'desc': '大型语言模型（LLMs）在自然语言处理（NLP）领域带来了重要变革，推动了文本生成、翻译和特定领域推理的发展。封闭源模型如GPT-4依赖于专有数据集和强大的计算资源，表现出色，但因其“黑箱”特性受到批评，限制了可访问性和可重复性。相比之下，开源项目如LLaMA和BLOOM通过社区驱动的发展和计算效率，优先考虑民主化，显著缩小了在语言多样性和特定领域应用中的性能差距。未来，结合封闭源和开源模型的优势，可能会推动LLM创新的发展，确保技术的可访问性和伦理部署。'}}}, {'id': 'https://huggingface.co/papers/2412.11974', 'title': 'Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning', 'url': 'https://huggingface.co/papers/2412.11974', 'abstract': 'Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that Emma-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning.', 'score': 2, 'issue_id': 1171, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': 'bdd5f30e4555bdf2', 'authors': ['Qi Sun', 'Pengfei Hong', 'Tej Deep Pala', 'Vernon Toh', 'U-Xuan Tan', 'Deepanway Ghosal', 'Soujanya Poria'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.11974.jpg', 'data': {'categories': ['#dataset', '#agents', '#multimodal', '#hallucinations', '#rl', '#robotics', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Emma-X: Мультимодальное управление роботами с обоснованным пошаговым мышлением', 'desc': 'Статья представляет модель Emma-X для управления роботами, объединяющую визуальное, языковое и действенное понимание. Модель использует иерархический набор данных с 60 000 траекторий манипуляций робота, аннотированных обоснованными рассуждениями о задачах и пространственными указаниями. Введена стратегия сегментации траекторий на основе состояний захвата и движения для уменьшения галлюцинаций при генерации рассуждений о подзадачах. Эксперименты показывают превосходство Emma-X над конкурентными базовыми моделями, особенно в реальных робототехнических задачах, требующих пространственных рассуждений.'}, 'en': {'title': 'Empowering Robots with Emma-X: Bridging Vision, Language, and Action', 'desc': 'This paper introduces Emma-X, a new model designed to improve robotic control by integrating visual language understanding with actionable policy generation. Traditional methods struggle with generalization across different tasks and environments, while Emma-X utilizes a hierarchical dataset to enhance its reasoning capabilities. The model incorporates a novel trajectory segmentation strategy to reduce errors in task reasoning, making it more effective in real-world applications. Experimental results show that Emma-X outperforms existing models, especially in tasks that require complex spatial reasoning.'}, 'zh': {'title': 'Emma-X：提升机器人空间推理能力的创新模型', 'desc': '传统的基于强化学习的机器人控制方法通常是针对特定任务的，无法在不同环境或未见过的物体和指令中进行泛化。视觉语言模型（VLMs）在场景理解和规划能力上表现出色，但缺乏生成针对特定机器人实现的可操作策略的能力。为了解决这个问题，出现了视觉-语言-动作（VLA）模型，但在长时间跨度的空间推理和基础任务规划方面面临挑战。我们提出了具备基础思维链和前瞻性空间推理的具身多模态动作模型Emma-X，实验结果表明Emma-X在需要空间推理的真实机器人任务中表现优于竞争基线。'}}}, {'id': 'https://huggingface.co/papers/2412.11449', 'title': 'Whisper-GPT: A Hybrid Representation Audio Large Language Model', 'url': 'https://huggingface.co/papers/2412.11449', 'abstract': 'We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.', 'score': 2, 'issue_id': 1167, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': 'c00b21e73b5de127', 'authors': ['Prateek Verma'], 'affiliations': ['Stanford University, Stanford CA, 94305'], 'pdf_title_img': 'assets/pdf/title_img/2412.11449.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#audio', '#architecture'], 'emoji': '🎵', 'ru': {'title': 'Объединение непрерывного и дискретного для улучшения генерации аудио', 'desc': 'WHISPER-GPT - это генеративная большая языковая модель для речи и музыки, объединяющая непрерывные аудиопредставления и дискретные токены. Модель решает проблему длины контекста, характерную для архитектур, использующих только дискретные аудиотокены. Комбинируя спектрограммы и акустические токены, WHISPER-GPT сохраняет преимущества обоих подходов. Модель демонстрирует улучшенные показатели перплексии и отрицательной логарифмической вероятности при предсказании следующего токена по сравнению с токен-ориентированными моделями для речи и музыки.'}, 'en': {'title': 'WHISPER-GPT: Bridging Continuous and Discrete Audio for Enhanced Generative Modeling', 'desc': 'WHISPER-GPT is a novel generative large language model designed for processing both speech and music by integrating continuous audio representations with discrete tokens. This approach addresses the limitations of traditional models that struggle with context length when generating high-fidelity audio. By utilizing spectrograms alongside discrete acoustic tokens, WHISPER-GPT captures essential audio information while enabling effective future token predictions. Our experiments demonstrate that this architecture significantly enhances perplexity and negative log-likelihood scores, outperforming existing token-based models in audio generation tasks.'}, 'zh': {'title': 'WHISPER-GPT：音频生成的新突破', 'desc': '我们提出了WHISPER-GPT：一种生成性大型语言模型，能够同时处理连续音频表示和离散音频标记。这种模型结合了频谱图等连续音频表示和神经压缩算法生成的离散音频标记，克服了高保真生成架构中上下文长度处理的难题。通过这种结合，我们能够在单个标记中保留特定时间点的所有音频信息，同时允许模型预测未来的标记，从而利用离散空间的优势。我们的实验表明，与基于标记的语言模型相比，WHISPER-GPT在下一个标记预测的困惑度和负对数似然得分上有显著改善。'}}}, {'id': 'https://huggingface.co/papers/2412.10447', 'title': 'TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning', 'url': 'https://huggingface.co/papers/2412.10447', 'abstract': 'Exploiting the promise of recent advances in imitation learning for mobile manipulation will require the collection of large numbers of human-guided demonstrations. This paper proposes an open-source design for an inexpensive, robust, and flexible mobile manipulator that can support arbitrary arms, enabling a wide range of real-world household mobile manipulation tasks. Crucially, our design uses powered casters to enable the mobile base to be fully holonomic, able to control all planar degrees of freedom independently and simultaneously. This feature makes the base more maneuverable and simplifies many mobile manipulation tasks, eliminating the kinematic constraints that create complex and time-consuming motions in nonholonomic bases. We equip our robot with an intuitive mobile phone teleoperation interface to enable easy data acquisition for imitation learning. In our experiments, we use this interface to collect data and show that the resulting learned policies can successfully perform a variety of common household mobile manipulation tasks.', 'score': 2, 'issue_id': 1166, 'pub_date': '2024-12-11', 'pub_date_card': {'ru': '11 декабря', 'en': 'December 11', 'zh': '12月11日'}, 'hash': 'a7cdf999e4ed1689', 'authors': ['Jimmy Wu', 'William Chong', 'Robert Holmberg', 'Aaditya Prasad', 'Yihuai Gao', 'Oussama Khatib', 'Shuran Song', 'Szymon Rusinkiewicz', 'Jeannette Bohg'], 'affiliations': ['Dexterity', 'Princeton University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2412.10447.jpg', 'data': {'categories': ['#open_source', '#robotics', '#agents', '#data'], 'emoji': '🤖', 'ru': {'title': 'Доступный мобильный манипулятор для имитационного обучения в бытовых условиях', 'desc': 'В статье представлен проект недорогого и гибкого мобильного манипулятора с голономной базой на колесах. Устройство позволяет использовать различные манипуляторы и выполнять широкий спектр бытовых задач. Авторы разработали интуитивный интерфейс телеуправления через мобильный телефон для сбора данных. Эксперименты показали, что обученные на этих данных модели успешно справляются с различными бытовыми задачами мобильной манипуляции.'}, 'en': {'title': 'Empowering Mobile Manipulation with Holonomic Design and Imitation Learning', 'desc': 'This paper presents a new design for a mobile manipulator that is affordable and adaptable for various robotic arms. It emphasizes the importance of collecting numerous human-guided demonstrations to improve imitation learning in mobile manipulation tasks. The robot features a fully holonomic base, allowing it to move freely in all directions, which simplifies the execution of complex tasks. Additionally, an easy-to-use mobile phone interface is provided for data collection, enabling the training of effective policies for household tasks.'}, 'zh': {'title': '灵活移动操控，简化家庭任务', 'desc': '本文提出了一种开源设计，旨在开发一种经济、稳健且灵活的移动操控器，以支持各种人类指导的示范。该设计采用了动力万向轮，使移动底座具备完全的全向性，能够独立且同时控制所有平面自由度。这一特性提高了底座的机动性，简化了许多移动操控任务，消除了非全向底座带来的运动约束。我们还为机器人配备了直观的手机远程操作界面，以便于收集模仿学习所需的数据。'}}}, {'id': 'https://huggingface.co/papers/2412.11457', 'title': 'MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes', 'url': 'https://huggingface.co/papers/2412.11457', 'abstract': "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the model's comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the model's capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise a structure-guided timestep sampling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks.", 'score': 2, 'issue_id': 1158, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '415b98ca8c3ed003', 'authors': ['Ruijie Lu', 'Yixin Chen', 'Junfeng Ni', 'Baoxiong Jia', 'Yu Liu', 'Diwen Wan', 'Gang Zeng', 'Siyuan Huang'], 'affiliations': ['State Key Laboratory of General Artificial Intelligence, BIGAI', 'State Key Laboratory of General Artificial Intelligence, Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.11457.jpg', 'data': {'categories': ['#optimization', '#3d', '#training', '#dataset', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Улучшение структурной осведомленности для синтеза новых ракурсов сцен с несколькими объектами', 'desc': 'Статья представляет метод MOVIS для улучшения структурной осведомленности диффузионных моделей при синтезе новых ракурсов сцен с несколькими объектами. Авторы предлагают внедрять структурно-осведомленные признаки, такие как глубина и маски объектов, в U-Net для улучшения понимания моделью пространственных отношений. Они также вводят вспомогательную задачу предсказания масок объектов с новых ракурсов и разрабатывают специальный планировщик выборки временных шагов для баланса между глобальным размещением объектов и восстановлением мелких деталей. Авторы предлагают новые метрики для оценки правдоподобия синтезированных изображений.'}, 'en': {'title': 'Enhancing Multi-Object Novel View Synthesis with Structural Awareness', 'desc': "This paper introduces MOVIS, a method designed to improve multi-object novel view synthesis (NVS) using pre-trained diffusion models. The approach enhances the model's understanding of object structures by incorporating depth and object masks into the denoising U-Net architecture. Additionally, it introduces an auxiliary task that helps the model predict object masks for novel views, improving object differentiation and placement. The authors also propose a new sampling scheduler that balances global object placement with detailed recovery, and they evaluate the model's performance using cross-view consistency metrics alongside traditional NVS measures."}, 'zh': {'title': '提升多物体新视角合成的结构感知能力', 'desc': '本文提出了一种名为MOVIS的方法，旨在提高多物体新视角合成（NVS）中的结构感知能力。通过将深度信息和物体掩码等结构感知特征注入去噪U-Net，模型能够更好地理解物体实例及其空间关系。此外，模型还被要求同时预测新视角的物体掩码，从而增强其区分和放置物体的能力。最后，本文通过分析扩散采样过程，设计了一种结构引导的时间步采样调度器，以平衡全局物体放置和细节恢复的学习。'}}}, {'id': 'https://huggingface.co/papers/2412.11314', 'title': 'Reliable, Reproducible, and Really Fast Leaderboards with Evalica', 'url': 'https://huggingface.co/papers/2412.11314', 'abstract': 'The rapid advancement of natural language processing (NLP) technologies, such as instruction-tuned large language models (LLMs), urges the development of modern evaluation protocols with human and machine feedback. We introduce Evalica, an open-source toolkit that facilitates the creation of reliable and reproducible model leaderboards. This paper presents its design, evaluates its performance, and demonstrates its usability through its Web interface, command-line interface, and Python API.', 'score': 1, 'issue_id': 1170, 'pub_date': '2024-12-15', 'pub_date_card': {'ru': '15 декабря', 'en': 'December 15', 'zh': '12月15日'}, 'hash': '909b8af5f6b0e8af', 'authors': ['Dmitry Ustalov'], 'affiliations': ['JetBrains / Belgrade, Serbia'], 'pdf_title_img': 'assets/pdf/title_img/2412.11314.jpg', 'data': {'categories': ['#rlhf', '#open_source', '#benchmark'], 'emoji': '🏆', 'ru': {'title': 'Evalica: новый стандарт оценки языковых моделей', 'desc': 'Evalica - это инструментарий с открытым исходным кодом для создания надежных и воспроизводимых рейтингов языковых моделей. Он разработан в ответ на быстрое развитие технологий обработки естественного языка, включая большие языковые модели, настроенные на выполнение инструкций. Evalica предоставляет веб-интерфейс, интерфейс командной строки и Python API для удобного использования. Данный инструмент позволяет проводить оценку моделей с помощью как человеческой, так и машинной обратной связи.'}, 'en': {'title': 'Evalica: Revolutionizing NLP Model Evaluation', 'desc': 'This paper introduces Evalica, an open-source toolkit designed to enhance the evaluation of natural language processing models, particularly instruction-tuned large language models. It emphasizes the need for modern evaluation protocols that incorporate both human and machine feedback to ensure reliability and reproducibility. The authors detail the design of Evalica and assess its performance across various metrics. Additionally, they showcase its usability through multiple interfaces, including a web interface, command-line interface, and Python API.'}, 'zh': {'title': 'Evalica：提升模型评估的开源工具', 'desc': '随着自然语言处理技术的快速发展，特别是指令调优的大型语言模型，现代评估协议的需求日益增加。我们提出了Evalica，这是一个开源工具包，旨在创建可靠且可重复的模型排行榜。本文介绍了Evalica的设计，评估了其性能，并通过Web界面、命令行界面和Python API展示了其可用性。该工具包为研究人员和开发者提供了一个有效的评估平台。'}}}, {'id': 'https://huggingface.co/papers/2412.11689', 'title': 'Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning', 'url': 'https://huggingface.co/papers/2412.11689', 'abstract': 'Vertical Federated Learning (VFL) aims to enable collaborative training of deep learning models while maintaining privacy protection. However, the VFL procedure still has components that are vulnerable to attacks by malicious parties. In our work, we consider feature reconstruction attacks, a common risk targeting input data compromise. We theoretically claim that feature reconstruction attacks cannot succeed without knowledge of the prior distribution on data. Consequently, we demonstrate that even simple model architecture transformations can significantly impact the protection of input data during VFL. Confirming these findings with experimental results, we show that MLP-based models are resistant to state-of-the-art feature reconstruction attacks.', 'score': 1, 'issue_id': 1166, 'pub_date': '2024-12-16', 'pub_date_card': {'ru': '16 декабря', 'en': 'December 16', 'zh': '12月16日'}, 'hash': '3df0aeb1682ee705', 'authors': ['Andrei Semenov', 'Philip Zmushko', 'Alexander Pichugin', 'Aleksandr Beznosikov'], 'affiliations': ['ISP RAS, MIPT', 'MIPT'], 'pdf_title_img': 'assets/pdf/title_img/2412.11689.jpg', 'data': {'categories': ['#security', '#training', '#architecture'], 'emoji': '🛡️', 'ru': {'title': 'Повышение безопасности VFL через архитектурные преобразования', 'desc': 'Статья посвящена вертикальному федеративному обучению (VFL) и защите от атак реконструкции признаков. Авторы теоретически обосновывают, что без знания априорного распределения данных такие атаки невозможны. Они демонстрируют, что даже простые изменения архитектуры модели могут значительно повысить защищенность входных данных при VFL. Экспериментальные результаты подтверждают, что модели на основе многослойных персептронов устойчивы к современным атакам реконструкции признаков.'}, 'en': {'title': 'Enhancing Privacy in Vertical Federated Learning with MLP Resilience', 'desc': 'Vertical Federated Learning (VFL) allows multiple parties to collaboratively train deep learning models while keeping their data private. However, certain parts of the VFL process can still be attacked by malicious users, particularly through feature reconstruction attacks that aim to recover sensitive input data. Our research shows that these attacks are unlikely to succeed without prior knowledge of the data distribution. We also found that even basic changes to the model architecture can enhance data protection, with experiments indicating that MLP-based models are particularly robust against these advanced attacks.'}, 'zh': {'title': '保护隐私，抵御特征重构攻击的VFL', 'desc': '垂直联邦学习（VFL）旨在在保护隐私的同时实现深度学习模型的协作训练。然而，VFL过程中的某些组件仍然容易受到恶意攻击。我们研究了特征重构攻击，这是一种常见的针对输入数据的风险。我们的理论表明，特征重构攻击在没有数据先验分布知识的情况下无法成功，并且简单的模型架构变换可以显著提高输入数据的保护。'}}}, {'id': 'https://huggingface.co/papers/2412.11100', 'title': 'DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes', 'url': 'https://huggingface.co/papers/2412.11100', 'abstract': 'The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360{\\deg} panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose the DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce a Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via a diffusion model with fixed resolution through a seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering a training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution. Our project page is available at https://dynamic-scaler.pages.dev/.', 'score': 0, 'issue_id': 1171, 'pub_date': '2024-12-15', 'pub_date_card': {'ru': '15 декабря', 'en': 'December 15', 'zh': '12月15日'}, 'hash': 'e3d7dcbec72feb5d', 'authors': ['Jinxiu Liu', 'Shaoheng Lin', 'Yinxiao Li', 'Ming-Hsuan Yang'], 'affiliations': ['Google DeepMind', 'SCUT', 'UC Merced'], 'pdf_title_img': 'assets/pdf/title_img/2412.11100.jpg', 'data': {'categories': ['#diffusion', '#3d', '#video'], 'emoji': '🌐', 'ru': {'title': 'Создание масштабируемых панорамных видео для иммерсивных AR/VR приложений', 'desc': 'DynamicScaler - новый метод генерации панорамных видео высокого качества для AR/VR приложений. Он использует диффузионную модель с фиксированным разрешением и вращающимся окном для создания согласованных панорамных сцен произвольного размера. Метод включает механизм Global Motion Guidance для обеспечения локальной детализации и глобальной непрерывности движения. DynamicScaler не требует дополнительного обучения и эффективно работает с постоянным потреблением видеопамяти независимо от разрешения выходного видео.'}, 'en': {'title': 'DynamicScaler: Revolutionizing Panoramic Video Generation for AR/VR', 'desc': 'This paper presents the DynamicScaler, a novel approach for generating high-quality panoramic videos suitable for AR/VR applications. It overcomes limitations of existing video diffusion models by allowing for scalable and coherent dynamic scene synthesis across various resolutions and aspect ratios. The method utilizes an Offset Shifting Denoiser to ensure smooth transitions and consistency in the generated scenes, while a Global Motion Guidance mechanism maintains both detail and motion continuity. Experimental results show that DynamicScaler excels in content and motion quality, providing an efficient solution for immersive video generation without the need for extensive training.'}, 'zh': {'title': '动态场景合成的新突破', 'desc': '随着对沉浸式增强现实和虚拟现实应用的需求增加，生成高质量的场景级和360度全景视频变得尤为重要。现有的视频扩散模型在分辨率和宽高比上受到限制，影响了其在动态内容合成中的应用。我们提出的DynamicScaler通过引入偏移平移去噪器，解决了这些问题，实现了空间可扩展的全景动态场景合成，并保持了全景场景的一致性。我们的实验表明，该方法在全景场景级视频生成中具有更优的内容和运动质量，且在输出视频分辨率变化时，始终保持高效和可扩展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (7)', '#agents (3)', '#agi', '#alignment (2)', '#architecture (8)', '#audio (1)', '#benchmark (6)', '#cv (6)', '#data (2)', '#dataset (6)', '#diffusion (10)', '#ethics (1)', '#games', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (4)', '#open_source (8)', '#optimization (10)', '#plp', '#rag (2)', '#reasoning (3)', '#rl (1)', '#rlhf (2)', '#robotics (3)', '#science', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (8)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-17 19:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-17 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-17 19:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    