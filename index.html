
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. July 17.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">17 июля</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-16.html">⬅️ <span id="prev-date">16.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-18.html">➡️ <span id="next-date">18.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'};
        let feedDateNext = {'ru': '18.07', 'en': '07/18', 'zh': '7月18日'};
        let feedDatePrev = {'ru': '16.07', 'en': '07/16', 'zh': '7月16日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.09477', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs', 'url': 'https://huggingface.co/papers/2507.09477', 'abstract': 'This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.', 'score': 42, 'issue_id': 4862, 'pub_date': '2025-07-13', 'pub_date_card': {'ru': '13 июля', 'en': 'July 13', 'zh': '7月13日'}, 'hash': 'da4aa711048f0a7f', 'authors': ['Yangning Li', 'Weizhi Zhang', 'Yuyao Yang', 'Wei-Chieh Huang', 'Yaozu Wu', 'Junyu Luo', 'Yuanchen Bei', 'Henry Peng Zou', 'Xiao Luo', 'Yusheng Zhao', 'Chunkit Chan', 'Yankai Chen', 'Zhongfen Deng', 'Yinghui Li', 'Hai-Tao Zheng', 'Dongyuan Li', 'Renhe Jiang', 'Ming Zhang', 'Yangqiu Song', 'Philip S. Yu'], 'affiliations': ['HKUST', 'Peking University', 'The University of Tokyo', 'Tsinghua University', 'University of California, Los Angeles', 'University of Illinois Chicago', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2507.09477.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#survey', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Объединение извлечения информации и рассуждений для создания более мощных языковых моделей', 'desc': 'Это обзор интегрирует рассуждения и извлечение информации в больших языковых моделях (LLM) для улучшения фактической точности и многоступенчатого вывода. В статье рассматриваются подходы, сочетающие Retrieval-Augmented Generation (RAG) и методы рассуждений, что позволяет преодолеть ограничения каждого из них по отдельности. Авторы выделяют синергетические фреймворки RAG-Reasoning, демонстрирующие передовые результаты на задачах, требующих обширных знаний. Обзор также очерчивает направления будущих исследований в этой области.'}, 'en': {'title': 'Enhancing LLMs with Synergized Retrieval and Reasoning', 'desc': 'This paper discusses how to improve the accuracy and reasoning abilities of Large Language Models (LLMs) by combining retrieval and reasoning techniques. It introduces the concept of Retrieval-Augmented Generation (RAG), which enhances LLMs by providing them with external knowledge, but notes that RAG struggles with complex, multi-step reasoning tasks. The authors propose a unified framework that integrates advanced reasoning into RAG, allowing LLMs to better utilize retrieved information for deeper inference. They also highlight future research directions to create more effective and trustworthy systems that can adapt to various types of knowledge and user needs.'}, 'zh': {'title': '推理与检索的协同提升大型语言模型的能力', 'desc': '这篇论文调查了大型语言模型中的推理与检索的结合，以提高事实准确性和多步推理能力。检索增强生成（RAG）通过引入外部知识来提升大型语言模型的事实性，但在需要多步推理的问题上表现不足。论文提出了统一的推理-检索视角，展示了如何通过先进的推理优化RAG的每个阶段，并强调了新兴的协同RAG-推理框架。最后，论文分类了方法、数据集和开放挑战，并概述了未来研究方向，以构建更有效、适应多模态、可信赖和以人为本的RAG-推理系统。'}}}, {'id': 'https://huggingface.co/papers/2507.12465', 'title': 'PhysX: Physical-Grounded 3D Asset Generation', 'url': 'https://huggingface.co/papers/2507.12465', 'abstract': 'PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose PhysX, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose PhysXGen, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.', 'score': 20, 'issue_id': 4861, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'ece62f7e4ecd0487', 'authors': ['Ziang Cao', 'Zhaoxi Chen', 'Linag Pan', 'Ziwei Liu'], 'affiliations': ['Nanyang Technological University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.12465.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#games', '#3d', '#open_source', '#dataset'], 'emoji': '🧱', 'ru': {'title': 'Физически достоверная генерация 3D-объектов', 'desc': 'PhysX представляет собой новый подход к генерации 3D-объектов с учетом их физических свойств. Авторы создали датасет PhysXNet с аннотациями физических характеристик 3D-моделей и разработали фреймворк PhysXGen для интеграции физических знаний в процесс генерации. PhysXGen использует двухветвевую архитектуру для моделирования связей между 3D-структурами и физическими свойствами. Эксперименты показали превосходную производительность и способность к обобщению предложенного метода.'}, 'en': {'title': 'Bridging 3D Generation with Real-World Physics', 'desc': 'PhysX introduces a new approach to 3D asset generation by incorporating physical properties into the modeling process. It presents PhysXNet, a unique dataset that annotates 3D models with essential physical attributes like scale, material, and function. Additionally, PhysXGen is a framework that uses this dataset to generate 3D assets that not only look good but also behave realistically in physical simulations. This work aims to enhance the applicability of AI-generated 3D models in real-world scenarios, such as robotics and virtual simulations.'}, 'zh': {'title': '物理驱动的3D资产生成新方法', 'desc': 'PhysX提出了一种新的方法来解决3D生成模型中缺乏物理属性的问题。它引入了PhysXNet，这是一个物理注释的数据集，系统地标注了五个基础维度，包括绝对尺度、材料、可用性、运动学和功能描述。通过PhysXGen框架，物理知识被整合到3D资产生成中，利用双分支架构建模3D结构与物理属性之间的潜在关联。实验结果表明，该框架在生成具有可信物理预测的3D资产方面表现优越，具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.12463', 'title': 'MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding', 'url': 'https://huggingface.co/papers/2507.12463', 'abstract': 'A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorx2014such as motion, trajectories, and intentionx2014a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasksx2014ranging from motion prediction to motion generation and human behavior question answeringx2014thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io.', 'score': 13, 'issue_id': 4864, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'c5061caaead150e7', 'authors': ['Renjie Li', 'Ruijie Ye', 'Mingyang Wu', 'Hao Frank Yang', 'Zhiwen Fan', 'Hezhen Hu', 'Zhengzhong Tu'], 'affiliations': ['Brown University', 'Johns Hopkins University', 'Texas A&M University', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2507.12463.jpg', 'data': {'categories': ['#benchmark', '#agents', '#dataset'], 'emoji': '🚗', 'ru': {'title': 'MMHU: Комплексный анализ поведения человека для безопасного автономного вождения', 'desc': 'Представлен крупномасштабный бенчмарк MMHU для анализа поведения человека в контексте автономного вождения. Он включает богатые аннотации и разнообразные источники данных, охватывающие 57 тысяч клипов с движениями людей и 1,73 миллиона кадров. MMHU позволяет оценивать различные задачи, включая прогнозирование движения и ответы на вопросы о поведении. Бенчмарк предоставляет комплексный набор для оценки понимания человеческого поведения в системах автономного вождения.'}, 'en': {'title': 'MMHU: Advancing Human Behavior Analysis for Safer Autonomous Driving', 'desc': 'The paper introduces MMHU, a comprehensive benchmark designed for analyzing human behavior in the context of autonomous driving. It includes extensive annotations on human motion, trajectories, intentions, and safety-related behaviors, making it a valuable resource for researchers. The dataset consists of 57,000 motion clips and 1.73 million frames sourced from various platforms, including established driving datasets and real-world videos. By benchmarking multiple tasks such as motion prediction and behavior question answering, MMHU aims to enhance the understanding of human behavior in driving scenarios.'}, 'zh': {'title': 'MMHU：自动驾驶人类行为分析的新基准', 'desc': '本文提出了一个名为MMHU的大规模基准，用于分析自动驾驶中的人类行为。该基准包含丰富的注释和多样的数据来源，涵盖了人类运动、轨迹、意图等多个方面。数据集包括57,000个运动片段和173万帧，来源于知名的驾驶数据集和YouTube等平台。我们还开发了一个人机协作的注释流程，以生成详细的行为描述，并对多个任务进行了基准测试，包括运动预测和行为问答。'}}}, {'id': 'https://huggingface.co/papers/2507.11949', 'title': 'MOSPA: Human Motion Generation Driven by Spatial Audio', 'url': 'https://huggingface.co/papers/2507.11949', 'abstract': 'A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details.', 'score': 12, 'issue_id': 4862, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '60ad5621a3842ae5', 'authors': ['Shuyang Xu', 'Zhiyang Dou', 'Mingyi Shi', 'Liang Pan', 'Leo Ho', 'Jingbo Wang', 'Yuan Liu', 'Cheng Lin', 'Yuexin Ma', 'Wenping Wang', 'Taku Komura'], 'affiliations': ['Macau University of Science and Technology', 'Shanghai AI Lab', 'ShanghaiTech University', 'Texas A&M University', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.11949.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#benchmark', '#open_source', '#dataset'], 'emoji': '🎧', 'ru': {'title': 'Реалистичная анимация движений под пространственное аудио', 'desc': 'Исследователи представили MOSPA - генеративную модель на основе диффузии для моделирования движений человека в ответ на пространственное аудио. Для обучения модели был создан новый набор данных SAM, содержащий разнообразные пространственные аудио и соответствующие им движения. MOSPA использует эффективный механизм слияния для захвата взаимосвязи между движениями тела и пространственным звуком. Модель достигла лучших результатов в этой задаче по сравнению с существующими подходами.'}, 'en': {'title': 'Bridging Sound and Motion: MOSPA for Realistic Human Animation', 'desc': 'The paper presents MOSPA, a diffusion-based generative framework designed to model human motion in response to spatial audio. It introduces the SAM dataset, which is the first of its kind, containing high-quality spatial audio and corresponding human motion data. The framework effectively captures the relationship between body movements and spatial audio through a novel fusion mechanism. By training on this dataset, MOSPA can generate diverse and realistic human motions that respond dynamically to different auditory stimuli, achieving state-of-the-art results in this area.'}, 'zh': {'title': '空间音频驱动的人类运动生成新突破', 'desc': '本文介绍了一种基于扩散的生成框架MOSPA，用于建模人类在空间音频刺激下的运动。我们创建了首个综合性的空间音频驱动人类运动（SAM）数据集，包含多样化和高质量的空间音频与运动数据。MOSPA通过有效的融合机制，准确捕捉身体运动与空间音频之间的关系，能够生成多样且真实的人类运动。经过广泛的实验验证，我们的方法在这一任务上达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.12415', 'title': 'SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?', 'url': 'https://huggingface.co/papers/2507.12415', 'abstract': 'SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.  \t\t\t\t\tAI-generated summary \t\t\t\t Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.', 'score': 11, 'issue_id': 4864, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': '6be5b41deae78198', 'authors': ['Xinyi He', 'Qian Liu', 'Mingzhe Du', 'Lin Yan', 'Zhijie Fan', 'Yiming Huang', 'Zejian Yuan', 'Zejun Ma'], 'affiliations': ['National University of Singapore', 'TikTok', 'University of California San Diego', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12415.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#dataset'], 'emoji': '🚀', 'ru': {'title': 'SWE-Perf: Новый рубеж в оценке оптимизации кода с помощью LLM', 'desc': 'SWE-Perf - это бенчмарк для оценки способностей больших языковых моделей (LLM) в оптимизации производительности кода на основе реальных репозиториев. Он содержит 140 тщательно отобранных примеров из популярных GitHub-репозиториев, включающих кодовую базу, целевые функции, тесты производительности и экспертные патчи. Оценка существующих методов показала значительный разрыв между возможностями LLM и экспертным уровнем оптимизации. SWE-Perf открывает новые направления исследований в области применения LLM для повышения производительности кода.'}, 'en': {'title': 'Unlocking Code Efficiency: Evaluating LLMs with SWE-Perf', 'desc': 'SWE-Perf is a new benchmark designed to evaluate how well Large Language Models (LLMs) can optimize code performance using real-world data from software repositories. It focuses on the important task of improving code efficiency, which is essential for high-quality software systems. The benchmark includes 140 instances based on actual performance-enhancing pull requests from GitHub, providing a realistic testing environment. The study reveals that current LLMs significantly lag behind expert-level optimization, indicating a need for further research in this area.'}, 'zh': {'title': 'SWE-Perf：评估代码性能优化的新基准', 'desc': 'SWE-Perf是一个基准测试，用于评估大型语言模型在代码性能优化方面的表现。该基准测试使用真实的代码库数据，专注于软件工程中的代码性能优化。虽然大型语言模型在代码生成和错误修复方面表现出色，但它们在提升代码性能方面的能力尚未得到充分探索。通过对140个精心挑选的实例进行评估，SWE-Perf揭示了现有大型语言模型与专家级优化性能之间的显著差距，指出了这一新兴领域中的重要研究机会。'}}}, {'id': 'https://huggingface.co/papers/2507.11527', 'title': 'DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering', 'url': 'https://huggingface.co/papers/2507.11527', 'abstract': "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.", 'score': 9, 'issue_id': 4861, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': 'e6f20729b2c748f9', 'authors': ['Yinsheng Li', 'Zhen Dong', 'Yi Shao'], 'affiliations': ['Department of Civil Engineering McGill University', 'NVIDIA', 'UC Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2507.11527.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#open_source', '#long_context', '#agents'], 'emoji': '📐', 'ru': {'title': 'DrafterBench: Комплексная оценка LLM-агентов в инженерном проектировании', 'desc': 'DrafterBench - это открытый бенчмарк для оценки агентов на основе больших языковых моделей (LLM) в задаче проверки технических чертежей. Он оценивает способности агентов в понимании структурированных данных, выполнении функций, следовании инструкциям и критическом мышлении. Бенчмарк содержит 12 типов задач, основанных на реальных чертежах, с 46 специальными функциями и инструментами, всего 1920 заданий. DrafterBench предлагает детальный анализ точности выполнения задач и статистики ошибок, чтобы глубже понять возможности агентов и определить цели для улучшения интеграции LLM в инженерные приложения.'}, 'en': {'title': 'DrafterBench: Evaluating LLMs for Technical Drawing Mastery', 'desc': 'DrafterBench is an open-source benchmark designed to evaluate Large Language Model (LLM) agents specifically in the area of technical drawing revision. It includes twelve task types derived from real-world drawing files, featuring 46 customized functions and a total of 1920 tasks. The benchmark assesses LLM agents on their abilities in structured data comprehension, function execution, instruction following, and critical reasoning. By providing detailed analysis of task accuracy and error statistics, DrafterBench aims to enhance the understanding of LLM capabilities and identify areas for improvement in engineering applications.'}, 'zh': {'title': 'DrafterBench：评估LLM代理的技术图纸修订能力', 'desc': 'DrafterBench是一个开源基准，用于评估大型语言模型（LLM）代理在技术图纸修订中的能力。它涵盖了结构化数据理解、功能执行、指令遵循和批判性推理等多个方面。该基准包含来自真实绘图文件的十二种任务，提供了46种定制功能和1920个任务。DrafterBench旨在深入分析代理的能力，帮助识别在工程应用中整合LLM的改进目标。'}}}, {'id': 'https://huggingface.co/papers/2507.11412', 'title': 'Seq vs Seq: An Open Suite of Paired Encoders and Decoders', 'url': 'https://huggingface.co/papers/2507.11412', 'abstract': 'The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.', 'score': 9, 'issue_id': 4871, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '569bfad741b150bd', 'authors': ['Orion Weller', 'Kathryn Ricci', 'Marc Marone', 'Antoine Chaffin', 'Dawn Lawrie', 'Benjamin Van Durme'], 'affiliations': ['Johns Hopkins University', 'LightOn'], 'pdf_title_img': 'assets/pdf/title_img/2507.11412.jpg', 'data': {'categories': ['#optimization', '#architecture', '#open_source', '#training', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Ettin: новый стандарт для энкодеров и декодеров в машинном обучении', 'desc': 'Исследователи представили набор моделей Ettin, включающий энкодер-только и декодер-только архитектуры от 17 миллионов до 1 миллиарда параметров. Модели обучены на 2 триллионах токенов и превосходят существующие аналоги в своих категориях. Подтверждено, что энкодеры лучше справляются с задачами классификации и поиска, а декодеры - с генеративными задачами. Показано, что адаптация декодера к задачам энкодера (и наоборот) путем дообучения уступает использованию изначально правильной архитектуры.'}, 'en': {'title': 'Unlocking the Power of Encoder and Decoder Models in NLP', 'desc': 'This paper discusses the comparison between encoder-only and decoder-only language models in the context of machine learning. It introduces the Ettin suite, which consists of paired models of both types, ensuring they are trained under the same conditions for a fair comparison. The findings reveal that encoder-only models are better suited for classification and retrieval tasks, while decoder-only models excel in text generation. Additionally, the study shows that adapting models for different tasks through continued training is less effective than using models specifically designed for those tasks.'}, 'zh': {'title': 'Ettin模型：编码器与解码器的完美结合', 'desc': '本文介绍了一种新的模型套件Ettin，包含配对的编码器和解码器模型，参数范围从1700万到10亿，训练数据达到2万亿个标记。研究表明，编码器模型在分类和检索任务中表现优异，而解码器模型在生成任务中更具优势。通过相同的训练方法，Ettin模型在各自的类别中达到了最新的性能，超越了现代的BERT和Llama 3.2等模型。我们还发现，继续训练解码器模型以适应编码器任务的效果不如直接使用编码器模型。'}}}, {'id': 'https://huggingface.co/papers/2507.02857', 'title': 'AnyI2V: Animating Any Conditional Image with Motion Control', 'url': 'https://huggingface.co/papers/2507.02857', 'abstract': 'AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in video generation, particularly in diffusion models, have driven notable progress in text-to-video (T2V) and image-to-video (I2V) synthesis. However, challenges remain in effectively integrating dynamic motion signals and flexible spatial constraints. Existing T2V methods typically rely on text prompts, which inherently lack precise control over the spatial layout of generated content. In contrast, I2V methods are limited by their dependence on real images, which restricts the editability of the synthesized content. Although some methods incorporate ControlNet to introduce image-based conditioning, they often lack explicit motion control and require computationally expensive training. To address these limitations, we propose AnyI2V, a training-free framework that animates any conditional images with user-defined motion trajectories. AnyI2V supports a broader range of modalities as the conditional image, including data types such as meshes and point clouds that are not supported by ControlNet, enabling more flexible and versatile video generation. Additionally, it supports mixed conditional inputs and enables style transfer and editing via LoRA and text prompts. Extensive experiments demonstrate that the proposed AnyI2V achieves superior performance and provides a new perspective in spatial- and motion-controlled video generation. Code is available at https://henghuiding.com/AnyI2V/.', 'score': 5, 'issue_id': 4864, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '5a06d615e806a525', 'authors': ['Ziye Li', 'Hao Luo', 'Xincheng Shuai', 'Henghui Ding'], 'affiliations': ['DAMO Academy, Alibaba group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.02857.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Универсальная анимация изображений с контролем движения', 'desc': 'AnyI2V - это фреймворк для анимации условных изображений с пользовательскими траекториями движения без необходимости обучения. Он поддерживает различные типы данных, включая сетки и облака точек, что позволяет создавать более гибкие и универсальные видео. AnyI2V превосходит существующие методы текст-в-видео и изображение-в-видео, предоставляя точный контроль над пространственным расположением и движением генерируемого контента. Фреймворк также поддерживает смешанные условные входные данные и позволяет выполнять перенос стиля и редактирование с помощью LoRA и текстовых подсказок.'}, 'en': {'title': 'AnyI2V: Freedom in Motion-Controlled Video Generation', 'desc': 'AnyI2V is a novel framework designed for animating images based on user-defined motion paths without the need for extensive training. It addresses the limitations of existing text-to-video and image-to-video methods by allowing for greater control over spatial layouts and dynamic motion signals. This framework supports various data types, including meshes and point clouds, which enhances its versatility in video generation. Through extensive testing, AnyI2V has shown to outperform previous methods, offering a fresh approach to generating videos with precise motion and spatial control.'}, 'zh': {'title': 'AnyI2V：无训练的灵活视频生成框架', 'desc': 'AnyI2V是一个无需训练的框架，可以根据用户定义的运动轨迹为条件图像添加动画，支持多种数据类型，从而实现灵活的视频生成。该方法解决了现有文本到视频（T2V）和图像到视频（I2V）合成中的动态运动信号和空间约束整合问题。与传统方法不同，AnyI2V不依赖于真实图像，允许更高的可编辑性，并支持混合条件输入和风格转移。实验结果表明，AnyI2V在空间和运动控制的视频生成方面表现优越，提供了新的视角。'}}}, {'id': 'https://huggingface.co/papers/2507.12462', 'title': 'SpatialTrackerV2: 3D Point Tracking Made Easy', 'url': 'https://huggingface.co/papers/2507.12462', 'abstract': 'SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SpatialTrackerV2, a feed-forward 3D point tracking method for monocular videos. Going beyond modular pipelines built on off-the-shelf components for 3D tracking, our approach unifies the intrinsic connections between point tracking, monocular depth, and camera pose estimation into a high-performing and feedforward 3D point tracker. It decomposes world-space 3D motion into scene geometry, camera ego-motion, and pixel-wise object motion, with a fully differentiable and end-to-end architecture, allowing scalable training across a wide range of datasets, including synthetic sequences, posed RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms existing 3D tracking methods by 30%, and matches the accuracy of leading dynamic 3D reconstruction approaches while running 50times faster.', 'score': 4, 'issue_id': 4867, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 июля', 'en': 'July 16', 'zh': '7月16日'}, 'hash': 'a30fc76bcf7ffd92', 'authors': ['Yuxi Xiao', 'Jianyuan Wang', 'Nan Xue', 'Nikita Karaev', 'Yuri Makarov', 'Bingyi Kang', 'Xing Zhu', 'Hujun Bao', 'Yujun Shen', 'Xiaowei Zhou'], 'affiliations': ['Ant Group', 'Bytedance Seed', 'Oxford', 'Pixelwise AI', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12462.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Единая архитектура для быстрого и точного 3D-трекинга в монокулярном видео', 'desc': 'SpatialTrackerV2 - это метод отслеживания 3D точек в монокулярных видео с прямой связью. Он объединяет отслеживание точек, монокулярную оценку глубины и оценку положения камеры в единую сквозную архитектуру. Модель декомпозирует 3D движение в мировом пространстве на геометрию сцены, эго-движение камеры и попиксельное движение объектов. SpatialTrackerV2 превосходит существующие методы 3D-трекинга на 30% и соответствует точности ведущих подходов динамической 3D-реконструкции, работая при этом в 50 раз быстрее.'}, 'en': {'title': 'Unified 3D Point Tracking at Lightning Speed', 'desc': 'SpatialTrackerV2 is a novel method for tracking 3D points in monocular videos using a feed-forward architecture. It combines point tracking, monocular depth estimation, and camera pose estimation into a single, efficient model. This approach breaks down 3D motion into components like scene geometry and camera movement, enabling it to learn from diverse datasets effectively. As a result, SpatialTrackerV2 achieves a 30% improvement over existing methods and operates 50 times faster than traditional dynamic 3D reconstruction techniques.'}, 'zh': {'title': '高效快速的3D点跟踪新方法', 'desc': 'SpatialTrackerV2是一种用于单目视频的前馈3D点跟踪方法。它将点跟踪、单目深度和相机姿态估计整合到一个统一的端到端架构中，从而实现高性能和快速处理。该方法将世界空间中的3D运动分解为场景几何、相机自运动和逐像素的物体运动，支持在多种数据集上进行可扩展训练。通过从异构数据中联合学习几何和运动，SpatialTrackerV2的性能比现有的3D跟踪方法提高了30%，并且运行速度比领先的动态3D重建方法快50倍。'}}}, {'id': 'https://huggingface.co/papers/2507.11764', 'title': 'AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles', 'url': 'https://huggingface.co/papers/2507.11764', 'abstract': "Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).", 'score': 2, 'issue_id': 4871, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '73387831285ecb12', 'authors': ['Matteo Fasulo', 'Luca Babboni', 'Luca Tedeschini'], 'affiliations': ['Department of Computer Science and Engineering (DISI) - University of Bologna'], 'pdf_title_img': 'assets/pdf/title_img/2507.11764.jpg', 'data': {'categories': ['#multilingual', '#machine_translation', '#architecture', '#low_resource', '#training'], 'emoji': '🌐', 'ru': {'title': 'Сентимент-усиленные трансформеры покоряют многоязычность', 'desc': 'Статья представляет метод улучшения классификаторов на основе трансформеров для обнаружения субъективности в многоязычных и zero-shot сценариях. Авторы интегрировали оценки сентимента, полученные от вспомогательной модели, в представления предложений. Эксперименты показали, что такой подход значительно повышает производительность, особенно F1-меру для субъективного класса. Метод продемонстрировал высокие результаты в соревновании CLEF 2025 CheckThat!, заняв первое место для греческого языка.'}, 'en': {'title': 'Boosting Subjectivity Detection with Sentiment-Enhanced Transformers', 'desc': "This paper discusses a method to improve the detection of subjective and objective sentences in news articles using advanced transformer-based classifiers. The authors enhanced these classifiers by incorporating sentiment scores from an auxiliary model, which helped in better understanding the context of sentences. They tested their approach on multiple languages, including unseen ones, to ensure the model's ability to generalize. The results showed that this sentiment-augmented method significantly improved performance, particularly in identifying subjective content, achieving top rankings for Greek."}, 'zh': {'title': '情感增强变换器提升多语言主观性检测', 'desc': '本文介绍了AI Wizards在CLEF 2025 CheckThat! Lab Task 1中的参与，旨在对新闻文章中的句子进行主观性检测。我们提出了一种增强型的变换器分类器，通过将情感分数与句子表示结合，来提高模型在单语、多语和零样本设置下的表现。实验结果表明，情感特征的整合显著提升了主观性F1分数，尤其是在希腊语中取得了第一名的优异成绩。我们还通过优化决策阈值来解决各语言间的类别不平衡问题。'}}}, {'id': 'https://huggingface.co/papers/2507.09025', 'title': 'Lizard: An Efficient Linearization Framework for Large Language Models', 'url': 'https://huggingface.co/papers/2507.09025', 'abstract': "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.", 'score': 2, 'issue_id': 4861, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '3490901c2a32da3d', 'authors': ['Chien Van Nguyen', 'Ruiyi Zhang', 'Hanieh Deilamsalehy', 'Puneet Mathur', 'Viet Dac Lai', 'Haoliang Wang', 'Jayakumar Subramanian', 'Ryan A. Rossi', 'Trung Bui', 'Nikos Vlassis', 'Franck Dernoncourt', 'Thien Huu Nguyen'], 'affiliations': ['Adobe Research', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2507.09025.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#optimization', '#training', '#long_context'], 'emoji': '🦎', 'ru': {'title': 'Lizard: эффективные языковые модели с бесконечным контекстом', 'desc': 'Lizard - это фреймворк линеаризации, который преобразует трансформерные языковые модели в субквадратичные архитектуры для эффективной генерации с бесконечным контекстом. Он использует гибридный механизм внимания, сочетающий линейное внимание с гейтингом и оконное внимание с мета-памятью. Lizard позволяет адаптивно управлять памятью, поддерживает вывод с постоянным объемом памяти и обеспечивает сильную обобщаемость по длине. Эксперименты показывают, что Lizard почти без потерь восстанавливает производительность исходной модели на стандартных задачах языкового моделирования.'}, 'en': {'title': 'Lizard: Efficient Infinite-Context Generation for Transformers', 'desc': 'Lizard is a framework designed to improve the efficiency of Transformer-based Large Language Models (LLMs) by transforming them into subquadratic architectures. It addresses the challenges of memory and computation that arise with longer context lengths by implementing a hybrid attention mechanism that approximates softmax attention while maintaining output quality. The framework incorporates a gating module for adaptive memory control, allowing for constant-memory inference and enhanced model flexibility. Experimental results demonstrate that Lizard not only preserves the performance of traditional models but also significantly enhances their capabilities on various language tasks.'}, 'zh': {'title': 'Lizard：高效无限上下文生成的新框架', 'desc': 'Lizard是一个线性化框架，旨在将基于Transformer的大型语言模型（LLMs）转变为灵活的亚二次架构，以实现高效的无限上下文生成。该框架通过引入一种亚二次注意力机制，克服了传统softmax注意力在上下文长度增加时的内存和计算瓶颈，同时保持输出质量。Lizard还结合了门控模块，支持自适应内存控制和常量内存推理，增强了模型设计的灵活性。通过混合门控线性注意力和滑动窗口注意力，Lizard能够有效捕捉长距离依赖和细粒度的局部交互，显著提升了模型性能。'}}}, {'id': 'https://huggingface.co/papers/2507.07451', 'title': 'RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.07451', 'abstract': 'RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) for large language models is an energy-intensive endeavor: training can be unstable, and the policy may gradually drift away from its pretrained weights. We present RLEP\\, -- \\,Reinforcement Learning with Experience rePlay\\, -- \\,a two-phase framework that first collects verified trajectories and then replays them during subsequent training. At every update step, the policy is optimized on mini-batches that blend newly generated rollouts with these replayed successes. By replaying high-quality examples, RLEP steers the model away from fruitless exploration, focuses learning on promising reasoning paths, and delivers both faster convergence and stronger final performance. On the Qwen2.5-Math-7B base model, RLEP reaches baseline peak accuracy with substantially fewer updates and ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%, on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our code, datasets, and checkpoints are publicly available at https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further research.', 'score': 2, 'issue_id': 4867, 'pub_date': '2025-07-10', 'pub_date_card': {'ru': '10 июля', 'en': 'July 10', 'zh': '7月10日'}, 'hash': '18c4eecefe9abb01', 'authors': ['Hongzhi Zhang', 'Jia Fu', 'Jingyuan Zhang', 'Kai Fu', 'Qi Wang', 'Fuzheng Zhang', 'Guorui Zhou'], 'affiliations': ['Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2507.07451.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#optimization', '#dataset', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'RLEP: Ускорение обучения языковых моделей через воспроизведение успешного опыта', 'desc': 'RLEP - это фреймворк обучения с подкреплением, использующий повторное воспроизведение опыта для улучшения обучения больших языковых моделей. Он фокусируется на высококачественных примерах, что приводит к более быстрой сходимости и улучшенной производительности на математических бенчмарках. RLEP использует двухфазный подход: сначала собирает проверенные траектории, а затем воспроизводит их во время последующего обучения. Этот метод позволяет избежать бесполезного исследования и сосредоточиться на перспективных путях рассуждений.'}, 'en': {'title': 'RLEP: Accelerating Learning with Experience Replay', 'desc': 'RLEP is a reinforcement learning framework designed to improve the training of large language models by utilizing experience replay. It operates in two phases: first, it collects high-quality training examples, and then it replays these examples during the training process. This method helps the model focus on successful strategies and reduces the time spent on ineffective exploration. As a result, RLEP achieves faster convergence and better performance on math-related tasks, significantly enhancing accuracy on various benchmarks.'}, 'zh': {'title': 'RLEP：高效强化学习与经验重放的结合', 'desc': 'RLEP是一种强化学习框架，结合了经验重放，旨在提高大型语言模型的训练效率。该框架通过收集经过验证的轨迹，并在后续训练中重放这些轨迹，来优化学习过程。通过重放高质量的示例，RLEP能够引导模型避免无效的探索，专注于有前景的推理路径。实验结果表明，RLEP在多个数学基准测试中显著提高了模型的准确性，且所需的更新次数大幅减少。'}}}, {'id': 'https://huggingface.co/papers/2507.05065', 'title': 'Replacing thinking with tool usage enables reasoning in small language\n  models', 'url': 'https://huggingface.co/papers/2507.05065', 'abstract': 'A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances have established a new machine learning paradigm based on scaling up compute at inference time as well as at training time. In that line of work, a combination of Supervised Fine-Tuning (SFT) on synthetic demonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is used for training Large Language Models to expend extra compute during inference in the form of "thoughts" expressed in natural language. In this paper, we propose to instead format these tokens as a multi-turn interaction trace with a stateful tool. At each turn, the new state of the tool is appended to the context of the model, whose job is to generate the tokens necessary to control the tool via a custom DSL. We benchmark this approach on the problem of repairing malfunctioning Python code, and show that this constrained setup allows for faster sampling of experience and a denser reward signal, allowing even models of size up to 3B parameters to learn how to proficiently expend additional compute on the task.', 'score': 2, 'issue_id': 4866, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 июля', 'en': 'July 7', 'zh': '7月7日'}, 'hash': '31f9493dba1b5054', 'authors': ['Corrado Rainone', 'Tim Bakker', 'Roland Memisevic'], 'affiliations': ['Qualcomm AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.05065.jpg', 'data': {'categories': ['#rl', '#optimization', '#plp', '#training', '#transfer_learning', '#benchmark'], 'emoji': '🛠️', 'ru': {'title': 'Эффективное обучение LLM через взаимодействие с инструментами', 'desc': 'Статья предлагает новый подход к обучению больших языковых моделей (LLM), форматируя токены как многоходовый след взаимодействия со статическим инструментом. Это позволяет ускорить сэмплирование и получить более плотные сигналы вознаграждения для таких задач, как исправление кода на Python. Метод использует специальный DSL для управления инструментом и добавляет новое состояние инструмента в контекст модели на каждом шаге. Эксперименты показали, что даже модели размером до 3 миллиардов параметров могут эффективно использовать дополнительные вычисления для решения задачи.'}, 'en': {'title': 'Empowering Language Models with Stateful Interaction for Enhanced Learning', 'desc': "This paper introduces a novel method for training Large Language Models (LLMs) by using a multi-turn interaction trace with a stateful tool. Instead of relying solely on traditional training methods, the approach allows the model to generate tokens that control the tool through a custom domain-specific language (DSL). This setup enhances the model's ability to learn from its interactions, particularly in tasks like repairing Python code, by providing faster sampling and richer reward signals. The results demonstrate that even smaller models, with up to 3 billion parameters, can effectively utilize additional computational resources to improve their performance."}, 'zh': {'title': '多轮交互：提升大型语言模型的学习效率', 'desc': '本文提出了一种新的方法，将令牌格式化为多轮交互轨迹，并使用有状态的工具来训练大型语言模型。这种方法可以加快采样速度，并为修复Python代码等任务提供更密集的奖励信号。通过在每一轮中将工具的新状态附加到模型的上下文中，模型能够生成控制工具所需的令牌。实验表明，即使是参数量达到30亿的模型，也能有效学习如何在任务中合理使用额外的计算资源。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment', '#architecture (5)', '#audio', '#benchmark (7)', '#cv', '#data', '#dataset (6)', '#diffusion (2)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math', '#multilingual (1)', '#multimodal (3)', '#open_source (5)', '#optimization (6)', '#plp (1)', '#rag (1)', '#reasoning (3)', '#rl (2)', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (5)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-17 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-17 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-17 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    