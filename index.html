
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. April 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 апреля</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-03.html">⬅️ <span id="prev-date">03.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-07.html">➡️ <span id="next-date">07.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 апреля', 'en': 'April 4', 'zh': '4月4日'};
        let feedDateNext = {'ru': '07.04', 'en': '04/07', 'zh': '4月7日'};
        let feedDatePrev = {'ru': '03.04', 'en': '04/03', 'zh': '4月3日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.01990', 'title': 'Advances and Challenges in Foundation Agents: From Brain-Inspired\n  Intelligence to Evolutionary, Collaborative, and Safe Systems', 'url': 'https://huggingface.co/papers/2504.01990', 'abstract': 'The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.', 'score': 79, 'issue_id': 3069, 'pub_date': '2025-03-31', 'pub_date_card': {'ru': '31 марта', 'en': 'March 31', 'zh': '3月31日'}, 'hash': 'f72a29b6411b97b1', 'authors': ['Bang Liu', 'Xinfeng Li', 'Jiayi Zhang', 'Jinlin Wang', 'Tanjin He', 'Sirui Hong', 'Hongzhang Liu', 'Shaokun Zhang', 'Kaitao Song', 'Kunlun Zhu', 'Yuheng Cheng', 'Suyuchen Wang', 'Xiaoqiang Wang', 'Yuyu Luo', 'Haibo Jin', 'Peiyan Zhang', 'Ollie Liu', 'Jiaqi Chen', 'Huan Zhang', 'Zhaoyang Yu', 'Haochen Shi', 'Boyan Li', 'Dekun Wu', 'Fengwei Teng', 'Xiaojun Jia', 'Jiawei Xu', 'Jinyu Xiang', 'Yizhang Lin', 'Tianming Liu', 'Tongliang Liu', 'Yu Su', 'Huan Sun', 'Glen Berseth', 'Jianyun Nie', 'Ian Foster', 'Logan Ward', 'Qingyun Wu', 'Yu Gu', 'Mingchen Zhuge', 'Xiangru Tang', 'Haohan Wang', 'Jiaxuan You', 'Chi Wang', 'Jian Pei', 'Qiang Yang', 'Xiaoliang Qi', 'Chenglin Wu'], 'affiliations': ['Argonne National Laboratory', 'Canada CIFAR AI Chair', 'Duke University', 'Google DeepMind', 'King Abdullah University of Science and Technology', 'MetaGPT', 'Microsoft Research Asia', 'Mila - Quebec AI Institute', 'Nanyang Technological University', 'Penn State University', 'Stanford University', 'The Hong Kong Polytechnic University', 'The Hong Kong University of Science and Technology', 'The Ohio State University', 'University of Georgia', 'University of Illinois at Urbana-Champaign', 'University of Southern California', 'University of Sydney', 'Université de Montréal', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01990.jpg', 'data': {'categories': ['#architecture', '#survey', '#security', '#ethics', '#agi', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Интеллектуальные агенты нового поколения: от нейронауки к безопасному ИИ', 'desc': 'Эта статья представляет собой всесторонний обзор интеллектуальных агентов на основе больших языковых моделей (LLM). Авторы предлагают модульную архитектуру агентов, вдохновленную человеческим мозгом, интегрируя принципы когнитивной науки, нейронауки и вычислительных исследований. Рассматриваются механизмы самосовершенствования агентов, их адаптивная эволюция и взаимодействие в многоагентных системах. Особое внимание уделяется вопросам безопасности, этики и надежности при разработке и внедрении таких систем искусственного интеллекта.'}, 'en': {'title': 'Building Intelligent Agents: From Brain-Inspired Design to Safe AI Deployment', 'desc': 'This paper explores the development of large language models (LLMs) and their role in creating advanced intelligent agents that can reason, perceive, and act in various environments. It presents a modular architecture inspired by the human brain, detailing how cognitive, perceptual, and operational modules correspond to brain functions like memory and emotion. The paper also discusses how these agents can improve themselves through adaptive learning and optimization techniques, including AutoML. Finally, it emphasizes the importance of ensuring that AI systems are safe, ethical, and reliable for real-world applications.'}, 'zh': {'title': '智能体的未来：从大脑启发到安全应用', 'desc': '本文探讨了大型语言模型（LLMs）在人工智能领域的变革性影响，强调了智能体的设计、评估和持续改进所面临的复杂挑战。我们将智能体框架置于模块化的、受大脑启发的架构中，结合了认知科学、神经科学和计算研究的原则。文章分为四个部分，首先分析智能体的模块化基础，映射其认知、感知和操作模块与人类大脑功能的相似性。接着讨论自我增强和适应性进化机制，最后强调构建安全、可靠和有益的人工智能系统的重要性。'}}}, {'id': 'https://huggingface.co/papers/2504.02826', 'title': 'Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual\n  Editing', 'url': 'https://huggingface.co/papers/2504.02826', 'abstract': 'Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.', 'score': 52, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'dbb1c07cd5a01838', 'authors': ['Xiangyu Zhao', 'Peiyuan Zhang', 'Kexian Tang', 'Hao Li', 'Zicheng Zhang', 'Guangtao Zhai', 'Junchi Yan', 'Hua Yang', 'Xue Yang', 'Haodong Duan'], 'affiliations': ['Shanghai Jiao Tong University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02826.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'RISEBench: Новый рубеж в оценке визуального редактирования с рассуждениями', 'desc': 'RISEBench - это новый эталонный тест для оценки визуального редактирования с учетом рассуждений в мультимодальных моделях. Он фокусируется на четырех типах рассуждений: временном, причинно-следственном, пространственном и логическом. Тест оценивает понимание инструкций, сохранение внешнего вида и визуальную правдоподобность с помощью как человеческих оценщиков, так и LLM-судей. Эксперименты показали, что даже современные модели, такие как GPT-4, испытывают трудности с задачами логического рассуждения.'}, 'en': {'title': 'RISEBench: Advancing Reasoning in Visual Editing', 'desc': 'This paper introduces RISEBench, a new benchmark designed to evaluate Reasoning-Informed Visual Editing (RISE) in Large Multi-modality Models (LMMs). It identifies challenges in visual editing, such as following complex instructions and maintaining appearance consistency. RISEBench categorizes reasoning into four types: Temporal, Causal, Spatial, and Logical, and provides a framework for assessing these reasoning types through both human and model evaluations. The findings indicate that even advanced models like GPT-4o-Native struggle with logical reasoning, suggesting a need for further research in this area.'}, 'zh': {'title': '推理驱动的视觉编辑新基准', 'desc': '大型多模态模型（LMMs）在视觉理解和生成方面取得了显著进展，但在通用视觉编辑中仍面临挑战，尤其是在遵循复杂指令、保持外观一致性和支持灵活输入格式方面。为了解决这一问题，我们引入了RISEBench，这是第一个用于评估推理驱动视觉编辑（RISE）的基准。RISEBench专注于四种关键推理类型：时间推理、因果推理、空间推理和逻辑推理。我们的实验表明，尽管GPT-4o-Native在性能上显著优于其他模型，但在逻辑推理任务上仍然存在困难，显示出这一领域仍需深入探索。'}}}, {'id': 'https://huggingface.co/papers/2504.02782', 'title': 'GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image\n  Generation', 'url': 'https://huggingface.co/papers/2504.02782', 'abstract': "The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.", 'score': 32, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '5346697bd326eed4', 'authors': ['Zhiyuan Yan', 'Junyan Ye', 'Weijia Li', 'Zilong Huang', 'Shenghai Yuan', 'Xiangyang He', 'Kaiqing Lin', 'Jun He', 'Conghui He', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'Shanghai AI Laboratory', 'Shenzhen University', 'Sun Yat-sen University', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02782.jpg', 'data': {'categories': ['#cv', '#open_source', '#benchmark', '#architecture', '#diffusion', '#interpretability', '#optimization', '#hallucinations'], 'emoji': '🖼️', 'ru': {'title': 'GPT-4o: Новый рубеж в генерации и редактировании изображений с помощью ИИ', 'desc': 'Статья представляет первый оценочный бенчмарк (GPT-ImgEval) для модели GPT-4o от OpenAI, анализирующий ее способности в генерации и редактировании изображений. Исследование оценивает качество генерации, мастерство редактирования и семантический синтез на основе мировых знаний, демонстрируя превосходство GPT-4o над существующими методами. Авторы также предполагают, что архитектура GPT-4o включает авторегрессионную модель в сочетании с диффузионной головкой для декодирования изображений. Кроме того, статья анализирует ограничения GPT-4o, сравнивает ее с Gemini 2.0 Flash и обсуждает вопросы безопасности, связанные с выходными данными модели.'}, 'en': {'title': 'Unleashing the Power of GPT-4o in Image Generation and Editing', 'desc': "This paper evaluates the performance of OpenAI's GPT-4o model in image generation and editing using a new benchmark called GPT-ImgEval. The evaluation focuses on three key areas: the quality of generated images, the model's ability to edit images, and its understanding of semantic context. Results show that GPT-4o outperforms existing models in both image generation and editing, while also demonstrating strong reasoning capabilities. The paper also explores the model's architecture and limitations, providing insights for future research in image generation."}, 'zh': {'title': 'GPT-4o：图像生成与编辑的新突破', 'desc': '本论文介绍了OpenAI的GPT-4o模型在图像生成和编辑方面的最新突破。我们提出了一个名为GPT-ImgEval的评估基准，定量和定性地分析了GPT-4o在生成质量、编辑能力和知识推理等三个关键维度的表现。研究表明，GPT-4o在图像生成控制和输出质量上显著优于现有方法，并展示了卓越的知识推理能力。此外，我们还探讨了GPT-4o的架构，并识别了其在图像生成中常见的合成伪影和局限性。'}}}, {'id': 'https://huggingface.co/papers/2504.02587', 'title': 'Rethinking RL Scaling for Vision Language Models: A Transparent,\n  From-Scratch Framework and Comprehensive Evaluation Scheme', 'url': 'https://huggingface.co/papers/2504.02587', 'abstract': 'Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.', 'score': 22, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '58300c3a6e30995f', 'authors': ['Yan Ma', 'Steffi Chern', 'Xuyang Shen', 'Yiran Zhong', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative Artificial Intelligence Lab (GAIR)', 'Minimax', 'SII', 'Shanghai Jiao Tong University (SJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2504.02587.jpg', 'data': {'categories': ['#rl', '#reasoning', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Прозрачное обучение с подкреплением для визуально-языковых моделей', 'desc': 'Эта статья представляет новый подход к обучению с подкреплением (RL) для визуально-языковых моделей (VLM). Авторы предлагают прозрачную и воспроизводимую систему для применения RL в VLM, включающую четырехэтапный конвейер. Они также вводят стандартизированную схему оценки для анализа динамики обучения и рефлексивного поведения моделей. Эксперименты показывают, что RL превосходит обычное обучение с учителем в задачах визуального рассуждения и обобщения.'}, 'en': {'title': 'Reinforcement Learning Revolutionizes Vision-Language Models!', 'desc': 'This paper presents a new framework for applying reinforcement learning (RL) to vision-language models (VLMs), addressing issues of reproducibility and accessibility in existing methods. The authors propose a simple four-step pipeline that can be easily validated across different models and datasets. They also introduce a standardized evaluation scheme to better assess training dynamics and reflective behaviors in VLMs. The experiments reveal that RL outperforms supervised fine-tuning in generalization, highlighting the importance of response length and reflection in visual reasoning tasks.'}, 'zh': {'title': '建立可重复的强化学习框架', 'desc': '强化学习（RL）在提升大型语言模型的推理能力方面展现出强大的潜力，并正在积极扩展到视觉语言模型（VLMs）。然而，现有的RL应用往往依赖于复杂的框架，限制了可重复性和可访问性，同时缺乏标准化的评估协议，使得结果比较和训练动态解释变得困难。本文提出了一个透明的、从零开始的RL框架，提供了一个经过多个模型和数据集验证的最小功能四步流程。此外，提出了一种标准化的评估方案，以评估训练动态和反思行为。'}}}, {'id': 'https://huggingface.co/papers/2504.02398', 'title': 'Scaling Analysis of Interleaved Speech-Text Language Models', 'url': 'https://huggingface.co/papers/2504.02398', 'abstract': 'Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.', 'score': 17, 'issue_id': 3067, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'c03d1b64b7e6e276', 'authors': ['Gallil Maimon', 'Michael Hassid', 'Amit Roth', 'Yossi Adi'], 'affiliations': ['Department of Computer Science and Engineering, Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2504.02398.jpg', 'data': {'categories': ['#dataset', '#data', '#audio', '#open_source', '#synthetic', '#transfer_learning', '#training'], 'emoji': '🎙️', 'ru': {'title': 'Эффективное масштабирование речевых моделей через инициализацию текстовыми моделями', 'desc': 'Исследование показывает, что речевые языковые модели (SLM), инициализированные с помощью предобученных текстовых моделей, масштабируются более эффективно, чем чисто речевые модели. Авторы провели анализ масштабирования таких интерлейвных SLM, обучив несколько десятков моделей и изучив тенденции. Результаты указывают на то, что при таком подходе следует выделять больше вычислительных ресурсов на увеличение размера модели, а не на увеличение объема обучающих данных. Масштабированная модель авторов достигает сопоставимой производительности с ведущими моделями при использовании меньшего количества вычислений и данных.'}, 'en': {'title': 'Interleaved SLMs: Efficient Scaling for Speech Models!', 'desc': 'This paper investigates the efficiency of interleaved Speech Language Models (SLMs) compared to traditional textless SLMs. It finds that interleaved SLMs, which leverage pre-trained Text Language Models (TextLMs), require less compute and data while achieving competitive performance on speech tasks. The authors conduct a scaling analysis that reveals distinct scaling dynamics, suggesting a need for more compute allocation towards model size rather than training data. Additionally, the study highlights the importance of synthetic data and various TextLM families in enhancing the performance of SLMs.'}, 'zh': {'title': '交错SLM：更高效的扩展之路', 'desc': '本论文探讨了交错语音语言模型（SLM）的扩展效率。研究表明，交错SLM在计算资源的使用上比无文本SLM更为高效。我们发现，交错SLM的扩展动态与无文本SLM显著不同，建议在增加模型规模时应更多地分配计算预算。最终，经过扩展的模型在语音语义指标上表现出与领先模型相当的性能，同时使用的计算和数据量更少。'}}}, {'id': 'https://huggingface.co/papers/2504.02436', 'title': 'SkyReels-A2: Compose Anything in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2504.02436', 'abstract': 'This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.', 'score': 16, 'issue_id': 3063, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '86b46513a72dbd76', 'authors': ['Zhengcong Fei', 'Debang Li', 'Di Qiu', 'Jiahua Wang', 'Yikun Dou', 'Rui Wang', 'Jingtao Xu', 'Mingyuan Fan', 'Guibin Chen', 'Yang Li', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.02436.jpg', 'data': {'categories': ['#inference', '#multimodal', '#video', '#benchmark', '#open_source'], 'emoji': '🎬', 'ru': {'title': 'Контролируемая генерация видео из отдельных элементов', 'desc': 'SkyReels-A2 - это система генерации видео, способная собирать произвольные визуальные элементы в синтезированные видео на основе текстовых подсказок. Она использует модель совместного встраивания изображений и текста для сохранения согласованности элементов и глобальной связности. Авторы оптимизировали процесс вывода для скорости и стабильности, а также создали специальный набор данных для оценки. SkyReels-A2 является первой моделью с открытым исходным кодом коммерческого уровня для генерации видео из элементов (E2V).'}, 'en': {'title': 'SkyReels-A2: Mastering Video Generation with Element Control', 'desc': 'This paper introduces SkyReels-A2, a framework for generating videos by combining various visual elements based on text descriptions. The main challenge is to keep each visual element true to its reference image while ensuring that the overall scene looks coherent and natural. To tackle this, the authors developed a data pipeline for training the model with specific triplets of prompts, references, and videos, and created a new image-text joint embedding model to enhance the generative process. The results show that SkyReels-A2 can produce high-quality, diverse videos with precise control over the elements, marking a significant advancement in the field of controllable video generation.'}, 'zh': {'title': 'SkyReels-A2：可控视频生成的新突破', 'desc': '本文介绍了SkyReels-A2，一个可控的视频生成框架，能够根据文本提示将任意视觉元素（如角色、物体、背景）组合成合成视频，同时保持与每个元素的参考图像的一致性。我们将这一任务称为元素到视频（E2V），其主要挑战在于保持每个参考元素的真实性，确保场景的连贯性，以及实现自然的输出。为了解决这些问题，我们首先设计了一个全面的数据管道，以构建提示-参考-视频三元组用于模型训练。实验表明，我们的框架能够生成多样化、高质量的视频，并实现精确的元素控制。'}}}, {'id': 'https://huggingface.co/papers/2504.00502', 'title': 'ShortV: Efficient Multimodal Large Language Models by Freezing Visual\n  Tokens in Ineffective Layers', 'url': 'https://huggingface.co/papers/2504.00502', 'abstract': "Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV", 'score': 14, 'issue_id': 3067, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': '1b236225c1d92fd7', 'authors': ['Qianhao Yuan', 'Qingyu Zhang', 'Yanjiang Liu', 'Jiawei Chen', 'Yaojie Lu', 'Hongyu Lin', 'Jia Zheng', 'Xianpei Han', 'Le Sun'], 'affiliations': ['Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.00502.jpg', 'data': {'categories': ['#optimization', '#training', '#multimodal', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация MLLM: меньше вычислений, та же эффективность', 'desc': 'Исследователи представили новый метод оценки вклада слоев в мультимодальных больших языковых моделях (MLLM) с помощью метрики Layer Contribution (LC). Они обнаружили, что многие слои MLLM минимально влияют на обработку визуальных токенов. На основе этого наблюдения был разработан метод ShortV, который идентифицирует неэффективные слои и замораживает обновления визуальных токенов в них. ShortV позволяет заморозить визуальные токены примерно в 60% слоев MLLM, значительно снижая вычислительные затраты без потери производительности.'}, 'en': {'title': 'Optimizing MLLMs: Freeze the Unnecessary Layers!', 'desc': 'This paper addresses the high computational costs associated with Multimodal Large Language Models (MLLMs) by analyzing layer-wise redundancy. It introduces a new metric called Layer Contribution (LC) to measure how much each layer affects the processing of visual and text tokens. The findings indicate that many layers contribute little to the processing of visual tokens, allowing for optimization. The authors propose a method called ShortV, which identifies and freezes these ineffective layers, resulting in significant reductions in computational costs while preserving model performance.'}, 'zh': {'title': '优化多模态模型，降低计算成本', 'desc': '多模态大型语言模型（MLLMs）由于其庞大的规模和大量的视觉标记，面临着高计算成本的问题。本文提出了一种新颖的度量标准——层贡献（Layer Contribution，LC），用于量化模型中各层对视觉和文本标记的影响。通过计算去除某层变换后模型输出的差异，LC能够评估该层的贡献。实验表明，许多层在处理视觉标记时的贡献很小，因此我们提出了ShortV方法，能够识别并冻结这些无效层，从而显著降低计算成本。'}}}, {'id': 'https://huggingface.co/papers/2504.02507', 'title': 'ZClip: Adaptive Spike Mitigation for LLM Pre-Training', 'url': 'https://huggingface.co/papers/2504.02507', 'abstract': 'Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.', 'score': 11, 'issue_id': 3065, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '290019150fe5b4c9', 'authors': ['Abhay Kumar', 'Louis Owen', 'Nilabhra Roy Chowdhury', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2504.02507.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '📊', 'ru': {'title': 'ZClip: адаптивное ограничение градиентов для стабильного обучения языковых моделей', 'desc': 'В статье представлен новый алгоритм адаптивного ограничения градиентов под названием ZClip для обучения больших языковых моделей (LLM). ZClip динамически корректирует порог ограничения на основе статистических свойств норм градиентов во времени. Алгоритм использует обнаружение аномалий на основе z-оценки для выявления и смягчения больших всплесков градиентов. ZClip помогает предотвратить вредные всплески потерь, не мешая сходимости в остальных случаях.'}, 'en': {'title': 'ZClip: Smart Gradient Clipping for Stable LLM Training', 'desc': 'This paper addresses the challenges of training large language models (LLMs) by introducing ZClip, an adaptive gradient clipping algorithm. Traditional methods often fail to manage gradient instability and loss spikes effectively, leading to inefficient training and the need for manual adjustments. ZClip improves upon these methods by dynamically adjusting the clipping threshold based on the statistical behavior of gradient norms, allowing for a more responsive training process. By using z-score-based anomaly detection, ZClip prevents harmful loss spikes while maintaining the overall convergence of the model.'}, 'zh': {'title': '自适应梯度裁剪，提升训练稳定性', 'desc': '在训练大型语言模型时，常常会遇到梯度不稳定和损失峰值等问题，这可能导致灾难性的发散。传统的梯度裁剪技术无法有效解决这些问题，因为它们依赖于固定的阈值或启发式方法，导致学习效率低下。我们提出了一种名为ZClip的自适应梯度裁剪算法，它根据梯度范数的统计特性动态调整裁剪阈值。ZClip通过基于z-score的异常检测来识别和减轻大梯度峰值，从而防止损失峰值，同时不干扰收敛过程。'}}}, {'id': 'https://huggingface.co/papers/2504.02542', 'title': 'Audio-visual Controlled Video Diffusion with Masked Selective State\n  Spaces Modeling for Natural Talking Head Generation', 'url': 'https://huggingface.co/papers/2504.02542', 'abstract': 'Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.', 'score': 9, 'issue_id': 3064, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'fa93ea3aeacd0dbc', 'authors': ['Fa-Ting Hong', 'Zunnan Xu', 'Zixiang Zhou', 'Jun Zhou', 'Xiu Li', 'Qin Lin', 'Qinglin Lu', 'Dan Xu'], 'affiliations': ['HKUST', 'Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02542.jpg', 'data': {'categories': ['#video', '#multimodal', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'Гибкий синтез говорящей головы с множественным контролем', 'desc': 'ACTalker - это новая модель для синтеза видео с говорящей головой, использующая диффузионный подход. Она поддерживает как мультимодальное, так и одномодальное управление генерацией видео. Модель использует параллельную структуру mamba с несколькими ветвями для обработки различных управляющих сигналов. ACTalker применяет механизм маскирования для предотвращения конфликтов между разными модальностями управления.'}, 'en': {'title': 'ACTalker: Multi-Signal Control for Natural Talking Head Synthesis', 'desc': 'This paper presents ACTalker, a novel framework for generating talking head videos that can be controlled by multiple signals simultaneously. Unlike traditional methods that rely on a single control modality, ACTalker employs a parallel mamba structure with multiple branches, each dedicated to a specific facial region. A gate mechanism allows for flexible control, ensuring that different driving signals can manipulate facial features without interference. The introduction of a mask-drop strategy further enhances this capability, enabling independent control of facial regions and resulting in more natural and coordinated video outputs.'}, 'zh': {'title': '多信号控制的对话头像生成新方法', 'desc': '本文介绍了一种名为ACTalker的端到端视频扩散框架，旨在生成虚拟头像的对话视频。该方法支持多信号和单信号控制，克服了现有方法的局限性。通过设计并行的mamba结构，允许不同的驱动信号控制面部的特定区域，并使用门控机制实现灵活控制。实验结果表明，ACTalker能够生成自然的面部视频，并且能够无冲突地整合多种驱动信号。'}}}, {'id': 'https://huggingface.co/papers/2504.02495', 'title': 'Inference-Time Scaling for Generalist Reward Modeling', 'url': 'https://huggingface.co/papers/2504.02495', 'abstract': 'Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.', 'score': 8, 'issue_id': 3071, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': '07408fa4b72ccb6c', 'authors': ['Zijun Liu', 'Peiyi Wang', 'Runxin Xu', 'Shirong Ma', 'Chong Ruan', 'Peng Li', 'Yang Liu', 'Yu Wu'], 'affiliations': ['DeepSeek-AI', 'Dept. of Computer Sci. & Tech., Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.02495.jpg', 'data': {'categories': ['#training', '#benchmark', '#rl', '#rlhf', '#reasoning', '#open_source', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабируемое моделирование вознаграждений для LLM с помощью самокритики', 'desc': 'Эта статья исследует улучшение моделирования вознаграждений (RM) для крупных языковых моделей (LLM) с использованием обучения с подкреплением (RL). Авторы предлагают метод Self-Principled Critique Tuning (SPCT) для улучшения генерации вознаграждений и масштабируемости моделей. Они также вводят мета-RM для руководства процессом голосования при масштабировании во время вывода. Результаты показывают, что SPCT значительно улучшает качество и масштабируемость генеративных моделей вознаграждения (GRM), превосходя существующие методы в различных тестах RM.'}, 'en': {'title': 'Enhancing Language Models with Scalable Reward Learning', 'desc': 'This paper explores advancements in reinforcement learning (RL) for enhancing large language models (LLMs) by focusing on reward modeling (RM) for general queries. The authors introduce a novel approach called Self-Principled Critique Tuning (SPCT) that improves the generation of reward signals, enabling better inference-time scalability. They also propose a pointwise generative reward modeling (GRM) technique that allows flexibility in handling different input types. Empirical results demonstrate that SPCT significantly enhances the quality and scalability of GRMs, outperforming existing methods while addressing challenges in generalist reward systems.'}, 'zh': {'title': '提升语言模型推理能力的强化学习方法', 'desc': '强化学习（RL）在大规模语言模型（LLMs）的后训练中得到了广泛应用。本文探讨了如何通过改进奖励建模（RM）来提高一般查询的推理时间可扩展性，并提出了自我原则批评调优（SPCT）方法，以促进GRM中的可扩展奖励生成行为。我们采用点对点生成奖励建模（GRM），以适应不同输入类型并实现推理时间的可扩展性。实验结果表明，SPCT显著提高了GRM的质量和可扩展性，超越了现有方法和模型。'}}}, {'id': 'https://huggingface.co/papers/2504.02119', 'title': 'Efficient Model Selection for Time Series Forecasting via LLMs', 'url': 'https://huggingface.co/papers/2504.02119', 'abstract': 'Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.', 'score': 7, 'issue_id': 3063, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '7c31e20ce0a7813b', 'authors': ['Wang Wei', 'Tiankai Yang', 'Hongjie Chen', 'Ryan A. Rossi', 'Yue Zhao', 'Franck Dernoncourt', 'Hoda Eldardiry'], 'affiliations': ['Adobe Research San Jose, CA, USA', 'Adobe Research Seattle, WA, USA', 'Department of Computer Science University of South California Los Angeles, CA, USA', 'Department of Computer Science Virginia Tech Blacksburg, VA, USA', 'Dolby Labs Atlanta, GA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2504.02119.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'LLM как эффективный инструмент выбора моделей в прогнозировании временных рядов', 'desc': 'Статья предлагает использовать большие языковые модели (LLM) для автоматизации выбора моделей в прогнозировании временных рядов. Этот подход устраняет необходимость в предварительно созданных матрицах производительности, опираясь на внутренние знания и способности рассуждения LLM. Эксперименты с моделями LLaMA, GPT и Gemini показывают, что предложенный метод превосходит традиционные техники мета-обучения и эвристические базовые линии. Результаты подчеркивают потенциал LLM в эффективном выборе моделей для прогнозирования временных рядов.'}, 'en': {'title': 'Revolutionizing Model Selection with Large Language Models', 'desc': 'This paper addresses the challenge of model selection in time series forecasting, which usually requires evaluating many models across different datasets. The authors introduce a novel approach that uses Large Language Models (LLMs) to automate this selection process without needing costly performance matrices. By leveraging the reasoning abilities of LLMs, their method simplifies the model selection task and reduces computational costs. Experimental results show that this approach outperforms traditional meta-learning methods and heuristic techniques, highlighting the effectiveness of LLMs in this domain.'}, 'zh': {'title': '利用大型语言模型优化时间序列预测的模型选择', 'desc': '本研究探讨了时间序列预测中的模型选择问题，传统方法需要在多个数据集上进行广泛的性能评估。我们提出了一种利用大型语言模型（LLMs）作为轻量级替代方案的方法，避免了构建昂贵的性能矩阵。通过与LLaMA、GPT和Gemini的广泛实验，我们的方法在性能上超越了传统的元学习技术和启发式基线，同时显著降低了计算开销。这些结果强调了LLMs在时间序列预测中高效模型选择的潜力。'}}}, {'id': 'https://huggingface.co/papers/2504.02012', 'title': 'Instruction-Guided Autoregressive Neural Network Parameter Generation', 'url': 'https://huggingface.co/papers/2504.02012', 'abstract': "Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.", 'score': 6, 'issue_id': 3065, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'cbdd586ccd2b682d', 'authors': ['Soro Bedionita', 'Bruno Andreis', 'Song Chong', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai, South Korea', 'KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2504.02012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'IGPG: Универсальный генератор параметров нейросетей для различных задач и архитектур', 'desc': 'IGPG - это авторегрессивная система для генерации параметров нейронных сетей, основанная на VQ-VAE и авторегрессивной модели. Она создает параметры на основе описания задачи, датасета и архитектуры сети, обеспечивая согласованность между слоями. IGPG работает на уровне токенов, эффективно охватывая сложные распределения параметров из широкого спектра предобученных моделей. Эксперименты показывают, что IGPG превосходит современные методы по масштабируемости и эффективности для крупных архитектур.'}, 'en': {'title': 'IGPG: Unifying Neural Network Parameter Generation for Enhanced Adaptability', 'desc': 'This paper introduces IGPG (Instruction Guided Parameter Generation), a new framework for generating neural network parameters based on task descriptions and architecture specifications. Unlike previous methods, IGPG uses an autoregressive approach that ensures coherence between layers and adapts efficiently to different models and datasets. By employing a VQ-VAE and generating weights at the token level, IGPG captures complex parameter distributions from various pretrained models. The results show that IGPG outperforms existing methods in scalability and efficiency, making it a valuable tool for model adaptation and fine-tuning.'}, 'zh': {'title': 'IGPG：灵活的神经网络参数生成工具', 'desc': '本文提出了一种名为IGPG（指令引导参数生成）的自回归框架，旨在生成神经网络参数，以适应不同的任务描述和架构规范。IGPG通过结合VQ-VAE和自回归模型，能够在多种任务和架构中统一参数合成，确保层间一致性。该方法在生成神经网络权重时，采用了基于token的生成方式，有效捕捉来自多种预训练模型的复杂参数分布。实验结果表明，IGPG在多个视觉数据集上表现出色，尤其在大规模架构的可扩展性和效率方面，超越了现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2503.22444', 'title': 'Scaling Laws in Scientific Discovery with AI and Robot Scientists', 'url': 'https://huggingface.co/papers/2503.22444', 'abstract': "Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.", 'score': 6, 'issue_id': 3069, 'pub_date': '2025-03-28', 'pub_date_card': {'ru': '28 марта', 'en': 'March 28', 'zh': '3月28日'}, 'hash': 'c2d75e49d08273c1', 'authors': ['Pengsong Zhang', 'Heng Zhang', 'Huazhe Xu', 'Renjun Xu', 'Zhenting Wang', 'Cong Wang', 'Animesh Garg', 'Zhibin Li', 'Arash Ajoudani', 'Xinyu Liu'], 'affiliations': ['Georgia Tech', 'Harvard University', 'Istituto Italiano di Tecnologia', 'Rutgers University', 'Tsinghua University', 'Universita di Genova', 'University College of London', 'University of Toronto', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.22444.jpg', 'data': {'categories': ['#agents', '#robotics', '#agi', '#science'], 'emoji': '🤖', 'ru': {'title': 'Автономный учёный-универсал: революция в научных открытиях', 'desc': 'Статья представляет концепцию автономного учёного-универсала (AGS), объединяющего агентный ИИ и воплощенную робототехнику для автоматизации всего цикла научных исследований. Система AGS может взаимодействовать с физической и виртуальной средой, интегрируя знания из различных научных дисциплин. Предполагается, что внедрение AGS значительно сократит время и ресурсы, необходимые для научных открытий. Авторы предполагают, что с развитием таких систем научные открытия могут подчиняться новым законам масштабирования, зависящим от количества и возможностей автономных систем.'}, 'en': {'title': 'Revolutionizing Science with Autonomous Generalist Scientists', 'desc': 'This paper introduces the concept of an Autonomous Generalist Scientist (AGS) that combines artificial intelligence and robotics to automate the entire scientific research process. The AGS can interact with both physical and virtual environments, facilitating knowledge integration across various scientific fields. By automating tasks such as literature review, hypothesis generation, experimentation, and manuscript writing, the AGS aims to significantly reduce the time and resources required for scientific discovery. The authors suggest that as these systems become more integrated into research, they could change how knowledge is generated and evolve, potentially leading to new scaling laws in scientific discovery.'}, 'zh': {'title': '自主科学家：加速科学发现的未来', 'desc': '这篇论文提出了一种自主通用科学家（AGS）的概念，结合了智能代理AI和具身机器人，旨在自动化整个研究生命周期。该系统能够动态与物理和虚拟环境互动，并促进不同科学学科之间的知识整合。通过在文献回顾、假设生成、实验和论文写作等研究阶段应用这些技术，AGS希望显著减少科学发现所需的时间和资源。随着这些自主系统越来越多地融入研究过程，科学发现可能会遵循新的规模法则，提供关于知识生成和演变的新视角。'}}}, {'id': 'https://huggingface.co/papers/2504.01871', 'title': 'Interpreting Emergent Planning in Model-Free Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.01871', 'abstract': "We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL", 'score': 5, 'issue_id': 3069, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': 'bf12d8cbe28bb942', 'authors': ['Thomas Bush', 'Stephen Chung', 'Usman Anwar', 'Adrià Garriga-Alonso', 'David Krueger'], 'affiliations': ['FAR AI', 'Mila, University of Montreal', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2504.01871.jpg', 'data': {'categories': ['#interpretability', '#reasoning', '#benchmark', '#rl', '#games', '#agents'], 'emoji': '🧠', 'ru': {'title': 'Агенты обучения с подкреплением без модели способны к планированию', 'desc': 'Исследователи представили первое механистическое доказательство того, что агенты обучения с подкреплением без модели могут научиться планировать. Они применили методологию, основанную на интерпретируемости концепций, к агенту без модели в игре Sokoban. Было продемонстрировано, что агент DRC использует выученные концептуальные представления для внутреннего формулирования планов, которые предсказывают долгосрочные эффекты действий и влияют на выбор действий. Методология включала зондирование релевантных для планирования концепций, исследование формирования планов в представлениях агента и проверку причинно-следственной связи обнаруженных планов с поведением агента через вмешательства.'}, 'en': {'title': 'Unveiling Planning in Model-Free Reinforcement Learning Agents', 'desc': 'This paper provides evidence that model-free reinforcement learning agents can learn to plan by using concept-based interpretability. The authors focus on a model-free agent called DRC, which learns to create internal plans that predict the outcomes of its actions in the Sokoban environment. They explore how the agent identifies relevant concepts, forms plans, and how these plans affect its behavior through interventions. The study reveals that as the agent develops planning capabilities, it also shows improved performance with additional computational resources, resembling advanced search algorithms.'}, 'zh': {'title': '无模型强化学习中的规划能力', 'desc': '本文首次提供了无模型强化学习代理能够学习规划的机制性证据。我们通过在Sokoban这一常用基准上应用基于概念的可解释性方法，展示了DRC代理如何利用学习到的概念表示来内部制定计划。具体而言，代理能够预测行动对环境的长期影响，并影响行动选择。我们的研究表明，代理的计划形成与其行为之间存在因果关系，并且这种计划的出现与代理在测试时利用额外计算能力的能力相吻合。'}}}, {'id': 'https://huggingface.co/papers/2504.00891', 'title': 'GenPRM: Scaling Test-Time Compute of Process Reward Models via\n  Generative Reasoning', 'url': 'https://huggingface.co/papers/2504.00891', 'abstract': 'Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.', 'score': 5, 'issue_id': 3066, 'pub_date': '2025-04-01', 'pub_date_card': {'ru': '1 апреля', 'en': 'April 1', 'zh': '4月1日'}, 'hash': 'b22a54f43f9d7a89', 'authors': ['Jian Zhao', 'Runze Liu', 'Kaiyan Zhang', 'Zhimu Zhou', 'Junqi Gao', 'Dong Li', 'Jiafei Lyu', 'Zhouyi Qian', 'Biqing Qi', 'Xiu Li', 'Bowen Zhou'], 'affiliations': ['BUPT', 'Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2504.00891.jpg', 'data': {'categories': ['#training', '#optimization', '#math', '#reasoning', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'GenPRM: Новая парадигма контроля процесса рассуждений в больших языковых моделях', 'desc': 'Статья представляет GenPRM - генеративную модель вознаграждения процесса, которая использует рассуждения по цепочке мыслей и верификацию кода для оценки каждого шага рассуждений. Авторы предлагают методы относительной оценки прогресса и синтеза обоснований для получения качественных обучающих данных. Эксперименты показывают, что GenPRM значительно превосходит предыдущие модели PRM на нескольких задачах математических рассуждений, используя всего 23 тысячи обучающих примеров. Модель также демонстрирует способность выступать в роли критика для улучшения политики в больших языковых моделях.'}, 'en': {'title': 'GenPRM: Elevating LLMs with Generative Process Reward Models', 'desc': 'This paper presents GenPRM, a generative process reward model designed to improve the performance of large language models (LLMs) by addressing key challenges faced by existing process reward models (PRMs). GenPRM utilizes Chain-of-Thought (CoT) reasoning and incorporates code verification to enhance the quality of its judgments at each reasoning step. The authors introduce a novel method called Relative Progress Estimation (RPE) to generate high-quality supervision labels and rationale data, leading to significant performance improvements on various reasoning tasks. Experimental results demonstrate that GenPRM outperforms previous PRMs and shows strong capabilities as a critic model for refining policy models, establishing a new approach for process supervision in LLMs.'}, 'zh': {'title': '生成性过程奖励模型：提升LLMs的新范式', 'desc': '最近，大型语言模型（LLMs）的进展表明，使用过程奖励模型（PRMs）作为验证器可以提升LLMs的性能。然而，当前的PRMs面临三个主要挑战：有限的过程监督和泛化能力、依赖于标量值预测而未利用LLMs的生成能力，以及无法扩展PRMs的测试时间计算。本文提出了GenPRM，这是一种生成性过程奖励模型，通过代码验证进行明确的思维链推理，然后对每个推理步骤进行判断。实验结果表明，GenPRM在多个数学推理任务上显著优于之前的PRMs，展示了其作为政策模型精炼的批评模型的强大能力。'}}}, {'id': 'https://huggingface.co/papers/2504.02821', 'title': 'Sparse Autoencoders Learn Monosemantic Features in Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2504.02821', 'abstract': 'Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.', 'score': 4, 'issue_id': 3069, 'pub_date': '2025-04-03', 'pub_date_card': {'ru': '3 апреля', 'en': 'April 3', 'zh': '4月3日'}, 'hash': 'd4568f020b5f2eca', 'authors': ['Mateusz Pach', 'Shyamgopal Karthik', 'Quentin Bouniot', 'Serge Belongie', 'Zeynep Akata'], 'affiliations': ['Helmholtz Munich', 'Munich Center of Machine Learning', 'Munich Data Science Institute', 'Technical University of Munich', 'University of Copenhagen', 'University of Tubingen'], 'pdf_title_img': 'assets/pdf/title_img/2504.02821.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Разреженные автоэнкодеры раскрывают потенциал мультимодальных моделей', 'desc': 'Исследователи применили разреженные автоэнкодеры (SAE) к мультимодальным моделям, объединяющим зрение и язык (VLM). Эксперименты показали, что SAE значительно улучшают моносемантичность отдельных нейронов в VLM и формируют иерархические представления, соответствующие экспертным таксономиям. Авторы продемонстрировали возможность управления выходными данными мультимодальных языковых моделей путем вмешательства в энкодер зрения CLIP с помощью SAE. Результаты подчеркивают эффективность SAE для повышения интерпретируемости и управляемости VLM.'}, 'en': {'title': 'Enhancing Vision-Language Models with Sparse Autoencoders', 'desc': 'This paper explores the use of Sparse Autoencoders (SAEs) to improve the interpretability and steerability of Vision-Language Models (VLMs) like CLIP. The authors present a framework to assess how well these models represent single concepts, known as monosemanticity. Their experiments show that SAEs can enhance the clarity of individual neurons in VLMs and align these representations with established expert categories. Importantly, they demonstrate that SAEs can influence the output of multimodal language models without changing the original model architecture.'}, 'zh': {'title': '稀疏自编码器提升视觉-语言模型的可解释性与操控性', 'desc': '稀疏自编码器（SAEs）最近被证明可以提高大型语言模型（LLMs）的可解释性和可操控性。本文将SAEs的应用扩展到视觉-语言模型（VLMs），如CLIP，并引入了一个全面的框架来评估视觉表示的单义性。实验结果表明，在VLMs上训练的SAEs显著增强了单个神经元的单义性，并展示了与专家定义结构（如iNaturalist分类法）良好对齐的层次表示。最重要的是，我们证明了将SAEs应用于CLIP视觉编码器，可以直接操控多模态LLMs（如LLaVA）的输出，而无需对基础模型进行任何修改。'}}}, {'id': 'https://huggingface.co/papers/2504.02154', 'title': 'FreSca: Unveiling the Scaling Space in Diffusion Models', 'url': 'https://huggingface.co/papers/2504.02154', 'abstract': "Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.", 'score': 1, 'issue_id': 3074, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 апреля', 'en': 'April 2', 'zh': '4月2日'}, 'hash': '1c4d8a95379fa9a1', 'authors': ['Chao Huang', 'Susan Liang', 'Yunlong Tang', 'Li Ma', 'Yapeng Tian', 'Chenliang Xu'], 'affiliations': ['Netflix Eyeline Studios', 'The University of Texas at Dallas', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.02154.jpg', 'data': {'categories': ['#cv', '#dataset', '#diffusion', '#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'Частотное масштабирование для улучшения контроля диффузионных моделей', 'desc': 'Статья исследует потенциал пространства масштабирования в диффузионных моделях для точного семантического манипулирования изображениями. Авторы проводят Фурье-анализ предсказаний шума и обнаруживают, что низко- и высокочастотные компоненты развиваются по-разному в процессе диффузии. На основе этого наблюдения они предлагают метод FreSca, который применяет масштабирование управления независимо к различным частотным диапазонам в частотной области. FreSca улучшает существующие методы редактирования изображений без переобучения и показывает эффективность в задачах понимания изображений, таких как оценка глубины.'}, 'en': {'title': 'Unlocking Image Control with Frequency-Based Scaling', 'desc': "This paper explores the potential of diffusion models in image tasks, focusing on how noise predictions can be manipulated for better control. It introduces a concept called 'scaling space' that allows for fine-tuned semantic editing by analyzing the differences in noise predictions. The authors present FreSca, a novel method that applies guidance scaling to different frequency components of noise in the Fourier domain. This approach not only improves image editing techniques but also enhances performance in image understanding tasks like depth estimation across various datasets."}, 'zh': {'title': '探索扩散模型的缩放空间', 'desc': '扩散模型在图像任务中提供了出色的可控性，主要通过噪声预测来编码特定任务的信息，并通过无分类器引导实现可调缩放。本文探讨了这种缩放机制所定义的“缩放空间”，并重点研究了基于反演的编辑方法，其中条件和无条件噪声预测之间的差异携带了重要的语义信息。我们通过对噪声预测的傅里叶分析，发现其低频和高频成分在扩散过程中以不同方式演变。基于这一见解，我们提出了FreSca方法，能够在傅里叶域中独立地对不同频带应用引导缩放，从而显著提升现有的图像编辑方法。'}}}, {'id': 'https://huggingface.co/papers/2503.23542', 'title': 'Whisper-LM: Improving ASR Models with Language Models for Low-Resource\n  Languages', 'url': 'https://huggingface.co/papers/2503.23542', 'abstract': 'Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\\% for in-distribution datasets and up to 34\\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.', 'score': 1, 'issue_id': 3072, 'pub_date': '2025-03-30', 'pub_date_card': {'ru': '30 марта', 'en': 'March 30', 'zh': '3月30日'}, 'hash': 'c8ef94a1b05fc918', 'authors': ['Xabier de Zuazo', 'Eva Navas', 'Ibon Saratxaga', 'Inma Hernáez Rioja'], 'affiliations': ['HiTZ - University of the Basque Country - UPV/EHU, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2503.23542.jpg', 'data': {'categories': ['#training', '#inference', '#multilingual', '#dataset', '#low_resource', '#open_source'], 'emoji': '🗣️', 'ru': {'title': 'Улучшение распознавания речи для редких языков с помощью языковых моделей', 'desc': 'Исследование посвящено улучшению работы систем автоматического распознавания речи для малоресурсных языков. Авторы интегрируют традиционные и новые языковые модели с тонко настроенными моделями Whisper. Эксперименты показали значительное снижение частоты ошибок в словах, особенно для языков с ограниченными ресурсами. Подход сочетает преимущества предобученной модели Whisper с адаптивностью языковых моделей, открывая путь к более инклюзивным технологиям распознавания речи.'}, 'en': {'title': 'Enhancing ASR for Minority Languages with Whisper and Language Models', 'desc': 'This paper discusses advancements in automatic speech recognition (ASR) systems, particularly focusing on multilingual and multitask models like Whisper. It highlights the challenges these models face with minority languages and proposes a solution by combining traditional and novel language models with fine-tuned Whisper models. The study shows significant improvements in word error rates for low-resource languages through rigorous fine-tuning and evaluation. The results indicate that optimized language model parameters can enhance ASR performance across various linguistic contexts, paving the way for more inclusive ASR technologies.'}, 'zh': {'title': '提升少数语言语音识别的包容性', 'desc': '本研究探讨了如何提高自动语音识别系统在少数语言中的表现，尤其是通过结合传统和新型语言模型与微调的Whisper模型。尽管Whisper在多语言处理上表现出色，但在处理少数语言时仍存在不足。我们通过严格的微调和多数据集评估，显著降低了词错误率，尤其是在资源匮乏的情况下。研究结果表明，优化语言模型参数对提升模型性能至关重要，从而推动了更具包容性的语音识别技术的发展。'}}}, {'id': 'https://huggingface.co/papers/2503.23162', 'title': 'NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact\n  3D Representations', 'url': 'https://huggingface.co/papers/2503.23162', 'abstract': '3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.', 'score': 1, 'issue_id': 3073, 'pub_date': '2025-03-29', 'pub_date_card': {'ru': '29 марта', 'en': 'March 29', 'zh': '3月29日'}, 'hash': '33d8d348d9e9fccf', 'authors': ['Zhenyu Tang', 'Chaoran Feng', 'Xinhua Cheng', 'Wangbo Yu', 'Junwu Zhang', 'Yuan Liu', 'Xiaoxiao Long', 'Wenping Wang', 'Li Yuan'], 'affiliations': ['Hong Kong University of Science and Technology', 'Peking University', 'Texas A&M University'], 'pdf_title_img': 'assets/pdf/title_img/2503.23162.jpg', 'data': {'categories': ['#optimization', '#inference', '#dataset', '#3d'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие 3D гауссовых сплаттингов с помощью нейронных полей', 'desc': 'Статья представляет NeuralGS - метод сжатия 3D гауссовых сплаттингов (3DGS) с использованием нейронных полей. Авторы применяют многослойные перцептроны (MLP) для кодирования атрибутов 3D гауссианов, что позволяет значительно уменьшить размер модели без потери качества визуализации. Метод использует стратегию кластеризации и подгонки гауссианов с разными небольшими MLP для каждого кластера. Эксперименты показывают 45-кратное среднее уменьшение размера модели при сохранении визуального качества.'}, 'en': {'title': 'NeuralGS: Compact 3D Gaussian Compression with Neural Fields', 'desc': 'This paper introduces NeuralGS, a novel method for compressing 3D Gaussian Splatting (3DGS) representations into a more compact form without relying on complex voxel structures. By leveraging neural fields and Multi-Layer Perceptron (MLP) networks, NeuralGS can effectively encode the attributes of 3D Gaussians while significantly reducing storage requirements. The approach involves clustering Gaussians and fitting them with small MLPs based on their importance scores, leading to a remarkable 45-times reduction in model size. Experimental results show that NeuralGS maintains high visual quality and achieves compression performance comparable to existing methods that use Scaffold-GS.'}, 'zh': {'title': '用神经场压缩3D高斯点云的创新方法', 'desc': '3D高斯点云（3DGS）在质量和渲染速度上表现优异，但其存储和传输成本高昂。本文提出了一种名为NeuralGS的简单有效的方法，通过神经场表示来压缩原始3DGS，避免了复杂的体素结构和量化策略。我们利用多层感知器（MLP）神经网络对3D高斯的属性进行编码，仅需少量存储空间，即使在大规模场景中也能保持良好的视觉质量。实验结果表明，我们的方法在模型大小上平均减少了45倍，压缩性能与现有的体素基础压缩方法相当，展示了直接使用神经场压缩3DGS的巨大潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (2)', '#alignment', '#architecture (4)', '#audio (1)', '#benchmark (6)', '#cv (5)', '#data (1)', '#dataset (5)', '#diffusion (3)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (3)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (5)', '#open_source (6)', '#optimization (11)', '#plp', '#rag', '#reasoning (6)', '#rl (3)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (9)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-04 14:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-04 14:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-04 14:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    