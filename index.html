
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (18 статей)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #03dac6;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 1.6em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }

        .svg-container span {
            position: relative;
            z-index: 1;
        }

        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["минуту", "минуты", "минут"],
            hour: ["час", "часа", "часов"],
            day: ["день", "дня", "дней"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return 'только что';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} назад`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} назад`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} назад`;
        }
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "статей";
        } else if (lastDigit === 1) {
            word = "статья";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "статьи";
        } else {
            word = "статей";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">хф дэйли</h1>
            <p>14 октября | 18 статей</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 Сортировка по</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">🔍 Фильтр</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">градиент обреченный</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф дэйли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.08565', 'title': 'Baichuan-Omni Technical Report', 'url': 'https://huggingface.co/papers/2410.08565', 'abstract': 'The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Baichuan-Omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction.', 'score': 58, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'Статья представляет Baichuan-Omni - первую открытую мультимодальную языковую модель на 7 миллиардов параметров. Модель способна одновременно обрабатывать изображения, видео, аудио и текст, обеспечивая продвинутый интерактивный опыт. Авторы предлагают эффективную схему мультимодального обучения в два этапа: мультимодальное выравнивание и мультизадачная донастройка. Baichuan-Omni демонстрирует высокие результаты на различных мультимодальных бенчмарках.', 'tags': ['#мультимодальнаяМодель', '#Baichuan-Omni', '#открытыйИсходныйКод'], 'categories': ['#multimodal', '#nlp', '#cv', '#audio', '#benchmark'], 'emoji': '🌐', 'title': 'Baichuan-Omni: открытая мультимодальная модель для обработки текста, изображений, видео и аудио'}}, {'id': 'https://huggingface.co/papers/2410.08261', 'title': 'Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis', 'url': 'https://huggingface.co/papers/2410.08261', 'abstract': "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing 1024 times 1024 resolution images.", 'score': 35, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Meissonic - это новая модель для генерации изображений по текстовому описанию, основанная на подходе masked image modeling (MIM). Она сочетает архитектурные инновации, продвинутые стратегии позиционного кодирования и оптимизированные условия сэмплирования для улучшения производительности и эффективности MIM. Meissonic использует качественные обучающие данные, микро-условия на основе оценок предпочтений человека и слои сжатия признаков для повышения качества и разрешения изображений. Модель демонстрирует результаты на уровне или превосходящие современные диффузионные модели вроде SDXL.', 'tags': ['#masked_image_modeling', '#text_to_image', '#high_resolution_generation'], 'categories': ['#cv', '#multimodal', '#benchmark', '#code'], 'emoji': '🎨', 'title': 'Meissonic: Революция в генерации изображений с MIM'}}, {'id': 'https://huggingface.co/papers/2410.06456', 'title': 'From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning', 'url': 'https://huggingface.co/papers/2410.06456', 'abstract': 'Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning. Our code are available at https://github.com/baiyang4/VITask.', 'score': 25, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'VITask - новый фреймворк для улучшения адаптации крупных визуально-языковых моделей (VLM) к конкретным задачам. Он использует три стратегии: примерное подсказывание, выравнивание распределения ответов и контрастную настройку ответов. VITask интегрирует специализированные модели для конкретных задач, чтобы улучшить производительность VLM. Эксперименты на 12 наборах данных медицинской диагностики показали превосходство VITask над обычными VLM и специализированными моделями.', 'tags': ['#VITask', '#медицинская_диагностика', '#адаптация_моделей'], 'categories': ['#multimodal', '#nlp', '#cv', '#benchmark', '#code'], 'emoji': '🔬', 'title': 'VITask: Преодоление разрыва между предобучением и тонкой настройкой в визуально-языковых моделях'}}, {'id': 'https://huggingface.co/papers/2410.07133', 'title': 'EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.07133', 'abstract': 'Recent advancements in generation models have showcased remarkable capabilities in generating fantastic content. However, most of them are trained on proprietary high-quality data, and some models withhold their parameters and only provide accessible application programming interfaces (APIs), limiting their benefits for downstream tasks. To explore the feasibility of training a text-to-image generation model comparable to advanced models using publicly available resources, we introduce EvolveDirector. This framework interacts with advanced models through their public APIs to obtain text-image data pairs to train a base model. Our experiments with extensive data indicate that the model trained on generated data of the advanced model can approximate its generation capability. However, it requires large-scale samples of 10 million or more. This incurs significant expenses in time, computational resources, and especially the costs associated with calling fee-based APIs. To address this problem, we leverage pre-trained large vision-language models (VLMs) to guide the evolution of the base model. VLM continuously evaluates the base model during training and dynamically updates and refines the training dataset by the discrimination, expansion, deletion, and mutation operations. Experimental results show that this paradigm significantly reduces the required data volume. Furthermore, when approaching multiple advanced models, EvolveDirector can select the best samples generated by them to learn powerful and balanced abilities. The final trained model Edgen is demonstrated to outperform these advanced models. The code and model weights are available at https://github.com/showlab/EvolveDirector.', 'score': 15, 'issue_id': 90, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Статья представляет EvolveDirector - фреймворк для обучения модели генерации изображений по тексту, сопоставимой с продвинутыми моделями, но используя только общедоступные ресурсы. Авторы используют API существующих моделей для получения пар текст-изображение и обучения базовой модели. Для уменьшения необходимого объема данных применяются предобученные мультимодальные модели, которые оценивают и корректируют датасет в процессе обучения. Результатом стала модель Edgen, превосходящая по возможностям исходные продвинутые модели.', 'tags': ['#text2image', '#modelDistillation', '#datasetEvolution'], 'categories': ['#multimodal', '#cv', '#nlp', '#code', '#benchmark'], 'emoji': '🎨', 'title': 'Эволюционное обучение генеративных моделей на публичных данных'}}, {'id': 'https://huggingface.co/papers/2410.08815', 'title': 'StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization', 'url': 'https://huggingface.co/papers/2410.08815', 'abstract': 'Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.', 'score': 12, 'issue_id': 92, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'StructRAG - новый фреймворк для улучшения работы больших языковых моделей в задачах, требующих интенсивного использования знаний. Он преодолевает ограничения существующих методов RAG, структурируя исходную информацию оптимальным для конкретной задачи образом. StructRAG идентифицирует наиболее подходящий тип структуры, реконструирует исходные документы в этот формат и делает выводы на основе полученной структуры. Эксперименты показывают, что StructRAG достигает передовых результатов, особенно в сложных сценариях.', 'tags': ['#structrag', '#knowledge_intensive_reasoning', '#structured_knowledge'], 'categories': ['#nlp', '#rag', '#benchmark'], 'emoji': '🧠', 'title': 'StructRAG: структурированное знание для эффективного рассуждения'}}, {'id': 'https://huggingface.co/papers/2410.07035', 'title': 'PositionID: LLMs can Control Lengths, Copy and Paste with Explicit Positional Awareness', 'url': 'https://huggingface.co/papers/2410.07035', 'abstract': "Large Language Models (LLMs) demonstrate impressive capabilities across various domains, including role-playing, creative writing, mathematical reasoning, and coding. Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations. We identify this issue as stemming from a lack of positional awareness and propose novel approaches--PositionID Prompting and PositionID Fine-Tuning--to address it. These methods enhance the model's ability to continuously monitor and manage text length during generation. Additionally, we introduce PositionID CP Prompting to enable LLMs to perform copy and paste operations accurately. Furthermore, we develop two benchmarks for evaluating length control and copy-paste abilities. Our experiments demonstrate that our methods significantly improve the model's adherence to length constraints and copy-paste accuracy without compromising response quality.", 'score': 11, 'issue_id': 91, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'Исследователи предлагают новые методы для улучшения контроля длины текста в больших языковых моделях (LLM). Они вводят PositionID Prompting и PositionID Fine-Tuning для повышения позиционной осведомленности моделей. Также представлен метод PositionID CP Prompting для выполнения операций копирования и вставки. Разработаны два новых бенчмарка для оценки контроля длины и возможностей копирования-вставки.', 'tags': ['#позиционное_внедрение', '#контроль_длины_текста', '#копирование_вставка_в_LLM'], 'categories': ['#nlp', '#benchmark', '#rlhf'], 'emoji': '📏', 'title': 'Точный контроль длины текста в LLM с помощью позиционной осведомленности'}}, {'id': 'https://huggingface.co/papers/2410.09008', 'title': 'SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights', 'url': 'https://huggingface.co/papers/2410.09008', 'abstract': "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm", 'score': 11, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'SuperCorrect - это новый двухэтапный фреймворк для улучшения математических рассуждений маленьких языковых моделей. Он использует большую модель-учителя для руководства и коррекции процессов рассуждения и рефлексии меньшей модели-ученика. На первом этапе извлекаются иерархические шаблоны мыслей из модели-учителя. На втором этапе применяется кросс-модельная оптимизация прямых предпочтений для улучшения способностей модели-ученика к самокоррекции.', 'tags': ['#математическиеРассуждения', '#двухэтапноеОбучение', '#кросс-модельнаяОптимизация'], 'categories': ['#nlp', '#rlhf', '#benchmark', '#code', '#reasoning'], 'emoji': '🧮', 'title': "SuperCorrect: Подход 'учитель-ученик' для улучшения математических рассуждений малых языковых моделей"}}, {'id': 'https://huggingface.co/papers/2410.09009', 'title': 'Semantic Score Distillation Sampling for Compositional Text-to-3D Generation', 'url': 'https://huggingface.co/papers/2410.09009', 'abstract': 'Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: https://github.com/YangLing0818/SemanticSDS-3D', 'score': 10, 'issue_id': 90, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'В статье представлен новый подход к генерации 3D-контента на основе текстовых описаний - Semantic Score Distillation Sampling (SemanticSDS). Метод использует семантические эмбеддинги для улучшения выразительности и точности композиционной генерации 3D-объектов. SemanticSDS трансформирует эмбеддинги в семантическую карту, которая направляет процесс оптимизации для каждого региона изображения. Эксперименты показывают, что предложенный подход позволяет создавать сложные 3D-сцены и объекты высокого качества.', 'tags': ['#3D-генерация', '#семантические_эмбеддинги', '#score_distillation_sampling'], 'categories': ['#cv', '#3d', '#diffusion', '#text2image', '#code'], 'emoji': '🎨', 'title': 'SemanticSDS: Точный контроль над генерацией сложных 3D-сцен'}}, {'id': 'https://huggingface.co/papers/2410.07656', 'title': 'Mechanistic Permutability: Match Features Across Layers', 'url': 'https://huggingface.co/papers/2410.07656', 'abstract': 'Understanding how features evolve across layers in deep neural networks is a fundamental challenge in mechanistic interpretability, particularly due to polysemanticity and feature superposition. While Sparse Autoencoders (SAEs) have been used to extract interpretable features from individual layers, aligning these features across layers has remained an open problem. In this paper, we introduce SAE Match, a novel, data-free method for aligning SAE features across different layers of a neural network. Our approach involves matching features by minimizing the mean squared error between the folded parameters of SAEs, a technique that incorporates activation thresholds into the encoder and decoder weights to account for differences in feature scales. Through extensive experiments on the Gemma 2 language model, we demonstrate that our method effectively captures feature evolution across layers, improving feature matching quality. We also show that features persist over several layers and that our approach can approximate hidden states across layers. Our work advances the understanding of feature dynamics in neural networks and provides a new tool for mechanistic interpretability studies.', 'score': 8, 'issue_id': 96, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет новый метод SAE Match для выравнивания признаков, извлеченных разреженными автоэнкодерами (SAE), между слоями нейронной сети. Метод минимизирует среднеквадратичную ошибку между свернутыми параметрами SAE, учитывая пороги активации. Эксперименты на языковой модели Gemma 2 показывают эффективность метода в отслеживании эволюции признаков. Исследование также демонстрирует, что признаки сохраняются на протяжении нескольких слоев.', 'tags': ['#SAEMatch', '#MechanisticInterpretability', '#FeatureAlignment'], 'categories': ['#nlp', '#interpretability', '#featureExtraction'], 'emoji': '🔍', 'title': 'SAE Match: Новый взгляд на эволюцию признаков в глубоких нейронных сетях'}}, {'id': 'https://huggingface.co/papers/2410.08102', 'title': 'Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining', 'url': 'https://huggingface.co/papers/2410.08102', 'abstract': 'Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain of 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.', 'score': 8, 'issue_id': 93, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Представлен новый механизм многоагентного совместного отбора данных для предобучения больших языковых моделей (LLM). Каждый метод отбора данных выступает в роли независимого агента, а консоль агентов динамически интегрирует информацию от всех агентов в процессе обучения LLM. Экспериментальные результаты показывают значительное повышение эффективности использования данных и ускорение сходимости при обучении LLM. Предложенный подход достигает среднего прироста производительности в 10.5% по сравнению с современными методами на нескольких эталонных тестах языковых моделей.', 'tags': ['#многоагентный_отбор_данных', '#эффективность_предобучения', '#оптимизация_LLM'], 'categories': ['#nlp', '#dataset', '#benchmark'], 'emoji': '🤖', 'title': 'Многоагентный подход к оптимизации данных для LLM'}}, {'id': 'https://huggingface.co/papers/2410.08391', 'title': 'KV Prediction for Improved Time to First Token', 'url': 'https://huggingface.co/papers/2410.08391', 'abstract': "Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of 15%-50% across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to 30% on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .", 'score': 7, 'issue_id': 91, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет новый метод под названием KV Prediction для ускорения вывода трансформерных языковых моделей. Метод использует вспомогательную модель для аппроксимации KV-кэша основной модели, что значительно сокращает время до первого токена (TTFT). Авторы демонстрируют улучшение точности на 15-50% на наборе данных TriviaQA и до 30% на HumanEval при фиксированном бюджете FLOPS для TTFT. Эксперименты на CPU Apple M2 Pro подтверждают ускорение TTFT на реальном оборудовании.', 'tags': ['#KVPrediction', '#TimeToFirstToken', '#EdgeInference'], 'categories': ['#nlp', '#benchmark', '#inference'], 'emoji': '⚡', 'title': 'KV Prediction: молниеносный старт для языковых моделей'}}, {'id': 'https://huggingface.co/papers/2410.08168', 'title': 'ZeroComp: Zero-shot Object Compositing from Image Intrinsics via Diffusion', 'url': 'https://huggingface.co/papers/2410.08168', 'abstract': 'We present ZeroComp, an effective zero-shot 3D object compositing approach that does not require paired composite-scene images during training. Our method leverages ControlNet to condition from intrinsic images and combines it with a Stable Diffusion model to utilize its scene priors, together operating as an effective rendering engine. During training, ZeroComp uses intrinsic images based on geometry, albedo, and masked shading, all without the need for paired images of scenes with and without composite objects. Once trained, it seamlessly integrates virtual 3D objects into scenes, adjusting shading to create realistic composites. We developed a high-quality evaluation dataset and demonstrate that ZeroComp outperforms methods using explicit lighting estimations and generative techniques in quantitative and human perception benchmarks. Additionally, ZeroComp extends to real and outdoor image compositing, even when trained solely on synthetic indoor data, showcasing its effectiveness in image compositing.', 'score': 4, 'issue_id': 98, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'ZeroComp — это эффективный подход к композиции 3D-объектов с нулевым обучением, не требующий парных изображений сцен во время тренировки. Метод использует ControlNet для обусловливания по внутренним изображениям и комбинирует его с моделью Stable Diffusion для использования её априорных знаний о сценах. ZeroComp обучается на внутренних изображениях, основанных на геометрии, альбедо и маскированном затенении. После обучения метод может реалистично интегрировать виртуальные 3D-объекты в сцены, корректируя затенение.', 'tags': ['#3DCompositing', '#ZeroShot', '#IntrinsicImages'], 'categories': ['#cv', '#multimodal', '#dataset'], 'emoji': '🎭', 'title': 'Реалистичная композиция 3D-объектов без парных данных'}}, {'id': 'https://huggingface.co/papers/2410.09045', 'title': 'MiRAGeNews: Multimodal Realistic AI-Generated News Detection', 'url': 'https://huggingface.co/papers/2410.09045', 'abstract': 'The proliferation of inflammatory or misleading "fake" news content has become increasingly common in recent years. Simultaneously, it has become easier than ever to use AI tools to generate photorealistic images depicting any scene imaginable. Combining these two -- AI-generated fake news content -- is particularly potent and dangerous. To combat the spread of AI-generated fake news, we propose the MiRAGeNews Dataset, a dataset of 12,500 high-quality real and AI-generated image-caption pairs from state-of-the-art generators. We find that our dataset poses a significant challenge to humans (60% F-1) and state-of-the-art multi-modal LLMs (< 24% F-1). Using our dataset we train a multi-modal detector (MiRAGe) that improves by +5.1% F-1 over state-of-the-art baselines on image-caption pairs from out-of-domain image generators and news publishers. We release our code and data to aid future work on detecting AI-generated content.', 'score': 2, 'issue_id': 101, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'Статья представляет набор данных MiRAGeNews, состоящий из 12 500 пар изображений и подписей, как реальных, так и сгенерированных ИИ. Авторы обнаружили, что этот датасет представляет значительную сложность как для людей (60% F1-меры), так и для современных мультимодальных языковых моделей (<24% F1-меры). На основе этого набора данных был обучен мультимодальный детектор MiRAGe, который превосходит современные базовые модели на 5,1% по F1-мере при работе с парами изображение-подпись из сторонних генераторов изображений и новостных издателей. Код и данные опубликованы для содействия будущим исследованиям по обнаружению контента, сгенерированного ИИ.', 'tags': ['#fake_news_detection', '#image_caption_pairs', '#AI_generated_content'], 'categories': ['#dataset', '#multimodal', '#benchmark', '#cv', '#nlp'], 'emoji': '🕵️', 'title': 'MiRAGeNews: Борьба с дезинформацией в эпоху ИИ-генерации'}}, {'id': 'https://huggingface.co/papers/2410.07331', 'title': 'DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models', 'url': 'https://huggingface.co/papers/2410.07331', 'abstract': 'We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.', 'score': 2, 'issue_id': 95, 'pub_date': '2024-10-09', 'pub_date_ru': '9 октября', 'data': {'desc': 'DA-Code - это новый бенчмарк для оценки способностей языковых моделей в задачах генерации кода для анализа данных. Он включает сложные задачи, требующие продвинутых навыков кодирования, основан на реальных разнообразных данных и охватывает широкий спектр задач по обработке и анализу данных. Бенчмарк создан в контролируемой исполняемой среде, имитирующей реальные сценарии анализа данных. Эксперименты показали, что даже лучшие современные языковые модели достигают точности лишь 30.5% на этом бенчмарке.', 'tags': ['#кодогенерация', '#анализданных', '#агентноеПрограммирование'], 'categories': ['#benchmark', '#code', '#dataset'], 'emoji': '📊', 'title': 'DA-Code: Новый рубеж в оценке ИИ для анализа данных'}}, {'id': 'https://huggingface.co/papers/2410.09038', 'title': 'SimpleStrat: Diversifying Language Model Generation with Stratification', 'url': 'https://huggingface.co/papers/2410.09038', 'abstract': "Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose , an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.", 'score': 1, 'issue_id': 102, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'В статье представлен новый метод генерации разнообразных ответов от больших языковых моделей (LLM) под названием SimpleStrat. Авторы показывают, что традиционный подход увеличения температуры для повышения разнообразия имеет недостатки. SimpleStrat использует саму языковую модель для разделения пространства на страты, из которых затем выбирается случайная страта для генерации ответа. Для оценки разнообразия был создан датасет CoverageQA с недоопределенными вопросами, имеющими несколько равновероятных ответов. Эксперименты показали, что SimpleStrat достигает более высокого recall и меньшего KL-расхождения по сравнению с базовыми методами.', 'tags': ['#SimpleStrat', '#CoverageQA', '#разнообразие_генерации'], 'categories': ['#nlp', '#dataset', '#benchmark'], 'emoji': '🎭', 'title': 'SimpleStrat: новый подход к разнообразной генерации текста'}}, {'id': 'https://huggingface.co/papers/2410.07536', 'title': 'I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow', 'url': 'https://huggingface.co/papers/2410.07536', 'abstract': "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.", 'score': 1, 'issue_id': 99, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет новый фреймворк I-Max для улучшения экстраполяции разрешения в трансформерах с выпрямленным потоком (RFT) для генерации изображений по тексту. I-Max включает стратегию Projected Flow для стабильной экстраполяции и продвинутый инструментарий для вывода, позволяющий обобщать знания модели на более высокие разрешения. Эксперименты показывают, что I-Max повышает стабильность экстраполяции разрешения и улучшает детализацию изображений. Авторы утверждают, что этот подход демонстрирует практическую ценность экстраполяции разрешения без дополнительного обучения.', 'tags': ['#RFT', '#ResolutionExtrapolation', '#I-Max'], 'categories': ['#cv', '#multimodal'], 'emoji': '🔍', 'title': 'I-Max: Революция в экстраполяции разрешения для генерации изображений'}}, {'id': 'https://huggingface.co/papers/2410.09037', 'title': 'Mentor-KD: Making Small Language Models Better Multi-step Reasoners', 'url': 'https://huggingface.co/papers/2410.09037', 'abstract': "Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.", 'score': 1, 'issue_id': 97, 'pub_date': '2024-10-11', 'pub_date_ru': '11 октября', 'data': {'desc': 'Статья представляет новый метод под названием Mentor-KD для эффективной дистилляции способности к многошаговым рассуждениям от больших языковых моделей (LLM) к меньшим моделям. Метод использует промежуточную модель-наставника для расширения набора аннотаций Chain-of-Thought и предоставления мягких меток для модели-ученика. Авторы решают проблемы, связанные с качеством данных и предоставлением мягких меток при дистилляции рассуждений. Эксперименты подтверждают эффективность Mentor-KD для различных моделей и сложных задач рассуждения.', 'tags': ['#knowledge_distillation', '#chain_of_thought', '#reasoning_distillation'], 'categories': ['#nlp'], 'emoji': '🧠', 'title': 'Mentor-KD: Эффективная передача навыков рассуждения от больших языковых моделей к меньшим'}}, {'id': 'https://huggingface.co/papers/2410.08193', 'title': 'GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment', 'url': 'https://huggingface.co/papers/2410.08193', 'abstract': 'Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining.', 'score': 0, 'issue_id': 102, 'pub_date': '2024-10-10', 'pub_date_ru': '10 октября', 'data': {'desc': 'Статья представляет GenARM - новый подход к тестовому выравниванию больших языковых моделей (LLM) с использованием авторегрессивных моделей вознаграждения. Этот метод позволяет эффективно направлять замороженные LLM к желаемому распределению без переобучения. Экспериментальные результаты показывают, что GenARM превосходит существующие методы тестового выравнивания и соответствует производительности методов обучения. Подход также обеспечивает эффективное руководство от слабого к сильному и поддерживает многоцелевое выравнивание для удовлетворения разнообразных предпочтений пользователей.', 'tags': ['#генеративная_модель', '#авторегрессивная_модель', '#выравнивание_LLM'], 'categories': ['#nlp', '#rlhf'], 'emoji': '🎯', 'title': 'GenARM: Эффективное тестовое выравнивание LLM без переобучения'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'default';        

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "хф найтли";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'default';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            return Array.from(categories);
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = '🏷️ Фильтр';
            } else {
                categoryToggle.textContent = `🏷️ Фильтр (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const savedCategories = localStorage.getItem('selectedCategories');
            if (savedCategories) {
                if (savedCategories != '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);
                    updateCategoryButtonStates();
                }
            }
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const tags = item["data"]["tags"].join(" ");
                const articleHTML = `
                    <article>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">📅 Статья от ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">Статья</a>
                            </div>
                            <p class="tags">${tags}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiffRu('2024-10-14 18:50');
        } 

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();  
    </script>
</body>
</html>
    