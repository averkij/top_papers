
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 34 papers. October 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">34 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-27.html">â¬…ï¸ <span id="prev-date">27.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-29.html">â¡ï¸ <span id="next-date">29.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 28', 'zh': '10æœˆ28æ—¥'};
        let feedDateNext = {'ru': '29.10', 'en': '10/29', 'zh': '10æœˆ29æ—¥'};
        let feedDatePrev = {'ru': '27.10', 'en': '10/27', 'zh': '10æœˆ27æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.23607', 'title': 'Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations', 'url': 'https://huggingface.co/papers/2510.23607', 'abstract': "Concerto, a minimalist model combining 3D self-distillation and 2D-3D joint embedding, achieves superior spatial feature learning and outperforms existing models in scene understanding and open-world perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.", 'score': 143, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'c99437fd83f1fbf7', 'authors': ['Yujia Zhang', 'Xiaoyang Wu', 'Yixing Lao', 'Chengyao Wang', 'Zhuotao Tian', 'Naiyan Wang', 'Hengshuang Zhao'], 'affiliations': ['Harbin Institute of Technology (Shenzhen)', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.23607.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#benchmark', '#3d', '#optimization', '#training'], 'emoji': 'ğŸ¼', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ 2D Ğ¸ 3D', 'desc': 'Concerto â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ¾Ğ², Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ğ°Ñ Ñ‚ĞµĞ¼, ĞºĞ°Ğº Ğ»ÑĞ´Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞµĞ½ÑĞ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ 3D self-distillation Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ joint embedding Ğ¼ĞµĞ¶Ğ´Ñƒ 2D Ğ¸ 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ 2D Ğ¸ 3D self-supervised Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 14.2% Ğ¸ 4.8% ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ 3D ÑÑ†ĞµĞ½. Concerto Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ²Ñ€Ğ¾Ğ´Ğµ ScanNet Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ open-world Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ CLIP.'}, 'en': {'title': 'Concerto: Revolutionizing Spatial Understanding with Minimalist Design', 'desc': "Concerto is a novel machine learning model designed for spatial cognition, inspired by how humans learn concepts through multiple senses. It utilizes 3D self-distillation and a joint embedding of 2D and 3D data to enhance feature learning. The model demonstrates significant improvements in scene understanding, outperforming existing state-of-the-art models in both 2D and 3D tasks. Additionally, Concerto's ability to project representations into language space facilitates open-world perception, showcasing its versatility and effectiveness in spatial representation learning."}, 'zh': {'title': 'Concertoï¼šç®€çº¦æ¨¡å‹ï¼Œå“è¶Šç©ºé—´ç†è§£', 'desc': 'Concertoæ˜¯ä¸€ç§ç®€çº¦æ¨¡å‹ï¼Œç»“åˆäº†3Dè‡ªæˆ‘è’¸é¦å’Œ2D-3Dè”åˆåµŒå…¥ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å­¦ä¹ ç©ºé—´ç‰¹å¾ã€‚å®ƒå—åˆ°äººç±»å¤šæ„Ÿå®˜ååŒå­¦ä¹ çš„å¯å‘ï¼Œæ¨¡æ‹Ÿäº†äººç±»çš„æ¦‚å¿µå­¦ä¹ è¿‡ç¨‹ã€‚å°½ç®¡ç»“æ„ç®€å•ï¼ŒConcertoåœ¨ç©ºé—´è®¤çŸ¥ä¸Šè¡¨ç°å‡ºæ›´è¿è´¯å’Œä¿¡æ¯ä¸°å¯Œçš„ç‰¹å¾ï¼Œè¶…è¶Šäº†ç°æœ‰çš„2Då’Œ3Dè‡ªç›‘ç£æ¨¡å‹ã€‚é€šè¿‡å…¨é‡è°ƒä¼˜ï¼ŒConcertoåœ¨å¤šä¸ªåœºæ™¯ç†è§£åŸºå‡†ä¸Šè®¾å®šäº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå±•ç¤ºäº†å…¶åœ¨å¼€æ”¾ä¸–ç•Œæ„ŸçŸ¥ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23564', 'title': 'ReCode: Unify Plan and Action for Universal Granularity Control', 'url': 'https://huggingface.co/papers/2510.23564', 'abstract': 'ReCode, a recursive code generation paradigm, unifies high-level planning and low-level action in a single representation, enhancing decision granularity and data efficiency in LLM-based agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.', 'score': 97, 'issue_id': 6646, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '599b143427f11827', 'authors': ['Zhaoyang Yu', 'Jiayi Zhang', 'Huixue Su', 'Yufan Zhao', 'Yifan Wu', 'Mingyi Deng', 'Jinyu Xiang', 'Yizhang Lin', 'Lingxiao Tang', 'Yingchao Li', 'Yuyu Luo', 'Bang Liu', 'Chenglin Wu'], 'affiliations': ['DeepWisdom', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'UniversitÃ© de MontrÃ©al & Mila', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23564.jpg', 'data': {'categories': ['#agi', '#training', '#agents', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'ReCode â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ñ‹ ĞºĞ°Ğº Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸-Ğ·Ğ°Ğ³Ğ»ÑƒÑˆĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¿Ğ»Ğ¾Ñ‚ÑŒ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¶Ñ‘ÑÑ‚ĞºÑƒÑ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ inference, Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Unifying Planning and Action for Smarter Decision-Making', 'desc': "ReCode introduces a new way for Large Language Model (LLM)-based agents to make decisions by combining high-level planning and low-level actions into one unified representation. This approach allows agents to adapt their decision-making process dynamically, breaking down complex tasks into simpler actions recursively. By treating high-level plans as abstract functions that can be decomposed, ReCode enhances the model's ability to learn from diverse training data and improves its performance on various tasks. The results show that ReCode outperforms existing methods in both efficiency and effectiveness, demonstrating the benefits of integrating planning and action."}, 'zh': {'title': 'ç»Ÿä¸€è§„åˆ’ä¸è¡ŒåŠ¨çš„é€’å½’ä»£ç ç”Ÿæˆ', 'desc': 'ReCodeæ˜¯ä¸€ç§é€’å½’ä»£ç ç”ŸæˆèŒƒå¼ï¼Œå®ƒå°†é«˜å±‚æ¬¡çš„è§„åˆ’å’Œä½å±‚æ¬¡çš„è¡ŒåŠ¨ç»Ÿä¸€åœ¨ä¸€ä¸ªè¡¨ç¤ºä¸­ï¼Œä»è€Œå¢å¼ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å†³ç­–ç²’åº¦å’Œæ•°æ®æ•ˆç‡ã€‚å½“å‰çš„LLMä»£ç†åœ¨å†³ç­–ç²’åº¦ä¸Šå­˜åœ¨å±€é™ï¼Œæ— æ³•çµæ´»åœ°åœ¨é«˜å±‚æ¬¡è§„åˆ’å’Œä½å±‚æ¬¡è¡ŒåŠ¨ä¹‹é—´åˆ‡æ¢ã€‚ReCodeé€šè¿‡å°†é«˜å±‚æ¬¡è®¡åˆ’è§†ä¸ºæŠ½è±¡å ä½ç¬¦å‡½æ•°ï¼Œå¹¶é€’å½’åœ°åˆ†è§£ä¸ºæ›´ç»†ç²’åº¦çš„å­å‡½æ•°ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒReCodeåœ¨æ¨ç†æ€§èƒ½ä¸Šæ˜¾è‘—è¶…è¶Šäº†å…ˆè¿›çš„åŸºçº¿ï¼Œå¹¶åœ¨è®­ç»ƒä¸­å±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23587', 'title': 'A Survey of Data Agents: Emerging Paradigm or Overstated Hype?', 'url': 'https://huggingface.co/papers/2510.23587', 'abstract': 'A systematic taxonomy for data agents is introduced to clarify their autonomy levels and capabilities, addressing terminological ambiguity and guiding future research and development.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term "data agent" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.', 'score': 50, 'issue_id': 6649, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '6b7dccbc6baf14f9', 'authors': ['Yizhang Zhu', 'Liangwei Wang', 'Chenyu Yang', 'Xiaotian Lin', 'Boyan Li', 'Wei Zhou', 'Xinyu Liu', 'Zhangyang Peng', 'Tianqi Luo', 'Yu Li', 'Chengliang Chai', 'Chong Chen', 'Shimin Di', 'Ju Fan', 'Ji Sun', 'Nan Tang', 'Fugee Tsung', 'Jiannan Wang', 'Chenglin Wu', 'Yanwei Xu', 'Shaolei Zhang', 'Yong Zhang', 'Xuanhe Zhou', 'Guoliang Li', 'Yuyu Luo'], 'affiliations': ['Beijing Institute of Technology, Beijing, China', 'DeepWisdom', 'Huawei', 'Renmin University of China, Beijing, China', 'Shanghai Jiao Tong University, Shanghai, China', 'Southeast University, Nanjing, China', 'The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.23587.jpg', 'data': {'categories': ['#survey', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ data-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ´Ğ»Ñ data-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² â€” Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ AI Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑˆĞµÑÑ‚Ğ¸ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²ÑƒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ (Ğ¾Ñ‚ L0 Ğ´Ğ¾ L5), Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½ÑƒÑ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ¼ SAE J3016 Ğ´Ğ»Ñ Ğ±ĞµÑĞ¿Ğ¸Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€Ğ°Ğ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ñƒ Ğ¾Ñ‚ L2 Ğº L3, Ğ³Ğ´Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ¾Ñ€ĞºĞµÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ·Ğ¾Ñ€ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾.'}, 'en': {'title': 'Clarifying Data Agents: A Taxonomy for Autonomy Levels', 'desc': "This paper presents a structured classification system for data agents, which are autonomous systems that manage data and AI tasks. It addresses the confusion surrounding the term 'data agent' by establishing a clear hierarchy of autonomy levels, from basic manual operations to advanced fully autonomous systems. The taxonomy helps clarify the capabilities and responsibilities of different types of data agents, facilitating better understanding and expectations in the industry. Additionally, the paper outlines the current state of research and identifies key areas for future development, particularly in enhancing the autonomy of these systems."}, 'zh': {'title': 'æ•°æ®ä»£ç†çš„è‡ªä¸»æ€§åˆ†ç±»ä¸æœªæ¥å±•æœ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç³»ç»Ÿçš„åˆ†ç±»æ³•ï¼Œç”¨äºæ˜ç¡®æ•°æ®ä»£ç†çš„è‡ªä¸»æ€§æ°´å¹³å’Œèƒ½åŠ›ï¼Œè§£å†³äº†æœ¯è¯­æ¨¡ç³Šçš„é—®é¢˜ï¼Œå¹¶æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶å’Œå¼€å‘ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ•°æ®ä»£ç†ä½œä¸ºè‡ªä¸»ç³»ç»Ÿåº”è¿è€Œç”Ÿï¼Œæ—¨åœ¨åè°ƒæ•°æ®ä¸äººå·¥æ™ºèƒ½ç”Ÿæ€ç³»ç»Ÿä»¥åº”å¯¹å¤æ‚çš„æ•°æ®ç›¸å…³ä»»åŠ¡ã€‚æ–‡ç« æå‡ºäº†å…­ä¸ªå±‚çº§çš„åˆ†ç±»ï¼Œä»æ‰‹åŠ¨æ“ä½œï¼ˆL0ï¼‰åˆ°å®Œå…¨è‡ªä¸»çš„æ•°æ®ä»£ç†ï¼ˆL5ï¼‰ï¼Œæ¸…æ™°ç•Œå®šäº†èƒ½åŠ›è¾¹ç•Œå’Œè´£ä»»åˆ†é…ã€‚æœ€åï¼Œæ–‡ç« å±•æœ›äº†ä¸»åŠ¨ç”Ÿæˆæ•°æ®ä»£ç†çš„æœªæ¥å‘å±•æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23588', 'title': 'FARMER: Flow AutoRegressive Transformer over Pixels', 'url': 'https://huggingface.co/papers/2510.23588', 'abstract': 'FARMER, a generative framework combining Normalizing Flows and Autoregressive models, achieves competitive image synthesis from raw pixels with exact likelihoods and scalable training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.', 'score': 42, 'issue_id': 6646, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '43d1242260382b1e', 'authors': ['Guangting Zheng', 'Qinyu Zhao', 'Tao Yang', 'Fei Xiao', 'Zhijie Lin', 'Jie Wu', 'Jiajun Deng', 'Yanyong Zhang', 'Rui Zhu'], 'affiliations': ['ANU', 'ByteDance Seed China', 'ByteDance Seed Singapore', 'NUS', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2510.23588.jpg', 'data': {'categories': ['#cv', '#diffusion', '#architecture', '#training', '#inference', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'FARMER: ĞĞ¾Ğ²Ğ°Ñ ÑÑ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° FARMER, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Normalizing Flows Ğ¸ Autoregressive Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸. FARMER Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ. Ğ”Ğ»Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑĞ°Ğ¼Ğ¾ÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FARMER Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'FARMER: Unifying Flows and Autoregressive Models for Superior Image Synthesis', 'desc': 'FARMER is a new generative framework that combines Normalizing Flows and Autoregressive models to improve image synthesis from raw pixels. It directly models the likelihood of data distribution, which is crucial for scaling in machine learning. The framework uses an invertible autoregressive flow to convert images into latent sequences, allowing for better likelihood estimation. Additionally, FARMER includes a self-supervised dimension reduction method and a fast inference technique to enhance image generation quality and efficiency.'}, 'zh': {'title': 'FARMERï¼šé«˜æ•ˆçš„å›¾åƒç”Ÿæˆæ–°æ¡†æ¶', 'desc': 'FARMERæ˜¯ä¸€ç§ç»“åˆäº†å½’ä¸€åŒ–æµå’Œè‡ªå›å½’æ¨¡å‹çš„ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿä»åŸå§‹åƒç´ ä¸­å®ç°é«˜è´¨é‡çš„å›¾åƒåˆæˆã€‚è¯¥æ¡†æ¶é€šè¿‡å¯é€†çš„è‡ªå›å½’æµå°†å›¾åƒè½¬æ¢ä¸ºæ½œåœ¨åºåˆ—ï¼Œå¹¶åˆ©ç”¨è‡ªå›å½’æ¨¡å‹éšå¼å»ºæ¨¡å…¶åˆ†å¸ƒã€‚ä¸ºäº†è§£å†³åƒç´ çº§å»ºæ¨¡ä¸­çš„å†—ä½™å’Œå¤æ‚æ€§ï¼ŒFARMERæå‡ºäº†ä¸€ç§è‡ªç›‘ç£çš„ç»´åº¦å‡å°‘æ–¹æ¡ˆï¼Œå°†æ½œåœ¨é€šé“åˆ†ä¸ºä¿¡æ¯æ€§å’Œå†—ä½™ç»„ï¼Œä»è€Œæé«˜è‡ªå›å½’å»ºæ¨¡çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒFARMERè¿˜è®¾è®¡äº†ä¸€ç§ä¸€æ­¥è’¸é¦æ–¹æ¡ˆï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œå¹¶å¼•å…¥æ— åˆ†ç±»å™¨å¼•å¯¼ç®—æ³•ä»¥æå‡å›¾åƒç”Ÿæˆè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23581', 'title': 'Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation', 'url': 'https://huggingface.co/papers/2510.23581', 'abstract': 'Lookahead Anchoring improves audio-driven human animation by using future keyframes as dynamic guides, enhancing lip synchronization, identity preservation, and visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.', 'score': 39, 'issue_id': 6647, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'c609d842462b5e14', 'authors': ['Junyoung Seo', 'Rodrigo Mira', 'Alexandros Haliassos', 'Stella Bounareli', 'Honglie Chen', 'Linh Tran', 'Seungryong Kim', 'Zoe Landgraf', 'Jie Shen'], 'affiliations': ['Imperial College London', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2510.23581.jpg', 'data': {'categories': ['#architecture', '#audio', '#video', '#multimodal', '#games'], 'emoji': 'âš“', 'ru': {'title': 'Ğ¯ĞºĞ¾Ñ€ÑŒ Ğ¸Ğ· Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾: ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿ĞµÑ€ĞµĞ¶Ğ°ÑÑ‰ĞµĞµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Lookahead Anchoring Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ´ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ¾ĞºĞ½Ğ°, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ĞºĞ°Ğº Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ¼Ğ°ÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ° Ğ¿Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµÑ‚ÑÑ Ğº ÑÑ‚Ğ¸Ğ¼ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğ¼ ÑĞºĞ¾Ñ€ÑĞ¼, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€ÑƒÑ Ğ½Ğ° Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ€Ğ°Ğ·Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ÑƒĞ±, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½Ğ° Ñ‚Ñ€Ñ‘Ñ… ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°.'}, 'en': {'title': 'Dynamic Future Guidance for Consistent Animation', 'desc': 'Lookahead Anchoring enhances audio-driven human animation by utilizing future keyframes as dynamic guides, which helps in maintaining character identity and improving visual quality. Traditional methods often lead to identity drift during the generation process, but this approach allows the model to continuously align with future keyframes, acting as directional beacons. By doing so, it enables self-keyframing, eliminating the need for a separate keyframe generation stage. The method also allows for a balance between expressivity and consistency by adjusting the temporal lookahead distance, resulting in better lip synchronization and overall animation quality across various models.'}, 'zh': {'title': 'æœªæ¥å…³é”®å¸§å¼•å¯¼ï¼Œæå‡åŠ¨ç”»è´¨é‡', 'desc': 'Lookahead Anchoringæ˜¯ä¸€ç§æ”¹è¿›éŸ³é¢‘é©±åŠ¨äººç±»åŠ¨ç”»çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨æœªæ¥çš„å…³é”®å¸§ä½œä¸ºåŠ¨æ€æŒ‡å¯¼ï¼Œå¢å¼ºäº†å˜´å”‡åŒæ­¥ã€èº«ä»½ä¿æŒå’Œè§†è§‰è´¨é‡ã€‚ä¼ ç»Ÿçš„éŸ³é¢‘é©±åŠ¨åŠ¨ç”»æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸¸å¸¸ä¼šå‡ºç°èº«ä»½æ¼‚ç§»çš„é—®é¢˜ï¼Œè€ŒLookahead Anchoringé€šè¿‡åˆ©ç”¨æœªæ¥æ—¶é—´æ­¥çš„å…³é”®å¸§ï¼Œé¿å…äº†è¿™ç§èº«ä»½ä¸§å¤±ã€‚è¯¥æ–¹æ³•ä½¿å¾—å…³é”®å¸§ä¸å†æ˜¯å›ºå®šçš„è¾¹ç•Œï¼Œè€Œæ˜¯åŠ¨æ€çš„æŒ‡å¼•ï¼Œæ¨¡å‹åœ¨å“åº”éŸ³é¢‘ä¿¡å·çš„åŒæ—¶ï¼ŒæŒç»­è¿½è¸ªè¿™äº›æœªæ¥çš„å…³é”®å¸§ã€‚å®éªŒè¡¨æ˜ï¼ŒLookahead Anchoringåœ¨å¤šä¸ªåŠ¨ç”»æ¨¡å‹ä¸­å®ç°äº†æ›´å¥½çš„å˜´å”‡åŒæ­¥ã€èº«ä»½ä¿æŒå’Œè§†è§‰æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.21817', 'title': 'VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing,\n  Speaking, and Acting', 'url': 'https://huggingface.co/papers/2510.21817', 'abstract': "VITA-E, a dual-model embodied interaction framework, enables concurrent and interruptible vision-language-action capabilities, enhancing real-time user interaction and multitasking.  \t\t\t\t\tAI-generated summary \t\t\t\t Current Vision-Language-Action (VLA) models are often constrained by a rigid, static interaction paradigm, which lacks the ability to see, hear, speak, and act concurrently as well as handle real-time user interruptions dynamically. This hinders seamless embodied collaboration, resulting in an inflexible and unresponsive user experience. To address these limitations, we introduce VITA-E, a novel embodied interaction framework designed for both behavioral concurrency and nearly real-time interruption. The core of our approach is a dual-model architecture where two parallel VLA instances operate as an ``Active Model'' and a ``Standby Model'', allowing the embodied agent to observe its environment, listen to user speech, provide verbal responses, and execute actions, all concurrently and interruptibly, mimicking human-like multitasking capabilities. We further propose a ``model-as-controller'' paradigm, where we fine-tune the VLM to generate special tokens that serve as direct system-level commands, coupling the model's reasoning with the system's behavior. Experiments conducted on a physical humanoid platform demonstrate that VITA-E can reliably handle complex interactive scenarios. Our framework is compatible with various dual-system VLA models, achieving an extremely high success rate on emergency stops and speech interruptions while also successfully performing concurrent speech and action. This represents a significant step towards more natural and capable embodied assistants.", 'score': 39, 'issue_id': 6645, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 21', 'zh': '10æœˆ21æ—¥'}, 'hash': 'e4786dd390bcb847', 'authors': ['Xiaoyu Liu', 'Chaoyou Fu', 'Chi Yan', 'Chu Wu', 'Haihan Gao', 'Yi-Fan Zhang', 'Shaoqi Dong', 'Cheng Qian', 'Bin Luo', 'Xiuyong Yang', 'Guanwu Li', 'Yusheng Cai', 'Yunhang Shen', 'Deqiang Jiang', 'Haoyu Cao', 'Xing Sun', 'Caifeng Shan', 'Ran He'], 'affiliations': ['CASIA', 'Fourier Intelligence Inc.', 'Nanjing University', 'Tencent Youtu Lab'], 'pdf_title_img': 'assets/pdf/title_img/2510.21817.jpg', 'data': {'categories': ['#multimodal', '#agents', '#agi', '#interpretability', '#reasoning', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¿Ğ¾ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ¼Ñƒ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ñ Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ, ÑĞ»Ñ‹ÑˆĞ°Ñ‚ÑŒ, Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ĞºĞ°Ğº ÑÑ‚Ğ¾ Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ»ÑĞ´Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ VITA-E â€” Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ²ÑƒĞ¼Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ VLA Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸ Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ½Ğ¾Ğ¹), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñƒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Â«Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-ĞºĞ°Ğº-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Â», Ğ³Ğ´Ğµ VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ ÑĞºÑÑ‚Ñ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°Ñ…, Ğ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ°Ğ½Ğ¸ÑÑ… Ñ€ĞµÑ‡ÑŒÑ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering Multitasking in AI with VITA-E', 'desc': "VITA-E is a new framework that improves how machines interact with users by allowing them to see, hear, speak, and act at the same time. It uses a dual-model system with an 'Active Model' for immediate tasks and a 'Standby Model' ready to respond to interruptions, making interactions feel more natural and fluid. This framework addresses the limitations of traditional Vision-Language-Action (VLA) models, which often struggle with multitasking and real-time responses. Experiments show that VITA-E can effectively manage complex interactions, enhancing the capabilities of embodied assistants."}, 'zh': {'title': 'VITA-Eï¼šå®ç°å®æ—¶å¤šä»»åŠ¡çš„æ™ºèƒ½äº¤äº’æ¡†æ¶', 'desc': 'VITA-Eæ˜¯ä¸€ä¸ªåŒæ¨¡å‹çš„å…·èº«äº¤äº’æ¡†æ¶ï¼Œèƒ½å¤ŸåŒæ—¶è¿›è¡Œè§†è§‰-è¯­è¨€-è¡ŒåŠ¨çš„å¤„ç†ï¼Œæå‡å®æ—¶ç”¨æˆ·äº¤äº’å’Œå¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€‚å½“å‰çš„è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹å¸¸å¸¸å—åˆ°é™æ€äº¤äº’æ¨¡å¼çš„é™åˆ¶ï¼Œæ— æ³•åŠ¨æ€åº”å¯¹ç”¨æˆ·çš„å®æ—¶å¹²æ‰°ã€‚VITA-Eé€šè¿‡åŒæ¨¡å‹æ¶æ„ï¼Œä½¿å¾—å…·èº«ä»£ç†èƒ½å¤ŸåŒæ—¶è§‚å¯Ÿç¯å¢ƒã€å¬å–ç”¨æˆ·è¯­éŸ³ã€æä¾›å£å¤´å›åº”å’Œæ‰§è¡ŒåŠ¨ä½œï¼Œæ¨¡æ‹Ÿäººç±»çš„å¤šä»»åŠ¡èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒVITA-Eåœ¨å¤æ‚äº¤äº’åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†ç´§æ€¥åœæ­¢å’Œè¯­éŸ³å¹²æ‰°ï¼Œæ ‡å¿—ç€å…·èº«åŠ©æ‰‹å‘æ›´è‡ªç„¶å’Œæ›´å¼ºå¤§çš„æ–¹å‘è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22201', 'title': 'ACG: Action Coherence Guidance for Flow-based VLA models', 'url': 'https://huggingface.co/papers/2510.22201', 'abstract': 'Action Coherence Guidance (ACG) improves action coherence in Vision-Language-Action (VLA) models during test time, enhancing performance in diverse manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion and flow matching models have emerged as powerful robot policies, enabling Vision-Language-Action (VLA) models to generalize across diverse scenes and instructions. Yet, when trained via imitation learning, their high generative capacity makes them sensitive to noise in human demonstrations: jerks, pauses, and jitter which reduce action coherence. Reduced action coherence causes instability and trajectory drift during deployment, failures that are catastrophic in fine-grained manipulation where precision is crucial. In this paper, we present Action Coherence Guidance (ACG) for VLA models, a training-free test-time guidance algorithm that improves action coherence and thereby yields performance gains. Evaluated on RoboCasa, DexMimicGen, and real-world SO-101 tasks, ACG consistently improves action coherence and boosts success rates across diverse manipulation tasks. Code and project page are available at https://github.com/DAVIAN-Robotics/ACG and https://DAVIAN-Robotics.github.io/ACG , respectively.', 'score': 32, 'issue_id': 6645, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': 'bf324c122f095102', 'authors': ['Minho Park', 'Kinam Kim', 'Junha Hyung', 'Hyojin Jang', 'Hoiyeong Jin', 'Jooyeol Yun', 'Hojoon Lee', 'Jaegul Choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.22201.jpg', 'data': {'categories': ['#robotics', '#games', '#agents', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Action Coherence Guidance (ACG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Vision-Language-Action (VLA) Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ flow matching Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº ÑˆÑƒĞ¼Ñƒ Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ… â€” Ñ€Ñ‹Ğ²ĞºĞ°Ğ¼, Ğ¿Ğ°ÑƒĞ·Ğ°Ğ¼ Ğ¸ Ğ´Ñ€Ğ¾Ğ¶Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°. ACG â€” ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ guidance, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… RoboCasa, DexMimicGen Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… SO-101 Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Robotic Precision with Action Coherence Guidance', 'desc': 'This paper introduces Action Coherence Guidance (ACG), a method designed to enhance the action coherence of Vision-Language-Action (VLA) models during their testing phase. ACG addresses the issue of noise in human demonstrations, which can lead to erratic movements and reduced performance in robotic manipulation tasks. By applying ACG, the models can maintain stability and precision, crucial for tasks requiring fine manipulation. The effectiveness of ACG is demonstrated through evaluations on various benchmarks, showing significant improvements in action coherence and overall success rates.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„åŠ¨ä½œä¸€è‡´æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨ä½œä¸€è‡´æ€§å¼•å¯¼ï¼ˆACGï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹åœ¨æµ‹è¯•é˜¶æ®µçš„åŠ¨ä½œä¸€è‡´æ€§ã€‚é€šè¿‡è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­äººç±»æ¼”ç¤ºçš„å™ªå£°é—®é¢˜ï¼ŒACGèƒ½å¤Ÿå‡å°‘å› æŠ–åŠ¨ã€åœé¡¿ç­‰å¯¼è‡´çš„åŠ¨ä½œä¸ç¨³å®šæ€§ã€‚è¯¥æ–¹æ³•åœ¨RoboCasaã€DexMimicGenå’ŒçœŸå®ä¸–ç•ŒSO-101ä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºACGæ˜¾è‘—æé«˜äº†åŠ¨ä½œä¸€è‡´æ€§å’ŒæˆåŠŸç‡ã€‚ACGæ˜¯ä¸€ç§æ— è®­ç»ƒçš„æµ‹è¯•æ—¶å¼•å¯¼ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æ“ä½œä»»åŠ¡ä¸­æå‡æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22521', 'title': 'Open Multimodal Retrieval-Augmented Factual Image Generation', 'url': 'https://huggingface.co/papers/2510.22521', 'abstract': 'ORIG, an agentic open multimodal retrieval-augmented framework, enhances factual consistency and image quality in factual image generation by iteratively integrating refined web-based evidence into prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.', 'score': 29, 'issue_id': 6658, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': 'dd425eefee2810bd', 'authors': ['Yang Tian', 'Fan Liu', 'Jingyuan Zhang', 'Wei Bi', 'Yupeng Hu', 'Liqiang Nie'], 'affiliations': ['Harbin Institute of Technology', 'Kuaishou Technology', 'National University of Singapore', 'Shandong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22521.jpg', 'data': {'categories': ['#multimodal', '#rag', '#benchmark', '#games', '#interpretability', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¤Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ORIG â€” Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ retrieval Ğ¸Ğ· Ğ²ĞµĞ±Ğ°. Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ°Ñ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸ÑÑ…, Ğ¿Ñ€Ğ¸Ğ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ORIG Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚Ğ°, Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FIG-Eval, Ğ½Ğ° ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ORIG Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Image Generation with Dynamic Factual Retrieval', 'desc': 'The paper introduces ORIG, a novel framework designed to improve the accuracy and quality of images generated by large multimodal models (LMMs). ORIG addresses the common issue of factual inconsistency in AI-generated images by iteratively integrating refined web-based evidence into the generation process. Unlike traditional methods that rely on static information, ORIG dynamically retrieves and filters multimodal evidence, ensuring that the generated images are both visually realistic and factually grounded. The authors also present FIG-Eval, a benchmark for evaluating the performance of factual image generation across various dimensions, demonstrating that ORIG significantly enhances both factual consistency and image quality compared to existing approaches.'}, 'zh': {'title': 'æå‡äº‹å®ä¸€è‡´æ€§çš„å›¾åƒç”Ÿæˆæ¡†æ¶', 'desc': 'ORIGæ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜äº‹å®å›¾åƒç”Ÿæˆä¸­çš„äº‹å®ä¸€è‡´æ€§å’Œå›¾åƒè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡è¿­ä»£åœ°ä»ç½‘ç»œä¸­æ£€ç´¢å’Œè¿‡æ»¤å¤šæ¨¡æ€è¯æ®ï¼Œå°†ç²¾ç‚¼çš„çŸ¥è¯†é€æ­¥æ•´åˆåˆ°æç¤ºä¸­ï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿçš„æ£€ç´¢å¢å¼ºæ–¹æ³•ä¸åŒï¼ŒORIGèƒ½å¤ŸåŸºäºå‡†ç¡®å’Œä¸æ–­æ¼”å˜çš„çŸ¥è¯†è¿›è¡Œç”Ÿæˆï¼Œè§£å†³äº†é™æ€æ¥æºå’Œæµ…å±‚è¯æ®æ•´åˆçš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒORIGåœ¨äº‹å®ä¸€è‡´æ€§å’Œæ•´ä½“å›¾åƒè´¨é‡æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ï¼Œå±•ç¤ºäº†å¼€æ”¾å¤šæ¨¡æ€æ£€ç´¢åœ¨äº‹å®å›¾åƒç”Ÿæˆä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22733', 'title': 'E^2Rank: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker', 'url': 'https://huggingface.co/papers/2510.22733', 'abstract': 'A unified framework extends a single text embedding model to perform both retrieval and listwise reranking, achieving state-of-the-art results with low latency.  \t\t\t\t\tAI-generated summary \t\t\t\t Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework E^2Rank, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E^2Rank achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.', 'score': 26, 'issue_id': 6645, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '487d3fd10fdcad61', 'authors': ['Qi Liu', 'Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Pengjun Xie', 'Jiaxin Mao'], 'affiliations': ['Alibaba Group', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.22733.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#rag'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ´Ğ½Ğ° embedding-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ E^2Rank â€” ĞµĞ´Ğ¸Ğ½ÑƒÑæ¡†æ¶, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ embedding-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ñ‚Ğ°Ğº Ğ¸ listwise reranking. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ listwise ranking objective, Ğ³Ğ´Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸Ğ· Ñ‚Ğ¾Ğ¿-K Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ pseudo-relevance feedback. Ğ”Ğ»Ñ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğµ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ¸Ğ·ĞºÑƒÑ latency. E^2Rank Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… BEIR Ğ¸ BRIGHT, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ´Ğ½Ğ° embedding-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ retrieval Ğ¸ reranking.'}, 'en': {'title': 'Unifying Retrieval and Reranking with E^2Rank', 'desc': 'This paper introduces E^2Rank, a unified framework that enhances a single text embedding model to perform both document retrieval and listwise reranking. By training the model under a listwise ranking objective, it achieves high-quality retrieval while maintaining low latency. The framework utilizes cosine similarity for ranking, allowing it to incorporate signals from top candidate documents, similar to traditional pseudo-relevance feedback. The results show that E^2Rank outperforms existing methods on various benchmarks, demonstrating that a single embedding model can efficiently unify retrieval and reranking tasks.'}, 'zh': {'title': 'ç»Ÿä¸€æ¡†æ¶ï¼Œå®ç°é«˜æ•ˆæ£€ç´¢ä¸é‡æ’åº', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶E^2Rankï¼Œæ—¨åœ¨é€šè¿‡æ‰©å±•å•ä¸€æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå®ç°é«˜æ•ˆçš„æ£€ç´¢å’Œåˆ—è¡¨é‡æ’åºã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨åˆ—è¡¨æ’åºç›®æ ‡ä¸‹ç»§ç»­è®­ç»ƒï¼Œæå‡äº†æ£€ç´¢è´¨é‡å’Œé‡æ’åºæ€§èƒ½ã€‚E^2Rankåˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºç»Ÿä¸€çš„æ’åºå‡½æ•°ï¼Œå¢å¼ºäº†æŸ¥è¯¢çš„ä¿¡å·ï¼Œä»è€Œæé«˜äº†é‡æ’åºçš„æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒE^2Rankåœ¨BEIRé‡æ’åºåŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œå¹¶åœ¨BRIGHTåŸºå‡†ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä½å»¶è¿Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22706', 'title': 'IGGT: Instance-Grounded Geometry Transformer for Semantic 3D\n  Reconstruction', 'url': 'https://huggingface.co/papers/2510.22706', 'abstract': "InstanceGrounded Geometry Transformer (IGGT) unifies 3D reconstruction and instance-level understanding using a unified transformer and 3D-Consistent Contrastive Learning, supported by a new dataset InsScene-15K.  \t\t\t\t\tAI-generated summary \t\t\t\t Humans naturally perceive the geometric structure and semantic content of a 3D world as intertwined dimensions, enabling coherent and accurate understanding of complex scenes. However, most prior approaches prioritize training large geometry models for low-level 3D reconstruction and treat high-level spatial understanding in isolation, overlooking the crucial interplay between these two fundamental aspects of 3D-scene analysis, thereby limiting generalization and leading to poor performance in downstream 3D understanding tasks. Recent attempts have mitigated this issue by simply aligning 3D models with specific language models, thus restricting perception to the aligned model's capacity and limiting adaptability to downstream tasks. In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), an end-to-end large unified transformer to unify the knowledge for both spatial reconstruction and instance-level contextual understanding. Specifically, we design a 3D-Consistent Contrastive Learning strategy that guides IGGT to encode a unified representation with geometric structures and instance-grounded clustering through only 2D visual inputs. This representation supports consistent lifting of 2D visual inputs into a coherent 3D scene with explicitly distinct object instances. To facilitate this task, we further construct InsScene-15K, a large-scale dataset with high-quality RGB images, poses, depth maps, and 3D-consistent instance-level mask annotations with a novel data curation pipeline.", 'score': 26, 'issue_id': 6646, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '741ebd23a0f4073c', 'authors': ['Hao Li', 'Zhengyu Zou', 'Fangfu Liu', 'Xuanyang Zhang', 'Fangzhou Hong', 'Yukang Cao', 'Yushi Lan', 'Manyuan Zhang', 'Gang Yu', 'Dingwen Zhang', 'Ziwei Liu'], 'affiliations': ['MMLab, CUHK', 'NWPU', 'S-Lab, NTU', 'StepFun, Inc.', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2510.22706.jpg', 'data': {'categories': ['#dataset', '#games', '#optimization', '#3d'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ IGGT â€” Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ 3D-ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ‘Ñ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ² Ğ½ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞµĞ´Ğ¸Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ 2D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ InsScene-15K Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ĞºĞ°Ñ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ² 3D. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾.'}, 'en': {'title': 'Unifying 3D Reconstruction and Instance Understanding with IGGT', 'desc': "The InstanceGrounded Geometry Transformer (IGGT) is a novel approach that integrates 3D reconstruction with instance-level understanding using a unified transformer architecture. It employs 3D-Consistent Contrastive Learning to create a cohesive representation that captures both geometric structures and distinct object instances from 2D visual inputs. This method addresses the limitations of previous models that treated spatial understanding and geometry separately, enhancing generalization for downstream 3D tasks. Additionally, the introduction of the InsScene-15K dataset provides high-quality data necessary for training and evaluating the model's performance in complex scene analysis."}, 'zh': {'title': 'ç»Ÿä¸€3Dé‡å»ºä¸å®ä¾‹ç†è§£çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†å®ä¾‹åŸºç¡€å‡ ä½•å˜æ¢å™¨ï¼ˆIGGTï¼‰ï¼Œæ—¨åœ¨å°†3Dé‡å»ºä¸å®ä¾‹çº§ç†è§£ç»Ÿä¸€èµ·æ¥ã€‚é€šè¿‡3Dä¸€è‡´æ€§å¯¹æ¯”å­¦ä¹ ï¼ŒIGGTèƒ½å¤Ÿä»2Dè§†è§‰è¾“å…¥ä¸­ç¼–ç å‡ºå‡ ä½•ç»“æ„å’Œå®ä¾‹èšç±»çš„ç»Ÿä¸€è¡¨ç¤ºã€‚è¯¥æ–¹æ³•å…‹æœäº†ä»¥å¾€æ–¹æ³•ä¸­ä½çº§3Dé‡å»ºä¸é«˜çº§ç©ºé—´ç†è§£ç›¸äº’å­¤ç«‹çš„é—®é¢˜ï¼Œæå‡äº†3Dåœºæ™¯åˆ†æçš„æ€§èƒ½ã€‚ä¸ºæ”¯æŒè¿™ä¸€ç ”ç©¶ï¼Œä½œè€…è¿˜æ„å»ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†InsScene-15Kï¼ŒåŒ…å«é«˜è´¨é‡çš„RGBå›¾åƒã€å§¿æ€ã€æ·±åº¦å›¾å’Œ3Dä¸€è‡´çš„å®ä¾‹çº§æ©ç æ³¨é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23451', 'title': 'Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with\n  Free-Form Preferences', 'url': 'https://huggingface.co/papers/2510.23451', 'abstract': 'Omni-Reward addresses modality imbalance and preference rigidity in reward models by introducing a benchmark, dataset, and model that support multiple modalities and free-form preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.', 'score': 22, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '957ca9be32cf1b0d', 'authors': ['Zhuoran Jin', 'Hongbang Yuan', 'Kejian Zhu', 'Jiachun Li', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.23451.jpg', 'data': {'categories': ['#data', '#multimodal', '#benchmark', '#dataset', '#alignment', '#architecture'], 'emoji': 'ğŸ', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Omni-Reward â€” ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ reward models, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (Ñ‚ĞµĞºÑÑ‚, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ°ÑƒĞ´Ğ¸Ğ¾, 3D). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ´Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹: Ğ´Ğ¸ÑĞ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…) Ğ¸ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ½Ğµ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹). Omni-RewardBench â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… reward models ÑĞ¾ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 9 Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Omni-RewardData ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 248K Ğ¿Ğ°Ñ€ Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ¸ 69K Ğ¿Ğ°Ñ€ Ğ´Ğ»Ñ instruction-tuning, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Omni-RewardModel Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Empowering AI with Omni-Modal Reward Modeling', 'desc': 'The paper introduces Omni-Reward, a new approach to improve reward models (RMs) in AI by addressing two main issues: modality imbalance and preference rigidity. Modality imbalance refers to the limited focus of existing RMs on text and images, neglecting other important modalities like video and audio. Preference rigidity highlights the problem of using fixed binary preferences, which do not reflect the diverse and complex nature of human preferences. Omni-Reward provides a comprehensive benchmark, a large multimodal dataset, and a versatile model that supports various modalities and free-form preferences, enhancing the alignment of AI behaviors with human values.'}, 'zh': {'title': 'å…¨æ¨¡æ€å¥–åŠ±å»ºæ¨¡çš„æœªæ¥', 'desc': 'Omni-Reward æ—¨åœ¨è§£å†³å¥–åŠ±æ¨¡å‹ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡å’Œåå¥½åˆšæ€§é—®é¢˜ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªåŸºå‡†ã€æ•°æ®é›†å’Œæ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ¨¡æ€å’Œè‡ªç”±å½¢å¼çš„åå¥½ã€‚è¯¥ç ”ç©¶åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šOmni-RewardBenchåŸºå‡†ï¼Œæ¶µç›–æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œ3Dç­‰äº”ç§æ¨¡æ€çš„ä¹ä¸ªä»»åŠ¡ï¼›Omni-RewardDataæ•°æ®é›†ï¼ŒåŒ…å«248Kä¸ªé€šç”¨åå¥½å¯¹å’Œ69Kä¸ªæŒ‡ä»¤è°ƒä¼˜å¯¹ï¼›ä»¥åŠOmni-RewardModelæ¨¡å‹ï¼Œç»“åˆäº†åˆ¤åˆ«å¼å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼Œåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23052', 'title': 'Knocking-Heads Attention', 'url': 'https://huggingface.co/papers/2510.23052', 'abstract': 'Knocking-heads attention (KHA) enhances multi-head attention by enabling cross-head interactions, improving training dynamics and performance in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to "knock" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.', 'score': 21, 'issue_id': 6646, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '229162a49692e72d', 'authors': ['Zhanchao Zhou', 'Xiaodong Chen', 'Haoxing Chen', 'Zhenzhong Lan', 'Jianguo Li'], 'affiliations': ['Ant Group', 'Renmin University of China', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23052.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ“Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ knocking-heads attention (KHA), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ multi-head attention, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ³Ğ´Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ‹ Ğ³Ğ¾Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ¸Ñ€ÑƒÑÑ‚ÑÑ, KHA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾ĞµĞºÑ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñƒ Ñ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸. ĞœĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ attention (MHA, GQA, GTA). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° MoE Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ 6.1B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ½Ğ° downstream Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Attention with Cross-Head Interactions', 'desc': "Knocking-heads attention (KHA) improves multi-head attention (MHA) by allowing attention heads to interact with each other, enhancing the model's ability to learn complex representations. Traditional MHA limits head interactions, which can weaken the overall performance as more heads are added. KHA introduces a shared projection matrix that maintains individual head specialization while enabling cross-head feature interactions. This method not only adds minimal computational overhead but also leads to better training stability and performance in large language models."}, 'zh': {'title': 'å‡»å¤´æ³¨æ„åŠ›ï¼šæå‡å¤šå¤´æ³¨æ„åŠ›çš„æ–°æ–¹æ³•', 'desc': 'å‡»å¤´æ³¨æ„åŠ›ï¼ˆKHAï¼‰é€šè¿‡å…è®¸å¤šå¤´ä¹‹é—´çš„äº¤äº’ï¼Œå¢å¼ºäº†å¤šå¤´æ³¨æ„åŠ›çš„æ•ˆæœï¼Œä»è€Œæ”¹å–„äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒåŠ¨æ€å’Œæ€§èƒ½ã€‚ä¼ ç»Ÿçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åœ¨å¢åŠ å¤´æ•°æ—¶ï¼Œå¾€å¾€ä¼šå‰Šå¼±æ¯ä¸ªå¤´çš„èƒ½åŠ›ï¼Œè€ŒKHAé€šè¿‡è®©å¤´éƒ¨ä¹‹é—´è¿›è¡Œç‰¹å¾çº§çš„äº¤äº’ï¼Œè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚KHAä½¿ç”¨å…±äº«çš„å¯¹è§’åˆå§‹åŒ–æŠ•å½±çŸ©é˜µï¼Œä¿æŒäº†å¤´éƒ¨çš„ä¸“ä¸šåŒ–ï¼ŒåŒæ—¶å…è®¸æ¨¡å‹é€æ­¥å­¦ä¹ ç»¼åˆçš„è·¨å¤´è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒKHAåœ¨è®­ç»ƒåŠ¨æ€å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23603', 'title': 'PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity', 'url': 'https://huggingface.co/papers/2510.23603', 'abstract': 'PixelRefer is a unified region-level MLLM framework that enhances fine-grained object-centric understanding using a Scale-Adaptive Object Tokenizer and Object-Centric Infusion module, achieving high performance and efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.', 'score': 19, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '17dedd3c9dff5a56', 'authors': ['Yuqian Yuan', 'Wenqiao Zhang', 'Xin Li', 'Shihao Wang', 'Kehan Li', 'Wentong Li', 'Jun Xiao', 'Lei Zhang', 'Beng Chin Ooi'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'The Hong Kong Polytechnic University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23603.jpg', 'data': {'categories': ['#cv', '#multimodal', '#games', '#dataset', '#optimization', '#reasoning', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM', 'desc': 'PixelRefer â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑÑ†ĞµĞ½Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Scale-Adaptive Object Tokenizer Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ· Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹. ĞĞ±Ğ»ĞµĞ³Ñ‡Ñ‘Ğ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ PixelRefer-Lite Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Object-Centric Infusion Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ PixelRefer-2.2M Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Object Understanding with PixelRefer', 'desc': 'PixelRefer is a new framework designed to improve how machines understand specific objects in images and videos, rather than just the overall scene. It uses a Scale-Adaptive Object Tokenizer to create detailed representations of objects, allowing for better reasoning about them. The framework also includes an Object-Centric Infusion module that helps combine global context with object information, making it more efficient. With a specially curated dataset for training, PixelRefer shows strong performance even with fewer examples, while its lighter version, PixelRefer-Lite, maintains accuracy with reduced computational demands.'}, 'zh': {'title': 'PixelReferï¼šç»†ç²’åº¦ç‰©ä½“ç†è§£çš„æ–°çªç ´', 'desc': 'PixelReferæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŒºåŸŸçº§å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¯¹ç»†ç²’åº¦ç‰©ä½“çš„ç†è§£ã€‚å®ƒé‡‡ç”¨äº†å¯ç¼©æ”¾çš„ç‰©ä½“æ ‡è®°å™¨å’Œç‰©ä½“ä¸­å¿ƒæ³¨å…¥æ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒå’Œè§†é¢‘ä¸­å®ç°ç”¨æˆ·æŒ‡å®šåŒºåŸŸçš„é«˜çº§ç†è§£ã€‚é€šè¿‡ç”Ÿæˆç´§å‡‘ä¸”è¯­ä¹‰ä¸°å¯Œçš„ç‰©ä½“è¡¨ç¤ºï¼ŒPixelReferæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è¯­ä¹‰ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPixelReferåœ¨è¾ƒå°‘çš„è®­ç»ƒæ ·æœ¬ä¸‹å®ç°äº†é¢†å…ˆçš„æ€§èƒ½ï¼Œè€Œå…¶é«˜æ•ˆå˜ä½“PixelRefer-Liteåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šä¹Ÿè¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23393', 'title': 'The Best of N Worlds: Aligning Reinforcement Learning with Best-of-N\n  Sampling via max@k Optimisation', 'url': 'https://huggingface.co/papers/2510.23393', 'abstract': "Optimizing the max@k metric through unbiased gradient estimates in RLVR improves the diversity and performance of Large Language Models in Best-of-N sampling scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The application of Reinforcement Learning with Verifiable Rewards (RLVR) to mathematical and coding domains has demonstrated significant improvements in the reasoning and problem-solving abilities of Large Language Models. Despite its success in single generation problem solving, the reinforcement learning fine-tuning process may harm the model's exploration ability, as reflected in decreased diversity of generations and a resulting degradation of performance during Best-of-N sampling for large N values. In this work, we focus on optimizing the max@k metric, a continuous generalization of pass@k. We derive an unbiased on-policy gradient estimate for direct optimization of this metric. Furthermore, we extend our derivations to the off-policy updates, a common element in modern RLVR algorithms, that allows better sample efficiency. Empirically, we show that our objective effectively optimizes max@k metric in off-policy scenarios, aligning the model with the Best-of-N inference strategy.", 'score': 12, 'issue_id': 6652, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '90ecb5b7986c30b0', 'authors': ['Farid Bagirov', 'Mikhail Arkhipov', 'Ksenia Sycheva', 'Evgeniy Glukhov', 'Egor Bogomolov'], 'affiliations': ['JetBrains Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.23393.jpg', 'data': {'categories': ['#rlhf', '#training', '#reasoning', '#rl', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ LLM Ğ´Ğ»Ñ Best-of-N ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RLVR) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚ÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Best-of-N ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸ N. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ max@k Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞ¼ĞµÑ‰Ñ‘Ğ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ² on-policy, Ñ‚Ğ°Ğº Ğ¸ Ğ² off-policy ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Best-of-N Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Enhancing Diversity and Performance in LLMs with RLVR Optimization', 'desc': "This paper discusses how to improve the performance and diversity of Large Language Models (LLMs) using Reinforcement Learning with Verifiable Rewards (RLVR). The authors identify that while RLVR enhances problem-solving abilities, it can reduce the model's exploration, leading to less diverse outputs during Best-of-N sampling. They propose a method to optimize the max@k metric, which helps in evaluating the quality of generated outputs. By deriving unbiased gradient estimates for both on-policy and off-policy updates, they demonstrate that their approach enhances sample efficiency and aligns the model better with Best-of-N strategies."}, 'zh': {'title': 'ä¼˜åŒ–max@kæŒ‡æ ‡ï¼Œæå‡è¯­è¨€æ¨¡å‹å¤šæ ·æ€§ä¸æ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¥ä¼˜åŒ–max@kæŒ‡æ ‡çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨Best-of-Né‡‡æ ·åœºæ™¯ä¸­çš„å¤šæ ·æ€§å’Œæ€§èƒ½ã€‚å°½ç®¡RLVRåœ¨å•ä¸€ç”Ÿæˆé—®é¢˜è§£å†³ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å¾®è°ƒè¿‡ç¨‹å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›ï¼Œå¯¼è‡´ç”Ÿæˆçš„å¤šæ ·æ€§ä¸‹é™ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— åçš„åœ¨æ”¿ç­–æ¢¯åº¦ä¼°è®¡æ–¹æ³•ï¼Œç›´æ¥ä¼˜åŒ–max@kæŒ‡æ ‡ï¼Œå¹¶æ‰©å±•åˆ°ç¦»æ”¿ç­–æ›´æ–°ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¦»æ”¿ç­–åœºæ™¯ä¸­æœ‰æ•ˆä¼˜åŒ–äº†max@kæŒ‡æ ‡ï¼Œä½¿æ¨¡å‹æ›´å¥½åœ°ä¸Best-of-Næ¨ç†ç­–ç•¥å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22946', 'title': 'LightBagel: A Light-weighted, Double Fusion Framework for Unified\n  Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2510.22946', 'abstract': 'A unified multimodal model achieves competitive performance with efficient fusion of generation and understanding models, using interleaved multimodal self-attention blocks and minimal training resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.', 'score': 10, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '0276884d3592cb5a', 'authors': ['Zeyu Wang', 'Zilong Chen', 'Chenhui Gou', 'Feng Li', 'Chaorui Deng', 'Deyao Zhu', 'Kunchang Li', 'Weihao Yu', 'Haoqin Tu', 'Haoqi Fan', 'Cihang Xie'], 'affiliations': ['ByteDance Seed Project', 'Monash University', 'Tsinghua University', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2510.22946.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ”—', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞ¹ Ğ¸ Ğ²Ğ»Ğ°ÑÑ‚Ğ²ÑƒĞ¹: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ÑƒÑ‚Ñ‘Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ ÑƒĞ¶Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ self-attention Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 35 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ ĞºĞ¾Ğ´, Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Efficient Fusion for Unified Multimodal Mastery', 'desc': 'This paper presents a unified multimodal model that efficiently combines generation and understanding capabilities using interleaved multimodal self-attention blocks. By fusing existing models instead of training from scratch, the approach significantly reduces computational resource requirements while maintaining competitive performance. The design allows for effective integration of high-level semantic information from understanding tasks with low-level spatial data from generation tasks. The model achieves strong results on various benchmarks with minimal training data, promoting further research in unified multimodal modeling.'}, 'zh': {'title': 'é«˜æ•ˆèåˆï¼Œç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„æœªæ¥', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œé€šè¿‡é«˜æ•ˆèåˆç”Ÿæˆå’Œç†è§£æ¨¡å‹ï¼Œå–å¾—äº†ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ç ”ç©¶è€…ä»¬é‡‡ç”¨äº†äº¤é”™çš„å¤šæ¨¡æ€è‡ªæ³¨æ„åŠ›å—ï¼Œå¹¶ä¸”åªéœ€æœ€å°‘çš„è®­ç»ƒèµ„æºã€‚è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¾¾åˆ°äº†0.91çš„GenEvalåˆ†æ•°ã€‚é€šè¿‡å…¬å¼€ä»£ç ã€æ¨¡å‹æƒé‡å’Œæ•°æ®é›†ï¼Œä½œè€…å¸Œæœ›æ”¯æŒæœªæ¥çš„ç»Ÿä¸€å¤šæ¨¡æ€å»ºæ¨¡ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22200', 'title': 'LongCat-Video Technical Report', 'url': 'https://huggingface.co/papers/2510.22200', 'abstract': 'LongCat-Video, a 13.6B parameter video generation model based on the Diffusion Transformer framework, excels in efficient and high-quality long video generation across multiple tasks using unified architecture, coarse-to-fine generation, and block sparse attention.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation is a critical pathway toward world models, with efficient long video inference as a key capability. Toward this end, we introduce LongCat-Video, a foundational video generation model with 13.6B parameters, delivering strong performance across multiple video generation tasks. It particularly excels in efficient and high-quality long video generation, representing our first step toward world models. Key features include: Unified architecture for multiple tasks: Built on the Diffusion Transformer (DiT) framework, LongCat-Video supports Text-to-Video, Image-to-Video, and Video-Continuation tasks with a single model; Long video generation: Pretraining on Video-Continuation tasks enables LongCat-Video to maintain high quality and temporal coherence in the generation of minutes-long videos; Efficient inference: LongCat-Video generates 720p, 30fps videos within minutes by employing a coarse-to-fine generation strategy along both the temporal and spatial axes. Block Sparse Attention further enhances efficiency, particularly at high resolutions; Strong performance with multi-reward RLHF: Multi-reward RLHF training enables LongCat-Video to achieve performance on par with the latest closed-source and leading open-source models. Code and model weights are publicly available to accelerate progress in the field.', 'score': 9, 'issue_id': 6645, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '70ad765a1f2c1821', 'authors': ['Meituan LongCat Team', 'Xunliang Cai', 'Qilong Huang', 'Zhuoliang Kang', 'Hongyu Li', 'Shijun Liang', 'Liya Ma', 'Siyu Ren', 'Xiaoming Wei', 'Rixu Xie', 'Tong Zhang'], 'affiliations': ['Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2510.22200.jpg', 'data': {'categories': ['#open_source', '#video', '#rlhf', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹', 'desc': 'LongCat-Video â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer Ñ 13.6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°ÑÑÑ Ğ½Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ñ‚ Ğ³Ñ€ÑƒĞ±Ğ¾Ğ³Ğ¾ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ (coarse-to-fine) Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾ÑÑĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ (Block Sparse Attention). ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ multi-reward RLHF Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ²Ğ¸Ğ´ĞµĞ¾ 720p Ğ¿Ñ€Ğ¸ 30 fps Ğ·Ğ° ÑÑ‡Ğ¸Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹.'}, 'en': {'title': 'LongCat-Video: Efficient Long Video Generation Unleashed!', 'desc': 'LongCat-Video is a powerful video generation model with 13.6 billion parameters, designed to create high-quality long videos efficiently. It utilizes the Diffusion Transformer framework, allowing it to perform various tasks like Text-to-Video and Video-Continuation with a single architecture. The model excels in generating long videos by maintaining quality and coherence through pretraining on specific tasks and employing a coarse-to-fine generation strategy. Additionally, its use of Block Sparse Attention improves efficiency, enabling the generation of 720p videos at 30 frames per second in just minutes, while multi-reward RLHF training enhances its performance to match leading models.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆé«˜è´¨é‡é•¿è§†é¢‘çš„é©å‘½æ€§æ¨¡å‹', 'desc': 'LongCat-Videoæ˜¯ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨æ¡†æ¶çš„13.6äº¿å‚æ•°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„é•¿è§†é¢‘ã€‚è¯¥æ¨¡å‹æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è§†é¢‘ã€å›¾åƒåˆ°è§†é¢‘å’Œè§†é¢‘ç»­å†™ï¼Œé‡‡ç”¨ç»Ÿä¸€æ¶æ„è¿›è¡Œå¤„ç†ã€‚é€šè¿‡åœ¨è§†é¢‘ç»­å†™ä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒï¼ŒLongCat-Videoèƒ½å¤Ÿåœ¨ç”Ÿæˆæ•°åˆ†é’Ÿé•¿çš„è§†é¢‘æ—¶ä¿æŒé«˜è´¨é‡å’Œæ—¶é—´ä¸€è‡´æ€§ã€‚å…¶é«˜æ•ˆæ¨ç†èƒ½åŠ›ä½¿å¾—ç”Ÿæˆ720pã€30fpsçš„è§†é¢‘ä»…éœ€å‡ åˆ†é’Ÿï¼Œä¸”é€šè¿‡å—ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶è¿›ä¸€æ­¥æå‡äº†é«˜åˆ†è¾¨ç‡ä¸‹çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23544', 'title': 'LimRank: Less is More for Reasoning-Intensive Information Reranking', 'url': 'https://huggingface.co/papers/2510.23544', 'abstract': 'LIMRANK-SYNTHESIZER generates synthetic data to fine-tune LIMRANK, achieving competitive performance with minimal supervision on information reranking tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.', 'score': 7, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'ea49406b584c9f63', 'authors': ['Tingyu Song', 'Yilun Zhao', 'Siyue Zhang', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23544.jpg', 'data': {'categories': ['#data', '#rag', '#open_source', '#benchmark', '#dataset', '#synthetic', '#reasoning', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LIMRANK-SYNTHESIZER â€” pipeline Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ LIMRANK Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ fine-tuning Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼ĞµĞ½ĞµĞµ 5% Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. LIMRANK Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… BRIGHT Ğ¸ FollowIR, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ Ğ¸ retrieval-augmented generation.'}, 'en': {'title': 'Efficient Reranking with Minimal Supervision', 'desc': 'LIMRANK-SYNTHESIZER is a novel approach that generates synthetic data to enhance the performance of the LIMRANK model for information reranking tasks. Unlike traditional methods that require extensive fine-tuning with large datasets, this method shows that high-quality supervision can be minimal yet effective. The pipeline produces diverse and realistic reranking examples, allowing LIMRANK to be fine-tuned efficiently. Evaluations on benchmarks like BRIGHT and FollowIR reveal that LIMRANK can achieve competitive results while using significantly less training data than previous approaches.'}, 'zh': {'title': 'ç”¨åˆæˆæ•°æ®æå‡ä¿¡æ¯é‡æ’åºæ€§èƒ½', 'desc': 'LIMRANK-SYNTHESIZER æ˜¯ä¸€ç§ç”Ÿæˆåˆæˆæ•°æ®çš„æ–¹æ³•ï¼Œç”¨äºå¾®è°ƒ LIMRANK æ¨¡å‹ï¼Œä»¥åœ¨ä¿¡æ¯é‡æ’åºä»»åŠ¡ä¸­å®ç°ç«äº‰æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•éœ€è¦å¤§é‡è®¡ç®—èµ„æºè¿›è¡Œå¤§è§„æ¨¡å¾®è°ƒä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€å°‘é‡é«˜è´¨é‡çš„ç›‘ç£æ•°æ®ã€‚æˆ‘ä»¬è®¾è®¡äº† LIMRANK-SYNTHESIZERï¼Œè¿™æ˜¯ä¸€ç§å¯é‡ç”¨çš„å¼€æºç®¡é“ï¼Œç”¨äºç”Ÿæˆå¤šæ ·åŒ–ã€å…·æœ‰æŒ‘æˆ˜æ€§å’ŒçœŸå®æ„Ÿçš„é‡æ’åºç¤ºä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLIMRANK åœ¨ä½¿ç”¨ä¸åˆ° 5% çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œä»èƒ½åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23272', 'title': 'Code Aesthetics with Agentic Reward Feedback', 'url': 'https://huggingface.co/papers/2510.23272', 'abstract': 'A new pipeline enhances the aesthetic quality of LLM-generated code through instruction-tuning, agentic reward feedback, and joint optimization, outperforming existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.', 'score': 6, 'issue_id': 6649, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '50c67df7fc73ebb2', 'authors': ['Bang Xiao', 'Lingjie Jiang', 'Shaohan Huang', 'Tengchao Lv', 'Yupan Huang', 'Xun Wu', 'Lei Cui', 'Furu Wei'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'Zhiyuan College, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23272.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#training', '#rlhf', '#optimization', '#architecture', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²-Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ´Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AesCode-358K Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ´Ğ°, Ğ½Ğ¾ Ğ¸ ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO-AR, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºÑƒ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AesCoder-4B. Ğ­Ñ‚Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° GPT-4o Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğ¼Ğ¸ open-source Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° 480-685 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Code Aesthetics in LLMs with Innovative Techniques', 'desc': 'This paper presents a new pipeline designed to improve the aesthetic quality of code generated by Large Language Models (LLMs). It introduces AesCode-358K, a dataset specifically for instruction-tuning focused on code aesthetics. The authors also propose a multi-agent system called agentic reward feedback, which evaluates various aspects of code aesthetics. By integrating these components into the GRPO algorithm, the study demonstrates significant improvements in code aesthetics and functionality, outperforming existing models in benchmark tests.'}, 'zh': {'title': 'æå‡ä»£ç ç¾å­¦çš„æ™ºèƒ½ç®¡é“', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ç®¡é“ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒä¼˜ã€ä»£ç†å¥–åŠ±åé¦ˆå’Œè”åˆä¼˜åŒ–æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä»£ç çš„ç¾å­¦è´¨é‡ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºAesCode-358Kçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¸“æ³¨äºä»£ç ç¾å­¦ï¼Œå¹¶å¼•å…¥äº†ä»£ç†å¥–åŠ±åé¦ˆç³»ç»Ÿæ¥è¯„ä¼°ä»£ç çš„å¯æ‰§è¡Œæ€§å’Œç¾è§‚æ€§ã€‚é€šè¿‡å°†è¿™äº›ä¿¡å·æ•´åˆåˆ°GRPOç®—æ³•ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†åŠŸèƒ½æ€§ä¸ä»£ç ç¾å­¦çš„è”åˆä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•æ˜¾è‘—æå‡äº†ä»£ç ç¾å­¦çš„è¡¨ç°ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.21003', 'title': 'Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models\n  with Conditional Score Distillation', 'url': 'https://huggingface.co/papers/2510.21003', 'abstract': 'A new method, Distilled Decoding 2 (DD2), enables one-step sampling for image auto-regressive models with minimal performance degradation and significant speed-up compared to previous methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel conditional score distillation loss to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3times training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/imagination-research/Distilled-Decoding-2.', 'score': 6, 'issue_id': 6645, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 23', 'zh': '10æœˆ23æ—¥'}, 'hash': 'c525f5bb0a9bd473', 'authors': ['Enshu Liu', 'Qian Chen', 'Xuefei Ning', 'Shengen Yan', 'Guohao Dai', 'Zinan Lin', 'Yu Wang'], 'affiliations': ['Infinigence-AI Beijing, China', 'Microsoft Research Redmond, WA, USA', 'Shanghai Jiaotong University Shanghai, China', 'Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.21003.jpg', 'data': {'categories': ['#optimization', '#cv', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞĞ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ score-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Distilled Decoding 2 (DD2) Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° DD1, Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‘Ğ½Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ conditional score Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². DD2 Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° FID ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ 3.40 Ğ´Ğ¾ 5.43 Ğ½Ğ° ImageNet-256. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ AR Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ½Ğ° 67% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 12.3 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ DD1.'}, 'en': {'title': 'Fast and Flexible One-Step Sampling with DD2', 'desc': 'The paper introduces Distilled Decoding 2 (DD2), a new method for improving one-step sampling in image auto-regressive (AR) models. DD2 addresses the slow generation speed of previous methods by eliminating the need for a pre-defined mapping, which enhances flexibility. It utilizes a conditional score distillation loss to train a generator that predicts the conditional score based on previous tokens, significantly reducing performance degradation. Experimental results demonstrate that DD2 achieves faster sampling with minimal quality loss, making it a promising advancement in the field of visual generative models.'}, 'zh': {'title': 'ä¸€æ­¥é‡‡æ ·ï¼Œå¿«é€Ÿé«˜æ•ˆçš„å›¾åƒç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç§°ä¸ºè’¸é¦è§£ç 2ï¼ˆDD2ï¼‰ï¼Œæ—¨åœ¨æé«˜å›¾åƒè‡ªå›å½’æ¨¡å‹çš„ä¸€æ­¥é‡‡æ ·æ•ˆç‡ã€‚ä¸ä¹‹å‰çš„è’¸é¦è§£ç 1ï¼ˆDD1ï¼‰ç›¸æ¯”ï¼ŒDD2åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—åŠ å¿«äº†ç”Ÿæˆé€Ÿåº¦ã€‚DD2ä¸ä¾èµ–äºé¢„å®šä¹‰çš„æ˜ å°„ï¼Œè€Œæ˜¯å°†åŸå§‹è‡ªå›å½’æ¨¡å‹è§†ä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡æ¡ä»¶åˆ†æ•°è’¸é¦æŸå¤±æ¥è®­ç»ƒç”Ÿæˆå™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDD2åœ¨ImageNet-256ä¸Šå®ç°äº†ä¸€æ­¥é‡‡æ ·ï¼ŒFIDå€¼ä»…ä»3.40å¢åŠ åˆ°5.43ï¼Œä¸”è®­ç»ƒé€Ÿåº¦æé«˜äº†12.3å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23571', 'title': 'RobotArena infty: Scalable Robot Benchmarking via Real-to-Sim\n  Translation', 'url': 'https://huggingface.co/papers/2510.23571', 'abstract': 'A new benchmarking framework uses large-scale simulated environments with human feedback to evaluate robot policies, addressing limitations in real-world testing and existing simulation benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining "success" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today\'s robotics landscape.', 'score': 5, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '6fc2464d3d86a26e', 'authors': ['Yash Jangir', 'Yidi Zhang', 'Kashu Yamazaki', 'Chenyu Zhang', 'Kuan-Hsun Tu', 'Tsung-Wei Ke', 'Lei Ke', 'Yonatan Bisk', 'Katerina Fragkiadaki'], 'affiliations': ['Carnegie Mellon University', 'National Taiwan University', 'Peking University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.23571.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#agents', '#games', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ VLA (Vision-Language-Action) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ² ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ VLM, 2D-to-3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ VLM-ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ ĞºÑ€Ğ°ÑƒĞ´Ğ²Ğ¾Ñ€ĞºĞµÑ€Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´Ñ‹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Robot Evaluation with Human-Enhanced Simulations', 'desc': 'This paper presents a new benchmarking framework designed to evaluate robot policies in large-scale simulated environments with human feedback. It addresses the limitations of real-world testing, which is often slow and unsafe, and existing simulation benchmarks that fail to assess models trained on real-world data. The framework utilizes advances in vision-language models and generative modeling to create simulated environments from video demonstrations, allowing for more effective evaluation of robot policies. By incorporating human preference judgments and systematically varying simulation conditions, the framework enhances the robustness and scalability of robot policy assessments.'}, 'zh': {'title': 'æ–°åŸºå‡†æ¡†æ¶ï¼šæå‡æœºå™¨äººç­–ç•¥è¯„ä¼°çš„æœ‰æ•ˆæ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¤§è§„æ¨¡æ¨¡æ‹Ÿç¯å¢ƒå’Œäººç±»åé¦ˆæ¥è¯„ä¼°æœºå™¨äººç­–ç•¥ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ç°å®ä¸–ç•Œæµ‹è¯•å’Œç°æœ‰æ¨¡æ‹ŸåŸºå‡†çš„å±€é™æ€§ï¼Œå°¤å…¶æ˜¯åœ¨è¯„ä¼°å¤šæ ·åŒ–ä»»åŠ¡çš„æœºå™¨äººé€šç”¨æ€§æ–¹é¢ã€‚é€šè¿‡åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹å’Œç”Ÿæˆå»ºæ¨¡æŠ€æœ¯ï¼Œæœ¬æ–‡èƒ½å¤Ÿå°†è§†é¢‘æ¼”ç¤ºè‡ªåŠ¨è½¬æ¢ä¸ºæ¨¡æ‹Ÿç¯å¢ƒä¸­çš„å¯¹åº”ç‰©ã€‚æœ€ç»ˆï¼Œè¿™ç§æ¡†æ¶æä¾›äº†ä¸€ç§å¯æŒç»­ã€å¯é‡å¤å’Œå¯æ‰©å±•çš„åŸºå‡†ï¼Œå¸®åŠ©è¯„ä¼°åœ¨çœŸå®ä¸–ç•Œä¸­è®­ç»ƒçš„æœºå™¨äººæ“ä½œç­–ç•¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23479', 'title': 'MergeMix: A Unified Augmentation Paradigm for Visual and Multi-Modal\n  Understanding', 'url': 'https://huggingface.co/papers/2510.23479', 'abstract': 'MergeMix, a training-time augmentation method, combines attention-aware image mixing and preference-driven training to improve vision-language alignment in multi-modal large language models with enhanced efficiency and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language alignment in multi-modal large language models (MLLMs) typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). SFT is stable and efficient but requires large-scale human annotations and cannot capture subtle preferences, while RL brings in a reward signal for training, but suffers from overhead and instability. These limitations highlight a trade-off between scalability, robustness, and alignment quality. To address this, we propose MergeMix, a training-time augmentation paradigm that bridges SFT and RL. It first applies an attention-aware image mixing via token merge with more cluster representation and spatial context, and then presents a preference-driven training paradigm for MLLMs by building preference pairs with mixed images and raw images, and optimizing via SimPO loss. As a mixup augmentation, MergeMix enhances attention consistency and efficiency, surpassing other heuristic-based methods in classification. Extensive experiments demonstrate that MergeMix achieves competitive accuracy with improved efficiency, providing a scalable approach to preference alignment in classification and MLLMs.', 'score': 4, 'issue_id': 6659, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': 'e077751d95318e8c', 'authors': ['Xin Jin', 'Siyuan Li', 'Siyong Jian', 'Kai Yu', 'Huan Wang'], 'affiliations': ['Westlake University, Hangzhou, China', 'Zhejiang University, College of Computer Science and Technology, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.23479.jpg', 'data': {'categories': ['#training', '#multimodal', '#optimization', '#alignment'], 'emoji': 'ğŸ¨', 'ru': {'title': 'MergeMix: ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MergeMix â€” Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLMs), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ attention-Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ÑĞ¼ĞµÑˆĞ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ SimPO loss. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° supervised fine-tuning Ğ¸ reinforcement learning, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ….'}, 'en': {'title': 'MergeMix: Bridging the Gap in Vision-Language Alignment', 'desc': 'MergeMix is a novel training-time augmentation technique designed to enhance vision-language alignment in multi-modal large language models (MLLMs). It combines attention-aware image mixing with a preference-driven training approach, addressing the limitations of traditional supervised fine-tuning and reinforcement learning. By utilizing token merging for better cluster representation and spatial context, MergeMix optimizes model performance through a SimPO loss that leverages preference pairs. The method not only improves classification accuracy but also increases training efficiency, making it a scalable solution for aligning preferences in MLLMs.'}, 'zh': {'title': 'MergeMixï¼šæå‡å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰-è¯­è¨€å¯¹é½', 'desc': 'MergeMixæ˜¯ä¸€ç§è®­ç»ƒæ—¶å¢å¼ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„è§†è§‰-è¯­è¨€å¯¹é½æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚å®ƒç»“åˆäº†æ³¨æ„åŠ›æ„ŸçŸ¥çš„å›¾åƒæ··åˆå’ŒåŸºäºåå¥½çš„è®­ç»ƒï¼Œå…‹æœäº†ä¼ ç»Ÿç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§ã€‚é€šè¿‡ä½¿ç”¨æ··åˆå›¾åƒå’ŒåŸå§‹å›¾åƒæ„å»ºåå¥½å¯¹ï¼Œå¹¶é€šè¿‡SimPOæŸå¤±è¿›è¡Œä¼˜åŒ–ï¼ŒMergeMixå¢å¼ºäº†æ³¨æ„åŠ›ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMergeMixåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„åå¥½å¯¹é½æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23594', 'title': 'PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection', 'url': 'https://huggingface.co/papers/2510.23594', 'abstract': "PRISM-Bench evaluates models' reasoning processes by identifying errors in step-by-step solutions to visual puzzles, highlighting gaps between fluent generation and logical consistency.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.", 'score': 3, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '0225ecde49430d54', 'authors': ['Yusu Qian', 'Cheng Wan', 'Chao Jia', 'Yinfei Yang', 'Qingyu Zhao', 'Zhe Gan'], 'affiliations': ['Apple', 'Cornell', 'Weill Cornell Medicine'], 'pdf_title_img': 'assets/pdf/title_img/2510.23594.jpg', 'data': {'categories': ['#cv', '#multimodal', '#benchmark', '#interpretability', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¸, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ²: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ AI', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ PRISM-Bench â€” Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğ»Ğ¾Ğ¼Ğ¾Ğº. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, PRISM-Bench Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ (chain-of-thought). Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ… ÑƒĞ¼ĞµĞ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…, Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Evaluating Reasoning, Not Just Answers', 'desc': 'PRISM-Bench is a new benchmark designed to evaluate how well machine learning models reason through visual puzzles. It focuses on identifying errors in the step-by-step reasoning process rather than just checking if the final answer is correct. By requiring models to pinpoint the first mistake in a chain-of-thought, it assesses their logical consistency and error detection abilities. This approach reveals that many advanced models struggle with reasoning, even when they generate plausible solutions, highlighting the importance of thorough diagnostic evaluations in machine learning.'}, 'zh': {'title': 'æ¨ç†èƒ½åŠ›çš„ç»†è‡´è¯„ä¼°ï¼šPRISM-Bench', 'desc': 'PRISM-Bench æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨¡å‹æ¨ç†è¿‡ç¨‹çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯†åˆ«è§†è§‰éš¾é¢˜é€æ­¥è§£å†³ä¸­çš„é”™è¯¯ã€‚ä¸ä»¥å¾€ä»…æµ‹é‡æœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„è¯„ä¼°ä¸åŒï¼ŒPRISM-Bench å¼•å…¥äº†ä¸€é¡¹è¯Šæ–­ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªè§†è§‰éš¾é¢˜å’ŒåŒ…å«ä¸€ä¸ªé”™è¯¯çš„é€æ­¥æ¨ç†é“¾ï¼Œæ¨¡å‹å¿…é¡»è¯†åˆ«å‡ºç¬¬ä¸€ä¸ªé”™è¯¯æ­¥éª¤ã€‚è¿™ä¸ªè®¾ç½®ä½¿å¾—å¯¹é€»è¾‘ä¸€è‡´æ€§ã€é”™è¯¯æ£€æµ‹å’Œè§†è§‰æ¨ç†çš„ç»†è‡´è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæµç•…çš„æ¨ç†é“¾ï¼Œä½†åœ¨è¯†åˆ«ç®€å•é€»è¾‘é”™è¯¯æ–¹é¢ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22975', 'title': 'VoMP: Predicting Volumetric Mechanical Property Fields', 'url': 'https://huggingface.co/papers/2510.22975', 'abstract': "VoMP uses a feed-forward method with a Geometry Transformer to predict accurate volumetric material properties from 3D objects, outperforming existing methods in both accuracy and speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Physical simulation relies on spatially-varying mechanical properties, often laboriously hand-crafted. VoMP is a feed-forward method trained to predict Young's modulus (E), Poisson's ratio (nu), and density (rho) throughout the volume of 3D objects, in any representation that can be rendered and voxelized. VoMP aggregates per-voxel multi-view features and passes them to our trained Geometry Transformer to predict per-voxel material latent codes. These latents reside on a manifold of physically plausible materials, which we learn from a real-world dataset, guaranteeing the validity of decoded per-voxel materials. To obtain object-level training data, we propose an annotation pipeline combining knowledge from segmented 3D datasets, material databases, and a vision-language model, along with a new benchmark. Experiments show that VoMP estimates accurate volumetric properties, far outperforming prior art in accuracy and speed.", 'score': 3, 'issue_id': 6648, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '9cad0f0687c4a50a', 'authors': ['Rishit Dagli', 'Donglai Xiang', 'Vismay Modi', 'Charles Loop', 'Clement Fuji Tsang', 'Anka He Chen', 'Anita Hu', 'Gavriel State', 'David I. W. Levin', 'Maria Shugrina'], 'affiliations': ['NVIDIA', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.22975.jpg', 'data': {'categories': ['#science', '#dataset', '#benchmark', '#optimization', '#3d'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸Ğ· 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°', 'desc': 'VoMP â€” ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² (Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ®Ğ½Ğ³Ğ°, ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ ĞŸÑƒĞ°ÑÑĞ¾Ğ½Ğ° Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ) Ğ´Ğ»Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Geometry Transformer, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ²Ğ¸Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ĞºÑĞµĞ»Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ñ‹ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ², Ğ»ĞµĞ¶Ğ°Ñ‰Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ². Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ pipeline Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ 3D-Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹, Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ¸ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. VoMP Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ².'}, 'en': {'title': 'Predicting Material Properties with Speed and Accuracy', 'desc': "VoMP is a machine learning method that predicts the mechanical properties of materials in 3D objects using a feed-forward approach with a Geometry Transformer. It focuses on estimating properties like Young's modulus, Poisson's ratio, and density across the volume of objects, which can be represented in various formats. By aggregating features from multiple views and utilizing a trained model, VoMP ensures that the predicted material properties are physically plausible. The method also introduces a new annotation pipeline for training data, significantly improving the accuracy and speed of volumetric property estimation compared to existing techniques."}, 'zh': {'title': 'VoMPï¼šå¿«é€Ÿå‡†ç¡®çš„3Dææ–™å±æ€§é¢„æµ‹', 'desc': 'VoMPæ˜¯ä¸€ç§å‰é¦ˆæ–¹æ³•ï¼Œåˆ©ç”¨å‡ ä½•å˜æ¢å™¨ä»3Dç‰©ä½“ä¸­é¢„æµ‹å‡†ç¡®çš„ä½“ç§¯ææ–™å±æ€§ã€‚å®ƒèƒ½å¤Ÿé¢„æµ‹æ¨æ°æ¨¡é‡ã€æ³Šæ¾æ¯”å’Œå¯†åº¦ï¼Œå¹¶ä¸”åœ¨å‡†ç¡®æ€§å’Œé€Ÿåº¦ä¸Šè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ã€‚VoMPé€šè¿‡èšåˆæ¯ä¸ªä½“ç´ çš„å¤šè§†è§’ç‰¹å¾ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™è®­ç»ƒå¥½çš„å‡ ä½•å˜æ¢å™¨ï¼Œæ¥é¢„æµ‹æ¯ä¸ªä½“ç´ çš„ææ–™æ½œåœ¨ç¼–ç ã€‚è¿™äº›æ½œåœ¨ç¼–ç ä½äºç‰©ç†ä¸Šåˆç†çš„ææ–™æµå½¢ä¸Šï¼Œç¡®ä¿äº†è§£ç çš„æ¯ä¸ªä½“ç´ ææ–™çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22907', 'title': 'Language Server CLI Empowers Language Agents with Process Rewards', 'url': 'https://huggingface.co/papers/2510.22907', 'abstract': 'Lanser-CLI orchestrates Language Server Protocol servers for coding agents and CI, providing deterministic workflows and actionable process rewards based on verified code facts.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent\'s planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli', 'score': 3, 'issue_id': 6645, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '9c063981ba8b33a5', 'authors': ['Yifan Zhang', 'Lanser Contributors'], 'affiliations': ['Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22907.jpg', 'data': {'categories': ['#hallucinations', '#agents', '#optimization', '#plp', '#training'], 'emoji': 'ğŸ”§', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: language servers ĞºĞ°Ğº Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ° Ğ´Ğ»Ñ coding-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Lanser-CLI â€” Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Language Server Protocol ÑĞµÑ€Ğ²ĞµÑ€Ğ°Ğ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ»Ñ coding-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ CI/CD ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ API Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°, language servers Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ¾Ğ´Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Analysis Bundles, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ°Ğ´Ñ€ĞµÑĞ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Selector DSL Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¾Ñ‚ language server ĞºĞ°Ğº process reward Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¼ Ğ¸ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°.'}, 'en': {'title': 'Reliable Coding with Deterministic Workflows', 'desc': 'Lanser-CLI is a command-line interface that manages Language Server Protocol (LSP) servers to enhance coding agents and continuous integration (CI) processes. It aims to provide reliable workflows by utilizing verified code facts, addressing the common issues of large language models that often generate incorrect API calls and misplace code edits. The system introduces a Selector DSL for precise code addressing, deterministic Analysis Bundles for consistent responses, and a safety envelope for code modifications. Additionally, it offers a process-reward mechanism based on language server diagnostics, ensuring that the planning of coding agents aligns with actual program behavior.'}, 'zh': {'title': 'Lanser-CLIï¼šç¼–ç¨‹ä»£ç†çš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'Lanser-CLI æ˜¯ä¸€ä¸ªç”¨äºç¼–ç¨‹ä»£ç†å’ŒæŒç»­é›†æˆçš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå®ƒåè°ƒè¯­è¨€æœåŠ¡å™¨åè®®ï¼ˆLSPï¼‰æœåŠ¡å™¨ï¼Œæä¾›ç¡®å®šæ€§çš„å·¥ä½œæµç¨‹å’ŒåŸºäºéªŒè¯ä»£ç äº‹å®çš„å¯æ“ä½œè¿‡ç¨‹å¥–åŠ±ã€‚è¯¥å·¥å…·è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ API ç”Ÿæˆå’Œç¼–è¾‘å®šä½æ–¹é¢çš„ä¸è¶³ï¼Œé€šè¿‡æä¾›ç»“æ„ä¿¡æ¯å’Œå¯æ“ä½œçš„è¿‡ç¨‹å¥–åŠ±ï¼Œå¸®åŠ©ä»£ç†æ›´å¥½åœ°ä¸ç¨‹åºç°å®å¯¹é½ã€‚Lanser-CLI å¼•å…¥äº†ä¸€ç§å¼ºå¤§çš„åœ°å€æ–¹æ¡ˆï¼Œç¡®ä¿è¯­è¨€æœåŠ¡å™¨çš„å“åº”æ ‡å‡†åŒ–ï¼Œå¹¶æä¾›å®‰å…¨çš„æ“ä½œç¯å¢ƒã€‚é€šè¿‡åœ¨çº¿è®¡ç®—å’Œç¦»çº¿é‡æ”¾çš„è¿‡ç¨‹å¥–åŠ±ï¼ŒLanser-CLI ä½¿å¾—è¿‡ç¨‹ç›‘ç£å’Œåäº‹å®åˆ†æå˜å¾—æ›´åŠ å¯è¡Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22849', 'title': 'Once Upon an Input: Reasoning via Per-Instance Program Synthesis', 'url': 'https://huggingface.co/papers/2510.22849', 'abstract': 'Per-Instance Program Synthesis (PIPS) enhances LLM performance by generating and refining instance-level programs with structural feedback, improving accuracy and reducing undesirable solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.', 'score': 2, 'issue_id': 6657, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': '37b787d547a66df6', 'authors': ['Adam Stein', 'Neelay Velingker', 'Mayur Naik', 'Eric Wong'], 'affiliations': ['University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2510.22849.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#optimization', '#benchmark', '#math'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ˜Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ PIPS, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, PIPS Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñƒ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚-ĞºĞµĞ¹ÑĞ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ñ€ÑĞ¼Ñ‹Ğ¼ inference Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 8.6% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ PoT Ğ¸ 9.4% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ CoT, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 65.1% Ğ½Ğ° Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing LLMs with Instance-Level Program Synthesis', 'desc': 'Per-Instance Program Synthesis (PIPS) is a novel approach that enhances the performance of large language models (LLMs) by generating and refining programs tailored to specific instances. It utilizes structural feedback to improve the accuracy of these programs while minimizing the occurrence of undesirable outputs, particularly in complex reasoning tasks. Unlike previous methods that depend on task-specific guidance, PIPS operates independently, allowing for more flexible and effective program synthesis. Additionally, it employs a confidence metric to determine whether to use direct inference or program synthesis for each instance, leading to significant improvements in accuracy across various benchmarks.'}, 'zh': {'title': 'æ¯å®ä¾‹ç¨‹åºåˆæˆï¼šæå‡LLMæ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºæ¯å®ä¾‹ç¨‹åºåˆæˆï¼ˆPIPSï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚PIPSé€šè¿‡ç”Ÿæˆå’Œä¼˜åŒ–å®ä¾‹çº§ç¨‹åºï¼Œå¹¶åˆ©ç”¨ç»“æ„åé¦ˆæ¥æé«˜å‡†ç¡®æ€§ï¼Œå‡å°‘ä¸ç†æƒ³çš„è§£å†³æ–¹æ¡ˆã€‚ä¸ä¼ ç»Ÿçš„ä¸­é—´æ¨ç†æ­¥éª¤æ–¹æ³•ç›¸æ¯”ï¼ŒPIPSä¸ä¾èµ–äºç‰¹å®šä»»åŠ¡çš„æŒ‡å¯¼æˆ–æ˜ç¡®çš„æµ‹è¯•ç”¨ä¾‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPIPSåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ï¼Œå¹¶å‡å°‘äº†ç®—æ³•ä»»åŠ¡ä¸­ä¸ç†æƒ³ç¨‹åºçš„ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22236', 'title': 'DiffusionLane: Diffusion Model for Lane Detection', 'url': 'https://huggingface.co/papers/2510.22236', 'abstract': 'DiffusionLane, a diffusion-based model, enhances lane detection by refining noisy lane anchors through a hybrid diffusion decoder and auxiliary head, achieving superior performance across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present a novel diffusion-based model for lane detection, called DiffusionLane, which treats the lane detection task as a denoising diffusion process in the parameter space of the lane. Firstly, we add the Gaussian noise to the parameters (the starting point and the angle) of ground truth lanes to obtain noisy lane anchors, and the model learns to refine the noisy lane anchors in a progressive way to obtain the target lanes. Secondly, we propose a hybrid decoding strategy to address the poor feature representation of the encoder, resulting from the noisy lane anchors. Specifically, we design a hybrid diffusion decoder to combine global-level and local-level decoders for high-quality lane anchors. Then, to improve the feature representation of the encoder, we employ an auxiliary head in the training stage to adopt the learnable lane anchors for enriching the supervision on the encoder. Experimental results on four benchmarks, Carlane, Tusimple, CULane, and LLAMAS, show that DiffusionLane possesses a strong generalization ability and promising detection performance compared to the previous state-of-the-art methods. For example, DiffusionLane with ResNet18 surpasses the existing methods by at least 1\\% accuracy on the domain adaptation dataset Carlane. Besides, DiffusionLane with MobileNetV4 gets 81.32\\% F1 score on CULane, 96.89\\% accuracy on Tusimple with ResNet34, and 97.59\\% F1 score on LLAMAS with ResNet101. Code will be available at https://github.com/zkyntu/UnLanedet.', 'score': 2, 'issue_id': 6654, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '3d725d654c6c0ad6', 'authors': ['Kunyang Zhou', 'Yeqin Shao'], 'affiliations': ['School of Automation, Southeast University', 'School of ZhangJian, Nantong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22236.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#cv', '#benchmark', '#architecture'], 'emoji': 'ğŸ›£ï¸', 'ru': {'title': 'Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²', 'desc': 'DiffusionLane - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»Ğ¾Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°ÑˆÑƒĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ»Ğ¾Ñ (Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ¸ ÑƒĞ³Ğ¾Ğ»). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ¸Ğ¹ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… (Carlane, Tusimple, CULane, LLAMAS), Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ½Ğ° 1% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ»ĞµĞ³ĞºĞ¸Ğ¼Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ğ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ MobileNetV4.'}, 'en': {'title': 'Refining Lane Detection with DiffusionLane', 'desc': 'DiffusionLane is a new model designed for lane detection that uses a diffusion process to improve the accuracy of lane anchors. It starts by adding Gaussian noise to the parameters of the true lanes, creating noisy anchors that the model learns to refine progressively. The model features a hybrid diffusion decoder that combines both global and local decoding strategies to enhance feature representation. Experimental results demonstrate that DiffusionLane outperforms existing methods on several benchmarks, showcasing its strong generalization and detection capabilities.'}, 'zh': {'title': 'DiffusionLaneï¼šæå‡è½¦é“æ£€æµ‹çš„æ–°æ–¹æ³•', 'desc': 'DiffusionLaneæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„è½¦é“æ£€æµ‹æ¨¡å‹ï¼Œé€šè¿‡æ··åˆæ‰©æ•£è§£ç å™¨å’Œè¾…åŠ©å¤´æ¥ä¼˜åŒ–å™ªå£°è½¦é“é”šç‚¹ï¼Œä»è€Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ¨¡å‹å°†è½¦é“æ£€æµ‹ä»»åŠ¡è§†ä¸ºå‚æ•°ç©ºé—´ä¸­çš„å»å™ªæ‰©æ•£è¿‡ç¨‹ï¼Œé¦–å…ˆå¯¹çœŸå®è½¦é“çš„å‚æ•°æ·»åŠ é«˜æ–¯å™ªå£°ï¼Œç”Ÿæˆå™ªå£°è½¦é“é”šç‚¹ã€‚ç„¶åï¼Œæ¨¡å‹é€æ­¥å­¦ä¹ ä¼˜åŒ–è¿™äº›å™ªå£°é”šç‚¹ï¼Œä»¥è·å¾—ç›®æ ‡è½¦é“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusionLaneåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä¼˜å¼‚çš„æ£€æµ‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.20512', 'title': 'EchoDistill: Bidirectional Concept Distillation for One-Step Diffusion\n  Personalization', 'url': 'https://huggingface.co/papers/2510.20512', 'abstract': "A bidirectional concept distillation framework enhances one-step text-to-image diffusion models by leveraging a multi-step model, improving personalization and generative quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in accelerating text-to-image (T2I) diffusion models have enabled the synthesis of high-fidelity images even in a single step. However, personalizing these models to incorporate novel concepts remains a challenge due to the limited capacity of one-step models to capture new concept distributions effectively. We propose a bidirectional concept distillation framework, EchoDistill, to enable one-step diffusion personalization (1-SDP). Our approach involves an end-to-end training process where a multi-step diffusion model (teacher) and a one-step diffusion model (student) are trained simultaneously. The concept is first distilled from the teacher model to the student, and then echoed back from the student to the teacher. During the EchoDistill, we share the text encoder between the two models to ensure consistent semantic understanding. Following this, the student model is optimized with adversarial losses to align with the real image distribution and with alignment losses to maintain consistency with the teacher's output. Furthermore, we introduce the bidirectional echoing refinement strategy, wherein the student model leverages its faster generation capability to feedback to the teacher model. This bidirectional concept distillation mechanism not only enhances the student ability to personalize novel concepts but also improves the generative quality of the teacher model. Our experiments demonstrate that this collaborative framework significantly outperforms existing personalization methods over the 1-SDP setup, establishing a novel paradigm for rapid and effective personalization in T2I diffusion models.", 'score': 2, 'issue_id': 6652, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 23', 'zh': '10æœˆ23æ—¥'}, 'hash': '44e490a1af9633bd', 'authors': ['Yixiong Yang', 'Tao Wu', 'Senmao Li', 'Shiqi Yang', 'Yaxing Wang', 'Joost van de Weijer', 'Kai Wang'], 'affiliations': ['City University of Hong Kong, HK SAR, China', 'Computer Vision Center, Universitat AutÃ²noma de Barcelona, Spain', 'Harbin Institute of Technology (Shenzhen), China', 'Program of Computer Science, City University of Hong Kong (Dongguan), China', 'VCIP, CS, Nankai University, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.20512.jpg', 'data': {'categories': ['#cv', '#multimodal', '#training', '#diffusion', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ”Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº EchoDistill Ğ´Ğ»Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ğ° Ğ¾Ğ´Ğ¸Ğ½ ÑˆĞ°Ğ³ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¾Ğ¼, Ğ³Ğ´Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿ĞµÑ€ĞµĞ´Ğ°ÑÑ‚ÑÑ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğº ÑƒÑ‡ĞµĞ½Ğ¸ĞºÑƒ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Â«ÑÑ…Ğ¾Ğ¼Â» Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ÑÑ‚ÑÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ñ Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼, Ğ° ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ adversarial Ğ¸ alignment Ğ»Ğ¾ÑÑĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ°Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ°Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ğ°Ğ¼.'}, 'en': {'title': 'Enhancing Personalization in Text-to-Image Models with EchoDistill', 'desc': "This paper introduces EchoDistill, a bidirectional concept distillation framework designed to enhance one-step text-to-image (T2I) diffusion models. By training a multi-step model (teacher) alongside a one-step model (student), the framework allows for effective personalization of the student model to incorporate new concepts. The process involves distilling knowledge from the teacher to the student and then refining the teacher's understanding based on the student's faster generation capabilities. The results show that this collaborative approach significantly improves both the personalization of novel concepts and the overall generative quality of the models."}, 'zh': {'title': 'åŒå‘æ¦‚å¿µè’¸é¦ï¼Œæå‡ä¸ªæ€§åŒ–ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒå‘æ¦‚å¿µè’¸é¦æ¡†æ¶ï¼Œåä¸ºEchoDistillï¼Œæ—¨åœ¨å¢å¼ºä¸€æ­¥æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸ªæ€§åŒ–å’Œç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡åŒæ—¶è®­ç»ƒå¤šæ­¥æ‰©æ•£æ¨¡å‹ï¼ˆæ•™å¸ˆï¼‰å’Œä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ï¼Œå®ç°äº†æ¦‚å¿µçš„åŒå‘è’¸é¦ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡å¯¹æŠ—æŸå¤±å’Œå¯¹é½æŸå¤±è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æ›´å¥½åœ°ä¸çœŸå®å›¾åƒåˆ†å¸ƒå¯¹é½ï¼Œå¹¶ä¿æŒä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¸ªæ€§åŒ–æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿæœ‰æ•ˆä¸ªæ€§åŒ–å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.16320', 'title': 'Scaling Laws for Deepfake Detection', 'url': 'https://huggingface.co/papers/2510.16320', 'abstract': 'Research on scaling laws in deepfake detection using the largest dataset to date reveals power-law scaling similar to large language models, enabling performance forecasting and data-centric countermeasures against evolving deepfake technology.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a systematic study of scaling laws for the deepfake detection task. Specifically, we analyze the model performance against the number of real image domains, deepfake generation methods, and training images. Since no existing dataset meets the scale requirements for this research, we construct ScaleDF, the largest dataset to date in this field, which contains over 5.8 million real images from 51 different datasets (domains) and more than 8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we observe power-law scaling similar to that shown in large language models (LLMs). Specifically, the average detection error follows a predictable power-law decay as either the number of real domains or the number of deepfake methods increases. This key observation not only allows us to forecast the number of additional real domains or deepfake methods required to reach a target performance, but also inspires us to counter the evolving deepfake technology in a data-centric manner. Beyond this, we examine the role of pre-training and data augmentations in deepfake detection under scaling, as well as the limitations of scaling itself.', 'score': 2, 'issue_id': 6656, 'pub_date': '2025-10-18', 'pub_date_card': {'ru': '18 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 18', 'zh': '10æœˆ18æ—¥'}, 'hash': '26365953102fe7b7', 'authors': ['Wenhao Wang', 'Longqi Cai', 'Taihong Xiao', 'Yuxiao Wang', 'Ming-Hsuan Yang'], 'affiliations': ['Google DeepMind', 'University of Technology Sydney'], 'pdf_title_img': 'assets/pdf/title_img/2510.16320.jpg', 'data': {'categories': ['#security', '#data', '#optimization', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ¡Ñ‚ĞµĞ¿ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ² ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ScaleDF Ñ 5.8 Ğ¼Ğ»Ğ½ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· 51 Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ¸ 8.8 Ğ¼Ğ»Ğ½ Ñ„ĞµĞ¹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… 102 Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚Ğ¾Ğ¼Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ² LLM: Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ data-centric Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ±Ğ¾Ñ€ÑŒĞ±Ğµ Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ². Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ°Ğ¼Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ğ¿Ñ„ĞµĞ¹ĞºĞ¾Ğ².'}, 'en': {'title': 'Harnessing Scaling Laws for Enhanced Deepfake Detection', 'desc': 'This paper investigates how the performance of deepfake detection models improves as they are trained on larger datasets. It introduces ScaleDF, the largest dataset for this purpose, containing millions of real and fake images. The study finds that the detection error decreases in a predictable way, following a power-law pattern, similar to trends seen in large language models. This insight helps in predicting how much more data is needed to enhance detection capabilities and suggests strategies for developing effective countermeasures against deepfake technology.'}, 'zh': {'title': 'æ·±åº¦ä¼ªé€ æ£€æµ‹çš„è§„æ¨¡æ³•åˆ™ç ”ç©¶', 'desc': 'æœ¬è®ºæ–‡ç³»ç»Ÿç ”ç©¶äº†æ·±åº¦ä¼ªé€ æ£€æµ‹ä»»åŠ¡ä¸­çš„è§„æ¨¡æ³•åˆ™ã€‚æˆ‘ä»¬åˆ†æäº†æ¨¡å‹æ€§èƒ½ä¸çœŸå®å›¾åƒåŸŸæ•°é‡ã€æ·±åº¦ä¼ªé€ ç”Ÿæˆæ–¹æ³•å’Œè®­ç»ƒå›¾åƒæ•°é‡ä¹‹é—´çš„å…³ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ScaleDFï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢è¯¥é¢†åŸŸæœ€å¤§çš„æ•°æ®åº“ï¼ŒåŒ…å«è¶…è¿‡580ä¸‡å¼ æ¥è‡ª51ä¸ªä¸åŒæ•°æ®é›†çš„çœŸå®å›¾åƒå’Œè¶…è¿‡880ä¸‡å¼ ç”±102ç§æ·±åº¦ä¼ªé€ æ–¹æ³•ç”Ÿæˆçš„å‡å›¾åƒã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ£€æµ‹è¯¯å·®éšç€çœŸå®åŸŸæˆ–æ·±åº¦ä¼ªé€ æ–¹æ³•æ•°é‡çš„å¢åŠ è€Œå‘ˆç°å¯é¢„æµ‹çš„å¹‚å¾‹è¡°å‡ï¼Œè¿™ä¸ºé¢„æµ‹æ€§èƒ½ç›®æ ‡æ‰€éœ€çš„é¢å¤–çœŸå®åŸŸæˆ–æ·±åº¦ä¼ªé€ æ–¹æ³•æ•°é‡æä¾›äº†ä¾æ®ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.07723', 'title': 'SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view\n  Human Reconstruction', 'url': 'https://huggingface.co/papers/2510.07723', 'abstract': 'SyncHuman combines 2D multiview and 3D native generative models with pixel-aligned synchronization and feature injection to achieve high-quality, photorealistic 3D human reconstruction from single images.  \t\t\t\t\tAI-generated summary \t\t\t\t Photorealistic 3D full-body human reconstruction from a single image is a critical yet challenging task for applications in films and video games due to inherent ambiguities and severe self-occlusions. While recent approaches leverage SMPL estimation and SMPL-conditioned image generative models to hallucinate novel views, they suffer from inaccurate 3D priors estimated from SMPL meshes and have difficulty in handling difficult human poses and reconstructing fine details. In this paper, we propose SyncHuman, a novel framework that combines 2D multiview generative model and 3D native generative model for the first time, enabling high-quality clothed human mesh reconstruction from single-view images even under challenging human poses. Multiview generative model excels at capturing fine 2D details but struggles with structural consistency, whereas 3D native generative model generates coarse yet structurally consistent 3D shapes. By integrating the complementary strengths of these two approaches, we develop a more effective generation framework. Specifically, we first jointly fine-tune the multiview generative model and the 3D native generative model with proposed pixel-aligned 2D-3D synchronization attention to produce geometrically aligned 3D shapes and 2D multiview images. To further improve details, we introduce a feature injection mechanism that lifts fine details from 2D multiview images onto the aligned 3D shapes, enabling accurate and high-fidelity reconstruction. Extensive experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D human reconstruction, even for images with challenging poses. Our method outperforms baseline methods in geometric accuracy and visual fidelity, demonstrating a promising direction for future 3D generation models.', 'score': 2, 'issue_id': 6656, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 9', 'zh': '10æœˆ9æ—¥'}, 'hash': '6ae3abf7cc1b5eff', 'authors': ['Wenyue Chen', 'Peng Li', 'Wangguandong Zheng', 'Chengfeng Zhao', 'Mengfei Li', 'Yaolong Zhu', 'Zhiyang Dou', 'Ronggang Wang', 'Yuan Liu'], 'affiliations': ['HKU', 'HKUST', 'PKU', 'SEU'], 'pdf_title_img': 'assets/pdf/title_img/2510.07723.jpg', 'data': {'categories': ['#cv', '#diffusion', '#games', '#3d'], 'emoji': 'ğŸ§', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ 2D Ğ¸ 3D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'SyncHuman - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ĞºĞ¸Ğ½Ğ¾ Ğ¸ Ğ¸Ğ³Ñ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ 2D multiview Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ 3D native Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. 2D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ¼ĞµĞ»ĞºĞ¸Ğ¼Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸, Ğ° 3D Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‘Ñ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚. Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ feature injection Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ Ğ¸Ğ· 2D Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½ÑƒÑ 3D Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ´Ğ°Ğ¶Ğµ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ°Ñ….'}, 'en': {'title': 'Revolutionizing 3D Human Reconstruction with SyncHuman', 'desc': 'SyncHuman is a novel framework that enhances 3D human reconstruction from single images by integrating 2D multiview and 3D native generative models. It addresses challenges like self-occlusions and ambiguous poses by synchronizing pixel-aligned features, allowing for high-quality mesh generation. The framework fine-tunes both models to ensure geometrical alignment and injects fine details from 2D images into the 3D shapes. Extensive testing shows that SyncHuman significantly improves accuracy and visual quality compared to existing methods.'}, 'zh': {'title': 'é«˜è´¨é‡ä¸‰ç»´äººç±»é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'SyncHumanæ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œç»“åˆäº†2Då¤šè§†è§’ç”Ÿæˆæ¨¡å‹å’Œ3DåŸç”Ÿç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿä»å•å¼ å›¾åƒä¸­é«˜è´¨é‡é‡å»ºç©¿è¡£äººç±»çš„ä¸‰ç»´ç½‘æ ¼ã€‚è¯¥æ–¹æ³•é€šè¿‡åƒç´ å¯¹é½çš„2D-3DåŒæ­¥æ³¨æ„æœºåˆ¶ï¼Œè”åˆå¾®è°ƒè¿™ä¸¤ç§æ¨¡å‹ï¼Œä»¥ç”Ÿæˆå‡ ä½•å¯¹é½çš„3Då½¢çŠ¶å’Œ2Då¤šè§†è§’å›¾åƒã€‚ä¸ºäº†è¿›ä¸€æ­¥æå‡ç»†èŠ‚ï¼ŒSyncHumanå¼•å…¥äº†ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œå°†2Då¤šè§†è§’å›¾åƒä¸­çš„ç»†èŠ‚æå‡åˆ°å¯¹é½çš„3Då½¢çŠ¶ä¸Šï¼Œä»è€Œå®ç°å‡†ç¡®ä¸”é«˜ä¿çœŸçš„é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSyncHumanåœ¨å‡ ä½•ç²¾åº¦å’Œè§†è§‰ä¿çœŸåº¦ä¸Šä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†æœªæ¥3Dç”Ÿæˆæ¨¡å‹çš„æœ‰å¸Œæœ›æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.23605', 'title': 'Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling', 'url': 'https://huggingface.co/papers/2510.23605', 'abstract': 'TIRE method enhances identity preservation in 3D/4D generation by tracking, inpainting, and resplatting regions of a 3D asset using video tracking and a subject-driven 2D inpainting model.  \t\t\t\t\tAI-generated summary \t\t\t\t Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.', 'score': 1, 'issue_id': 6648, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 27', 'zh': '10æœˆ27æ—¥'}, 'hash': '25fc1a00ca39c96f', 'authors': ['Shuhong Zheng', 'Ashkan Mirzaei', 'Igor Gilitschenski'], 'affiliations': ['Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.23605.jpg', 'data': {'categories': ['#optimization', '#3d'], 'emoji': 'ğŸ¯', 'ru': {'title': 'TIRE: Ğ¢Ñ€ĞµĞºĞ¸Ğ½Ğ³, Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ Ğ¸ resplatting Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 3D', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ TIRE Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D/4D Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ‚Ñ€ĞµĞºĞ¸Ğ½Ğ³Ğ°, Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³ ÑÑ‚Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ 2D Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² 3D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ñ‡ĞµÑ€ĞµĞ· resplatting. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D/4D ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğµ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ÑÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'TIRE: Keeping Identity Alive in 3D/4D Generation!', 'desc': "The TIRE method improves how we create 3D and 4D images by focusing on keeping the identity of a subject consistent from different angles. It starts with a basic 3D model and uses video tracking to find areas that need changes. Then, it fills in these areas using a special 2D inpainting technique that is tailored to the subject. Finally, it combines the updated 2D images back into a 3D format, ensuring that the subject's identity remains clear and consistent throughout the process."}, 'zh': {'title': 'TIREï¼šæå‡3D/4Dç”Ÿæˆä¸­çš„èº«ä»½ä¿ç•™', 'desc': 'TIREæ–¹æ³•é€šè¿‡è§†é¢‘è·Ÿè¸ªã€åŒºåŸŸä¿®å¤å’Œé‡æ–°æ˜ å°„æŠ€æœ¯ï¼Œå¢å¼ºäº†3D/4Dç”Ÿæˆä¸­çš„èº«ä»½ä¿ç•™ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„3Dç”Ÿæˆæ¨¡å‹ç”Ÿæˆåˆå§‹3Dèµ„äº§ï¼Œç„¶åé€šè¿‡è§†é¢‘è·Ÿè¸ªè¯†åˆ«éœ€è¦ä¿®æ”¹çš„åŒºåŸŸã€‚æ¥ç€ï¼Œé‡‡ç”¨åŸºäºä¸»é¢˜çš„2Dä¿®å¤æ¨¡å‹é€æ­¥å¡«å……è¿™äº›åŒºåŸŸã€‚æœ€åï¼Œå°†ä¿®æ”¹åçš„2Då¤šè§†è§’è§‚å¯Ÿç»“æœé‡æ–°æ˜ å°„å›3Dï¼ŒåŒæ—¶ä¿æŒä¸€è‡´æ€§ï¼Œæ˜¾è‘—æé«˜äº†èº«ä»½ä¿ç•™çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22603', 'title': 'Mitigating Attention Sinks and Massive Activations in Audio-Visual\n  Speech Recognition with LLMS', 'url': 'https://huggingface.co/papers/2510.22603', 'abstract': 'Attention sinks and massive activations in multimodal speech recognition are identified and mitigated using a decorrelation loss, improving word error rate under high feature downsampling.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently advanced auditory speech recognition (ASR), visual speech recognition (VSR), and audio-visual speech recognition (AVSR). However, understanding of their internal dynamics under fine-tuning remains limited. In natural language processing, recent work has revealed attention sinks, tokens that attract disproportionately high attention, and associated massive activations in which some features of sink tokens exhibit huge activation in LLMs. In this work, we are the first to study these phenomena in multimodal speech recognition. Through a detailed analysis of audio-visual LLMs, we identify attention sinks and massive activations not only at the BOS token but also at intermediate low-semantic tokens across ASR, VSR, and AVSR. We show that massive activations originate in the MLP layers and correspond to fixed feature indices across all sink tokens. We further show that intermediate sink tokens exhibit high cosine similarity to the BOS token, thereby amplifying attention and activation. Building on these insights, we introduce a simple decorrelation loss that reduces cosine similarity between BOS and other tokens, effectively mitigating intermediate sinks and massive activations. Furthermore, our method improves word error rate (WER) under high audio-visual feature downsampling while remaining stable at lower downsampling rates.', 'score': 1, 'issue_id': 6651, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 26', 'zh': '10æœˆ26æ—¥'}, 'hash': 'bf8ba5a46bcb9532', 'authors': ['Anand', 'Umberto Cappellazzo', 'Stavros Petridis', 'Maja Pantic'], 'affiliations': ['Imperial College London, UK', 'University of British Columbia, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2510.22603.jpg', 'data': {'categories': ['#training', '#optimization', '#multimodal', '#audio', '#interpretability'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Ğ£ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Â«Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²-Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ğ¾Ğ²Â» Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ attention sinks (Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€Ğ¸Ñ‚ÑĞ³Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ) Ğ¸ massive activations (Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹) Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹-Ğ¼Ğ°Ğ³Ğ½Ğ¸Ñ‚Ñ‹ Ğ¿Ğ¾ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (BOS token), Ğ½Ğ¾ Ğ¸ ÑÑ€ĞµĞ´Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹, Ğ¿Ñ€Ğ¸Ñ‡Ñ‘Ğ¼ Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² MLP ÑĞ»Ğ¾ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ sink Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ¼ĞµÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ BOS Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ decorrelation loss ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑÑ‚Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ word error rate Ğ¿Ñ€Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¼ Ğ´Ğ°ÑƒĞ½ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ².'}, 'en': {'title': 'Mitigating Attention Sinks for Better Speech Recognition', 'desc': 'This paper investigates the issues of attention sinks and massive activations in multimodal speech recognition systems, particularly in large language models (LLMs). Attention sinks are tokens that receive excessive focus, leading to inflated activations in the model. The authors introduce a decorrelation loss to reduce the similarity between these problematic tokens and the beginning-of-sequence (BOS) token, which helps to alleviate the identified issues. As a result, their approach improves the word error rate (WER) in speech recognition tasks, especially when audio-visual features are downsampled significantly.'}, 'zh': {'title': 'å‡è½»å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«ä¸­çš„æ³¨æ„åŠ›æ²‰æ²¡ä¸æ¿€æ´»', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€è¯­éŸ³è¯†åˆ«ä¸­çš„æ³¨æ„åŠ›æ²‰æ²¡å’Œå¤§è§„æ¨¡æ¿€æ´»ç°è±¡ã€‚æˆ‘ä»¬å‘ç°è¿™äº›ç°è±¡ä¸ä»…å‡ºç°åœ¨å¼€å§‹æ ‡è®°ï¼ˆBOSï¼‰ä¸Šï¼Œè¿˜å‡ºç°åœ¨ä¸­é—´çš„ä½è¯­ä¹‰æ ‡è®°ä¸Šã€‚é€šè¿‡å¼•å…¥å»ç›¸å…³æŸå¤±ï¼Œæˆ‘ä»¬æœ‰æ•ˆé™ä½äº†BOSä¸å…¶ä»–æ ‡è®°ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä»è€Œå‡è½»äº†ä¸­é—´æ²‰æ²¡å’Œå¤§è§„æ¨¡æ¿€æ´»çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é«˜ç‰¹å¾ä¸‹é‡‡æ ·ä¸‹æ”¹å–„äº†è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼ŒåŒæ—¶åœ¨ä½ä¸‹é‡‡æ ·ç‡ä¸‹ä¿æŒç¨³å®šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22317', 'title': 'Memory-based Language Models: An Efficient, Explainable, and\n  Eco-friendly Approach to Large Language Modeling', 'url': 'https://huggingface.co/papers/2510.22317', 'abstract': 'Memory-based language modeling provides an efficient, eco-friendly alternative to deep neural networks, offering scalable performance and strong memorization with low ecological impact.  \t\t\t\t\tAI-generated summary \t\t\t\t We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model.', 'score': 1, 'issue_id': 6650, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 25', 'zh': '10æœˆ25æ—¥'}, 'hash': '5bc9a104bf1b6510', 'authors': ['Antal van den Bosch', 'Ainhoa Risco PatÃ³n', 'Teun Buijse', 'Peter Berck', 'Maarten van Gompel'], 'affiliations': ['Lund University', 'Royal Netherlands Academy of Arts and Sciences', 'Utrecht University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22317.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#training', '#inference', '#data', '#architecture'], 'emoji': 'ğŸŒ±', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ÑĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¸ ÑĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½ÑƒÑ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñƒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ OLIFANT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ k-Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° CPU Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒĞ³Ğ»ĞµÑ€Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ÑĞ»ĞµĞ´Ğµ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ GPT-2 Ğ¸ GPT-Neo, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ° Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ĞµĞµ ÑĞ½ĞµÑ€Ğ³Ğ¾Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ° ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'Eco-Friendly Language Modeling with Memory Efficiency', 'desc': "This paper introduces memory-based language modeling as a sustainable alternative to traditional deep neural networks for language tasks. It highlights the model's ability to predict the next token efficiently while maintaining strong memorization capabilities. The approach utilizes k-nearest neighbor classification, which allows it to operate with low ecological impact by relying on CPUs and achieving low latency. The authors compare their model, OLIFANT, with popular models like GPT-2 and GPT-Neo, focusing on accuracy, environmental emissions, and processing speeds."}, 'zh': {'title': 'åŸºäºè®°å¿†çš„è¯­è¨€å»ºæ¨¡ï¼šé«˜æ•ˆç¯ä¿çš„æ–°é€‰æ‹©', 'desc': 'åŸºäºè®°å¿†çš„è¯­è¨€å»ºæ¨¡æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”ç¯ä¿çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå–ä»£äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„è¯­è¨€å»ºæ¨¡æ–¹æ³•ã€‚å®ƒåœ¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ€§èƒ½ä¸Šå…·æœ‰å¯¹æ•°çº¿æ€§å¯æ‰©å±•æ€§ï¼Œå¹¶ä¸”å…·å¤‡å¼ºå¤§çš„è®°å¿†èƒ½åŠ›ã€‚é€šè¿‡å¿«é€Ÿè¿‘é‚»åˆ†ç±»çš„è¿‘ä¼¼å®ç°ï¼ŒåŸºäºè®°å¿†çš„è¯­è¨€å»ºæ¨¡åœ¨è®­ç»ƒå’Œæ¨ç†æ¨¡å¼ä¸‹éƒ½èƒ½ä¿æŒè¾ƒå°çš„ç”Ÿæ€è¶³è¿¹ï¼Œå®Œå…¨ä¾èµ–CPUå¹¶å®ç°ä½å»¶è¿Ÿã€‚æˆ‘ä»¬å°†åŸºäºè®°å¿†çš„è¯­è¨€å»ºæ¨¡å®ç°OLIFANTä¸GPT-2å’ŒGPT-Neoåœ¨ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å‡†ç¡®æ€§ã€ä¼°è®¡æ’æ”¾å’Œé€Ÿåº¦æ–¹é¢è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶æä¾›äº†æ›´æ·±å…¥çš„æ¨¡å‹åˆ†æã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.22010', 'title': 'FlowOpt: Fast Optimization Through Whole Flow Processes for\n  Training-Free Editing', 'url': 'https://huggingface.co/papers/2510.22010', 'abstract': "FlowOpt, a zero-order optimization framework, enables efficient control of diffusion and flow-matching models for image editing tasks without backpropagation.  \t\t\t\t\tAI-generated summary \t\t\t\t The remarkable success of diffusion and flow-matching models has ignited a surge of works on adapting them at test time for controlled generation tasks. Examples range from image editing to restoration, compression and personalization. However, due to the iterative nature of the sampling process in those models, it is computationally impractical to use gradient-based optimization to directly control the image generated at the end of the process. As a result, existing methods typically resort to manipulating each timestep separately. Here we introduce FlowOpt - a zero-order (gradient-free) optimization framework that treats the entire flow process as a black box, enabling optimization through the whole sampling path without backpropagation through the model. Our method is both highly efficient and allows users to monitor the intermediate optimization results and perform early stopping if desired. We prove a sufficient condition on FlowOpt's step-size, under which convergence to the global optimum is guaranteed. We further show how to empirically estimate this upper bound so as to choose an appropriate step-size. We demonstrate how FlowOpt can be used for image editing, showcasing two options: (i) inversion (determining the initial noise that generates a given image), and (ii) directly steering the edited image to be similar to the source image while conforming to a target text prompt. In both cases, FlowOpt achieves state-of-the-art results while using roughly the same number of neural function evaluations (NFEs) as existing methods. Code and examples are available on the project's webpage.", 'score': 1, 'issue_id': 6658, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 24', 'zh': '10æœˆ24æ—¥'}, 'hash': 'b6000f4ae09d1fdb', 'authors': ['Or Ronai', 'Vladimir Kulikov', 'Tomer Michaeli'], 'affiliations': ['Technion - Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.22010.jpg', 'data': {'categories': ['#training', '#diffusion', '#cv', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'FlowOpt â€” ÑÑ‚Ğ¾ framework Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ flow-matching Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ backpropagation. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ñ‡Ñ‘Ñ€Ğ½Ñ‹Ğ¹ ÑÑ‰Ğ¸Ğº Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ zero-order Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼ÑƒĞ¼Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° ÑˆĞ°Ğ³Ğ°. FlowOpt Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ ÑÑ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¶Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹.'}, 'en': {'title': 'Efficient Image Editing with Zero-Order Optimization', 'desc': 'FlowOpt is a novel zero-order optimization framework designed for controlling diffusion and flow-matching models in image editing tasks without the need for backpropagation. It treats the entire image generation process as a black box, allowing for efficient optimization across the entire sampling path. This method enables users to monitor intermediate results and implement early stopping, enhancing the optimization process. FlowOpt guarantees convergence to the global optimum under specific conditions, demonstrating superior performance in image editing tasks while maintaining computational efficiency.'}, 'zh': {'title': 'FlowOptï¼šé«˜æ•ˆçš„é›¶é˜¶ä¼˜åŒ–æ¡†æ¶', 'desc': 'FlowOpt æ˜¯ä¸€ä¸ªé›¶é˜¶ä¼˜åŒ–æ¡†æ¶ï¼Œèƒ½å¤Ÿé«˜æ•ˆæ§åˆ¶æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹ï¼Œç”¨äºå›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œè€Œæ— éœ€åå‘ä¼ æ’­ã€‚è¯¥æ–¹æ³•å°†æ•´ä¸ªæµè¿‡ç¨‹è§†ä¸ºä¸€ä¸ªé»‘ç®±ï¼Œå…è®¸åœ¨æ•´ä¸ªé‡‡æ ·è·¯å¾„ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œé¿å…äº†é€æ­¥å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—å¤æ‚æ€§ã€‚FlowOpt é€šè¿‡ç›‘æ§ä¸­é—´ä¼˜åŒ–ç»“æœï¼Œæ”¯æŒç”¨æˆ·è¿›è¡Œæ—©æœŸåœæ­¢ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚æˆ‘ä»¬å±•ç¤ºäº† FlowOpt åœ¨å›¾åƒç¼–è¾‘ä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬åæ¼”å’Œç›´æ¥å¼•å¯¼ç¼–è¾‘å›¾åƒï¼Œä½¿å…¶ä¸æºå›¾åƒç›¸ä¼¼ï¼ŒåŒæ—¶ç¬¦åˆç›®æ ‡æ–‡æœ¬æç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.21800', 'title': 'MARS-M: When Variance Reduction Meets Matrices', 'url': 'https://huggingface.co/papers/2510.21800', 'abstract': 'MARS-M, a new optimizer combining Muon and MARS techniques, achieves faster convergence and better performance in large-scale neural network training.  \t\t\t\t\tAI-generated summary \t\t\t\t Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of mathcal{O}(T^{-1/3}), which improves upon mathcal{O}(T^{-1/4}) rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/MARS_M.', 'score': 1, 'issue_id': 6645, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 20', 'zh': '10æœˆ20æ—¥'}, 'hash': 'fbe37bd50ecb7e40', 'authors': ['Yifeng Liu', 'Angela Yuan', 'Quanquan Gu'], 'affiliations': ['Department of Computer Science, University of California, Los Angeles, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.21800.jpg', 'data': {'categories': ['#architecture', '#benchmark', '#optimization', '#training'], 'emoji': 'ğŸš€', 'ru': {'title': 'MARS-M: ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MARS-M â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ· Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Muon Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ¸Ğ· MARS. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MARS-M Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ O(T^{-1/3}), Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ O(T^{-1/4}) Ñƒ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Muon. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM).'}, 'en': {'title': 'MARS-M: Faster Training for Large Neural Networks!', 'desc': 'MARS-M is a new optimizer that combines the strengths of Muon and MARS techniques to enhance the training of large-scale neural networks. By integrating variance-reduction methods from MARS with the efficiency of matrix-based optimizers like Muon, MARS-M achieves faster convergence rates. Theoretical analysis shows that MARS-M converges to a stationary point more quickly than Muon alone, improving the convergence rate from O(T^{-1/4}) to O(T^{-1/3}). Empirical results indicate that MARS-M consistently outperforms existing optimizers in terms of loss reduction and overall performance on various tasks, including language modeling and computer vision.'}, 'zh': {'title': 'MARS-Mï¼šåŠ é€Ÿå¤§è§„æ¨¡ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ–°ä¼˜åŒ–å™¨', 'desc': 'MARS-Mæ˜¯ä¸€ç§æ–°å‹ä¼˜åŒ–å™¨ï¼Œç»“åˆäº†Muonå’ŒMARSæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè®­ç»ƒä¸­å®ç°æ›´å¿«çš„æ”¶æ•›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºçŸ©é˜µçš„é¢„æ¡ä»¶ä¼˜åŒ–å™¨å¦‚Muonåœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶æ¯”æ ‡é‡ä¼˜åŒ–å™¨æ›´é«˜æ•ˆã€‚åŒæ—¶ï¼ŒMARSæŠ€æœ¯é€šè¿‡å‡å°‘æ–¹å·®ï¼Œæ˜¾è‘—åŠ å¿«äº†é¢„è®­ç»ƒçš„é€Ÿåº¦ã€‚æœ¬æ–‡è¯æ˜äº†MARS-Måœ¨æ ‡å‡†æ¡ä»¶ä¸‹ä»¥æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦è¾¾åˆ°ä¸€é˜¶é©»ç‚¹ï¼Œå¹¶åœ¨è¯­è¨€å»ºæ¨¡å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´ä½çš„æŸå¤±å’Œæ›´å¥½çš„æ€§èƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (6)', '#agi (2)', '#alignment (2)', '#architecture (10)', '#audio (2)', '#benchmark (14)', '#cv (8)', '#data (4)', '#dataset (8)', '#diffusion (6)', '#ethics', '#games (7)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability (5)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (11)', '#open_source (4)', '#optimization (25)', '#plp (1)', '#rag (3)', '#reasoning (7)', '#rl (1)', '#rlhf (3)', '#robotics (2)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (17)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-28 20:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-28 20:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-28 20:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    