
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 47 papers. October 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">47 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-07.html">â¬…ï¸ <span id="prev-date">07.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-09.html">â¡ï¸ <span id="next-date">09.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'};
        let feedDateNext = {'ru': '09.10', 'en': '10/09', 'zh': '10æœˆ9æ—¥'};
        let feedDatePrev = {'ru': '07.10', 'en': '10/07', 'zh': '10æœˆ7æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.04871', 'title': 'Less is More: Recursive Reasoning with Tiny Networks', 'url': 'https://huggingface.co/papers/2510.04871', 'abstract': 'Tiny Recursive Model (TRM) achieves high generalization on complex puzzle tasks using a small, two-layer network with minimal parameters, outperforming larger language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.', 'score': 72, 'issue_id': 6309, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '6a09a2f2d85ae333', 'authors': ['Alexia Jolicoeur-Martineau'], 'affiliations': ['Samsung SAIL Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.04871.jpg', 'data': {'categories': ['#agi', '#training', '#reasoning', '#small_models'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ñ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚Ğ¾Ğ²: 7M Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Tiny Recursive Model (TRM) â€” Ğ¼Ğ¸Ğ½Ğ¸Ğ°Ñ‚ÑÑ€Ğ½ÑƒÑ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ÑĞµĞ³Ğ¾ Ğ¸Ğ· 2 ÑĞ»Ğ¾Ñ‘Ğ² Ğ¸ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. TRM Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 45% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ARC-AGI-1 Ğ¸ 8% Ğ½Ğ° ARC-AGI-2, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ LLM Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 0.01% Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¾ĞºĞ¾Ğ»Ğ¾ 1000 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²) Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° ÑÑƒĞ´Ğ¾ĞºÑƒ, Ğ»Ğ°Ğ±Ğ¸Ñ€Ğ¸Ğ½Ñ‚Ğ¾Ğ² Ğ¸ ARC-AGI. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ°Ğ¼Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ğµ AI-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': "Small Network, Big Solutions: TRM's Power in Puzzle Solving", 'desc': 'The Tiny Recursive Model (TRM) is a compact neural network architecture designed to solve complex puzzle tasks with high generalization. It utilizes a simple two-layer structure with only 7 million parameters, significantly fewer than larger language models. TRM outperforms existing models on challenging tasks like ARC-AGI, demonstrating that smaller networks can achieve competitive accuracy. This approach highlights the potential of minimalistic designs in machine learning for effective problem-solving.'}, 'zh': {'title': 'å°å‹é€’å½’æ¨¡å‹ï¼Œè¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'Tiny Recursive Model (TRM) æ˜¯ä¸€ç§æ–°é¢–çš„é€’å½’æ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨ä¸€ä¸ªåªæœ‰ä¸¤å±‚çš„å°å‹ç¥ç»ç½‘ç»œï¼Œå‚æ•°ä»…ä¸º700ä¸‡ã€‚å®ƒåœ¨å¤æ‚çš„æ‹¼å›¾ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹ã€‚TRM çš„è®¾è®¡çµæ„Ÿæ¥æºäºç”Ÿç‰©å­¦ï¼Œèƒ½å¤Ÿåœ¨å°æ•°æ®é›†ä¸Šå®ç°é«˜æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡ TRM ç»“æ„ç®€å•ï¼Œä½†åœ¨è§£å†³å›°éš¾é—®é¢˜æ—¶å±•ç°äº†å·¨å¤§çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24107', 'title': 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and\n  Synthesis for SLMs', 'url': 'https://huggingface.co/papers/2509.24107', 'abstract': 'Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.', 'score': 57, 'issue_id': 6301, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '17db556aca22a151', 'authors': ['Shreyas Singh', 'Kunal Singh', 'Pradeep Moturi'], 'affiliations': ['Fractal AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.24107.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agents', '#rl', '#dataset', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Fathom-DeepResearch, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ open-weights Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch', 'desc': 'Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.'}, 'zh': {'title': 'æ™ºèƒ½æœç´¢ä¸æŠ¥å‘Šåˆæˆçš„æœªæ¥', 'desc': 'Fathom-DeepResearch æ˜¯ä¸€ä¸ªæ™ºèƒ½ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºç½‘ç»œæœç´¢å’ŒæŠ¥å‘Šåˆæˆï¼Œèƒ½å¤Ÿåœ¨å¼€æ”¾å¼ä¿¡æ¯æ£€ç´¢ä»»åŠ¡å’Œå¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥ç³»ç»Ÿç”±ä¸¤ä¸ªä¸“é—¨æ¨¡å‹ç»„æˆï¼šFathom-Search-4B å’Œ Fathom-Synthesizer-4Bã€‚Fathom-Search-4B é€šè¿‡å®æ—¶ç½‘ç»œæœç´¢å’Œé’ˆå¯¹ç½‘é¡µæŸ¥è¯¢è¿›è¡Œè¯æ®åŸºç¡€è°ƒæŸ¥ï¼Œé‡‡ç”¨äº†å¤šé¡¹å…ˆè¿›æŠ€æœ¯æ¥ä¼˜åŒ–å…¶æ€§èƒ½ã€‚Fathom-Synthesizer-4B åˆ™å°†å¤šè½® DeepSearch è¿½è¸ªè½¬æ¢ä¸ºç»“æ„åŒ–çš„ã€å¼•ç”¨å¯†é›†çš„ DeepResearch æŠ¥å‘Šï¼Œç¡®ä¿ä¿¡æ¯çš„å…¨é¢åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06217', 'title': 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.06217', 'abstract': 'TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.', 'score': 55, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'bdefddb943fa9266', 'authors': ['Jiaru Zou', 'Soumya Roy', 'Vinay Kumar Verma', 'Ziyi Wang', 'David Wipf', 'Pan Lu', 'Sumit Negi', 'James Zou', 'Jingrui He'], 'affiliations': ['Amazon', 'Purdue University', 'Stanford University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.06217.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'TaTToo: Process Reward Model Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TaTToo â€” Ğ½Ğ¾Ğ²ÑƒÑ Process Reward Model Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² LLM. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ PRM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TaTToo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ PRM Ñ 72B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 30.9% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, fact-checking Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Tabular Reasoning with TaTToo!', 'desc': 'TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters.'}, 'zh': {'title': 'TaTTooï¼šæå‡è¡¨æ ¼æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'TaTTooæ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºè¡¨æ ¼çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ˜ç¡®å¤„ç†è¡¨æ ¼ç‰¹å®šæ“ä½œå’Œæ•´åˆå·¥å…·éªŒè¯ï¼Œæ˜¾è‘—æ”¹å–„äº†ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ¨ç†æ—¶å­˜åœ¨ç“¶é¢ˆï¼Œè€ŒTaTTooé€šè¿‡è®¾è®¡å¯æ‰©å±•çš„æ•°æ®æ•´ç†ç®¡é“å’ŒåŒé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚ç»è¿‡è¯„ä¼°ï¼ŒTaTTooåœ¨å¤šä¸ªè¡¨æ ¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†ä¸‹æ¸¸å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26328', 'title': 'Fast-dLLM v2: Efficient Block-Diffusion LLM', 'url': 'https://huggingface.co/papers/2509.26328', 'abstract': "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.", 'score': 32, 'issue_id': 6298, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'd003eca6c18f4d37', 'authors': ['Chengyue Wu', 'Hao Zhang', 'Shuchen Xue', 'Shizhe Diao', 'Yonggan Fu', 'Zhijian Liu', 'Pavlo Molchanov', 'Ping Luo', 'Song Han', 'Enze Xie'], 'affiliations': ['MIT', 'NVIDIA', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26328.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#diffusion', '#optimization', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fast-dLLM v2 â€” Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ²ÑĞµĞ³Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ² 500 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation', 'desc': "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."}, 'zh': {'title': 'å¿«é€Ÿé«˜æ•ˆçš„å—æ‰©æ•£è¯­è¨€æ¨¡å‹', 'desc': 'Fast-dLLM v2æ˜¯ä¸€ç§å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹è½¬æ¢ä¸ºå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…éœ€çº¦10äº¿ä¸ªæ ‡è®°è¿›è¡Œå¾®è°ƒï¼Œç›¸æ¯”äºå…¨æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å‡å°‘äº†500å€ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥å—æ‰©æ•£æœºåˆ¶å’Œäº’è¡¥æ³¨æ„åŠ›æ©ç ï¼ŒFast-dLLM v2å®ç°äº†å—çº§åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¹¶è®¾è®¡äº†åˆ†å±‚ç¼“å­˜æœºåˆ¶ä»¥åŠ é€Ÿè§£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨å‡†ç¡®æ€§ä¸Šä¸è‡ªå›å½’åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05592', 'title': 'In-the-Flow Agentic System Optimization for Effective Planning and Tool\n  Use', 'url': 'https://huggingface.co/papers/2510.05592', 'abstract': 'AgentFlow, a trainable agentic framework with in-the-flow optimization, enhances reasoning in large language models by coordinating specialized modules and outperforms top baselines across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.', 'score': 25, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '72e152fd27b3b07f', 'authors': ['Zhuofeng Li', 'Haoxiang Zhang', 'Seungju Han', 'Sheng Liu', 'Jianwen Xie', 'Yu Zhang', 'Yejin Choi', 'James Zou', 'Pan Lu'], 'affiliations': ['Lambda', 'Stanford University', 'Texas A&M University', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.05592.jpg', 'data': {'categories': ['#training', '#agents', '#rl', '#optimization', '#reasoning', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼Ğ°Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'AgentFlow â€” ÑÑ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹: Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»Ñ, Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Flow-GRPO. Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ñ…, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ´Ğ½Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. AgentFlow Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ½Ğ° 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4o Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ…, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'AgentFlow: Optimizing Reasoning in AI with Modular Coordination', 'desc': 'AgentFlow is a new framework designed to improve reasoning in large language models by using a trainable system that coordinates multiple specialized modules. Unlike traditional methods that use a single policy for all tasks, AgentFlow breaks down the process into four modules: planner, executor, verifier, and generator, which work together dynamically. It employs a novel training method called Flow-GRPO that allows the model to learn in real-time during interactions, making it more effective at handling complex tasks with long-term goals. The results show that AgentFlow significantly outperforms existing models on various benchmarks, demonstrating its ability to enhance reasoning and tool usage in AI applications.'}, 'zh': {'title': 'AgentFlowï¼šæ™ºèƒ½æ¨ç†çš„æ–°çºªå…ƒ', 'desc': 'AgentFlow æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„æ™ºèƒ½æ¡†æ¶ï¼Œé€šè¿‡åœ¨æµç¨‹ä¸­ä¼˜åŒ–æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚å®ƒåè°ƒå››ä¸ªä¸“é—¨æ¨¡å—ï¼ˆè§„åˆ’è€…ã€æ‰§è¡Œè€…ã€éªŒè¯è€…å’Œç”Ÿæˆå™¨ï¼‰ï¼Œå¹¶åœ¨å¤šè½®äº¤äº’ä¸­ç›´æ¥ä¼˜åŒ–è§„åˆ’è€…ã€‚ä¸ä¼ ç»Ÿçš„å•ä¸€ç­–ç•¥æ–¹æ³•ä¸åŒï¼ŒAgentFlow é€šè¿‡æµå¼ç»„ç²¾ç‚¼ç­–ç•¥ä¼˜åŒ–ï¼ˆFlow-GRPOï¼‰æ¥å¤„ç†é•¿æ—¶é—´è·¨åº¦å’Œç¨€ç–å¥–åŠ±çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentFlow åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡†ç¡®ç‡æé«˜äº†14.9%ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æ¨ç†å’Œå·¥å…·è°ƒç”¨æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03270', 'title': 'CoDA: Coding LM via Diffusion Adaptation', 'url': 'https://huggingface.co/papers/2510.03270', 'abstract': 'CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.', 'score': 25, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '49ba9b2131a56dc8', 'authors': ['Haolin Chen', 'Shiyu Wang', 'Can Qin', 'Bo Pang', 'Zuxin Liu', 'Jielin Qiu', 'Jianguo Zhang', 'Yingbo Zhou', 'Zeyuan Chen', 'Ran Xu', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang', 'Weiran Yao'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03270.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#diffusion', '#small_models', '#dataset'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'CoDA â€” ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ 1.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğµ. CoDA Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° TPU Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ confidence-guided sampling Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanEval Ğ¸ MBPP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'CoDA: Lightweight Diffusion Coding with Competitive Performance', 'desc': 'CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants.'}, 'zh': {'title': 'CoDAï¼šè½»é‡çº§æ‰©æ•£ç¼–ç çš„æœªæ¥', 'desc': 'CoDAæ˜¯ä¸€ç§å…·æœ‰17äº¿å‚æ•°çš„æ‰©æ•£ç¼–ç å™¨ï¼Œé€šè¿‡ä¿¡å¿ƒå¼•å¯¼é‡‡æ ·å®ç°äº†ä¸æ›´å°æ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚å®ƒç»“åˆäº†å¤§è§„æ¨¡çš„æ‰©æ•£é¢„è®­ç»ƒå’Œä»¥ä»£ç ä¸ºä¸­å¿ƒçš„ä¸­æœŸè®­ç»ƒï¼Œä»¥åŠæŒ‡ä»¤è°ƒä¼˜ï¼Œä»è€Œä¿æŒäº†æ¨ç†å»¶è¿Ÿçš„ç«äº‰åŠ›ã€‚CoDAåœ¨Humanevalã€MBPPå’ŒEvalPlusç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¸é«˜è¾¾70äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹æ£€æŸ¥ç‚¹ã€è¯„ä¼°å·¥å…·å’ŒTPUè®­ç»ƒç®¡é“ï¼Œä»¥åŠ é€Ÿè½»é‡çº§æ‰©æ•£ç¼–ç åŠ©æ‰‹çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04162', 'title': 'Drax: Speech Recognition with Discrete Flow Matching', 'url': 'https://huggingface.co/papers/2510.04162', 'abstract': 'Drax, a discrete flow matching framework for ASR, achieves state-of-the-art recognition accuracy with improved efficiency by constructing an audio-conditioned probability path.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion and flow-based non-autoregressive (NAR) models have shown strong promise in large language modeling, however, their potential for automatic speech recognition (ASR) remains largely unexplored. We propose Drax, a discrete flow matching framework for ASR that enables efficient parallel decoding. To better align training with inference, we construct an audio-conditioned probability path that guides the model through trajectories resembling likely intermediate inference errors, rather than direct random noise to target transitions. Our theoretical analysis links the generalization gap to divergences between training and inference occupancies, controlled by cumulative velocity errors, thereby motivating our design choice. Empirical evaluation demonstrates that our approach attains recognition accuracy on par with state-of-the-art speech models while offering improved accuracy-efficiency trade-offs, highlighting discrete flow matching as a promising direction for advancing NAR ASR.', 'score': 22, 'issue_id': 6309, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}, 'hash': 'be6cacf8dbe4e879', 'authors': ['Aviv Navon', 'Aviv Shamsian', 'Neta Glazer', 'Yael Segal-Feldman', 'Gill Hetz', 'Joseph Keshet', 'Ethan Fetaya'], 'affiliations': ['aiOla Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.04162.jpg', 'data': {'categories': ['#diffusion', '#audio', '#optimization'], 'emoji': 'ğŸ¤', 'ru': {'title': 'Drax: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğµ flow matching', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Drax â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ framework Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ‡Ğ¸ (ASR) Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ discrete flow matching. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¿ÑƒÑ‚Ñ‘Ğ¼, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑÑ…, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ° Ğ½Ğµ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ ÑˆÑƒĞ¼Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ state-of-the-art Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞµ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹.'}, 'en': {'title': 'Drax: Efficient and Accurate ASR with Discrete Flow Matching', 'desc': 'Drax is a new framework designed for automatic speech recognition (ASR) that uses discrete flow matching to enhance recognition accuracy and efficiency. It introduces an audio-conditioned probability path that helps the model navigate through common inference errors, improving the decoding process. The framework addresses the gap between training and inference by managing cumulative velocity errors, which helps in better generalization. Empirical results show that Drax achieves competitive accuracy compared to leading speech models while also being more efficient, suggesting a valuable advancement in non-autoregressive ASR techniques.'}, 'zh': {'title': 'Draxï¼šæå‡ASRæ•ˆç‡ä¸å‡†ç¡®æ€§çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶', 'desc': 'Draxæ˜¯ä¸€ç§ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ç¦»æ•£æµåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºéŸ³é¢‘æ¡ä»¶æ¦‚ç‡è·¯å¾„ï¼Œå®ç°äº†æ›´é«˜çš„è¯†åˆ«å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚è¯¥æ¡†æ¶æ”¯æŒé«˜æ•ˆçš„å¹¶è¡Œè§£ç ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å°†è®­ç»ƒä¸æ¨ç†å¯¹é½ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æå°†æ³›åŒ–è¯¯å·®ä¸è®­ç»ƒå’Œæ¨ç†çš„å ç”¨å·®å¼‚è”ç³»èµ·æ¥ï¼Œä»è€Œæ¨åŠ¨äº†è®¾è®¡é€‰æ‹©ã€‚å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒDraxåœ¨è¯†åˆ«å‡†ç¡®æ€§ä¸Šä¸æœ€å…ˆè¿›çš„è¯­éŸ³æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´æä¾›äº†æ›´å¥½çš„æƒè¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04081', 'title': 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.04081', 'abstract': "Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.", 'score': 18, 'issue_id': 6299, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}, 'hash': '4f2356e6d1057b79', 'authors': ['Honglin Lin', 'Qizhi Pei', 'Xin Gao', 'Zhuoshi Pan', 'Yu Li', 'Juntao Li', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.04081.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#math', '#data', '#training'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Caco â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Caco-1.3M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Caco: Automating High-Quality Reasoning for LLMs', 'desc': 'Caco is a new framework designed to improve the reasoning abilities of large language models (LLMs) in solving mathematical problems. It automates the creation of high-quality reasoning data by using code to generate diverse and verifiable reasoning paths. This approach addresses the limitations of traditional Chain-of-Thought (CoT) methods, which often struggle with quality and scalability. By incorporating automated validation and a closed-loop synthesis process, Caco ensures that the generated reasoning data is both executable and adaptable to various tasks, leading to better performance on mathematical reasoning benchmarks.'}, 'zh': {'title': 'Cacoï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ¨ç†æ•°æ®ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'Cacoæ˜¯ä¸€ä¸ªä»£ç è¾…åŠ©çš„æ€ç»´é“¾æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„æ¨ç†æ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»£ç é©±åŠ¨çš„å¢å¼ºæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ€ç»´é“¾æ–¹æ³•åœ¨ç”Ÿæˆæ§åˆ¶ã€è´¨é‡ä¸è¶³å’Œæ¨ç†è·¯å¾„æœ‰é™ç­‰é—®é¢˜ã€‚Cacoé¦–å…ˆåœ¨ç»Ÿä¸€çš„ä»£ç æ ¼å¼ä¸Šå¾®è°ƒä»£ç åŸºç¡€çš„æ€ç»´é“¾ç”Ÿæˆå™¨ï¼Œç„¶åæ‰©å±•æ•°æ®ç”Ÿæˆä»¥è·å¾—å¤§é‡å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ã€‚é€šè¿‡ä»£ç æ‰§è¡Œå’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤ï¼ŒCacoç¡®ä¿äº†é€»è¾‘æ­£ç¡®æ€§å’Œç»“æ„å¤šæ ·æ€§ï¼Œä»è€Œå®ç°äº†å®Œå…¨è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„æ¨ç†æ•°æ®åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06052', 'title': 'MixReasoning: Switching Modes to Think', 'url': 'https://huggingface.co/papers/2510.06052', 'abstract': 'MixReasoning dynamically adjusts reasoning depth in models to improve efficiency without sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.', 'score': 16, 'issue_id': 6303, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'b40a91a849b36453', 'authors': ['Haiquan Lu', 'Gongfan Fang', 'Xinyin Ma', 'Qi Li', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.06052.jpg', 'data': {'categories': ['#reasoning', '#math', '#training'], 'emoji': 'ğŸšï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ´ÑƒĞ¼Ğ°Ğ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ°Ğ¼, Ğ³Ğ´Ğµ ÑÑ‚Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MixReasoning â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑˆĞ°Ğ³Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ğ¾Ğ³Ğ¾ Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ ĞºĞ¾ Ğ²ÑĞµĞ¼ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸, Ğ³Ğ´Ğµ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºÑ€Ğ°Ñ‚ĞºĞ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ñƒ chain of thought Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… GSM8K, MATH-500 Ğ¸ AIME Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Reasoning Depth for Efficient AI Performance', 'desc': 'MixReasoning is a novel framework that optimizes reasoning depth in machine learning models to enhance efficiency while maintaining accuracy. It recognizes that not all reasoning steps require the same level of detail, as some are more complex than others. By dynamically adjusting the depth of reasoning, MixReasoning allows models to focus on challenging sub-problems with detailed analysis, while simplifying the approach for easier tasks. Experiments demonstrate that this method reduces the overall reasoning length and significantly boosts performance on various benchmarks without sacrificing the quality of the answers.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œæé«˜æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'MixReasoningæ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´æ¨ç†æ·±åº¦çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨¡å‹çš„æ•ˆç‡è€Œä¸ç‰ºç‰²å‡†ç¡®æ€§ã€‚ä¼ ç»Ÿçš„æ¨ç†æ¨¡å‹é€šè¿‡é€æ­¥è§£å†³é—®é¢˜æ¥æå‡æ€§èƒ½ï¼Œä½†åœ¨æ¯ä¸€æ­¥éƒ½è¿›è¡Œæ·±å…¥æ¨ç†ä¼šå¯¼è‡´å†—ä½™ï¼Œå› ä¸ºå­é—®é¢˜çš„éš¾åº¦å’Œå¤æ‚æ€§å·®å¼‚å¾ˆå¤§ã€‚MixReasoningå…è®¸æ¨¡å‹æ ¹æ®é—®é¢˜çš„å¤æ‚æ€§è‡ªé€‚åº”åœ°è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œä»è€Œåœ¨å›°éš¾æ­¥éª¤ä¸Šè¿›è¡Œè¯¦ç»†æ¨ç†ï¼Œè€Œåœ¨ç®€å•æ­¥éª¤ä¸Šè¿›è¡Œç®€æ´æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMixReasoningåœ¨ä¸é™ä½å‡†ç¡®æ€§çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—ç¼©çŸ­äº†æ¨ç†é•¿åº¦ï¼Œæé«˜äº†æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06062', 'title': 'ASPO: Asymmetric Importance Sampling Policy Optimization', 'url': 'https://huggingface.co/papers/2510.06062', 'abstract': 'ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.', 'score': 10, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '3090778074ce994d', 'authors': ['Jiakang Wang', 'Runze Liu', 'Lei Lin', 'Wenping Hu', 'Xiu Li', 'Fuzheng Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06062.jpg', 'data': {'categories': ['#training', '#rl', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ASPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ Â«Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸ÑÂ» ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Importance Sampling Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Balancing Token Weighting for Better LLM Training', 'desc': 'The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.'}, 'zh': {'title': 'ä¼˜åŒ–ä»¤ç‰ŒåŠ æƒï¼Œæå‡è®­ç»ƒç¨³å®šæ€§', 'desc': 'ASPOï¼ˆä¸å¯¹ç§°é‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰è§£å†³äº†åœ¨ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆOSRLï¼‰ä¸­ä»¤ç‰ŒåŠ æƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡ç¿»è½¬æ­£ä¼˜åŠ¿ä»¤ç‰Œçš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼ŒASPOä½¿å¾—æ­£è´Ÿä»¤ç‰Œçš„æ›´æ–°æ–¹å‘ä¸€è‡´ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒASPOè¿˜å¼•å…¥äº†ä¸€ç§è½¯åŒå‰ªåˆ‡æœºåˆ¶ï¼Œä»¥ç¨³å®šæç«¯æ›´æ–°å¹¶ä¿æŒæ¢¯åº¦æµåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASPOåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æ”¹å–„äº†è®­ç»ƒæ•ˆæœï¼Œå‡å°‘äº†è¿‡æ—©æ”¶æ•›ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05571', 'title': 'Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for\n  Academic Presentations', 'url': 'https://huggingface.co/papers/2510.05571', 'abstract': 'EvoPresent, a self-improvement agent framework using multi-task reinforcement learning, enhances academic paper promotion by generating coherent narratives, aesthetically pleasing designs, and realistic presentations, supported by a comprehensive benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: there is no way to improve it when you cannot evaluate it right. To address this, we introduce EvoPresent, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is PresAesth, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce EvoPresent Benchmark, a comprehensive benchmark comprising: Presentation Generation Quality, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and Aesthetic Awareness, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.', 'score': 10, 'issue_id': 6309, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '8aef2a97aec62eb1', 'authors': ['Chengzhi Liu', 'Yuzhe Yang', 'Kaiwen Zhou', 'Zhen Zhang', 'Yue Fan', 'Yannan Xie', 'Peng Qi', 'Xin Eric Wang'], 'affiliations': ['Uniphore', 'University of California, Santa Barbara', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2510.05571.jpg', 'data': {'categories': ['#benchmark', '#story_generation', '#rl', '#multimodal', '#agents', '#optimization', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹', 'desc': 'EvoPresent â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ multi-task reinforcement learning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ PresAesth, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºÑƒ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ Ğ¸ Ğ´Ğ°Ñ‘Ñ‚ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ 650 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ñ… AI-ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¹ Ğ¸ 2000 Ğ¿Ğ°Ñ€ ÑĞ»Ğ°Ğ¹Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ° Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°, Ğ° multi-task RL Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Enhancing Academic Paper Promotion with EvoPresent: A Self-Improvement Framework', 'desc': 'EvoPresent is a framework designed to improve the promotion of academic papers using multi-task reinforcement learning. It addresses challenges in storytelling, aesthetic quality, and self-adjustment by integrating coherent narratives and visually appealing designs. The core of EvoPresent is the PresAesth model, which provides aesthetic scoring and feedback for iterative self-improvement. The framework is evaluated using the EvoPresent Benchmark, which assesses presentation quality and aesthetic awareness based on a large dataset of academic resources.'}, 'zh': {'title': 'EvoPresentï¼šæå‡å­¦æœ¯è®ºæ–‡æ¨å¹¿çš„æ™ºèƒ½ä»£ç†æ¡†æ¶', 'desc': 'EvoPresentæ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ™ºèƒ½ä»£ç†æ¡†æ¶ï¼Œåˆ©ç”¨å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ æ¥æå‡å­¦æœ¯è®ºæ–‡çš„æ¨å¹¿æ•ˆæœã€‚å®ƒé€šè¿‡ç”Ÿæˆè¿è´¯çš„å™è¿°ã€ç¾è§‚çš„è®¾è®¡å’Œé€¼çœŸçš„æ¼”ç¤ºï¼Œè§£å†³äº†ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•åœ¨è®²æ•…äº‹ã€å®¡ç¾è´¨é‡å’Œè‡ªæˆ‘è°ƒæ•´æ–¹é¢çš„ä¸è¶³ã€‚EvoPresentçš„æ ¸å¿ƒæ˜¯PresAesthï¼Œä¸€ä¸ªå¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ ç¾å­¦æ¨¡å‹ï¼Œèƒ½å¤Ÿæä¾›å¯é çš„ç¾å­¦è¯„åˆ†å’Œç¼ºé™·è°ƒæ•´ï¼Œæ”¯æŒåœ¨æœ‰é™çš„ç¾å­¦è®­ç»ƒæ•°æ®ä¸‹è¿›è¡Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›ã€‚é€šè¿‡EvoPresentåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¼”ç¤ºç”Ÿæˆè´¨é‡å’Œç¾å­¦æ„è¯†ï¼Œå‘ç°é«˜è´¨é‡åé¦ˆå¯¹ä»£ç†è‡ªæˆ‘æ”¹è¿›è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.23379', 'title': 'CCD: Mitigating Hallucinations in Radiology MLLMs via Clinical\n  Contrastive Decoding', 'url': 'https://huggingface.co/papers/2509.23379', 'abstract': 'Clinical Contrastive Cecoding (CCD) enhances radiology report generation by integrating structured clinical signals, reducing medical hallucinations without altering the base MLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal large language models (MLLMs) have recently achieved remarkable progress in radiology by integrating visual perception with natural language understanding. However, they often generate clinically unsupported descriptions, known as medical hallucinations, which pose serious risks in medical applications that demand accuracy and image-grounded outputs. Through empirical analysis, we find that prompt-induced hallucinations remain prevalent in radiology MLLMs, largely due to over-sensitivity to clinical sections. To address this, we introduce Clinical Contrastive Cecoding (CCD), a training-free and retrieval-free inference framework that integrates structured clinical signals from task-specific radiology expert models. CCD introduces a dual-stage contrastive mechanism to refine token-level logits during generation, thereby enhancing clinical fidelity without modifying the base MLLM. Experiments on three datasets and multiple models demonstrate that CCD consistently improves overall performance on radiology report generation (RRG). On the MIMIC-CXR dataset, it yields up to a 17% improvement in RadGraph-F1 when applied to state-of-the-art RRG models. Our approach provides a lightweight and generalisable solution for mitigating medical hallucinations, effectively bridging expert models and MLLMs in radiology.', 'score': 10, 'issue_id': 6308, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '5dc19169d1633b88', 'authors': ['Xi Zhang', 'Zaiqiao Meng', 'Jake Lever', 'Edmond S. L. Ho'], 'affiliations': ['School of Computing Science, University of Glasgow, UK'], 'pdf_title_img': 'assets/pdf/title_img/2509.23379.jpg', 'data': {'categories': ['#healthcare', '#training', '#multimodal', '#hallucinations', '#data', '#science'], 'emoji': '\U0001fa7b', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğ¼Ğ¸ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ² AI-Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Clinical Contrastive Decoding (CCD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM. ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½ĞµĞ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸), Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ…. CCD Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ñ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 17% Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ RadGraph-F1 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MIMIC-CXR.'}, 'en': {'title': 'Enhancing Radiology Reports with Clinical Contrastive Cecoding', 'desc': 'Clinical Contrastive Cecoding (CCD) is a novel framework designed to improve the accuracy of radiology report generation by incorporating structured clinical signals. It addresses the issue of medical hallucinations, which are incorrect or unsupported descriptions generated by multimodal large language models (MLLMs). CCD employs a dual-stage contrastive mechanism to refine the output of these models without altering their underlying architecture. Experimental results show that CCD significantly enhances performance, achieving up to a 17% improvement in clinical accuracy on the MIMIC-CXR dataset.'}, 'zh': {'title': 'ä¸´åºŠå¯¹æ¯”ç¼–ç ï¼šæå‡æ”¾å°„å­¦æŠ¥å‘Šçš„å‡†ç¡®æ€§', 'desc': 'ä¸´åºŠå¯¹æ¯”ç¼–ç ï¼ˆCCDï¼‰é€šè¿‡æ•´åˆç»“æ„åŒ–ä¸´åºŠä¿¡å·ï¼Œå¢å¼ºäº†æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆï¼Œå‡å°‘äº†åŒ»å­¦å¹»è§‰çš„å‘ç”Ÿï¼Œè€Œä¸æ”¹å˜åŸºç¡€çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ã€‚ç ”ç©¶å‘ç°ï¼Œæ”¾å°„å­¦MLLMåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°ä¸´åºŠéƒ¨åˆ†çš„è¿‡åº¦æ•æ„Ÿå½±å“ï¼Œå¯¼è‡´ç”Ÿæˆä¸æ”¯æŒä¸´åºŠçš„æè¿°ã€‚CCDé‡‡ç”¨åŒé˜¶æ®µå¯¹æ¯”æœºåˆ¶ï¼Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç²¾ç‚¼æ ‡è®°çº§åˆ«çš„é€»è¾‘å€¼ï¼Œä»è€Œæé«˜ä¸´åºŠå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCDåœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šå‡èƒ½æ˜¾è‘—æå‡æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06208', 'title': 'ShapeGen4D: Towards High Quality 4D Shape Generation from Videos', 'url': 'https://huggingface.co/papers/2510.06208', 'abstract': 'A video-to-4D shape generation framework uses temporal attention, time-aware point sampling, and noise sharing to produce dynamic 3D representations from videos, enhancing temporal stability and perceptual fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Video-conditioned 4D shape generation aims to recover time-varying 3D geometry and view-consistent appearance directly from an input video. In this work, we introduce a native video-to-4D shape generation framework that synthesizes a single dynamic 3D representation end-to-end from the video. Our framework introduces three key components based on large-scale pre-trained 3D models: (i) a temporal attention that conditions generation on all frames while producing a time-indexed dynamic representation; (ii) a time-aware point sampling and 4D latent anchoring that promote temporally consistent geometry and texture; and (iii) noise sharing across frames to enhance temporal stability. Our method accurately captures non-rigid motion, volume changes, and even topological transitions without per-frame optimization. Across diverse in-the-wild videos, our method improves robustness and perceptual fidelity and reduces failure modes compared with the baselines.', 'score': 9, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '08bc6b783f4e2e13', 'authors': ['Jiraphon Yenphraphai', 'Ashkan Mirzaei', 'Jianqi Chen', 'Jiaxu Zou', 'Sergey Tulyakov', 'Raymond A. Yeh', 'Peter Wonka', 'Chaoyang Wang'], 'affiliations': ['KAUST', 'Purdue University', 'Snap'], 'pdf_title_img': 'assets/pdf/title_img/2510.06208.jpg', 'data': {'categories': ['#video', '#3d'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğº Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ 3D-Ñ„Ğ¾Ñ€Ğ¼Ğµ: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 4D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-Ñ„Ğ¾Ñ€Ğ¼ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ temporal attention Ğ´Ğ»Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ° Ğ²ÑĞµÑ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾, time-aware ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡ĞµĞº Ğ´Ğ»Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑˆÑƒĞ¼Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ end-to-end Ğ±ĞµĞ· Ğ¿Ğ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ non-rigid Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Transforming Videos into Dynamic 3D Shapes with Precision', 'desc': 'This paper presents a novel framework for generating dynamic 3D shapes from videos, referred to as video-to-4D shape generation. It employs temporal attention to ensure that the generated 3D representation is consistent across all frames of the video. The framework also incorporates time-aware point sampling and noise sharing to enhance the stability and quality of the output, capturing complex motions and changes in geometry. Overall, the approach significantly improves the robustness and visual fidelity of 3D reconstructions compared to existing methods.'}, 'zh': {'title': 'è§†é¢‘é©±åŠ¨çš„åŠ¨æ€3Då½¢çŠ¶ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†é¢‘åˆ°4Då½¢çŠ¶ç”Ÿæˆçš„æ¡†æ¶ï¼Œæ—¨åœ¨ä»è§†é¢‘ä¸­ç”ŸæˆåŠ¨æ€çš„3Dè¡¨ç¤ºã€‚è¯¥æ¡†æ¶åˆ©ç”¨æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶ã€æ—¶é—´æ„ŸçŸ¥ç‚¹é‡‡æ ·å’Œå™ªå£°å…±äº«ç­‰æŠ€æœ¯ï¼Œå¢å¼ºäº†ç”Ÿæˆç»“æœçš„æ—¶é—´ç¨³å®šæ€§å’Œæ„ŸçŸ¥çœŸå®æ„Ÿã€‚é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„3Dæ¨¡å‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå‡†ç¡®æ•æ‰éåˆšæ€§è¿åŠ¨ã€ä½“ç§¯å˜åŒ–å’Œæ‹“æ‰‘è½¬å˜ã€‚ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šæ ·åŒ–çš„è§†é¢‘ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œæ„ŸçŸ¥çœŸå®æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06131', 'title': 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation', 'url': 'https://huggingface.co/papers/2510.06131', 'abstract': 'MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.', 'score': 9, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '0641a3ba669f441d', 'authors': ['Jiawei Mao', 'Yuhan Wang', 'Lifeng Chen', 'Can Zhao', 'Yucheng Tang', 'Dong Yang', 'Liangqiong Qu', 'Daguang Xu', 'Yuyin Zhou'], 'affiliations': ['NVIDIA', 'UC Santa Cruz', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.06131.jpg', 'data': {'categories': ['#diffusion', '#science', '#healthcare', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'MeDiM â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸) Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ LLM Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹: ÑƒĞ±Ñ€Ğ°Ğ½Ğ° ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ timestep embeddings Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° downstream-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚.'}, 'en': {'title': 'Unifying Biomedical Data with MeDiM: A Multimodal Diffusion Revolution', 'desc': "MeDiM is a novel medical discrete diffusion model designed to integrate various types of biomedical data, such as images, text, and clinical notes. It overcomes the limitations of traditional models that operate within specific modalities by learning shared distributions across all data types. By utilizing a multimodal large language model as its backbone, MeDiM can generate high-quality medical outputs and translate between different modalities effectively. The model's innovative design features, like bidirectional context and continuous timestep embeddings, contribute to its superior performance in generating coherent and clinically relevant multimodal outputs."}, 'zh': {'title': 'MeDiMï¼šåŒ»å­¦å¤šæ¨¡æ€ç”Ÿæˆçš„åˆ›æ–°æ¡¥æ¢', 'desc': 'MeDiMæ˜¯ä¸€ç§åŒ»å­¦ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ å›¾åƒã€æ–‡æœ¬å’Œä¸´åºŠè®°å½•ä¹‹é—´çš„å…±äº«åˆ†å¸ƒæ¥æ•´åˆå¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æ•°æ®ã€‚è¯¥æ¨¡å‹å…‹æœäº†ä¼ ç»Ÿç”ŸæˆåŒ»å­¦æ¨¡å‹åœ¨ç‰¹å®šæ¨¡æ€åœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œå®ç°äº†é«˜ä¿çœŸåº¦çš„ç”Ÿæˆå’Œå¢å¼ºçš„ä¸‹æ¸¸æ€§èƒ½ã€‚MeDiMç»Ÿä¸€äº†å¤šç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„ç¿»è¯‘ï¼Œä»¥åŠæ ¹æ®æç¤ºå…±åŒç”Ÿæˆè·¨é¢†åŸŸçš„å›¾åƒ-æŠ¥å‘Šå¯¹ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ‰©æ•£éª¨å¹²ï¼ŒMeDiMåœ¨ä¸€ä¸ªå…±äº«çš„æ¦‚ç‡ç©ºé—´ä¸­æ¡¥æ¥äº†è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06182', 'title': 'Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context', 'url': 'https://huggingface.co/papers/2510.06182', 'abstract': 'Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  \t\t\t\t\tAI-generated summary \t\t\t\t A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.', 'score': 8, 'issue_id': 6300, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'ebd0b1b2a51e6c0a', 'authors': ['Yoav Gur-Arieh', 'Mor Geva', 'Atticus Geiger'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Goodfire', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.06182.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#data', '#multimodal', '#interpretability', '#architecture'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ¢Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°. ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ², Ğ½Ğ¾ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. LLM ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ¾ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ (Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°) Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ (Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸). Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²ÑĞµ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 95% Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unraveling Entity Binding in Language Models', 'desc': 'This paper explores how language models (LMs) bind and retrieve entities during in-context reasoning. It identifies three mechanisms used by LMs: positional, lexical, and reflexive, which help in accurately predicting the next tokens. The study reveals that while the positional mechanism works well for short lists of entities, it struggles with longer contexts, leading to the use of lexical and reflexive mechanisms for better retrieval. By developing a causal model that integrates these mechanisms, the authors achieve high accuracy in predicting token distributions across various tasks and input lengths.'}, 'zh': {'title': 'è¯­è¨€æ¨¡å‹çš„å®ä½“ç»‘å®šä¸æ£€ç´¢æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹å¦‚ä½•åœ¨ä¸Šä¸‹æ–‡ä¸­ç»‘å®šå’Œæ£€ç´¢å®ä½“ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„åŸºäºä½ç½®çš„æœºåˆ¶åœ¨å¤æ‚æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå› æ­¤è¯­è¨€æ¨¡å‹è¿˜ä½¿ç”¨äº†è¯æ±‡æœºåˆ¶å’Œåå°„æœºåˆ¶æ¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹ä¹ç§æ¨¡å‹å’Œåä¸ªç»‘å®šä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¯­è¨€æ¨¡å‹å¦‚ä½•æ··åˆä½¿ç”¨è¿™äº›æœºåˆ¶æ¥é©±åŠ¨æ¨¡å‹è¡Œä¸ºã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»“åˆä¸‰ç§æœºåˆ¶çš„å› æœæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ›´é•¿çš„è¾“å…¥æ–‡æœ¬ä¸­æœ‰æ•ˆåœ°é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03506', 'title': 'OneFlow: Concurrent Mixed-Modal and Interleaved Generation with Edit\n  Flows', 'url': 'https://huggingface.co/papers/2510.03506', 'abstract': 'OneFlow, a non-autoregressive multimodal model, achieves superior performance in text-image generation and understanding tasks with reduced computational cost compared to autoregressive and diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t We present OneFlow, the first non-autoregressive multimodal model that enables variable-length and concurrent mixed-modal generation. Unlike autoregressive models that enforce rigid causal ordering between text and image generation, OneFlow combines an insertion-based Edit Flow for discrete text tokens with Flow Matching for image latents. OneFlow enables concurrent text-image synthesis with hierarchical sampling that prioritizes content over grammar. Through controlled experiments across model sizes from 1B to 8B, we demonstrate that OneFlow outperforms autoregressive baselines on both generation and understanding tasks while using up to 50% fewer training FLOPs. OneFlow surpasses both autoregressive and diffusion-based approaches while unlocking new capabilities for concurrent generation, iterative refinement, and natural reasoning-like generation.', 'score': 8, 'issue_id': 6306, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'faa9eab196ef87b3', 'authors': ['John Nguyen', 'Marton Havasi', 'Tariq Berrada', 'Luke Zettlemoyer', 'Ricky T. Q. Chen'], 'affiliations': ['CNRS', 'FAIR at Meta', 'Grenoble INP', 'Inria', 'LJK, France', 'Univ. Grenoble Alpes'], 'pdf_title_img': 'assets/pdf/title_img/2510.03506.jpg', 'data': {'categories': ['#training', '#optimization', '#games', '#architecture', '#diffusion', '#multimodal'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸', 'desc': 'OneFlow â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ½ĞµĞ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµÑ‚ Edit Flow Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Flow Matching Ğ´Ğ»Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ¶Ñ‘ÑÑ‚ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. OneFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ° 50% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ñ…Ğ¾Ğ¶ĞµĞ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'OneFlow: Revolutionizing Text-Image Generation with Efficiency and Flexibility', 'desc': 'OneFlow is a groundbreaking non-autoregressive multimodal model designed for generating and understanding text and images. It utilizes an innovative insertion-based Edit Flow for text and Flow Matching for images, allowing for simultaneous generation without the strict sequence required by traditional autoregressive models. This approach not only enhances the efficiency of the generation process but also improves performance on various tasks while consuming significantly fewer computational resources. Through extensive testing, OneFlow has shown to outperform existing models, paving the way for more advanced and flexible multimodal applications.'}, 'zh': {'title': 'OneFlowï¼šé«˜æ•ˆçš„æ–‡æœ¬å›¾åƒç”Ÿæˆæ–°æ¨¡å¼', 'desc': 'OneFlowæ˜¯ä¸€ç§éè‡ªå›å½’çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤ŸåŒæ—¶ç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒï¼Œä¸”è®¡ç®—æˆæœ¬ä½äºè‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹ã€‚ä¸è‡ªå›å½’æ¨¡å‹ä¸åŒï¼ŒOneFlowä¸éœ€è¦ä¸¥æ ¼çš„å› æœé¡ºåºï¼Œè€Œæ˜¯ç»“åˆäº†åŸºäºæ’å…¥çš„ç¼–è¾‘æµå’Œå›¾åƒæ½œå˜é‡çš„æµåŒ¹é…ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆ†å±‚é‡‡æ ·ä¼˜å…ˆè€ƒè™‘å†…å®¹è€Œéè¯­æ³•ï¼Œå®ç°äº†æ–‡æœ¬å’Œå›¾åƒçš„å¹¶å‘åˆæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOneFlowåœ¨ç”Ÿæˆå’Œç†è§£ä»»åŠ¡ä¸Šå‡ä¼˜äºè‡ªå›å½’åŸºçº¿ï¼ŒåŒæ—¶è®­ç»ƒæ‰€éœ€çš„FLOPså‡å°‘äº†50%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05485', 'title': 'TensorBLEU: Vectorized GPU-based BLEU Score Implementation for\n  Per-Sentence In-Training Evaluation', 'url': 'https://huggingface.co/papers/2510.05485', 'abstract': 'TensorBLEU is a GPU-accelerated BLEU metric implementation for efficient in-training evaluation of natural language processing models, offering significant speedups over CPU-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using torch.unique, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a "Token-ID BLEU" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning.', 'score': 7, 'issue_id': 6306, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'af039fb3475e6646', 'authors': ['Adam Filipek'], 'affiliations': ['Reactive AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.05485.jpg', 'data': {'categories': ['#training', '#open_source', '#optimization', '#benchmark'], 'emoji': 'âš¡', 'ru': {'title': 'ĞœĞ¾Ğ»Ğ½Ğ¸ĞµĞ½Ğ¾ÑĞ½Ğ°Ñ BLEU Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ½Ğ° GPU Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'TensorBLEU â€” ÑÑ‚Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ BLEU, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° GPU Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ NLP. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ n-Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ‚Ñ‡-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ torch.unique, Ñ‡Ñ‚Ğ¾ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¾Ğ³Ñ€Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ° GPU NVIDIA T4 Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² 13 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ CPU-Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¸Ğ· NLTK, Ğ° Ğ½Ğ° A100 â€” Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ² 40 Ñ€Ğ°Ğ· Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ. Ğ­Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Reinforcement Learning Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ñƒ, Ğ³Ğ´Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ reward ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Accelerating NLP Evaluation with TensorBLEU', 'desc': 'TensorBLEU is a new implementation of the BLEU metric that runs on GPUs, making it much faster than traditional CPU methods. It is specifically designed for evaluating natural language processing models during training, which is crucial for tasks like Reinforcement Learning. By using a unique memory-efficient approach to count n-grams, TensorBLEU can handle large vocabularies without the high memory costs of older methods. Benchmarks show that it can be over 13 times faster on consumer GPUs and more than 40 times faster on high-end data center GPUs, significantly speeding up the training process.'}, 'zh': {'title': 'TensorBLEUï¼šåŠ é€Ÿè‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹è¯„ä¼°çš„åˆ©å™¨', 'desc': 'TensorBLEUæ˜¯ä¸€ç§é’ˆå¯¹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹çš„BLEUæŒ‡æ ‡çš„GPUåŠ é€Ÿå®ç°ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°æ•ˆç‡ã€‚å®ƒé€šè¿‡åœ¨PyTorchä¸­è¿›è¡Œå®Œå…¨å‘é‡åŒ–çš„æ¯å¥è®¡ç®—ï¼Œæ˜¾è‘—æå‡äº†è®¡ç®—é€Ÿåº¦ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡è¯æ±‡æ¨¡å‹æ—¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å†…å­˜é«˜æ•ˆçš„è®¡æ•°æœºåˆ¶ï¼Œé¿å…äº†ä¼ ç»Ÿå“ˆå¸Œå‘é‡åŒ–çš„é«˜å†…å­˜æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTensorBLEUåœ¨æ¶ˆè´¹çº§GPUä¸Šé€Ÿåº¦æå‡è¶…è¿‡13å€ï¼Œåœ¨æ•°æ®ä¸­å¿ƒçº§ç¡¬ä»¶ä¸Šè¶…è¿‡40å€ï¼Œæå¤§åœ°å‡å°‘äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„è®¡ç®—ç“¶é¢ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05560', 'title': 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video', 'url': 'https://huggingface.co/papers/2510.05560', 'abstract': "HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.", 'score': 6, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'fcf790fe9803a797', 'authors': ['Hongchi Xia', 'Chih-Hao Lin', 'Hao-Yu Hsu', 'Quentin Leboutet', 'Katelyn Gao', 'Michael Paulitsch', 'Benjamin Ummenhofer', 'Shenlong Wang'], 'affiliations': ['Intel', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.05560.jpg', 'data': {'categories': ['#3d', '#optimization', '#games', '#benchmark'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼', 'desc': "HoloScene â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ prior'Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ sampling Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ AR/VR, Ğ¸Ğ³Ñ€ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."}, 'en': {'title': 'Revolutionizing 3D Reconstruction with HoloScene', 'desc': 'HoloScene is a cutting-edge framework for creating interactive 3D reconstructions that meet essential criteria for realistic simulations. It addresses common shortcomings in existing methods by ensuring geometry completeness, object interactivity, physical plausibility, and photorealistic rendering. The framework utilizes an energy-based optimization approach that combines observational data and physical constraints to produce accurate digital twins. Its effectiveness is demonstrated through superior performance on benchmark datasets and practical applications in gaming and digital-twin manipulation.'}, 'zh': {'title': 'HoloSceneï¼šå®ç°çœŸå®æ„Ÿçš„äº¤äº’å¼3Dé‡å»º', 'desc': 'HoloSceneæ˜¯ä¸€ä¸ªäº¤äº’å¼3Dé‡å»ºæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å‡ ä½•å®Œæ•´æ€§ã€ç‰©ä½“äº¤äº’æ€§ã€ç‰©ç†åˆç†æ€§ã€ç…§ç‰‡çº§æ¸²æŸ“å’ŒçœŸå®çš„ç‰©ç†å±æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡èƒ½é‡ä¼˜åŒ–é—®é¢˜æ¥æ•´åˆè§‚å¯Ÿæ•°æ®ã€ç‰©ç†çº¦æŸå’Œç”Ÿæˆå…ˆéªŒï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„ç›®æ ‡ã€‚HoloSceneåˆ©ç”¨å…¨é¢çš„äº¤äº’åœºæ™¯å›¾è¡¨ç¤ºï¼Œç¼–ç ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€å¤–è§‚å’Œç‰©ç†å±æ€§ï¼ŒåŒæ—¶è€ƒè™‘å±‚æ¬¡å’Œç‰©ä½“é—´çš„å…³ç³»ã€‚é€šè¿‡ç»“åˆåŸºäºé‡‡æ ·çš„æ¢ç´¢å’ŒåŸºäºæ¢¯åº¦çš„ç»†åŒ–ï¼Œä¼˜åŒ–è¿‡ç¨‹é«˜æ•ˆè¿›è¡Œï¼Œæœ€ç»ˆç”Ÿæˆçš„æ•°å­—åŒèƒèƒåœ¨æ–°è§†è§’ä¸‹å±•ç°å‡ºå®Œæ•´ç²¾ç¡®çš„å‡ ä½•å½¢çŠ¶å’ŒçœŸå®çš„æ¸²æŸ“æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05432', 'title': 'AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems', 'url': 'https://huggingface.co/papers/2510.05432', 'abstract': 'AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.', 'score': 6, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'effef15175939fca', 'authors': ['Shambhavi Mishra', 'Gaurav Sahu', 'Marco Pedersoli', 'Laurent Charlin', 'Jose Dolz', 'Christopher Pal'], 'affiliations': ['Canada CIFAR AI Chair', 'HEC Montreal', 'International Laboratory on Learning Systems (ILLS)', 'LIVIA, ETS Montreal', 'Mila Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.05432.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#rlhf', '#agents'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ AI ÑÑ‚Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AInstein â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ AI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ ICLR 2025 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼: ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹, Ğ½Ğ¾ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'AInstein: Unveiling the Problem-Solving Power of LLMs', 'desc': "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç§‘å­¦é—®é¢˜è§£å†³èƒ½åŠ›', 'desc': 'AInsteinæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³é—®é¢˜èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒæµ‹è¯•è¿™äº›æ¨¡å‹åœ¨æ²¡æœ‰é¢†åŸŸç‰¹å®šå¾®è°ƒæˆ–å¤–éƒ¨å¸®åŠ©çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆçš„AIç ”ç©¶é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æå–é«˜è´¨é‡ICLR 2025æäº¤çš„ç²¾ç‚¼é—®é¢˜é™ˆè¿°ï¼ŒAInsteinæ¨¡æ‹Ÿç§‘å­¦ç ”ç©¶ä¸­çš„ææ¡ˆã€å®¡æŸ¥å’Œä¿®è®¢å¾ªç¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿé‡æ–°å‘ç°å¯è¡Œçš„è§£å†³æ–¹æ¡ˆå¹¶å¶å°”æå‡ºåˆ›é€ æ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜èƒ½åŠ›ä»ç„¶è„†å¼±ï¼Œä¸”å¯¹é—®é¢˜çš„è¡¨è¿°éå¸¸æ•æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06036', 'title': 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?', 'url': 'https://huggingface.co/papers/2510.06036', 'abstract': "Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", 'score': 5, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '0d9aa29b1d58dbd6', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Linyi Yang', 'Wenxuan Huang', 'Wenjie Li', 'Xiting Wang', 'Jaehong Yoon', 'YunXing', 'XingYu', 'Jinjin Gu'], 'affiliations': ['East China Normal University', 'Hong Kong Polytechnic University', 'INSAIT', 'Nanyang Technological University', 'Renmin University', 'Southern University of Science and Technology', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06036.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#data', '#alignment', '#interpretability', '#training'], 'emoji': 'ğŸ§—', 'ru': {'title': 'ĞĞ±Ñ€Ñ‹Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ½ĞµĞ·Ğ°Ğ¿Ğ½Ğ¾ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Â«Ğ¾Ğ±Ñ€Ñ‹Ğ²Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Â» Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… reasoning-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ÑÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ñ€ĞµĞ·ĞºĞ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ attention heads, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cliff-as-a-Judge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1.7% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. ĞĞ±Ğ»ÑÑ†Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ 3% Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… attention heads ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ¸Ğ¶Ğµ 10%.'}, 'en': {'title': 'Understanding and Mitigating the Refusal Cliff in Reasoning Models', 'desc': "This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data."}, 'zh': {'title': 'æ­ç¤ºæ¨ç†æ¨¡å‹çš„æ‹’ç»æ‚¬å´–æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„æ‹’ç»æ‚¬å´–æœºåˆ¶ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºå‰æ‹’ç»æ„å›¾ä¼šæ€¥å‰§ä¸‹é™ã€‚é€šè¿‡çº¿æ€§æ¢æµ‹æ–¹æ³•ï¼Œæˆ‘ä»¬è¿½è¸ªäº†æ‹’ç»æ„å›¾åœ¨æ ‡è®°ä½ç½®çš„å˜åŒ–ï¼Œå‘ç°è®¸å¤šæ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè¯†åˆ«æœ‰å®³æç¤ºï¼Œä½†åœ¨è¾“å‡ºå‰çš„æœ€åå‡ ä¸ªæ ‡è®°å¤„æ‹’ç»æ„å›¾å´æ˜¾è‘—é™ä½ã€‚æˆ‘ä»¬é€šè¿‡å› æœå¹²é¢„åˆ†æï¼Œè¯†åˆ«å‡ºå°‘é‡å¯¹æ‹’ç»è¡Œä¸ºäº§ç”Ÿè´Ÿé¢å½±å“çš„æ³¨æ„åŠ›å¤´ï¼Œå»é™¤è¿™äº›å¤´å¯ä»¥æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨æ‹’ç»æ‚¬å´–çš„ç‰¹å¾æ¥é«˜æ•ˆä¿®å¤æ¨ç†æ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05367', 'title': 'LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation', 'url': 'https://huggingface.co/papers/2510.05367', 'abstract': 'The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .', 'score': 5, 'issue_id': 6299, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'da5c54f2d1fce894', 'authors': ['Yang Xiao', 'Gen Li', 'Kaiyuan Deng', 'Yushu Wu', 'Zheng Zhan', 'Yanzhi Wang', 'Xiaolong Ma', 'Bo Hui'], 'affiliations': ['Clemson University', 'Microsoft Research', 'Northeastern University', 'The University of Arizona', 'University of Tulsa'], 'pdf_title_img': 'assets/pdf/title_img/2510.05367.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#video', '#optimization', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ (ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€ĞµĞ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ ĞºÑÑˆĞ°, Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ñ€ĞµĞ·ĞºĞ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Accelerating Video Generation with Stage-Specific Strategies', 'desc': 'This paper focuses on improving the efficiency of video generation using diffusion models by introducing stage-specific strategies. It identifies that the inference process can be broken down into three stages: encoding, denoising, and decoding, and highlights the memory issues that arise during the latter two stages. The authors propose methods such as Asynchronous Cache Swapping, Feature Chunking, and Slicing Latents to optimize memory usage without significantly increasing processing time. Overall, their approach results in faster inference speeds and reduced memory consumption while keeping quality loss minimal.'}, 'zh': {'title': 'é˜¶æ®µç‰¹å®šç­–ç•¥åŠ é€Ÿè§†é¢‘ç”Ÿæˆæ¨ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ¨ç†åŠ é€Ÿçš„é˜¶æ®µç‰¹å®šç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘å†…å­˜ä½¿ç”¨å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºç¼–ç ã€å»å™ªå’Œè§£ç ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶å‘ç°åŸºäºç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•åœ¨åä¸¤ä¸ªé˜¶æ®µå¸¸å¸¸å¯¼è‡´å†…å­˜æ¿€å¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒé˜¶æ®µæ¨ç†çš„ç‰¹å¾ï¼Œå¹¶æå‡ºäº†ä¸‰ç§å‡å°‘å†…å­˜æ¶ˆè€—çš„ç­–ç•¥ï¼šå¼‚æ­¥ç¼“å­˜äº¤æ¢ã€ç‰¹å¾å—å’Œåˆ‡ç‰‡æ½œå˜é‡è§£ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç¡®ä¿è¿™ä¸‰ç§ç­–ç•¥å¼•å…¥çš„æ—¶é—´å¼€é”€ä½äºåŠ é€Ÿå¸¦æ¥çš„æ”¶ç›Šã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—èŒƒå›´å†…çš„è´¨é‡ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05342', 'title': 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization', 'url': 'https://huggingface.co/papers/2510.05342', 'abstract': 'MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.', 'score': 5, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '1a257d25fefce283', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.05342.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ', 'desc': 'MADPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ DPO Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ reward model Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğº loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ DPO Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ sample. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ±Ğ°Ñ‚Ñ‡-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ uniformĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +33.3% Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Preference Alignment with Adaptive Weighting', 'desc': "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."}, 'zh': {'title': 'è¾¹é™…è‡ªé€‚åº”ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹åå¥½å¯¹é½', 'desc': 'MADPOæ˜¯ä¸€ç§è¾¹é™…è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡ä¸ºDPOæŸå¤±æä¾›å®ä¾‹çº§è‡ªé€‚åº”åŠ æƒï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥ä¼°è®¡åå¥½è¾¹é™…ï¼Œç„¶åæ ¹æ®è¿™äº›è¾¹é™…ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åº”ç”¨è¿ç»­çš„è‡ªé€‚åº”æƒé‡ã€‚MADPOçš„é‡åŠ æƒæ–¹æ¡ˆå¯¹å›°éš¾æ ·æœ¬å¢å¼ºä¿¡å·ï¼Œå¯¹ç®€å•æ ·æœ¬å‡å¼±ä¿¡å·ï¼Œä»è€Œå®ç°äº†å¯¹å­¦ä¹ ä¿¡å·çš„ç»†è‡´æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMADPOåœ¨æƒ…æ„Ÿç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºåŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨åå¥½å¯¹é½æ–¹é¢çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05318', 'title': 'BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language\n  Models via Lens of Dynamic Interactions', 'url': 'https://huggingface.co/papers/2510.05318', 'abstract': "BIRD-INTERACT is a benchmark for multi-turn text-to-SQL tasks that simulates realistic database assistant challenges through dynamic interactions, hierarchical knowledge bases, and autonomous decision-making.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.", 'score': 5, 'issue_id': 6305, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '88e68f710dc7b7fc', 'authors': ['Nan Huo', 'Xiaohan Xu', 'Jinyang Li', 'Per Jacobsson', 'Shipei Lin', 'Bowen Qin', 'Binyuan Hui', 'Xiaolong Li', 'Ge Qu', 'Shuzheng Si', 'Linheng Han', 'Edward Alexander', 'Xintong Zhu', 'Rui Qin', 'Ruihan Yu', 'Yiyao Jin', 'Feige Zhou', 'Weihao Zhong', 'Yun Chen', 'Hongyu Liu', 'Chenhao Ma', 'Fatma Ozcan', 'Yannis Papakonstantinou', 'Reynold Cheng'], 'affiliations': ['Google Cloud', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.05318.jpg', 'data': {'categories': ['#agents', '#interpretability', '#benchmark', '#reasoning'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… SQL-Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BIRD-INTERACT â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², BIRD-INTERACT Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ¼ CRUD-Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 600 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ´Ğ°Ğ¶Ğµ GPT-5 ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ 8-17% Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ.'}, 'en': {'title': 'BIRD-INTERACT: Elevating Multi-Turn Text-to-SQL Challenges', 'desc': 'BIRD-INTERACT is a new benchmark designed for multi-turn text-to-SQL tasks, addressing the limitations of existing benchmarks that do not accurately simulate real-world database interactions. It creates a dynamic environment where models can engage in conversations, ask for clarifications, and recover from errors autonomously, reflecting the complexities of actual database applications. The benchmark includes two evaluation settings: a structured conversational protocol and an open-ended setting, allowing for a range of interactions. Empirical results show that even advanced models like GPT-5 struggle with these tasks, emphasizing the need for effective interaction in complex text-to-SQL scenarios.'}, 'zh': {'title': 'BIRD-INTERACTï¼šçœŸå®å¤šè½®äº¤äº’çš„æ•°æ®åº“åŠ©æ‰‹æŒ‘æˆ˜', 'desc': 'BIRD-INTERACTæ˜¯ä¸€ä¸ªç”¨äºå¤šè½®æ–‡æœ¬åˆ°SQLä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿç°å®æ•°æ®åº“åŠ©æ‰‹é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ç°æœ‰çš„åŸºå‡†ä¸åŒï¼ŒBIRD-INTERACTé€šè¿‡åŠ¨æ€äº¤äº’å’Œåˆ†å±‚çŸ¥è¯†åº“ï¼Œå…è®¸æ¨¡å‹åœ¨æ²¡æœ‰äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªä¸»å†³ç­–å’Œé”™è¯¯æ¢å¤ã€‚è¯¥åŸºå‡†æä¾›äº†ä¸¤ç§è¯„ä¼°è®¾ç½®ï¼Œåˆ†åˆ«æ˜¯é¢„å®šä¹‰çš„å¯¹è¯åè®®å’Œå¼€æ”¾å¼è‡ªä¸»è®¾ç½®ï¼Œæ¶µç›–äº†å•†ä¸šæ™ºèƒ½å’Œæ“ä½œç”¨ä¾‹çš„å®Œæ•´CRUDä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBIRD-INTERACTçš„ä»»åŠ¡éš¾åº¦è¾ƒé«˜ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®Œæˆè¿™äº›ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05251', 'title': 'Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2510.05251', 'abstract': "Exploratory Annealed Decoding (EAD) improves sample efficiency in reinforcement learning with verifiable rewards by dynamically adjusting the sampling temperature during generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.", 'score': 5, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'b57b7195043b802a', 'authors': ['Chenghao Yang', 'Lin Gui', 'Chenxiao Yang', 'Victor Veitch', 'Lizhu Zhang', 'Zhuokai Zhao'], 'affiliations': ['Data Science Institute, University of Chicago', 'Department of Computer Science, University of Chicago', 'Department of Statistics, University of Chicago', 'Meta AI', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2510.05251.jpg', 'data': {'categories': ['#reasoning', '#training', '#optimization', '#rl'], 'emoji': 'ğŸŒ¡ï¸', 'ru': {'title': 'Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞ¹ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼: Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Exploratory Annealed Decoding (EAD) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ´ĞµÑ â€” Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñƒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸: Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ° Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. EAD Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Dynamic Exploration for Enhanced Learning Efficiency', 'desc': 'Exploratory Annealed Decoding (EAD) enhances sample efficiency in reinforcement learning with verifiable rewards by adjusting the sampling temperature dynamically. This method addresses the challenges of maintaining sample quality while ensuring training stability during exploration. EAD employs a strategy of exploring more at the beginning of the sequence and exploiting at the end, which allows for high-level diversity initially and preserves quality later. The results show that EAD outperforms traditional fixed-temperature sampling methods, making it a valuable tool for improving the reasoning capabilities of large language models.'}, 'zh': {'title': 'æ¢ç´¢æ€§é€€ç«è§£ç ï¼šæå‡æ ·æœ¬æ•ˆç‡çš„åˆ›æ–°ç­–ç•¥', 'desc': 'æ¢ç´¢æ€§é€€ç«è§£ç ï¼ˆEADï¼‰æ˜¯ä¸€ç§æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±æ ·æœ¬æ•ˆç‡çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´é‡‡æ ·æ¸©åº¦ï¼Œè§£å†³äº†æ ·æœ¬è´¨é‡ä¸è®­ç»ƒç¨³å®šæ€§ä¹‹é—´çš„çŸ›ç›¾ã€‚EADé‡‡ç”¨äº†â€œå¼€å§‹æ¢ç´¢ï¼Œç»“æŸåˆ©ç”¨â€çš„ç­–ç•¥ï¼ŒåˆæœŸé«˜æ¸©é‡‡æ ·ä¿ƒè¿›å¤šæ ·æ€§ï¼ŒåæœŸä½æ¸©é‡‡æ ·ä¿æŒæ ·æœ¬è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒEADåœ¨å¤šç§å¼ºåŒ–å­¦ä¹ ç®—æ³•å’Œæ¨¡å‹è§„æ¨¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºå›ºå®šæ¸©åº¦é‡‡æ ·ï¼Œæå‡äº†æ ·æœ¬æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04506', 'title': 'GRACE: Generative Representation Learning via Contrastive Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2510.04506', 'abstract': 'GRACE uses contrastive policy optimization to train LLMs as generative agents that produce interpretable rationales, improving embeddings and transparency.  \t\t\t\t\tAI-generated summary \t\t\t\t Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.', 'score': 5, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'efc35bb65ba488c2', 'authors': ['Jiashuo Sun', 'Shixuan Liu', 'Zhaochen Su', 'Xianrui Zhong', 'Pengcheng Jiang', 'Bowen Jin', 'Peiran Li', 'Weijia Shi', 'Jiawei Han'], 'affiliations': ['Australian National University', 'Hong Kong University of Science and Technology', 'University of Illinois Urbana-Champaign', 'University of Washington', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2510.04506.jpg', 'data': {'categories': ['#reasoning', '#agents', '#rl', '#interpretability', '#optimization', '#training', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ GRACE, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºÑƒ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¸Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº, Ğ³Ğ´Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ°Ğº Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ reinforcement learning Ğ°Ğ³ĞµĞ½Ñ‚Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (rationales) ÑĞ²Ğ¾ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ MTEB Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 11.5% Ğ² supervised Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ğ¸ 6.9% Ğ² unsupervised Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming LLMs into Interpretable Generative Agents with GRACE', 'desc': "GRACE is a new framework that enhances the training of Large Language Models (LLMs) by using contrastive policy optimization. Instead of treating the model as a black box, GRACE allows the LLM to generate clear and interpretable rationales for its decisions, improving transparency. It uses a reward-based approach to guide the model's learning, maximizing the similarity of positive examples while minimizing that of negative ones. This results in better embeddings and a more understandable reasoning process, achieving significant performance improvements on benchmark tests."}, 'zh': {'title': 'GRACEï¼šå°†å¯¹æ¯”ä¼˜åŒ–è½¬åŒ–ä¸ºå¯è§£é‡Šçš„ç”Ÿæˆèƒ½åŠ›', 'desc': 'GRACEæ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡å¯¹æ¯”ç­–ç•¥ä¼˜åŒ–æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆå¯è§£é‡Šçš„æ¨ç†ã€‚è¿™ç§æ–¹æ³•å°†å¯¹æ¯”ä¿¡å·è§†ä¸ºå¥–åŠ±ï¼Œè€Œä¸æ˜¯ç®€å•çš„æŸå¤±ï¼Œä»è€Œå¼•å¯¼ç”Ÿæˆç­–ç•¥ã€‚GRACEä½¿å¾—LLMèƒ½å¤Ÿç”Ÿæˆç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œæå‡äº†æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¤šç»„ä»¶å¥–åŠ±å‡½æ•°çš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ŒGRACEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02300', 'title': 'Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models', 'url': 'https://huggingface.co/papers/2510.02300', 'abstract': 'Equilibrium Matching (EqM) is a generative modeling framework that learns an equilibrium gradient of an implicit energy landscape, enabling efficient sampling and outperforming traditional diffusion and flow models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256times256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.', 'score': 4, 'issue_id': 6306, 'pub_date': '2025-10-02', 'pub_date_card': {'ru': '2 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 2', 'zh': '10æœˆ2æ—¥'}, 'hash': '2955dd166f02c0c7', 'authors': ['Runqian Wang', 'Yilun Du'], 'affiliations': ['Harvard University', 'MIT'], 'pdf_title_img': 'assets/pdf/title_img/2510.02300.jpg', 'data': {'categories': ['#inference', '#optimization', '#cv', '#diffusion', '#multimodal'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸', 'desc': 'Equilibrium Matching (EqM) â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰ĞµĞ¹ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ flow-based Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ EqM Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ½ĞµÑĞ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ FID 1.90 Ğ½Ğ° ImageNet 256Ã—256, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, OOD-Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. EqM ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ flow-Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ energy-based Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ»Ğ°Ğ½Ğ´ÑˆĞ°Ñ„Ñ‚.'}, 'en': {'title': 'Harnessing Equilibrium for Superior Generative Modeling', 'desc': 'Equilibrium Matching (EqM) is a new framework for generative modeling that focuses on learning the equilibrium gradient of an implicit energy landscape. Unlike traditional models that rely on time-dependent dynamics, EqM uses a stable equilibrium approach to improve sampling efficiency. This method allows for optimization-based sampling during inference, where samples are generated through gradient descent on the learned landscape. Empirical results show that EqM outperforms existing diffusion and flow models, achieving impressive performance metrics while also being adaptable for various tasks like image denoising and out-of-distribution detection.'}, 'zh': {'title': 'å¹³è¡¡åŒ¹é…ï¼šé«˜æ•ˆç”Ÿæˆçš„æ–°æ–¹æ³•', 'desc': 'å¹³è¡¡åŒ¹é…ï¼ˆEqMï¼‰æ˜¯ä¸€ç§ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œæ—¨åœ¨å­¦ä¹ éšå¼èƒ½é‡æ™¯è§‚çš„å¹³è¡¡æ¢¯åº¦ï¼Œä»è€Œå®ç°é«˜æ•ˆé‡‡æ ·ã€‚ä¸ä¼ ç»Ÿçš„æ‰©æ•£å’Œæµæ¨¡å‹ä¸åŒï¼ŒEqMæ‘’å¼ƒäº†éå¹³è¡¡çš„æ—¶é—´æ¡ä»¶åŠ¨æ€ï¼Œä¸“æ³¨äºå¹³è¡¡çŠ¶æ€çš„å­¦ä¹ ã€‚é€šè¿‡ä¼˜åŒ–åŸºç¡€çš„é‡‡æ ·è¿‡ç¨‹ï¼ŒEqMåœ¨æ¨ç†æ—¶èƒ½å¤Ÿé€šè¿‡æ¢¯åº¦ä¸‹é™è·å–æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆæ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿæ¨¡å‹ã€‚é™¤äº†ç”Ÿæˆä»»åŠ¡ï¼ŒEqMè¿˜çµæ´»åœ°å¤„ç†éƒ¨åˆ†å™ªå£°å›¾åƒå»å™ªã€å¼‚å¸¸æ£€æµ‹å’Œå›¾åƒåˆæˆç­‰ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05137', 'title': 'Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics', 'url': 'https://huggingface.co/papers/2510.05137', 'abstract': "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.", 'score': 4, 'issue_id': 6298, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'd780a77899923141', 'authors': ['Maojia Song', 'Renhang Liu', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Soujanya Poria', 'Jingren Zhou'], 'affiliations': ['Nanyang Technological University (NTU)', 'Singapore University of Technology and Design (SUTD)', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.05137.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#benchmark', '#leakage', '#architecture', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'WebDetective: ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ° Ğ½Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼', 'desc': 'WebDetective â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Â«ÑƒÑ‚ĞµÑ‡ĞºÑƒÂ» Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Wikipedia Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ.'}, 'en': {'title': 'WebDetective: Enhancing Multi-Hop Reasoning in AI Systems', 'desc': 'WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities.'}, 'zh': {'title': 'WebDetectiveï¼šæå‡å¤šè·³æ¨ç†çš„è¯„ä¼°æ ‡å‡†', 'desc': 'WebDetectiveæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°RAGç³»ç»Ÿå’Œç½‘ç»œä»£ç†çš„å¤šè·³æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³æ¨ç†è·¯å¾„æ³„æ¼å’Œå•æ¬¡è¯„ä¼°çš„é—®é¢˜ã€‚è¯¥åŸºå‡†æä¾›æ— æç¤ºçš„å¤šè·³é—®é¢˜ï¼Œå¹¶é…å¤‡ä¸€ä¸ªå—æ§çš„ç»´åŸºç™¾ç§‘æ²™ç®±ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¡Œä¸ºçš„å¯è¿½æº¯æ€§ã€‚é€šè¿‡å¯¹25ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨çŸ¥è¯†åˆ©ç”¨æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ï¼Œå°½ç®¡æœ‰è¶³å¤Ÿçš„è¯æ®ï¼Œä½†åœ¨ç¼ºä¹è¯æ®æ—¶å‡ ä¹æ²¡æœ‰é€‚å½“çš„æ‹’ç»è¡Œä¸ºã€‚æˆ‘ä»¬å¼€å‘äº†EvidenceLoopå·¥ä½œæµç¨‹ï¼Œä¸“é—¨é’ˆå¯¹åŸºå‡†è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œæ”¹è¿›äº†æœç´¢å’Œç»¼åˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06219', 'title': 'Human3R: Everyone Everywhere All at Once', 'url': 'https://huggingface.co/papers/2510.06219', 'abstract': 'Human3R is a unified, feed-forward framework for real-time 4D human-scene reconstruction from monocular videos, achieving state-of-the-art performance with a single model and minimal dependencies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies ("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a single forward pass ("all-at-once"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R\'s rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R', 'score': 3, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'aa48bcc34ddfe0e6', 'authors': ['Yue Chen', 'Xingyu Chen', 'Yuxuan Xue', 'Anpei Chen', 'Yuliang Xiu', 'Gerard Pons-Moll'], 'affiliations': ['Max Planck Institute for Informatics', 'Uni of Tubingen, Tubingen AI Center', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06219.jpg', 'data': {'categories': ['#cv', '#video', '#3d'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ°Ñ 4D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ ÑÑ†ĞµĞ½ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Human3R â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 4D ÑÑ†ĞµĞ½ Ñ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸Ğ· Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Human3R Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ»ÑĞ´ĞµĞ¹ Ğ¸Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ», ÑÑ†ĞµĞ½ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµÑ‘ Ğ¾Ñ‡ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹. Human3R Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Real-Time 4D Human-Scene Reconstruction Made Simple', 'desc': 'Human3R is a novel framework designed for real-time 4D reconstruction of humans and scenes from single-camera videos. It simplifies the reconstruction process by using a single feed-forward model, avoiding the need for complex multi-stage pipelines and heavy dependencies like human detection or depth estimation. The model efficiently reconstructs multiple human bodies and dense 3D scenes simultaneously, achieving high performance with minimal computational resources. With its ability to operate at real-time speeds and low memory usage, Human3R sets a new standard for efficient human-scene reconstruction in machine learning.'}, 'zh': {'title': 'Human3Rï¼šå®æ—¶4Däººç±»åœºæ™¯é‡å»ºçš„ç»Ÿä¸€æ¡†æ¶', 'desc': 'Human3Ræ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å‰é¦ˆæ¡†æ¶ï¼Œèƒ½å¤Ÿå®æ—¶ä»å•ç›®è§†é¢‘ä¸­é‡å»º4Däººç±»åœºæ™¯ã€‚ä¸ä»¥å¾€ä¾èµ–å¤šé˜¶æ®µæµç¨‹å’Œé‡è¿­ç²¾ç»†åŒ–çš„æ–¹æ³•ä¸åŒï¼ŒHuman3Rå¯ä»¥åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­åŒæ—¶æ¢å¤å¤šä¸ªäººä½“æ¨¡å‹ã€å¯†é›†çš„3Dåœºæ™¯å’Œç›¸æœºè½¨è¿¹ã€‚è¯¥æ–¹æ³•åŸºäºCUT3Ræ¨¡å‹ï¼Œé‡‡ç”¨é«˜æ•ˆçš„è§†è§‰æç¤ºè°ƒä¼˜ï¼Œä¿æŒäº†ä¸°å¯Œçš„æ—¶ç©ºå…ˆéªŒï¼ŒåŒæ—¶å®ç°äº†å¤šä¸ªäººä½“æ¨¡å‹çš„ç›´æ¥è¾“å‡ºã€‚ç»è¿‡ä¸€å¤©çš„è®­ç»ƒï¼ŒHuman3Råœ¨å°è§„æ¨¡åˆæˆæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿä»¥å®æ—¶é€Ÿåº¦é‡å»ºå¤šä¸ªè§’è‰²å’Œåœºæ™¯ï¼Œå…·æœ‰ä½å†…å­˜å ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06218', 'title': 'EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark', 'url': 'https://huggingface.co/papers/2510.06218', 'abstract': 'EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.', 'score': 3, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'a863fc0993d7f2f6', 'authors': ['Deheng Zhang', 'Yuqian Fu', 'Runyi Yang', 'Yang Miao', 'Tianwen Qian', 'Xu Zheng', 'Guolei Sun', 'Ajad Chhatkuli', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Luc Van Gool', 'Danda Pani Paudel'], 'affiliations': ['East China Normal University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06218.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#benchmark', '#games', '#transfer_learning', '#synthetic', '#cv'], 'emoji': 'ğŸŒ™', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI-Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞ¼Ğ½Ğ¾Ñ‚Ğµ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°', 'desc': 'EgoNight â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ egocentric-Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° visual question answering. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3658 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° 90 Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ² Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… multimodal LLM Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ğº Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ VQA, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ-Ğ½Ğ¾Ñ‡ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Nighttime Vision for AI', 'desc': 'EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field.'}, 'zh': {'title': 'å¤œé—´è§†è§‰é—®ç­”çš„æ–°åŸºå‡†ï¼šEgoNight', 'desc': 'EgoNightæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤œé—´è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ï¼Œç‰¹åˆ«æ˜¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚ç°æœ‰çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç™½å¤©åœºæ™¯ï¼Œå¿½è§†äº†ä½å…‰ç…§æ¡ä»¶ä¸‹çš„åº”ç”¨éœ€æ±‚ã€‚EgoNighté€šè¿‡å¼•å…¥æ—¥å¤œå¯¹é½çš„è§†é¢‘ï¼Œæå‡äº†å¤œé—´æ ‡æ³¨çš„è´¨é‡ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ã€‚è¯¥åŸºå‡†åŒ…å«3658ä¸ªé—®ç­”å¯¹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05156', 'title': 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation', 'url': 'https://huggingface.co/papers/2510.05156', 'abstract': "VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", 'score': 3, 'issue_id': 6300, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'cfcbeb2c67a2faa9', 'authors': ['Lesly Miculicich', 'Mihir Parmar', 'Hamid Palangi', 'Krishnamurthy Dj Dvijotham', 'Mirko Montanari', 'Tomas Pfister', 'Long T. Le'], 'affiliations': ['Google Cloud AI', 'Google Cloud AI Research', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.05156.jpg', 'data': {'categories': ['#inference', '#alignment', '#agents', '#security', '#healthcare'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VeriGuard â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ ĞµĞ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ, Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ adversarial-Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Ensuring Safety in AI Agents with VeriGuard', 'desc': "VeriGuard is a framework designed to ensure the safety of large language model (LLM)-based agents by providing formal guarantees through a two-stage process. The first stage involves offline validation, where user intent is clarified to create specific safety specifications, and a behavioral policy is synthesized and rigorously tested for compliance. The second stage focuses on online monitoring, where the agent's actions are continuously validated against the pre-verified policy before they are executed. This approach enhances the reliability of AI agents in sensitive areas by ensuring they adhere to safety constraints and reducing risks associated with their deployment."}, 'zh': {'title': 'VeriGuardï¼šç¡®ä¿æ™ºèƒ½ä½“å®‰å…¨çš„åŒé˜¶æ®µæ¡†æ¶', 'desc': 'VeriGuardæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“æä¾›æ­£å¼çš„å®‰å…¨ä¿éšœã€‚å®ƒé€šè¿‡ç¦»çº¿éªŒè¯å’Œåœ¨çº¿ç›‘æ§çš„åŒé˜¶æ®µæ¶æ„ï¼Œç¡®ä¿æ™ºèƒ½ä½“çš„è¡Œä¸ºç¬¦åˆé¢„å®šä¹‰çš„å®‰å…¨çº¦æŸã€‚é¦–å…ˆï¼Œåœ¨ç¦»çº¿é˜¶æ®µï¼ŒVeriGuardé€šè¿‡æ˜ç¡®ç”¨æˆ·æ„å›¾æ¥å»ºç«‹å®‰å…¨è§„èŒƒï¼Œå¹¶åˆæˆè¡Œä¸ºç­–ç•¥ï¼Œç»è¿‡æµ‹è¯•å’Œæ­£å¼éªŒè¯ä»¥ç¡®ä¿åˆè§„ã€‚ç„¶åï¼Œåœ¨åœ¨çº¿é˜¶æ®µï¼ŒVeriGuardä½œä¸ºè¿è¡Œæ—¶ç›‘æ§å™¨ï¼ŒéªŒè¯æ¯ä¸ªæè®®çš„æ™ºèƒ½ä½“åŠ¨ä½œï¼Œä»¥ç¡®ä¿å…¶ç¬¦åˆé¢„å…ˆéªŒè¯çš„æ”¿ç­–ï¼Œä»è€Œæé«˜LLMæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05122', 'title': 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support\n  Conversation', 'url': 'https://huggingface.co/papers/2510.05122', 'abstract': 'CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.', 'score': 3, 'issue_id': 6301, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'a08f3bb063148c1c', 'authors': ['Jie Zhu', 'Yuanchen Zhou', 'Shuo Jiang', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05122.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning'], 'emoji': 'ğŸ¤—', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CARE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, CARE Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¼Ğ¿Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ….'}, 'en': {'title': 'Enhancing Emotional Support with Cognitive Reasoning', 'desc': 'CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.'}, 'zh': {'title': 'CAREï¼šæå‡æƒ…æ„Ÿæ”¯æŒå¯¹è¯çš„è®¤çŸ¥æ¨ç†èƒ½åŠ›', 'desc': 'CAREæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºæƒ…æ„Ÿæ”¯æŒå¯¹è¯ä¸­çš„è®¤çŸ¥æ¨ç†ï¼Œæå‡å“åº”è´¨é‡å’ŒåŒç†å¿ƒï¼Œè€Œä¸ä¾èµ–äºå¤§è§„æ¨¡çš„åˆæˆæ•°æ®ã€‚æƒ…æ„Ÿæ”¯æŒå¯¹è¯åœ¨ç¼“è§£å¿ƒç†å‹åŠ›å’Œæä¾›æƒ…æ„Ÿä»·å€¼æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°æ®å¢å¼ºå’Œåˆæˆè¯­æ–™åº“çš„æ„å»ºä¸Šï¼Œå¿½è§†äº†æœ‰æ•ˆæƒ…æ„Ÿæ”¯æŒèƒŒåçš„æ·±å±‚è®¤çŸ¥æ¨ç†è¿‡ç¨‹ã€‚CAREåˆ©ç”¨åŸå§‹çš„æƒ…æ„Ÿæ”¯æŒå¯¹è¯è®­ç»ƒé›†ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆé€»è¾‘è¿è´¯å’Œæ”¯æŒæ€§çš„å“åº”ï¼Œä»è€Œæ˜¾è‘—æå‡è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06107', 'title': 'Distributional Semantics Tracing: A Framework for Explaining\n  Hallucinations in Large Language Models', 'url': 'https://huggingface.co/papers/2510.06107', 'abstract': "A framework called Distributional Semantics Tracing identifies the layers and pathways in Transformers where hallucinations occur, revealing a correlation between internal semantic coherence and hallucination rates.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary contributions.First, to enable the reliable tracing of internal semantic failures, we propose Distributional Semantics Tracing (DST), a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific commitment layer where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic associative pathway (akin to System 1) and a slow, deliberate contextual pathway (akin to System 2), leading to predictable failure modes such as Reasoning Shortcut Hijacks. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation (rho = -0.863) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.", 'score': 2, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'b743e310dc0ab5e3', 'authors': ['Gagan Bhatia', 'Somayajulu G Sripada', 'Kevin Allan', 'Jacobo Azcona'], 'affiliations': ['University of Aberdeen'], 'pdf_title_img': 'assets/pdf/title_img/2510.06107.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#hallucinations', '#inference', '#interpretability'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ°Ñ€Ñ‚Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: ĞºĞ°Ğº Ğ¸ Ğ³Ğ´Ğµ LLM Ñ‚ĞµÑ€ÑÑÑ‚ ÑĞ²ÑĞ·ÑŒ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Distributional Semantics Tracing (DST), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ´Ğ¸Ñ‚ÑŒ, Ğ½Ğ° ĞºĞ°ĞºĞ¸Ñ… Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ÑÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Â«ÑĞ»Ğ¾Ğ¹ Ñ„Ğ¸ĞºÑĞ°Ñ†Ğ¸Ğ¸Â», Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ¶Ğµ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒÑÑ Ğº Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ. ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ ĞºÑ€Ğ¾ĞµÑ‚ÑÑ Ğ² ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ğµ Ğ´Ğ²ÑƒÑ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¹: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ (ĞºĞ°Ğº Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° 1 Ğ² Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸) Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ (Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° 2). ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ÑĞ¸Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ĞµĞ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ÑÑ‚Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Tracing Hallucinations in Transformers: Understanding the Roots of AI Errors', 'desc': "This paper introduces a framework called Distributional Semantics Tracing (DST) to analyze where and why hallucinations occur in Transformer models. It combines various interpretability techniques to create a causal map of a model's reasoning, focusing on how meaning is influenced by context. The authors identify a specific layer in the model where hallucinations become unavoidable and explain the underlying mechanisms using dual-process theory, highlighting the conflict between fast and slow reasoning pathways. Their findings show a strong negative correlation between the coherence of the contextual pathway and hallucination rates, suggesting that these errors stem from weaknesses in the model's internal semantics."}, 'zh': {'title': 'æ­ç¤ºå˜æ¢å™¨æ¨¡å‹ä¸­çš„å¹»è§‰æœºåˆ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåˆ†å¸ƒè¯­ä¹‰è¿½è¸ªï¼ˆDSTï¼‰çš„æ¡†æ¶ï¼Œç”¨äºè¯†åˆ«å˜æ¢å™¨æ¨¡å‹ä¸­å¹»è§‰å‘ç”Ÿçš„å±‚æ¬¡å’Œè·¯å¾„ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹çš„å†…éƒ¨è¯­ä¹‰ä¸€è‡´æ€§ä¸å¹»è§‰å‘ç”Ÿç‡ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è´Ÿç›¸å…³å…³ç³»ã€‚é€šè¿‡åˆ†ææ¨¡å‹çš„è®¡ç®—è·¯å¾„ï¼Œä½œè€…æ­ç¤ºäº†å¹»è§‰çš„æ ¹æœ¬æœºåˆ¶ï¼ŒæŒ‡å‡ºäº†å¿«é€Ÿå¯å‘å¼è·¯å¾„ä¸æ…¢é€Ÿä¸Šä¸‹æ–‡è·¯å¾„ä¹‹é—´çš„å†²çªã€‚æœ€ç»ˆï¼Œæœ¬æ–‡ä¸ºç†è§£å˜æ¢å™¨æ¶æ„ä¸­å¹»è§‰çš„å‘ç”Ÿæä¾›äº†æœºåˆ¶æ€§è§£é‡Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06030', 'title': 'Adaptive Pruning for Increased Robustness and Reduced Computational\n  Overhead in Gaussian Process Accelerated Saddle Point Searches', 'url': 'https://huggingface.co/papers/2510.06030', 'abstract': 'Geometry-aware optimal transport and active pruning enhance Gaussian process regression for efficient saddle point searches on high-dimensional energy surfaces.  \t\t\t\t\tAI-generated summary \t\t\t\t Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.', 'score': 2, 'issue_id': 6312, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'dd655c94092fb8a5', 'authors': ['Rohit Goswami', 'Hannes JÃ³nsson'], 'affiliations': ['Science Institute & Faculty of Physical Sciences University of Iceland, 107 ReykjavÃ­k, Iceland', 'University of Iceland, 107 ReykjavÃ­k, Iceland'], 'pdf_title_img': 'assets/pdf/title_img/2510.06030.jpg', 'data': {'categories': ['#optimization', '#data', '#science', '#training'], 'emoji': 'ğŸ”ï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞµĞ´Ğ»Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»Ñ‘Ğ½Ğ½Ğ¾Ğ¹ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° ÑĞµĞ´Ğ»Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ°ÑƒÑÑĞ¾Ğ²ÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°ÑÑ‚ÑƒÑ‰Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚ Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ÑĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ’Ğ°ÑÑĞµÑ€ÑˆÑ‚ĞµĞ¹Ğ½Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ´Ğ¸ÑƒÑĞ° Ğ´Ğ¾Ğ²ĞµÑ€Ğ¸Ñ. ĞĞ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 238 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ» ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ²Ğ´Ğ²Ğ¾Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¸ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ…Ğ¸Ğ¼Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Gaussian Processes for Efficient Energy Surface Searches', 'desc': 'This paper presents enhancements to Gaussian process (GP) regression for more efficient saddle point searches in high-dimensional energy landscapes. It introduces geometry-aware optimal transport and an active pruning strategy to manage the computational costs associated with hyperparameter optimization and GP updates. By employing Wasserstein-1 distances, the method selects a diverse set of configurations, ensuring better representation of the energy surface. The proposed improvements significantly reduce computational time, making GP regression a more robust tool for complex energy evaluations in chemical reactions.'}, 'zh': {'title': 'å‡ ä½•æ„ŸçŸ¥ä¸ä¸»åŠ¨ä¿®å‰ªæå‡é«˜æ–¯è¿‡ç¨‹å›å½’æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„é«˜æ–¯è¿‡ç¨‹å›å½’æ–¹æ³•ï¼Œç”¨äºé«˜ç»´èƒ½é‡è¡¨é¢çš„éç‚¹æœç´¢ã€‚é€šè¿‡å‡ ä½•æ„ŸçŸ¥çš„æœ€ä¼˜ä¼ è¾“å’Œä¸»åŠ¨ä¿®å‰ªç­–ç•¥ï¼Œå‡å°‘äº†èƒ½é‡åŠå…¶å¯¼æ•°çš„è®¡ç®—æ¬¡æ•°ï¼Œä»è€Œæé«˜äº†æ•ˆç‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨Wasserstein-1è·ç¦»è¿›è¡Œè¿œthestç‚¹é‡‡æ ·ï¼Œé€‰æ‹©å‡ ä½•å¤šæ ·æ€§çš„é…ç½®å­é›†ï¼Œé¿å…äº†è®¡ç®—æˆæœ¬çš„å¿«é€Ÿå¢åŠ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨238ä¸ªå¤æ‚é…ç½®ä¸Šå°†å¹³å‡è®¡ç®—æ—¶é—´å‡å°‘äº†ä¸€åŠï¼Œè¯æ˜äº†å…¶åœ¨éç‚¹æœç´¢ä¸­çš„æœ‰æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05396', 'title': 'Scalable In-context Ranking with Generative Models', 'url': 'https://huggingface.co/papers/2510.05396', 'abstract': "BlockRank optimizes in-context ranking by enforcing inter-document block sparsity and enhancing query-document relevance, improving efficiency and scalability in large-scale information retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.", 'score': 2, 'issue_id': 6315, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '55ca388238eda44f', 'authors': ['Nilesh Gupta', 'Chong You', 'Srinadh Bhojanapalli', 'Sanjiv Kumar', 'Inderjit Dhillon', 'Felix Yu'], 'affiliations': ['Google', 'Google DeepMind', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2510.05396.jpg', 'data': {'categories': ['#benchmark', '#training', '#long_context', '#data', '#architecture', '#optimization'], 'emoji': 'ğŸ“š', 'ru': {'title': 'BlockRank: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ LLM', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° BlockRank Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. BlockRank Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ LLM, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ¼ĞµĞ¶Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°-Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ Ğ·Ğ° ÑÑ‡Ñ‘Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BlockRank Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Efficient In-Context Ranking with BlockRank', 'desc': 'BlockRank is a new method designed to improve in-context ranking (ICR) in information retrieval by focusing on the structure of attention in large language models (LLMs). It identifies that attention is dense within document blocks but sparse across different documents, allowing for a more efficient attention mechanism. By enforcing inter-document block sparsity, BlockRank reduces the complexity of attention operations from quadratic to linear, maintaining performance while enhancing efficiency. Additionally, it optimizes the relevance of query-document pairs during training, leading to better retrieval results and scalability for large candidate lists.'}, 'zh': {'title': 'BlockRankï¼šæå‡ä¿¡æ¯æ£€ç´¢æ•ˆç‡çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBlockRankçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸Šä¸‹æ–‡æ’åï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ã€‚é€šè¿‡å¼ºåˆ¶æ–‡æ¡£é—´å—ç¨€ç–æ€§å’Œå¢å¼ºæŸ¥è¯¢-æ–‡æ¡£ç›¸å…³æ€§ï¼ŒBlockRankæé«˜äº†å¤§è§„æ¨¡ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨æ–‡æ¡£å—å†…æ˜¯å¯†é›†çš„ï¼Œè€Œåœ¨ä¸åŒæ–‡æ¡£ä¹‹é—´æ˜¯ç¨€ç–çš„ï¼Œè¿™ä¸€ç‰¹æ€§è¢«æœ‰æ•ˆåˆ©ç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBlockRankåœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03978', 'title': 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2510.03978', 'abstract': 'Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.', 'score': 2, 'issue_id': 6301, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 4', 'zh': '10æœˆ4æ—¥'}, 'hash': 'bb2f9d23dbe6e4fa', 'authors': ['Min Woo Sun', 'Alejandro Lozano', 'Javier Gamazo Tejero', 'Vishwesh Nath', 'Xiao Xiao Sun', 'James Burgess', 'Yuhui Zhang', 'Kun Yuan', 'Robert Tibshirani', 'Sean Huver', 'Serena Yeung-Levy'], 'affiliations': ['NVIDIA, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.03978.jpg', 'data': {'categories': ['#data', '#benchmark', '#long_context', '#healthcare', '#dataset', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ»Ğ¾Ğ² â€” Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (Ğ´Ğ¾ 77 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²) Ñ‚ĞµÑ€ÑÑÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ BIOMEDICA-LongCAP Ğ¸Ğ· 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BMC-LongCLIP, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾ 512 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ² 6.6 Ñ€Ğ°Ğ· Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ° Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ 55% Ğ´Ğ¾ 2.2%, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 30% Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 2%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ.'}, 'en': {'title': 'Unlocking the Power of Long Contexts in Biomedical Captioning', 'desc': 'This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.'}, 'zh': {'title': 'æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œæå‡ç”Ÿç‰©åŒ»å­¦æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ‰©å±•æ–‡æœ¬ç¼–ç å™¨çš„ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç”Ÿç‰©åŒ»å­¦æè¿°ä»»åŠ¡çš„å½±å“ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨è¾ƒçŸ­çš„æ–‡æœ¬çª—å£ï¼Œè¿™å¯¼è‡´é•¿æ ¼å¼çš„æè¿°è¢«æˆªæ–­ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡å¯ä»¥æé«˜æ£€ç´¢å’Œåˆ†ç±»çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹åˆ©ç”¨æ›´è¯¦ç»†çš„æè¿°ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†BIOMEDICA-LongCAPï¼Œå¹¶è®­ç»ƒäº†æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹BMC-LongCLIPï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02341', 'title': 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning', 'url': 'https://huggingface.co/papers/2510.02341', 'abstract': 'DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.', 'score': 2, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '1bde94710320cd61', 'authors': ['Yifan Wang', 'Bolian Li', 'Junlin Wu', 'Zhaoxuan Tan', 'Zheli Liu', 'Ruqi Zhang', 'Ananth Grama', 'Qingkai Zeng'], 'affiliations': ['College of Computer Science, Nankai University', 'Department of Computer Science and Engineering, University of Notre Dame', 'Department of Computer Science, Purdue University', 'Department of Computer Science, Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2510.02341.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ½Ğ° Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DRIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€ĞµĞ´ĞºĞ¾. DRIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° ĞºĞ°Ğº ÑĞºĞ¾Ñ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WildFeedback Ğ¸ UltraFeedback Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +12.29% Ğ¿Ğ¾ win rate Ğ½Ğ° AlpacaEval2, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº DPO Ğ¸ SPIN, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Harnessing User Dissatisfaction for Better Language Models', 'desc': 'The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs.'}, 'zh': {'title': 'åˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'DRIFTæ˜¯ä¸€ç§åŸºäºç”¨æˆ·ä¸æ»¡ä¿¡å·çš„è¿­ä»£åå¥½è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çœŸå®ä¸–ç•Œä¸­çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·ï¼ŒåŠ¨æ€é‡‡æ ·æ­£åé¦ˆï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç”¨æˆ·çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DRIFTè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚DRIFTä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜ä¿æŒäº†æ¢ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„é«˜å¥–åŠ±è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06213', 'title': 'Training Dynamics Impact Post-Training Quantization Robustness', 'url': 'https://huggingface.co/papers/2510.06213', 'abstract': 'Quantization robustness in large language models is influenced by learning rate and other hyperparameters, not dataset scale, as demonstrated through controlled training experiments.  \t\t\t\t\tAI-generated summary \t\t\t\t While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.', 'score': 1, 'issue_id': 6307, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '301cc349b005a081', 'authors': ['Albert Catalan-Tatjer', 'NiccolÃ² Ajroldi', 'Jonas Geiping'], 'affiliations': ['ELLIS Institute Tubingen', 'Max Planck Institute for Intelligent Systems', 'Tubingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2510.06213.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸, ĞºĞ°Ğº post-training ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ¾ 32B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° 15T Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ learning rate Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ° Ğ½Ğµ Ğ¾Ñ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°. ĞŸĞ¾ÑĞ»Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ learning rate Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‚ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ğ±Ğ¾Ñ€ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Hyperparameters Over Dataset Size: Key to Quantization Robustness', 'desc': 'This paper explores how the robustness of quantization in large language models is affected by training hyperparameters, particularly the learning rate, rather than the size of the dataset. Through controlled experiments, the authors analyze the relationship between training dynamics and quantization performance across various model sizes and training token counts. They find that as learning rates decrease, the validation loss and quantization error begin to diverge, indicating that these factors are largely independent of the amount of training data. The study suggests that by strategically adjusting hyperparameters, it is possible to enhance quantization quality, challenging the notion that larger datasets automatically lead to worse quantization outcomes.'}, 'zh': {'title': 'é‡åŒ–é²æ£’æ€§ï¼šè¶…å‚æ•°çš„åŠ›é‡', 'desc': 'åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œé‡åŒ–é²æ£’æ€§å—åˆ°å­¦ä¹ ç‡å’Œå…¶ä»–è¶…å‚æ•°çš„å½±å“ï¼Œè€Œä¸æ•°æ®é›†è§„æ¨¡æ— å…³ã€‚é€šè¿‡æ§åˆ¶è®­ç»ƒå®éªŒï¼Œæˆ‘ä»¬åˆ†æäº†é‡åŒ–æ€§èƒ½ä¸è®­ç»ƒåŠ¨æ€ä¹‹é—´çš„å…³ç³»ã€‚ç ”ç©¶å‘ç°ï¼Œå­¦ä¹ ç‡çš„è¡°å‡ä¼šå¯¼è‡´éªŒè¯æŸå¤±å’Œé‡åŒ–è¯¯å·®çš„åˆ†æ­§ï¼Œè¿™ä¸€ç°è±¡ä¸è®­ç»ƒæ•°æ®è§„æ¨¡åŸºæœ¬æ— å…³ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œé€šè¿‡è°ƒæ•´è®­ç»ƒè¶…å‚æ•°ï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­æ”¹å–„é‡åŒ–è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06139', 'title': 'Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.06139', 'abstract': "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.", 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '27bd729b0bb095f1', 'authors': ['Zanyi Wang', 'Dengyang Jiang', 'Liuzhuozheng Li', 'Sizhe Dang', 'Chengzu Li', 'Harry Yang', 'Guang Dai', 'Mengmeng Wang', 'Jingdong Wang'], 'affiliations': ['Baidu', 'SGIT AI Lab, State Grid Corporation of China', 'The Hong Kong University of Science and Technology', 'The University of Tokyo', 'University of California, San Diego', 'University of Cambridge', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.06139.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#games', '#alignment'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'FlowRVS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RVOS), Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞµÑ‘ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Â«Ğ½Ğ°Ğ¹Ñ‚Ğ¸-Ğ·Ğ°Ñ‚ĞµĞ¼-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ», Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ pretrained text-to-video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… RVOS, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MeViS Ğ¸ Ref-DAVIS17.'}, 'en': {'title': 'Revolutionizing Video Segmentation with Continuous Flow', 'desc': 'FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding.'}, 'zh': {'title': 'FlowRVSï¼šè§†é¢‘ç‰©ä½“åˆ†å‰²çš„æ–°æ€è·¯', 'desc': 'FlowRVSæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è§†é¢‘ç‰©ä½“åˆ†å‰²ä¸­çš„å¼•ç”¨é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªè¿ç»­æµåŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼Œå®ç°äº†å¯¹è§†é¢‘ä¸­ç›®æ ‡çš„ç²¾ç»†åƒç´ æ§åˆ¶å’Œè¯­ä¹‰å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„â€œå®šä½-å†åˆ†å‰²â€æµç¨‹ä¸åŒï¼ŒFlowRVSé€šè¿‡ç›´æ¥å­¦ä¹ è¯­è¨€å¼•å¯¼çš„å˜å½¢ï¼Œä»è§†é¢‘çš„æ•´ä½“è¡¨ç¤ºåˆ°ç›®æ ‡æ©è†œï¼Œä¿æŒäº†æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨ä¸»è¦çš„å¼•ç”¨è§†é¢‘ç‰©ä½“åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå±•ç¤ºäº†å°†è§†é¢‘ç†è§£ä»»åŠ¡å»ºæ¨¡ä¸ºè¿ç»­å˜å½¢è¿‡ç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06071', 'title': 'Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI\n  Models for Scatterplot-Related Tasks', 'url': 'https://huggingface.co/papers/2510.06071', 'abstract': "A benchmark for scatterplot-specific tasks using synthetic datasets evaluates AI models' performance in counting clusters and identifying outliers, with mixed results for localization tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.", 'score': 1, 'issue_id': 6308, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'e1656709eb874610', 'authors': ['JoÃ£o Palmeiro', 'Diogo Duarte', 'Rita Costa', 'Pedro Bizarro'], 'affiliations': ['Feedzai'], 'pdf_title_img': 'assets/pdf/title_img/2510.06071.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#dataset', '#optimization'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ½Ğ¾ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ÑÑ‚ Ğ³Ğ´Ğµ Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ñ€Ğ°ÑÑĞµÑĞ½Ğ¸Ñ (scatterplots), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 18000 Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ OpenAI Ğ¸ Gemini 2.5 Flash Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (Ğ±Ğ¾Ğ»ĞµĞµ 90%) Ğ² Ğ¿Ğ¾Ğ´ÑÑ‡Ñ‘Ñ‚Ğµ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ N-shot prompting. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ÑÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ° ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 50% Ğ¸Ğ»Ğ¸ Ğ½Ğ¸Ğ¶Ğµ, Ğ·Ğ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Flash Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (65%). Ğ”Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‚Ğ¾Ñ€Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ğ½Ğ¾ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ñ… ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¹ Ñ€Ğ°ÑĞºÑ€Ğ°ÑĞºĞ¸.'}, 'en': {'title': 'Benchmarking AI for Scatterplot Analysis', 'desc': 'This paper presents a benchmark for evaluating AI models on scatterplot-specific tasks, focusing on counting clusters and identifying outliers. It introduces a synthetic dataset of over 18,000 scatterplots created from various data generators and chart designs. The study assesses the performance of models from OpenAI and Google using N-shot prompting across five tasks related to cluster and outlier detection. While some models show high accuracy in counting clusters and identifying outliers, their performance in localization tasks is generally poor, highlighting the need for improved methods in this area.'}, 'zh': {'title': 'æ•£ç‚¹å›¾ä»»åŠ¡çš„AIæ€§èƒ½è¯„ä¼°åŸºå‡†', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹æ•£ç‚¹å›¾ç‰¹å®šä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†æ¥è¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è®¡æ•°èšç±»å’Œè¯†åˆ«å¼‚å¸¸å€¼æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡18,000ä¸ªæ•£ç‚¹å›¾çš„åˆæˆæ³¨é‡Šæ•°æ®é›†ï¼Œå¹¶åŸºäºæ­¤è¿›è¡Œè¯„ä¼°ã€‚é€šè¿‡å¯¹OpenAIå’ŒGoogleçš„ä¸“æœ‰æ¨¡å‹è¿›è¡ŒN-shotæç¤ºï¼Œç»“æœæ˜¾ç¤ºè¿™äº›æ¨¡å‹åœ¨è®¡æ•°èšç±»å’Œå¼‚å¸¸å€¼è¯†åˆ«æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å®šä½ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œå›¾è¡¨è®¾è®¡å¯¹æ€§èƒ½çš„å½±å“è¾ƒå°ï¼Œä½†å»ºè®®é¿å…ä½¿ç”¨å®½çºµæ¨ªæ¯”çš„æ•£ç‚¹å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06056', 'title': 'Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research', 'url': 'https://huggingface.co/papers/2510.06056', 'abstract': 'DeepEvolve integrates deep research with algorithm evolution to propose, refine, implement, and test new hypotheses, improving initial algorithms across various scientific domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.', 'score': 1, 'issue_id': 6311, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'a4df9d07d5dc32b9', 'authors': ['Gang Liu', 'Yihan Zhu', 'Jie Chen', 'Meng Jiang'], 'affiliations': ['MIT-IBM Watson AI Lab, IBM Research', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2510.06056.jpg', 'data': {'categories': ['#science', '#benchmark', '#agents', '#optimization', '#data', '#math'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ', 'desc': 'DeepEvolve â€” ÑÑ‚Ğ¾ AI-Ğ°Ğ³ĞµĞ½Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸ĞµĞ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² (ĞºĞ°Ğº Ğ² AlphaEvolve), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ LLM Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ°Ñ‚Ğ¾, DeepEvolve Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞº Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ´Ğ° Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºÑƒ Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ. ĞšĞ°Ğ¶Ğ´Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹, Ğ½Ğ¾ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚, Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ…, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ ĞºĞ°Ğº Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº. Ğ’ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ… Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸Ğ· Ñ…Ğ¸Ğ¼Ğ¸Ğ¸, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ² DeepEvolve ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Bridging Research and Evolution for Better Algorithms', 'desc': 'DeepEvolve is a novel approach that combines deep research with algorithm evolution to enhance the development of scientific algorithms. Unlike existing methods that either focus solely on evolving algorithms or conducting research without practical validation, DeepEvolve integrates both strategies. It utilizes a feedback-driven iterative loop to propose, refine, implement, and test new hypotheses, ensuring that improvements are both meaningful and executable. This method has shown consistent success across various scientific fields, leading to significant advancements in algorithm performance.'}, 'zh': {'title': 'DeepEvolveï¼šç§‘å­¦ç®—æ³•å‘ç°çš„æ–°æ¡†æ¶', 'desc': 'DeepEvolve æ˜¯ä¸€ç§å°†æ·±åº¦ç ”ç©¶ä¸ç®—æ³•è¿›åŒ–ç›¸ç»“åˆçš„æ™ºèƒ½ä½“ï¼Œæ—¨åœ¨æå‡ºã€å®Œå–„ã€å®æ–½å’Œæµ‹è¯•æ–°çš„å‡è®¾ï¼Œä»è€Œåœ¨å¤šä¸ªç§‘å­¦é¢†åŸŸä¸­æ”¹è¿›åˆå§‹ç®—æ³•ã€‚ä¸ä»…ä¾èµ–ç®—æ³•è¿›åŒ–æˆ–å•ç‹¬è¿›è¡Œæ·±åº¦ç ”ç©¶çš„ç°æœ‰æ™ºèƒ½ä½“ä¸åŒï¼ŒDeepEvolve é€šè¿‡åé¦ˆé©±åŠ¨çš„è¿­ä»£å¾ªç¯ï¼Œæ•´åˆäº†å¤–éƒ¨çŸ¥è¯†æ£€ç´¢ã€è·¨æ–‡ä»¶ä»£ç ç¼–è¾‘å’Œç³»ç»Ÿè°ƒè¯•ã€‚æ¯æ¬¡è¿­ä»£ä¸ä»…æå‡ºæ–°çš„å‡è®¾ï¼Œè¿˜å¯¹å…¶è¿›è¡Œå®Œå–„ã€å®æ–½å’Œæµ‹è¯•ï¼Œé¿å…äº†è¡¨é¢æ”¹è¿›å’Œæ— æ•ˆçš„è¿‡åº¦å®Œå–„ã€‚é€šè¿‡åœ¨åŒ–å­¦ã€æ•°å­¦ã€ç”Ÿç‰©å­¦ã€ææ–™å’Œä¸“åˆ©ç­‰ä¹ä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeepEvolve ä¸€ç›´åœ¨æŒç»­æ”¹è¿›åˆå§‹ç®—æ³•ï¼Œç”Ÿæˆå¯æ‰§è¡Œçš„æ–°ç®—æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05681', 'title': 'Verifier-free Test-Time Sampling for Vision Language Action Models', 'url': 'https://huggingface.co/papers/2510.05681', 'abstract': "MG-Select, a novel test-time scaling framework for Vision-Language-Action models, improves performance by using KL divergence from a reference distribution generated with masked inputs, achieving significant gains in both in-distribution and out-of-distribution tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.", 'score': 1, 'issue_id': 6309, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'd398ec0b5549236c', 'authors': ['Suhyeok Jang', 'Dongyoung Kim', 'Changyeon Kim', 'Youngsuk Kim', 'Jinwoo Shin'], 'affiliations': ['KAIST', 'RLWRLD', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05681.jpg', 'data': {'categories': ['#cv', '#training', '#robotics', '#agents', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ MG-Select â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Vision-Language-Action Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 28-35% Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ 168% Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing VLA Performance with MG-Select Framework', 'desc': "MG-Select is a new framework designed to enhance the performance of Vision-Language-Action (VLA) models during test time. It uses KL divergence to compare the model's action choices against a reference distribution created from masked inputs, allowing for better decision-making without needing extra training. This method helps the model select the best action from several options by measuring confidence based on internal properties. The results show that MG-Select significantly boosts performance in both familiar and unfamiliar tasks, making it a valuable advancement in robot control applications."}, 'zh': {'title': 'MG-Selectï¼šæå‡è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'MG-Selectæ˜¯ä¸€ç§æ–°é¢–çš„æµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶ï¼Œä¸“ä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡ä½¿ç”¨ä»æ©è”½è¾“å…¥ç”Ÿæˆçš„å‚è€ƒåˆ†å¸ƒçš„KLæ•£åº¦æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–ä»»åŠ¡çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæˆ–å¤–éƒ¨æ¨¡å—ï¼Œåˆ©ç”¨æ¨¡å‹çš„å†…éƒ¨ç‰¹æ€§æ¥é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMG-Selectåœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨RoboCasaçš„æ‹¾å–å’Œæ”¾ç½®ä»»åŠ¡ä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04087', 'title': 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling', 'url': 'https://huggingface.co/papers/2510.04087', 'abstract': 'A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}, 'hash': '77a7f937000c3b18', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.04087.jpg', 'data': {'categories': ['#inference', '#training', '#data', '#rlhf', '#alignment'], 'emoji': 'ğŸš¦', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Â«Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¾Ğ¿Ñ†Ğ¸ÑÂ» Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ reward models ÑƒĞ¼ĞµÑÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ°ĞºĞ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¼. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ½Ğ¾ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ inference Ğ½Ğ° 22%.'}, 'en': {'title': 'Enhancing Preference Alignment with Outside Options', 'desc': 'This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%.'}, 'zh': {'title': 'æå‡åå¥½å¯¹é½çš„å¯é æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨åå¥½æ•°æ®æ”¶é›†å’Œå»ºæ¨¡ä¸­å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œæå‡äº†åå¥½å¯¹é½æŠ€æœ¯çš„å¯é æ€§å’Œæ•ˆç‡ã€‚ç°ä»£çš„åå¥½å¯¹é½æŠ€æœ¯ï¼Œå¦‚æœ€ä½³Nï¼ˆBoNï¼‰é‡‡æ ·ï¼Œä¾èµ–äºé€šè¿‡æˆå¯¹æ¯”è¾ƒæ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œè™½ç„¶èƒ½æœ‰æ•ˆå­¦ä¹ ç›¸å¯¹åå¥½ï¼Œä½†æœªèƒ½æ•æ‰å“åº”å¯æ¥å—æ€§çš„ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œè®­ç»ƒå‡ºèƒ½å¤ŸåŒºåˆ†ä¸ä»…æ˜¯æ›´å¥½è€Œæ˜¯è¶³å¤Ÿå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œè§£å†³äº†å¯é æ€§ç¼ºå£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹é½å’Œæ¨ç†åŠ é€Ÿæ–¹é¢å‡æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ä¸ªçµæ´»çš„ç®¡ç†å¯é æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´æƒè¡¡çš„å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21499', 'title': 'On Code-Induced Reasoning in LLMs', 'url': 'https://huggingface.co/papers/2509.21499', 'abstract': 'Systematic investigation reveals that large language models are more sensitive to structural than semantic code perturbations, with implications for training data design.  \t\t\t\t\tAI-generated summary \t\t\t\t Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.', 'score': 1, 'issue_id': 6311, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '4194d2df93542366', 'authors': ['Abdul Waheed', 'Zhen Wu', 'Carolyn RosÃ©', 'Daphne Ippolito'], 'affiliations': ['Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21499.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#data', '#optimization', '#training', '#plp'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ĞºĞ¾Ğ´Ğ° Ğ²Ğ°Ğ¶Ğ½ĞµĞµ ÑĞ¼Ñ‹ÑĞ»Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€Ğ¾Ğ²ĞµĞ´Ñ Ğ±Ğ¾Ğ»ĞµĞµ 3000 ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ LLM Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹ Ğº Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸ÑĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ĞºĞ¾Ğ´Ğ°, Ñ‡ĞµĞ¼ Ğº ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¿ÑĞµĞ²Ğ´Ğ¾ĞºĞ¾Ğ´ Ğ¸ Ğ±Ğ»Ğ¾Ğº-ÑÑ…ĞµĞ¼Ñ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ»ÑŒ Ğ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ°Ğº Ğ¸ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´, Ğ° Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾Ğ²Ñ€ĞµĞ¶Ğ´Ñ‘Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ñ Ğ½ĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ¾ÑÑ‚Ğ°Ñ‘Ñ‚ÑÑ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ·Ñ‹ĞºĞ° Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ: Python Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Java Ğ¸ Rust â€” Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Understanding LLM Sensitivity: Structure Over Semantics in Code', 'desc': 'This paper investigates how large language models (LLMs) respond to changes in code structure versus changes in code meaning. The authors create datasets in ten programming languages and apply specific alterations to either the structural or semantic aspects of the code. Their findings reveal that LLMs are more affected by structural changes, especially in tasks related to math and code. The study suggests that using simplified representations like pseudocode can maintain or even enhance model performance, indicating the importance of training data design for improving LLM reasoning.'}, 'zh': {'title': 'ç»“æ„ä¼˜å…ˆï¼šå¤§å‹è¯­è¨€æ¨¡å‹å¯¹ä»£ç æ‰°åŠ¨çš„æ•æ„Ÿæ€§', 'desc': 'æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯¹ä»£ç ç»“æ„å’Œè¯­ä¹‰æ‰°åŠ¨çš„æ•æ„Ÿæ€§ã€‚ç»“æœè¡¨æ˜ï¼ŒLLMså¯¹ç»“æ„æ€§æ‰°åŠ¨çš„è„†å¼±æ€§é«˜äºè¯­ä¹‰æ€§æ‰°åŠ¨ï¼Œå°¤å…¶æ˜¯åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€‚å½“çš„æŠ½è±¡å½¢å¼å¦‚ä¼ªä»£ç å’Œæµç¨‹å›¾å¯ä»¥æœ‰æ•ˆæ›¿ä»£ä»£ç ï¼ŒåŒæ—¶ç”¨æ›´å°‘çš„æ ‡è®°ç¼–ç ç›¸åŒä¿¡æ¯ï¼Œç”šè‡³å¯èƒ½æé«˜æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸åŒçš„è¯­æ³•é£æ ¼å¯¹ä»»åŠ¡ç‰¹å®šçš„è¡¨ç°ä¹Ÿæœ‰å½±å“ï¼ŒPythonæ›´æœ‰åˆ©äºè‡ªç„¶è¯­è¨€æ¨ç†ï¼Œè€ŒJavaå’ŒRustç­‰ä½çº§è¯­è¨€åˆ™æ›´é€‚åˆæ•°å­¦ä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06101', 'title': 'The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2510.06101', 'abstract': 'Research on distilling coding skills from large language models to smaller ones reveals a "valley of code reasoning" where performance initially decreases with more data before improving sharply, and that small models benefit more from easier questions during distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a valley of code reasoning: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition', 'score': 0, 'issue_id': 6310, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'ca03a603a5277862', 'authors': ['Muyu He', 'Muhammad Ali Shafique', 'Anand Kumar', 'Tsach Mackey', 'Nazneen Rajani'], 'affiliations': ['Collinear AI', 'DeepSeek', 'OpenCodeReasoning (OCR)', 'OpenThoughts', 'QwQ Team', 'rStar-Coder'], 'pdf_title_img': 'assets/pdf/title_img/2510.06101.jpg', 'data': {'categories': ['#transfer_learning', '#reasoning', '#small_models', '#training', '#optimization'], 'emoji': 'ğŸ”ï¸', 'ru': {'title': 'Ğ”Ğ¾Ğ»Ğ¸Ğ½Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: ĞºĞ°Ğº Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¸Ğ·ÑƒÑ‡Ğ°Ğ»Ğ¸, ĞºĞ°Ğº Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ. ĞĞ½Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚ Â«Ğ´Ğ¾Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´ĞµÂ»: Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ñ€ĞµĞ·ĞºĞ¾ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°ĞµÑ‚. ĞĞºĞ°Ğ·Ğ°Ğ»Ğ¾ÑÑŒ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡ĞµĞ¼ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ…, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ…. Ğ£Ğ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ½Ğ¾ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Navigating the Valley of Code Reasoning in Model Distillation', 'desc': "This paper explores how to transfer coding skills from large language models (LLMs) to smaller models through a process called distillation. It identifies a phenomenon termed the 'valley of code reasoning,' where the performance of smaller models initially declines as more training data is introduced, before improving significantly. The research shows that smaller models perform better when trained on simpler coding questions rather than more complex ones during the distillation process. Additionally, it reveals that the accuracy of the training data does not impact the effectiveness of the distillation, providing new insights into the training dynamics of code reasoning."}, 'zh': {'title': 'æ­ç¤ºç¼–ç æ¨ç†çš„è°·åº•ä¸æå‡ä¹‹è·¯', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†ä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼ç¼–ç æŠ€èƒ½åˆ°å°å‹æ¨¡å‹çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬å‘ç°å­˜åœ¨ä¸€ä¸ªâ€œç¼–ç æ¨ç†è°·â€ï¼Œå³åœ¨æ•°æ®é‡å¢åŠ æ—¶ï¼Œæ¨¡å‹æ€§èƒ½æœ€åˆä¼šä¸‹é™ï¼Œéšååˆä¼šè¿…é€Ÿæå‡ã€‚å°å‹æ¨¡å‹åœ¨æç‚¼è¿‡ç¨‹ä¸­æ›´å®¹æ˜“ä»ç®€å•é—®é¢˜ä¸­è·ç›Šï¼Œè€Œä¸æ˜¯å¤æ‚é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºç†è§£ç¼–ç æ¨ç†æç‚¼çš„è®­ç»ƒåŠ¨æ€æä¾›äº†æ–°çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05934', 'title': 'Revisiting Modeling and Evaluation Approaches in Speech Emotion\n  Recognition: Considering Subjectivity of Annotators and Ambiguity of Emotions', 'url': 'https://huggingface.co/papers/2510.05934', 'abstract': "Embracing minority ratings, multiple annotators, and multi-emotion predictions in speech emotion recognition improves system robustness and alignment with human perception.  \t\t\t\t\tAI-generated summary \t\t\t\t Over the past two decades, speech emotion recognition (SER) has received growing attention. To train SER systems, researchers collect emotional speech databases annotated by crowdsourced or in-house raters who select emotions from predefined categories. However, disagreements among raters are common. Conventional methods treat these disagreements as noise, aggregating labels into a single consensus target. While this simplifies SER as a single-label task, it ignores the inherent subjectivity of human emotion perception. This dissertation challenges such assumptions and asks: (1) Should minority emotional ratings be discarded? (2) Should SER systems learn from only a few individuals' perceptions? (3) Should SER systems predict only one emotion per sample?   Psychological studies show that emotion perception is subjective and ambiguous, with overlapping emotional boundaries. We propose new modeling and evaluation perspectives: (1) Retain all emotional ratings and represent them with soft-label distributions. Models trained on individual annotator ratings and jointly optimized with standard SER systems improve performance on consensus-labeled tests. (2) Redefine SER evaluation by including all emotional data and allowing co-occurring emotions (e.g., sad and angry). We propose an ``all-inclusive rule'' that aggregates all ratings to maximize diversity in label representation. Experiments on four English emotion databases show superior performance over majority and plurality labeling. (3) Construct a penalization matrix to discourage unlikely emotion combinations during training. Integrating it into loss functions further improves performance. Overall, embracing minority ratings, multiple annotators, and multi-emotion predictions yields more robust and human-aligned SER systems.", 'score': 0, 'issue_id': 6304, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '18b4f51d0ffd5cdd', 'authors': ['Huang-Cheng Chou', 'Chi-Chun Lee'], 'affiliations': ['ACII 2017 (åœ‹éš›æƒ…æ„Ÿè¨ˆç®—æœƒè­°)', 'APSIPA (Transactions on Signal and Information Processing)', 'Google (æ±äºå­¸ç”Ÿæ—…è¡Œçå‹µ)', 'IEEE (Transactions on Affective Computing)', 'ISCA (Student Advisory Committee)', 'ä¸­è¯æ‰¶è¼ª (æ‰¶è¼ªçå­¸é‡‘)', 'å‚‘å‡ºäººæ‰ç™¼å±•åŸºé‡‘æœƒ', 'åœ‹å®¶ç§‘å­¸æŠ€è¡“å§”å“¡æœƒ', 'åœ‹ç«‹æ¸…è¯å¤§å­¸ (æ ¡é•·åšå£«ç”Ÿå“è¶Šçå­¸é‡‘)', 'åœ‹ç«‹æ¸…è¯å¤§å­¸ é›»æ©Ÿå·¥ç¨‹å­¸ç³»', 'åœ‹éš›å£èªæºé€šå­¸æœƒ (INTERSPEECH 2022)', 'è¯è© ç§‘æŠ€ (è¯è© åšå£«çå­¸é‡‘)', 'è¨ˆç®—èªè¨€å­¸èˆ‡ä¸­æ–‡èªè¨€è™•ç†å­¸æœƒ'], 'pdf_title_img': 'assets/pdf/title_img/2510.05934.jpg', 'data': {'categories': ['#audio', '#alignment', '#interpretability'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡ÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾: ÑƒÑ‡Ñ‘Ñ‚ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ° Ğ² Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµÑ‡Ğ¸', 'desc': 'Ğ”Ğ¸ÑÑĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸ (SER) Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ³Ğ´Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ»Ğ°ÑĞ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ÑÑ‚ÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ°, Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ soft-label Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€Ñ‘Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒÑ‡Ñ‘Ñ‚ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´ĞµĞ»Ğ°ÑÑ‚ SER-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Enhancing Speech Emotion Recognition through Diversity in Emotion Representation', 'desc': 'This paper discusses improvements in speech emotion recognition (SER) by incorporating minority ratings, multiple annotators, and multi-emotion predictions. It argues that traditional methods, which simplify emotional data into single labels, overlook the complexity of human emotion perception. By using soft-label distributions and allowing for co-occurring emotions, the proposed approach enhances model performance and aligns better with human understanding of emotions. The findings demonstrate that embracing diverse emotional inputs leads to more robust SER systems.'}, 'zh': {'title': 'æ‹¥æŠ±å¤šæ ·æ€§ï¼Œæå‡è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„é²æ£’æ€§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸­çš„æƒ…æ„Ÿæ ‡æ³¨é—®é¢˜ï¼Œå¼ºè°ƒäº†å°‘æ•°æ ‡æ³¨å’Œå¤šæƒ…æ„Ÿé¢„æµ‹çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•å°†æ ‡æ³¨è€…ä¹‹é—´çš„åˆ†æ­§è§†ä¸ºå™ªå£°ï¼Œç®€å•åœ°èšåˆæˆå•ä¸€æ ‡ç­¾ï¼Œå¿½è§†äº†æƒ…æ„Ÿæ„ŸçŸ¥çš„ä¸»è§‚æ€§ã€‚è®ºæ–‡æå‡ºä¿ç•™æ‰€æœ‰æƒ…æ„Ÿè¯„åˆ†ï¼Œå¹¶ä½¿ç”¨è½¯æ ‡ç­¾åˆ†å¸ƒæ¥è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥æƒ©ç½šçŸ©é˜µå’Œå¤šæƒ…æ„Ÿé¢„æµ‹ï¼Œç ”ç©¶è¡¨æ˜è¿™ç§æ–¹æ³•èƒ½æ›´å¥½åœ°ä¸äººç±»æƒ…æ„Ÿæ„ŸçŸ¥å¯¹é½ï¼Œå¢å¼ºç³»ç»Ÿçš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04514', 'title': 'ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in\n  Complex Chart Question Answering', 'url': 'https://huggingface.co/papers/2510.04514', 'abstract': "ChartAgent, a novel agentic framework, performs visual reasoning directly within charts, achieving state-of-the-art accuracy on ChartBench and ChartX benchmarks by iteratively decomposing queries and using specialized visual actions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents.", 'score': 0, 'issue_id': 6313, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'b607bdb92d04b3d9', 'authors': ['Rachneet Kaur', 'Nishan Srishankar', 'Zhen Zeng', 'Sumitra Ganesh', 'Manuela Veloso'], 'affiliations': ['J.P. Morgan AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.04514.jpg', 'data': {'categories': ['#reasoning', '#agents', '#multimodal', '#interpretability', '#benchmark', '#cv'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹', 'desc': 'ChartAgent â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² chain-of-thought, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹: Ñ€Ğ¸ÑÑƒĞµÑ‚ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸, Ğ²Ñ‹Ñ€ĞµĞ·Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ Ğ¾ÑĞ¸. ChartAgent Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… ChartBench Ğ¸ ChartX, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 16-17% Ğ½Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ°Ñ… Ğ±ĞµĞ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº plug-and-play Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'ChartAgent: Mastering Visual Reasoning in Charts', 'desc': 'ChartAgent is a new framework designed for visual reasoning in charts, which enhances the ability to answer questions about chart data. It works by breaking down complex queries into smaller visual tasks and using specific actions to interact with the chart, such as drawing or cropping. This method allows ChartAgent to achieve high accuracy on benchmarks like ChartBench and ChartX, outperforming previous models significantly. The framework is versatile, effective across different types of charts, and can improve the performance of various underlying language models.'}, 'zh': {'title': 'ChartAgentï¼šå›¾è¡¨ç†è§£çš„æ–°æ™ºèƒ½æ¡†æ¶', 'desc': 'ChartAgentæ˜¯ä¸€ç§æ–°é¢–çš„æ™ºèƒ½æ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥åœ¨å›¾è¡¨ä¸­è¿›è¡Œè§†è§‰æ¨ç†ã€‚å®ƒé€šè¿‡å°†æŸ¥è¯¢è¿­ä»£åˆ†è§£ä¸ºè§†è§‰å­ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨ä¸“é—¨çš„è§†è§‰æ“ä½œæ¥ä¸å›¾è¡¨å›¾åƒè¿›è¡Œäº¤äº’ï¼Œä»è€Œå®ç°äº†åœ¨ChartBenchå’ŒChartXåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„å‡†ç¡®ç‡ã€‚ä¸ä¼ ç»Ÿçš„æ–‡æœ¬æ¨ç†ä¸åŒï¼ŒChartAgenté‡‡ç”¨äº†æ›´æ¥è¿‘äººç±»è®¤çŸ¥ç­–ç•¥çš„æ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§ç±»å‹çš„å›¾è¡¨ã€‚è¯¥æ¡†æ¶ä¸ä»…åœ¨æœªæ³¨é‡Šçš„å›¾è¡¨ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜èƒ½æå‡å¤šç§åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.00880', 'title': 'HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate\n  Hallucinations in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2510.00880', 'abstract': 'HalluGuard, a 4B-parameter Small Reasoning Model, effectively mitigates hallucinations in Retrieval-Augmented Generation by classifying document-claim pairs and providing evidence-grounded justifications, achieving high balanced accuracy on the LLM-AggreFact benchmark.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies document-claim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) a domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill large-model reasoning into a smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance.', 'score': 0, 'issue_id': 6304, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': '436314721ab72bf1', 'authors': ['Loris Bergeron', 'Ioana Buhnila', 'JÃ©rÃ´me FranÃ§ois', 'Radu State'], 'affiliations': ['ATILF, University of LorraineCNRS', 'Banque de Luxembourg', 'Center for Data Science in Humanities, Chosun University', 'SnT, University of Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2510.00880.jpg', 'data': {'categories': ['#training', '#hallucinations', '#dataset', '#rag', '#small_models', '#synthetic', '#benchmark', '#optimization'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'HalluGuard: Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'HalluGuard â€” ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Retrieval-Augmented Generation. ĞĞ½Ğ° ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ, ĞºĞ°ĞºĞ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ñ„Ğ°ĞºÑ‚Ğ°Ñ…, Ğ° ĞºĞ°ĞºĞ¸Ğµ â€” Ğ½Ğ° Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. HalluGuard Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğ¾ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'HalluGuard: Reducing Hallucinations with Small Reasoning Power', 'desc': 'HalluGuard is a Small Reasoning Model with 4 billion parameters designed to reduce hallucinations in Retrieval-Augmented Generation tasks. It works by classifying document-claim pairs as either grounded or hallucinated and provides justifications based on evidence to enhance transparency. The model is trained on a synthetic dataset that has been carefully curated and refined, allowing it to perform effectively with fewer parameters than larger models. HalluGuard achieves competitive accuracy on the LLM-AggreFact benchmark, demonstrating its capability to rival larger models while maintaining efficiency.'}, 'zh': {'title': 'HalluGuardï¼šå‡å°‘å¹»è§‰ï¼Œæé«˜ç”Ÿæˆé€æ˜åº¦', 'desc': 'HalluGuardæ˜¯ä¸€ç§å…·æœ‰40äº¿å‚æ•°çš„å°å‹æ¨ç†æ¨¡å‹ï¼Œæ—¨åœ¨å‡å°‘æ£€ç´¢å¢å¼ºç”Ÿæˆä¸­çš„å¹»è§‰ç°è±¡ã€‚å®ƒé€šè¿‡å¯¹æ–‡æ¡£-å£°æ˜å¯¹è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸ºçœŸå®ä¾æ®ï¼Œå¹¶æä¾›åŸºäºè¯æ®çš„è§£é‡Šï¼Œä»è€Œæé«˜é€æ˜åº¦ã€‚è¯¥æ¨¡å‹åœ¨LLM-AggreFactåŸºå‡†æµ‹è¯•çš„RAGTruthå­é›†ä¸Šè¾¾åˆ°äº†84.0%çš„å¹³è¡¡å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¸ä¸€äº›ä¸“é—¨æ¨¡å‹ç›¸å½“ã€‚HalluGuardç»“åˆäº†åˆæˆæ•°æ®é›†å’Œåå¥½ä¼˜åŒ–çš„å¾®è°ƒæ–¹æ³•ï¼ŒæˆåŠŸå°†å¤§å‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æç‚¼åˆ°æ›´å°çš„æ¨¡å‹ä¸­ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (11)', '#agi (1)', '#alignment (7)', '#architecture (8)', '#audio (2)', '#benchmark (18)', '#cv (5)', '#data (11)', '#dataset (8)', '#diffusion (7)', '#ethics', '#games (5)', '#graphs', '#hallucinations (3)', '#healthcare (4)', '#inference (8)', '#interpretability (7)', '#leakage (1)', '#long_context (4)', '#low_resource', '#machine_translation', '#math (3)', '#multilingual', '#multimodal (11)', '#open_source (4)', '#optimization (25)', '#plp (1)', '#rag (2)', '#reasoning (18)', '#rl (8)', '#rlhf (5)', '#robotics (1)', '#science (5)', '#security (1)', '#small_models (4)', '#story_generation (1)', '#survey', '#synthetic (3)', '#training (24)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-08 23:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-08 23:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-08 23:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    