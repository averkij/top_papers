
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. July 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 июля</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-21.html">⬅️ <span id="prev-date">21.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-23.html">➡️ <span id="next-date">23.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'};
        let feedDateNext = {'ru': '23.07', 'en': '07/23', 'zh': '7月23日'};
        let feedDatePrev = {'ru': '21.07', 'en': '07/21', 'zh': '7月21日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.15846', 'title': 'GUI-G^2: Gaussian Reward Modeling for GUI Grounding', 'url': 'https://huggingface.co/papers/2507.15846', 'abstract': 'Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current reinforcement learning approaches use binary rewards that treat elements as hit-or-miss targets, creating sparse signals that ignore the continuous nature of spatial interactions. Motivated by human clicking behavior that naturally forms Gaussian distributions centered on target elements, we introduce GUI Gaussian Grounding Rewards (GUI-G^2), a principled reward framework that models GUI elements as continuous Gaussian distributions across the interface plane. GUI-G^2 incorporates two synergistic mechanisms: Gaussian point rewards model precise localization through exponentially decaying distributions centered on element centroids, while coverage rewards assess spatial alignment by measuring the overlap between predicted Gaussian distributions and target regions. To handle diverse element scales, we develop an adaptive variance mechanism that calibrates reward distributions based on element dimensions. This framework transforms GUI grounding from sparse binary classification to dense continuous optimization, where Gaussian distributions generate rich gradient signals that guide models toward optimal interaction positions. Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro benchmarks demonstrate that GUI-G^2, substantially outperforms state-of-the-art method UI-TARS-72B, with the most significant improvement of 24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides superior robustness to interface variations and enhanced generalization to unseen layouts, establishing a new paradigm for spatial reasoning in GUI interaction tasks.', 'score': 96, 'issue_id': 4936, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'd36bacfa3f66add9', 'authors': ['Fei Tang', 'Zhangxuan Gu', 'Zhengxi Lu', 'Xuyang Liu', 'Shuheng Shen', 'Changhua Meng', 'Wen Wang', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Ant Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15846.jpg', 'data': {'categories': ['#agents', '#rl', '#optimization', '#reasoning', '#benchmark'], 'emoji': '🖱️', 'ru': {'title': 'Гауссово моделирование для точного взаимодействия с GUI', 'desc': 'Статья представляет новый подход к обучению моделей машинного обучения для взаимодействия с графическим пользовательским интерфейсом (GUI). Авторы предлагают метод GUI Gaussian Grounding Rewards (GUI-G^2), который моделирует элементы интерфейса как непрерывные гауссовы распределения на плоскости интерфейса. Этот метод включает в себя гауссовы точечные награды для точной локализации и награды за покрытие для оценки пространственного выравнивания. Эксперименты показывают, что GUI-G^2 значительно превосходит современные методы на нескольких бенчмарках, демонстрируя улучшение до 24.7% на ScreenSpot-Pro.'}, 'en': {'title': 'Revolutionizing GUI Interaction with Continuous Gaussian Rewards', 'desc': 'This paper presents a new method called GUI Gaussian Grounding Rewards (GUI-G^2) for improving how machines interact with graphical user interfaces (GUIs) using natural language instructions. Unlike traditional reinforcement learning methods that use simple binary rewards, GUI-G^2 models GUI elements as continuous Gaussian distributions, allowing for more nuanced and effective learning. The framework includes mechanisms for precise localization and spatial alignment, which help the model understand where to click based on human-like behavior. Experiments show that GUI-G^2 significantly outperforms existing methods, demonstrating better adaptability to different interface designs and improved overall performance in GUI tasks.'}, 'zh': {'title': '高斯奖励框架提升GUI交互精度', 'desc': '本论文提出了一种新的奖励框架，称为GUI Gaussian Grounding Rewards（GUI-G^2），用于将自然语言指令映射到图形用户界面（GUI）的精确位置。与传统的二元奖励方法不同，GUI-G^2通过将GUI元素建模为连续的高斯分布，提供了更丰富的梯度信号，促进了模型的优化。该框架结合了高斯点奖励和覆盖奖励，能够更好地处理不同元素的尺度，并提高了模型在界面变化中的鲁棒性。实验结果表明，GUI-G^2在多个基准测试中显著优于现有的最先进方法，展示了其在GUI交互任务中的新范式。'}}}, {'id': 'https://huggingface.co/papers/2507.14683', 'title': 'MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via\n  Context-Aware Multi-Stage Policy Optimization', 'url': 'https://huggingface.co/papers/2507.14683', 'abstract': 'The MiroMind-M1 series of open-source reasoning language models achieves state-of-the-art performance on mathematical reasoning benchmarks through a two-stage training process and Context-Aware Multi-Stage Policy Optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have recently evolved from fluent text generation to advanced reasoning across diverse domains, giving rise to reasoning language models. Among these domains, mathematical reasoning serves as a representative benchmark as it requires precise multi-step logic and abstract reasoning, which can be generalized to other tasks. While closed-source RLMs such as GPT-o3 demonstrate impressive reasoning capabilities, their proprietary nature limits transparency and reproducibility. Although many open-source projects aim to close this gap, most of them lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. To contribute toward greater transparency in RLM development, we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone that match or exceed the performance of existing open-source RLMs. Specifically, our models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. To enhance the robustness and efficiency of the RLVR process, we introduce Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty to encourage context-aware RL training. Our model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B, MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K, MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope these resources will support further research and foster community advancement.', 'score': 86, 'issue_id': 4937, 'pub_date': '2025-07-19', 'pub_date_card': {'ru': '19 июля', 'en': 'July 19', 'zh': '7月19日'}, 'hash': '47799c3d5002f685', 'authors': ['Xingxuan Li', 'Yao Xiao', 'Dianwen Ng', 'Hai Ye', 'Yue Deng', 'Xiang Lin', 'Bin Wang', 'Zhanfeng Mo', 'Chong Zhang', 'Yueyi Zhang', 'Zonglin Yang', 'Ruilin Li', 'Lei Lei', 'Shihao Xu', 'Han Zhao', 'Weiling Chen', 'Feng Ji', 'Lidong Bing'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.14683.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#open_source', '#math', '#reasoning'], 'emoji': '🧮', 'ru': {'title': 'Открытые модели для математических рассуждений на новом уровне', 'desc': 'MiroMind-M1 - это серия открытых языковых моделей для математических рассуждений, достигающих передовых результатов на соответствующих бенчмарках. Модели обучаются в два этапа: сначала на корпусе из 719 тысяч математических задач с верифицированными решениями, затем с помощью обучения с подкреплением на 62 тысячах сложных задач. Авторы представляют новый алгоритм Context-Aware Multi-Stage Policy Optimization для повышения эффективности обучения с подкреплением. Все ресурсы, включая модели, датасеты и конфигурации, открыто опубликованы для воспроизводимости результатов.'}, 'en': {'title': 'Open-Source Models for Superior Mathematical Reasoning', 'desc': "The MiroMind-M1 series introduces open-source reasoning language models that excel in mathematical reasoning tasks through a two-stage training approach. The first stage involves supervised fine-tuning (SFT) on a large dataset of math problems, while the second stage employs reinforcement learning with verified responses (RLVR) to refine the model's reasoning capabilities. To improve training efficiency, the authors propose a novel Context-Aware Multi-Stage Policy Optimization algorithm that adapts training based on context and problem complexity. By providing complete access to models, datasets, and training configurations, this work aims to enhance transparency and reproducibility in the development of reasoning language models."}, 'zh': {'title': '开源推理模型的透明性与先进性', 'desc': 'MiroMind-M1系列是一个开源推理语言模型，通过两阶段训练过程和上下文感知多阶段策略优化，在数学推理基准测试中取得了最先进的表现。这些模型首先在经过精心挑选的719K数学推理问题上进行监督微调，然后在62K具有挑战性的问题上进行强化学习验证。为了提高强化学习验证过程的鲁棒性和效率，提出了一种新的算法，结合了长度渐进训练和自适应重复惩罚。我们希望通过发布完整的模型、数据集和训练配置，促进研究的可重复性和社区的进步。'}}}, {'id': 'https://huggingface.co/papers/2507.14843', 'title': 'The Invisible Leash: Why RLVR May Not Escape Its Origin', 'url': 'https://huggingface.co/papers/2507.14843', 'abstract': "Theoretical and empirical analysis reveals that Reinforcement Learning with Verifiable Rewards (RLVR) enhances precision but narrows exploration, limiting its ability to discover novel solutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large reasoning models highlight Reinforcement Learning with Verifiable Rewards (RLVR) as a promising method for enhancing AI's capabilities, particularly in solving complex logical tasks. However, it remains unclear whether RLVR truly expands a model's reasoning boundary or merely amplifies high-reward outputs that the base model already knows for improved precision. This study presents a theoretical and empirical investigation that provides fresh insights into the potential limits of RLVR. First, we offer a new theoretical perspective that RLVR is constrained by the base model's support-unable to sample solutions with zero initial probability-and operates as a conservative reweighting mechanism that may restrict the discovery of entirely original solutions. We also identify an entropy-reward tradeoff: while RLVR reliably enhances precision, it may progressively narrow exploration and potentially overlook correct yet underrepresented solutions. Extensive empirical experiments validate that while RLVR consistently improves pass@1, the shrinkage of empirical support generally outweighs the expansion of empirical support under larger sampling budgets, failing to recover correct answers that were previously accessible to the base model. Interestingly, we also observe that while RLVR sometimes increases token-level entropy, resulting in greater uncertainty at each generation step, answer-level entropy declines, indicating that these seemingly more uncertain paths ultimately converge onto a smaller set of distinct answers. Taken together, these findings reveal potential limits of RLVR in extending reasoning horizons. Breaking this invisible leash may require future algorithmic innovations such as explicit exploration mechanisms or hybrid strategies that seed probability mass into underrepresented solution regions.", 'score': 53, 'issue_id': 4940, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'bb8fd850ce625ea5', 'authors': ['Fang Wu', 'Weihao Xuan', 'Ximing Lu', 'Zaid Harchaoui', 'Yejin Choi'], 'affiliations': ['RIKEN AIP', 'Stanford University', 'University of Tokyo', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.14843.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#optimization', '#rl'], 'emoji': '🔍', 'ru': {'title': 'RLVR: повышение точности ценой ограничения исследования', 'desc': 'Исследование анализирует метод обучения с подкреплением с проверяемыми вознаграждениями (RLVR) в контексте решения сложных логических задач. Авторы обнаружили, что RLVR повышает точность модели, но ограничивает ее способность находить новые решения. Теоретический анализ показывает, что RLVR ограничен возможностями базовой модели и действует как механизм консервативного перевзвешивания. Эмпирические эксперименты подтверждают, что RLVR улучшает показатель pass@1, но сужает область исследования, потенциально упуская правильные, но недопредставленные решения.'}, 'en': {'title': 'Balancing Precision and Exploration in RLVR', 'desc': "This paper investigates Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on AI's problem-solving abilities. It finds that while RLVR improves precision in generating high-reward outputs, it limits exploration, which can hinder the discovery of novel solutions. The study introduces a theoretical framework showing that RLVR acts as a conservative mechanism, unable to sample solutions with zero initial probability. Empirical results indicate that although RLVR enhances performance metrics like pass@1, it often reduces the diversity of solutions, suggesting a need for new strategies to encourage exploration."}, 'zh': {'title': '强化学习与可验证奖励的探索限制', 'desc': '强化学习与可验证奖励（RLVR）在提高精度方面表现出色，但却限制了探索能力，可能导致无法发现新颖的解决方案。研究表明，RLVR的效果受到基础模型的支持限制，无法采样初始概率为零的解决方案。虽然RLVR在提高精度方面表现稳定，但其对探索的压缩可能会忽视一些正确但代表性不足的解决方案。未来的算法创新可能需要引入显式探索机制或混合策略，以便在未被充分代表的解决方案区域中注入概率质量。'}}}, {'id': 'https://huggingface.co/papers/2507.14119', 'title': 'NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining', 'url': 'https://huggingface.co/papers/2507.14119', 'abstract': 'An automated pipeline mines high-fidelity image editing triplets using generative models and a task-tuned validator, enabling large-scale training without human labeling.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments.', 'score': 30, 'issue_id': 4944, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '4c0e1974ad169d32', 'authors': ['Maksim Kuprashevich', 'Grigorii Alekseenko', 'Irina Tolstykh', 'Georgii Fedorov', 'Bulat Suleimanov', 'Vladimir Dokholyan', 'Aleksandr Gordeev'], 'affiliations': ['Layer Team, SALUTEDEV'], 'pdf_title_img': 'assets/pdf/title_img/2507.14119.jpg', 'data': {'categories': ['#dataset', '#cv', '#diffusion', '#open_source', '#training', '#data'], 'emoji': '🖼️', 'ru': {'title': 'Автоматизированное создание данных для ИИ-редактирования изображений', 'desc': 'Статья представляет автоматизированный конвейер для создания высококачественных триплетов для обучения моделей редактирования изображений. Система использует генеративные модели и настроенный валидатор Gemini для оценки соответствия инструкциям и эстетики без участия человека. Предложенный подход позволяет создавать обучающие данные в большом масштабе без ручной разметки. Авторы выпускают открытый набор данных NHR-Edit из 358 тысяч триплетов и модель Bagel-NHR-Edit, достигающую современного уровня производительности.'}, 'en': {'title': 'Automating Image Editing: High-Fidelity Triplet Generation Without Human Input', 'desc': 'This paper presents an automated pipeline that generates high-quality image editing triplets using generative models, which consist of an original image, an instruction, and the edited image. The system employs a task-tuned validator to ensure that the edits adhere to the specified instructions while maintaining visual appeal and coherence. By automating the mining process, the pipeline significantly increases the volume of training data available for image editing models without requiring human labeling. The authors also introduce the NHR-Edit dataset, containing 358,000 high-quality triplets, and a fine-tuned model that achieves state-of-the-art performance in image editing tasks.'}, 'zh': {'title': '自动化挖掘高保真图像编辑三元组的创新方法', 'desc': '这篇论文介绍了一种自动化的管道，用于挖掘高保真图像编辑三元组，利用生成模型和任务调优的验证器，实现大规模训练而无需人工标注。该系统能够在不同领域、分辨率和风格中自动生成原始图像、指令和编辑图像的三元组，解决了传统方法中对编辑质量评估的不足。通过使用任务调优的验证器，系统直接评分指令遵循性和美学，省去了分割或基础模型的需求。论文还发布了NHR-Edit数据集，包含358k个高质量三元组，推动了这一资源密集型领域的研究。'}}}, {'id': 'https://huggingface.co/papers/2507.15061', 'title': 'WebShaper: Agentically Data Synthesizing via Information-Seeking\n  Formalization', 'url': 'https://huggingface.co/papers/2507.15061', 'abstract': 'A formalization-driven framework called WebShaper synthesizes information-seeking datasets using set theory and Knowledge Projections, enhancing the performance of LLM-powered agents on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks.', 'score': 29, 'issue_id': 4936, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': '16ab84cfe7ace89e', 'authors': ['Zhengwei Tao', 'Jialong Wu', 'Wenbiao Yin', 'Junkai Zhang', 'Baixuan Li', 'Haiyang Shen', 'Kuan Li', 'Liwen Zhang', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.15061.jpg', 'data': {'categories': ['#agents', '#dataset', '#synthetic', '#reasoning', '#benchmark'], 'emoji': '🕸️', 'ru': {'title': 'Формализация для синтеза данных: новый подход к обучению ИИ-агентов поиску информации', 'desc': 'WebShaper - это фреймворк для синтеза наборов данных для задач поиска информации, основанный на формализации с использованием теории множеств и Проекций Знаний. Он позволяет улучшить производительность агентов на основе больших языковых моделей (LLM) в открытых задачах. WebShaper систематически формализует задачи поиска информации и использует многоэтапный процесс расширения для создания сложных вопросов. Эксперименты показывают, что WebShaper достигает наилучших результатов среди открытых агентов поиска информации на бенчмарках GAIA и WebWalkerQA.'}, 'en': {'title': 'Enhancing LLM Agents with Structured Data Synthesis', 'desc': 'WebShaper is a framework designed to improve information-seeking datasets for Large Language Model (LLM)-powered agents. It uses set theory and a method called Knowledge Projections to create a structured approach for synthesizing data. This helps ensure that the reasoning behind questions and answers is consistent and logical. Experiments show that WebShaper significantly enhances the performance of these agents on various benchmarks.'}, 'zh': {'title': 'WebShaper：提升信息检索智能体性能的创新框架', 'desc': 'WebShaper是一个基于形式化驱动的框架，利用集合论和知识投影技术合成信息检索数据集，从而提升大型语言模型（LLM）驱动的智能体在开放式任务中的表现。该框架通过系统化的形式化过程，确保信息结构与推理结构的一致性，解决了现有方法中常见的数据不一致问题。WebShaper的核心是知识投影（KP）概念，通过KP操作组合实现对推理结构的精确控制。实验结果表明，WebShaper在GAIA和WebWalkerQA基准测试中，达到了开源信息检索智能体的最先进性能。'}}}, {'id': 'https://huggingface.co/papers/2507.11061', 'title': 'Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with\n  Regularized Score Distillation Sampling', 'url': 'https://huggingface.co/papers/2507.11061', 'abstract': 'A novel framework, RoMaP, improves precise local 3D editing through robust 3D mask generation and enhanced SDS loss regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D neural representations and instance-level editing models have enabled the efficient creation of high-quality 3D content. However, achieving precise local 3D edits remains challenging, especially for Gaussian Splatting, due to inconsistent multi-view 2D part segmentations and inherently ambiguous nature of Score Distillation Sampling (SDS) loss. To address these limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that enables precise and drastic part-level modifications. First, we introduce a robust 3D mask generation module with our 3D-Geometry Aware Label Prediction (3D-GALP), which uses spherical harmonics (SH) coefficients to model view-dependent label variations and soft-label property, yielding accurate and consistent part segmentations across viewpoints. Second, we propose a regularized SDS loss that combines the standard SDS loss with additional regularizers. In particular, an L1 anchor loss is introduced via our Scheduled Latent Mixing and Part (SLaMP) editing method, which generates high-quality part-edited 2D images and confines modifications only to the target region while preserving contextual coherence. Additional regularizers, such as Gaussian prior removal, further improve flexibility by allowing changes beyond the existing context, and robust 3D masking prevents unintended edits. Experimental results demonstrate that our RoMaP achieves state-of-the-art local 3D editing on both reconstructed and generated Gaussian scenes and objects qualitatively and quantitatively, making it possible for more robust and flexible part-level 3D Gaussian editing. Code is available at https://janeyeon.github.io/romap.', 'score': 26, 'issue_id': 4939, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '4c8104d951622fec', 'authors': ['Hayeon Kim', 'Ji Ha Jang', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'INMC & IPAI Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2507.11061.jpg', 'data': {'categories': ['#3d'], 'emoji': '✏️', 'ru': {'title': 'Точное локальное 3D-редактирование с помощью робастных масок и улучшенной регуляризации', 'desc': 'RoMaP - это новая система для точного локального 3D-редактирования, использующая генерацию робастных 3D-масок и улучшенную регуляризацию функции потерь SDS. Она включает модуль 3D-GALP для создания согласованных сегментаций частей объекта с разных ракурсов. RoMaP также применяет регуляризованную функцию потерь SDS с дополнительными регуляризаторами, включая L1-якорную потерю через метод SLaMP. Эксперименты показывают, что RoMaP достигает наилучших результатов в локальном 3D-редактировании как реконструированных, так и сгенерированных гауссовых сцен и объектов.'}, 'en': {'title': 'RoMaP: Revolutionizing Local 3D Editing with Precision and Flexibility', 'desc': 'The paper introduces RoMaP, a new framework designed to enhance local 3D editing by generating robust 3D masks and improving the Score Distillation Sampling (SDS) loss regularization. It addresses challenges in achieving precise edits in 3D content, particularly with Gaussian Splatting, by utilizing a 3D-Geometry Aware Label Prediction (3D-GALP) module for accurate part segmentations. The framework also incorporates a regularized SDS loss that includes an L1 anchor loss to ensure modifications are confined to specific areas while maintaining overall coherence. Experimental results show that RoMaP outperforms existing methods in local 3D editing, providing a more flexible and effective approach for part-level modifications.'}, 'zh': {'title': 'RoMaP：精确局部3D编辑的新框架', 'desc': 'RoMaP是一个新颖的局部3D编辑框架，旨在通过强大的3D掩模生成和增强的SDS损失正则化来提高精确的局部3D编辑能力。该框架引入了3D几何感知标签预测模块，利用球谐系数建模视角依赖的标签变化，从而实现准确一致的部分分割。通过结合标准SDS损失和额外的正则化项，RoMaP能够在目标区域内进行高质量的部分编辑，同时保持上下文的一致性。实验结果表明，RoMaP在重建和生成的高斯场景及物体上实现了最先进的局部3D编辑效果。'}}}, {'id': 'https://huggingface.co/papers/2507.15493', 'title': 'GR-3 Technical Report', 'url': 'https://huggingface.co/papers/2507.15493', 'abstract': 'A large-scale vision-language-action model demonstrates exceptional generalization, fine-tuning efficiency, and robust performance in complex robotic tasks, outperforming existing baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t We report our recent progress towards building generalist robot policies, the development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model. It showcases exceptional capabilities in generalizing to novel objects, environments, and instructions involving abstract concepts. Furthermore, it can be efficiently fine-tuned with minimal human trajectory data, enabling rapid and cost-effective adaptation to new settings. GR-3 also excels in handling long-horizon and dexterous tasks, including those requiring bi-manual manipulation and mobile movement, showcasing robust and reliable performance. These capabilities are achieved through a multi-faceted training recipe that includes co-training with web-scale vision-language data, efficient fine-tuning from human trajectory data collected via VR devices, and effective imitation learning with robot trajectory data. In addition, we introduce ByteMini, a versatile bi-manual mobile robot designed with exceptional flexibility and reliability, capable of accomplishing a wide range of tasks when integrated with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the state-of-the-art baseline method, pi_0, on a wide variety of challenging tasks. We hope GR-3 can serve as a step towards building generalist robots capable of assisting humans in daily life.', 'score': 24, 'issue_id': 4938, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '5e91567240893b65', 'authors': ['Chilam Cheang', 'Sijin Chen', 'Zhongren Cui', 'Yingdong Hu', 'Liqun Huang', 'Tao Kong', 'Hang Li', 'Yifeng Li', 'Yuxiao Liu', 'Xiao Ma', 'Hao Niu', 'Wenxuan Ou', 'Wanli Peng', 'Zeyu Ren', 'Haixin Shi', 'Jiawen Tian', 'Hongtao Wu', 'Xin Xiao', 'Yuyang Xiao', 'Jiafeng Xu', 'Yichu Yang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.15493.jpg', 'data': {'categories': ['#optimization', '#robotics', '#agents', '#training', '#agi'], 'emoji': '🤖', 'ru': {'title': 'GR-3: Шаг к универсальным роботам-помощникам', 'desc': 'Статья представляет GR-3 - крупномасштабную модель визуально-языкового действия (VLA) для робототехники. Модель демонстрирует исключительные способности к обобщению на новые объекты, среды и инструкции, включая абстрактные концепции. GR-3 может эффективно дообучаться на минимальном количестве человеческих траекторий, что позволяет быстро адаптироваться к новым условиям. Модель превосходит существующие базовые методы в широком спектре сложных задач, включая длительные и требующие ловкости операции.'}, 'en': {'title': 'GR-3: A Leap Towards Generalist Robots for Everyday Tasks', 'desc': "The paper presents GR-3, a large-scale vision-language-action model that excels in generalizing across various robotic tasks. It can adapt quickly to new environments and instructions with minimal human input, making it efficient for fine-tuning. GR-3 is particularly effective in performing complex tasks that require dexterity and coordination, such as bi-manual manipulation. The model's training combines web-scale data and imitation learning, leading to superior performance compared to existing methods."}, 'zh': {'title': 'GR-3：通用机器人政策的未来', 'desc': 'GR-3是一个大型的视觉-语言-动作模型，能够在复杂的机器人任务中表现出色。它具有很强的泛化能力，能够适应新物体、新环境和抽象概念的指令。该模型可以通过少量的人类轨迹数据进行高效的微调，快速适应新环境。通过与网络规模的视觉-语言数据共同训练，GR-3在长时间和灵巧任务中表现出强大的性能，展示了其在日常生活中辅助人类的潜力。'}}}, {'id': 'https://huggingface.co/papers/2507.15852', 'title': 'SeC: Advancing Complex Video Object Segmentation via Progressive Concept\n  Construction', 'url': 'https://huggingface.co/papers/2507.15852', 'abstract': 'Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.', 'score': 23, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '8d4e431fe003417f', 'authors': ['Zhixiong Zhang', 'Shuangrui Ding', 'Xiaoyi Dong', 'Songxin He', 'Jianfan Lin', 'Junsong Tang', 'Yuhang Zang', 'Yuhang Cao', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2507.15852.jpg', 'data': {'categories': ['#cv', '#interpretability', '#benchmark', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Концептуальное понимание для улучшения сегментации объектов в видео', 'desc': 'Статья представляет новый подход к сегментации объектов в видео под названием Segment Concept (SeC). SeC использует большие мультимодальные модели для создания концептуальных представлений объектов, что позволяет лучше справляться со сложными сценариями. Авторы также представляют новый датасет SeCVOS для оценки методов сегментации в сложных семантических сценариях. SeC показывает значительное улучшение результатов по сравнению с существующими методами на этом датасете.'}, 'en': {'title': 'Revolutionizing Video Object Segmentation with Conceptual Understanding', 'desc': 'This paper introduces Segment Concept (SeC), a new framework for Video Object Segmentation (VOS) that enhances object tracking and segmentation in videos. Unlike traditional methods that rely heavily on appearance matching, SeC focuses on building high-level, object-centric representations using Large Vision-Language Models (LVLMs). This approach allows the model to better understand and adapt to complex visual changes and occlusions, leading to improved segmentation accuracy. The authors also present a new benchmark, SeCVOS, to evaluate VOS methods in challenging scenarios, where SeC demonstrates significant performance improvements over existing techniques.'}, 'zh': {'title': '概念驱动的视频目标分割新突破', 'desc': '视频目标分割（VOS）是计算机视觉中的一项核心任务，要求模型在视频帧中跟踪和分割目标物体。尽管近年来取得了一些进展，但现有技术在处理剧烈的视觉变化、遮挡和复杂场景变化时仍然落后于人类能力。为了解决这一问题，我们提出了Segment Concept（SeC），它通过构建和利用高层次的以对象为中心的表示，转变了传统的特征匹配方法。SeC结合了大型视觉语言模型（LVLMs），在推理过程中形成全面的语义表示，从而实现对后续帧的稳健分割。'}}}, {'id': 'https://huggingface.co/papers/2507.15597', 'title': 'Being-H0: Vision-Language-Action Pretraining from Large-Scale Human\n  Videos', 'url': 'https://huggingface.co/papers/2507.15597', 'abstract': 'Being-H0 is a Vision-Language-Action model trained on human videos, addressing dexterity and generalization issues through physical instruction tuning and part-level motion tokenization, achieving superior hand motion generation and real-world robotic manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained on large-scale human videos. Existing VLAs struggle with complex manipulation tasks requiring high dexterity and generalize poorly to novel scenarios and tasks, primarily due to their reliance on synthetic data with significant sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To address this data bottleneck, we propose leveraging human hands as a foundation manipulator, capitalizing on the rich dexterity and scalability present in web data. Our approach centers on physical instruction tuning, a novel training paradigm that combines large-scale VLA pretraining from human videos, physical space alignment for 3D reasoning, and post-training adaptation for robotic tasks. Additionally, we introduce a part-level motion tokenization method which achieves millimeter-level reconstruction accuracy to model precise hand trajectories for action learning. To support our proposed paradigm, we further develop a comprehensive data curation pipeline that integrates heterogeneous sources -- including motion capture, VR, and RGB-only videos -- into a large-scale dataset with millions of motion-based instructional instances. We empirically show the excellence of Being-H0 in hand motion generation and instruction following, and it also scales well with model and data sizes. Importantly, we observe the expected gains of Being-H0 in real-world robotic manipulation as physical instruction tuning is applied. More details are available at https://beingbeyond.github.io/Being-H0.', 'score': 20, 'issue_id': 4942, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'c48aaae53a1f9330', 'authors': ['Hao Luo', 'Yicheng Feng', 'Wanpeng Zhang', 'Sipeng Zheng', 'Ye Wang', 'Haoqi Yuan', 'Jiazheng Liu', 'Chaoyi Xu', 'Qin Jin', 'Zongqing Lu'], 'affiliations': ['BeingBeyond', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.15597.jpg', 'data': {'categories': ['#agi', '#robotics', '#dataset', '#training', '#optimization', '#multimodal', '#data'], 'emoji': '🤖', 'ru': {'title': 'Обучение роботов человеческим движениям через видео', 'desc': 'Being-H0 - это модель зрения-языка-действия (VLA), обученная на видео с людьми для решения задач манипуляции. Модель использует физическое обучение по инструкциям и токенизацию движений на уровне частей тела для улучшения точности и обобщаемости. Being-H0 демонстрирует превосходные результаты в генерации движений рук и реальной робототехнической манипуляции. Модель обучается на масштабном наборе данных, включающем захват движений, VR и RGB-видео.'}, 'en': {'title': 'Empowering Robots with Human-Like Dexterity through Vision-Language-Action!', 'desc': 'Being-H0 is a cutting-edge Vision-Language-Action model designed to enhance robotic manipulation by learning from human videos. It tackles challenges in dexterity and generalization by utilizing physical instruction tuning and part-level motion tokenization, which allows for precise hand motion generation. The model is trained on a diverse dataset that includes various sources, ensuring it can adapt to real-world scenarios effectively. As a result, Being-H0 demonstrates superior performance in both generating hand motions and executing complex tasks in robotic applications.'}, 'zh': {'title': 'Being-H0：灵巧的视觉-语言-动作模型', 'desc': 'Being-H0 是一种视觉-语言-动作模型，专注于从人类视频中学习，以解决灵巧性和泛化能力的问题。该模型通过物理指令调优和部件级运动标记化，能够生成精确的手部动作并在真实世界中进行机器人操作。与传统模型相比，Being-H0 更好地处理复杂的操作任务，并能有效适应新场景。我们的研究表明，Being-H0 在手部动作生成和指令跟随方面表现出色，且在实际机器人操作中也取得了显著的进展。'}}}, {'id': 'https://huggingface.co/papers/2507.15778', 'title': 'Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for\n  RLVR', 'url': 'https://huggingface.co/papers/2507.15778', 'abstract': 'Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose Archer, an entropy-aware RLVR approach with dual-token constraints and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size. The code is available at https://github.com/wizard-III/ArcherCodeR.', 'score': 15, 'issue_id': 4936, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '8e0f7bdfedf50691', 'authors': ['Jiakang Wang', 'Runze Liu', 'Fuzheng Zhang', 'Xiu Li', 'Guorui Zhou'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15778.jpg', 'data': {'categories': ['#rl', '#optimization', '#reasoning', '#training', '#benchmark'], 'emoji': '🎯', 'ru': {'title': 'Точное обучение с подкреплением: улучшение рассуждений ИИ с помощью энтропийно-адаптивного подхода', 'desc': 'Статья представляет новый метод обучения с подкреплением для улучшения рассуждающих способностей больших языковых моделей, называемый Archer. Этот подход учитывает энтропию токенов и применяет различные ограничения к токенам знаний и рассуждений. Archer использует более слабую KL-регуляризацию и более высокие пороги отсечения для токенов рассуждений, чтобы стимулировать исследование, сохраняя при этом фактические знания. Экспериментальные результаты показывают, что Archer превосходит предыдущие методы RLVR на нескольких бенчмарках математических рассуждений и генерации кода.'}, 'en': {'title': 'Archer: Smart Token Training for Better Reasoning in LLMs', 'desc': 'This paper introduces Archer, a new method for Reinforcement Learning with Verifiable Rewards (RLVR) that enhances the reasoning capabilities of Large Language Models (LLMs). Unlike previous methods that treat all tokens equally, Archer distinguishes between low-entropy knowledge tokens and high-entropy reasoning tokens, applying different training strategies to each. By using weaker KL regularization for reasoning tokens, Archer promotes exploration while enforcing stronger constraints on knowledge tokens to preserve factual accuracy. The results demonstrate that Archer significantly improves performance on mathematical reasoning and code generation tasks, achieving state-of-the-art results for models of similar size.'}, 'zh': {'title': '提升推理能力的双重令牌强化学习', 'desc': '本文提出了一种新的强化学习方法，称为Archer，旨在提高大型语言模型的推理能力。Archer通过双重令牌约束和同步更新，分别对知识相关的低熵令牌和推理相关的高熵令牌施加不同的训练信号。与以往的算法不同，Archer在推理令牌上使用较弱的KL正则化，以鼓励探索，同时对知识令牌施加更强的约束，以保持事实知识的准确性。实验结果表明，Archer在多个数学推理和代码生成基准测试中显著优于之前的RLVR方法。'}}}, {'id': 'https://huggingface.co/papers/2507.14417', 'title': 'Inverse Scaling in Test-Time Compute', 'url': 'https://huggingface.co/papers/2507.14417', 'abstract': 'Evaluating Large Reasoning Models across different reasoning lengths reveals that increased test-time compute can degrade performance and exacerbate specific reasoning failures.  \t\t\t\t\tAI-generated summary \t\t\t\t We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy. Our evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks. We identify five distinct failure modes when models reason for longer: 1) Claude models become increasingly distracted by irrelevant information; 2) OpenAI o-series models resist distractors but overfit to problem framings; 3) models shift from reasonable priors to spurious correlations; 4) all models show difficulties in maintaining focus on complex deductive tasks; and 5) extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation. These findings suggest that while test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns. Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs.', 'score': 15, 'issue_id': 4945, 'pub_date': '2025-07-19', 'pub_date_card': {'ru': '19 июля', 'en': 'July 19', 'zh': '7月19日'}, 'hash': 'e6c3904a07b73089', 'authors': ['Aryo Pradipta Gema', 'Alexander Hägele', 'Runjin Chen', 'Andy Arditi', 'Jacob Goldman-Wetzler', 'Kit Fraser-Taliente', 'Henry Sleight', 'Linda Petrini', 'Julian Michael', 'Beatrice Alex', 'Pasquale Minervini', 'Yanda Chen', 'Joe Benton', 'Ethan Perez'], 'affiliations': ['Anthropic', 'Anthropic Fellows Program', 'Anthropic Fellows Program, EPFL', 'Anthropic Fellows Program, University of Edinburgh', 'Anthropic Fellows Program, University of Texas at Austin', 'Constellation', 'Independent', 'Scale AI', 'University of Edinburgh', 'University of Edinburgh, Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.14417.jpg', 'data': {'categories': ['#benchmark', '#hallucinations', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Больше вычислений - не всегда лучше: парадокс масштабирования в моделях рассуждений', 'desc': 'Исследование показывает, что увеличение вычислительной мощности при тестировании крупных моделей рассуждений (LRM) может ухудшить их производительность. Выявлено пять режимов отказа, включая отвлечение на нерелевантную информацию и переобучение на особенностях формулировки задачи. Модели демонстрируют трудности в поддержании фокуса на сложных дедуктивных задачах и могут усиливать проблемные шаблоны рассуждений. Результаты подчеркивают важность оценки моделей на различных длинах рассуждений для выявления и устранения этих проблем.'}, 'en': {'title': 'Longer Reasoning, Lower Accuracy: The Inverse Scaling Dilemma', 'desc': "This paper investigates how increasing the reasoning length of Large Reasoning Models (LRMs) can lead to worse performance, highlighting an inverse relationship between the amount of compute used during testing and the models' accuracy. The authors create evaluation tasks that reveal five specific failure modes, such as models becoming distracted by irrelevant information or overfitting to specific problem framings. They also note that longer reasoning can exacerbate issues like reliance on spurious correlations and difficulties in complex deductive reasoning. Overall, the study emphasizes the need for careful evaluation of LRMs across varying reasoning lengths to uncover and mitigate these performance issues."}, 'zh': {'title': '推理长度与模型性能的反向关系', 'desc': '本研究探讨了大型推理模型在不同推理长度下的表现，发现增加测试时计算量可能会降低性能并加剧特定的推理失败。我们设计了四类评估任务，结果显示推理长度的延长与准确率之间存在反向缩放关系。研究中识别了五种不同的失败模式，包括模型对无关信息的干扰和对问题框架的过拟合等。我们的发现强调了在多样化推理长度下评估模型的重要性，以识别和解决大型推理模型中的这些失败模式。'}}}, {'id': 'https://huggingface.co/papers/2507.15028', 'title': 'Towards Video Thinking Test: A Holistic Benchmark for Advanced Video\n  Reasoning and Understanding', 'url': 'https://huggingface.co/papers/2507.15028', 'abstract': 'Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance.', 'score': 14, 'issue_id': 4938, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': '7f71d09a9b276de8', 'authors': ['Yuanhan Zhang', 'Yunice Chew', 'Yuhao Dong', 'Aria Leo', 'Bo Hu', 'Ziwei Liu'], 'affiliations': ['Independent Researcher', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15028.jpg', 'data': {'categories': ['#interpretability', '#security', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Новый рубеж в оценке видео-LLM: человекоподобное понимание реального мира', 'desc': 'Статья представляет новый тест Video Thinking Test (Video-TT) для оценки способности видео-LLM интерпретировать реальные видео на уровне человека. Video-TT состоит из 1000 коротких YouTube-видео с открытыми и провокационными вопросами, оценивающими понимание сложных визуальных нарративов. Тест выявляет существенный разрыв между производительностью видео-LLM и человеческим интеллектом в корректности и устойчивости интерпретации видео. Video-TT направлен на оценку способности моделей точно и устойчиво интерпретировать визуальный контент в сложных условиях.'}, 'en': {'title': 'Bridging the Gap: Evaluating Video LLMs with the Video Thinking Test', 'desc': "This paper discusses the importance of correctness and robustness in video understanding, which are essential for mimicking human intelligence. It highlights that current benchmarks do not adequately measure how well video large language models (LLMs) interpret videos compared to humans. To address this, the authors introduce the Video Thinking Test (Video-TT), designed to evaluate the performance of video LLMs on real-world videos. The test includes 1,000 YouTube Shorts videos with questions that challenge the models' understanding of complex visual narratives, revealing a significant performance gap between the models and human interpreters."}, 'zh': {'title': '视频理解的挑战：人类与模型的差距', 'desc': '本论文探讨了视频理解中的正确性和鲁棒性问题。尽管视频大型语言模型（视频LLMs）取得了一定进展，但现有基准测试未能充分反映这些模型与人类智能在视频解释中的差距。我们提出了视频思维测试（Video-TT），旨在评估视频LLMs是否能像人类一样有效地理解现实世界的视频。测试包含1000个YouTube Shorts视频，每个视频配有一个开放性问题和四个针对视觉和叙事复杂性的对抗性问题，评估结果显示视频LLMs与人类表现之间存在显著差距。'}}}, {'id': 'https://huggingface.co/papers/2507.15629', 'title': 'Gaussian Splatting with Discretized SDF for Relightable Assets', 'url': 'https://huggingface.co/papers/2507.15629', 'abstract': '3D Gaussian splatting (3DGS) has shown its detailed expressive ability and highly efficient rendering speed in the novel view synthesis (NVS) task. The application to inverse rendering still faces several challenges, as the discrete nature of Gaussian primitives makes it difficult to apply geometry constraints. Recent works introduce the signed distance field (SDF) as an extra continuous representation to regularize the geometry defined by Gaussian primitives. It improves the decomposition quality, at the cost of increasing memory usage and complicating training. Unlike these works, we introduce a discretized SDF to represent the continuous SDF in a discrete manner by encoding it within each Gaussian using a sampled value. This approach allows us to link the SDF with the Gaussian opacity through an SDF-to-opacity transformation, enabling rendering the SDF via splatting and avoiding the computational cost of ray marching.The key challenge is to regularize the discrete samples to be consistent with the underlying SDF, as the discrete representation can hardly apply the gradient-based constraints (\\eg Eikonal loss). For this, we project Gaussians onto the zero-level set of SDF and enforce alignment with the surface from splatting, namely a projection-based consistency loss. Thanks to the discretized SDF, our method achieves higher relighting quality, while requiring no extra memory beyond GS and avoiding complex manually designed optimization. The experiments reveal that our method outperforms existing Gaussian-based inverse rendering methods. Our code is available at https://github.com/NK-CS-ZZL/DiscretizedSDF.', 'score': 13, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'f2fc3e4b855b88d5', 'authors': ['Zuo-Liang Zhu', 'Jian Yang', 'Beibei Wang'], 'affiliations': ['Nanjing University', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15629.jpg', 'data': {'categories': ['#3d'], 'emoji': '🎨', 'ru': {'title': 'Дискретизированное SDF для улучшенного обратного рендеринга с гауссовским сплаттингом', 'desc': 'Статья представляет новый подход к обратному рендерингу с использованием дискретизированного поля расстояний со знаком (SDF) в контексте 3D гауссовского сплаттинга. Авторы кодируют SDF в каждом гауссиане с помощью дискретных значений, что позволяет связать SDF с прозрачностью гауссианов через специальное преобразование. Для регуляризации дискретных выборок вводится проекционная функция потерь, обеспечивающая согласованность с базовым SDF. Эксперименты показывают, что предложенный метод превосходит существующие подходы к обратному рендерингу на основе гауссианов по качеству перерисовки освещения.'}, 'en': {'title': 'Efficient Inverse Rendering with Discretized SDF and Gaussian Splatting', 'desc': 'This paper presents a novel approach to inverse rendering using a discretized signed distance field (SDF) integrated with 3D Gaussian splatting (3DGS). By encoding the continuous SDF within each Gaussian, the method links SDF with Gaussian opacity, allowing for efficient rendering without the heavy computational costs of ray marching. The authors introduce a projection-based consistency loss to ensure that the discrete samples align with the underlying SDF, improving the quality of relighting. Overall, this approach enhances the performance of Gaussian-based inverse rendering while maintaining low memory usage and simplifying the optimization process.'}, 'zh': {'title': '离散化SDF提升逆向渲染质量', 'desc': '3D高斯点云（3DGS）在新视图合成（NVS）任务中表现出色，但在逆向渲染中仍面临挑战。我们提出了一种离散化的有符号距离场（SDF），通过在每个高斯中编码采样值来表示连续的SDF，从而简化了几何约束的应用。该方法通过SDF与高斯不透明度的转换，避免了光线行进的计算成本，同时提高了重光照质量。实验结果表明，我们的方法在性能上优于现有的基于高斯的逆向渲染方法。'}}}, {'id': 'https://huggingface.co/papers/2507.15375', 'title': 'STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for\n  Spoken Language Models', 'url': 'https://huggingface.co/papers/2507.15375', 'abstract': 'Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: https://d223302.github.io/STITCH.', 'score': 8, 'issue_id': 4937, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '2a7d1e1e1882f002', 'authors': ['Cheng-Han Chiang', 'Xiaofei Wang', 'Linjie Li', 'Chung-Ching Lin', 'Kevin Lin', 'Shujie Liu', 'Zhendong Wang', 'Zhengyuan Yang', 'Hung-yi Lee', 'Lijuan Wang'], 'affiliations': ['Microsoft', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15375.jpg', 'data': {'categories': ['#training', '#reasoning', '#audio', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Stitch: Думай и говори одновременно', 'desc': 'Статья представляет новый метод генерации речи для разговорных языковых моделей под названием Stitch. Этот метод позволяет моделям осуществлять внутренний процесс мышления, чередуя генерацию невысказанных рассуждений и произносимых ответов. Stitch использует свободное время во время воспроизведения аудио для генерации следующего фрагмента невысказанных рассуждений, что позволяет модели одновременно думать и говорить. Результаты показывают, что Stitch превосходит базовые модели на 15% в задачах математических рассуждений, сохраняя при этом такую же задержку и производительность на других наборах данных.'}, 'en': {'title': 'Stitch: Simultaneous Thinking and Talking for Enhanced Spoken Language Models', 'desc': 'This paper introduces Stitch, a new method for Spoken Language Models (SLMs) that allows them to think internally while responding to speech. Unlike traditional SLMs that generate responses without prior reasoning, Stitch alternates between generating unspoken reasoning chunks and spoken responses. This approach minimizes latency by utilizing the time taken to play audio responses to continue generating reasoning. As a result, Stitch not only matches the response time of existing models but also improves performance on math reasoning tasks by 15%.'}, 'zh': {'title': '同步思考与表达的口语模型', 'desc': '本论文提出了一种新的口语语言模型生成方法，名为Stitch。该方法通过交替生成无声推理片段和口语响应片段，解决了传统模型在回应前缺乏内在思考过程的问题。Stitch利用口语响应的音频持续时间，充分利用剩余时间生成推理内容，从而实现思考与表达的同步进行。实验结果表明，Stitch在数学推理数据集上比基线模型提高了15%的性能，同时在非推理数据集上表现也与基线模型相当。'}}}, {'id': 'https://huggingface.co/papers/2507.13428', 'title': '"PhyWorldBench": A Comprehensive Evaluation of Physical Realism in\n  Text-to-Video Models', 'url': 'https://huggingface.co/papers/2507.13428', 'abstract': 'Video generation models have achieved remarkable progress in creating high-quality, photorealistic content. However, their ability to accurately simulate physical phenomena remains a critical and unresolved challenge. This paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate video generation models based on their adherence to the laws of physics. The benchmark covers multiple levels of physical phenomena, ranging from fundamental principles like object motion and energy conservation to more complex scenarios involving rigid body interactions and human or animal motion. Additionally, we introduce a novel ""Anti-Physics"" category, where prompts intentionally violate real-world physics, enabling the assessment of whether models can follow such instructions while maintaining logical consistency. Besides large-scale human evaluation, we also design a simple yet effective method that could utilize current MLLM to evaluate the physics realism in a zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation models, including five open-source and five proprietary models, with a detailed comparison and analysis. we identify pivotal challenges models face in adhering to real-world physics. Through systematic testing of their outputs across 1,050 curated prompts-spanning fundamental, composite, and anti-physics scenarios-we identify pivotal challenges these models face in adhering to real-world physics. We then rigorously examine their performance on diverse physical phenomena with varying prompt types, deriving targeted recommendations for crafting prompts that enhance fidelity to physical principles.', 'score': 6, 'issue_id': 4943, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '0bcb2373e179ecb8', 'authors': ['Jing Gu', 'Xian Liu', 'Yu Zeng', 'Ashwin Nagarajan', 'Fangrui Zhu', 'Daniel Hong', 'Yue Fan', 'Qianqi Yan', 'Kaiwen Zhou', 'Ming-Yu Liu', 'Xin Eric Wang'], 'affiliations': ['NVIDIA Research', 'Northeastern University', 'University of California, Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2507.13428.jpg', 'data': {'categories': ['#games', '#interpretability', '#optimization', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Физика в виртуальном мире: новый бенчмарк для оценки реалистичности видеогенерации', 'desc': "Статья представляет PhyWorldBench - комплексный бенчмарк для оценки моделей генерации видео на основе их соответствия законам физики. Бенчмарк охватывает различные уровни физических явлений, от базовых принципов до сложных сценариев, включая категорию 'Анти-физика'. Авторы оценили 12 современных моделей text-to-video на 1050 специально подобранных промптах. В результате были выявлены ключевые проблемы, с которыми сталкиваются модели при соблюдении реальной физики, и даны рекомендации по составлению промптов для повышения физической достоверности."}, 'en': {'title': 'Evaluating Video Generation with Physics: PhyWorldBench', 'desc': "This paper introduces PhyWorldBench, a benchmark for evaluating video generation models based on their ability to simulate physical laws accurately. It assesses models across various physical phenomena, from basic principles like motion and energy conservation to complex interactions involving living beings. A unique 'Anti-Physics' category is included to test models' responses to prompts that contradict real-world physics, ensuring logical consistency in their outputs. The study evaluates 12 leading text-to-video models, revealing significant challenges in maintaining physical realism and providing insights for improving prompt design to enhance adherence to physical principles."}, 'zh': {'title': '评估视频生成模型的物理真实性', 'desc': '视频生成模型在创建高质量、逼真的内容方面取得了显著进展。然而，它们准确模拟物理现象的能力仍然是一个关键且未解决的挑战。本文提出了PhyWorldBench，这是一个全面的基准，用于评估视频生成模型在遵循物理法则方面的表现。我们评估了12个最先进的文本到视频生成模型，并识别出这些模型在遵循现实物理方面面临的主要挑战。'}}}, {'id': 'https://huggingface.co/papers/2507.11539', 'title': 'Streaming 4D Visual Geometry Transformer', 'url': 'https://huggingface.co/papers/2507.11539', 'abstract': 'A streaming 4D visual geometry transformer uses causal attention and knowledge distillation to achieve real-time 4D reconstruction with high spatial consistency and competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.', 'score': 6, 'issue_id': 4937, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '03e472d31e5edcaf', 'authors': ['Dong Zhuo', 'Wenzhao Zheng', 'Jiahe Guo', 'Yuqi Wu', 'Jie Zhou', 'Jiwen Lu'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.11539.jpg', 'data': {'categories': ['#inference', '#long_context', '#optimization', '#benchmark', '#architecture', '#cv'], 'emoji': '🔄', 'ru': {'title': 'Реконструкция 4D-геометрии в реальном времени с помощью потокового трансформера', 'desc': 'Статья представляет потоковый 4D-трансформер визуальной геометрии для реконструкции пространственно-временной геометрии из видео в реальном времени. Модель использует каузальную архитектуру трансформера и временное каузальное внимание для обработки входной последовательности в онлайн-режиме. Для эффективного обучения применяется дистилляция знаний от более плотной двунаправленной модели. Эксперименты показывают, что предложенный подход увеличивает скорость вывода, сохраняя конкурентоспособную производительность.'}, 'en': {'title': 'Real-Time 4D Reconstruction with Streaming Transformers', 'desc': 'This paper presents a streaming 4D visual geometry transformer that utilizes causal attention and knowledge distillation for real-time 4D reconstruction from video data. The model processes input sequences in an online manner, leveraging a causal transformer architecture to maintain high spatial consistency while integrating historical information. By employing temporal causal attention and caching past data, the system achieves efficient long-term reconstruction. The approach is validated through extensive experiments, showing improved inference speed and competitive performance, making it suitable for interactive 4D vision applications.'}, 'zh': {'title': '实时4D重建的创新变换器', 'desc': '本文提出了一种流式4D视觉几何变换器，利用因果注意力和知识蒸馏技术，实现实时的4D重建。该模型采用因果变换器架构，能够在线处理输入序列，并通过缓存历史信息来提高重建效率。通过从密集双向视觉几何变换器中蒸馏知识，模型在训练过程中得以优化。实验结果表明，该模型在保持高空间一致性的同时，显著提高了在线推理速度，适用于可扩展的交互式4D视觉系统。'}}}, {'id': 'https://huggingface.co/papers/2507.15856', 'title': 'Latent Denoising Makes Good Visual Tokenizers', 'url': 'https://huggingface.co/papers/2507.15856', 'abstract': 'Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design.', 'score': 5, 'issue_id': 4940, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '60a696cb47720198', 'authors': ['Jiawei Yang', 'Tianhong Li', 'Lijie Fan', 'Yonglong Tian', 'Yue Wang'], 'affiliations': ['Google DeepMind', 'MIT CSAIL', 'OpenAI', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2507.15856.jpg', 'data': {'categories': ['#cv', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧹', 'ru': {'title': 'Шумоподавление как ключ к эффективным визуальным токенизаторам', 'desc': 'Статья представляет новый подход к разработке визуальных токенизаторов для генеративных моделей. Авторы предлагают метод Latent Denoising Tokenizer (l-DeTok), который обучается восстанавливать чистые изображения из зашумленных латентных представлений. Эксперименты на ImageNet показывают превосходство l-DeTok над стандартными токенизаторами для шести различных генеративных моделей. Исследование подчеркивает важность принципа шумоподавления в разработке токенизаторов для генеративного моделирования.'}, 'en': {'title': 'Enhancing Generative Models with Denoising Tokenizers', 'desc': 'This paper explores how visual tokenizers can be improved for generative modeling by focusing on a process called denoising. The authors propose a new tokenizer, the Latent Denoising Tokenizer (l-DeTok), which aligns its embeddings with the goal of reconstructing clean images from corrupted inputs. By training this tokenizer to handle noise and masking, it becomes more effective at generating high-quality outputs. The results show that l-DeTok outperforms traditional tokenizers in various generative models, suggesting that denoising should be a key consideration in future tokenizer designs.'}, 'zh': {'title': '去噪：分词器设计的新原则', 'desc': '本论文探讨了视觉分词器在生成建模中的有效性，提出了对分词器嵌入与去噪目标进行对齐的概念。我们引入了潜在去噪分词器（l-DeTok），该分词器旨在从受到干扰的潜在嵌入中重建干净图像。实验结果表明，l-DeTok在多个生成模型上优于传统分词器，验证了去噪作为分词器设计的重要原则。我们希望这一发现能够为未来的分词器设计提供新的视角。'}}}, {'id': 'https://huggingface.co/papers/2507.15815', 'title': 'LLM Economist: Large Population Models and Mechanism Design in\n  Multi-Agent Generative Simulacra', 'url': 'https://huggingface.co/papers/2507.15815', 'abstract': 'We present the LLM Economist, a novel framework that uses agent-based modeling to design and assess economic policies in strategic environments with hierarchical decision-making. At the lower level, bounded rational worker agents -- instantiated as persona-conditioned prompts sampled from U.S. Census-calibrated income and demographic statistics -- choose labor supply to maximize text-based utility functions learned in-context. At the upper level, a planner agent employs in-context reinforcement learning to propose piecewise-linear marginal tax schedules anchored to the current U.S. federal brackets. This construction endows economic simulacra with three capabilities requisite for credible fiscal experimentation: (i) optimization of heterogeneous utilities, (ii) principled generation of large, demographically realistic agent populations, and (iii) mechanism design -- the ultimate nudging problem -- expressed entirely in natural language. Experiments with populations of up to one hundred interacting agents show that the planner converges near Stackelberg equilibria that improve aggregate social welfare relative to Saez solutions, while a periodic, persona-level voting procedure furthers these gains under decentralized governance. These results demonstrate that large language model-based agents can jointly model, simulate, and govern complex economic systems, providing a tractable test bed for policy evaluation at the societal scale to help build better civilizations.', 'score': 4, 'issue_id': 4937, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'ad03ed3ae6e4256b', 'authors': ['Seth Karten', 'Wenzhe Li', 'Zihan Ding', 'Samuel Kleiner', 'Yu Bai', 'Chi Jin'], 'affiliations': ['Princeton University', 'Salesforce Research'], 'pdf_title_img': 'assets/pdf/title_img/2507.15815.jpg', 'data': {'categories': ['#optimization', '#agents', '#agi', '#multimodal', '#rl', '#science'], 'emoji': '🤖', 'ru': {'title': 'Искусственный интеллект как экономист: моделирование и оптимизация экономической политики', 'desc': "Статья представляет новую концепцию под названием 'LLM Economist', которая использует агентное моделирование для разработки и оценки экономической политики в стратегических средах с иерархическим принятием решений. На нижнем уровне ограниченно рациональные агенты-работники выбирают предложение труда для максимизации текстовых функций полезности, изученных в контексте. На верхнем уровне агент-планировщик использует обучение с подкреплением для предложения кусочно-линейных графиков предельных налоговых ставок. Эксперименты показывают, что планировщик сходится к равновесиям, улучшающим совокупное общественное благосостояние по сравнению с решениями Саеза."}, 'en': {'title': 'Harnessing AI for Smarter Economic Policy Design', 'desc': 'The LLM Economist is a new framework that combines agent-based modeling with large language models to evaluate economic policies in complex decision-making environments. It features two levels of agents: lower-level worker agents that optimize their labor supply based on learned utility functions, and an upper-level planner agent that uses reinforcement learning to create tax schedules. This approach allows for realistic simulations of diverse populations and effective mechanism design, all expressed in natural language. The framework shows promising results in improving social welfare through strategic interactions among agents, making it a valuable tool for testing economic policies.'}, 'zh': {'title': '利用大语言模型优化经济政策', 'desc': '本文介绍了一种名为LLM Economist的新框架，利用基于代理的建模来设计和评估具有层级决策的经济政策。在低层次，有限理性的工人代理根据美国人口普查的收入和人口统计数据选择劳动供给，以最大化基于文本的效用函数。在高层次，规划者代理使用上下文强化学习提出与当前美国联邦税率相结合的分段线性边际税率。这种构建使经济模拟具备了优化异质效用、生成大规模人口和机制设计等三种能力，能够在自然语言中进行有效的财政实验。'}}}, {'id': 'https://huggingface.co/papers/2507.15640', 'title': 'Data Mixing Agent: Learning to Re-weight Domains for Continual\n  Pre-training', 'url': 'https://huggingface.co/papers/2507.15640', 'abstract': "Data Mixing Agent, a model-based framework using reinforcement learning, effectively re-weights training data to balance performance across source and target fields in continual pre-training of large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Continual pre-training on small-scale task-specific data is an effective method for improving large language models in new target fields, yet it risks catastrophic forgetting of their original capabilities. A common solution is to re-weight training data mixtures from source and target fields on a domain space to achieve balanced performance. Previous domain reweighting strategies rely on manual designation with certain heuristics based on human intuition or empirical results. In this work, we prove that more general heuristics can be parameterized by proposing Data Mixing Agent, the first model-based, end-to-end framework that learns to re-weight domains. The agent learns generalizable heuristics through reinforcement learning on large quantities of data mixing trajectories with corresponding feedback from an evaluation environment. Experiments in continual pre-training on math reasoning show that Data Mixing Agent outperforms strong baselines in achieving balanced performance across source and target field benchmarks. Furthermore, it generalizes well across unseen source fields, target models, and domain spaces without retraining. Direct application to the code generation field also indicates its adaptability across target domains. Further analysis showcases the agents' well-aligned heuristics with human intuitions and their efficiency in achieving superior model performance with less source-field data.", 'score': 3, 'issue_id': 4942, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '2ce6b05c03e1226b', 'authors': ['Kailai Yang', 'Xiao Liu', 'Lei Ji', 'Hao Li', 'Yeyun Gong', 'Peng Cheng', 'Mao Yang'], 'affiliations': ['Microsoft Research', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2507.15640.jpg', 'data': {'categories': ['#training', '#optimization', '#transfer_learning', '#agents', '#rl'], 'emoji': '🔀', 'ru': {'title': 'Умное смешивание данных для адаптивного обучения языковых моделей', 'desc': 'Data Mixing Agent - это фреймворк на основе обучения с подкреплением для переобучения больших языковых моделей. Он эффективно перевзвешивает обучающие данные для сбалансансировки производительности между исходными и целевыми областями. Этот подход превосходит сильные базовые модели в достижении сбалансированной производительности в задачах математических рассуждений. Data Mixing Agent хорошо обобщается на новые исходные области, целевые модели и пространства доменов без переобучения.'}, 'en': {'title': 'Reinforcement Learning for Balanced Data Mixing in Language Models', 'desc': 'The paper introduces the Data Mixing Agent, a novel framework that utilizes reinforcement learning to dynamically re-weight training data for continual pre-training of large language models. This approach addresses the challenge of catastrophic forgetting by balancing the performance between source and target fields without relying on manual heuristics. The agent learns effective data mixing strategies through interactions with a feedback-rich environment, allowing it to generalize across various domains. Experimental results demonstrate that the Data Mixing Agent significantly improves performance in tasks like math reasoning and code generation, showcasing its versatility and efficiency in leveraging limited source-field data.'}, 'zh': {'title': '数据混合代理：平衡源与目标领域的智能学习', 'desc': '数据混合代理是一种基于模型的框架，利用强化学习有效地重新加权训练数据，以平衡在持续预训练中源领域和目标领域的性能。该方法解决了在小规模特定任务数据上持续预训练时可能出现的灾难性遗忘问题。通过提出数据混合代理，研究者证明了更通用的启发式方法可以被参数化，从而实现端到端的学习。实验结果表明，该代理在数学推理的持续预训练中表现优于强基线，并且在未见过的源领域和目标模型上具有良好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2507.15550', 'title': 'PhysGym: Benchmarking LLMs in Interactive Physics Discovery with\n  Controlled Priors', 'url': 'https://huggingface.co/papers/2507.15550', 'abstract': "PhysGym, a new benchmark suite, evaluates large language model-based agents' scientific reasoning in interactive physics environments, focusing on their handling of complexity and prior knowledge.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating the scientific discovery capabilities of large language model based agents, particularly how they cope with varying environmental complexity and utilize prior knowledge, requires specialized benchmarks currently lacking in the landscape. To address this gap, we introduce PhysGym, a novel benchmark suite and simulation platform for rigorously assessing LLM-based scientific reasoning in interactive physics environments. PhysGym's primary contribution lies in its sophisticated control over the level of prior knowledge provided to the agent. This allows researchers to dissect agent performance along axes including the complexity of the problem and the prior knowledge levels. The benchmark comprises a suite of interactive simulations, where agents must actively probe environments, gather data sequentially under constraints and formulate hypotheses about underlying physical laws. PhysGym provides standardized evaluation protocols and metrics for assessing hypothesis accuracy and model fidelity. We demonstrate the benchmark's utility by presenting results from baseline LLMs, showcasing its ability to differentiate capabilities based on varying priors and task complexity.", 'score': 3, 'issue_id': 4947, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '2cbec0e62f66698a', 'authors': ['Yimeng Chen', 'Piotr Piȩkos', 'Mateusz Ostaszewski', 'Firas Laakom', 'Jürgen Schmidhuber'], 'affiliations': ['Center of Excellence for Generative AI, KAUST', 'NNAISENSE', 'The Swiss AI Lab, IDSIA-USI/SUPSI'], 'pdf_title_img': 'assets/pdf/title_img/2507.15550.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning', '#science'], 'emoji': '🧠', 'ru': {'title': 'PhysGym: испытание научного мышления ИИ в физическом мире', 'desc': 'PhysGym - это новый набор тестов для оценки научного мышления агентов на основе больших языковых моделей в интерактивных физических средах. Он фокусируется на способности агентов справляться со сложностью и использовать предварительные знания. PhysGym позволяет контователям анализировать производительность агентов в зависимости от сложности задачи и уровня предварительных знаний. Бвключает интерактивные симуляции, где агенты должны активно исследовать среду, собирать данные и формулировать гипотезы о физических законах.'}, 'en': {'title': 'PhysGym: Benchmarking Scientific Reasoning in Physics with LLMs', 'desc': 'PhysGym is a new benchmark suite designed to evaluate how well large language model (LLM) agents can reason scientifically in interactive physics settings. It focuses on how these agents manage different levels of complexity in their environments and how they use prior knowledge to solve problems. The benchmark includes a variety of simulations where agents must explore, collect data, and form hypotheses about physical laws. By providing standardized evaluation metrics, PhysGym helps researchers understand the performance of LLMs based on their prior knowledge and the complexity of tasks they face.'}, 'zh': {'title': 'PhysGym：评估智能体科学推理的新基准', 'desc': 'PhysGym是一个新的基准套件，用于评估基于大型语言模型的智能体在互动物理环境中的科学推理能力。它专注于智能体如何处理复杂性和利用先前知识。PhysGym的主要贡献在于对提供给智能体的先前知识水平进行精细控制，从而帮助研究人员分析智能体在不同问题复杂性和知识水平下的表现。该基准包括一系列互动模拟，智能体需要在约束条件下主动探测环境、收集数据并形成关于物理法则的假设。'}}}, {'id': 'https://huggingface.co/papers/2507.14295', 'title': 'A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning', 'url': 'https://huggingface.co/papers/2507.14295', 'abstract': 'Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let\'s try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback', 'score': 3, 'issue_id': 4945, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': '6b7877061c71a067', 'authors': ['Licheng Liu', 'Zihan Wang', 'Linjie Li', 'Chenwei Xu', 'Yiping Lu', 'Han Liu', 'Avirup Sil', 'Manling Li'], 'affiliations': ['IBM Research AI', 'Imperial College London', 'Northwestern University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.14295.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#optimization', '#rl', '#training'], 'emoji': '🧠', 'ru': {'title': 'Улучшение многоэтапных рассуждений ИИ с помощью минимальной обратной связи', 'desc': 'Статья посвящена улучшению способности больших языковых моделей (LLM) решать задачи в многоэтапном режиме с учетом обратной связи. Авторы предлагают метод обучения с подкреплением (RL) под названием Unary Feedback as Observation (UFO), использующий минимальную унарную обратную связь от пользователя. Эксперименты показывают, что UFO улучшает точность многоэтапных рассуждений до 14% при сохранении производительности в одноэтапном режиме. Также разработаны структуры вознаграждений, поощряющие модели давать тщательные и продуманные ответы на каждом этапе.'}, 'en': {'title': 'Empowering LRMs with Unary Feedback for Better Multi-Turn Reasoning', 'desc': 'This paper addresses the challenges faced by Large Reasoning Models (LRMs) in multi-turn problem solving, particularly their ability to reflect on and revise their answers based on feedback. The authors highlight that traditional Reinforcement Learning (RL) methods often lead to models that struggle with multi-turn interactions and produce repetitive responses. They propose a novel approach called Unary Feedback as Observation (UFO), which utilizes simple unary feedback to enhance both single-turn and multi-turn reasoning capabilities. Experimental results demonstrate that this method improves multi-turn reasoning accuracy significantly while maintaining performance in single-turn tasks.'}, 'zh': {'title': '多轮推理，单元反馈助力', 'desc': '多轮问题解决对大型推理模型（LRMs）至关重要，但也很具挑战性。现有的强化学习（RL）方法通常在单轮范式下训练模型，导致模型在多轮上下文中难以反思和修正答案。我们提出了一种新的方法，称为单元反馈作为观察（UFO），通过使用简单的反馈来提高模型的多轮推理能力。实验结果表明，使用UFO的强化学习训练可以保持单轮性能，并将多轮推理的准确性提高多达14%。'}}}, {'id': 'https://huggingface.co/papers/2507.15728', 'title': 'TokensGen: Harnessing Condensed Tokens for Long Video Generation', 'url': 'https://huggingface.co/papers/2507.15728', 'abstract': 'Generating consistent long videos is a complex challenge: while diffusion-based generative models generate visually impressive short clips, extending them to longer durations often leads to memory bottlenecks and long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage framework that leverages condensed tokens to address these issues. Our method decomposes long video generation into three core tasks: (1) inner-clip semantic control, (2) long-term consistency control, and (3) inter-clip smooth transition. First, we train To2V (Token-to-Video), a short video diffusion model guided by text and video tokens, with a Video Tokenizer that condenses short clips into semantically rich tokens. Second, we introduce T2To (Text-to-Token), a video token diffusion transformer that generates all tokens at once, ensuring global consistency across clips. Finally, during inference, an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips, reducing boundary artifacts and enhancing smooth transitions. Experimental results demonstrate that our approach significantly enhances long-term temporal and content coherence without incurring prohibitive computational overhead. By leveraging condensed tokens and pre-trained short video models, our method provides a scalable, modular solution for long video generation, opening new possibilities for storytelling, cinematic production, and immersive simulations. Please see our project page at https://vicky0522.github.io/tokensgen-webpage/ .', 'score': 2, 'issue_id': 4947, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'a1ab25505568523b', 'authors': ['Wenqi Ouyang', 'Zeqi Xiao', 'Danni Yang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'SenseTime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15728.jpg', 'data': {'categories': ['#diffusion', '#story_generation', '#long_context', '#multimodal', '#video', '#inference'], 'emoji': '🎬', 'ru': {'title': 'TokensGen: Революция в генерации длинных видео с помощью сжатых токенов', 'desc': 'TokensGen - это новый двухэтапный подход к генерации длинных видео, использующий сжатые токены. Модель To2V генерирует короткие клипы на основе текста и видеотокенов, а T2To создает последовательность токенов для обеспечения глобальной согласованности. Стратегия FIFO-Diffusion плавно соединяет соседние клипы, уменьшая артефакты на границах. Этот метод значительно улучшает долгосрочную временную и содержательную согласованность видео без чрезмерных вычислительных затрат.'}, 'en': {'title': 'TokensGen: Seamless Long Video Generation with Condensed Tokens', 'desc': 'This paper introduces TokensGen, a two-stage framework designed to generate long videos more effectively. It addresses challenges like memory limitations and inconsistencies in long video generation by using condensed tokens. The method involves training a short video diffusion model and a video token diffusion transformer to ensure semantic richness and global consistency. The results show that TokensGen improves the coherence of long videos while maintaining manageable computational demands, making it suitable for various applications in storytelling and simulations.'}, 'zh': {'title': 'TokensGen：长视频生成的新解决方案', 'desc': '生成一致的长视频是一个复杂的挑战。虽然基于扩散的生成模型可以生成视觉上令人印象深刻的短片，但扩展到更长的时长常常会导致内存瓶颈和长期不一致性。我们提出了一种名为TokensGen的新型两阶段框架，通过浓缩的标记来解决这些问题。我们的研究表明，该方法在长视频生成中显著提高了时间和内容的一致性，同时避免了过高的计算开销。'}}}, {'id': 'https://huggingface.co/papers/2507.10935', 'title': 'GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised\n  Cross-View Localization', 'url': 'https://huggingface.co/papers/2507.10935', 'abstract': "Cross-view localization, the task of estimating a camera's 3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with satellite images, is crucial for large-scale outdoor applications like autonomous navigation and augmented reality. Existing methods often rely on fully supervised learning, which requires costly ground-truth pose annotations. In this work, we propose GeoDistill, a Geometry guided weakly supervised self distillation framework that uses teacher-student learning with Field-of-View (FoV)-based masking to enhance local feature learning for robust cross-view localization. In GeoDistill, the teacher model localizes a panoramic image, while the student model predicts locations from a limited FoV counterpart created by FoV-based masking. By aligning the student's predictions with those of the teacher, the student focuses on key features like lane lines and ignores textureless regions, such as roads. This results in more accurate predictions and reduced uncertainty, regardless of whether the query images are panoramas or limited FoV images. Our experiments show that GeoDistill significantly improves localization performance across different frameworks. Additionally, we introduce a novel orientation estimation network that predicts relative orientation without requiring precise planar position ground truth. GeoDistill provides a scalable and efficient solution for real-world cross-view localization challenges. Code and model can be found at https://github.com/tongshw/GeoDistill.", 'score': 1, 'issue_id': 4942, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 июля', 'en': 'July 15', 'zh': '7月15日'}, 'hash': '8d8109c5462763ac', 'authors': ['Shaowen Tong', 'Zimin Xia', 'Alexandre Alahi', 'Xuming He', 'Yujiao Shi'], 'affiliations': ['Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland', 'ShanghaiTech University, China'], 'pdf_title_img': 'assets/pdf/title_img/2507.10935.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#cv'], 'emoji': '🌍', 'ru': {'title': 'GeoDistill: геометрия на службе кросс-видовой локализации', 'desc': 'GeoDistill - это новый подход к кросс-видовой локализации, использующий слабо контролируемое самообучение на основе геометрии. Метод применяет обучение по схеме учитель-ученик с маскированием по полю зрения для улучшения извлечения локальных признаков. Учитель локализует панорамное изображение, а ученик предсказывает положение по ограниченному полю зрения. Такой подход позволяет модели фокусироваться на ключевых особенностях и игнорировать бестекстурные области, повышая точность локализации.'}, 'en': {'title': 'GeoDistill: Enhancing Cross-View Localization with Weak Supervision', 'desc': "This paper presents GeoDistill, a novel framework for cross-view localization that estimates a camera's 3-DoF pose by aligning ground-level images with satellite images. It addresses the challenge of requiring expensive ground-truth pose annotations by employing a weakly supervised self-distillation approach. The framework utilizes a teacher-student model where the teacher localizes a panoramic image, while the student learns from a limited Field-of-View (FoV) version of the same image. By focusing on important features and ignoring irrelevant textures, GeoDistill enhances localization accuracy and reduces uncertainty, making it a scalable solution for outdoor applications like autonomous navigation."}, 'zh': {'title': 'GeoDistill：高效的跨视角定位解决方案', 'desc': '跨视角定位是通过将地面图像与卫星图像对齐来估计相机的三自由度姿态，这在自动导航和增强现实等大规模户外应用中至关重要。现有方法通常依赖于完全监督学习，这需要昂贵的真实姿态标注。我们提出了GeoDistill，一个几何引导的弱监督自蒸馏框架，利用教师-学生学习和基于视场(FoV)的掩蔽来增强局部特征学习，从而实现稳健的跨视角定位。GeoDistill通过对齐学生模型和教师模型的预测，帮助学生模型专注于关键特征，提高了定位精度并减少了不确定性。'}}}, {'id': 'https://huggingface.co/papers/2507.14102', 'title': 'UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography', 'url': 'https://huggingface.co/papers/2507.14102', 'abstract': 'Accurate classification of computed tomography (CT) images is essential for diagnosis and treatment planning, but existing methods often struggle with the subtle and spatially diverse nature of pathological features. Current approaches typically process images uniformly, limiting their ability to detect localized abnormalities that require focused analysis. We introduce UGPL, an uncertainty-guided progressive learning framework that performs a global-to-local analysis by first identifying regions of diagnostic ambiguity and then conducting detailed examination of these critical areas. Our approach employs evidential deep learning to quantify predictive uncertainty, guiding the extraction of informative patches through a non-maximum suppression mechanism that maintains spatial diversity. This progressive refinement strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate both contextual information and fine-grained details. Experiments across three CT datasets demonstrate that UGPL consistently outperforms state-of-the-art methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our analysis shows that the uncertainty-guided component provides substantial benefits, with performance dramatically increasing when the full progressive learning pipeline is implemented. Our code is available at: https://github.com/shravan-18/UGPL', 'score': 0, 'issue_id': 4945, 'pub_date': '2025-07-18', 'pub_date_card': {'ru': '18 июля', 'en': 'July 18', 'zh': '7月18日'}, 'hash': 'd4150eeda5b606d4', 'authors': ['Shravan Venkatraman', 'Pavan Kumar S', 'Rakesh Raj Madavan', 'Chandrakala S'], 'affiliations': ['Shiv Nadar University, Chennai, India', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2507.14102.jpg', 'data': {'categories': ['#healthcare', '#training', '#data'], 'emoji': '🔬', 'ru': {'title': 'UGPL: Умный анализ КТ от общего к частному', 'desc': 'UGPL - это новый метод анализа КТ-изображений, использующий принцип прогрессивного обучения от общего к частному. Система сначала выявляет области диагностической неопределенности, а затем детально анализирует эти критические участки. UGPL применяет эвиденциальное глубокое обучение для количественной оценки неопределенности прогнозов и извлечения информативных фрагментов изображения. Эксперименты показали, что UGPL превосходит современные методы в точности обнаружения аномалий почек, рака легких и COVID-19 на КТ-снимках.'}, 'en': {'title': 'Enhancing CT Image Classification with Uncertainty-Guided Learning', 'desc': 'This paper presents UGPL, a novel framework for improving the classification of CT images by focusing on areas of diagnostic uncertainty. Unlike traditional methods that analyze images uniformly, UGPL first identifies ambiguous regions and then conducts a detailed examination of these areas. It utilizes evidential deep learning to measure predictive uncertainty, which helps in selecting informative patches while preserving spatial diversity. The results show that UGPL significantly enhances accuracy in detecting kidney abnormalities, lung cancer, and COVID-19 compared to existing techniques.'}, 'zh': {'title': '不确定性引导的渐进学习，提升CT图像分类精度', 'desc': '本文提出了一种名为UGPL的不确定性引导渐进学习框架，用于提高计算机断层扫描（CT）图像的分类准确性。UGPL通过首先识别诊断模糊区域，然后对这些关键区域进行详细分析，实现了从全局到局部的分析。该方法利用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取信息丰富的图像块，保持空间多样性。实验结果表明，UGPL在肾脏异常、肺癌和COVID-19检测中均显著优于现有方法，准确率分别提高了3.29%、2.46%和8.08%。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (6)', '#agi (3)', '#alignment', '#architecture (1)', '#audio (1)', '#benchmark (10)', '#cv (5)', '#data (3)', '#dataset (5)', '#diffusion (3)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (2)', '#optimization (12)', '#plp', '#rag', '#reasoning (10)', '#rl (7)', '#rlhf (2)', '#robotics (2)', '#science (2)', '#security (1)', '#small_models', '#story_generation (1)', '#survey', '#synthetic (1)', '#training (12)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-22 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-22 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-22 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    