
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. January 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 ÑĞ½Ğ²Ğ°Ñ€Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-01-13.html">â¬…ï¸ <span id="prev-date">13.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-01-15.html">â¡ï¸ <span id="next-date">15.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-01.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 14', 'zh': '1æœˆ14æ—¥'};
        let feedDateNext = {'ru': '15.01', 'en': '01/15', 'zh': '1æœˆ15æ—¥'};
        let feedDatePrev = {'ru': '13.01', 'en': '01/13', 'zh': '1æœˆ13æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.07022', 'title': 'Solar Open Technical Report', 'url': 'https://huggingface.co/papers/2601.07022', 'abstract': 'Solar Open presents a 102B-parameter bilingual Mixture-of-Experts language model that addresses data scarcity in underserved languages through synthetic data generation, progressive curriculum coordination, and scalable reinforcement learning optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.', 'score': 42, 'issue_id': 568, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': 'f88e2ba7e40d5615', 'authors': ['Sungrae Park', 'Sanghoon Kim', 'Jungho Cho', 'Gyoungjin Gim', 'Dawoon Jung', 'Mikyoung Cha', 'Eunhae Choo', 'Taekgyu Hong', 'Minbyul Jeong', 'SeHwan Joo', 'Minsoo Khang', 'Eunwon Kim', 'Minjeong Kim', 'Sujeong Kim', 'Yunsu Kim', 'Hyeonju Lee', 'Seunghyun Lee', 'Sukyung Lee', 'Siyoung Park', 'Gyungin Shin', 'Inseo Song', 'Wonho Song', 'Seonghoon Yang', 'Seungyoun Yi', 'Sanghoon Yoon', 'Jeonghyun Ko', 'Seyoung Song', 'Keunwoo Choi', 'Hwalsuk Lee', 'Sunghun Kim', 'Du-Seong Chang', 'Kyunghyun Cho', 'Junsuk Choe', 'Hwaran Lee', 'Jae-Gil Lee', 'KyungTae Lim', 'Alice Oh'], 'affiliations': ['Upstage'], 'pdf_title_img': 'assets/pdf/title_img/2601.07022.jpg', 'data': {'categories': ['#low_resource', '#training', '#data', '#rlhf', '#synthetic', '#optimization', '#open_source', '#multilingual', '#reasoning', '#architecture'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ĞºĞ°Ğº Ğ¼Ğ¾ÑÑ‚ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ² Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Solar Open Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Mixture-of-Experts, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ°Ñ 102 Ğ¼Ğ»Ñ€Ğ´ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ´Ğ²Ğ° ÑĞ·Ñ‹ĞºĞ°. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 4.5 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ¸Ñ… ÑĞ¾ÑÑ‚Ğ°Ğ², ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ· 20 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ”Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ñ‘Ğ½ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ reinforcement learning Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ SnapPO, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ¾ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ².'}, 'en': {'title': "Empowering Underserved Languages with Solar Open's 102B-Parameter Model", 'desc': 'The paper introduces Solar Open, a large bilingual Mixture-of-Experts language model with 102 billion parameters, designed to improve AI capabilities in underserved languages. It tackles the challenge of data scarcity by generating 4.5 trillion tokens of synthetic, high-quality data tailored for specific domains. The model employs a progressive curriculum that optimizes the composition and quality of the training data across a vast dataset of 20 trillion tokens. Additionally, it utilizes a scalable reinforcement learning framework called SnapPO to enhance reasoning abilities, achieving competitive results in benchmarks for English and Korean.'}, 'zh': {'title': 'ä¸ºæ¬ æœåŠ¡è¯­è¨€å¼€è¾Ÿæ–°å¤©åœ°çš„Solar Open', 'desc': 'Solar Open æ˜¯ä¸€ä¸ªæ‹¥æœ‰1020äº¿å‚æ•°çš„åŒè¯­æ··åˆä¸“å®¶è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³èµ„æºåŒ®ä¹è¯­è¨€çš„æ•°æ®ä¸è¶³é—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡åˆæˆæ•°æ®ç”Ÿæˆã€æ¸è¿›å¼è¯¾ç¨‹åè°ƒå’Œå¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¥å®ç°é«˜æ•ˆè®­ç»ƒã€‚æˆ‘ä»¬åˆæˆäº†4.5ä¸‡äº¿ä¸ªé«˜è´¨é‡ã€ç‰¹å®šé¢†åŸŸå’Œé¢å‘å¼ºåŒ–å­¦ä¹ çš„æ•°æ®ï¼Œä»¥åº”å¯¹æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚é€šè¿‡åœ¨è‹±è¯­å’ŒéŸ©è¯­çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒSolar Open å±•ç¤ºäº†å…¶åœ¨æ¬ æœåŠ¡è¯­è¨€AIå¼€å‘ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08225', 'title': 'User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale', 'url': 'https://huggingface.co/papers/2601.08225', 'abstract': 'Large reasoning models enable scalable multi-turn dialogue generation through automated task-oriented simulation and user-oriented behavioral modeling for enhanced human-agent interaction datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.', 'score': 35, 'issue_id': 568, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '4d4a176d79a2d78d', 'authors': ['Jungho Cho', 'Minbyul Jeong', 'Sungrae Park'], 'affiliations': ['Upstage AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.08225.jpg', 'data': {'categories': ['#synthetic', '#reasoning'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒĞ³ÑƒĞ±Ğ¾ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸, Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ¼, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ, ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ‹ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Human-Agent Dialogue with Dynamic User Simulation', 'desc': 'This paper discusses the development of large reasoning models (LRMs) for improving multi-turn dialogue generation in human-agent interactions. It highlights the limitations of existing datasets that rely on static tools, which do not capture the complexity of real-world conversations. The authors propose a new framework that separates task generation from user simulation, allowing for more realistic and extended dialogues that mimic human behavior. This approach results in a scalable system that can generate diverse, high-density datasets for training agents in task-oriented scenarios.'}, 'zh': {'title': 'æå‡äººæœºäº¤äº’çš„å¤šè½®å¯¹è¯ç”Ÿæˆ', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹åœ¨å¤šè½®å¯¹è¯ç”Ÿæˆä¸­çš„åº”ç”¨ï¼Œæ—¨åœ¨æå‡äººæœºäº¤äº’çš„æ•°æ®é›†è´¨é‡ã€‚ç°æœ‰çš„æ•°æ®é›†å’Œç”Ÿæˆæ–¹æ³•å—é™äºé™æ€çš„å·¥å…·é›†ï¼Œæ— æ³•æ»¡è¶³å¼€æ”¾å¼äººæœºåä½œçš„å¤æ‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶è€…ä»¬å¼€å‘äº†ä¸€ç§åŸºäºLRMçš„è‡ªåŠ¨åŒ–ä»»åŠ¡å¯¼å‘å¯¹è¯ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤ŸåŠ¨æ€ç”Ÿæˆé«˜ä»·å€¼çš„é¢†åŸŸç‰¹å®šå·¥å…·ã€‚é€šè¿‡å¼•å…¥ç”¨æˆ·å¯¼å‘çš„æ¨¡æ‹Ÿæ–¹æ³•ï¼Œç ”ç©¶è€…ä»¬å®ç°äº†æ›´çœŸå®çš„å¤šè½®å¯¹è¯ï¼Œåæ˜ äº†ç°å®ä¸–ç•Œé—®é¢˜è§£å†³çš„è¿­ä»£ç‰¹æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06789', 'title': 'MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences', 'url': 'https://huggingface.co/papers/2601.06789', 'abstract': 'MemGovern framework transforms unstructured GitHub data into structured experiential memory for autonomous software engineering agents, improving bug resolution rates through enhanced experience retrieval.  \t\t\t\t\tAI-generated summary \t\t\t\t While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.', 'score': 32, 'issue_id': 567, 'pub_date': '2026-01-11', 'pub_date_card': {'ru': '11 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 11', 'zh': '1æœˆ11æ—¥'}, 'hash': '0aaa42ea5e9dedf0', 'authors': ['Qihao Wang', 'Ziming Cheng', 'Shuo Zhang', 'Fan Liu', 'Rui Xu', 'Heng Lian', 'Kunyi Wang', 'Xiaoming Yu', 'Jianghao Yin', 'Sen Hu', 'Yue Hu', 'Shaolei Zhang', 'Yanbing Liu', 'Ronghao Chen', 'Huacan Wang'], 'affiliations': ['ECNU', 'FDU', 'HKUST(GZ)', 'NUS', 'PKU', 'QuantaAlpha', 'RUC', 'UBC', 'UCAS', 'XDU'], 'pdf_title_img': 'assets/pdf/title_img/2601.06789.jpg', 'data': {'categories': [], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¿Ñ‹Ñ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° ĞºĞ°Ğº Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° MemGovern, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· GitHub Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ² ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ 135 Ñ‚Ñ‹ÑÑÑ‡ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½Ğ° 4.65% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Verified. MemGovern Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸, ÑƒĞ´Ğ¾Ğ±Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Transforming GitHub Data into Smart Memory for Better Bug Fixing', 'desc': 'The MemGovern framework enhances autonomous software engineering agents by converting unstructured GitHub data into structured experiential memory. This transformation allows agents to leverage historical human experiences, overcoming the limitations of relying solely on local context for bug resolution. By creating 135,000 governed experience cards, MemGovern facilitates logic-driven retrieval of relevant expertise, significantly improving bug resolution rates. This innovative approach provides a plug-in solution for integrating memory infrastructure into software engineering agents.'}, 'zh': {'title': 'æå‡è‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„ç»éªŒè®°å¿†', 'desc': 'MemGovernæ¡†æ¶å°†éç»“æ„åŒ–çš„GitHubæ•°æ®è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ç»éªŒè®°å¿†ï¼Œä»¥æ”¯æŒè‡ªä¸»è½¯ä»¶å·¥ç¨‹ä»£ç†çš„å·¥ä½œã€‚é€šè¿‡å¢å¼ºç»éªŒæ£€ç´¢ï¼ŒMemGovernæ˜¾è‘—æé«˜äº†é”™è¯¯è§£å†³ç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»éªŒæ²»ç†å°†äººç±»ç»éªŒè½¬æ¢ä¸ºé€‚åˆä»£ç†ä½¿ç”¨çš„ç»éªŒå¡ç‰‡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åŸºäºé€»è¾‘çš„ç»éªŒæœç´¢ç­–ç•¥ã€‚æœ€ç»ˆï¼ŒMemGovernç”Ÿæˆäº†135,000ä¸ªç»éªŒå¡ç‰‡ï¼Œä½¿å¾—åœ¨SWE-bench Verifiedä¸Šçš„è§£å†³ç‡æå‡äº†4.65%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2512.24965', 'title': 'ShowUI-Ï€: Flow-based Generative Models as GUI Dexterous Hands', 'url': 'https://huggingface.co/papers/2512.24965', 'abstract': "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop ShowUI-Ï€, the first flow-based generative model as GUI dexterous hand, featuring the following designs: (i) Unified Discrete-Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes; (ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories; (iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g. PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents' drag capabilities. Our experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g. Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-Ï€ achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. We hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.", 'score': 27, 'issue_id': 568, 'pub_date': '2026-12-31', 'pub_date_card': {'ru': '31 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 31', 'zh': '12æœˆ31æ—¥'}, 'hash': 'eee0e07745c9406a', 'authors': ['Siyuan Hu', 'Kevin Qinghong Lin', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2512.24965.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#agents', '#small_models', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°Ñ…: Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ ShowUI-Ï€ â€” Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ¼, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ°Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğº Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ ĞºĞ»Ğ¸ĞºĞ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¸ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ĞºÑƒÑ€ÑĞ¾Ñ€Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ 20 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ScreenDrag Ñ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ğ°Ğ¼Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ğº Ğ² Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ShowUI-Ï€ Ñ 450 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ² Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Empowering GUI Agents with Human-like Dexterity', 'desc': "This paper introduces ShowUI-Ï€, a novel flow-based generative model designed to enhance GUI agents' dexterous manipulation capabilities. Unlike traditional models that only predict discrete click actions, ShowUI-Ï€ integrates both discrete and continuous actions, allowing for more fluid interactions like dragging. The model utilizes a lightweight action expert to generate smooth cursor movements based on real-time visual input, improving the agent's adaptability across various tasks. The authors also present a new benchmark, ScreenDrag, to evaluate drag performance, demonstrating that ShowUI-Ï€ outperforms existing agents significantly in this challenging domain."}, 'zh': {'title': 'æå‡GUIæ™ºèƒ½ä»£ç†çš„çµæ´»æ“æ§èƒ½åŠ›', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºShowUI-Ï€çš„æµå¼ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰æ™ºèƒ½ä»£ç†çš„çµæ´»æ€§å’Œæ“æ§èƒ½åŠ›ã€‚è¯¥æ¨¡å‹ç»“åˆäº†ç¦»æ•£ç‚¹å‡»å’Œè¿ç»­æ‹–åŠ¨çš„åŠ¨ä½œï¼Œèƒ½å¤Ÿåœ¨å¤šç§äº¤äº’æ¨¡å¼ä¸‹è‡ªå¦‚é€‚åº”ã€‚é€šè¿‡æµå¼åŠ¨ä½œç”Ÿæˆï¼ŒShowUI-Ï€èƒ½å¤Ÿæ ¹æ®å®æ—¶è§†è§‰ä¿¡æ¯é¢„æµ‹å…‰æ ‡çš„ç»†å¾®è°ƒæ•´ï¼Œä»è€Œå®ç°å¹³æ»‘ç¨³å®šçš„æ‹–åŠ¨è½¨è¿¹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«2ä¸‡æ¡æ‹–åŠ¨è½¨è¿¹çš„æ•°æ®é›†å’Œè¯„ä¼°åŸºå‡†ScreenDragï¼Œä»¥æµ‹è¯•GUIä»£ç†çš„æ‹–åŠ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04745', 'title': 'KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions', 'url': 'https://huggingface.co/papers/2601.04745', 'abstract': "Long-horizon memory benchmarks based on autobiographical narratives evaluate models' ability to infer stable motivations and decision principles through evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \\BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \\BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in KnowMeBench{https://github.com/QuantaAlpha/KnowMeBench}.", 'score': 25, 'issue_id': 571, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '9ef26640427ffeb7', 'authors': ['Tingyu Wu', 'Zhisheng Chen', 'Ziyan Weng', 'Shuhe Wang', 'Chenglong Li', 'Shuo Zhang', 'Sen Hu', 'Silin Wu', 'Qizhen Lan', 'Huacan Wang', 'Ronghao Chen'], 'affiliations': ['CITYU-DG', 'HAINNU', 'NUS', 'PKU', 'QuantaAlpha', 'THU', 'UCAS', 'UTHealth'], 'pdf_title_img': 'assets/pdf/title_img/2601.04745.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#long_context'], 'emoji': 'ğŸ“–', 'ru': {'title': 'ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ñ‹: ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿Ğ¾Ğ¸ÑĞº Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº KnowMeBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ±Ğ¸Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ¾Ğ². Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚Ñ€Ñ‘Ñ… ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ñ„Ğ°ĞºÑ‚-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾-Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ²ĞµÑ€Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'Unlocking Long-Term Memory in AI with Autobiographical Narratives', 'desc': "This paper introduces a new benchmark called \\\\BenchName, designed to assess machine learning models' understanding of long-term memory through autobiographical narratives. Unlike previous benchmarks that relied on dialogues or synthetic histories, \\\\BenchName uses real-life stories to provide rich context for evaluating models' abilities to infer motivations and decision-making principles. The benchmark includes questions that require factual recall, understanding of subjective states, and reasoning at a principle level. The findings indicate that while retrieval-augmented systems can improve factual accuracy, they struggle with temporally grounded explanations and higher-level reasoning, suggesting a need for advanced memory mechanisms in AI."}, 'zh': {'title': 'é•¿æ—¶è®°å¿†è¯„ä¼°çš„æ–°åŸºå‡†æµ‹è¯•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æµ‹è¯•ï¼Œåä¸º\\BenchNameï¼Œæ—¨åœ¨è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨é•¿æ—¶é—´è®°å¿†æ–¹é¢çš„è¡¨ç°ã€‚è¯¥åŸºå‡†æµ‹è¯•åŸºäºè‡ªä¼ å™è¿°ï¼Œæä¾›äº†ä¸°å¯Œçš„è¯æ®æ¥æ¨æ–­ç¨³å®šçš„åŠ¨æœºå’Œå†³ç­–åŸåˆ™ã€‚ä¸ç°æœ‰çš„åŸºå‡†æµ‹è¯•ä¸åŒï¼Œ\\BenchNameå…³æ³¨äºäº‹å®å›å¿†ã€ä¸»è§‚çŠ¶æ€å½’å› å’ŒåŸåˆ™çº§æ¨ç†ç­‰æ–¹é¢ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿåœ¨äº‹å®å‡†ç¡®æ€§ä¸Šæœ‰æ‰€æé«˜ï¼Œä½†åœ¨æ—¶é—´åŸºç¡€çš„è§£é‡Šå’Œæ›´é«˜å±‚æ¬¡çš„æ¨ç†ä¸Šä»å­˜åœ¨é”™è¯¯ï¼Œå¼ºè°ƒäº†éœ€è¦è¶…è¶Šç®€å•æ£€ç´¢çš„è®°å¿†æœºåˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.06487', 'title': 'ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking', 'url': 'https://huggingface.co/papers/2601.06487', 'abstract': 'Reinforcement learning for large language model agents suffers from discrimination collapse in open-ended tasks due to pointwise scalar scoring, which ArenaRL addresses through relative ranking and pairwise evaluation mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.', 'score': 22, 'issue_id': 566, 'pub_date': '2026-01-10', 'pub_date_card': {'ru': '10 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 10', 'zh': '1æœˆ10æ—¥'}, 'hash': 'fa021a43db4c9cb5', 'authors': ['Qiang Zhang', 'Boli Chen', 'Fanrui Zhang', 'Ruixue Ding', 'Shihang Wang', 'Qiuchen Wang', 'Yinfeng Huang', 'Haonan Zhang', 'Rongxiang Zhu', 'Pengyong Wang', 'Ailin Ren', 'Xin Li', 'Pengjun Xie', 'Jiawei Liu', 'Ning Guo', 'Jingren Zhou', 'Zheng-Jun Zha'], 'affiliations': ['Amap, Alibaba Group', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2601.06487.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#rl', '#optimization', '#reasoning', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ†', 'ru': {'title': 'ĞÑ‚ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ArenaRL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ±ĞµĞ· Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, ĞºĞ¾Ğ³Ğ´Ğ° ÑĞºĞ°Ğ»ÑÑ€Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ½Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº ÑÑ‚Ğ°Ğ³Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ğ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ°Ñ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ O(N) Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ O(NÂ²). Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ´Ğ²Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Open-Travel Ğ¸ Open-DeepResearch, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°.'}, 'en': {'title': 'ArenaRL: Elevating Reinforcement Learning for Language Models through Relative Ranking', 'desc': 'This paper discusses the challenges of using reinforcement learning (RL) for large language model (LLM) agents in open-ended tasks, where traditional pointwise scalar scoring leads to discrimination collapse. The authors introduce ArenaRL, a new RL framework that replaces scalar scoring with relative ranking and pairwise evaluations to better differentiate between various solutions. By implementing a tournament-based ranking system and multi-level rubrics, ArenaRL provides more stable and precise reward signals, enhancing the optimization process. The results demonstrate that ArenaRL significantly improves the performance of LLM agents on complex tasks compared to standard RL methods.'}, 'zh': {'title': 'ArenaRLï¼šæå‡å¼€æ”¾å¼ä»»åŠ¡çš„å¼ºåŒ–å­¦ä¹ æ•ˆæœ', 'desc': 'å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„å¼€æ”¾å¼ä»»åŠ¡ä¸­é¢ä¸´ç€è¯„åˆ†æ­§è§†å´©æºƒçš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹æ ‡é‡è¯„åˆ†æ–¹æ³•éš¾ä»¥æœ‰æ•ˆåŒºåˆ†ä¸åŒè·¯å¾„ä¹‹é—´çš„ç»†å¾®ä¼˜åŠ¿ï¼Œå¯¼è‡´å¥–åŠ±ä¿¡å·è¢«å™ªå£°ä¸»å¯¼ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒArenaRLæå‡ºäº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹æ’åå’Œæˆå¯¹è¯„ä¼°æœºåˆ¶æ¥ä¼˜åŒ–è¯„åˆ†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒArenaRLåœ¨å¤æ‚ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºæ ‡å‡†å¼ºåŒ–å­¦ä¹ åŸºçº¿ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¼ºå¥çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.07264', 'title': 'The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents', 'url': 'https://huggingface.co/papers/2601.07264', 'abstract': "Tool-integrated language model agents exhibit different calibration behaviors based on tool type, with a reinforcement learning framework improving both task accuracy and reliable uncertainty estimation across diverse domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent's ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.", 'score': 15, 'issue_id': 566, 'pub_date': '2026-01-12', 'pub_date_card': {'ru': '12 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 12', 'zh': '1æœˆ12æ—¥'}, 'hash': '3b69d6831de81c4b', 'authors': ['Weihao Xuan', 'Qingcheng Zeng', 'Heli Qi', 'Yunze Xiao', 'Junjue Wang', 'Naoto Yokoya'], 'affiliations': ['Carnegie Mellon University', 'Northwestern University', 'RIKEN AIP', 'The University of Tokyo', 'Waseda University'], 'pdf_title_img': 'assets/pdf/title_img/2601.07264.jpg', 'data': {'categories': ['#benchmark', '#agents', '#rl'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ¶Ğ°Ñ‚ÑŒ Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸ (ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞ²Ğ¾Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…) Ğ² LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºÑƒ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·-Ğ·Ğ° ÑˆÑƒĞ¼Ğ° Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ´Ğ°) Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ÑÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ²ĞµĞ±-ÑÑ€ĞµĞ´Ñ‹ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Enhancing Trustworthiness in AI: Calibration through Reinforcement Learning', 'desc': "This paper explores how tool-integrated language model agents behave differently in terms of calibration, which is their ability to express confidence accurately about their performance. It identifies that different types of tools, like evidence tools and verification tools, affect the agents' confidence levels, with evidence tools often leading to overconfidence due to noisy information. The authors propose a reinforcement learning framework that enhances both the accuracy of tasks and the reliability of uncertainty estimation across various domains. Their findings emphasize the importance of tailored calibration strategies for agents that use different tools, paving the way for more trustworthy AI systems in real-world applications."}, 'zh': {'title': 'æå‡å·¥å…·é›†æˆä»£ç†çš„æ ¡å‡†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†é›†æˆå·¥å…·çš„è¯­è¨€æ¨¡å‹ä»£ç†åœ¨ä¸åŒå·¥å…·ç±»å‹ä¸‹çš„æ ¡å‡†è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯æ®å·¥å…·ï¼ˆå¦‚ç½‘ç»œæœç´¢ï¼‰ä¼šå¯¼è‡´è¿‡åº¦è‡ªä¿¡ï¼Œè€ŒéªŒè¯å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ï¼‰åˆ™èƒ½é€šè¿‡ç¡®å®šæ€§åé¦ˆæ¥æ”¹å–„æ ¡å‡†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åŒæ—¶ä¼˜åŒ–ä»»åŠ¡å‡†ç¡®æ€§å’Œæ ¡å‡†æ•ˆæœã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„ä»£ç†åœ¨å¤šç§é¢†åŸŸä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ ¡å‡†èƒ½åŠ›å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08584', 'title': 'Ministral 3', 'url': 'https://huggingface.co/papers/2601.08584', 'abstract': 'The Ministral 3 series consists of parameter-efficient dense language models with three sizes (3B, 8B, 14B) and three variants per size, trained using cascade distillation for compute-constrained applications.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.', 'score': 13, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '3895109cdeca63b2', 'authors': ['Alexander H. Liu', 'Kartik Khandelwal', 'Sandeep Subramanian', 'Victor Jouault', 'Abhinav Rastogi', 'Adrien SadÃ©', 'Alan Jeffares', 'Albert Jiang', 'Alexandre Cahill', 'Alexandre Gavaudan', 'Alexandre Sablayrolles', 'AmÃ©lie HÃ©liou', 'Amos You', 'Andy Ehrenberg', 'Andy Lo', 'Anton Eliseev', 'Antonia Calvi', 'Avinash Sooriyarachchi', 'Baptiste Bout', 'Baptiste RoziÃ¨re', 'Baudouin De Monicault', 'ClÃ©mence Lanfranchi', 'Corentin Barreau', 'Cyprien Courtot', 'Daniele Grattarola', 'Darius Dabert', 'Diego de las Casas', 'Elliot Chane-Sane', 'Faruk Ahmed', 'Gabrielle Berrada', 'GaÃ«tan Ecrepont', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Kunsch', 'Guillaume Lample', 'Guillaume Martin', 'Gunshi Gupta', 'Jan Ludziejewski', 'Jason Rute', 'Joachim Studnia', 'Jonas Amar', 'JosÃ©phine Delas', 'Josselin Somerville Roberts', 'Karmesh Yadav', 'Khyathi Chandu', 'Kush Jain', 'Laurence Aitchison', 'Laurent Fainsin', 'LÃ©onard Blier', 'Lingxiao Zhao', 'Louis Martin', 'Lucile Saulnier', 'Luyu Gao', 'Maarten Buyl', 'Margaret Jennings', 'Marie Pellat', 'Mark Prins', 'Mathieu PoirÃ©e', 'Mathilde Guillaumin', 'Matthieu Dinot', 'Matthieu Futeral', 'Maxime Darrin', 'Maximilian Augustin', 'Mia Chiquier', 'Michel Schimpf', 'Nathan Grinsztajn', 'Neha Gupta', 'Nikhil Raghuraman', 'Olivier Bousquet', 'Olivier Duchenne', 'Patricia Wang', 'Patrick von Platen', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'Pavankumar Reddy Muddireddy', 'PhilomÃ¨ne Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'Quentin Torroba', 'Romain Sauvestre', 'Roman Soletskyi', 'Rupert Menneer', 'Sagar Vaze', 'Samuel Barry', 'Sanchit Gandhi', 'Siddhant Waghjale', 'Siddharth Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Sumukh Aithal', 'Szymon Antoniak', 'Teven Le Scao', 'ThÃ©o Cachet', 'Theo Simon Sorg', 'Thibaut Lavril', 'Thiziri Nait Saada', 'Thomas Chabal', 'Thomas Foubert', 'Thomas Robert', 'Thomas Wang', 'Tim Lawson', 'Tom Bewley', 'Tom Bewley', 'Tom Edwards', 'Umar Jamil', 'Umberto Tomasini', 'Valeriia Nemychnikova', 'Van Phung', 'Vincent MaladiÃ¨re', 'Virgile Richard', 'Wassim Bouaziz', 'Wen-Ding Li', 'William Marshall', 'Xinghui Li', 'Xinyu Yang', 'Yassine El Ouahidi', 'Yihan Wang', 'Yunhao Tang', 'Zaccharie Ramzi'], 'affiliations': ['Mistral AI'], 'pdf_title_img': 'assets/pdf/title_img/2601.08584.jpg', 'data': {'categories': [], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ministral 3 â€” ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ¾Ñ‚ 3 Ğ´Ğ¾ 14 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ²ĞµÑ€ÑĞ¸Ñ Ñ fine-tuning Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑÑ… Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ cascade distillation â€” Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ¸ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑÑ‚ÑÑ Ğ¿Ğ¾Ğ´ Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸ĞµĞ¹ Apache 2.0.'}, 'en': {'title': 'Efficient Language Models for Resource-Constrained Applications', 'desc': 'The Ministral 3 series introduces a set of dense language models that are efficient in terms of parameters, making them suitable for applications with limited computational resources. Each model size, which includes 3B, 8B, and 14B parameters, offers three distinct variants: a general-purpose pretrained model, an instruction-tuned model for specific tasks, and a reasoning model aimed at solving complex problems. The models are developed using a technique called Cascade Distillation, which involves iterative pruning and continued training to enhance performance. Additionally, these models are equipped with image understanding capabilities and are released under the Apache 2.0 license.'}, 'zh': {'title': 'é«˜æ•ˆå¯†é›†è¯­è¨€æ¨¡å‹ï¼ŒåŠ©åŠ›è®¡ç®—å—é™åº”ç”¨', 'desc': 'Ministral 3ç³»åˆ—æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¯†é›†è¯­è¨€æ¨¡å‹å®¶æ—ï¼Œä¸“ä¸ºè®¡ç®—å’Œå†…å­˜å—é™çš„åº”ç”¨è€Œè®¾è®¡ã€‚è¯¥ç³»åˆ—åŒ…æ‹¬ä¸‰ç§æ¨¡å‹å¤§å°ï¼š3Bã€8Bå’Œ14Bå‚æ•°ï¼Œæ¯ç§å¤§å°éƒ½æœ‰ä¸‰ä¸ªå˜ä½“ï¼Œåˆ†åˆ«æ˜¯é€šç”¨é¢„è®­ç»ƒæ¨¡å‹ã€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œå¤æ‚é—®é¢˜è§£å†³æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨çº§è”è’¸é¦çš„æ–¹æ³•æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§è¿­ä»£å‰ªæå’ŒæŒç»­è®­ç»ƒçš„æŠ€æœ¯ã€‚æ¯ä¸ªæ¨¡å‹éƒ½å…·å¤‡å›¾åƒç†è§£èƒ½åŠ›ï¼Œå¹¶åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08828', 'title': 'Motion Attribution for Video Generation', 'url': 'https://huggingface.co/papers/2601.08828', 'abstract': 'Motive is a gradient-based data attribution framework that identifies influential video clips for motion improvement in text-to-video models through motion-weighted loss masking.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.', 'score': 4, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': 'b846910920901a8a', 'authors': ['Xindi Wu', 'Despoina Paschalidou', 'Jun Gao', 'Antonio Torralba', 'Laura Leal-TaixÃ©', 'Olga Russakovsky', 'Sanja Fidler', 'Jonathan Lorraine'], 'affiliations': ['MIT CSAIL', 'NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08828.jpg', 'data': {'categories': ['#video', '#data', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Motive â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ»Ğ¸ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… text-to-video. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ°ÑĞºĞ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ²Ğ·Ğ²ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ½Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰ÑƒÑ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Motive Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ»Ğ¸Ğ¿Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¸Ğ»Ğ¸ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Motive, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ³Ğ»Ğ°Ğ´ĞºĞ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° 74.1% ÑĞ¾Ğ³Ğ»Ğ°ÑĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼ Ğ»ÑĞ´ĞµĞ¹.'}, 'en': {'title': 'Motive: Enhancing Motion in Video Generation through Data Attribution', 'desc': 'Motive is a novel framework designed to analyze and improve motion in text-to-video generation models. It uses a gradient-based approach to identify which video clips significantly influence motion dynamics, separating motion effects from static visuals. By applying motion-weighted loss masking, Motive efficiently computes the impact of different clips on temporal consistency. This method not only enhances the quality of generated videos but also guides the selection of training data to optimize motion characteristics, achieving notable improvements in user preference tests.'}, 'zh': {'title': 'Motiveï¼šæå‡è§†é¢‘ç”Ÿæˆæ¨¡å‹è¿åŠ¨è¡¨ç°çš„å…³é”®', 'desc': 'Motiveæ˜¯ä¸€ä¸ªåŸºäºæ¢¯åº¦çš„æ•°æ®å½’å› æ¡†æ¶ï¼Œæ—¨åœ¨è¯†åˆ«å¯¹æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­è¿åŠ¨æ”¹è¿›æœ‰å½±å“çš„è§†é¢‘ç‰‡æ®µã€‚è¯¥æ¡†æ¶é€šè¿‡è¿åŠ¨åŠ æƒæŸå¤±æ©ç ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°ä»é™æ€å¤–è§‚ä¸­åˆ†ç¦»å‡ºæ—¶é—´åŠ¨æ€ã€‚Motiveå¯ä»¥å¤„ç†ç°ä»£å¤§å‹é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ï¼Œå¸®åŠ©ç ”ç©¶å“ªäº›å¾®è°ƒç‰‡æ®µèƒ½å¤Ÿæ”¹å–„æˆ–æ¶åŒ–æ—¶é—´åŠ¨æ€ã€‚é€šè¿‡Motiveé€‰æ‹©çš„é«˜å½±å“æ•°æ®ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿åŠ¨å¹³æ»‘æ€§å’ŒåŠ¨æ€ç¨‹åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08587', 'title': 'End-to-End Video Character Replacement without Structural Guidance', 'url': 'https://huggingface.co/papers/2601.08587', 'abstract': 'MoCha enables controllable video character replacement using a single frame mask through condition-aware RoPE and a comprehensive data construction pipeline with specialized datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha', 'score': 4, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '07a6935bace7df25', 'authors': ['Zhengbo Xu', 'Jie Ma', 'Ziheng Wang', 'Zhan Peng', 'Jun Liang', 'Jing Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.08587.jpg', 'data': {'categories': ['#rl', '#multimodal', '#data', '#dataset', '#video', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞµ Ğ±ĞµĞ· Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹', 'desc': 'MoCha â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ñƒ Ğ¼Ğ°ÑĞºÑƒ Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ€Ğ°Ğ½ĞµĞµ Ğ¼Ğ°ÑĞ¾Ğº Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ condition-aware RoPE Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¸Ñ†Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ½ĞµÑ…Ğ²Ğ°Ñ‚ĞºĞ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ²: ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ² Unreal Engine 5, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Video Character Replacement with MoCha', 'desc': 'MoCha is a novel framework for replacing characters in videos using just a single frame mask, addressing the challenges of previous methods that needed detailed per-frame segmentation. It introduces a condition-aware RoPE to adapt to various input conditions and enhance facial identity, while also incorporating a reinforcement learning-based post-training stage for better results. The framework is supported by a comprehensive data construction pipeline that includes three specialized datasets to mitigate the lack of paired video data. Experimental results show that MoCha significantly outperforms existing methods, making it a promising solution for controllable video character replacement.'}, 'zh': {'title': 'MoChaï¼šå•å¸§æ©ç å®ç°å¯æ§è§†é¢‘è§’è‰²æ›¿æ¢', 'desc': 'MoChaæ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿé€šè¿‡å•å¸§æ©ç å®ç°å¯æ§çš„è§†é¢‘è§’è‰²æ›¿æ¢ã€‚è¯¥æ–¹æ³•å…‹æœäº†ä»¥å¾€éœ€è¦é€å¸§åˆ†å‰²æ©ç å’Œç»“æ„æŒ‡å¯¼çš„é™åˆ¶ï¼Œé€‚åº”å¤æ‚åœºæ™¯ä¸­çš„é®æŒ¡å’Œè§’è‰²äº¤äº’ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¡ä»¶æ„ŸçŸ¥çš„RoPEå’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„åè®­ç»ƒé˜¶æ®µï¼Œä»¥å¢å¼ºé¢éƒ¨èº«ä»½çš„é€‚åº”æ€§ã€‚é€šè¿‡æ„å»ºé«˜ä¿çœŸåº¦çš„æ•°æ®é›†å’Œå…¶ä»–ä¸“é—¨æ•°æ®é›†ï¼ŒMoChaåœ¨å®éªŒä¸­æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08665', 'title': 'VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory', 'url': 'https://huggingface.co/papers/2601.08665', 'abstract': 'VLingNav enhances embodied navigation through linguistic-driven cognition with adaptive reasoning and visual-assisted memory, achieving state-of-the-art performance and zero-shot transfer to real robots.  \t\t\t\t\tAI-generated summary \t\t\t\t VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.', 'score': 3, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '80a64f457ad37ab4', 'authors': ['Shaoan Wang', 'Yuanfei Luo', 'Xingyu Chen', 'Aocheng Luo', 'Dongyue Li', 'Chang Liu', 'Sheng Chen', 'Yangang Zhang', 'Junzhi Yu'], 'affiliations': ['ByteDance Seed', 'Peking University', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2601.08665.jpg', 'data': {'categories': ['#rl', '#transfer_learning', '#multimodal', '#reasoning', '#dataset', '#robotics', '#agents', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ: ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚ Ğ´ÑƒĞ¼Ğ°ĞµÑ‚, Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ', 'desc': 'VLingNav â€” ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ³Ğ´Ğ° ÑÑ‚Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ”Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ÑƒĞ»ĞµĞ²ÑƒÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„ĞµÑ€Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ñ‹.'}, 'en': {'title': 'Empowering Navigation with Linguistic Reasoning and Memory', 'desc': 'VLingNav is a novel model designed to improve embodied navigation by integrating linguistic-driven cognition with advanced reasoning and memory capabilities. It introduces an adaptive chain-of-thought mechanism that allows the agent to switch between quick, intuitive actions and more thoughtful planning as needed. Additionally, it features a visual-assisted memory module that helps the agent remember past observations, reducing repetitive actions and enhancing navigation in dynamic environments. The model has been trained on a large dataset with reasoning annotations and has shown exceptional performance in both simulated and real-world robotic navigation tasks, achieving zero-shot transfer capabilities.'}, 'zh': {'title': 'è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½å¯¼èˆªæ–°çªç ´', 'desc': 'VLingNavæ˜¯ä¸€ç§åŸºäºè¯­è¨€é©±åŠ¨è®¤çŸ¥çš„ä½“æ„Ÿå¯¼èˆªæ¨¡å‹ï¼Œæ—¨åœ¨æå‡å¯¼èˆªèƒ½åŠ›ã€‚å®ƒå¼•å…¥äº†è‡ªé€‚åº”æ€ç»´é“¾æœºåˆ¶ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨å¿«é€Ÿç›´è§‰æ‰§è¡Œå’Œæ…¢é€Ÿæ·±æ€ç†Ÿè™‘ä¹‹é—´çµæ´»åˆ‡æ¢ã€‚ä¸ºäº†å¤„ç†é•¿æ—¶é—´çš„ç©ºé—´ä¾èµ–ï¼ŒVLingNavå¼€å‘äº†è§†è§‰è¾…åŠ©è¯­è¨€è®°å¿†æ¨¡å—ï¼Œå¸®åŠ©æ™ºèƒ½ä½“å›å¿†è¿‡å»çš„è§‚å¯Ÿï¼Œé¿å…é‡å¤æ¢ç´¢ã€‚é€šè¿‡æ„å»ºNav-AdaCoT-2.9Mæ•°æ®é›†å’Œåœ¨çº¿ä¸“å®¶æŒ‡å¯¼çš„å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼ŒVLingNavåœ¨å¤šç§ä½“æ„Ÿå¯¼èˆªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶èƒ½åœ¨é›¶æ ·æœ¬æƒ…å†µä¸‹è¿ç§»åˆ°çœŸå®æœºå™¨äººä¸Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08468', 'title': 'JudgeRLVR: Judge First, Generate Second for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2601.08468', 'abstract': 'Reinforcement learning with verifiable rewards is enhanced through a judge-then-generate paradigm that improves both efficiency and accuracy in mathematical problem-solving.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.', 'score': 1, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '3af6b50faf280f19', 'authors': ['Jiangshan Duo', 'Hanyu Li', 'Hailin Zhang', 'Yudong Wang', 'Sujian Li', 'Liang Zhao'], 'affiliations': ['CFCS, School of Computer Science, Peking University', 'LLM-Core Xiaomi', 'State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.08468.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#reasoning', '#math', '#training'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡ÑƒĞ´ÑŒÑ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ JudgeRLVR â€” Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ (Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ ÑÑƒĞ´ÑŒĞ¸), Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ ÑƒÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ¿Ğµ Ñ‚Ğ° Ğ¶Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ ÑÑƒĞ´ÑŒĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ JudgeRLVR Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ: ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° +3.7 Ğ±Ğ°Ğ»Ğ»Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 42% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'JudgeRLVR: Efficient and Accurate Reinforcement Learning for Math Problem-Solving', 'desc': 'This paper introduces JudgeRLVR, a novel approach in reinforcement learning that enhances the efficiency and accuracy of mathematical problem-solving. It operates in two stages: first, the model learns to evaluate solution responses for correctness, and then it generates solutions based on this learned judgment. By distinguishing valid solutions, the model reduces unnecessary exploration and focuses on more promising paths, improving both the quality of answers and the speed of generation. The results show significant improvements in accuracy and reduced verbosity compared to traditional methods, demonstrating better generalization across different problem domains.'}, 'zh': {'title': 'åˆ¤åˆ«-ç”ŸæˆèŒƒå¼æå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºJudgeRLVRçš„ä¸¤é˜¶æ®µåˆ¤åˆ«-ç”ŸæˆèŒƒå¼ï¼Œä»¥æé«˜å¼ºåŒ–å­¦ä¹ ä¸­å¯éªŒè¯å¥–åŠ±çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ åˆ¤æ–­å…·æœ‰å¯éªŒè¯ç­”æ¡ˆçš„è§£å†³æ–¹æ¡ˆå“åº”ï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹åœ¨åˆ¤åˆ«çš„åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒç”Ÿæˆã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼ŒJudgeRLVRåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­å®ç°äº†æ›´å¥½çš„è´¨é‡ä¸æ•ˆç‡å¹³è¡¡ï¼Œæ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡å¹¶å‡å°‘äº†ç”Ÿæˆé•¿åº¦ã€‚è¯¥æ–¹æ³•åœ¨é¢†åŸŸå†…å’Œé¢†åŸŸå¤–çš„åŸºå‡†æµ‹è¯•ä¸­å‡è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.08303', 'title': 'SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices', 'url': 'https://huggingface.co/papers/2601.08303', 'abstract': 'An efficient diffusion transformer framework for mobile and edge devices that maintains high-generation quality while reducing computational costs through compact architecture, elastic training, and knowledge-guided distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.', 'score': 1, 'issue_id': 566, 'pub_date': '2026-01-13', 'pub_date_card': {'ru': '13 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 13', 'zh': '1æœˆ13æ—¥'}, 'hash': '2bf6eff80c892659', 'authors': ['Dongting Hu', 'Aarush Gupta', 'Magzhan Gabidolla', 'Arpit Sahni', 'Huseyin Coskun', 'Yanyu Li', 'Yerlan Idelbayev', 'Ahsan Mahmood', 'Aleksei Lebedev', 'Dishani Lahiri', 'Anujraaj Goyal', 'Ju Hu', 'Mingming Gong', 'Sergey Tulyakov', 'Anil Kag'], 'affiliations': ['MBZUAI', 'Snap Inc.', 'The University of Melbourne'], 'pdf_title_img': 'assets/pdf/title_img/2601.08303.jpg', 'data': {'categories': ['#architecture', '#small_models', '#training', '#inference'], 'emoji': 'ğŸ“±', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ² Ğ½Ğ° Ğ»Ğ°Ğ´Ğ¾Ğ½Ğ¸', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ°Ñ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ ÑÑƒĞ¿ĞµÑ€ÑĞµÑ‚Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Knowledge-Guided Distribution Matching Distillation Ğ´Ğ»Ñ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞµ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ²Ñ‘Ñ€Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ… Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Efficient Diffusion Transformers for Mobile and Edge Devices', 'desc': "This paper introduces an efficient diffusion transformer (DiT) framework designed specifically for mobile and edge devices, addressing the high computational demands of traditional models. It features a compact architecture that utilizes a global-local sparse attention mechanism to effectively balance the need for global context and local detail in image generation. The framework also includes an elastic training approach that allows the model to adapt its capacity based on the hardware it runs on, optimizing performance without sacrificing quality. Additionally, the authors implement a Knowledge-Guided Distribution Matching Distillation method to enhance the model's efficiency and fidelity, enabling real-time image generation on resource-constrained devices."}, 'zh': {'title': 'é«˜æ•ˆæ‰©æ•£å˜æ¢å™¨ï¼šç§»åŠ¨è®¾å¤‡ä¸Šçš„é«˜è´¨é‡ç”Ÿæˆ', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œä¸“ä¸ºç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡è®¾è®¡ï¼Œæ—¨åœ¨åœ¨ä¸¥æ ¼çš„èµ„æºé™åˆ¶ä¸‹ä¿æŒé«˜è´¨é‡çš„å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç´§å‡‘çš„æ‰©æ•£å˜æ¢å™¨æ¶æ„ï¼Œç»“åˆäº†è‡ªé€‚åº”çš„å…¨å±€-å±€éƒ¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œå±€éƒ¨ç»†èŠ‚ä¿ç•™ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼¹æ€§è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ç»Ÿä¸€çš„è¶…ç½‘ç»œä¸­ä¼˜åŒ–ä¸åŒå®¹é‡çš„å­æ‰©æ•£å˜æ¢å™¨ï¼Œä»è€Œä½¿å•ä¸ªæ¨¡å‹èƒ½å¤Ÿæ ¹æ®ä¸åŒç¡¬ä»¶åŠ¨æ€è°ƒæ•´ä»¥å®ç°é«˜æ•ˆæ¨ç†ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†çŸ¥è¯†å¼•å¯¼çš„åˆ†å¸ƒåŒ¹é…è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡ä¸å°‘æ­¥æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç›¸ç»“åˆï¼Œç”Ÿæˆé«˜ä¿çœŸã€ä½å»¶è¿Ÿçš„å›¾åƒï¼Œé€‚åˆå®æ—¶è®¾å¤‡ä½¿ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.04582', 'title': 'Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization', 'url': 'https://huggingface.co/papers/2601.04582', 'abstract': 'A reinforcement learning framework for text-to-visualization generation that improves chart quality and code execution by optimizing multiple objectives using post-execution feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.', 'score': 0, 'issue_id': 566, 'pub_date': '2026-01-08', 'pub_date_card': {'ru': '8 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 8', 'zh': '1æœˆ8æ—¥'}, 'hash': '596be1c607cd146a', 'authors': ['Mizanur Rahman', 'Mohammed Saidul Islam', 'Md Tahmid Rahman Laskar', 'Shafiq Joty', 'Enamul Hoque'], 'affiliations': ['Nanyang Technological University', 'Salesforce AI Research', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2601.04582.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#rl', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RL-Text2Vis â€” Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Group Relative Policy Optimization (GRPO) Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ´Ğ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ 22% Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GPT-4o Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ° Ñ 78% Ğ´Ğ¾ 97%. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ GRPO Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Reinforcement Learning for Enhanced Text-to-Visualization Generation', 'desc': 'This paper introduces RL-Text2Vis, a novel reinforcement learning framework designed to enhance the generation of visualizations from text queries. It addresses the limitations of existing models by optimizing multiple objectives, including textual accuracy, code validity, and visualization quality, using feedback obtained after code execution. The framework employs Group Relative Policy Optimization (GRPO) to improve the quality of charts generated, achieving a significant increase in both chart quality and code execution success rates. The results demonstrate that RL-Text2Vis outperforms traditional methods and shows strong generalization capabilities across different datasets.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡æ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬åˆ°å¯è§†åŒ–ç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºRL-Text2Visã€‚è¯¥æ¡†æ¶é€šè¿‡ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼Œåˆ©ç”¨æ‰§è¡Œåçš„åé¦ˆæ¥æé«˜å›¾è¡¨è´¨é‡å’Œä»£ç æ‰§è¡Œæ•ˆæœã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒRL-Text2Visé‡‡ç”¨äº†ä¸€ç§æ–°çš„å¤šç›®æ ‡å¥–åŠ±æœºåˆ¶ï¼Œèƒ½å¤ŸåŒæ—¶ä¼˜åŒ–æ–‡æœ¬å‡†ç¡®æ€§ã€ä»£ç æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRL-Text2Visåœ¨å›¾è¡¨è´¨é‡ä¸Šç›¸è¾ƒäºGPT-4oæœ‰22%çš„ç›¸å¯¹æå‡ï¼Œä»£ç æ‰§è¡ŒæˆåŠŸç‡ä»78%æé«˜åˆ°97%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.02669', 'title': 'Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking', 'url': 'https://huggingface.co/papers/2601.02669', 'abstract': "FactArena is an automated evaluation framework that comprehensively assesses large language models across all stages of the fact-checking pipeline, revealing gaps between claim verification accuracy and overall fact-checking capability.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.", 'score': 0, 'issue_id': 571, 'pub_date': '2026-01-06', 'pub_date_card': {'ru': '6 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 6', 'zh': '1æœˆ6æ—¥'}, 'hash': '9a7c27380caaf64c', 'authors': ['Hongzhan Lin', 'Zixin Chen', 'Zhiqi Shen', 'Ziyang Luo', 'Zhen Ye', 'Jing Ma', 'Tat-Seng Chua', 'Guandong Xu'], 'affiliations': ['Department of Computer Science, Hong Kong Baptist University', 'National University of Singapore', 'Salesforce AI Research', 'The Education University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.02669.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞÑ‚ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğº Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ñ‹', 'desc': 'FactArena Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²ÑĞµÑ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM-ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ° Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ Ğ°Ñ€ĞµĞ½Ñ‹-ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 16 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ². Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ñ„Ğ°ĞºÑ‚-Ñ‡ĞµĞºĞ¸Ğ½Ğ³Ğ°.'}, 'en': {'title': 'FactArena: Holistic Evaluation for Reliable Fact-Checking in LLMs', 'desc': "FactArena is a new evaluation framework designed to assess large language models (LLMs) throughout the entire fact-checking process, not just their ability to verify claims. It identifies weaknesses in LLMs by examining their performance in claim extraction, evidence retrieval, and justification of verdicts. The framework includes a standardized process for fact-checking, a fair judgment system for comparing models, and a module that generates challenging claims to test LLMs' robustness. By revealing gaps between claim verification accuracy and overall fact-checking ability, FactArena aims to improve the reliability of LLMs in critical applications."}, 'zh': {'title': 'å…¨é¢è¯„ä¼°è¯­è¨€æ¨¡å‹çš„äº‹å®æ ¸æŸ¥èƒ½åŠ›', 'desc': 'FactArenaæ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ï¼Œå…¨é¢è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨äº‹å®æ ¸æŸ¥æµç¨‹ä¸­çš„å„ä¸ªé˜¶æ®µï¼Œæ­ç¤ºäº†å£°æ˜éªŒè¯å‡†ç¡®æ€§ä¸æ•´ä½“äº‹å®æ ¸æŸ¥èƒ½åŠ›ä¹‹é—´çš„å·®è·ã€‚ç°æœ‰è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨å£°æ˜éªŒè¯ï¼Œå¿½è§†äº†æ›´å¹¿æ³›çš„äº‹å®æ ¸æŸ¥å·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬å£°æ˜æå–å’Œè¯æ®æ£€ç´¢ã€‚FactArenaé€šè¿‡æ ‡å‡†åŒ–çš„å£°æ˜åˆ†è§£ã€å·¥å…·å¢å¼ºçš„è¯æ®æ£€ç´¢å’ŒåŸºäºç†ç”±çš„åˆ¤å†³é¢„æµ‹ï¼Œæä¾›äº†å…¨é¢çš„é˜¶æ®µæ€§åŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶ä¸ºè¯Šæ–­å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®æ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„èŒƒå¼ï¼Œæ¨åŠ¨äº†åœ¨å®‰å…¨å…³é”®çš„äº‹å®æ ¸æŸ¥åº”ç”¨ä¸­å¯é éƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›å±•ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (4)', '#agi', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (4)', '#cv', '#data (3)', '#dataset (4)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (3)', '#open_source (3)', '#optimization (4)', '#plp', '#rag', '#reasoning (6)', '#rl (6)', '#rlhf (2)', '#robotics (1)', '#science', '#security', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (7)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-01-14 10:27',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-14 10:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-14 10:27')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    