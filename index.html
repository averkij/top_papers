
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. October 8.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-07.html">â¬…ï¸ <span id="prev-date">07.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-09.html">â¡ï¸ <span id="next-date">09.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '8 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 8', 'zh': '10æœˆ8æ—¥'};
        let feedDateNext = {'ru': '09.10', 'en': '10/09', 'zh': '10æœˆ9æ—¥'};
        let feedDatePrev = {'ru': '07.10', 'en': '10/07', 'zh': '10æœˆ7æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.06217', 'title': 'TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.06217', 'abstract': 'TaTToo, a novel table-grounded Process Reward Model, enhances tabular reasoning by explicitly addressing table-specific operations and integrating tool-based verification, leading to significant performance improvements over existing PRMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Process Reward Models (PRMs) have recently emerged as a powerful framework for enhancing the reasoning capabilities of large reasoning models (LRMs), particularly in the context of test-time scaling (TTS). However, their potential for supervising LRMs on tabular reasoning domains remains underexplored. Through detailed empirical analyses, we identify that existing PRMs, though widely adopted for supervising text-only reasoning steps, struggle with table-specific operations such as sub-table retrieval and schema interaction, leading to critical performance bottlenecks. To address this limitation, we propose TaTToo, a novel table-grounded PRM framework that (i) reasons explicitly over tabular reasoning steps and (ii) integrates tool-based verification to provide precise reward supervision. Concretely, we first design a scalable data curation pipeline that constructs over 60k high-quality step-level annotations by integrating table verification rationales with tool-based executions. Building on the collected data, we train TaTToo with a dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use reasoning patterns, followed by reinforcement learning with tool-grounded reward shaping to align our model with table-based verification. We provide a comprehensive evaluation of the policy improvement induced by our newly designed PRM. Across 5 challenging tabular reasoning benchmarks covering numerical reasoning, fact-checking, and data analysis, TaTToo improves downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong generalizability across diverse TTS strategies.', 'score': 32, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'bdefddb943fa9266', 'authors': ['Jiaru Zou', 'Soumya Roy', 'Vinay Kumar Verma', 'Ziyi Wang', 'David Wipf', 'Pan Lu', 'Sumit Negi', 'James Zou', 'Jingrui He'], 'affiliations': ['Amazon', 'Purdue University', 'Stanford University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2510.06217.jpg', 'data': {'categories': ['#reasoning', '#rl', '#training', '#data', '#benchmark', '#optimization', '#dataset'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'TaTToo: Process Reward Model Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ TaTToo â€” Ğ½Ğ¾Ğ²ÑƒÑ Process Reward Model Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ´ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ² LLM. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ PRM Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ ÑÑ…ĞµĞ¼Ğ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. TaTToo Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ PRM Ñ 72B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 30.9% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, fact-checking Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Tabular Reasoning with TaTToo!', 'desc': 'TaTToo is a new framework designed to improve how large reasoning models handle tables in machine learning. It focuses on specific operations related to tables, like retrieving sub-tables and interacting with schemas, which previous models struggled with. By using a combination of supervised learning and reinforcement learning, TaTToo provides better guidance for reasoning tasks involving tables. The results show that TaTToo significantly enhances performance on various tabular reasoning challenges, outperforming existing models with fewer parameters.'}, 'zh': {'title': 'TaTTooï¼šæå‡è¡¨æ ¼æ¨ç†çš„æ–°æ–¹æ³•', 'desc': 'TaTTooæ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºè¡¨æ ¼çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œæ—¨åœ¨æå‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ˜ç¡®å¤„ç†è¡¨æ ¼ç‰¹å®šæ“ä½œå’Œæ•´åˆå·¥å…·éªŒè¯ï¼Œæ˜¾è‘—æ”¹å–„äº†ç°æœ‰è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹åœ¨å¤„ç†è¡¨æ ¼æ¨ç†æ—¶å­˜åœ¨ç“¶é¢ˆï¼Œè€ŒTaTTooé€šè¿‡è®¾è®¡å¯æ‰©å±•çš„æ•°æ®æ•´ç†ç®¡é“å’ŒåŒé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚ç»è¿‡è¯„ä¼°ï¼ŒTaTTooåœ¨å¤šä¸ªè¡¨æ ¼æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†ä¸‹æ¸¸å¤§è§„æ¨¡æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.26328', 'title': 'Fast-dLLM v2: Efficient Block-Diffusion LLM', 'url': 'https://huggingface.co/papers/2509.26328', 'abstract': "Fast-dLLM v2, a block diffusion language model, efficiently converts pretrained autoregressive models for parallel text generation, achieving significant speedup without compromising accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.", 'score': 19, 'issue_id': 6298, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'd003eca6c18f4d37', 'authors': ['Chengyue Wu', 'Hao Zhang', 'Shuchen Xue', 'Shizhe Diao', 'Yonggan Fu', 'Zhijian Liu', 'Pavlo Molchanov', 'Ping Luo', 'Song Han', 'Enze Xie'], 'affiliations': ['MIT', 'NVIDIA', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.26328.jpg', 'data': {'categories': ['#training', '#inference', '#open_source', '#diffusion', '#optimization', '#architecture'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Fast-dLLM v2 â€” Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ LLM Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ²ÑĞµĞ³Ğ¾ 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ (Ğ² 500 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ñƒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ÑÑ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Speed Meets Accuracy: Fast-dLLM v2 Revolutionizes Text Generation', 'desc': "Fast-dLLM v2 is a block diffusion language model that transforms pretrained autoregressive models for faster text generation. It achieves this by using a novel block diffusion mechanism and a complementary attention mask, allowing for efficient parallel processing while maintaining the model's accuracy. The model requires significantly less fine-tuning data, only about 1 billion tokens, compared to traditional methods that need hundreds of billions. With a hierarchical caching system, Fast-dLLM v2 can generate text up to 2.5 times faster than standard autoregressive decoding, making it a powerful tool for natural language tasks."}, 'zh': {'title': 'å¿«é€Ÿé«˜æ•ˆçš„å—æ‰©æ•£è¯­è¨€æ¨¡å‹', 'desc': 'Fast-dLLM v2æ˜¯ä¸€ç§å—æ‰©æ•£è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†é¢„è®­ç»ƒçš„è‡ªå›å½’æ¨¡å‹è½¬æ¢ä¸ºå¹¶è¡Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…éœ€çº¦10äº¿ä¸ªæ ‡è®°è¿›è¡Œå¾®è°ƒï¼Œç›¸æ¯”äºå…¨æ³¨æ„åŠ›æ‰©æ•£æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å‡å°‘äº†500å€ï¼ŒåŒæ—¶ä¿æŒäº†åŸå§‹æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¼•å…¥å—æ‰©æ•£æœºåˆ¶å’Œäº’è¡¥æ³¨æ„åŠ›æ©ç ï¼ŒFast-dLLM v2å®ç°äº†å—çº§åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œå¹¶è®¾è®¡äº†åˆ†å±‚ç¼“å­˜æœºåˆ¶ä»¥åŠ é€Ÿè§£ç ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFast-dLLM v2åœ¨å‡†ç¡®æ€§ä¸Šä¸è‡ªå›å½’åŸºçº¿ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶åœ¨æ•ˆç‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03270', 'title': 'CoDA: Coding LM via Diffusion Adaptation', 'url': 'https://huggingface.co/papers/2510.03270', 'abstract': 'CoDA, a 1.7B-parameter diffusion coder, achieves competitive performance with smaller models through confidence-guided sampling and is released with open-source tools.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.', 'score': 16, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '49ba9b2131a56dc8', 'authors': ['Haolin Chen', 'Shiyu Wang', 'Can Qin', 'Bo Pang', 'Zuxin Liu', 'Jielin Qiu', 'Jianguo Zhang', 'Yingbo Zhou', 'Zeyuan Chen', 'Ran Xu', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang', 'Weiran Yao'], 'affiliations': ['Salesforce AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2510.03270.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#diffusion', '#small_models', '#dataset'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ĞµÑ€ Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹', 'desc': 'CoDA â€” ÑÑ‚Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ñ 1.7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğµ. CoDA Ğ¾Ğ±ÑƒÑ‡Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° TPU Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ confidence-guided sampling Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… HumanEval Ğ¸ MBPP Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ Ğ´Ğ¾ 7B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹.'}, 'en': {'title': 'CoDA: Lightweight Diffusion Coding with Competitive Performance', 'desc': 'CoDA is a diffusion coder with 1.7 billion parameters that competes effectively with larger models by using confidence-guided sampling. It leverages a unique training approach that combines large-scale diffusion pre-training with code-focused mid-training and instruction tuning. This allows CoDA to maintain low inference latency while providing advanced capabilities like bidirectional context and infilling. The model is open-source, including tools and checkpoints to support further research in lightweight diffusion-based coding assistants.'}, 'zh': {'title': 'CoDAï¼šè½»é‡çº§æ‰©æ•£ç¼–ç çš„æœªæ¥', 'desc': 'CoDAæ˜¯ä¸€ç§å…·æœ‰17äº¿å‚æ•°çš„æ‰©æ•£ç¼–ç å™¨ï¼Œé€šè¿‡ä¿¡å¿ƒå¼•å¯¼é‡‡æ ·å®ç°äº†ä¸æ›´å°æ¨¡å‹çš„ç«äº‰æ€§èƒ½ã€‚å®ƒç»“åˆäº†å¤§è§„æ¨¡çš„æ‰©æ•£é¢„è®­ç»ƒå’Œä»¥ä»£ç ä¸ºä¸­å¿ƒçš„ä¸­æœŸè®­ç»ƒï¼Œä»¥åŠæŒ‡ä»¤è°ƒä¼˜ï¼Œä»è€Œä¿æŒäº†æ¨ç†å»¶è¿Ÿçš„ç«äº‰åŠ›ã€‚CoDAåœ¨Humanevalã€MBPPå’ŒEvalPlusç­‰åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¡¨ç°ä¸é«˜è¾¾70äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ç›¸å½“æˆ–æ›´å¥½ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ¨¡å‹æ£€æŸ¥ç‚¹ã€è¯„ä¼°å·¥å…·å’ŒTPUè®­ç»ƒç®¡é“ï¼Œä»¥åŠ é€Ÿè½»é‡çº§æ‰©æ•£ç¼–ç åŠ©æ‰‹çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.24107', 'title': 'Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and\n  Synthesis for SLMs', 'url': 'https://huggingface.co/papers/2509.24107', 'abstract': 'Fathom-DeepResearch, an agentic system with specialized models for web search and report synthesis, achieves state-of-the-art performance on open-ended information-seeking tasks and diverse reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Tool-integrated reasoning has emerged as a key focus for enabling agentic applications. Among these, DeepResearch Agents have gained significant attention for their strong performance on complex, open-ended information-seeking tasks. We introduce Fathom-DeepResearch, an agentic system composed of two specialized models. The first is Fathom-Search-4B, a DeepSearch model trained from Qwen3-4B and optimized for evidence-based investigation through live web search and targeted webpage querying. Its training combines three advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent self-play that enforces strict web-search dependence and heterogeneous source grounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes multi-turn Reinforcement Learning with Verifiable Rewards through curriculum pruning, reward-aware advantage scaling, and per-prompt replay buffers; and (iii) a steerable step-level reward that classifies each tool call by cognitive behavior and marginal utility, enabling explicit control over search trajectory breadth, depth, and horizon. These improvements enable reliable extension of tool-calling beyond 20 calls when warranted. The second is Fathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn DeepSearch traces into structured, citation-dense DeepResearch Reports for comprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES, WebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves state-of-the-art performance in the open-weights category while demonstrating strong generalization to diverse reasoning tasks including HLE, AIME-25, GPQA-Diamond, and MedQA.', 'score': 11, 'issue_id': 6301, 'pub_date': '2025-09-28', 'pub_date_card': {'ru': '28 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 28', 'zh': '9æœˆ28æ—¥'}, 'hash': '17db556aca22a151', 'authors': ['Shreyas Singh', 'Kunal Singh', 'Pradeep Moturi'], 'affiliations': ['Fractal AI Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.24107.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#agents', '#rl', '#dataset', '#optimization'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Fathom-DeepResearch, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸĞµÑ€Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğº ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ 20 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ñ‹ Ñ Ñ†Ğ¸Ñ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ open-weights Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Revolutionizing Web Search and Report Synthesis with Fathom-DeepResearch', 'desc': 'Fathom-DeepResearch is an advanced agentic system designed for effective web search and report synthesis. It consists of two specialized models: Fathom-Search-4B, which excels in evidence-based investigations through live web searches, and Fathom-Synthesizer-4B, which transforms search results into structured reports. The system incorporates innovative techniques like DUETQA for dataset generation and RAPO for enhancing reinforcement learning stability. With its state-of-the-art performance on various benchmarks, Fathom-DeepResearch demonstrates exceptional capabilities in handling complex information-seeking and reasoning tasks.'}, 'zh': {'title': 'æ™ºèƒ½æœç´¢ä¸æŠ¥å‘Šåˆæˆçš„æœªæ¥', 'desc': 'Fathom-DeepResearch æ˜¯ä¸€ä¸ªæ™ºèƒ½ç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºç½‘ç»œæœç´¢å’ŒæŠ¥å‘Šåˆæˆï¼Œèƒ½å¤Ÿåœ¨å¼€æ”¾å¼ä¿¡æ¯æ£€ç´¢ä»»åŠ¡å’Œå¤šæ ·åŒ–æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥ç³»ç»Ÿç”±ä¸¤ä¸ªä¸“é—¨æ¨¡å‹ç»„æˆï¼šFathom-Search-4B å’Œ Fathom-Synthesizer-4Bã€‚Fathom-Search-4B é€šè¿‡å®æ—¶ç½‘ç»œæœç´¢å’Œé’ˆå¯¹ç½‘é¡µæŸ¥è¯¢è¿›è¡Œè¯æ®åŸºç¡€è°ƒæŸ¥ï¼Œé‡‡ç”¨äº†å¤šé¡¹å…ˆè¿›æŠ€æœ¯æ¥ä¼˜åŒ–å…¶æ€§èƒ½ã€‚Fathom-Synthesizer-4B åˆ™å°†å¤šè½® DeepSearch è¿½è¸ªè½¬æ¢ä¸ºç»“æ„åŒ–çš„ã€å¼•ç”¨å¯†é›†çš„ DeepResearch æŠ¥å‘Šï¼Œç¡®ä¿ä¿¡æ¯çš„å…¨é¢åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06062', 'title': 'ASPO: Asymmetric Importance Sampling Policy Optimization', 'url': 'https://huggingface.co/papers/2510.06062', 'abstract': 'ASPO addresses the imbalance in token weighting during OSRL by flipping Importance Sampling ratios and incorporating a soft dual-clipping mechanism, improving training stability and performance in LLMs.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.', 'score': 7, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '3090778074ce994d', 'authors': ['Jiakang Wang', 'Runze Liu', 'Lei Lin', 'Wenping Hu', 'Xiu Li', 'Fuzheng Zhang', 'Guorui Zhou', 'Kun Gai'], 'affiliations': ['Kuaishou Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06062.jpg', 'data': {'categories': ['#training', '#rl', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°Ğ¼Ğ¸. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ASPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¿ÑƒÑ‚ĞµĞ¼ Â«Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸ÑÂ» ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Importance Sampling Ğ´Ğ»Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¼ÑĞ³ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Balancing Token Weighting for Better LLM Training', 'desc': 'The paper introduces Asymmetric Importance Sampling Policy Optimization (ASPO) to improve training in Large Language Models (LLMs) during Outcome-Supervised Reinforcement Learning (OSRL). It identifies a problem with the Importance Sampling ratios, which causes an imbalance in how positive and negative tokens are weighted, leading to ineffective learning. ASPO addresses this by flipping the ratios for positive-advantage tokens and adding a soft dual-clipping mechanism to stabilize updates. Experiments show that ASPO enhances training stability and performance, reducing premature convergence compared to existing methods.'}, 'zh': {'title': 'ä¼˜åŒ–ä»¤ç‰ŒåŠ æƒï¼Œæå‡è®­ç»ƒç¨³å®šæ€§', 'desc': 'ASPOï¼ˆä¸å¯¹ç§°é‡è¦æ€§é‡‡æ ·ç­–ç•¥ä¼˜åŒ–ï¼‰è§£å†³äº†åœ¨ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆOSRLï¼‰ä¸­ä»¤ç‰ŒåŠ æƒä¸å¹³è¡¡çš„é—®é¢˜ã€‚é€šè¿‡ç¿»è½¬æ­£ä¼˜åŠ¿ä»¤ç‰Œçš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼ŒASPOä½¿å¾—æ­£è´Ÿä»¤ç‰Œçš„æ›´æ–°æ–¹å‘ä¸€è‡´ï¼Œä»è€Œæé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼ŒASPOè¿˜å¼•å…¥äº†ä¸€ç§è½¯åŒå‰ªåˆ‡æœºåˆ¶ï¼Œä»¥ç¨³å®šæç«¯æ›´æ–°å¹¶ä¿æŒæ¢¯åº¦æµåŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒASPOåœ¨ç¼–ç å’Œæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æ”¹å–„äº†è®­ç»ƒæ•ˆæœï¼Œå‡å°‘äº†è¿‡æ—©æ”¶æ•›ç°è±¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06036', 'title': 'Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?', 'url': 'https://huggingface.co/papers/2510.06036', 'abstract': "Research identifies a mechanism called the refusal cliff in large reasoning models, where refusal intentions drop sharply before output generation, and proposes a method to improve safety by focusing on specific attention heads and training examples.  \t\t\t\t\tAI-generated summary \t\t\t\t Large reasoning models (LRMs) with multi-step reasoning capabilities have shown remarkable problem-solving abilities, yet they exhibit concerning safety vulnerabilities that remain poorly understood. In this work, we investigate why safety alignment fails in reasoning models through a mechanistic interpretability lens. Using a linear probing approach to trace refusal intentions across token positions, we discover a striking phenomenon termed as refusal cliff: many poorly-aligned reasoning models correctly identify harmful prompts and maintain strong refusal intentions during their thinking process, but experience a sharp drop in refusal scores at the final tokens before output generation. This suggests that these models are not inherently unsafe; rather, their refusal intentions are systematically suppressed. Through causal intervention analysis, we identify a sparse set of attention heads that negatively contribute to refusal behavior. Ablating just 3\\% of these heads can reduce attack success rates below 10\\%. Building on these mechanistic insights, we propose Cliff-as-a-Judge, a novel data selection method that identifies training examples exhibiting the largest refusal cliff to efficiently repair reasoning models' safety alignment. This approach achieves comparable safety improvements using only 1.7\\% of the vanilla safety training data, demonstrating a less-is-more effect in safety alignment.", 'score': 5, 'issue_id': 6301, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '0d9aa29b1d58dbd6', 'authors': ['Qingyu Yin', 'Chak Tou Leong', 'Linyi Yang', 'Wenxuan Huang', 'Wenjie Li', 'Xiting Wang', 'Jaehong Yoon', 'YunXing', 'XingYu', 'Jinjin Gu'], 'affiliations': ['East China Normal University', 'Hong Kong Polytechnic University', 'INSAIT', 'Nanyang Technological University', 'Renmin University', 'Southern University of Science and Technology', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06036.jpg', 'data': {'categories': ['#reasoning', '#architecture', '#data', '#alignment', '#interpretability', '#training'], 'emoji': 'ğŸ§—', 'ru': {'title': 'ĞĞ±Ñ€Ñ‹Ğ² Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°: Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ½ĞµĞ·Ğ°Ğ¿Ğ½Ğ¾ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½ Â«Ğ¾Ğ±Ñ€Ñ‹Ğ²Ğ° Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Â» Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… reasoning-Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…: Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ÑÑ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ½Ğ¾ Ñ€ĞµĞ·ĞºĞ¾ Ñ‚ĞµÑ€ÑÑÑ‚ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, ÑƒÑ‡Ñ‘Ğ½Ñ‹Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ attention heads, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¾Ğº Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cliff-as-a-Judge, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 1.7% Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼. ĞĞ±Ğ»ÑÑ†Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ 3% Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ñ… attention heads ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ´Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ½Ğ¸Ğ¶Ğµ 10%.'}, 'en': {'title': 'Understanding and Mitigating the Refusal Cliff in Reasoning Models', 'desc': "This paper explores a phenomenon called the refusal cliff in large reasoning models (LRMs), where the models show a significant drop in their intention to refuse harmful prompts just before generating an output. The authors use mechanistic interpretability to analyze how these models can recognize harmful inputs but fail to maintain their refusal intentions at the final stages of processing. They identify specific attention heads that contribute negatively to this behavior and demonstrate that reducing the influence of just a small percentage of these heads can greatly enhance the models' safety. Additionally, they introduce a new method called Cliff-as-a-Judge, which selects training examples that highlight the refusal cliff, allowing for effective safety improvements with minimal data."}, 'zh': {'title': 'æ­ç¤ºæ¨ç†æ¨¡å‹çš„æ‹’ç»æ‚¬å´–æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹æ¨ç†æ¨¡å‹ä¸­çš„æ‹’ç»æ‚¬å´–æœºåˆ¶ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨ç”Ÿæˆè¾“å‡ºå‰æ‹’ç»æ„å›¾ä¼šæ€¥å‰§ä¸‹é™ã€‚é€šè¿‡çº¿æ€§æ¢æµ‹æ–¹æ³•ï¼Œæˆ‘ä»¬è¿½è¸ªäº†æ‹’ç»æ„å›¾åœ¨æ ‡è®°ä½ç½®çš„å˜åŒ–ï¼Œå‘ç°è®¸å¤šæ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿè¯†åˆ«æœ‰å®³æç¤ºï¼Œä½†åœ¨è¾“å‡ºå‰çš„æœ€åå‡ ä¸ªæ ‡è®°å¤„æ‹’ç»æ„å›¾å´æ˜¾è‘—é™ä½ã€‚æˆ‘ä»¬é€šè¿‡å› æœå¹²é¢„åˆ†æï¼Œè¯†åˆ«å‡ºå°‘é‡å¯¹æ‹’ç»è¡Œä¸ºäº§ç”Ÿè´Ÿé¢å½±å“çš„æ³¨æ„åŠ›å¤´ï¼Œå»é™¤è¿™äº›å¤´å¯ä»¥æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨æ‹’ç»æ‚¬å´–çš„ç‰¹å¾æ¥é«˜æ•ˆä¿®å¤æ¨ç†æ¨¡å‹çš„å®‰å…¨å¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05432', 'title': 'AInstein: Assessing the Feasibility of AI-Generated Approaches to\n  Research Problems', 'url': 'https://huggingface.co/papers/2510.05432', 'abstract': 'AInstein evaluates the problem-solving capabilities of large language models by testing their ability to generate valid solutions to AI research problems using only pretrained knowledge, revealing both their potential and limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.', 'score': 5, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'effef15175939fca', 'authors': ['Shambhavi Mishra', 'Gaurav Sahu', 'Marco Pedersoli', 'Laurent Charlin', 'Jose Dolz', 'Christopher Pal'], 'affiliations': ['Canada CIFAR AI Chair', 'HEC Montreal', 'International Laboratory on Learning Systems (ILLS)', 'LIVIA, ETS Montreal', 'Mila Quebec AI Institute', 'Polytechnique Montreal', 'ServiceNow Research', 'Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2510.05432.jpg', 'data': {'categories': ['#science', '#reasoning', '#benchmark', '#rlhf', '#agents'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ĞœĞ¾Ğ¶ĞµÑ‚ Ğ»Ğ¸ AI ÑÑ‚Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼ Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AInstein â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ AI, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ñ„Ğ°Ğ¹Ğ½Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğ° Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ĞµĞ¹ ICLR 2025 Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ¿Ğ¾ Ñ‚Ñ€Ñ‘Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼: ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LLM Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¾ÑĞ¼Ñ‹ÑĞ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ‚ÑŒ ĞºÑ€ĞµĞ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹, Ğ½Ğ¾ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ñ…Ñ€ÑƒĞ¿ĞºĞ¸Ğ¼Ğ¸ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‚ Ğ¾Ñ‚ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'AInstein: Unveiling the Problem-Solving Power of LLMs', 'desc': "The paper introduces AInstein, a framework designed to evaluate the problem-solving abilities of large language models (LLMs) in generating valid solutions to AI research problems using only their pretrained knowledge. It tests LLMs without any fine-tuning or external aids, focusing on their capacity to produce solutions through iterative critique loops similar to scientific review processes. The evaluation is based on 1,214 ICLR papers and uses metrics like Success Rate, Rediscovery, and Novelty to assess the LLMs' performance. The findings indicate that while LLMs can rediscover existing solutions and occasionally suggest novel ideas, their problem-solving capabilities are fragile and highly dependent on how problems are framed."}, 'zh': {'title': 'è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„ç§‘å­¦é—®é¢˜è§£å†³èƒ½åŠ›', 'desc': 'AInsteinæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³é—®é¢˜èƒ½åŠ›çš„æ¡†æ¶ã€‚å®ƒæµ‹è¯•è¿™äº›æ¨¡å‹åœ¨æ²¡æœ‰é¢†åŸŸç‰¹å®šå¾®è°ƒæˆ–å¤–éƒ¨å¸®åŠ©çš„æƒ…å†µä¸‹ï¼Œæ˜¯å¦èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆçš„AIç ”ç©¶é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æå–é«˜è´¨é‡ICLR 2025æäº¤çš„ç²¾ç‚¼é—®é¢˜é™ˆè¿°ï¼ŒAInsteinæ¨¡æ‹Ÿç§‘å­¦ç ”ç©¶ä¸­çš„ææ¡ˆã€å®¡æŸ¥å’Œä¿®è®¢å¾ªç¯ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå°½ç®¡LLMsèƒ½å¤Ÿé‡æ–°å‘ç°å¯è¡Œçš„è§£å†³æ–¹æ¡ˆå¹¶å¶å°”æå‡ºåˆ›é€ æ€§çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å®ƒä»¬çš„è§£å†³é—®é¢˜èƒ½åŠ›ä»ç„¶è„†å¼±ï¼Œä¸”å¯¹é—®é¢˜çš„è¡¨è¿°éå¸¸æ•æ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04081', 'title': 'Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model\n  Reasoning', 'url': 'https://huggingface.co/papers/2510.04081', 'abstract': "Caco, a code-assisted chain-of-thought framework, automates the generation of high-quality, verifiable, and diverse reasoning data, enhancing the performance of large language models on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose Caco (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, Caco first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created Caco-1.3M dataset demonstrate that Caco-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that Caco's code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention.", 'score': 5, 'issue_id': 6299, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}, 'hash': '4f2356e6d1057b79', 'authors': ['Honglin Lin', 'Qizhi Pei', 'Xin Gao', 'Zhuoshi Pan', 'Yu Li', 'Juntao Li', 'Conghui He', 'Lijun Wu'], 'affiliations': ['OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.04081.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#benchmark', '#math', '#data', '#training'], 'emoji': 'ğŸ”¢', 'ru': {'title': 'ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Caco â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² LLM Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ´Ğ°. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (chain-of-thought) Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾ Ğ² ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ‚Ğ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñƒ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Caco-1.3M, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±ĞµĞ· ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Caco: Automating High-Quality Reasoning for LLMs', 'desc': 'Caco is a new framework designed to improve the reasoning abilities of large language models (LLMs) in solving mathematical problems. It automates the creation of high-quality reasoning data by using code to generate diverse and verifiable reasoning paths. This approach addresses the limitations of traditional Chain-of-Thought (CoT) methods, which often struggle with quality and scalability. By incorporating automated validation and a closed-loop synthesis process, Caco ensures that the generated reasoning data is both executable and adaptable to various tasks, leading to better performance on mathematical reasoning benchmarks.'}, 'zh': {'title': 'Cacoï¼šè‡ªåŠ¨åŒ–é«˜è´¨é‡æ¨ç†æ•°æ®ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'Cacoæ˜¯ä¸€ä¸ªä»£ç è¾…åŠ©çš„æ€ç»´é“¾æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¯éªŒè¯å’Œå¤šæ ·åŒ–çš„æ¨ç†æ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚è¯¥æ¡†æ¶é€šè¿‡ä»£ç é©±åŠ¨çš„å¢å¼ºæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ€ç»´é“¾æ–¹æ³•åœ¨ç”Ÿæˆæ§åˆ¶ã€è´¨é‡ä¸è¶³å’Œæ¨ç†è·¯å¾„æœ‰é™ç­‰é—®é¢˜ã€‚Cacoé¦–å…ˆåœ¨ç»Ÿä¸€çš„ä»£ç æ ¼å¼ä¸Šå¾®è°ƒä»£ç åŸºç¡€çš„æ€ç»´é“¾ç”Ÿæˆå™¨ï¼Œç„¶åæ‰©å±•æ•°æ®ç”Ÿæˆä»¥è·å¾—å¤§é‡å¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹ã€‚é€šè¿‡ä»£ç æ‰§è¡Œå’ŒåŸºäºè§„åˆ™çš„è¿‡æ»¤ï¼ŒCacoç¡®ä¿äº†é€»è¾‘æ­£ç¡®æ€§å’Œç»“æ„å¤šæ ·æ€§ï¼Œä»è€Œå®ç°äº†å®Œå…¨è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„æ¨ç†æ•°æ®åˆæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06182', 'title': 'Mixing Mechanisms: How Language Models Retrieve Bound Entities\n  In-Context', 'url': 'https://huggingface.co/papers/2510.06182', 'abstract': 'Language models use positional, lexical, and reflexive mechanisms to bind and retrieve entities, with a causal model achieving high accuracy in predicting next tokens across various tasks and input lengths.  \t\t\t\t\tAI-generated summary \t\t\t\t A key component of in-context reasoning is the ability of language models (LMs) to bind entities for later retrieval. For example, an LM might represent "Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann" when asked "Who loves pie?" Prior research on short lists of bound entities found strong evidence that LMs implement such retrieval via a positional mechanism, where "Ann" is retrieved based on its position in context. In this work, we find that this mechanism generalizes poorly to more complex settings; as the number of bound entities in context increases, the positional mechanism becomes noisy and unreliable in middle positions. To compensate for this, we find that LMs supplement the positional mechanism with a lexical mechanism (retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism (retrieving "Ann" through a direct pointer). Through extensive experiments on nine models and ten binding tasks, we uncover a consistent pattern in how LMs mix these mechanisms to drive model behavior. We leverage these insights to develop a causal model combining all three mechanisms that estimates next token distributions with 95% agreement. Finally, we show that our model generalizes to substantially longer inputs of open-ended text interleaved with entity groups, further demonstrating the robustness of our findings in more natural settings. Overall, our study establishes a more complete picture of how LMs bind and retrieve entities in-context.', 'score': 4, 'issue_id': 6300, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'ebd0b1b2a51e6c0a', 'authors': ['Yoav Gur-Arieh', 'Mor Geva', 'Atticus Geiger'], 'affiliations': ['Blavatnik School of Computer Science and AI, Tel Aviv University', 'Goodfire', 'Pr(Ai)2R Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.06182.jpg', 'data': {'categories': ['#reasoning', '#long_context', '#data', '#multimodal', '#interpretability', '#architecture'], 'emoji': 'ğŸ”—', 'ru': {'title': 'Ğ¢Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, ĞºĞ°Ğº ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ÑÑ‚ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°. ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… ÑĞ¿Ğ¸ÑĞºĞ¾Ğ², Ğ½Ğ¾ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑÑ Ğ½ĞµĞ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹. LLM ĞºĞ¾Ğ¼Ğ¿ĞµĞ½ÑĞ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ğ¾ Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ (Ğ¿Ğ¾Ğ¸ÑĞº Ñ‡ĞµÑ€ĞµĞ· ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°) Ğ¸ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ (Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸). Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²ÑĞµ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ°, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ 95% Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Unraveling Entity Binding in Language Models', 'desc': 'This paper explores how language models (LMs) bind and retrieve entities during in-context reasoning. It identifies three mechanisms used by LMs: positional, lexical, and reflexive, which help in accurately predicting the next tokens. The study reveals that while the positional mechanism works well for short lists of entities, it struggles with longer contexts, leading to the use of lexical and reflexive mechanisms for better retrieval. By developing a causal model that integrates these mechanisms, the authors achieve high accuracy in predicting token distributions across various tasks and input lengths.'}, 'zh': {'title': 'è¯­è¨€æ¨¡å‹çš„å®ä½“ç»‘å®šä¸æ£€ç´¢æœºåˆ¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹å¦‚ä½•åœ¨ä¸Šä¸‹æ–‡ä¸­ç»‘å®šå’Œæ£€ç´¢å®ä½“ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¼ ç»Ÿçš„åŸºäºä½ç½®çš„æœºåˆ¶åœ¨å¤æ‚æƒ…å†µä¸‹è¡¨ç°ä¸ä½³ï¼Œå› æ­¤è¯­è¨€æ¨¡å‹è¿˜ä½¿ç”¨äº†è¯æ±‡æœºåˆ¶å’Œåå°„æœºåˆ¶æ¥æé«˜æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹ä¹ç§æ¨¡å‹å’Œåä¸ªç»‘å®šä»»åŠ¡çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬æ­ç¤ºäº†è¯­è¨€æ¨¡å‹å¦‚ä½•æ··åˆä½¿ç”¨è¿™äº›æœºåˆ¶æ¥é©±åŠ¨æ¨¡å‹è¡Œä¸ºã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»“åˆä¸‰ç§æœºåˆ¶çš„å› æœæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ›´é•¿çš„è¾“å…¥æ–‡æœ¬ä¸­æœ‰æ•ˆåœ°é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06131', 'title': 'Discrete Diffusion Models with MLLMs for Unified Medical Multimodal\n  Generation', 'url': 'https://huggingface.co/papers/2510.06131', 'abstract': 'MeDiM, a medical discrete diffusion model, integrates multimodal biomedical data by learning shared distributions across images, text, and clinical notes, achieving high-fidelity generation and enhanced downstream performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in generative medical models are constrained by modality-specific scenarios that hinder the integration of complementary evidence from imaging, pathology, and clinical notes. This fragmentation limits their evolution into foundation models that can learn and reason across the full spectrum of biomedical data. We propose MeDiM, the first medical discrete diffusion model that learns shared distributions across modalities without modality-specific components. MeDiM unifies multiple generative tasks: translating between images and text, and jointly producing image-report pairs across domains in response to prompts. Built on a discrete diffusion framework, MeDiM bridges vision and language representations through a shared probabilistic space. To enable unified and flexible medical generation, we employ a multimodal large language model (MLLM) as the diffusion backbone, leveraging its prior knowledge and cross-modal reasoning. Two key designs are introduced: (1) removing the causal attention mask for bidirectional context, and (2) injecting continuous timestep embeddings for diffusion awareness. Experiments demonstrate high-fidelity medical generation (FID 16.60 on MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR 0.2650 and 0.2580). Jointly generated image-report pairs further enhance downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2, plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports coherent and clinically grounded multimodal outputs.', 'score': 4, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '0641a3ba669f441d', 'authors': ['Jiawei Mao', 'Yuhan Wang', 'Lifeng Chen', 'Can Zhao', 'Yucheng Tang', 'Dong Yang', 'Liangqiong Qu', 'Daguang Xu', 'Yuyin Zhou'], 'affiliations': ['NVIDIA', 'UC Santa Cruz', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.06131.jpg', 'data': {'categories': ['#diffusion', '#science', '#healthcare', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'MeDiM â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸) Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¼ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ². Ğ’ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ĞµĞ¶Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ LLM Ñ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹: ÑƒĞ±Ñ€Ğ°Ğ½Ğ° ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ´Ğ²ÑƒĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ timestep embeddings Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° downstream-Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚.'}, 'en': {'title': 'Unifying Biomedical Data with MeDiM: A Multimodal Diffusion Revolution', 'desc': "MeDiM is a novel medical discrete diffusion model designed to integrate various types of biomedical data, such as images, text, and clinical notes. It overcomes the limitations of traditional models that operate within specific modalities by learning shared distributions across all data types. By utilizing a multimodal large language model as its backbone, MeDiM can generate high-quality medical outputs and translate between different modalities effectively. The model's innovative design features, like bidirectional context and continuous timestep embeddings, contribute to its superior performance in generating coherent and clinically relevant multimodal outputs."}, 'zh': {'title': 'MeDiMï¼šåŒ»å­¦å¤šæ¨¡æ€ç”Ÿæˆçš„åˆ›æ–°æ¡¥æ¢', 'desc': 'MeDiMæ˜¯ä¸€ç§åŒ»å­¦ç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å­¦ä¹ å›¾åƒã€æ–‡æœ¬å’Œä¸´åºŠè®°å½•ä¹‹é—´çš„å…±äº«åˆ†å¸ƒæ¥æ•´åˆå¤šæ¨¡æ€ç”Ÿç‰©åŒ»å­¦æ•°æ®ã€‚è¯¥æ¨¡å‹å…‹æœäº†ä¼ ç»Ÿç”ŸæˆåŒ»å­¦æ¨¡å‹åœ¨ç‰¹å®šæ¨¡æ€åœºæ™¯ä¸‹çš„å±€é™æ€§ï¼Œå®ç°äº†é«˜ä¿çœŸåº¦çš„ç”Ÿæˆå’Œå¢å¼ºçš„ä¸‹æ¸¸æ€§èƒ½ã€‚MeDiMç»Ÿä¸€äº†å¤šç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒä¸æ–‡æœ¬ä¹‹é—´çš„ç¿»è¯‘ï¼Œä»¥åŠæ ¹æ®æç¤ºå…±åŒç”Ÿæˆè·¨é¢†åŸŸçš„å›¾åƒ-æŠ¥å‘Šå¯¹ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ‰©æ•£éª¨å¹²ï¼ŒMeDiMåœ¨ä¸€ä¸ªå…±äº«çš„æ¦‚ç‡ç©ºé—´ä¸­æ¡¥æ¥äº†è§†è§‰å’Œè¯­è¨€è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05560', 'title': 'HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video', 'url': 'https://huggingface.co/papers/2510.05560', 'abstract': "HoloScene is an interactive 3D reconstruction framework that achieves geometry completeness, object interactivity, physical plausibility, photorealistic rendering, and realistic physical properties through an energy-based optimization problem.  \t\t\t\t\tAI-generated summary \t\t\t\t Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.", 'score': 4, 'issue_id': 6299, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'fcf790fe9803a797', 'authors': ['Hongchi Xia', 'Chih-Hao Lin', 'Hao-Yu Hsu', 'Quentin Leboutet', 'Katelyn Gao', 'Michael Paulitsch', 'Benjamin Ummenhofer', 'Shenlong Wang'], 'affiliations': ['Intel', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.05560.jpg', 'data': {'categories': ['#3d', '#optimization', '#games', '#benchmark'], 'emoji': 'ğŸ—ï¸', 'ru': {'title': 'Ğ¦Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ñ Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¾Ğ¹ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼', 'desc': "HoloScene â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ° Ğ² Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ€ĞµĞ´Ñ‹, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ Ğº ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ³Ñ€Ğ°Ñ„ ÑÑ†ĞµĞ½Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµÑ‚ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ğ¸Ñ… Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸. Ğ ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ prior'Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ sampling Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ â€” Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¸ĞºĞ¸ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ´Ğ»Ñ AR/VR, Ğ¸Ğ³Ñ€ Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸."}, 'en': {'title': 'Revolutionizing 3D Reconstruction with HoloScene', 'desc': 'HoloScene is a cutting-edge framework for creating interactive 3D reconstructions that meet essential criteria for realistic simulations. It addresses common shortcomings in existing methods by ensuring geometry completeness, object interactivity, physical plausibility, and photorealistic rendering. The framework utilizes an energy-based optimization approach that combines observational data and physical constraints to produce accurate digital twins. Its effectiveness is demonstrated through superior performance on benchmark datasets and practical applications in gaming and digital-twin manipulation.'}, 'zh': {'title': 'HoloSceneï¼šå®ç°çœŸå®æ„Ÿçš„äº¤äº’å¼3Dé‡å»º', 'desc': 'HoloSceneæ˜¯ä¸€ä¸ªäº¤äº’å¼3Dé‡å»ºæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°å‡ ä½•å®Œæ•´æ€§ã€ç‰©ä½“äº¤äº’æ€§ã€ç‰©ç†åˆç†æ€§ã€ç…§ç‰‡çº§æ¸²æŸ“å’ŒçœŸå®çš„ç‰©ç†å±æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡èƒ½é‡ä¼˜åŒ–é—®é¢˜æ¥æ•´åˆè§‚å¯Ÿæ•°æ®ã€ç‰©ç†çº¦æŸå’Œç”Ÿæˆå…ˆéªŒï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„ç›®æ ‡ã€‚HoloSceneåˆ©ç”¨å…¨é¢çš„äº¤äº’åœºæ™¯å›¾è¡¨ç¤ºï¼Œç¼–ç ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€å¤–è§‚å’Œç‰©ç†å±æ€§ï¼ŒåŒæ—¶è€ƒè™‘å±‚æ¬¡å’Œç‰©ä½“é—´çš„å…³ç³»ã€‚é€šè¿‡ç»“åˆåŸºäºé‡‡æ ·çš„æ¢ç´¢å’ŒåŸºäºæ¢¯åº¦çš„ç»†åŒ–ï¼Œä¼˜åŒ–è¿‡ç¨‹é«˜æ•ˆè¿›è¡Œï¼Œæœ€ç»ˆç”Ÿæˆçš„æ•°å­—åŒèƒèƒåœ¨æ–°è§†è§’ä¸‹å±•ç°å‡ºå®Œæ•´ç²¾ç¡®çš„å‡ ä½•å½¢çŠ¶å’ŒçœŸå®çš„æ¸²æŸ“æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05137', 'title': 'Demystifying deep search: a holistic evaluation with hint-free multi-hop\n  questions and factorised metrics', 'url': 'https://huggingface.co/papers/2510.05137', 'abstract': "WebDetective is a benchmark for evaluating multi-hop reasoning in RAG systems and web agents, addressing issues of reasoning path leakage and single-pass evaluation, and introducing a framework to improve knowledge utilization and refusal behavior.  \t\t\t\t\tAI-generated summary \t\t\t\t RAG (Retrieval-Augmented Generation) systems and web agents are increasingly evaluated on multi-hop deep search tasks, yet current practice suffers from two major limitations. First, most benchmarks leak the reasoning path in the question text, allowing models to follow surface cues rather than discover reasoning chains autonomously. Second, evaluation is typically reduced to a single pass rate, which collapses diverse behaviours into one score and obscures whether failures stem from inadequate search, poor knowledge use, or inappropriate refusal. To address these issues, we present WebDetective, a benchmark of hint-free multi-hop questions paired with a controlled Wikipedia sandbox that ensures full traceability of model actions, and a holistic evaluation framework that separates search sufficiency, knowledge utilisation, and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals systematic weaknesses across all architectures: models struggle with knowledge utilisation despite having sufficient evidence and demonstrate near-absent appropriate refusal when evidence is lacking. These patterns expose a fundamental gap: today's systems excel at executing given reasoning paths but fail when required to discover them. We develop an agentic workflow, EvidenceLoop, that explicitly targets the challenges our benchmark identifies, incorporating verification loops and systematic evidence tracking that improve both search and synthesis capabilities. This baseline demonstrates that WebDetective's diagnostic framework can guide concrete architectural improvements, establishing our benchmark as a critical tool for developing genuinely autonomous reasoning systems rather than pattern-following agents.", 'score': 4, 'issue_id': 6298, 'pub_date': '2025-10-01', 'pub_date_card': {'ru': '1 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 1', 'zh': '10æœˆ1æ—¥'}, 'hash': 'd780a77899923141', 'authors': ['Maojia Song', 'Renhang Liu', 'Xinyu Wang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Soujanya Poria', 'Jingren Zhou'], 'affiliations': ['Nanyang Technological University (NTU)', 'Singapore University of Technology and Design (SUTD)', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.05137.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rag', '#benchmark', '#leakage', '#architecture', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'WebDetective: ĞšĞ°Ğº Ğ½Ğ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ° Ğ½Ğµ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼', 'desc': 'WebDetective â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² RAG-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ñ…. Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ñ‹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Â«ÑƒÑ‚ĞµÑ‡ĞºÑƒÂ» Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞµ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¼Ñ‹ÑĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Wikipedia Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ñ‚ÑŒÑÑ Ğ¾Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 25 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ: ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ½ÑƒĞ¶Ğ½Ğ¾ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ.'}, 'en': {'title': 'WebDetective: Enhancing Multi-Hop Reasoning in AI Systems', 'desc': 'WebDetective is a new benchmark designed to evaluate how well RAG systems and web agents can perform multi-hop reasoning without leaking reasoning paths in the questions. It addresses the limitations of current evaluation methods that oversimplify model performance into a single score, which can hide specific weaknesses in knowledge utilization and refusal behavior. The benchmark includes hint-free questions and a controlled environment to track model actions, allowing for a more detailed analysis of how models search for information and use knowledge. The findings reveal that many models struggle with effectively utilizing available evidence and often fail to refuse when they lack sufficient information, highlighting the need for improvements in autonomous reasoning capabilities.'}, 'zh': {'title': 'WebDetectiveï¼šæå‡å¤šè·³æ¨ç†çš„è¯„ä¼°æ ‡å‡†', 'desc': 'WebDetectiveæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°RAGç³»ç»Ÿå’Œç½‘ç»œä»£ç†çš„å¤šè·³æ¨ç†åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³æ¨ç†è·¯å¾„æ³„æ¼å’Œå•æ¬¡è¯„ä¼°çš„é—®é¢˜ã€‚è¯¥åŸºå‡†æä¾›æ— æç¤ºçš„å¤šè·³é—®é¢˜ï¼Œå¹¶é…å¤‡ä¸€ä¸ªå—æ§çš„ç»´åŸºç™¾ç§‘æ²™ç®±ï¼Œä»¥ç¡®ä¿æ¨¡å‹è¡Œä¸ºçš„å¯è¿½æº¯æ€§ã€‚é€šè¿‡å¯¹25ä¸ªæœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨çŸ¥è¯†åˆ©ç”¨æ–¹é¢å­˜åœ¨ç³»ç»Ÿæ€§å¼±ç‚¹ï¼Œå°½ç®¡æœ‰è¶³å¤Ÿçš„è¯æ®ï¼Œä½†åœ¨ç¼ºä¹è¯æ®æ—¶å‡ ä¹æ²¡æœ‰é€‚å½“çš„æ‹’ç»è¡Œä¸ºã€‚æˆ‘ä»¬å¼€å‘äº†EvidenceLoopå·¥ä½œæµç¨‹ï¼Œä¸“é—¨é’ˆå¯¹åŸºå‡†è¯†åˆ«çš„æŒ‘æˆ˜ï¼Œæ”¹è¿›äº†æœç´¢å’Œç»¼åˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05342', 'title': 'Margin Adaptive DPO: Leveraging Reward Model for Granular Control in\n  Preference Optimization', 'url': 'https://huggingface.co/papers/2510.05342', 'abstract': 'MADPO, a margin-adaptive method, enhances preference alignment in large language models by providing instance-level adaptive weighting to the DPO loss, improving performance across datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of beta-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative beta values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.', 'score': 3, 'issue_id': 6298, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': '1a257d25fefce283', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.05342.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': 'âš–ï¸', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ´ĞµĞ»Ğ°ÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ', 'desc': 'MADPO â€” ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ DPO Ñ Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¾Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ reward model Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğº loss-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ DPO Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ sample. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ…, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ Ğ±Ğ°Ñ‚Ñ‡-ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ uniformĞ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +33.3% Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ baseline Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Preference Alignment with Adaptive Weighting', 'desc': "MADPO, or Margin-Adaptive Direct Preference Optimization, is a novel method designed to improve the alignment of large language models by adapting the weighting of the DPO loss at the instance level. This approach addresses the limitations of previous methods by providing a continuous and adaptive weight based on the estimated preference margins for each training sample. By amplifying the learning signal for difficult examples and reducing it for easier ones, MADPO enhances the model's ability to learn from diverse datasets effectively. Experimental results demonstrate that MADPO significantly outperforms existing methods, achieving notable performance improvements across various data quality levels."}, 'zh': {'title': 'è¾¹é™…è‡ªé€‚åº”ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹åå¥½å¯¹é½', 'desc': 'MADPOæ˜¯ä¸€ç§è¾¹é™…è‡ªé€‚åº”æ–¹æ³•ï¼Œé€šè¿‡ä¸ºDPOæŸå¤±æä¾›å®ä¾‹çº§è‡ªé€‚åº”åŠ æƒï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„åå¥½å¯¹é½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹æ¥ä¼°è®¡åå¥½è¾¹é™…ï¼Œç„¶åæ ¹æ®è¿™äº›è¾¹é™…ä¸ºæ¯ä¸ªè®­ç»ƒæ ·æœ¬åº”ç”¨è¿ç»­çš„è‡ªé€‚åº”æƒé‡ã€‚MADPOçš„é‡åŠ æƒæ–¹æ¡ˆå¯¹å›°éš¾æ ·æœ¬å¢å¼ºä¿¡å·ï¼Œå¯¹ç®€å•æ ·æœ¬å‡å¼±ä¿¡å·ï¼Œä»è€Œå®ç°äº†å¯¹å­¦ä¹ ä¿¡å·çš„ç»†è‡´æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMADPOåœ¨æƒ…æ„Ÿç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–å¼ºåŸºçº¿ï¼Œè¯æ˜äº†å…¶åœ¨åå¥½å¯¹é½æ–¹é¢çš„ç¨³å¥æ€§å’Œæœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05156', 'title': 'VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation', 'url': 'https://huggingface.co/papers/2510.05156', 'abstract': "VeriGuard is a framework that ensures formal safety guarantees for LLM-based agents through offline validation and online monitoring.  \t\t\t\t\tAI-generated summary \t\t\t\t The deployment of autonomous AI agents in sensitive domains, such as healthcare, introduces critical risks to safety, security, and privacy. These agents may deviate from user objectives, violate data handling policies, or be compromised by adversarial attacks. Mitigating these dangers necessitates a mechanism to formally guarantee that an agent's actions adhere to predefined safety constraints, a challenge that existing systems do not fully address. We introduce VeriGuard, a novel framework that provides formal safety guarantees for LLM-based agents through a dual-stage architecture designed for robust and verifiable correctness. The initial offline stage involves a comprehensive validation process. It begins by clarifying user intent to establish precise safety specifications. VeriGuard then synthesizes a behavioral policy and subjects it to both testing and formal verification to prove its compliance with these specifications. This iterative process refines the policy until it is deemed correct. Subsequently, the second stage provides online action monitoring, where VeriGuard operates as a runtime monitor to validate each proposed agent action against the pre-verified policy before execution. This separation of the exhaustive offline validation from the lightweight online monitoring allows formal guarantees to be practically applied, providing a robust safeguard that substantially improves the trustworthiness of LLM agents.", 'score': 3, 'issue_id': 6300, 'pub_date': '2025-10-03', 'pub_date_card': {'ru': '3 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 3', 'zh': '10æœˆ3æ—¥'}, 'hash': 'cfcbeb2c67a2faa9', 'authors': ['Lesly Miculicich', 'Mihir Parmar', 'Hamid Palangi', 'Krishnamurthy Dj Dvijotham', 'Mirko Montanari', 'Tomas Pfister', 'Long T. Le'], 'affiliations': ['Google Cloud AI', 'Google Cloud AI Research', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2510.05156.jpg', 'data': {'categories': ['#inference', '#alignment', '#agents', '#security', '#healthcare'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'VeriGuard â€” ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ² ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²Ğ° ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ ĞµĞ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼. Ğ¢Ğ°ĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ, Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»Ñ‘Ğ³ĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ°Ñ‰Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ, Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ adversarial-Ğ°Ñ‚Ğ°Ğº.'}, 'en': {'title': 'Ensuring Safety in AI Agents with VeriGuard', 'desc': "VeriGuard is a framework designed to ensure the safety of large language model (LLM)-based agents by providing formal guarantees through a two-stage process. The first stage involves offline validation, where user intent is clarified to create specific safety specifications, and a behavioral policy is synthesized and rigorously tested for compliance. The second stage focuses on online monitoring, where the agent's actions are continuously validated against the pre-verified policy before they are executed. This approach enhances the reliability of AI agents in sensitive areas by ensuring they adhere to safety constraints and reducing risks associated with their deployment."}, 'zh': {'title': 'VeriGuardï¼šç¡®ä¿æ™ºèƒ½ä½“å®‰å…¨çš„åŒé˜¶æ®µæ¡†æ¶', 'desc': 'VeriGuardæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™ºèƒ½ä½“æä¾›æ­£å¼çš„å®‰å…¨ä¿éšœã€‚å®ƒé€šè¿‡ç¦»çº¿éªŒè¯å’Œåœ¨çº¿ç›‘æ§çš„åŒé˜¶æ®µæ¶æ„ï¼Œç¡®ä¿æ™ºèƒ½ä½“çš„è¡Œä¸ºç¬¦åˆé¢„å®šä¹‰çš„å®‰å…¨çº¦æŸã€‚é¦–å…ˆï¼Œåœ¨ç¦»çº¿é˜¶æ®µï¼ŒVeriGuardé€šè¿‡æ˜ç¡®ç”¨æˆ·æ„å›¾æ¥å»ºç«‹å®‰å…¨è§„èŒƒï¼Œå¹¶åˆæˆè¡Œä¸ºç­–ç•¥ï¼Œç»è¿‡æµ‹è¯•å’Œæ­£å¼éªŒè¯ä»¥ç¡®ä¿åˆè§„ã€‚ç„¶åï¼Œåœ¨åœ¨çº¿é˜¶æ®µï¼ŒVeriGuardä½œä¸ºè¿è¡Œæ—¶ç›‘æ§å™¨ï¼ŒéªŒè¯æ¯ä¸ªæè®®çš„æ™ºèƒ½ä½“åŠ¨ä½œï¼Œä»¥ç¡®ä¿å…¶ç¬¦åˆé¢„å…ˆéªŒè¯çš„æ”¿ç­–ï¼Œä»è€Œæé«˜LLMæ™ºèƒ½ä½“çš„å¯ä¿¡åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05122', 'title': 'CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support\n  Conversation', 'url': 'https://huggingface.co/papers/2510.05122', 'abstract': 'CARE is a framework that enhances cognitive reasoning in emotional support conversations through reinforcement learning, improving response quality and empathy without relying on large-scale synthetic data.  \t\t\t\t\tAI-generated summary \t\t\t\t Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose CARE, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.', 'score': 3, 'issue_id': 6301, 'pub_date': '2025-09-30', 'pub_date_card': {'ru': '30 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 30', 'zh': '9æœˆ30æ—¥'}, 'hash': 'a08f3bb063148c1c', 'authors': ['Jie Zhu', 'Yuanchen Zhou', 'Shuo Jiang', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang', 'Fang Kong'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'School of Computer Science and Technology, Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2510.05122.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning'], 'emoji': 'ğŸ¤—', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CARE â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, CARE Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ reinforcement learning Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¼Ğ¿Ğ°Ñ‚Ğ¸Ğ¸ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ°Ñ….'}, 'en': {'title': 'Enhancing Emotional Support with Cognitive Reasoning', 'desc': 'CARE is a framework designed to improve emotional support conversations (ESC) by enhancing cognitive reasoning through reinforcement learning. Unlike previous methods that depend on large amounts of synthetic data, CARE focuses on using existing training data to create more coherent and empathetic responses. The framework emphasizes the importance of logical reasoning in generating supportive dialogue, which is crucial for effective emotional support. Experimental results show that CARE significantly boosts the quality and empathy of responses, making AI systems more human-like in their interactions.'}, 'zh': {'title': 'CAREï¼šæå‡æƒ…æ„Ÿæ”¯æŒå¯¹è¯çš„è®¤çŸ¥æ¨ç†èƒ½åŠ›', 'desc': 'CAREæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ å¢å¼ºæƒ…æ„Ÿæ”¯æŒå¯¹è¯ä¸­çš„è®¤çŸ¥æ¨ç†ï¼Œæå‡å“åº”è´¨é‡å’ŒåŒç†å¿ƒï¼Œè€Œä¸ä¾èµ–äºå¤§è§„æ¨¡çš„åˆæˆæ•°æ®ã€‚æƒ…æ„Ÿæ”¯æŒå¯¹è¯åœ¨ç¼“è§£å¿ƒç†å‹åŠ›å’Œæä¾›æƒ…æ„Ÿä»·å€¼æ–¹é¢èµ·ç€é‡è¦ä½œç”¨ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ•°æ®å¢å¼ºå’Œåˆæˆè¯­æ–™åº“çš„æ„å»ºä¸Šï¼Œå¿½è§†äº†æœ‰æ•ˆæƒ…æ„Ÿæ”¯æŒèƒŒåçš„æ·±å±‚è®¤çŸ¥æ¨ç†è¿‡ç¨‹ã€‚CAREåˆ©ç”¨åŸå§‹çš„æƒ…æ„Ÿæ”¯æŒå¯¹è¯è®­ç»ƒé›†ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆé€»è¾‘è¿è´¯å’Œæ”¯æŒæ€§çš„å“åº”ï¼Œä»è€Œæ˜¾è‘—æå‡è®¤çŸ¥æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.05367', 'title': 'LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation', 'url': 'https://huggingface.co/papers/2510.05367', 'abstract': 'The paper proposes stage-specific strategies to accelerate diffusion model inference in video generation, reducing memory usage and maintaining quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .', 'score': 2, 'issue_id': 6299, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 6', 'zh': '10æœˆ6æ—¥'}, 'hash': 'da5c54f2d1fce894', 'authors': ['Yang Xiao', 'Gen Li', 'Kaiyuan Deng', 'Yushu Wu', 'Zheng Zhan', 'Yanzhi Wang', 'Xiaolong Ma', 'Bo Hui'], 'affiliations': ['Clemson University', 'Microsoft Research', 'Northeastern University', 'The University of Arizona', 'University of Tulsa'], 'pdf_title_img': 'assets/pdf/title_img/2510.05367.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#video', '#optimization', '#inference'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ½Ğ° Ñ‚Ñ€Ğ¸ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ (ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ) Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€ĞµĞ·ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸: Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ğ¼ĞµĞ½ ĞºÑÑˆĞ°, Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ñ€ĞµĞ·ĞºĞ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Accelerating Video Generation with Stage-Specific Strategies', 'desc': 'This paper focuses on improving the efficiency of video generation using diffusion models by introducing stage-specific strategies. It identifies that the inference process can be broken down into three stages: encoding, denoising, and decoding, and highlights the memory issues that arise during the latter two stages. The authors propose methods such as Asynchronous Cache Swapping, Feature Chunking, and Slicing Latents to optimize memory usage without significantly increasing processing time. Overall, their approach results in faster inference speeds and reduced memory consumption while keeping quality loss minimal.'}, 'zh': {'title': 'é˜¶æ®µç‰¹å®šç­–ç•¥åŠ é€Ÿè§†é¢‘ç”Ÿæˆæ¨ç†', 'desc': 'æœ¬æ–‡æå‡ºäº†é’ˆå¯¹æ‰©æ•£æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆä¸­çš„æ¨ç†åŠ é€Ÿçš„é˜¶æ®µç‰¹å®šç­–ç•¥ï¼Œæ—¨åœ¨å‡å°‘å†…å­˜ä½¿ç”¨å¹¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºç¼–ç ã€å»å™ªå’Œè§£ç ä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶å‘ç°åŸºäºç¼“å­˜çš„åŠ é€Ÿæ–¹æ³•åœ¨åä¸¤ä¸ªé˜¶æ®µå¸¸å¸¸å¯¼è‡´å†…å­˜æ¿€å¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒé˜¶æ®µæ¨ç†çš„ç‰¹å¾ï¼Œå¹¶æå‡ºäº†ä¸‰ç§å‡å°‘å†…å­˜æ¶ˆè€—çš„ç­–ç•¥ï¼šå¼‚æ­¥ç¼“å­˜äº¤æ¢ã€ç‰¹å¾å—å’Œåˆ‡ç‰‡æ½œå˜é‡è§£ç ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ç¡®ä¿è¿™ä¸‰ç§ç­–ç•¥å¼•å…¥çš„æ—¶é—´å¼€é”€ä½äºåŠ é€Ÿå¸¦æ¥çš„æ”¶ç›Šã€‚ä¸åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—èŒƒå›´å†…çš„è´¨é‡ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.03978', 'title': 'No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language\n  Models', 'url': 'https://huggingface.co/papers/2510.03978', 'abstract': 'Extending the context length of text encoders in vision-language models improves performance on biomedical caption tasks by utilizing longer and more detailed descriptions.  \t\t\t\t\tAI-generated summary \t\t\t\t Embedding vision-language models (VLMs) are typically pretrained with short text windows (<77 tokens), which forces the truncation of long-format captions. Yet, the distribution of biomedical captions from large-scale open source literature reveals that a huge portion of captions far exceed 77 tokens. To this end, we investigate the impact of pretraining on long-format biomedical captions by extending the context length of text encoders in VLMs. We find that longer context (thus, enabling additional supervision provided in long-format captions) correlates with better retrieval and classification performance. Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M image-caption pairs enriched with context-aware descriptions from full-text articles, providing longer and additional textual supervision. Using BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a text encoder supporting windows of up to 512 tokens. Our model extends context capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in Recall@1 and +2% average improvements in classification, while also converging faster than short-context. Our results demonstrate that long-context modeling is a promising direction for advancing biomedical VLMs.', 'score': 2, 'issue_id': 6301, 'pub_date': '2025-10-04', 'pub_date_card': {'ru': '4 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 4', 'zh': '10æœˆ4æ—¥'}, 'hash': 'bb2f9d23dbe6e4fa', 'authors': ['Min Woo Sun', 'Alejandro Lozano', 'Javier Gamazo Tejero', 'Vishwesh Nath', 'Xiao Xiao Sun', 'James Burgess', 'Yuhui Zhang', 'Kun Yuan', 'Robert Tibshirani', 'Sean Huver', 'Serena Yeung-Levy'], 'affiliations': ['NVIDIA, USA', 'Stanford University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.03978.jpg', 'data': {'categories': ['#data', '#benchmark', '#long_context', '#healthcare', '#dataset', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ”Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ»Ğ¾Ğ² â€” Ğ»ÑƒÑ‡ÑˆĞµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ (Ğ´Ğ¾ 77 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²) Ñ‚ĞµÑ€ÑÑÑ‚ Ğ²Ğ°Ğ¶Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ BIOMEDICA-LongCAP Ğ¸Ğ· 1 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ñ‚ĞµĞºÑÑ‚ Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ BMC-LongCLIP, Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ´Ğ¾ 512 Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ² 6.6 Ñ€Ğ°Ğ· Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ° Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ 55% Ğ´Ğ¾ 2.2%, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° 30% Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° 2%. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğµ.'}, 'en': {'title': 'Unlocking the Power of Long Contexts in Biomedical Captioning', 'desc': 'This paper explores how increasing the context length of text encoders in vision-language models (VLMs) can enhance performance on biomedical captioning tasks. Traditional VLMs are limited to short text windows, which often leads to the loss of important information in longer biomedical captions. By extending the context length to 512 tokens, the authors introduce a new dataset, BIOMEDICA-LongCAP, which includes 1 million image-caption pairs with detailed descriptions. The results show that this approach significantly improves retrieval and classification metrics, highlighting the benefits of long-context modeling in biomedical applications.'}, 'zh': {'title': 'æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œæå‡ç”Ÿç‰©åŒ»å­¦æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­æ‰©å±•æ–‡æœ¬ç¼–ç å™¨çš„ä¸Šä¸‹æ–‡é•¿åº¦å¯¹ç”Ÿç‰©åŒ»å­¦æè¿°ä»»åŠ¡çš„å½±å“ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹é€šå¸¸ä½¿ç”¨è¾ƒçŸ­çš„æ–‡æœ¬çª—å£ï¼Œè¿™å¯¼è‡´é•¿æ ¼å¼çš„æè¿°è¢«æˆªæ–­ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡å¯ä»¥æé«˜æ£€ç´¢å’Œåˆ†ç±»çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒå…è®¸æ¨¡å‹åˆ©ç”¨æ›´è¯¦ç»†çš„æè¿°ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†BIOMEDICA-LongCAPï¼Œå¹¶è®­ç»ƒäº†æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„ç”Ÿç‰©åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹BMC-LongCLIPï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06218', 'title': 'EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark', 'url': 'https://huggingface.co/papers/2510.06218', 'abstract': 'EgoNight is a comprehensive benchmark for nighttime egocentric vision, focusing on visual question answering and revealing performance gaps between day and night conditions for multimodal large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': 'a863fc0993d7f2f6', 'authors': ['Deheng Zhang', 'Yuqian Fu', 'Runyi Yang', 'Yang Miao', 'Tianwen Qian', 'Xu Zheng', 'Guolei Sun', 'Ajad Chhatkuli', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Luc Van Gool', 'Danda Pani Paudel'], 'affiliations': ['East China Normal University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2510.06218.jpg', 'data': {'categories': ['#multimodal', '#long_context', '#benchmark', '#games', '#transfer_learning', '#synthetic', '#cv'], 'emoji': 'ğŸŒ™', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° AI-Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞ¼Ğ½Ğ¾Ñ‚Ğµ Ğ¾Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ»Ğ¸Ñ†Ğ°', 'desc': 'EgoNight â€” ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ egocentric-Ğ²Ğ¸Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° visual question answering. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 3658 Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° 90 Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ°Ğº Ğ² Ğ´Ğ½ĞµĞ²Ğ½Ğ¾Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ñ Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ†ĞµĞ½Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… multimodal LLM Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ½ĞµĞ²Ğ½Ñ‹Ñ… Ğº Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼. ĞŸĞ¾Ğ¼Ğ¸Ğ¼Ğ¾ VQA, Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞº ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ´ĞµĞ½ÑŒ-Ğ½Ğ¾Ñ‡ÑŒ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ² Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Bridging the Gap: Nighttime Vision for AI', 'desc': 'EgoNight is a new benchmark designed to improve nighttime egocentric vision, particularly in visual question answering (VQA). It highlights the performance differences of multimodal large language models (MLLMs) when operating in low-light conditions compared to daytime scenarios. The benchmark includes day-night aligned videos to enhance the quality of night annotations and reveals significant performance drops in models when transitioning from day to night. Additionally, EgoNight introduces tasks like day-night correspondence retrieval and egocentric depth estimation to further challenge and advance current models in this field.'}, 'zh': {'title': 'å¤œé—´è§†è§‰é—®ç­”çš„æ–°åŸºå‡†ï¼šEgoNight', 'desc': 'EgoNightæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºå¤œé—´è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ï¼Œç‰¹åˆ«æ˜¯è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡ã€‚ç°æœ‰çš„è‡ªæˆ‘ä¸­å¿ƒè§†è§‰åŸºå‡†ä¸»è¦é›†ä¸­åœ¨ç™½å¤©åœºæ™¯ï¼Œå¿½è§†äº†ä½å…‰ç…§æ¡ä»¶ä¸‹çš„åº”ç”¨éœ€æ±‚ã€‚EgoNighté€šè¿‡å¼•å…¥æ—¥å¤œå¯¹é½çš„è§†é¢‘ï¼Œæå‡äº†å¤œé—´æ ‡æ³¨çš„è´¨é‡ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹çš„æ€§èƒ½å·®è·ã€‚è¯¥åŸºå‡†åŒ…å«3658ä¸ªé—®ç­”å¯¹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªæˆ‘ä¸­å¿ƒè§†è§‰ç ”ç©¶çš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.06139', 'title': 'Deforming Videos to Masks: Flow Matching for Referring Video\n  Segmentation', 'url': 'https://huggingface.co/papers/2510.06139', 'abstract': "FlowRVS addresses the challenges of Referring Video Object Segmentation by reformulating the task as a continuous flow problem, leveraging pretrained T2V models and achieving state-of-the-art results.  \t\t\t\t\tAI-generated summary \t\t\t\t Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a J&F of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.", 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-07', 'pub_date_card': {'ru': '7 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 7', 'zh': '10æœˆ7æ—¥'}, 'hash': '27bd729b0bb095f1', 'authors': ['Zanyi Wang', 'Dengyang Jiang', 'Liuzhuozheng Li', 'Sizhe Dang', 'Chengzu Li', 'Harry Yang', 'Guang Dai', 'Mengmeng Wang', 'Jingdong Wang'], 'affiliations': ['Baidu', 'SGIT AI Lab, State Grid Corporation of China', 'The Hong Kong University of Science and Technology', 'The University of Tokyo', 'University of California, San Diego', 'University of Cambridge', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2510.06139.jpg', 'data': {'categories': ['#video', '#multimodal', '#benchmark', '#games', '#alignment'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ¡ĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'FlowRVS Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ (RVOS), Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ ĞµÑ‘ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Â«Ğ½Ğ°Ğ¹Ñ‚Ğ¸-Ğ·Ğ°Ñ‚ĞµĞ¼-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÂ», Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ pretrained text-to-video Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¼Ğ°ÑĞºÑƒ Ğ¿Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ²ÑĞµÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… RVOS, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MeViS Ğ¸ Ref-DAVIS17.'}, 'en': {'title': 'Revolutionizing Video Segmentation with Continuous Flow', 'desc': 'FlowRVS introduces a new way to tackle Referring Video Object Segmentation (RVOS) by treating it as a continuous flow problem. This approach allows the model to better connect language descriptions to specific video pixels, improving the segmentation process. Unlike previous methods that separate locating and segmenting tasks, FlowRVS maintains a unified framework that enhances temporal consistency and semantic understanding. By leveraging pretrained T2V models, it achieves state-of-the-art performance on major RVOS benchmarks, showcasing the effectiveness of continuous deformation in video understanding.'}, 'zh': {'title': 'FlowRVSï¼šè§†é¢‘ç‰©ä½“åˆ†å‰²çš„æ–°æ€è·¯', 'desc': 'FlowRVSæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•æ¥è§£å†³è§†é¢‘ç‰©ä½“åˆ†å‰²ä¸­çš„å¼•ç”¨é—®é¢˜ï¼Œå°†å…¶é‡æ–°å®šä¹‰ä¸ºä¸€ä¸ªè¿ç»­æµåŠ¨é—®é¢˜ã€‚è¯¥æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ï¼Œå®ç°äº†å¯¹è§†é¢‘ä¸­ç›®æ ‡çš„ç²¾ç»†åƒç´ æ§åˆ¶å’Œè¯­ä¹‰å¯¹é½ã€‚ä¸ä¼ ç»Ÿçš„â€œå®šä½-å†åˆ†å‰²â€æµç¨‹ä¸åŒï¼ŒFlowRVSé€šè¿‡ç›´æ¥å­¦ä¹ è¯­è¨€å¼•å¯¼çš„å˜å½¢ï¼Œä»è§†é¢‘çš„æ•´ä½“è¡¨ç¤ºåˆ°ç›®æ ‡æ©è†œï¼Œä¿æŒäº†æ—¶é—´ä¸€è‡´æ€§ã€‚è¯¥æ¡†æ¶åœ¨ä¸»è¦çš„å¼•ç”¨è§†é¢‘ç‰©ä½“åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ–°çš„æœ€å…ˆè¿›ç»“æœï¼Œå±•ç¤ºäº†å°†è§†é¢‘ç†è§£ä»»åŠ¡å»ºæ¨¡ä¸ºè¿ç»­å˜å½¢è¿‡ç¨‹çš„å·¨å¤§æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.04087', 'title': 'A Contextual Quality Reward Model for Reliable and Efficient Best-of-N\n  Sampling', 'url': 'https://huggingface.co/papers/2510.04087', 'abstract': 'A new framework using an outside option in preference data collection and modeling improves reliability and efficiency in preference alignment techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70\\%, and when tuned as an inference accelerator, it improves average inference speed by over 22\\% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-10-05', 'pub_date_card': {'ru': '5 Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 'en': 'October 5', 'zh': '10æœˆ5æ—¥'}, 'hash': '77a7f937000c3b18', 'authors': ['Hyung Gyu Rho'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.04087.jpg', 'data': {'categories': ['#inference', '#training', '#data', '#rlhf', '#alignment'], 'emoji': 'ğŸš¦', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾ Ğ»ÑƒÑ‡ÑˆĞµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ Â«Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¾Ğ¿Ñ†Ğ¸ÑÂ» Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ reward models ÑƒĞ¼ĞµÑÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ, ĞºĞ°ĞºĞ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ, Ğ½Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ, ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ²Ğ¾Ğ¾Ğ±Ñ‰Ğµ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ğ¼. ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ°Ñ‚ÑŒ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ½Ğ¾ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° 70% Ğ¸ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ inference Ğ½Ğ° 22%.'}, 'en': {'title': 'Enhancing Preference Alignment with Outside Options', 'desc': 'This paper presents a new framework for improving preference alignment techniques in machine learning by incorporating an outside option in data collection. Traditional methods, like Best-of-N sampling, often fail to identify acceptable responses, leading to poor choices among suboptimal options. The proposed framework enhances reward models by allowing them to recognize not only better options but also those that are sufficiently good. Experimental results demonstrate significant improvements in reliability and efficiency, reducing failures by 70% and increasing inference speed by over 22%.'}, 'zh': {'title': 'æå‡åå¥½å¯¹é½çš„å¯é æ€§ä¸æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡åœ¨åå¥½æ•°æ®æ”¶é›†å’Œå»ºæ¨¡ä¸­å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œæå‡äº†åå¥½å¯¹é½æŠ€æœ¯çš„å¯é æ€§å’Œæ•ˆç‡ã€‚ç°ä»£çš„åå¥½å¯¹é½æŠ€æœ¯ï¼Œå¦‚æœ€ä½³Nï¼ˆBoNï¼‰é‡‡æ ·ï¼Œä¾èµ–äºé€šè¿‡æˆå¯¹æ¯”è¾ƒæ•°æ®è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œè™½ç„¶èƒ½æœ‰æ•ˆå­¦ä¹ ç›¸å¯¹åå¥½ï¼Œä½†æœªèƒ½æ•æ‰å“åº”å¯æ¥å—æ€§çš„ä¿¡å·ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥å¤–éƒ¨é€‰é¡¹ï¼Œè®­ç»ƒå‡ºèƒ½å¤ŸåŒºåˆ†ä¸ä»…æ˜¯æ›´å¥½è€Œæ˜¯è¶³å¤Ÿå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œä»è€Œè§£å†³äº†å¯é æ€§ç¼ºå£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¯¹é½å’Œæ¨ç†åŠ é€Ÿæ–¹é¢å‡æ˜¾è‘—æé«˜äº†æ€§èƒ½ï¼Œæä¾›äº†ä¸€ä¸ªçµæ´»çš„ç®¡ç†å¯é æ€§ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´æƒè¡¡çš„å·¥å…·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2510.02341', 'title': 'DRIFT: Learning from Abundant User Dissatisfaction in Real-World\n  Preference Learning', 'url': 'https://huggingface.co/papers/2510.02341', 'abstract': 'DRIFT, a dissatisfaction-refined iterative preference training method, improves large language models using implicit user dissatisfaction signals, achieving better performance than existing methods on real-world datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce DRIFT (Dissatisfaction-Refined Iterative preFerence Training), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world WildFeedback datasets and synthetic UltraFeedback datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.', 'score': 1, 'issue_id': 6298, 'pub_date': '2025-09-27', 'pub_date_card': {'ru': '27 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 27', 'zh': '9æœˆ27æ—¥'}, 'hash': '1bde94710320cd61', 'authors': ['Yifan Wang', 'Bolian Li', 'Junlin Wu', 'Zhaoxuan Tan', 'Zheli Liu', 'Ruqi Zhang', 'Ananth Grama', 'Qingkai Zeng'], 'affiliations': ['College of Computer Science, Nankai University', 'Department of Computer Science and Engineering, University of Notre Dame', 'Department of Computer Science, Purdue University', 'Department of Computer Science, Washington University in St. Louis'], 'pdf_title_img': 'assets/pdf/title_img/2510.02341.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#rlhf'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ½Ğ° Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ DRIFT Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€Ñ‘Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº ÑĞ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ ÑĞ²ÑĞ·ÑŒ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°ĞµÑ‚ÑÑ Ñ€ĞµĞ´ĞºĞ¾. DRIFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ‚Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»ÑŒÑÑ‚Ğ²Ğ° ĞºĞ°Ğº ÑĞºĞ¾Ñ€ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… WildFeedback Ğ¸ UltraFeedback Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ +12.29% Ğ¿Ğ¾ win rate Ğ½Ğ° AlpacaEval2, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ°Ğº DPO Ğ¸ SPIN, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Harnessing User Dissatisfaction for Better Language Models', 'desc': 'The paper introduces DRIFT, a novel training method for large language models that utilizes implicit user dissatisfaction signals to enhance performance. Unlike traditional methods that depend on explicit positive feedback, DRIFT focuses on refining preferences based on real-world user interactions, which often include dissatisfaction. This approach allows for dynamic sampling of positive responses, leading to improved model performance on various tasks. Empirical results demonstrate that DRIFT significantly outperforms existing methods, particularly in larger models, while also maintaining diversity in generated outputs.'}, 'zh': {'title': 'åˆ©ç”¨ç”¨æˆ·ä¸æ»¡ä¿¡å·æå‡è¯­è¨€æ¨¡å‹æ€§èƒ½', 'desc': 'DRIFTæ˜¯ä¸€ç§åŸºäºç”¨æˆ·ä¸æ»¡ä¿¡å·çš„è¿­ä»£åå¥½è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•åˆ©ç”¨çœŸå®ä¸–ç•Œä¸­çš„ç”¨æˆ·ä¸æ»¡ä¿¡å·ï¼ŒåŠ¨æ€é‡‡æ ·æ­£åé¦ˆï¼Œä»è€Œæ›´å¥½åœ°é€‚åº”ç”¨æˆ·çš„éœ€æ±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨DRIFTè®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿæ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡æ¨¡å‹ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚DRIFTä¸ä»…æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿˜ä¿æŒäº†æ¢ç´¢èƒ½åŠ›ï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´å¤šæ ·åŒ–çš„é«˜å¥–åŠ±è§£å†³æ–¹æ¡ˆã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (4)', '#agi', '#alignment (6)', '#architecture (4)', '#audio', '#benchmark (9)', '#cv (1)', '#data (6)', '#dataset (5)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare (3)', '#inference (5)', '#interpretability (2)', '#leakage (1)', '#long_context (3)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (6)', '#open_source (3)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (4)', '#rlhf (5)', '#robotics', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-10-08 06:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-08 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-08 06:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    