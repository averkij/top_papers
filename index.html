
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. February 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 февраля</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-27.html">⬅️ <span id="prev-date">27.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-03.html">➡️ <span id="next-date">03.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'};
        let feedDateNext = {'ru': '03.03', 'en': '03/03', 'zh': '3月3日'};
        let feedDatePrev = {'ru': '27.02', 'en': '02/27', 'zh': '2月27日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.19613', 'title': 'Self-rewarding correction for mathematical reasoning', 'url': 'https://huggingface.co/papers/2502.19613', 'abstract': "We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.", 'score': 50, 'issue_id': 2455, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': 'e2535efc8aadcc9d', 'authors': ['Wei Xiong', 'Hanning Zhang', 'Chenlu Ye', 'Lichang Chen', 'Nan Jiang', 'Tong Zhang'], 'affiliations': ['University of Illinois Urbana-Champaign', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.19613.jpg', 'data': {'categories': ['#reasoning', '#training', '#inference', '#optimization', '#rl'], 'emoji': '🤖', 'ru': {'title': 'Самокорректирующиеся языковые модели: новый шаг к автономному ИИ', 'desc': 'Исследователи изучают языковые модели с самовознаграждением, способные генерировать пошаговые рассуждения и оценивать их корректность без внешней обратной связи. Предложен двухэтапный алгоритмический подход для создания таких моделей, использующий только самогенерируемые данные. На первом этапе применяется последовательная выборка с отклонением для синтеза длинных цепочек рассуждений, включающих механизмы самовознаграждения и самокоррекции. Второй этап усиливает способность моделей оценивать точность ответов и улучшать выходные данные с помощью обучения с подкреплением.'}, 'en': {'title': 'Empowering LLMs with Self-Rewarding Reasoning and Self-Correction', 'desc': "This paper explores self-rewarding reasoning in large language models (LLMs), enabling them to generate and evaluate their own reasoning without needing outside feedback. The focus is on self-correction, where models can identify and fix their mistakes independently. The authors introduce a two-stage framework that first uses sequential rejection sampling to create data for training the models on self-rewarding and self-correction. The second stage enhances the models' accuracy assessment and output refinement through reinforcement learning, showing that their method outperforms traditional self-correction techniques."}, 'zh': {'title': '自我奖励推理：模型的独立思考与修正', 'desc': '我们研究了自我奖励推理的大型语言模型（LLMs），这些模型能够在推理过程中同时生成逐步推理并评估输出的正确性，而无需外部反馈。这种集成方法使得单一模型能够独立引导其推理过程，为模型部署提供了计算优势。我们特别关注自我修正的任务，模型能够自主检测响应中的错误，修正输出，并决定何时终止迭代优化循环。为此，我们提出了一种两阶段的算法框架，利用自生成的数据构建自我奖励推理模型。'}}}, {'id': 'https://huggingface.co/papers/2502.19634', 'title': 'MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2502.19634', 'abstract': 'Reasoning is a critical frontier for advancing medical image analysis, where transparency and trustworthiness play a central role in both clinician trust and regulatory approval. Although Medical Visual Language Models (VLMs) show promise for radiological tasks, most existing VLMs merely produce final answers without revealing the underlying reasoning. To address this gap, we introduce MedVLM-R1, a medical VLM that explicitly generates natural language reasoning to enhance transparency and trustworthiness. Instead of relying on supervised fine-tuning (SFT), which often suffers from overfitting to training distributions and fails to foster genuine reasoning, MedVLM-R1 employs a reinforcement learning framework that incentivizes the model to discover human-interpretable reasoning paths without using any reasoning references. Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples. It also demonstrates robust domain generalization under out-of-distribution tasks. By unifying medical image analysis with explicit reasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable AI in clinical practice.', 'score': 43, 'issue_id': 2463, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': 'dffe0cc2daf1f23e', 'authors': ['Jiazhen Pan', 'Che Liu', 'Junde Wu', 'Fenglin Liu', 'Jiayuan Zhu', 'Hongwei Bran Li', 'Chen Chen', 'Cheng Ouyang', 'Daniel Rueckert'], 'affiliations': ['Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM) and TUM University Hospital, Germany', 'Data Science Institute, Imperial College London, UK', 'Department of Computing, Imperial College London, UK', 'Department of Engineering Science, University of Oxford, UK', 'Massachusetts General Hospital, Harvard Medical School, USA', 'School of Computer Science, University of Sheffield, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.19634.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#rl', '#training', '#healthcare'], 'emoji': '🧠', 'ru': {'title': 'Прозрачный ИИ для медицинской визуализации: рассуждения вместо чёрного ящика', 'desc': 'MedVLM-R1 - это новая модель визуального языка для медицинского анализа изображений, которая генерирует объяснения своих выводов на естественном языке. В отличие от обычного обучения с учителем, модель использует обучение с подкреплением для формирования интерпретируемых цепочек рассуждений. Несмотря на ограниченный объем обучающих данных, MedVLM-R1 значительно повышает точность анализа МРТ, КТ и рентгеновских снимков по сравнению с более крупными моделями. Это важный шаг к созданию прозрачного и надежного ИИ для клинической практики.'}, 'en': {'title': 'Enhancing Trust in Medical AI with Transparent Reasoning', 'desc': 'This paper presents MedVLM-R1, a Medical Visual Language Model designed to improve reasoning transparency in medical image analysis. Unlike traditional models that only provide final answers, MedVLM-R1 generates natural language explanations for its decisions, enhancing trust among clinicians. It utilizes a reinforcement learning approach to develop reasoning paths without relying on extensive training data or references, which helps avoid overfitting. The model shows significant performance improvements on various imaging benchmarks, indicating its potential for reliable and interpretable AI in healthcare.'}, 'zh': {'title': '医学影像分析中的透明推理新突破', 'desc': '本论文介绍了一种新的医学视觉语言模型MedVLM-R1，旨在提高医学图像分析中的透明度和可信度。与传统模型不同，MedVLM-R1能够生成自然语言推理，帮助医生理解模型的决策过程。该模型采用强化学习框架，而不是监督微调，避免了过拟合问题，并能发现可被人类理解的推理路径。尽管训练数据有限，MedVLM-R1在多个医学影像基准测试中表现出色，准确率显著提高，标志着可信赖的临床人工智能的重要进展。'}}}, {'id': 'https://huggingface.co/papers/2502.20395', 'title': 'R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2502.20395', 'abstract': 'In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)\' powerful reasoning capabilities, deterring LMMs\' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method "Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs\' performance on challenging benchmarks of diverse tasks, without training any base-model parameters.', 'score': 33, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'e8862deee761c4d0', 'authors': ['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['Johns Hopkins University', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.20395.jpg', 'data': {'categories': ['#multimodal', '#training', '#architecture', '#optimization', '#benchmark'], 'emoji': '🔀', 'ru': {'title': 'Оптимизация маршрутизации на лету для повышения эффективности мультимодальных моделей', 'desc': 'В статье представлен метод Re-Routing in Test-Time (R2-T2) для улучшения работы мультимодальных моделей на основе смеси экспертов (MoE). Авторы обнаружили, что маршрутизатор, обученный по принципу end-to-end, не всегда оптимально распределяет веса между экспертами для тестовых образцов. R2-T2 оптимизирует вектор весов маршрутизации во время тестирования, приближая его к векторам правильно предсказанных соседних образцов. Предложены три стратегии R2-T2 с различными целями оптимизации и пространствами поиска соседей.'}, 'en': {'title': 'Optimizing Multimodal Performance with Test-Time Re-Routing', 'desc': 'This paper addresses the performance gap in large multimodal models (LMMs) when processing non-language data compared to large language models (LLMs). The authors introduce a mixture-of-experts (MoE) approach to enhance the vision encoder, allowing for richer and more diverse representations. They identify that the router, which determines how to mix these expert representations, often fails to optimize routing weights effectively during testing. To solve this, they propose a method called Re-Routing in Test-Time (R2-T2), which fine-tunes routing weights based on nearby correctly predicted samples, significantly boosting the performance of LMMs on various challenging tasks without retraining the base model.'}, 'zh': {'title': '提升多模态模型性能的新方法', 'desc': '在大型多模态模型（LMMs）中，视觉表示的感知能力通常不如大型语言模型（LLMs）的推理能力，这影响了LMMs在复杂任务上的表现。最近，通过用专家混合（MoE）替换视觉编码器，缓解了这一弱点，提供了丰富且多样的表示。我们发现，端到端训练的路由器并不总能为每个测试样本生成最佳的路由权重。为了解决这个问题，我们提出了一种新颖且高效的方法“测试时重新路由（R2-T2）”，通过在测试时优化路由权重向量，显著提升了LMMs在多样化任务基准上的表现。'}}}, {'id': 'https://huggingface.co/papers/2502.20082', 'title': 'LongRoPE2: Near-Lossless LLM Context Window Scaling', 'url': 'https://huggingface.co/papers/2502.20082', 'abstract': 'LongRoPE2 is a novel approach that extends the effective context window of pre-trained large language models (LLMs) to the target length, while preserving the performance on the original shorter context window. This is achieved by three contributions: (1) a hypothesis that insufficient training in higher RoPE dimensions contributes to the persistent out-of-distribution (OOD) issues observed in existing methods; (2) an effective RoPE rescaling algorithm that adopts evolutionary search guided by "needle-driven" perplexity to address the insufficient training problem; (3) a mixed context window training approach that fine-tunes model weights to adopt rescaled RoPE for long-context sequences while preserving the short-context performance with the original RoPE. Extensive experiments on LLaMA3-8B and Phi3-mini-3.8B across various benchmarks validate the hypothesis and demonstrate the effectiveness of LongRoPE2. Remarkably, LongRoPE2 extends LLaMA3-8B to achieve a 128K effective context length while retaining over 98.5% of short-context performance, using only 10B tokens -- 80x fewer than Meta\'s approach, which fails to reach the target effective context length. Code will be available at https://github.com/microsoft/LongRoPE.', 'score': 21, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'ee15387b2b27d4c6', 'authors': ['Ning Shang', 'Li Lyna Zhang', 'Siyuan Wang', 'Gaokai Zhang', 'Gilsinia Lopez', 'Fan Yang', 'Weizhu Chen', 'Mao Yang'], 'affiliations': ['Microsoft', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20082.jpg', 'data': {'categories': ['#training', '#architecture', '#benchmark', '#long_context'], 'emoji': '🔬', 'ru': {'title': 'Расширение контекста языковых моделей без потери качества', 'desc': 'LongRoPE2 - это новый подход к расширению эффективного контекстного окна предобученных больших языковых моделей (LLM) до целевой длины. Метод основан на гипотезе о недостаточном обучении в высших измерениях RoPE и использует эволюционный поиск для эффективного масштабирования RoPE. LongRoPE2 применяет смешанное обучение на контекстах разной длины для адаптации весов модели. Эксперименты показали, что LongRoPE2 может расширить контекст LLaMA3-8B до 128 тысяч токенов, сохраняя производительность на коротких контекстах.'}, 'en': {'title': 'Extending Context Length Without Compromise', 'desc': 'LongRoPE2 is a new method that enhances the context length of large language models (LLMs) while maintaining their performance on shorter contexts. It introduces a hypothesis that inadequate training in higher dimensions of RoPE leads to out-of-distribution issues in existing models. The method employs a RoPE rescaling algorithm that uses evolutionary search to improve training effectiveness. Additionally, it utilizes a mixed context window training strategy to fine-tune model weights, allowing for long-context sequences without sacrificing short-context performance.'}, 'zh': {'title': '扩展上下文窗口，保持性能的创新方法', 'desc': 'LongRoPE2是一种新方法，旨在扩展预训练大型语言模型（LLMs）的有效上下文窗口，同时保持在原始较短上下文窗口上的性能。该方法通过三个贡献实现：首先，提出了一个假设，认为在更高RoPE维度上的训练不足导致了现有方法中持续存在的分布外（OOD）问题；其次，提出了一种有效的RoPE重缩放算法，通过“针驱动”的困惑度指导的进化搜索来解决训练不足的问题；最后，采用混合上下文窗口训练方法，微调模型权重以适应长上下文序列的重缩放RoPE，同时保持短上下文的原始RoPE性能。'}}}, {'id': 'https://huggingface.co/papers/2502.20238', 'title': "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", 'url': 'https://huggingface.co/papers/2502.20238', 'abstract': 'Many challenging reasoning tasks require not just rapid, intuitive responses, but a more deliberate, multi-step approach. Recent progress in large language models (LLMs) highlights an important shift from the "System 1" way of quick reactions to the "System 2" style of reflection-and-correction problem solving. However, current benchmarks heavily rely on the final-answer accuracy, leaving much of a model\'s intermediate reasoning steps unexamined. This fails to assess the model\'s ability to reflect and rectify mistakes within the reasoning process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark for fine-grained evaluation of LLMs\' reasoning capabilities. Each puzzle can be decomposed into atomic steps, making it ideal for rigorous validation of intermediate correctness. Building on this, we introduce two tasks: state checking, and state transition, for a comprehensive evaluation of how models assess the current situation and plan the next move. To support broader research, we also provide a puzzle training set aimed at enhancing performance on general mathematical tasks. We show that models trained on our state checking and transition data demonstrate gains in math reasoning by up to 5.1% on GSM8K.', 'score': 19, 'issue_id': 2458, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'e121721fdef71315', 'authors': ['Guizhen Chen', 'Weiwen Xu', 'Hao Zhang', 'Hou Pong Chan', 'Chaoqun Liu', 'Lidong Bing', 'Deli Zhao', 'Anh Tuan Luu', 'Yu Rong'], 'affiliations': ['DAMO Academy, Alibaba Group, Singapore', 'Nanyang Technological University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.20238.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#math'], 'emoji': '🧩', 'ru': {'title': 'FINEREASON: Новый подход к оценке рассуждений искусственного интеллекта', 'desc': 'Статья представляет новый бенчмарк FINEREASON для оценки способностей больших языковых моделей (LLM) к многоступенчатому рассуждению. В отличие от существующих бенчмарков, FINEREASON позволяет оценивать промежуточные шаги рассуждений, а не только конечный результат. Бенчмарк включает задачи проверки состояния и перехода между состояниями для комплексной оценки. Авторы также предоставляют набор данных для обучения, который улучшает результаты в задачах математического рассуждения.'}, 'en': {'title': 'Enhancing Reasoning in Language Models with FINEREASON', 'desc': "This paper introduces FINEREASON, a new benchmark designed to evaluate the reasoning capabilities of large language models (LLMs) through logic puzzles. Unlike traditional benchmarks that focus solely on final-answer accuracy, FINEREASON emphasizes the importance of intermediate reasoning steps, allowing for a more detailed assessment of a model's reflective and corrective abilities. The benchmark includes two specific tasks: state checking and state transition, which help evaluate how models understand their current context and plan their next actions. The authors demonstrate that training on this benchmark can improve LLM performance in mathematical reasoning tasks by up to 5.1%."}, 'zh': {'title': '提升推理能力的细致评估', 'desc': '许多复杂的推理任务需要不仅快速的直觉反应，还需要更深思熟虑的多步骤方法。最近大型语言模型（LLMs）的进展显示，从快速反应的“系统1”转向反思和纠正问题解决的“系统2”风格是一个重要的变化。当前的基准测试主要依赖最终答案的准确性，忽视了模型在推理过程中的中间步骤，这无法评估模型反思和纠正错误的能力。为了解决这个问题，我们提出了FINEREASON，这是一个逻辑难题基准，用于细致评估LLMs的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2502.16645', 'title': 'CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale', 'url': 'https://huggingface.co/papers/2502.16645', 'abstract': "Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.", 'score': 15, 'issue_id': 2456, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 февраля', 'en': 'February 23', 'zh': '2月23日'}, 'hash': '616cf3f6f2ab1d17', 'authors': ['Chenlong Wang', 'Zhaoyang Chu', 'Zhengxiang Cheng', 'Xuyi Yang', 'Kaiyue Qiu', 'Yao Wan', 'Zhou Zhao', 'Xuanhua Shi', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'Wuhuan University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16645.jpg', 'data': {'categories': ['#dataset', '#open_source', '#data', '#optimization', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Синхронизация языковых моделей с эволюцией кода', 'desc': 'Статья представляет CODESYNC - инструмент для выявления устаревших паттернов кода и сбора обновлений знаний о коде из сторонних библиотек Python в реальном времени. На основе CODESYNC разработан CODESYNCBENCH - комплексный бенчмарк для оценки способности моделей большого языка (LLM) синхронизироваться с эволюцией кода, охватывающий реальные обновления для 220 API из шести библиотек Python. Эксперименты на 14 современных LLM показали, что они испытывают трудности с динамической эволюцией кода даже при поддержке продвинутых методов обновления знаний. Авторы полагают, что их бенчмарк может стать основой для разработки более эффективных методов обновления знаний о коде в реальном времени.'}, 'en': {'title': 'CODESYNC: Keeping Code Knowledge Fresh for LLMs', 'desc': "This paper addresses the limitations of Large Language Models (LLMs) in adapting to changes in third-party library APIs, which can lead to outdated or inefficient code. It introduces CODESYNC, a data engine designed to identify outdated code patterns and gather real-time updates from Python libraries. Additionally, the authors present CODESYNCBENCH, a benchmark for evaluating LLMs' performance in keeping up with code evolution, featuring 3,300 test cases across various tasks. The findings indicate that even advanced LLMs struggle with dynamic code changes, highlighting the need for improved methods for real-time code knowledge updating."}, 'zh': {'title': '实时代码知识更新的基准测试', 'desc': '大型语言模型（LLMs）在软件工程中表现出色，但在适应不断变化的代码知识方面面临挑战，尤其是第三方库API的频繁更新。由于静态的预训练数据集，这种限制常常导致生成的代码无法执行或实现的安全性和效率不佳。为了解决这个问题，本文提出了CODESYNC，一个用于识别过时代码模式并收集来自Python第三方库的实时代码知识更新的数据引擎。基于CODESYNC，我们开发了CODESYNCBENCH，一个全面的基准测试，用于评估LLMs在代码演变中的同步能力，涵盖了来自六个Python库的220个API的真实更新。'}}}, {'id': 'https://huggingface.co/papers/2502.20321', 'title': 'UniTok: A Unified Tokenizer for Visual Generation and Understanding', 'url': 'https://huggingface.co/papers/2502.20321', 'abstract': 'The representation disparity between visual generation and understanding imposes a critical gap in integrating these capabilities into a single framework. To bridge this gap, we introduce UniTok, a discrete visual tokenizer that encodes fine-grained details for generation while also capturing high-level semantics for understanding. Despite recent studies have shown that these objectives could induce loss conflicts in training, we reveal that the underlying bottleneck stems from limited representational capacity of discrete tokens. We address this by introducing multi-codebook quantization, which divides vector quantization with several independent sub-codebooks to expand the latent feature space, while avoiding training instability caused by overlarge codebooks. Our method significantly raises the upper limit of unified discrete tokenizers to match or even surpass domain-specific continuous tokenizers. For instance, UniTok achieves a remarkable rFID of 0.38 (versus 0.87 for SD-VAE) and a zero-shot accuracy of 78.6% (versus 76.2% for CLIP) on ImageNet. Our code is available at https://github.com/FoundationVision/UniTok.', 'score': 13, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '1e65564c1594ef39', 'authors': ['Chuofan Ma', 'Yi Jiang', 'Junfeng Wu', 'Jihan Yang', 'Xin Yu', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaojuan Qi'], 'affiliations': ['ByteDance Inc.', 'Huazhong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.20321.jpg', 'data': {'categories': ['#training', '#architecture', '#multimodal', '#cv', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'UniTok: Единый токенизатор для генерации и понимания изображений', 'desc': 'UniTok - это дискретный визуальный токенизатор, который объединяет возможности генерации и понимания изображений. Он использует мультикодовую квантизацию для расширения пространства латентных признаков, избегая при этом нестабильности обучения. UniTok превосходит специализированные непрерывные токенизаторы, достигая rFID 0.38 и точности zero-shot классификации 78.6% на ImageNet. Это решение преодолевает разрыв между визуальной генерацией и пониманием в единой модели.'}, 'en': {'title': 'Bridging Visual Generation and Understanding with UniTok', 'desc': 'This paper presents UniTok, a novel discrete visual tokenizer designed to improve the integration of visual generation and understanding. It addresses the challenges of representation disparity by using multi-codebook quantization, which enhances the capacity of discrete tokens without causing training instability. The method allows for better encoding of both fine-grained details and high-level semantics, leading to improved performance metrics. UniTok outperforms existing models, achieving higher accuracy and fidelity on tasks like image classification compared to traditional continuous tokenizers.'}, 'zh': {'title': 'UniTok：统一视觉生成与理解的突破', 'desc': '本文提出了一种名为UniTok的离散视觉标记器，旨在解决视觉生成与理解之间的表示差异。UniTok能够编码细粒度的细节以进行生成，同时捕捉高层次的语义以便于理解。我们通过引入多代码本量化来扩展潜在特征空间，从而克服离散标记的表示能力限制。实验结果表明，UniTok在ImageNet上取得了显著的性能提升，超越了领域特定的连续标记器。'}}}, {'id': 'https://huggingface.co/papers/2502.19587', 'title': 'NeoBERT: A Next-Generation BERT', 'url': 'https://huggingface.co/papers/2502.19587', 'abstract': 'Recent innovations in architecture, pre-training, and fine-tuning have led to the remarkable in-context learning and reasoning abilities of large auto-regressive language models such as LLaMA and DeepSeek. In contrast, encoders like BERT and RoBERTa have not seen the same level of progress despite being foundational for many downstream NLP applications. To bridge this gap, we introduce NeoBERT, a next-generation encoder that redefines the capabilities of bidirectional models by integrating state-of-the-art advancements in architecture, modern data, and optimized pre-training methodologies. NeoBERT is designed for seamless adoption: it serves as a plug-and-play replacement for existing base models, relies on an optimal depth-to-width ratio, and leverages an extended context length of 4,096 tokens. Despite its compact 250M parameter footprint, it achieves state-of-the-art results on the massive MTEB benchmark, outperforming BERT large, RoBERTa large, NomicBERT, and ModernBERT under identical fine-tuning conditions. In addition, we rigorously evaluate the impact of each modification on GLUE and design a uniform fine-tuning and evaluation framework for MTEB. We release all code, data, checkpoints, and training scripts to accelerate research and real-world adoption.', 'score': 10, 'issue_id': 2461, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '7233167cf96a00e2', 'authors': ['Lola Le Breton', 'Quentin Fournier', 'Mariam El Mezouar', 'Sarath Chandar'], 'affiliations': ['Canada CIFAR AI Chair', 'Chandar Research Lab', 'Mila Quebec AI Institute', 'Polytechnique Montréal', 'Royal Military College of Canada'], 'pdf_title_img': 'assets/pdf/title_img/2502.19587.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#long_context', '#open_source', '#training'], 'emoji': '🚀', 'ru': {'title': 'NeoBERT: Революция в двунаправленных языковых моделях', 'desc': 'NeoBERT - это новое поколение энкодера, который переопределяет возможности двунаправленных моделей, интегрируя современные достижения в архитектуре, данных и методологиях предобучения. Несмотря на компактный размер в 250 миллионов параметров, NeoBERT достигает передовых результатов на масштабном бенчмарке MTEB, превосходя более крупные модели. Модель разработана для легкого внедрения и может служить заменой существующим базовым моделям. Авторы провели тщательную оценку влияния каждой модификации и разработали унифицированную структуру для дообучения и оценки на MTEB.'}, 'en': {'title': 'NeoBERT: Redefining Bidirectional Models for Superior NLP Performance', 'desc': 'This paper introduces NeoBERT, a new encoder model that enhances the capabilities of bidirectional models in natural language processing. Unlike previous models like BERT and RoBERTa, NeoBERT incorporates advanced architectural innovations and optimized pre-training techniques to improve performance. It is designed to be easily integrated into existing systems, featuring a compact size of 250 million parameters while supporting a longer context of 4,096 tokens. NeoBERT achieves state-of-the-art results on the MTEB benchmark, demonstrating its effectiveness compared to other leading models under the same fine-tuning conditions.'}, 'zh': {'title': 'NeoBERT：双向编码器的新突破', 'desc': '本文介绍了NeoBERT，这是一种新一代的双向编码器，旨在提升自然语言处理的能力。与传统的BERT和RoBERTa相比，NeoBERT结合了最新的架构、现代数据和优化的预训练方法，显著提高了性能。它具有250M参数，能够在MTEB基准测试中超越多个现有模型，且易于替换现有基础模型。我们还提供了所有代码、数据和训练脚本，以促进研究和实际应用。'}}}, {'id': 'https://huggingface.co/papers/2502.20172', 'title': 'Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think', 'url': 'https://huggingface.co/papers/2502.20172', 'abstract': 'The field of advanced text-to-image generation is witnessing the emergence of unified frameworks that integrate powerful text encoders, such as CLIP and T5, with Diffusion Transformer backbones. Although there have been efforts to control output images with additional conditions, like canny and depth map, a comprehensive framework for arbitrary text-image interleaved control is still lacking. This gap is especially evident when attempting to merge concepts or visual elements from multiple images in the generation process. To mitigate the gap, we conducted preliminary experiments showing that large multimodal models (LMMs) offer an effective shared representation space, where image and text can be well-aligned to serve as a condition for external diffusion models. Based on this discovery, we propose Dream Engine, an efficient and unified framework designed for arbitrary text-image interleaved control in image generation models. Building on powerful text-to-image models like SD3.5, we replace the original text-only encoders by incorporating versatile multimodal information encoders such as QwenVL. Our approach utilizes a two-stage training paradigm, consisting of joint text-image alignment and multimodal interleaved instruction tuning. Our experiments demonstrate that this training method is effective, achieving a 0.69 overall score on the GenEval benchmark, and matching the performance of state-of-the-art text-to-image models like SD3.5 and FLUX.', 'score': 9, 'issue_id': 2461, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'de63394c60041b93', 'authors': ['Liang Chen', 'Shuai Bai', 'Wenhao Chai', 'Weichu Xie', 'Haozhe Zhao', 'Leon Vinci', 'Junyang Lin', 'Baobao Chang'], 'affiliations': ['Alibaba Group', 'Bainance Labs', 'Beijing Institute of Technology', 'Peking University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2502.20172.jpg', 'data': {'categories': ['#benchmark', '#diffusion', '#multimodal', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Единая платформа для генерации изображений с комплексным мультимодальным контролем', 'desc': 'Статья представляет Dream Engine - новую систему для генерации изображений с комплексным текстово-визуальным контролем. Авторы интегрируют мультимодальные энкодеры, такие как QwenVL, в существующие модели диффузии для обработки как текстовых, так и визуальных входных данных. Предложенный двухэтапный метод обучения включает совместное выравнивание текста и изображений, а также мультимодальную настройку на основе инструкций. Эксперименты показывают, что Dream Engine достигает высоких результатов на бенчмарке GenEval, сопоставимых с современными моделями генерации изображений по тексту.'}, 'en': {'title': 'Dream Engine: Unifying Text and Image Generation for Enhanced Control', 'desc': 'This paper introduces Dream Engine, a new framework for generating images from text that allows for flexible control over the output. It combines advanced text encoders with Diffusion Transformer models to improve the alignment between text and images. The authors highlight the importance of using large multimodal models to create a shared representation space for better integration of multiple concepts. Their two-stage training method shows promising results, achieving competitive performance on established benchmarks.'}, 'zh': {'title': 'Dream Engine：实现任意文本与图像的交错控制', 'desc': '本文探讨了先进的文本到图像生成领域，提出了一种名为Dream Engine的统一框架。该框架结合了强大的文本编码器和扩散变换器，旨在实现任意文本与图像的交错控制。通过初步实验，我们发现大型多模态模型能够提供有效的共享表示空间，使图像和文本能够良好对齐。我们的训练方法经过验证，达到了GenEval基准的0.69分，表现与最先进的文本到图像模型相当。'}}}, {'id': 'https://huggingface.co/papers/2502.16944', 'title': 'Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance', 'url': 'https://huggingface.co/papers/2502.16944', 'abstract': 'Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40\\% and training time by 35\\% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.', 'score': 9, 'issue_id': 2459, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '5eba083b71966ca8', 'authors': ['Chenghua Huang', 'Lu Wang', 'Fangkai Yang', 'Pu Zhao', 'Zhixu Li', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'School of Computer Science, Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16944.jpg', 'data': {'categories': ['#alignment', '#rl', '#training', '#rlhf', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'DVPO: Эффективное обучение языковых моделей с подкреплением без взаимозависимости актора и критика', 'desc': 'Статья представляет новый метод обучения языковых моделей с подкреплением на основе обратной связи от человека, называемый Decoupled Value Policy Optimization (DVPO). DVPO заменяет традиционное моделирование вознаграждений предобученной глобальной моделью ценности (GVM), которая предсказывает оценки return-to-go на уровне токенов. Этот подход устраняет взаимозависимость актора и критика, что приводит к значительному снижению использования памяти GPU и времени обучения по сравнению с обычными методами RLHF. Эксперименты показывают, что DVPO превосходит эффективные методы RLHF и соответствует современным методам PPO по производительности.'}, 'en': {'title': 'Decoupling for Efficiency: DVPO Revolutionizes RLHF', 'desc': 'This paper introduces Decoupled Value Policy Optimization (DVPO), a new framework for improving Reinforcement Learning from Human Feedback (RLHF) in large language models. DVPO uses a pretrained global value model (GVM) to provide fixed supervisory signals, which helps to decouple the value model from policy training. This decoupling reduces the computational complexity and instability associated with traditional actor-critic methods, leading to significant reductions in GPU memory usage and training time. Experimental results demonstrate that DVPO not only outperforms existing efficient RLHF methods but also achieves performance comparable to state-of-the-art Proximal Policy Optimization (PPO).'}, 'zh': {'title': '解耦值策略优化：提升强化学习效率的新方法', 'desc': '本文提出了一种新的强化学习框架，称为解耦值策略优化（DVPO），旨在解决传统的基于人类反馈的强化学习方法中的一些问题。DVPO使用预训练的全局价值模型（GVM），而不是传统的奖励模型，从而减少了演员和评论家之间的相互依赖。通过这种方式，DVPO显著降低了GPU内存使用量和训练时间，同时在多个基准测试中表现优于现有的高效强化学习方法。该方法为大型语言模型的训练提供了更稳定和高效的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2502.20126', 'title': 'FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute', 'url': 'https://huggingface.co/papers/2502.20126', 'abstract': 'Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than 40\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to 75\\% less compute without compromising performance.', 'score': 9, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '405098fcd7a76209', 'authors': ['Sotiris Anagnostidis', 'Gregor Bachmann', 'Yeongmin Kim', 'Jonas Kohler', 'Markos Georgopoulos', 'Artsiom Sanakoyeu', 'Yuming Du', 'Albert Pumarola', 'Ali Thabet', 'Edgar Schönfeld'], 'affiliations': ['ETH Zurich', 'KAIST', 'Meta GenAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.20126.jpg', 'data': {'categories': ['#diffusion', '#cv', '#inference', '#video', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'FlexiDiT: Эффективная генерация без компромиссов', 'desc': 'Статья представляет новый подход к оптимизации работы моделей диффузионных трансформеров (DiT). Авторы предлагают динамическую стратегию распределения вычислительных ресурсов вместо статической, что позволяет значительно снизить требования к вычислительной мощности без потери качества генерации. Разработанный метод, названный FlexiDiT, применим к предобученным моделям DiT и позволяет сократить количество операций с плавающей запятой (FLOP) более чем на 40% при генерации изображений. Гибкость подхода демонстрируется его успешным применением к задачам генерации видео, где экономия вычислений достигает 75%.'}, 'en': {'title': 'Flexibility in Diffusion: Reducing Compute with FlexiDiT', 'desc': 'This paper addresses the high resource demands of Diffusion Transformers during image generation. The authors introduce a dynamic strategy that allows for flexible compute allocation during the denoising process, leading to the development of FlexiDiT models. These models can adapt to varying compute budgets while maintaining image quality, achieving over 40% reduction in FLOPs compared to traditional static models. Additionally, the approach is versatile and can be applied to video generation, achieving up to 75% less compute usage without sacrificing performance.'}, 'zh': {'title': '灵活计算，提升生成效率', 'desc': '现代扩散变换器在推理时需要大量资源，主要是因为每个去噪步骤都需要固定且大量的计算。本文提出了一种动态策略，取代传统的静态计算预算分配方法，使得预训练的扩散变换器模型（DiT）能够灵活处理不同的计算预算。我们的方法称为FlexiDiT，能够在不降低生成图像质量的情况下，减少超过40%的计算量。该方法通用且不依赖于输入和条件模式，甚至可以扩展到视频生成，显著降低计算需求。'}}}, {'id': 'https://huggingface.co/papers/2502.16750', 'title': 'Guardians of the Agentic System: Preventing Many Shots Jailbreak with Agentic System', 'url': 'https://huggingface.co/papers/2502.16750', 'abstract': 'The autonomous AI agents using large language models can create undeniable values in all span of the society but they face security threats from adversaries that warrants immediate protective solutions because trust and safety issues arise. Considering the many-shot jailbreaking and deceptive alignment as some of the main advanced attacks, that cannot be mitigated by the static guardrails used during the supervised training, points out a crucial research priority for real world robustness. The combination of static guardrails in dynamic multi-agent system fails to defend against those attacks. We intend to enhance security for LLM-based agents through the development of new evaluation frameworks which identify and counter threats for safe operational deployment. Our work uses three examination methods to detect rogue agents through a Reverse Turing Test and analyze deceptive alignment through multi-agent simulations and develops an anti-jailbreaking system by testing it with GEMINI 1.5 pro and llama-3.3-70B, deepseek r1 models using tool-mediated adversarial scenarios. The detection capabilities are strong such as 94\\% accuracy for GEMINI 1.5 pro yet the system suffers persistent vulnerabilities when under long attacks as prompt length increases attack success rates (ASR) and diversity metrics become ineffective in prediction while revealing multiple complex system faults. The findings demonstrate the necessity of adopting flexible security systems based on active monitoring that can be performed by the agents themselves together with adaptable interventions by system admin as the current models can create vulnerabilities that can lead to the unreliable and vulnerable system. So, in our work, we try to address such situations and propose a comprehensive framework to counteract the security issues.', 'score': 7, 'issue_id': 2469, 'pub_date': '2025-02-23', 'pub_date_card': {'ru': '23 февраля', 'en': 'February 23', 'zh': '2月23日'}, 'hash': 'e6f9253bc3e3a120', 'authors': ['Saikat Barua', 'Mostafizur Rahman', 'Md Jafor Sadek', 'Rafiul Islam', 'Shehnaz Khaled', 'Ahmedul Kabir'], 'affiliations': ['North South University, Dhaka', 'University of Dhaka, Dhaka'], 'pdf_title_img': 'assets/pdf/title_img/2502.16750.jpg', 'data': {'categories': ['#security', '#agents', '#inference', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'Укрепление безопасности ИИ-агентов: новые подходы к защите от современных угроз', 'desc': 'Статья посвящена проблемам безопасности автономных ИИ-агентов, использующих большие языковые модели (LLM). Авторы разрабатывают новые методы оценки для выявления и противодействия угрозам, включая обратный тест Тьюринга и анализ обманчивого выравнивания через мультиагентные симуляции. Они также создают систему против взлома, тестируя ее на моделях GEMINI 1.5 pro, llama-3.3-70B и deepseek r1. Результаты показывают необходимость гибких систем безопасности с активным мониторингом и адаптивным вмешательством администраторов.'}, 'en': {'title': 'Enhancing Security for Autonomous AI Agents Against Advanced Threats', 'desc': 'This paper discusses the security challenges faced by autonomous AI agents that use large language models (LLMs). It highlights the inadequacy of static guardrails against advanced attacks like many-shot jailbreaking and deceptive alignment, which threaten the trust and safety of these systems. The authors propose new evaluation frameworks to enhance the security of LLM-based agents, employing methods such as Reverse Turing Tests and multi-agent simulations to detect and counteract threats. Their findings indicate a need for flexible security systems that can adapt to ongoing attacks, ensuring reliable and safe operational deployment of AI agents.'}, 'zh': {'title': '增强自主AI代理的安全性', 'desc': '本论文探讨了基于大型语言模型的自主AI代理在社会各个领域的应用价值，但面临来自对手的安全威胁。研究指出，静态防护措施无法有效应对多次越狱和欺骗性对齐等高级攻击，因此需要新的评估框架来增强安全性。我们通过反图灵测试和多代理模拟分析欺骗性对齐，并开发反越狱系统，测试结果显示在长时间攻击下系统存在持续的脆弱性。研究结果强调了采用灵活的安全系统的重要性，以便代理能够主动监控并进行适应性干预。'}}}, {'id': 'https://huggingface.co/papers/2502.20307', 'title': 'Mobius: Text to Seamless Looping Video Generation via Latent Shift', 'url': 'https://huggingface.co/papers/2502.20307', 'abstract': "We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model's context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.", 'score': 7, 'issue_id': 2458, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '4763c15bca31d3b1', 'authors': ['Xiuli Bi', 'Jianfei Yuan', 'Bo Liu', 'Yong Zhang', 'Xiaodong Cun', 'Chi-Man Pun', 'Bin Xiao'], 'affiliations': ['Chongqing University of Post and Telecommunications, China', 'GVC Lab, Great Bay University, China', 'Meituan, China', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.20307.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#video', '#open_source'], 'emoji': '🔄', 'ru': {'title': 'Бесшовная генерация зацикленных видео из текста с помощью латентной диффузии', 'desc': 'Метод Mobius предлагает новый подход к генерации зацикленных видео на основе текстовых описаний без дополнительной разметки. Он использует предобученную модель латентной видео-диффузии, создавая латентный цикл путем соединения начального и конечного шума. Процесс многокадрового латентного шумоподавления поддерживает временную согласованность, постепенно смещая латентное представление первого кадра к концу на каждом шаге. Метод позволяет создавать более динамичные и качественные зацикленные видео по сравнению с традиционными синемаграфами, не требуя исходного изображения.'}, 'en': {'title': 'Seamless Video Creation from Text: Introducing Mobius!', 'desc': 'Mobius is a new technique that creates seamless looping videos directly from text descriptions without needing user input. It utilizes a pre-trained video latent diffusion model to generate these videos, ensuring that the start and end of the video connect smoothly. The method involves a latent cycle that allows for flexible video lengths while maintaining temporal consistency through multi-frame latent denoising. This approach enables the generation of dynamic and high-quality visuals, surpassing previous methods that relied on static images.'}, 'zh': {'title': 'Mobius：从文本生成无缝循环视频的新方法', 'desc': '本文介绍了一种名为Mobius的新方法，可以直接从文本描述生成无缝循环视频，而无需用户注释。这种方法利用预训练的视频潜在扩散模型，通过连接视频的起始和结束噪声构建潜在循环，从而生成循环视频。在推理过程中，我们通过逐步将第一帧的潜在表示转移到最后一帧，进行多帧潜在去噪，确保时间一致性。与以往的动态图片不同，该方法不需要图像作为外观，因此能够生成更动态的运动和更好的视觉质量。'}}}, {'id': 'https://huggingface.co/papers/2502.20127', 'title': 'SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning', 'url': 'https://huggingface.co/papers/2502.20127', 'abstract': 'Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.', 'score': 7, 'issue_id': 2456, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': 'ad848cf98c7468a7', 'authors': ['Zexiong Ma', 'Chao Peng', 'Pengfei Gao', 'Xiangxin Meng', 'Yanzhen Zou', 'Bing Xie'], 'affiliations': ['ByteDance', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20127.jpg', 'data': {'categories': ['#open_source', '#training', '#rlhf', '#rl', '#optimization'], 'emoji': '🔧', 'ru': {'title': 'SoRFT: Эффективное обучение ЯМ для автоматического исправления кода', 'desc': 'Авторы предлагают новый подход к обучению языковых моделей для решения проблем в программном коде - Subtask-oriented Reinforced Fine-Tuning (SoRFT). Метод разбивает задачу на подзадачи: локализацию файла, функции и строки кода, а также генерацию исправлений. SoRFT включает два этапа обучения: тонкую настройку с отбором данных и обучение с подкреплением на основе правил. Эксперименты показывают, что SoRFT значительно улучшает способность моделей решать проблемы в коде и обеспечивает экономичную альтернативу коммерческим моделям.'}, 'en': {'title': 'Enhancing Issue Resolution with SoRFT: A Cost-Effective Approach', 'desc': 'This paper introduces Subtask-oriented Reinforced Fine-Tuning (SoRFT), a new method designed to improve the issue-resolving capabilities of large language models (LLMs). It breaks down the issue resolution process into specific subtasks, such as file and function localization, and code editing. The training process involves two stages: first, a supervised fine-tuning phase that uses filtered data, and second, a reinforcement learning phase that applies Proximal Policy Optimization (PPO) with rewards based on ground-truth data. The results show that models trained with SoRFT outperform existing open-source models, achieving state-of-the-art results while being more cost-effective and maintaining better privacy.'}, 'zh': {'title': '提升问题解决能力的新方法', 'desc': '本论文提出了一种新的训练方法，称为子任务导向强化微调（SoRFT），旨在提高大型语言模型（LLMs）在问题解决方面的能力。我们将问题解决分解为结构化的子任务，包括文件定位、功能定位、行定位和代码编辑生成。SoRFT包含两个训练阶段：首先是基于拒绝采样的监督微调，其次是基于规则的强化学习，利用基于真实数据的奖励进行训练。实验结果表明，SoRFT显著提升了问题解决性能，改善了模型的泛化能力，并为商业模型提供了一种成本效益高的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2502.19459', 'title': 'Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting', 'url': 'https://huggingface.co/papers/2502.19459', 'abstract': 'Building articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement.', 'score': 6, 'issue_id': 2462, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '4220bf3b04cbef88', 'authors': ['Yu Liu', 'Baoxiong Jia', 'Ruijie Lu', 'Junfeng Ni', 'Song-Chun Zhu', 'Siyuan Huang'], 'affiliations': ['Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.19459.jpg', 'data': {'categories': ['#3d', '#cv', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'ArtGS: Гибкая реконструкция сочлененных объектов с помощью 3D гауссианов', 'desc': 'ArtGS - это новый подход к реконструкции и моделированию динамики сочлененных объектов в компьютерном зрении. Метод использует 3D гауссианы для гибкого и эффективного представления частей объекта. ArtGS включает канонические гауссианы с инициализацией от грубого к точному для выравнивания информации о частях в разных состояниях объекта. Подход также применяет модуль моделирования динамики частей, вдохновленный скиннингом, для улучшения реконструкции и обучения артикуляции. Эксперименты показывают, что ArtGS достигает современного уровня производительности в оценке параметров соединений и реконструкции сетки частей, особенно для сложных многокомпонентных объектов.'}, 'en': {'title': 'ArtGS: Revolutionizing Articulated Object Reconstruction with 3D Gaussians', 'desc': 'This paper presents ArtGS, a new method for building articulated objects in computer vision. It uses 3D Gaussians to effectively represent and align information across different states of multi-part objects, enhancing part-mesh reconstruction and dynamics modeling. The approach includes a skinning-inspired module that improves the learning of object articulation. Experiments show that ArtGS outperforms existing methods, providing better quality and efficiency in reconstructing complex articulated structures.'}, 'zh': {'title': 'ArtGS：提升关节物体重建的新方法', 'desc': '本文提出了一种名为ArtGS的新方法，旨在解决计算机视觉中构建关节物体的挑战。该方法利用3D高斯作为灵活高效的表示，能够更好地整合不同物体状态的信息。ArtGS通过粗到细的初始化和更新，优化关节部分信息的对齐，并采用类似蒙皮的部分动态建模模块，提升了部分网格重建和关节学习的效果。实验结果表明，ArtGS在多部分关节物体的参数估计和网格重建方面达到了最先进的性能，显著提高了重建质量和效率。'}}}, {'id': 'https://huggingface.co/papers/2502.19735', 'title': 'R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning', 'url': 'https://huggingface.co/papers/2502.19735', 'abstract': 'Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to catastrophic forgetting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation beyond MT sub-tasks to six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom resolution); (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery and anti-forgetting adaptation through RL with KL-constrained rewards. Experimental results indicate a steady translation performance improvement in 21 languages and 80 translation directions on Flores-101 test set, especially on the 15 languages unseen from training, with its general multilingual abilities preserved compared with plain SFT.', 'score': 6, 'issue_id': 2457, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '45b53278bddf4ab9', 'authors': ['Minggui He', 'Yilun Liu', 'Shimin Tao', 'Yuanchang Luo', 'Hongyong Zeng', 'Chang Su', 'Li Zhang', 'Hongxia Ma', 'Daimeng Wei', 'Weibin Meng', 'Hao Yang', 'Boxing Chen', 'Osamu Yoshie'], 'affiliations': ['Huawei Canada, Canada', 'Huawei, China', 'Waseda University, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2502.19735.jpg', 'data': {'categories': ['#reasoning', '#training', '#machine_translation', '#rl', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Разумный перевод: как научить ИИ думать как переводчик', 'desc': 'Эта статья представляет R1-Translator (R1-T1) - новый фреймворк для машинного перевода с использованием рассуждений во время вывода. Авторы предлагают шесть шаблонов цепочек рассуждений, основанных на стратегиях профессиональных переводчиков. Метод использует обучение с подкреплением для адаптации и улучшения перевода без катастрофического забывания. Эксперименты показывают улучшение качества перевода на 21 языке, особенно на 15 языках, не встречавшихся при обучении.'}, 'en': {'title': 'Revolutionizing Machine Translation with Reasoning-Enhanced Frameworks', 'desc': 'This paper presents R1-Translator (R1-T1), a new framework that enhances machine translation (MT) by incorporating reasoning during translation, similar to how human translators think. It addresses limitations of existing methods that either use fixed reasoning patterns or rely on supervised fine-tuning, which can lead to forgetting important information. The framework utilizes reinforcement learning (RL) to align translation with human reasoning through six expert-curated chain-of-thought (CoT) templates. Experimental results show that R1-T1 improves translation quality across multiple languages and tasks, particularly for languages not seen during training, while maintaining strong multilingual capabilities.'}, 'zh': {'title': '推理驱动的通用机器翻译新框架', 'desc': '本文探讨了在机器翻译中引入推理增强的大型语言模型（LLMs），尤其是如何在推理时进行有效的翻译。我们提出了一种新的框架R1-Translator（R1-T1），通过强化学习（RL）结合人类对推理链的理解，来实现通用的机器翻译。该方法创新性地扩展了推理翻译的应用范围，并制定了六种专家策划的推理模板，以适应不同的翻译任务。实验结果表明，该方法在21种语言和80个翻译方向上均表现出稳定的翻译性能提升，尤其是在训练中未见过的15种语言上。'}}}, {'id': 'https://huggingface.co/papers/2502.20388', 'title': 'Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation', 'url': 'https://huggingface.co/papers/2502.20388', 'abstract': "Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a ktimes k grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as continuous entity regression, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20times faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2times faster than the previous best-performing model without relying on vision foundation modules (\\eg, DINOv2) or advanced guidance interval sampling.", 'score': 5, 'issue_id': 2471, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '4f9c4a95a0bb1365', 'authors': ['Sucheng Ren', 'Qihang Yu', 'Ju He', 'Xiaohui Shen', 'Alan Yuille', 'Liang-Chieh Chen'], 'affiliations': ['ByteDance', 'Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2502.20388.jpg', 'data': {'categories': ['#training', '#benchmark', '#optimization', '#cv', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'xAR: Гибкая авторегрессия для улучшенной генерации изображений', 'desc': 'В статье представлен xAR - обобщенная авторегрессионная модель для генерации изображений. xAR расширяет понятие токена до сущности X, которая может представлять различные структуры изображения. Модель использует регрессию сущностей вместо классификации дискретных токенов, применяя методы сопоставления потоков. Этот подход позволяет избежать смещения экспозиции за счет обучения на зашумленных сущностях. xAR показывает высокую производительность на бенчмарке ImageNet-256, превосходя более крупные модели при более быстром выводе.'}, 'en': {'title': 'xAR: Redefining Tokens for Enhanced Generative Modeling', 'desc': "This paper introduces xAR, a novel autoregressive modeling framework that redefines the concept of a 'token' in generative tasks. Instead of using traditional discrete symbols, xAR allows for flexible prediction units, such as patches, groups of patches, or even entire images, enhancing the model's ability to capture complex spatial structures. The authors address the issue of exposure bias in AR models by employing Noisy Context Learning, which trains the model on noisy entities rather than relying solely on ground truth tokens. As a result, xAR not only improves performance on benchmarks like ImageNet-256 but also achieves faster inference times compared to existing models."}, 'zh': {'title': 'xAR：灵活的自回归模型，减轻曝光偏差', 'desc': '本文提出了一种名为xAR的自回归模型框架，旨在改进传统的令牌定义，以适应二维图像结构的复杂性。xAR将令牌扩展为实体X，可以表示单个补丁、邻近补丁的组合、远程补丁的非局部分组、不同分辨率的尺度，甚至整个图像。通过将离散令牌分类重新表述为连续实体回归，xAR利用流匹配方法在每个自回归步骤中进行训练，从而实现了噪声上下文学习，有效减轻了曝光偏差。实验结果表明，xAR在生成图像方面表现优异，且推理速度显著提升。'}}}, {'id': 'https://huggingface.co/papers/2502.17355', 'title': 'On Relation-Specific Neurons in Large Language Models', 'url': 'https://huggingface.co/papers/2502.17355', 'abstract': "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation r on the LLM's ability to handle (1) facts whose relation is r and (2) facts whose relation is a different relation r' neq r. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. (i) Neuron cumulativity. The neurons for r present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in r. (ii) Neuron versatility. Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. (iii) Neuron interference. Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons.", 'score': 4, 'issue_id': 2469, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 февраля', 'en': 'February 24', 'zh': '2月24日'}, 'hash': '5149cebb219a4499', 'authors': ['Yihong Liu', 'Runsheng Chen', 'Lea Hirlimann', 'Ahmad Dawar Hakimi', 'Mingyang Wang', 'Amir Hossein Kargaran', 'Sascha Rothe', 'François Yvon', 'Hinrich Schütze'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'Center for Information and Language Processing, LMU Munich', 'Google DeepMind, Zürich, Switzerland', 'Munich Center for Machine Learning (MCML)', 'Sorbonne Université, CNRS, ISIR, France', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2502.17355.jpg', 'data': {'categories': ['#transfer_learning', '#architecture', '#open_source', '#data', '#interpretability', '#multilingual'], 'emoji': '🧠', 'ru': {'title': 'Нейроны отношений: скрытые механизмы больших языковых моделей', 'desc': 'Исследование показывает, что в больших языковых моделях (LLM) существуют нейроны, специфичные для определенных отношений. Эти нейроны способны обнаруживать отношения в входном тексте и направлять генерацию, связанную с этими отношениями. Эксперименты на семействе моделей Llama-2 выявили кумулятивный эффект, универсальность и интерференцию этих нейронов. Результаты демонстрируют, что деактивация специфичных нейронов влияет на способность LLM обрабатывать факты, связанные с конкретными отношениями.'}, 'en': {'title': 'Unlocking Relation-Specific Neurons in Language Models', 'desc': "This paper explores how certain neurons in large language models (LLMs) can specifically encode knowledge about relations, independent of the entities involved. The authors hypothesize that these relation-specific neurons help the model recognize and generate text based on particular relationships. Through experiments with the Llama-2 model, they demonstrate that deactivating these neurons affects the model's performance on tasks related to those specific relations. The study reveals three key properties of these neurons: they show cumulative effects, can be versatile across different relations, and their interference can enhance performance on unrelated tasks."}, 'zh': {'title': '揭示特定关系神经元的秘密', 'desc': '在大型语言模型（LLMs）中，某些神经元可以存储在预训练过程中学到的特定知识。我们假设有些神经元专注于输入文本中的关系，而不依赖于任何实体。通过对Llama-2系列模型的研究，我们发现了特定于关系的神经元，并测量了选择性去激活这些神经元对模型处理不同关系事实的影响。我们的实验表明，这些特定于关系的神经元具有累积性、多功能性和干扰性等特征。'}}}, {'id': 'https://huggingface.co/papers/2502.18197', 'title': 'Training Consistency Models with Variational Noise Coupling', 'url': 'https://huggingface.co/papers/2502.18197', 'abstract': 'Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at 64 times 64 resolution in 2-step generation. Our code is available at https://github.com/sony/vct .', 'score': 3, 'issue_id': 2469, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 февраля', 'en': 'February 25', 'zh': '2月25日'}, 'hash': '6201a478f2f81d4c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#architecture', '#training', '#open_source', '#cv', '#optimization', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение генерации изображений через обучаемое связывание шума в моделях согласованности', 'desc': 'Статья представляет новый подход к тренировке моделей согласованности (Consistency Training) на основе метода Flow Matching. Авторы предлагают обучаемую схему связывания шума, вдохновленную архитектурой вариационных автоэнкодеров. Метод косвенно изучает геометрию отображения шума в данные с помощью энкодера, что улучшает процесс генерации изображений. Эмпирические результаты показывают значительные улучшения в генеративных задачах, превосходя базовые модели и достигая state-of-the-art показателей FID на нескольких наборах данных.'}, 'en': {'title': 'Revolutionizing Image Generation with Flow Matching in Consistency Training', 'desc': 'This paper introduces a new approach to Consistency Training (CT) for image generation, leveraging the Flow Matching framework. The authors address the issues of high variance and instability in non-distillation CT by proposing a noise-coupling scheme inspired by Variational Autoencoders (VAE). Their method involves training a data-dependent noise emission model, which helps to learn the noise-to-data mapping more effectively. The results demonstrate that their approach significantly improves generative performance, achieving state-of-the-art results on CIFAR-10 and competitive results on ImageNet.'}, 'zh': {'title': '一致性训练的新突破：流匹配框架下的噪声耦合方案', 'desc': '一致性训练（CT）作为一种新兴的图像生成方法，展现出与扩散模型相当的性能。然而，非蒸馏一致性训练常常面临高方差和不稳定性的问题，因此分析和改进其训练动态成为研究的热点。本文提出了一种基于流匹配框架的新型CT训练方法，主要贡献在于借鉴变分自编码器（VAE）架构的噪声耦合方案。通过训练一个依赖于数据的噪声发射模型，我们的方法能够间接学习噪声到数据映射的几何特性，从而在多个图像数据集上实现显著的生成改进。'}}}, {'id': 'https://huggingface.co/papers/2502.16111', 'title': 'PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving', 'url': 'https://huggingface.co/papers/2502.16111', 'abstract': 'Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN (sim8%uparrow), OlympiadBench (sim4%uparrow), DocFinQA (sim7%uparrow), and GPQA (sim1%uparrow). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems.', 'score': 1, 'issue_id': 2474, 'pub_date': '2025-02-22', 'pub_date_card': {'ru': '22 февраля', 'en': 'February 22', 'zh': '2月22日'}, 'hash': '99f42232c52b1f77', 'authors': ['Mihir Parmar', 'Xin Liu', 'Palash Goyal', 'Yanfei Chen', 'Long Le', 'Swaroop Mishra', 'Hossein Mobahi', 'Jindong Gu', 'Zifeng Wang', 'Hootan Nakhost', 'Chitta Baral', 'Chen-Yu Lee', 'Tomas Pfister', 'Hamid Palangi'], 'affiliations': ['Arizona State University', 'Google'], 'pdf_title_img': 'assets/pdf/title_img/2502.16111.jpg', 'data': {'categories': ['#inference', '#benchmark', '#reasoning', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'PlanGEN: Адаптивное планирование с учетом ограничений для сложных задач ИИ', 'desc': 'PlanGEN - это новая модель-агностическая структура для решения сложных задач планирования. Она включает в себя три ключевых компонента: агенты ограничений, верификации и выбора. PlanGEN использует итеративную верификацию с учетом ограничений для улучшения алгоритмов вывода, таких как Best of N, Tree-of-Thought и REBASE. Экспериментальные результаты показывают значительные улучшения по сравнению с базовыми методами на нескольких эталонных тестах, включая NATURAL PLAN, OlympiadBench, DocFinQA и GPQA.'}, 'en': {'title': 'PlanGEN: Enhancing Planning with Adaptive Verification and Selection', 'desc': 'The paper introduces PlanGEN, a new agent framework designed to tackle complex planning problems in machine learning. It combines three main components: constraint agents, verification agents, and selection agents, which work together to enhance the performance of inference-time algorithms. By using constraint-guided iterative verification, PlanGEN improves the effectiveness of algorithms like Best of N and Tree-of-Thought. The framework also adapts algorithm selection based on the complexity of the task, leading to significant performance gains across various benchmarks.'}, 'zh': {'title': 'PlanGEN：智能体框架的创新解决方案', 'desc': '本文提出了一种名为PlanGEN的模型无关且易于扩展的智能体框架，旨在解决复杂规划问题中的局限性。该框架包含三个关键组件：约束代理、验证代理和选择代理，能够有效地进行约束引导的迭代验证。通过优化算法选择，PlanGEN能够根据实例复杂性进行自适应调整，从而提高推理时间算法的性能。实验结果表明，PlanGEN在多个基准测试中显著超越了最强基线，达到了最新的研究水平。'}}}, {'id': 'https://huggingface.co/papers/2502.20378', 'title': 'Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling', 'url': 'https://huggingface.co/papers/2502.20378', 'abstract': 'Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.', 'score': 1, 'issue_id': 2469, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '0b7155b54c5f0829', 'authors': ['Hanyang Kong', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2502.20378.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Эффективный рендеринг динамических сцен с помощью разреженных гауссианов', 'desc': 'Статья представляет новый метод под названием Efficient Dynamic Gaussian Splatting (EDGS) для рендеринга динамических сцен из монокулярных видео. EDGS использует разреженное представление на основе якорной сетки для моделирования динамических сцен, что позволяет значительно сократить количество гауссианов. Метод применяет неконтролируемую стратегию для фильтрации статических областей и использует MLP только для деформируемых объектов. Эксперименты показывают, что EDGS значительно улучшает скорость рендеринга при сохранении высокого качества по сравнению с существующими методами.'}, 'en': {'title': 'Speeding Up Dynamic Scene Rendering with EDGS', 'desc': 'This paper presents Efficient Dynamic Gaussian Splatting (EDGS), a method for rendering dynamic scenes from monocular videos more efficiently. The authors address the issue of redundant Gaussians in existing methods, which slow down rendering speeds and cause jittering in static areas. EDGS utilizes a sparse anchor-grid representation to model dynamic scenes, focusing on time-variant attributes only for deformable objects. Experiments show that EDGS enhances rendering speed and quality compared to previous techniques, making it a significant advancement in the field.'}, 'zh': {'title': '高效动态高斯点云：提升渲染速度与质量的创新方法', 'desc': '本文提出了一种高效的动态高斯点云表示方法（EDGS），旨在解决从单目视频渲染动态场景的挑战。传统的高斯点云方法往往会导致冗余的高斯分布，影响渲染速度。EDGS通过稀疏的时间变化属性建模，减少了静态区域的高斯数量，从而提高了渲染效率。实验结果表明，EDGS在渲染速度和质量上均优于现有的最先进方法。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (1)', '#architecture (7)', '#audio', '#benchmark (10)', '#cv (7)', '#data (2)', '#dataset (2)', '#diffusion (4)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation (1)', '#math (1)', '#multilingual (2)', '#multimodal (4)', '#open_source (6)', '#optimization (11)', '#plp', '#rag', '#reasoning (5)', '#rl (5)', '#rlhf (2)', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (12)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-01 01:50',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-01 01:50')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-01 01:50')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    