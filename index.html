
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. October 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 октября</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-22.html">⬅️ <span id="prev-date">22.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-24.html">➡️ <span id="next-date">24.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'};
        let feedDateNext = {'ru': '24.10', 'en': '10/24', 'zh': '10月24日'};
        let feedDatePrev = {'ru': '22.10', 'en': '10/22', 'zh': '10月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.19363', 'title': 'LoongRL:Reinforcement Learning for Advanced Reasoning over Long Contexts', 'url': 'https://huggingface.co/papers/2510.19363', 'abstract': 'LoongRL, a data-driven reinforcement learning method, enhances long-context reasoning by transforming short multi-hop QA into high-difficulty tasks, improving accuracy and generalization in large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning over long contexts is essential for large language models. While reinforcement learning (RL) enhances short-context reasoning by inducing "Aha" moments in chain-of-thought, the advanced thinking patterns required for long-context reasoning remain largely unexplored, and high-difficulty RL data are scarce. In this paper, we introduce LoongRL, a data-driven RL method for advanced long-context reasoning. Central to LoongRL is KeyChain, a synthesis approach that transforms short multi-hop QA into high-difficulty long-context tasks by inserting UUID chains that hide the true question among large collections of distracting documents. Solving these tasks requires the model to trace the correct chain step-by-step, identify the true question, retrieve relevant facts and reason over them to answer correctly. RL training on KeyChain data induces an emergent plan-retrieve-reason-recheck reasoning pattern that generalizes far beyond training length. Models trained at 16K effectively solve 128K tasks without prohibitive full-length RL rollout costs. On Qwen2.5-7B and 14B, LoongRL substantially improves long-context multi-hop QA accuracy by +23.5% and +21.1% absolute gains. The resulting LoongRL-14B reaches a score of 74.2, rivaling much larger frontier models such as o3-mini (74.5) and DeepSeek-R1 (74.9). It also improves long-context retrieval, passes all 128K needle-in-a-haystack stress tests, and preserves short-context reasoning capabilities.', 'score': 12, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'c9f40dde34631067', 'authors': ['Siyuan Wang', 'Gaokai Zhang', 'Li Lyna Zhang', 'Ning Shang', 'Fan Yang', 'Dongyao Chen', 'Mao Yang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19363.jpg', 'data': {'categories': ['#training', '#rl', '#long_context', '#reasoning'], 'emoji': '🔗', 'ru': {'title': 'Обучение LLM длинному рассуждению через цепочки UUID', 'desc': 'LoongRL — это метод reinforcement learning для улучшения рассуждений над длинными контекстами в больших языковых моделях. Ключевая идея — превращать короткие multi-hop вопросы в сложные задачи длиной 128K токенов, вставляя цепочки UUID среди отвлекающих документов. Обучение на таких данных формирует у модели паттерн «планируй-ищи-рассуждай-перепроверяй», который обобщается далеко за пределы длины обучающих примеров. На моделях Qwen2.5 метод даёт прирост точности +23.5% и позволяет 14B модели достичь результатов, сравнимых с гораздо большими frontier моделями вроде o3-mini и DeepSeek-R1.'}, 'en': {'title': 'Unlocking Long-Context Reasoning with LoongRL', 'desc': 'LoongRL is a novel reinforcement learning method designed to enhance long-context reasoning in large language models. It transforms short multi-hop question-answering tasks into more complex challenges by using UUID chains to obscure the actual question among distracting information. This approach encourages models to develop a systematic reasoning pattern that involves planning, retrieving, and verifying information, which significantly improves their performance on long-context tasks. The results show that models trained with LoongRL achieve substantial accuracy gains and can handle a much larger set of tasks than previously possible, while still maintaining their short-context reasoning abilities.'}, 'zh': {'title': 'LoongRL：提升长上下文推理的强化学习新方法', 'desc': 'LoongRL是一种数据驱动的强化学习方法，旨在增强长上下文推理能力。它通过将短多跳问答转化为高难度任务，提升了大型语言模型的准确性和泛化能力。LoongRL的核心是KeyChain方法，它通过插入UUID链来隐藏真实问题，从而在大量干扰文档中生成长上下文任务。经过KeyChain数据的强化学习训练，模型能够形成有效的推理模式，显著提高了多跳问答的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.19336', 'title': 'DaMo: Data Mixing Optimizer in Fine-tuning Multimodal LLMs for Mobile\n  Phone Agents', 'url': 'https://huggingface.co/papers/2510.19336', 'abstract': "DaMo, a trainable network optimizing data mixtures for Multimodal Large Language Models, enhances performance across various mobile phone tasks and benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Mobile Phone Agents (MPAs) have emerged as a promising research direction due to their broad applicability across diverse scenarios. While Multimodal Large Language Models (MLLMs) serve as the foundation for MPAs, their effectiveness in handling multiple mobile phone tasks simultaneously remains limited. Although multitask supervised fine-tuning (SFT) is widely adopted for multitask learning, existing approaches struggle to determine optimal training data compositions for peak performance. To address this challenge, we propose DaMo (Data Mixture Optimizer) - a novel solution employing a trainable network that predicts optimal data mixtures by forecasting downstream task performance for any given dataset ratio. To support comprehensive evaluation, we introduce PhoneAgentBench, the first specialized benchmark to evaluate MLLMs on multimodal mobile phone tasks, comprising 1235 QA pairs spanning diverse real-world industrial mobile application scenarios. Demonstrating strong predictive capability (R^2=0.81) in small-scale pilot experiments, DaMo efficiently extrapolates optimal data mixing configurations. Our results show DaMo achieves a 3.38% performance improvement on PhoneAgentBench compared to alternative methods. Furthermore, extensive experiments across established benchmarks including BFCL-v3, MME-Reasoning, MME-Perception, and OCRBench reveal DaMo's superior generalization, outperforming other approaches by 2.57% in terms of average score. When used solely for MLLM optimization on the BFCL-v3 task, DaMo improves the metrics by 12.47% than other methods. Notably, DaMo maintains robust scalability, preserving its effectiveness when applied to other model architectures. The code and dataset are available at https://github.com/OPPO-Mente-Lab/DaMo.git", 'score': 4, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4d330d1d597c2031', 'authors': ['Kai Shi', 'Jun Yang', 'Ni Yang', 'Binqiang Pan', 'Qingsong Xie', 'Chao Zhang', 'Zhenyu Yang', 'Tianhuang Su', 'Haonan Lu'], 'affiliations': ['OPPO AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2510.19336.jpg', 'data': {'categories': ['#survey', '#training', '#architecture', '#dataset', '#data', '#benchmark', '#multimodal', '#optimization'], 'emoji': '📱', 'ru': {'title': 'DaMo: умная оптимизация данных для AI-агентов на смартфонах', 'desc': 'DaMo - это обучаемая нейросеть, которая оптимизирует состав обучающих данных для мультимодальных LLM, работающих в качестве агентов на мобильных телефонах. Метод предсказывает оптимальные пропорции различных датасетов для multitask обучения, достигая коэффициента детерминации R²=0.81 в экспериментах. Авторы представили PhoneAgentBench - первый специализированный бенчмарк для оценки мультимодальных задач на мобильных устройствах с 1235 вопросами из реальных приложений. DaMo показывает улучшение на 3.38% на PhoneAgentBench и на 2.57% в среднем на других бенчмарках по сравнению с альтернативными методами композиции данных.'}, 'en': {'title': 'Optimizing Data Mixtures for Enhanced Mobile Task Performance', 'desc': 'DaMo is a novel trainable network designed to optimize data mixtures for Multimodal Large Language Models (MLLMs), specifically enhancing their performance on mobile phone tasks. It addresses the challenge of determining the best training data compositions for multitask learning, which traditional methods struggle with. By predicting optimal data mixtures based on expected task performance, DaMo demonstrates significant improvements in various benchmarks, including a 3.38% increase on PhoneAgentBench. Additionally, it shows strong generalization capabilities across established benchmarks, outperforming existing methods and maintaining effectiveness across different model architectures.'}, 'zh': {'title': 'DaMo：优化多模态任务的数据组合', 'desc': 'DaMo是一种可训练的网络，旨在优化多模态大语言模型（MLLMs）中的数据组合，从而提升在手机任务和基准测试中的表现。该方法通过预测下游任务性能，确定最佳的数据混合比例，解决了多任务学习中数据组合选择的难题。我们还推出了PhoneAgentBench，这是第一个专门评估多模态手机任务的基准，包含1235个问答对，覆盖多种真实工业应用场景。实验结果表明，DaMo在多个基准测试中表现优越，尤其在BFCL-v3任务上提升了12.47%的指标。'}}}, {'id': 'https://huggingface.co/papers/2510.19817', 'title': 'olmOCR 2: Unit Test Rewards for Document OCR', 'url': 'https://huggingface.co/papers/2510.19817', 'abstract': 'olmOCR 2, a vision language model trained with reinforcement learning and verifiable rewards, achieves state-of-the-art performance in OCR tasks, particularly in math formula conversion, table parsing, and multi-column layouts.  \t\t\t\t\tAI-generated summary \t\t\t\t We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.', 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'f414d75457c4f9ee', 'authors': ['Jake Poznanski', 'Luca Soldaini', 'Kyle Lo'], 'affiliations': ['Allen Institute for AI'], 'pdf_title_img': 'assets/pdf/title_img/2510.19817.jpg', 'data': {'categories': ['#rl', '#dataset', '#benchmark', '#open_source', '#synthetic', '#optimization', '#cv'], 'emoji': '📄', 'ru': {'title': 'OCR нового поколения через обучение с подкреплением на unit-тестах', 'desc': "Представлена модель olmOCR 2 на базе vision language model с 7 миллиардами параметров для преобразования PDF-документов в текст. Модель обучена с помощью reinforcement learning с проверяемыми наградами (RLVR), где наградами служат бинарные unit-тесты на синтетических документах с известной разметкой. Система показывает state-of-the-art результаты на бенчмарке olmOCR-Bench, особенно в распознавании математических формул, парсинге таблиц и многоколоночных layout'ов. Модель, данные и код выпущены под открытыми лицензиями."}, 'en': {'title': 'Revolutionizing OCR with Reinforcement Learning!', 'desc': 'olmOCR 2 is an advanced optical character recognition (OCR) system that excels in converting printed documents into structured text. It utilizes a vision language model (VLM) with 7 billion parameters, trained through reinforcement learning with verifiable rewards to ensure high accuracy. The model is particularly effective in handling complex tasks such as math formula conversion, table parsing, and multi-column layouts. By generating synthetic documents for training, olmOCR 2 achieves state-of-the-art results on the olmOCR-Bench benchmark, demonstrating significant improvements over its predecessors.'}, 'zh': {'title': 'olmOCR 2：OCR任务的最优解', 'desc': 'olmOCR 2 是一种新型的视觉语言模型，专门用于光学字符识别（OCR）任务，特别是在数学公式转换、表格解析和多列布局方面表现出色。该模型使用强化学习和可验证奖励进行训练，确保了其在处理复杂文档时的高效性。我们开发了一种生成合成文档的管道，以创建多样化和具有挑战性的布局，并通过二元单元测试来评估模型性能。最终，olmOCR 2 在我们的英语OCR基准测试中达到了最先进的性能，显著提升了数学公式和表格的处理能力。'}}}, {'id': 'https://huggingface.co/papers/2510.19457', 'title': 'MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for\n  Large Multimodal Models', 'url': 'https://huggingface.co/papers/2510.19457', 'abstract': "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios.", 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '016313b1f4c1861b', 'authors': ['Kailin Jiang', 'Ning Jiang', 'Yuchen Ren', 'Yuchen Li', 'Yifan Gao', 'Jinhe Bi', 'Yunpu Ma', 'Qingqing Liu', 'Xianhao Wang', 'Yifan Jia', 'Hongbo Jiang', 'Yaocong Hu', 'Bin Li', 'Lei Liu', 'Yuntao Du'], 'affiliations': ['Anhui Polytechnic University', 'Beijing Institute of Technology', 'Ludwig Maximilian University of Munich', 'Northeast Forestry University', 'Shandong University', 'University of Science and Technology of China', 'University of Sydney', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19457.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#reasoning'], 'emoji': '⏰', 'ru': {'title': 'MINED: учим мультимодальные модели понимать время', 'desc': 'Статья представляет MINED — новый бенчмарк для оценки способности больших мультимодальных моделей (LMM) понимать знания, чувствительные ко времени. Бенчмарк включает 2104 образца по 6 типам знаний и оценивает модели по 11 задачам в 6 измерениях: когнитивность, осведомлённость, надёжность, понимание, рассуждение и робастность. Тестирование 15 популярных LMM показало, что Gemini-2.5-Pro достигает лучшего результата (63.07%), в то время как большинство open-source моделей испытывают трудности с временны́м пониманием. Авторы также исследуют методы knowledge editing для обновления временны́х знаний в LMM и демонстрируют их эффективность в сценариях единичного редактирования.'}, 'en': {'title': 'Enhancing Temporal Awareness in Large Multimodal Models with MINED', 'desc': "This paper discusses the limitations of Large Multimodal Models (LMMs) in understanding time-sensitive factual knowledge due to their static representations. The authors introduce MINED, a new benchmark designed to assess LMMs' temporal awareness across six dimensions and eleven tasks, including cognition and reasoning. The benchmark is built from Wikipedia and includes 2,104 samples of time-sensitive knowledge. The evaluation reveals that while some LMMs, like Gemini-2.5-Pro, perform well, many open-source models struggle with temporal understanding, particularly in areas like sports."}, 'zh': {'title': '提升时间敏感知识理解的基准评估', 'desc': '大型多模态模型（LMMs）通过跨模态预训练编码丰富的事实知识，但它们的静态表示在理解时间敏感的事实知识方面存在困难。现有的基准测试由于设计静态，无法充分评估LMMs对时间敏感知识的理解能力。为了解决这个问题，我们提出了MINED，这是一个全面的基准，评估时间意识的六个关键维度和十一项具有挑战性的任务。通过对15个广泛使用的LMMs进行评估，发现Gemini-2.5-Pro在时间敏感知识的理解上表现最佳，而大多数开源LMMs仍然缺乏这种能力。'}}}, {'id': 'https://huggingface.co/papers/2510.19386', 'title': 'ColorAgent: Building A Robust, Personalized, and Interactive OS Agent', 'url': 'https://huggingface.co/papers/2510.19386', 'abstract': "ColorAgent, an OS agent using step-wise reinforcement learning and a multi-agent framework, achieves high success rates in long-horizon interactions and personalized user engagement on Android benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the advancements in hardware, software, and large language model technologies, the interaction between humans and operating systems has evolved from the command-line interface to the rapidly emerging AI agent interactions. Building an operating system (OS) agent capable of executing user instructions and faithfully following user desires is becoming a reality. In this technical report, we present ColorAgent, an OS agent designed to engage in long-horizon, robust interactions with the environment while also enabling personalized and proactive user interaction. To enable long-horizon interactions with the environment, we enhance the model's capabilities through step-wise reinforcement learning and self-evolving training, while also developing a tailored multi-agent framework that ensures generality, consistency, and robustness. In terms of user interaction, we explore personalized user intent recognition and proactive engagement, positioning the OS agent not merely as an automation tool but as a warm, collaborative partner. We evaluate ColorAgent on the AndroidWorld and AndroidLab benchmarks, achieving success rates of 77.2% and 50.7%, respectively, establishing a new state of the art. Nonetheless, we note that current benchmarks are insufficient for a comprehensive evaluation of OS agents and propose further exploring directions in future work, particularly in the areas of evaluation paradigms, agent collaboration, and security. Our code is available at https://github.com/MadeAgents/mobile-use.", 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '671d46e29b93bd8f', 'authors': ['Ning Li', 'Qiqiang Lin', 'Zheng Wu', 'Xiaoyun Mo', 'Weiming Zhang', 'Yin Zhao', 'Xiangmou Qu', 'Jiamu Zhou', 'Jun Wang', 'Congmin Zheng', 'Yuanyi Song', 'Hongjiang Chen', 'Heyuan Huang', 'Jihong Wang', 'Jiaxin Yin', 'Jingwei Yu', 'Junwei Liao', 'Qiuying Peng', 'Xingyu Lou', 'Jun Wang', 'Weiwen Liu', 'Zhuosheng Zhang', 'Weinan Zhang'], 'affiliations': ['OPPO Research Institute', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19386.jpg', 'data': {'categories': ['#rl', '#games', '#benchmark', '#security', '#agents', '#open_source', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'ColorAgent: умный OS-агент с обучением через взаимодействие', 'desc': 'ColorAgent — это AI-агент для операционной системы Android, способный выполнять сложные многошаговые задачи и персонализированно взаимодействовать с пользователем. Система использует пошаговое reinforcement learning и самообучение для долгосрочных взаимодействий со средой, а также multi-agent фреймворк для обеспечения надёжности и согласованности. На бенчмарках AndroidWorld и AndroidLab агент достиг результатов 77.2% и 50.7% соответственно, установив новый state-of-the-art. Авторы позиционируют ColorAgent не просто как инструмент автоматизации, а как персонального помощника, способного распознавать намерения пользователя и проактивно взаимодействовать с ним.'}, 'en': {'title': 'ColorAgent: Your Personalized OS Partner for Intelligent Interaction', 'desc': 'ColorAgent is an operating system agent that utilizes step-wise reinforcement learning within a multi-agent framework to enhance user interactions over long periods. It focuses on personalized user engagement, allowing the agent to understand and anticipate user needs, making it more than just an automation tool. The agent has been tested on Android benchmarks, achieving impressive success rates of 77.2% and 50.7%, setting a new standard in the field. The paper also highlights the need for improved evaluation methods for OS agents and suggests future research directions in collaboration and security.'}, 'zh': {'title': 'ColorAgent：智能操作系统的个性化交互新纪元', 'desc': 'ColorAgent是一种操作系统代理，采用逐步强化学习和多智能体框架，能够在长时间交互中实现高成功率，并提供个性化的用户参与体验。该代理通过增强模型能力，支持与环境的长时间交互，并进行自我进化训练。ColorAgent不仅是一个自动化工具，更是一个温暖的合作伙伴，能够识别用户意图并主动参与。我们在AndroidWorld和AndroidLab基准测试中评估了ColorAgent，分别取得了77.2%和50.7%的成功率，创造了新的技术标准。'}}}, {'id': 'https://huggingface.co/papers/2510.19338', 'title': 'Every Attention Matters: An Efficient Hybrid Architecture for\n  Long-Context Reasoning', 'url': 'https://huggingface.co/papers/2510.19338', 'abstract': 'The Ring-linear model series, including Ring-mini-linear-2.0 and Ring-flash-linear-2.0, uses a hybrid architecture combining linear and softmax attention to reduce inference costs and improve training efficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.', 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'bf307f4447578ac2', 'authors': ['Ling Team', 'Bin Han', 'Caizhi Tang', 'Chen Liang', 'Donghao Zhang', 'Fan Yuan', 'Feng Zhu', 'Jie Gao', 'Jingyu Hu', 'Longfei Li', 'Meng Li', 'Mingyang Zhang', 'Peijie Jiang', 'Peng Jiao', 'Qian Zhao', 'Qingyuan Yang', 'Wenbo Shen', 'Xinxing Yang', 'Yalin Zhang', 'Yankun Ren', 'Yao Zhao', 'Yibo Cao', 'Yixuan Sun', 'Yue Zhang', 'Yuchen Fang', 'Zibin Lin', 'Zixuan Cheng', 'Jun Zhou'], 'affiliations': ['Ling Team'], 'pdf_title_img': 'assets/pdf/title_img/2510.19338.jpg', 'data': {'categories': ['#architecture', '#training', '#long_context', '#benchmark', '#inference', '#optimization'], 'emoji': '💍', 'ru': {'title': 'Гибридное внимание для эффективного вывода на длинных контекстах', 'desc': 'Серия моделей Ring-linear использует гибридную архитектуру, комбинирующую линейное и softmax внимание для снижения вычислительных затрат. Модели Ring-mini-linear-2.0 (16B параметров) и Ring-flash-linear-2.0 (104B параметров) уменьшают стоимость inference в 10 раз по сравнению с dense моделями. Благодаря оптимальному соотношению механизмов внимания и использованию FP8 операторов, эффективность обучения выросла на 50%. Модели демонстрируют SOTA результаты на сложных бенчмарках reasoning при стабильной оптимизации в фазе reinforcement learning.'}, 'en': {'title': 'Efficient Inference with Hybrid Attention Models', 'desc': 'The Ring-linear model series introduces two advanced models, Ring-mini-linear-2.0 and Ring-flash-linear-2.0, which utilize a hybrid architecture that combines linear and softmax attention mechanisms. This innovative approach significantly reduces inference costs and enhances training efficiency, making it suitable for long-context scenarios. With 16B and 104B parameters respectively, these models achieve a remarkable reduction in computational overhead, cutting inference costs to one-tenth of a comparable dense model. Additionally, the integration of a high-performance FP8 operator library has led to a 50% improvement in training efficiency, ensuring that the models maintain state-of-the-art performance across various complex reasoning tasks.'}, 'zh': {'title': '高效推理与训练的混合架构', 'desc': '本论文介绍了Ring-linear模型系列，包括Ring-mini-linear-2.0和Ring-flash-linear-2.0。这些模型采用混合架构，结合了线性注意力和softmax注意力，显著降低了推理成本并提高了训练效率。Ring-mini-linear-2.0拥有160亿参数，而Ring-flash-linear-2.0则有1040亿参数，二者在长上下文推理场景中表现出色。通过优化不同注意力机制的比例，我们找到了当前最佳的模型结构，并利用自研的高性能FP8运算库提高了训练效率。'}}}, {'id': 'https://huggingface.co/papers/2510.19316', 'title': 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via\n  Knowledge-Oriented Augmentations and Constraints', 'url': 'https://huggingface.co/papers/2510.19316', 'abstract': "KORE is a method for injecting new knowledge into large multimodal models while preserving old knowledge, using structured augmentations and covariance matrix constraints to minimize catastrophic forgetting.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models encode extensive factual knowledge in their pre-trained weights. However, its knowledge remains static and limited, unable to keep pace with real-world developments, which hinders continuous knowledge acquisition. Effective knowledge injection thus becomes critical, involving two goals: knowledge adaptation (injecting new knowledge) and knowledge retention (preserving old knowledge). Existing methods often struggle to learn new knowledge and suffer from catastrophic forgetting. To address this, we propose KORE, a synergistic method of KnOwledge-oRientEd augmentations and constraints for injecting new knowledge into large multimodal models while preserving old knowledge. Unlike general text or image data augmentation, KORE automatically converts individual knowledge items into structured and comprehensive knowledge to ensure that the model accurately learns new knowledge, enabling accurate adaptation. Meanwhile, KORE stores previous knowledge in the covariance matrix of LMM's linear layer activations and initializes the adapter by projecting the original weights into the matrix's null space, defining a fine-tuning direction that minimizes interference with previous knowledge, enabling powerful retention. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, LLaVA-v1.5-13B, and Qwen2.5-VL-7B, show that KORE achieves superior new knowledge injection performance and effectively mitigates catastrophic forgetting.", 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '97796b9efcce99e8', 'authors': ['Kailin Jiang', 'Hongbo Jiang', 'Ning Jiang', 'Zhi Gao', 'Jinhe Bi', 'Yuchen Ren', 'Bin Li', 'Yuntao Du', 'Lei Liu', 'Qing Li'], 'affiliations': ['Beijing Institute of Technology', 'Ludwig Maximilian University of Munich', 'Northeast Forestry University', 'Shandong University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'University of Science and Technology of China', 'University of Sydney', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2510.19316.jpg', 'data': {'categories': ['#transfer_learning', '#multimodal', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'KORE: инъекция знаний без забывания старого', 'desc': 'KORE — это метод для внедрения новых знаний в большие мультимодальные модели с сохранением ранее изученной информации. Метод использует структурированные аугментации данных, автоматически преобразуя отдельные факты в комплексные знания для точного обучения. Для предотвращения катастрофического забывания KORE сохраняет предыдущие знания в ковариационной матрице активаций линейных слоев LLM и инициализирует адаптер проекцией в нулевое пространство этой матрицы. Эксперименты на моделях LLaVA и Qwen2.5-VL показали превосходную производительность в инъекции знаний и эффективное сохранение старой информации.'}, 'en': {'title': 'KORE: Retain the Old, Embrace the New in Multimodal Learning!', 'desc': 'KORE is a novel method designed to enhance large multimodal models by injecting new knowledge while ensuring that previously learned information is retained. It utilizes structured augmentations and covariance matrix constraints to minimize the risk of catastrophic forgetting, which is when a model loses old knowledge while learning new information. By converting knowledge items into structured formats, KORE allows models to accurately learn and adapt to new data. Experimental results demonstrate that KORE significantly improves the performance of knowledge injection in various multimodal models without compromising their existing knowledge.'}, 'zh': {'title': 'KORE：知识注入与保留的完美平衡', 'desc': 'KORE是一种向大型多模态模型注入新知识的方法，同时保留旧知识。它通过结构化增强和协方差矩阵约束来最小化灾难性遗忘。KORE能够有效地将新知识适应到模型中，同时确保旧知识的保留。实验表明，KORE在多种大型多模态模型上表现出色，显著提高了新知识注入的效果。'}}}, {'id': 'https://huggingface.co/papers/2510.18313', 'title': 'OmniNWM: Omniscient Driving Navigation World Models', 'url': 'https://huggingface.co/papers/2510.18313', 'abstract': 'OmniNWM is a unified world model for autonomous driving that generates panoramic videos, encodes actions using Plucker ray-maps, and defines dense rewards based on 3D occupancy, achieving top performance in video generation, control, and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Autonomous driving world models are expected to work effectively across three core dimensions: state, action, and reward. Existing models, however, are typically restricted to limited state modalities, short video sequences, imprecise action control, and a lack of reward awareness. In this paper, we introduce OmniNWM, an omniscient panoramic navigation world model that addresses all three dimensions within a unified framework. For state, OmniNWM jointly generates panoramic videos of RGB, semantics, metric depth, and 3D occupancy. A flexible forcing strategy enables high-quality long-horizon auto-regressive generation. For action, we introduce a normalized panoramic Plucker ray-map representation that encodes input trajectories into pixel-level signals, enabling highly precise and generalizable control over panoramic video generation. Regarding reward, we move beyond learning reward functions with external image-based models: instead, we leverage the generated 3D occupancy to directly define rule-based dense rewards for driving compliance and safety. Extensive experiments demonstrate that OmniNWM achieves state-of-the-art performance in video generation, control accuracy, and long-horizon stability, while providing a reliable closed-loop evaluation framework through occupancy-grounded rewards. Project page is available at https://github.com/Arlo0o/OmniNWM.', 'score': 2, 'issue_id': 6567, 'pub_date': '2025-10-21', 'pub_date_card': {'ru': '21 октября', 'en': 'October 21', 'zh': '10月21日'}, 'hash': '234f9481e9fb3cf2', 'authors': ['Bohan Li', 'Zhuang Ma', 'Dalong Du', 'Baorui Peng', 'Zhujin Liang', 'Zhenqiang Liu', 'Chao Ma', 'Yueming Jin', 'Hao Zhao', 'Wenjun Zeng', 'Xin Jin'], 'affiliations': ['Eastern Institute of Technology, Ningbo', 'National University of Singapore', 'PhiGent', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2510.18313.jpg', 'data': {'categories': ['#training', '#video', '#games', '#agents', '#3d', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Всевидящая world model для автономного вождения с панорамным видео и 3D-наградами', 'desc': 'OmniNWM — это универсальная world model для автономного вождения, которая одновременно генерирует панорамные видео с RGB, семантикой, глубиной и 3D occupancy. Модель использует представление траекторий через нормализованные Plucker ray-maps на уровне пикселей, что обеспечивает точный контроль над генерацией видео. Вместо обучения функции reward через внешние модели, система определяет плотные награды напрямую из сгенерированной 3D occupancy на основе правил безопасности вождения. Эксперименты показывают state-of-the-art результаты в генерации видео, точности управления и стабильности на длинных горизонтах предсказания.'}, 'en': {'title': 'OmniNWM: The Future of Autonomous Driving World Models', 'desc': 'OmniNWM is a comprehensive world model designed for autonomous driving that excels in generating panoramic videos and encoding actions with Plucker ray-maps. It effectively integrates state, action, and reward dimensions into a single framework, overcoming limitations of previous models. The model generates high-quality panoramic videos that include RGB, semantics, metric depth, and 3D occupancy, while also providing precise control through pixel-level action encoding. By utilizing 3D occupancy for defining dense rewards, OmniNWM ensures compliance and safety in driving, achieving top performance in video generation and control stability.'}, 'zh': {'title': 'OmniNWM：自动驾驶的全景导航新模型', 'desc': 'OmniNWM是一种统一的世界模型，专为自动驾驶设计，能够生成全景视频，并使用Plucker光线图编码动作，同时基于3D占用定义密集奖励。该模型在状态、动作和奖励三个核心维度上表现出色，克服了现有模型的局限性。OmniNWM生成RGB、语义、度量深度和3D占用的全景视频，采用灵活的强制策略实现高质量的长时间自回归生成。通过直接利用生成的3D占用定义基于规则的密集奖励，OmniNWM在视频生成、控制精度和长期稳定性方面达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2510.19808', 'title': 'Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing', 'url': 'https://huggingface.co/papers/2510.19808', 'abstract': "Pico-Banana-400K is a large-scale, high-quality dataset for instruction-based image editing, featuring diverse edit pairs, multi-turn editing, preference subsets, and long-short instruction pairs, enabling comprehensive research and benchmarking.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.", 'score': 1, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '19f1485a76e3707f', 'authors': ['Yusu Qian', 'Eli Bocek-Rivele', 'Liangchen Song', 'Jialing Tong', 'Yinfei Yang', 'Jiasen Lu', 'Wenze Hu', 'Zhe Gan'], 'affiliations': ['Apple'], 'pdf_title_img': 'assets/pdf/title_img/2510.19808.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#synthetic', '#alignment', '#reasoning'], 'emoji': '🍌', 'ru': {'title': 'Pico-Banana-400K: масштабный датасет для обучения редактированию изображений по текстовым инструкциям', 'desc': 'Статья представляет Pico-Banana-400K — датасет из 400 тысяч изображений для обучения моделей редактированию картинок по текстовым командам. Датасет создан на основе реальных фотографий из OpenImages с использованием системы Nano-Banana и включает разнообразные типы редактирования с контролем качества через MLLM. Помимо базовых примеров, датасет содержит три специализированных подмножества: для последовательного многошагового редактирования (72K примеров), для обучения с предпочтениями и reward-моделей (56K примеров), а также пары длинных и коротких инструкций. Этот ресурс создаёт основу для обучения и бенчмаркинга следующего поколения моделей редактирования изображений по тексту.'}, 'en': {'title': 'Unlocking Advanced Image Editing with Pico-Banana-400K', 'desc': 'Pico-Banana-400K is a new dataset designed for instruction-based image editing, containing 400,000 high-quality images. It features diverse edit pairs and supports complex editing tasks through multi-turn editing and preference subsets. The dataset is built from real images, ensuring high quality and relevance, and includes specialized subsets for various research needs. This resource aims to enhance the development and evaluation of advanced text-guided image editing models.'}, 'zh': {'title': 'Pico-Banana-400K：图像编辑的新基石', 'desc': 'Pico-Banana-400K是一个大规模、高质量的图像编辑数据集，专注于基于指令的图像编辑。该数据集包含多样的编辑对、多轮编辑和偏好子集，支持复杂的编辑场景研究。通过利用Nano-Banana生成真实照片的编辑对，Pico-Banana-400K确保了编辑类型的全面覆盖和内容的精确保留。这个数据集为下一代文本引导的图像编辑模型的训练和基准测试提供了坚实的基础。'}}}, {'id': 'https://huggingface.co/papers/2510.19488', 'title': 'VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos', 'url': 'https://huggingface.co/papers/2510.19488', 'abstract': 'VideoAgentTrek automatically extracts GUI interaction data from YouTube videos using Video2Action, an inverse dynamics module, improving task success rates and step accuracy for computer-use agents.  \t\t\t\t\tAI-generated summary \t\t\t\t Training computer-use agents requires massive amounts of GUI interaction data, but manually annotating action trajectories at scale is prohibitively expensive. We present VideoAgentTrek, a scalable pipeline that automatically mines training data from publicly available screen-recorded videos at web scale, eliminating the need for manual annotation. Our approach addresses a key challenge: raw videos contain implicit demonstrations but lack explicit action labels. To solve this, we develop Video2Action, an inverse dynamics module (IDM) with two components: (1) a video grounding model that detects and localizes GUI actions with precise temporal boundaries and context, and (2) an action-content recognizer that extracts structured parameters like click coordinates and typed text with high fidelity. Applied to 39,000 YouTube tutorial videos, our pipeline generates 1.52 million interaction steps automatically. We leverage this data through continued pretraining followed by supervised fine-tuning. On OSWorld-Verified, our approach improves task success rates from 9.3% (SFT-only baseline) to 15.8%, a 70% relative improvement. On AgentNetBench, step accuracy increases from 64.1% to 69.3%. Our results demonstrate that passive internet videos can be transformed into high-quality supervision for computer-use agents, providing a scalable alternative to expensive manual annotation.', 'score': 1, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': '4237f40e97c8b930', 'authors': ['Dunjie Lu', 'Yiheng Xu', 'Junli Wang', 'Haoyuan Wu', 'Xinyuan Wang', 'Zekun Wang', 'Junlin Yang', 'Hongjin Su', 'Jixuan Chen', 'Junda Chen', 'Yuchen Mao', 'Jingren Zhou', 'Junyang Lin', 'Binyuan Hui', 'Tao Yu'], 'affiliations': ['Qwen Team, Alibaba Group', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.19488.jpg', 'data': {'categories': ['#training', '#dataset', '#video', '#data', '#agents', '#optimization', '#transfer_learning'], 'emoji': '🎥', 'ru': {'title': 'Обучение AI-агентов на YouTube видео вместо ручной разметки', 'desc': 'Исследователи создали VideoAgentTrek — систему, которая автоматически извлекает данные о взаимодействии с GUI из обычных YouTube-видео, устраняя необходимость дорогостоящей ручной разметки. В основе лежит модуль Video2Action, который определяет действия пользователя на видео и извлекает параметры вроде координат кликов и введённого текста. Обработав 39 тысяч обучающих видео, система сгенерировала 1.52 миллиона шагов взаимодействия для обучения AI-агентов. Результаты показали улучшение успешности выполнения задач на 70% по сравнению с базовым подходом, доказывая, что обычные видео из интернета могут стать качественным источником данных для обучения.'}, 'en': {'title': 'Transforming YouTube Videos into Training Gold for AI Agents', 'desc': 'VideoAgentTrek is a novel system that automatically extracts graphical user interface (GUI) interaction data from YouTube videos, addressing the challenge of obtaining large-scale training data for computer-use agents without manual annotation. It utilizes an inverse dynamics module called Video2Action, which includes a video grounding model to identify and time-stamp GUI actions, and an action-content recognizer to capture detailed parameters like click locations and text input. By applying this method to 39,000 tutorial videos, the system generated over 1.5 million interaction steps, significantly enhancing the training dataset. The results show a marked improvement in task success rates and step accuracy, demonstrating the effectiveness of using passive video content for training AI agents.'}, 'zh': {'title': '自动化提取视频交互数据，提升计算机代理性能', 'desc': 'VideoAgentTrek 是一个自动从 YouTube 视频中提取 GUI 交互数据的系统，使用了逆动态模块 Video2Action，显著提高了计算机使用代理的任务成功率和步骤准确性。该系统解决了手动标注大量交互数据的高成本问题，通过从公开的屏幕录制视频中自动挖掘训练数据。Video2Action 包含两个主要组件：视频定位模型和动作内容识别器，能够精确检测和提取 GUI 操作的时间边界和结构化参数。通过对 39,000 个 YouTube 教程视频的应用，我们的管道自动生成了 152 万个交互步骤，展示了被动互联网视频可以转化为高质量的计算机使用代理监督数据。'}}}, {'id': 'https://huggingface.co/papers/2510.19286', 'title': 'TheMCPCompany: Creating General-purpose Agents with Task-specific Tools', 'url': 'https://huggingface.co/papers/2510.19286', 'abstract': "TheMCPCompany evaluates tool-calling agents using REST APIs for interacting with real-world services, showing that advanced models perform well in simpler environments but struggle with complex enterprise environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Since the introduction of the Model Context Protocol (MCP), the number of available tools for Large Language Models (LLMs) has increased significantly. These task-specific tool sets offer an alternative to general-purpose tools such as web browsers, while being easier to develop and maintain than GUIs. However, current general-purpose agents predominantly rely on web browsers for interacting with the environment. Here, we introduce TheMCPCompany, a benchmark for evaluating tool-calling agents on tasks that involve interacting with various real-world services. We use the REST APIs of these services to create MCP servers, which include over 18,000 tools. We also provide manually annotated ground-truth tools for each task. In our experiments, we use the ground truth tools to show the potential of tool-calling agents for both improving performance and reducing costs assuming perfect tool retrieval. Next, we explore agent performance using tool retrieval to study the real-world practicality of tool-based agents. While all models with tool retrieval perform similarly or better than browser-based agents, smaller models cannot take full advantage of the available tools through retrieval. On the other hand, GPT-5's performance with tool retrieval is very close to its performance with ground-truth tools. Overall, our work shows that the most advanced reasoning models are effective at discovering tools in simpler environments, but seriously struggle with navigating complex enterprise environments. TheMCPCompany reveals that navigating tens of thousands of tools and combining them in non-trivial ways to solve complex problems is still a challenging task for current models and requires both better reasoning and better retrieval models.", 'score': 1, 'issue_id': 6567, 'pub_date': '2025-10-22', 'pub_date_card': {'ru': '22 октября', 'en': 'October 22', 'zh': '10月22日'}, 'hash': 'd7d7013ab05a9068', 'authors': ['Reza Esfandiarpoor', 'Vishwas Suryanarayanan', 'Stephen H. Bach', 'Vishal Chowdhary', 'Anthony Aue'], 'affiliations': ['Brown University', 'Microsoft'], 'pdf_title_img': 'assets/pdf/title_img/2510.19286.jpg', 'data': {'categories': ['#benchmark', '#agents', '#reasoning'], 'emoji': '🔧', 'ru': {'title': 'Тысячи инструментов — испытание для AI-агентов', 'desc': 'Исследователи представили TheMCPCompany — бенчмарк для оценки AI-агентов, использующих инструменты через REST API для взаимодействия с реальными сервисами. Бенчмарк включает более 18,000 инструментов, созданных на основе Model Context Protocol (MCP), и позволяет сравнивать performance агентов с инструментами и браузерных агентов. Эксперименты показали, что продвинутые модели типа GPT-5 хорошо справляются с простыми окружениями, но испытывают серьёзные трудности в сложных корпоративных средах с тысячами инструментов. Результаты демонстрируют, что навигация по большому количеству инструментов и их комбинирование для решения сложных задач остаётся вызовом для современных LLM и требует улучшения как reasoning, так и retrieval моделей.'}, 'en': {'title': 'Navigating Complexity: Evaluating Tool-Calling Agents in Real-World Environments', 'desc': 'The paper introduces TheMCPCompany, a benchmark designed to evaluate tool-calling agents that interact with real-world services through REST APIs. It highlights the performance of advanced models in simpler environments, where they excel, but reveals their limitations in complex enterprise settings. The study demonstrates that while tool retrieval enhances agent performance, smaller models struggle to utilize the available tools effectively. Ultimately, the findings suggest that improving reasoning and retrieval capabilities is essential for navigating intricate tasks involving numerous tools.'}, 'zh': {'title': '评估工具调用代理的挑战与机遇', 'desc': 'TheMCPCompany是一个评估工具调用代理的基准，使用REST API与真实世界服务进行交互。研究表明，尽管先进的模型在简单环境中表现良好，但在复杂的企业环境中却面临挑战。我们创建了一个包含超过18,000个工具的MCP服务器，并提供了每个任务的手动标注真实工具。实验结果显示，虽然所有模型在工具检索方面的表现优于基于浏览器的代理，但较小的模型无法充分利用可用工具。'}}}, {'id': 'https://huggingface.co/papers/2510.16844', 'title': 'FinSight: Towards Real-World Financial Deep Research', 'url': 'https://huggingface.co/papers/2510.16844', 'abstract': 'FinSight, a multi-agent framework using CAVM architecture and iterative vision-enhanced mechanism, generates high-quality, multimodal financial reports with superior accuracy and presentation quality compared to existing systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating professional financial reports is a labor-intensive and intellectually demanding process that current AI systems struggle to fully automate. To address this challenge, we introduce FinSight (Financial InSight), a novel multi agent framework for producing high-quality, multimodal financial reports. The foundation of FinSight is the Code Agent with Variable Memory (CAVM) architecture, which unifies external data, designed tools, and agents into a programmable variable space, enabling flexible data collection, analysis and report generation through executable code. To ensure professional-grade visualization, we propose an Iterative Vision-Enhanced Mechanism that progressively refines raw visual outputs into polished financial charts. Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis segments into coherent, citation-aware, and multimodal reports, ensuring both analytical depth and structural consistency. Experiments on various company and industry-level tasks demonstrate that FinSight significantly outperforms all baselines, including leading deep research systems in terms of factual accuracy, analytical depth, and presentation quality, demonstrating a clear path toward generating reports that approach human-expert quality.', 'score': 1, 'issue_id': 6567, 'pub_date': '2025-10-19', 'pub_date_card': {'ru': '19 октября', 'en': 'October 19', 'zh': '10月19日'}, 'hash': '4316f8c0ef12ddf6', 'authors': ['Jiajie Jin', 'Yuyao Zhang', 'Yimeng Xu', 'Hongjin Qian', 'Yutao Zhu', 'Zhicheng Dou'], 'affiliations': ['BAAI', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2510.16844.jpg', 'data': {'categories': ['#multimodal', '#agents', '#architecture'], 'emoji': '📊', 'ru': {'title': 'AI-аналитик создаёт финансовые отчёты профессионального уровня', 'desc': 'FinSight — это мультиагентный фреймворк для автоматической генерации профессиональных финансовых отчётов с графиками и аналитикой. Система использует архитектуру CAVM, которая объединяет данные, инструменты и агенты в программируемое пространство переменных для гибкого анализа через исполняемый код. Итеративный механизм улучшения визуализации последовательно превращает черновые графики в качественные финансовые диаграммы. Двухэтапный процесс написания расширяет краткие аналитические цепочки в связные мультимодальные отчёты со ссылками, превосходя существующие системы по точности и качеству презентации.'}, 'en': {'title': 'FinSight: Revolutionizing Financial Reporting with AI', 'desc': 'FinSight is a multi-agent framework designed to automate the generation of high-quality financial reports. It utilizes the Code Agent with Variable Memory (CAVM) architecture, which allows for flexible data integration and analysis through programmable code. The framework also includes an Iterative Vision-Enhanced Mechanism that improves visual outputs into professional-grade financial charts. Overall, FinSight outperforms existing AI systems in accuracy and presentation, making strides towards achieving human-expert quality in financial reporting.'}, 'zh': {'title': 'FinSight：智能生成高质量金融报告的未来', 'desc': 'FinSight是一个多智能体框架，采用可变内存的代码代理架构（CAVM），能够生成高质量的多模态金融报告。该系统通过可执行代码灵活地收集和分析数据，确保报告的准确性和专业性。为了提升可视化效果，FinSight引入了迭代视觉增强机制，逐步优化原始视觉输出。实验结果表明，FinSight在准确性、分析深度和展示质量上显著优于现有系统，接近人类专家的水平。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi', '#alignment (1)', '#architecture (3)', '#audio', '#benchmark (7)', '#cv (1)', '#data (2)', '#dataset (4)', '#diffusion', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (5)', '#open_source (3)', '#optimization (7)', '#plp', '#rag', '#reasoning (4)', '#rl (3)', '#rlhf', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (6)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-23 02:22',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-23 02:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-23 02:22')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    