
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. April 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-11.html">â¬…ï¸ <span id="prev-date">11.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-15.html">â¡ï¸ <span id="next-date">15.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 14', 'zh': '4æœˆ14æ—¥'};
        let feedDateNext = {'ru': '15.04', 'en': '04/15', 'zh': '4æœˆ15æ—¥'};
        let feedDatePrev = {'ru': '11.04', 'en': '04/11', 'zh': '4æœˆ11æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.08685', 'title': 'Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model', 'url': 'https://huggingface.co/papers/2504.08685', 'abstract': 'This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/', 'score': 77, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '43b42333b796033b', 'authors': ['Team Seawead', 'Ceyuan Yang', 'Zhijie Lin', 'Yang Zhao', 'Shanchuan Lin', 'Zhibei Ma', 'Haoyuan Guo', 'Hao Chen', 'Lu Qi', 'Sen Wang', 'Feng Cheng', 'Feilong Zuo Xuejiao Zeng', 'Ziyan Yang', 'Fangyuan Kong', 'Zhiwu Qing', 'Fei Xiao', 'Meng Wei', 'Tuyen Hoang', 'Siyu Zhang', 'Peihao Zhu', 'Qi Zhao', 'Jiangqiao Yan', 'Liangke Gui', 'Sheng Bi', 'Jiashi Li', 'Yuxi Ren', 'Rui Wang', 'Huixia Li', 'Xuefeng Xiao', 'Shu Liu', 'Feng Ling', 'Heng Zhang', 'Houmin Wei', 'Huafeng Kuang', 'Jerry Duncan', 'Junda Zhang', 'Junru Zheng', 'Li Sun', 'Manlin Zhang', 'Renfei Sun', 'Xiaobin Zhuang', 'Xiaojie Li', 'Xin Xia', 'Xuyan Chi', 'Yanghua Peng', 'Yuping Wang', 'Yuxuan Wang', 'Zhongkai Zhao', 'Zhuo Chen', 'Zuquan Song', 'Zhenheng Yang', 'Jiashi Feng', 'Jianchao Yang', 'Lu Jiang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2504.08685.jpg', 'data': {'categories': ['#video', '#optimization', '#transfer_learning', '#small_models', '#diffusion', '#training', '#architecture'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸', 'desc': 'Ğ­Ñ‚Ğ¾Ñ‚ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² (7B), Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½ÑƒÑ Seaweed-7B, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ñ Ğ½ÑƒĞ»Ñ Ğ·Ğ° 665 000 Ñ‡Ğ°ÑĞ¾Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ GPU H100. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Seaweed-7B Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ’ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Efficient Video Generation with Seaweed-7B: Small Size, Big Impact!', 'desc': 'This paper introduces Seaweed-7B, a mid-sized video generation model with 7 billion parameters, trained efficiently using 665,000 GPU hours. Despite its smaller size, Seaweed-7B achieves competitive performance against larger models, showcasing the importance of strategic design choices in resource-limited environments. The model demonstrates strong generalization capabilities, allowing it to adapt effectively to various downstream tasks through lightweight fine-tuning or continued training. Overall, the findings suggest that a well-designed medium-sized model can rival larger counterparts while being more cost-effective to train.'}, 'zh': {'title': 'ç»æµé«˜æ•ˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹è®­ç»ƒç­–ç•¥', 'desc': 'æœ¬æŠ€æœ¯æŠ¥å‘Šæå‡ºäº†ä¸€ç§ç»æµé«˜æ•ˆçš„è§†é¢‘ç”ŸæˆåŸºç¡€æ¨¡å‹è®­ç»ƒç­–ç•¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸ºSeaweed-7Bçš„ä¸­å‹ç ”ç©¶æ¨¡å‹ï¼Œå…·æœ‰çº¦70äº¿ä¸ªå‚æ•°ï¼Œä½¿ç”¨665,000ä¸ªH100 GPUå°æ—¶ä»é›¶å¼€å§‹è®­ç»ƒã€‚å°½ç®¡è®­ç»ƒèµ„æºé€‚ä¸­ï¼ŒSeaweed-7Bçš„æ€§èƒ½ä¸æ›´å¤§è§„æ¨¡çš„ç°ä»£è§†é¢‘ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ä»ç„¶å…·æœ‰ç«äº‰åŠ›ã€‚æŠ¥å‘Šå¼ºè°ƒäº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­å¢å¼ºä¸­å‹æ‰©æ•£æ¨¡å‹æ€§èƒ½çš„å…³é”®è®¾è®¡å†³ç­–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08736', 'title': 'GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for\n  Autoregressive Image Generation', 'url': 'https://huggingface.co/papers/2504.08736', 'abstract': 'In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.', 'score': 28, 'issue_id': 3216, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '4af199758c238fd4', 'authors': ['Tianwei Xiong', 'Jun Hao Liew', 'Zilong Huang', 'Jiashi Feng', 'Xihui Liu'], 'affiliations': ['ByteDance', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.08736.jpg', 'data': {'categories': ['#architecture', '#optimization', '#cv', '#training'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'GigaTok: Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ±ĞµĞ· ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ²', 'desc': 'GigaTok - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑƒÑ…ÑƒĞ´ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°. GigaTok Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'GigaTok: Balancing Image Quality and Generation in Autoregressive Models', 'desc': 'This paper presents GigaTok, a novel approach to enhance autoregressive image generation by improving visual tokenizers. It addresses the challenge of balancing image reconstruction quality with downstream generation quality, which often deteriorates when scaling tokenizers. The authors introduce semantic regularization to align tokenizer features with those from a pre-trained visual encoder, reducing latent space complexity. By implementing key practices for scaling, GigaTok achieves state-of-the-art results in image reconstruction and generation tasks.'}, 'zh': {'title': 'GigaTokï¼šæå‡å›¾åƒç”Ÿæˆä¸é‡å»ºçš„åˆ›æ–°æ–¹æ³•', 'desc': 'åœ¨è‡ªå›å½’å›¾åƒç”Ÿæˆä¸­ï¼Œè§†è§‰æ ‡è®°å™¨å°†å›¾åƒå‹ç¼©ä¸ºç´§å‡‘çš„ç¦»æ•£æ½œåœ¨æ ‡è®°ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ä¸‹æ¸¸è‡ªå›å½’æ¨¡å‹è®­ç»ƒã€‚å°½ç®¡æ‰©å¤§è§†è§‰æ ‡è®°å™¨å¯ä»¥æé«˜å›¾åƒé‡å»ºè´¨é‡ï¼Œä½†å¾€å¾€ä¼šé™ä½ä¸‹æ¸¸ç”Ÿæˆè´¨é‡ï¼Œè¿™æ˜¯ç°æœ‰æ–‡çŒ®ä¸­æœªèƒ½å……åˆ†è§£å†³çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GigaTokï¼Œè¿™æ˜¯ä¸€ç§åœ¨æ‰©å¤§è§†è§‰æ ‡è®°å™¨æ—¶åŒæ—¶æ”¹å–„å›¾åƒé‡å»ºã€ç”Ÿæˆå’Œè¡¨ç¤ºå­¦ä¹ çš„é¦–ä¸ªæ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡è¯­ä¹‰æ­£åˆ™åŒ–æ¥å‡è½»æ½œåœ¨ç©ºé—´å¤æ‚æ€§ï¼Œä»è€Œåœ¨é‡å»ºå’Œä¸‹æ¸¸ç”Ÿæˆä¹‹é—´å–å¾—ä¸€è‡´çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08388', 'title': 'MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft', 'url': 'https://huggingface.co/papers/2504.08388', 'abstract': 'World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.', 'score': 22, 'issue_id': 3215, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'f87f4b2c67ff61ca', 'authors': ['Junliang Guo', 'Yang Ye', 'Tianyu He', 'Haoyu Wu', 'Yushu Jiang', 'Tim Pearce', 'Jiang Bian'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.08388.jpg', 'data': {'categories': ['#inference', '#benchmark', '#diffusion', '#cv', '#open_source', '#agents', '#games'], 'emoji': 'ğŸ•¹ï¸', 'ru': {'title': 'MineWorld: Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¸Ñ€Ğ° Minecraft Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'MineWorld - ÑÑ‚Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ´Ğ»Ñ Minecraft, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ†ĞµĞ½ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑÑ†ĞµĞ½Ñ‹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4-7 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼, MineWorld Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸.'}, 'en': {'title': 'MineWorld: Real-Time World Modeling in Minecraft', 'desc': "This paper introduces MineWorld, a real-time interactive world model designed for the game Minecraft, which serves as a platform for testing world modeling techniques. The model utilizes a visual-action autoregressive Transformer that processes paired game scenes and actions to predict subsequent scenes based on player interactions. By converting visual inputs and actions into discrete token IDs, MineWorld learns to represent game states and the relationships between actions and outcomes effectively. The authors also present a new parallel decoding algorithm that enhances the model's speed, allowing it to generate multiple frames per second while maintaining high visual quality and action coherence."}, 'zh': {'title': 'MineWorldï¼šå®æ—¶äº’åŠ¨çš„æ™ºèƒ½ä¸–ç•Œæ¨¡å‹', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†MineWorldï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºMinecraftçš„å®æ—¶äº’åŠ¨ä¸–ç•Œæ¨¡å‹ã€‚è¯¥æ¨¡å‹ä½¿ç”¨è§†è§‰-åŠ¨ä½œè‡ªå›å½’Transformerï¼Œèƒ½å¤Ÿæ ¹æ®æ¸¸æˆåœºæ™¯å’Œç›¸åº”çš„åŠ¨ä½œç”Ÿæˆæ–°çš„åœºæ™¯ã€‚é€šè¿‡å°†è§†è§‰åœºæ™¯å’ŒåŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£çš„æ ‡è®°IDï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ¸¸æˆçŠ¶æ€çš„ä¸°å¯Œè¡¨ç¤ºåŠçŠ¶æ€ä¸åŠ¨ä½œä¹‹é—´çš„å…³ç³»ã€‚åœ¨è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°ç”Ÿæˆæ–°åœºæ™¯çš„è§†è§‰è´¨é‡å’ŒåŠ¨ä½œè·Ÿéšèƒ½åŠ›ï¼Œæ˜¾ç¤ºMineWorldåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¸–ç•Œæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08600', 'title': 'SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2504.08600', 'abstract': 'Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.', 'score': 12, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '6153f561c5040630', 'authors': ['Peixian Ma', 'Xialie Zhuang', 'Chengjin Xu', 'Xuhui Jiang', 'Ran Chen', 'Jian Guo'], 'affiliations': ['DataArc Tech Ltd.', 'IDEA Research, International Digital Economy Academy', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.08600.jpg', 'data': {'categories': ['#synthetic', '#benchmark', '#training', '#rl', '#dataset', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ SQL-R1 Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ† Ğ¸ Ğ²Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ NL2SQL Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ñ€Ñ‚Ğ° Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. SQL-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Spider Ğ¸ BIRD, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 7 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Transforming Natural Language Queries with Reinforcement Learning', 'desc': 'This paper presents SQL-R1, a new model designed to convert natural language queries into SQL statements more effectively, especially in complex scenarios like multi-table joins. It addresses the limitations of traditional supervised fine-tuning methods by employing reinforcement learning (RL) to improve reasoning performance. The authors introduce a specialized reward function for NL2SQL tasks and explore the challenges of cold start in training. SQL-R1 demonstrates impressive execution accuracy on benchmark datasets, achieving 88.6% on Spider and 66.6% on BIRD, using minimal synthetic data for training.'}, 'zh': {'title': 'æå‡NL2SQLæ¨ç†æ€§èƒ½çš„æ–°æ–¹æ³•', 'desc': 'è‡ªç„¶è¯­è¨€è½¬SQLï¼ˆNL2SQLï¼‰ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸æ•°æ®åº“è¿›è¡Œç›´è§‚äº¤äº’ã€‚å°½ç®¡åœ¨æ•°æ®åº“åº”ç”¨ä¸­äººæœºäº¤äº’æ–¹é¢å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†æ€§èƒ½ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯æ¶‰åŠå¤šè¡¨è¿æ¥å’ŒåµŒå¥—æŸ¥è¯¢çš„æƒ…å†µã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ¥è®­ç»ƒNL2SQLæ¨¡å‹ï¼Œè¿™å¯èƒ½é™åˆ¶äº†å…¶åœ¨æ–°ç¯å¢ƒä¸­çš„é€‚åº”æ€§å’Œå¯è§£é‡Šæ€§ã€‚ä¸ºæé«˜NL2SQLæ¨¡å‹åœ¨å¤æ‚æƒ…å†µä¸‹çš„æ¨ç†æ€§èƒ½ï¼Œæˆ‘ä»¬æå‡ºäº†SQL-R1ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•è®­ç»ƒçš„æ–°å‹NL2SQLæ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07963', 'title': 'PixelFlow: Pixel-Space Generative Models with Flow', 'url': 'https://huggingface.co/papers/2504.07963', 'abstract': 'We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.', 'score': 9, 'issue_id': 3217, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4cdb0cfa27fd5251', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Shilong Zhang', 'Peize Sun', 'Ping Luo'], 'affiliations': ['Adobe', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2504.07963.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#cv', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'PixelFlow: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ', 'desc': 'PixelFlow - ÑÑ‚Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ğ² Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑÑ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğµ (VAE) Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ²ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞºĞ²Ğ¾Ğ·Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°, PixelFlow Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ FID 1.98 Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ImageNet Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 256x256.'}, 'en': {'title': 'PixelFlow: Revolutionizing Image Generation in Raw Pixel Space', 'desc': 'PixelFlow is a new type of image generation model that works directly with raw pixels instead of using latent spaces like many existing models. This method simplifies the process by removing the need for a pre-trained Variational Autoencoder (VAE), allowing the entire model to be trained in one go. By using efficient cascade flow modeling, PixelFlow maintains low computational costs while generating high-quality images. It has shown impressive results, achieving a low FID score of 1.98 on the ImageNet benchmark, and demonstrates strong performance in generating artistic and semantically controlled images from text prompts.'}, 'zh': {'title': 'PixelFlowï¼šæ–°ä¸€ä»£å›¾åƒç”Ÿæˆæ¨¡å‹çš„çªç ´', 'desc': 'PixelFlowæ˜¯ä¸€ç§ç›´æ¥åœ¨åŸå§‹åƒç´ ç©ºé—´ä¸­æ“ä½œçš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œä¸ä¸»æµçš„æ½œåœ¨ç©ºé—´æ¨¡å‹ä¸åŒã€‚è¿™ç§æ–¹æ³•ç®€åŒ–äº†å›¾åƒç”Ÿæˆè¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†å¯¹é¢„è®­ç»ƒå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„éœ€æ±‚ï¼Œä½¿æ•´ä¸ªæ¨¡å‹å¯ä»¥ç«¯åˆ°ç«¯è®­ç»ƒã€‚é€šè¿‡é«˜æ•ˆçš„çº§è”æµå»ºæ¨¡ï¼ŒPixelFlowåœ¨åƒç´ ç©ºé—´ä¸­å®ç°äº†å¯æ‰¿å—çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨256x256çš„ImageNetæ¡ä»¶å›¾åƒç”ŸæˆåŸºå‡†ä¸Šè¾¾åˆ°äº†1.98çš„FIDå€¼ã€‚å…¶æ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆç»“æœæ˜¾ç¤ºï¼ŒPixelFlowåœ¨å›¾åƒè´¨é‡ã€è‰ºæœ¯æ€§å’Œè¯­ä¹‰æ§åˆ¶æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07405', 'title': 'FlexIP: Dynamic Control of Preservation and Personality for Customized\n  Image Generation', 'url': 'https://huggingface.co/papers/2504.07405', 'abstract': 'With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).', 'score': 7, 'issue_id': 3213, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'fb73f6a8f480a7a1', 'authors': ['Linyan Huang', 'Haonan Lin', 'Yanning Zhou', 'Kaiwen Xiao'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.07405.jpg', 'data': {'categories': ['#inference', '#cv', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'FlexIP - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ 2D Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ²ĞµÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FlexIP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ»ÑƒÑ‡ÑˆĞµĞµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'FlexIP: Balancing Identity and Personalization in 2D Generative Models', 'desc': "This paper presents FlexIP, a new framework designed to improve 2D generative models by separating the tasks of identity preservation and stylistic manipulation. It introduces two components: a Personalization Adapter that allows for diverse editing styles and a Preservation Adapter that ensures the subject's identity remains intact. By integrating these components, FlexIP enables users to dynamically adjust the balance between identity and personalization during the generation process. Experimental results show that FlexIP outperforms traditional methods, providing better identity retention while allowing for a wider range of personalized outputs."}, 'zh': {'title': 'çµæ´»çš„èº«ä»½ä¿æŒä¸ä¸ªæ€§åŒ–ç¼–è¾‘', 'desc': 'éšç€äºŒç»´ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œä¿æŒä¸»ä½“èº«ä»½åŒæ—¶å®ç°å¤šæ ·åŒ–ç¼–è¾‘æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨èº«ä»½ä¿æŒå’Œä¸ªæ€§åŒ–æ“ä½œä¹‹é—´å­˜åœ¨å›ºæœ‰çš„æƒè¡¡ã€‚æˆ‘ä»¬æå‡ºäº†FlexIPï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªä¸“é—¨çš„ç»„ä»¶è§£è€¦è¿™äº›ç›®æ ‡ï¼šä¸ªæ€§åŒ–é€‚é…å™¨ç”¨äºé£æ ¼åŒ–æ“ä½œï¼Œä¿æŒé€‚é…å™¨ç”¨äºèº«ä»½ç»´æŠ¤ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½é™åˆ¶ï¼Œå®ç°äº†æ›´ä¼˜çš„èº«ä»½ä¿æŒï¼ŒåŒæ—¶æ”¯æŒæ›´å¤šæ ·åŒ–çš„ä¸ªæ€§åŒ–ç”Ÿæˆèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08716', 'title': 'ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance', 'url': 'https://huggingface.co/papers/2504.08716', 'abstract': "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.", 'score': 5, 'issue_id': 3217, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '50998c3f1c1cc54d', 'authors': ['Wissam Antoun', 'BenoÃ®t Sagot', 'DjamÃ© Seddah'], 'affiliations': ['Inria, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.08716.jpg', 'data': {'categories': ['#optimization', '#architecture', '#benchmark', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° vs Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¶Ğ½ĞµĞµ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹?', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² ModernBERT Ğ¸ DeBERTaV3. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² ModernBERT Ğ½Ğ° Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¸ CamemBERTaV2 (Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeBERTaV3 Ğ´Ğ»Ñ Ñ„Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰ĞµĞµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ½Ğ¾ ModernBERT Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞºĞ¾Ğ½ĞµÑ‡Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Disentangling Architecture from Data in Transformer Models', 'desc': 'This paper investigates the performance of ModernBERT compared to DeBERTaV3 by controlling for training data. The authors find that while ModernBERT shows faster training and inference speeds, DeBERTaV3 outperforms it in terms of sample efficiency and overall benchmark results. The study highlights that high-quality pre-training data can speed up the training process but does not necessarily enhance final model performance. Ultimately, the research emphasizes the need to separate the effects of model architecture from the quality of training data when assessing transformer models.'}, 'zh': {'title': 'æ¶æ„åˆ›æ–°ä¸é¢„è®­ç»ƒæ•°æ®çš„åˆ†ç¦»è¯„ä¼°', 'desc': 'æœ¬æ–‡ç ”ç©¶äº†é¢„è®­ç»ƒçš„å˜æ¢å™¨ç¼–ç å™¨æ¨¡å‹ï¼Œå¦‚DeBERTaV3å’ŒModernBERTï¼Œæ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚å°½ç®¡ModernBERTåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºDeBERTaV3ï¼Œä½†ç”±äºç¼ºä¹å…¬å¼€çš„è®­ç»ƒæ•°æ®å’Œå…±äº«æ•°æ®é›†çš„æ¯”è¾ƒï¼Œéš¾ä»¥åˆ¤æ–­è¿™äº›æå‡æ˜¯ç”±äºæ¶æ„æ”¹è¿›è¿˜æ˜¯è®­ç»ƒæ•°æ®çš„å·®å¼‚ã€‚é€šè¿‡åœ¨ä¸CamemBERTaV2ç›¸åŒçš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒModernBERTï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œæ—§ä¸€ä»£æ¨¡å‹åœ¨æ ·æœ¬æ•ˆç‡å’Œæ•´ä½“åŸºå‡†æ€§èƒ½ä¸Šä»ç„¶ä¼˜äºæ–°æ¨¡å‹ï¼ŒModernBERTçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºæ›´å¿«çš„è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨è¯„ä¼°å˜æ¢å™¨æ¨¡å‹æ—¶ï¼Œå°†é¢„è®­ç»ƒæ•°æ®ä¸æ¶æ„åˆ›æ–°åˆ†å¼€è€ƒè™‘çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08727', 'title': 'Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images', 'url': 'https://huggingface.co/papers/2504.08727', 'abstract': 'We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.', 'score': 4, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '0bf30d396e917961', 'authors': ['Boyang Deng', 'Songyou Peng', 'Kyle Genova', 'Gordon Wetzstein', 'Noah Snavely', 'Leonidas Guibas', 'Thomas Funkhouser'], 'affiliations': ['Google DeepMind', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.08727.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#cv', '#interpretability'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ˜Ğ˜ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ, Ñ Ñ†ĞµĞ»ÑŒÑ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ°Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğµ Ğ±ĞµĞ· Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ½ÑÑ‚Ñ‹Ñ… Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°Ñ….'}, 'en': {'title': 'Unlocking Urban Trends with Multimodal LLMs', 'desc': 'This paper introduces a system that utilizes Multimodal Large Language Models (MLLMs) to analyze a vast collection of images taken over time, aiming to identify patterns in urban changes. The system is designed to answer open-ended questions about frequent changes in a city without relying on predefined labels or subjects, which sets it apart from traditional visual analysis methods. To handle the enormous dataset, the authors propose a bottom-up approach that breaks down the analysis into smaller, manageable sub-problems, each addressed with tailored MLLM solutions. Experimental results demonstrate that this innovative approach significantly outperforms existing methods, successfully uncovering notable urban trends from the image data.'}, 'zh': {'title': 'åˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å‘ç°åŸå¸‚å˜åŒ–è¶‹åŠ¿', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ†æå¤§å‹æ•°æ®åº“çš„ç³»ç»Ÿï¼Œè¯¥æ•°æ®åº“åŒ…å«æ•°åƒä¸‡å¼ åœ¨ä¸åŒæ—¶é—´æ‹æ‘„çš„å›¾åƒï¼Œæ—¨åœ¨å‘ç°æ—¶é—´å˜åŒ–ä¸­çš„æ¨¡å¼ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨åŸå¸‚åœ¨ä¸€å®šæ—¶é—´å†…é¢‘ç¹å…±ç°çš„å˜åŒ–ï¼ˆ"è¶‹åŠ¿"ï¼‰ã€‚ä¸ä»¥å¾€çš„è§†è§‰åˆ†æä¸åŒï¼Œæˆ‘ä»¬çš„åˆ†æèƒ½å¤Ÿå›ç­”å¼€æ”¾å¼é—®é¢˜ï¼Œè€Œæ— éœ€é¢„å…ˆè®¾å®šç›®æ ‡ä¸»é¢˜æˆ–è®­ç»ƒæ ‡ç­¾ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„æ–¹æ³•ï¼Œå°†åºå¤§çš„è§†è§‰åˆ†æé—®é¢˜åˆ†è§£ä¸ºæ›´æ˜“å¤„ç†çš„å­é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†åŸºäºMLLMçš„è§£å†³æ–¹æ¡ˆï¼Œå®éªŒç»“æœæ˜¾ç¤ºè¯¥ç³»ç»Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿ï¼Œèƒ½å¤Ÿä»å¤§åŸå¸‚çš„å›¾åƒä¸­å‘ç°æœ‰è¶£çš„è¶‹åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08641', 'title': 'Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization', 'url': 'https://huggingface.co/papers/2504.08641', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'a354f738033ac186', 'authors': ['Jialu Li', 'Shoubin Yu', 'Han Lin', 'Jaemin Cho', 'Jaehong Yoon', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2504.08641.jpg', 'data': {'categories': ['#training', '#multimodal', '#video', '#benchmark', '#diffusion'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Video-MSG: Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-MSG - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ñ… Ñ„Ğ¾Ğ½, Ğ¿ĞµÑ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ—Ğ°Ñ‚ĞµĞ¼ Video-MSG Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ text-to-video Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ ÑˆÑƒĞ¼Ğ° Ğ¸ Ğ´ĞµĞ½Ğ¾Ğ¹Ğ·Ğ¸Ğ½Ğ³. ĞœĞµÑ‚Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T2V.'}, 'en': {'title': 'Streamlining Text-to-Video Generation with Video-MSG', 'desc': 'This paper presents Video-MSG, a novel method for improving text-to-video (T2V) generation without the need for fine-tuning or additional memory during inference. It introduces a three-step process that creates a Video Sketch, which outlines the spatial and temporal elements of the video, including backgrounds and object movements. By guiding T2V diffusion models with this structured plan, Video-MSG enhances the alignment of generated videos with text descriptions. The method shows promising results across various T2V models and benchmarks, demonstrating its effectiveness in generating high-quality videos that accurately reflect the input prompts.'}, 'zh': {'title': 'æ— è®­ç»ƒçš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰æ‰©æ•£æ¨¡å‹çš„è¿›å±•æ˜¾è‘—æé«˜äº†ç”Ÿæˆè§†é¢‘çš„è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€æ–°çš„T2Væ¨¡å‹ï¼Œåœ¨å‡†ç¡®éµå¾ªæ–‡æœ¬æè¿°æ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç²¾ç¡®æ§åˆ¶ç©ºé—´å¸ƒå±€æˆ–ç‰©ä½“è½¨è¿¹æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-MSGï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¤šæ¨¡æ€è§„åˆ’å’Œç»“æ„åŒ–å™ªå£°åˆå§‹åŒ–çš„æ— è®­ç»ƒæŒ‡å¯¼æ–¹æ³•ã€‚Video-MSGé€šè¿‡åˆ›å»ºè§†é¢‘è‰å›¾æ¥å¼•å¯¼ä¸‹æ¸¸T2Væ‰©æ•£æ¨¡å‹ï¼Œä»è€Œåœ¨ä¸éœ€è¦é¢å¤–å†…å­˜çš„æƒ…å†µä¸‹æé«˜æ–‡æœ¬å¯¹é½æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08591', 'title': 'ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image\n  Restoration', 'url': 'https://huggingface.co/papers/2504.08591', 'abstract': 'Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.', 'score': 4, 'issue_id': 3227, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'd11a697d646326c2', 'authors': ['Yongsheng Yu', 'Haitian Zheng', 'Zhifei Zhang', 'Jianming Zhang', 'Yuqian Zhou', 'Connelly Barnes', 'Yuchen Liu', 'Wei Xiong', 'Zhe Lin', 'Jiebo Luo'], 'affiliations': ['Adobe Research', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2504.08591.jpg', 'data': {'categories': ['#optimization', '#training', '#cv', '#architecture', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ²ĞµÑ€Ñ…Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ', 'desc': 'ZipIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² 32 Ñ€Ğ°Ğ·Ğ°. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ Diffusion Transformer (DiT). ZipIR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Latent Pyramid VAE Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ´Ğ¾ 2K, ZipIR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµÑÑ‚Ğ°Ğ²Ñ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'ZipIR: Efficient High-Resolution Image Restoration with Latent Compression', 'desc': 'This paper presents ZipIR, a new framework designed to improve the efficiency and scalability of high-resolution image restoration using generative models. It addresses the challenges of long-range attention mechanisms in diffusion models, which can be computationally intensive. ZipIR utilizes a compressed latent representation that reduces the image size by 32 times, allowing for the application of high-capacity models like the Diffusion Transformer. The framework, trained on images up to 2K resolution, demonstrates superior speed and quality compared to existing methods, effectively restoring images from severely degraded conditions.'}, 'zh': {'title': 'ZipIRï¼šé«˜æ•ˆé«˜åˆ†è¾¨ç‡å›¾åƒä¿®å¤çš„æ–°æ¡†æ¶', 'desc': 'æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•æ˜¾è‘—æå‡äº†å›¾åƒä¿®å¤çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯é€šè¿‡å¼ºå¤§çš„æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿå‡ºè‰²åœ°æ¢å¤è¯­ä¹‰ç»†èŠ‚å’Œå±€éƒ¨æ¸…æ™°åº¦ã€‚ç„¶è€Œï¼Œåœ¨è¶…é«˜åˆ†è¾¨ç‡ä¸‹éƒ¨ç½²è¿™äº›æ¨¡å‹æ—¶ï¼Œç”±äºé•¿ç¨‹æ³¨æ„æœºåˆ¶çš„è®¡ç®—éœ€æ±‚ï¼Œé¢ä¸´è´¨é‡ä¸æ•ˆç‡ä¹‹é—´çš„å…³é”®æƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ZipIRï¼Œä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œå¢å¼ºäº†é«˜åˆ†è¾¨ç‡å›¾åƒä¿®å¤çš„æ•ˆç‡ã€å¯æ‰©å±•æ€§å’Œé•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ã€‚ZipIRé‡‡ç”¨é«˜åº¦å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºï¼Œå°†å›¾åƒå‹ç¼©32å€ï¼Œæœ‰æ•ˆå‡å°‘ç©ºé—´æ ‡è®°çš„æ•°é‡ï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨åƒæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰è¿™æ ·çš„é«˜å®¹é‡æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08366', 'title': 'In-2-4D: Inbetweening from Two Single-View Images to 4D Generation', 'url': 'https://huggingface.co/papers/2504.08366', 'abstract': 'We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/', 'score': 4, 'issue_id': 3213, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'df65a8f6baab7f84', 'authors': ['Sauradip Nag', 'Daniel Cohen-Or', 'Hao Zhang', 'Ali Mahdavi-Amiri'], 'affiliations': ['Simon Fraser University, Canada', 'Tel Aviv University, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2504.08366.jpg', 'data': {'categories': ['#video', '#diffusion', '#3d', '#optimization'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ˜Ğ· 2D Ğ² 4D: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ²ÑƒĞ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ In-2-4D Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 4D-Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ²ÑƒÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ÑÑ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Gaussian Splatting, Ğ° Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ°Ğ´Ñ€Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾Ğ»Ğµ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Transforming 2D Images into Smooth 4D Motion!', 'desc': 'This paper introduces the In-2-4D problem, which focuses on generating 4D representations of motion from just two images of an object in different states. The authors propose a hierarchical method that identifies keyframes to create smooth transitions between these states, addressing challenges posed by large frame-to-frame motions. They utilize Gaussian Splatting to construct 3D representations and apply a deformation field to guide the transformation of these representations into dynamic forms. The approach enhances temporal consistency and smoothness by merging motion segments and optimizing them to align with the original video, demonstrating its effectiveness through various experiments.'}, 'zh': {'title': 'ä»é™æ€åˆ°åŠ¨æ€ï¼š4Dè¿åŠ¨æ’å€¼çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼ŒIn-2-4Dï¼Œæ—¨åœ¨ä»ä¸¤ä¸ªä¸åŒè¿åŠ¨çŠ¶æ€çš„å•è§†å›¾å›¾åƒä¸­ç”Ÿæˆ4Dï¼ˆå³3D + åŠ¨ä½œï¼‰æ’å€¼ã€‚ç»™å®šè¡¨ç¤ºç‰©ä½“è¿åŠ¨èµ·å§‹å’Œç»“æŸçŠ¶æ€çš„ä¸¤å¹…å›¾åƒï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç”Ÿæˆå’Œé‡å»º4Dä¸­çš„è¿åŠ¨ã€‚æˆ‘ä»¬é‡‡ç”¨è§†é¢‘æ’å€¼æ¨¡å‹æ¥é¢„æµ‹è¿åŠ¨ï¼Œä½†å¤§å¹…åº¦çš„å¸§é—´è¿åŠ¨å¯èƒ½å¯¼è‡´æ¨¡ç³Šçš„è§£é‡Šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆ†å±‚æ–¹æ³•è¯†åˆ«ä¸è¾“å…¥çŠ¶æ€è§†è§‰ä¸Šæ¥è¿‘ä¸”è¿åŠ¨æ˜¾è‘—çš„å…³é”®å¸§ï¼Œç„¶ååœ¨å®ƒä»¬ä¹‹é—´ç”Ÿæˆå¹³æ»‘çš„ç‰‡æ®µã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07866', 'title': 'Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs', 'url': 'https://huggingface.co/papers/2504.07866', 'abstract': 'We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.', 'score': 4, 'issue_id': 3222, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': 'dbf55819969ad502', 'authors': ['Yichun Yin', 'Wenyong Huang', 'Kaikai Song', 'Yehui Tang', 'Xueyu Wu', 'Wei Guo', 'Peng Guo', 'Yaoyuan Wang', 'Xiaojun Meng', 'Yasheng Wang', 'Dong Li', 'Can Chen', 'Dandan Tu', 'Yin Li', 'Fisher Yu', 'Ruiming Tang', 'Yunhe Wang', 'Baojun Wang', 'Bin Wang', 'Bo Wang', 'Boxiao Liu', 'Changzheng Zhang', 'Duyu Tang', 'Fei Mi', 'Hui Jin', 'Jiansheng Wei', 'Jiarui Qin', 'Jinpeng Li', 'Jun Zhao', 'Liqun Deng', 'Lin Li', 'Minghui Xu', 'Naifu Zhang', 'Nianzu Zheng', 'Qiang Li', 'Rongju Ruan', 'Shengjun Cheng', 'Tianyu Guo', 'Wei He', 'Wei Li', 'Weiwen Liu', 'Wulong Liu', 'Xinyi Dai', 'Yonghan Dong', 'Yu Pan', 'Yue Li', 'Yufei Wang', 'Yujun Li', 'Yunsheng Ni', 'Zhe Liu', 'Zhenhe Zhang', 'Zhicheng Liu'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2504.07866.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#optimization', '#architecture'], 'emoji': 'ğŸš€', 'ru': {'title': 'Pangu Ultra: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ñ‹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Pangu Ultra - ÑÑ‚Ğ¾ ĞºÑ€ÑƒĞ¿Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ 135 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ°Ñ… Ascend. Ğ”Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ depth-scaled sandwich normalization. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 13,2 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 8192 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ñ€Ğ¾Ğ² Ascend. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Pangu Ultra Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ´Ğ°Ğ¶Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ².'}, 'en': {'title': 'Pangu Ultra: Redefining Large Language Model Training Efficiency', 'desc': 'Pangu Ultra is a Large Language Model (LLM) featuring 135 billion parameters, designed to enhance natural language processing tasks. The model employs depth-scaled sandwich normalization to stabilize training and prevent loss spikes, addressing challenges associated with training large-scale models. It is pre-trained on an extensive dataset of 13.2 trillion tokens and further refined to improve reasoning capabilities. Evaluations show that Pangu Ultra outperforms existing dense LLMs and competes well with sparse models, demonstrating the efficiency of Ascend NPUs in handling such large models.'}, 'zh': {'title': 'Pangu Ultraï¼šè¶…è¶Šå¤§å‹è¯­è¨€æ¨¡å‹çš„æé™', 'desc': 'Pangu Ultraæ˜¯ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ‹¥æœ‰1350äº¿ä¸ªå‚æ•°ï¼Œé‡‡ç”¨å¯†é›†çš„Transformeræ¨¡å—ï¼Œå¹¶åœ¨Ascendç¥ç»å¤„ç†å•å…ƒä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ·±åº¦ç¼©æ”¾çš„ä¸‰æ˜æ²»å½’ä¸€åŒ–æ–¹æ³•ï¼Œæœ‰æ•ˆæ¶ˆé™¤äº†æ·±åº¦æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ³¢åŠ¨ã€‚æˆ‘ä»¬åœ¨132ä¸‡äº¿ä¸ªå¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æ ‡è®°ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨åæœŸè®­ç»ƒä¸­è¿›ä¸€æ­¥å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒPangu Ultraåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†å¯†é›†å‹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†Ascend NPUåœ¨è®­ç»ƒè¶…è¿‡1000äº¿å‚æ•°çš„å¯†é›†æ¨¡å‹æ–¹é¢çš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.07615', 'title': 'VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model', 'url': 'https://huggingface.co/papers/2504.07615', 'abstract': 'Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs\' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1', 'score': 4, 'issue_id': 3226, 'pub_date': '2025-04-10', 'pub_date_card': {'ru': '10 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 10', 'zh': '4æœˆ10æ—¥'}, 'hash': '4a70c069a64a48af', 'authors': ['Haozhan Shen', 'Peng Liu', 'Jingcheng Li', 'Chunxin Fang', 'Yibo Ma', 'Jiajia Liao', 'Qiaoli Shen', 'Zilun Zhang', 'Kangjia Zhao', 'Qianqian Zhang', 'Ruochen Xu', 'Tiancheng Zhao'], 'affiliations': ['Binjiang Institute of Zhejiang University', 'Om AI Research', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.07615.jpg', 'data': {'categories': ['#open_source', '#rl', '#multimodal', '#reasoning', '#training', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ VLM-R1 - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ñ… Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ RL, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¾Ğ¹ Ğ¿Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ RL Ğ½Ğ° VLM, Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ñ€ÑĞ´ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Vision-Language Models with Reinforcement Learning', 'desc': 'This paper presents VLM-R1, a framework that applies reinforcement learning (RL) to enhance the visual reasoning capabilities of Vision-Language Models (VLMs). By utilizing a rule-based reward system, the framework effectively leverages tasks with clear ground-truth answers, leading to improved performance on various vision-language tasks. The experimental results demonstrate that VLM-R1 not only competes well with traditional Supervised Fine-Tuning (SFT) methods but also shows superior generalization abilities. Additionally, the study reveals important insights into reward dynamics and the effects of training data quality on model performance.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›', 'desc': 'æœ€è¿‘ï¼ŒDeepSeek R1å±•ç¤ºäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚R1çš„æ ¸å¿ƒåœ¨äºå…¶åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶ï¼Œåˆ©ç”¨å…·æœ‰ç¡®å®šæ€§çœŸå®ç­”æ¡ˆçš„ä»»åŠ¡æ¥å®ç°ç²¾ç¡®å’Œç¨³å®šçš„å¥–åŠ±è®¡ç®—ã€‚æˆ‘ä»¬å‘ç°ï¼Œè®¸å¤šè§†è§‰ç†è§£ä»»åŠ¡ä¹Ÿå…·å¤‡è‰¯å¥½çš„çœŸå®æ ‡æ³¨ï¼Œä½¿å…¶ä¸åŸºäºè§„åˆ™çš„å¥–åŠ±æœºåˆ¶è‡ªç„¶å…¼å®¹ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†VLM-R1æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05262', 'title': 'Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning\n  vs. Memorization in Large Language Models', 'url': 'https://huggingface.co/papers/2504.05262', 'abstract': 'Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on numerical addition, performance collapses to leq7.5\\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.', 'score': 4, 'issue_id': 3220, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': '428bb7d2af34af73', 'authors': ['Yang Yan', 'Yu Lu', 'Renjun Xu', 'Zhenzhong Lan'], 'affiliations': ['School of Engineering, Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.05262.jpg', 'data': {'categories': ['#alignment', '#math', '#architecture', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ğ°Ñ€Ğ¸Ñ„Ğ¼ĞµÑ‚Ğ¸ĞºĞµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ½Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğº Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰ĞµĞ¼Ñƒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ²ÑƒÑ… Ñ†ĞµĞ»Ñ‹Ñ… Ñ‡Ğ¸ÑĞµĞ» Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğµ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ. ĞŸÑ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ LLM Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Unveiling the Limits of LLMs: From Memorization to Mathematical Reasoning', 'desc': 'This paper examines whether Large Language Models (LLMs) truly understand mathematical principles or simply memorize patterns. The authors focus on basic two-integer addition and test LLMs on their ability to apply commutativity and compositional generalization. Despite high accuracy in straightforward addition tasks, LLMs perform poorly when faced with symbolic mappings, indicating a lack of generalization. The results suggest that LLMs depend more on memorization than on learning mathematical rules, revealing significant limitations in their architecture for true mathematical reasoning.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°å­¦æ¨ç†èƒ½åŠ›ä¸è¶³', 'desc': 'å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ï¼Œä½†å®ƒä»¬åœ¨ç®€å•é—®é¢˜ä¸Šå¸¸å¸¸å¤±è´¥ï¼Œè¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šLLMsæ˜¯å­¦ä¹ äº†æ•°å­¦åŸç†ï¼Œè¿˜æ˜¯ä»…ä»…è®°å¿†äº†æ¨¡å¼ï¼Ÿæˆ‘ä»¬é€šè¿‡ç ”ç©¶åŸºæœ¬çš„ä¸¤æ•´æ•°åŠ æ³•ï¼ˆ0åˆ°2^{64}ï¼‰æ¥æ¢è®¨è¿™ä¸€ç‚¹ï¼Œé‡ç‚¹å…³æ³¨ä¸¤ä¸ªæ ¸å¿ƒå±æ€§ï¼šäº¤æ¢å¾‹ï¼ˆA+B=B+Aï¼‰å’Œç»„åˆæ³›åŒ–ï¼ˆé€šè¿‡åŒæ„ç¬¦å·æ˜ å°„ï¼Œä¾‹å¦‚7æ˜ å°„åˆ°yï¼‰ã€‚å°½ç®¡æœ€å…ˆè¿›çš„LLMsåœ¨æ•°å­—åŠ æ³•ä¸Šå–å¾—äº†73.8-99.8%çš„å‡†ç¡®ç‡ï¼Œä½†åœ¨ç¬¦å·æ˜ å°„ä¸‹ï¼Œæ€§èƒ½éª¤é™è‡³ä¸è¶…è¿‡7.5%ï¼Œè¿™è¡¨æ˜å®ƒä»¬æœªèƒ½æ³›åŒ–æ‰€å­¦è§„åˆ™ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„LLMsä¾èµ–äºè®°å¿†æ¨¡å¼è€ŒéçœŸæ­£çš„è§„åˆ™å­¦ä¹ ï¼Œçªæ˜¾äº†å…¶æ¶æ„çš„å±€é™æ€§ï¼Œå¹¶éœ€è¦æ–°çš„æ–¹æ³•æ¥å®ç°çœŸæ­£çš„æ•°å­¦æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01883', 'title': 'CoRAG: Collaborative Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2504.01883', 'abstract': 'Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.', 'score': 3, 'issue_id': 3218, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': 'f3bd4bbb45b0315a', 'authors': ['Aashiq Muhamed', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Language Technologies Institute, Carnegie Mellon University', 'Machine Learning Department, Carnegie Mellon University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01883.jpg', 'data': {'categories': ['#transfer_learning', '#rag', '#benchmark', '#low_resource'], 'emoji': 'ğŸ¤', 'ru': {'title': 'CoRAG: Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CoRAG - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ RAG. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº CRAB Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CoRAG Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€Ğ¸ÑĞºĞ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ½Ñ‹Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹ Ğ¾Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Collaborative Learning with CoRAG: Enhancing RAG Models Together!', 'desc': 'The paper presents CoRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) models for collaborative learning environments. In this setup, multiple clients work together to train a shared model using a common passage store, which improves performance on knowledge-intensive tasks. The authors introduce CRAB, a benchmark for evaluating collaborative question answering, and show that CoRAG outperforms traditional methods in low-resource situations. Key insights include the importance of relevant passages, the unexpected advantages of irrelevant ones, and the risks posed by hard negatives, highlighting the complexities of collaborative knowledge sharing.'}, 'zh': {'title': 'åä½œå¢å¼ºç”Ÿæˆï¼šå…±äº«çŸ¥è¯†çš„åŠ›é‡ä¸æŒ‘æˆ˜', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCoRAGçš„æ¡†æ¶ï¼Œå®ƒæ‰©å±•äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨åä½œç¯å¢ƒä¸­å…±åŒè®­ç»ƒå…±äº«æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†CRABåŸºå‡†ï¼Œç”¨äºè¯„ä¼°åä½œåŒè´¨å¼€æ”¾åŸŸé—®ç­”çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoRAGåœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„å‚æ•°åä½œå­¦ä¹ æ–¹æ³•å’Œæœ¬åœ°è®­ç»ƒçš„RAGæ¨¡å‹ã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†å…±äº«çŸ¥è¯†åº“ä¸­ç›¸å…³æ®µè½çš„é‡è¦æ€§ï¼Œä»¥åŠå¼•å…¥æ— å…³æ®µè½çš„æ„å¤–å¥½å¤„å’Œå›°éš¾è´Ÿæ ·æœ¬å¯¹æ€§èƒ½çš„æ½œåœ¨è´Ÿé¢å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08192', 'title': 'SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs', 'url': 'https://huggingface.co/papers/2504.08192', 'abstract': 'Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.', 'score': 2, 'issue_id': 3218, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': 'e6e92f7d8a3d930d', 'authors': ['Aashiq Muhamed', 'Jacopo Bonato', 'Mona Diab', 'Virginia Smith'], 'affiliations': ['Carnegie Mellon University', 'Leonardo Labs'], 'pdf_title_img': 'assets/pdf/title_img/2504.08192.jpg', 'data': {'categories': ['#training', '#security', '#interpretability', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Dynamic DAE Guardrails (DSG), Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ…. DSG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ÑÑ‚ÑŒ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. DSG Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒÑ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ€Ğ°Ğ·Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Dynamic Unlearning: Enhancing Safety in LLMs with DSG', 'desc': 'This paper discusses a new method called Dynamic DAE Guardrails (DSG) for improving machine unlearning in large language models (LLMs). Traditional gradient-based unlearning methods face several challenges, including high computational costs and vulnerability to relearning attacks. The authors propose using Sparse Autoencoders (SAEs) to enhance unlearning efficiency and stability, demonstrating that DSG can outperform existing methods. Their experiments show that DSG provides better performance in terms of forget-utility trade-offs, making unlearning more effective and interpretable.'}, 'zh': {'title': 'åŠ¨æ€å»å™ªè‡ªç¼–ç å™¨ï¼šæå‡æœºå™¨é—å¿˜çš„æ•ˆç‡ä¸ç¨³å®šæ€§', 'desc': 'æœºå™¨é—å¿˜æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ä»æ¨¡å‹ä¸­ç§»é™¤ä¸å¿…è¦çš„çŸ¥è¯†æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„å®‰å…¨æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºæ¢¯åº¦çš„é—å¿˜æ–¹æ³•å­˜åœ¨è®¡ç®—æˆæœ¬é«˜ã€è¶…å‚æ•°ä¸ç¨³å®šã€é¡ºåºé—å¿˜èƒ½åŠ›å·®ç­‰é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŠ¨æ€å»å™ªè‡ªç¼–ç å™¨æ–¹æ³•ï¼ˆDSGï¼‰ï¼Œé€šè¿‡ç²¾ç¡®çš„ç‰¹å¾é€‰æ‹©å’ŒåŠ¨æ€åˆ†ç±»å™¨æ¥å®ç°æ›´æœ‰æ•ˆçš„é—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDSGåœ¨é—å¿˜æ•ˆç‡å’Œå®ç”¨æ€§ä¹‹é—´å–å¾—äº†æ˜¾è‘—çš„å¹³è¡¡ï¼Œè¶…è¶Šäº†ç°æœ‰çš„é—å¿˜æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.01786', 'title': 'BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing', 'url': 'https://huggingface.co/papers/2504.01786', 'abstract': "3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.", 'score': 2, 'issue_id': 3231, 'pub_date': '2025-04-02', 'pub_date_card': {'ru': '2 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 2', 'zh': '4æœˆ2æ—¥'}, 'hash': '2c60846bd7ed943f', 'authors': ['Yunqi Gu', 'Ian Huang', 'Jihyeon Je', 'Guandao Yang', 'Leonidas Guibas'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2504.01786.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#3d', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'BlenderGym: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ VLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ BlenderGym - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ vision-language models (VLM) Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸. BlenderGym Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ VLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ´Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ VLM-ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸, Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Blender. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ²Ğ»Ğ¸ÑÑÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸, Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑĞ°Ğ¼ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'BlenderGym: Benchmarking VLMs for 3D Graphics Editing', 'desc': 'This paper introduces BlenderGym, a new benchmark designed to evaluate vision-language models (VLMs) in the context of 3D graphics editing. The authors highlight the challenges of automating this complex task, which requires diverse skills and human-level perception. Through their evaluation, they find that even advanced VLMs struggle with tasks that are relatively simple for human users. Additionally, the study explores how inference scaling techniques can enhance VLM performance, revealing that optimizing the distribution of computational resources between generation and verification can lead to better outcomes.'}, 'zh': {'title': 'BlenderGymï¼š3Då›¾å½¢ç¼–è¾‘çš„æ™ºèƒ½è‡ªåŠ¨åŒ–æ–°åŸºå‡†', 'desc': '3Då›¾å½¢ç¼–è¾‘åœ¨ç”µå½±åˆ¶ä½œå’Œæ¸¸æˆè®¾è®¡ä¸­è‡³å…³é‡è¦ï¼Œä½†è¿™ä¸€è¿‡ç¨‹è€—æ—¶ä¸”éœ€è¦é«˜åº¦ä¸“ä¸šçš„é¢†åŸŸçŸ¥è¯†ã€‚è‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå›¾å½¢ç¼–è¾‘æ¶‰åŠå¤šç§ä»»åŠ¡ï¼Œæ¯ç§ä»»åŠ¡éƒ½éœ€è¦ä¸åŒçš„æŠ€èƒ½ã€‚æœ€è¿‘ï¼Œè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ¡†æ¶ï¼Œå¼€å§‹ç”¨äºè‡ªåŠ¨åŒ–ç¼–è¾‘è¿‡ç¨‹ï¼Œä½†ç¼ºä¹å…¨é¢çš„åŸºå‡†æµ‹è¯•é™åˆ¶äº†å…¶å¼€å‘å’Œè¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†BlenderGymï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç»¼åˆæ€§çš„VLMç³»ç»ŸåŸºå‡†ï¼Œç”¨äº3Då›¾å½¢ç¼–è¾‘ï¼Œè¯„ä¼°VLMç³»ç»Ÿåœ¨ä»£ç åŸºç¡€çš„3Dé‡å»ºä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.08635', 'title': 'Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging', 'url': 'https://huggingface.co/papers/2504.08635', 'abstract': 'This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE', 'score': 1, 'issue_id': 3224, 'pub_date': '2025-04-11', 'pub_date_card': {'ru': '11 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 11', 'zh': '4æœˆ11æ—¥'}, 'hash': '797102aac1993585', 'authors': ['Gabriele Lozupone', 'Alessandro Bria', 'Francesco Fontanella', 'Frederick J. A. Meijer', 'Claudio De Stefano', 'Henkjan Huisman'], 'affiliations': ['Department of Electrical and Information Engineering (DIEI), University of Cassino and Southern Lazio, Italy', 'Department of Medical Imaging, Radboud University Medical Center, Netherlands', 'Diagnostic Image Analysis Group, Radboud University Medical Center, Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.08635.jpg', 'data': {'categories': ['#science', '#inference', '#3d', '#healthcare', '#dataset', '#diffusion', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'LDAE: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ›Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞĞ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ (LDAE) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ñ Ñ„Ğ¾ĞºÑƒÑĞ¾Ğ¼ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ°. LDAE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸ Ğ² ÑĞ¶Ğ°Ñ‚Ğ¾Ğ¼ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ 3D Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ LDAE ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½ÑŒÑ ĞĞ»ÑŒÑ†Ğ³ĞµĞ¹Ğ¼ĞµÑ€Ğ° Ğ¸ ÑÑ‚Ğ°Ñ€ĞµĞ½Ğ¸ĞµĞ¼, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸, LDAE Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Medical Imaging with Latent Diffusion Autoencoder', 'desc': "This paper introduces the Latent Diffusion Autoencoder (LDAE), a new framework designed for efficient unsupervised learning in medical imaging, specifically targeting Alzheimer disease using brain MR images. LDAE innovatively applies the diffusion process in a compressed latent space rather than directly on images, which enhances computational efficiency and makes it easier to handle 3D medical data. The study validates LDAE's effectiveness by demonstrating its ability to capture meaningful representations related to Alzheimer and aging, achieving high-quality image generation and reconstruction. Experimental results show that LDAE significantly outperforms traditional methods in both speed and quality, making it a valuable tool for medical image analysis."}, 'zh': {'title': 'æ½œåœ¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼šåŒ»å­¦å½±åƒåˆ†æçš„æ–°çªç ´', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ½œåœ¨æ‰©æ•£è‡ªç¼–ç å™¨ï¼ˆLDAEï¼‰ï¼Œæ—¨åœ¨æé«˜åŒ»å­¦å½±åƒä¸­æ— ç›‘ç£å­¦ä¹ çš„æ•ˆç‡å’Œæ„ä¹‰ï¼Œç‰¹åˆ«å…³æ³¨é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆADï¼‰ã€‚ä¸ä¼ ç»Ÿçš„å›¾åƒç©ºé—´æ‰©æ•£è‡ªç¼–ç å™¨ä¸åŒï¼ŒLDAEåœ¨å‹ç¼©çš„æ½œåœ¨è¡¨ç¤ºä¸­åº”ç”¨æ‰©æ•£è¿‡ç¨‹ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡ï¼Œä½¿å¾—3DåŒ»å­¦å½±åƒçš„è¡¨ç¤ºå­¦ä¹ å˜å¾—å¯è¡Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLDAEèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸ADå’Œè¡°è€ç›¸å…³çš„3Dè„‘éƒ¨MRçš„æœ‰æ„ä¹‰è¯­ä¹‰è¡¨ç¤ºï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆå’Œé‡å»ºæ–¹é¢è¡¨ç°å‡ºé«˜è´¨é‡å’Œè®¡ç®—æ•ˆç‡ã€‚LDAEçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ‰©æ•£è‡ªç¼–ç å™¨ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†20å€ï¼ŒåŒæ—¶é‡å»ºè´¨é‡ä¹Ÿå¾—åˆ°äº†å¢å¼ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.06908', 'title': 'UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image\n  Segmentation', 'url': 'https://huggingface.co/papers/2504.06908', 'abstract': 'In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.', 'score': 0, 'issue_id': 3230, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 9', 'zh': '4æœˆ9æ—¥'}, 'hash': '43cceed4e0431425', 'authors': ['Emmanuelle Bourigault', 'Amir Jamaludin', 'Abdullah Hamdi'], 'affiliations': ['Visual Geometry Group, University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2504.06908.jpg', 'data': {'categories': ['#healthcare', '#open_source', '#synthetic', '#3d', '#benchmark', '#architecture', '#dataset', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UKBOB: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… 3D-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UKBOB - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ² Ñ‚ĞµĞ»Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ĞœĞ Ğ¢-ÑĞ½Ğ¸Ğ¼ĞºĞ°Ñ… Ğ¸Ğ· UK Biobank. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ETTA Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Swin-BOB Ğ´Ğ»Ñ 3D-ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ 3D-Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing Medical Imaging with UKBOB: A Giant Leap in Labeled Data', 'desc': 'This paper addresses the challenge of acquiring large-scale labeled data in medical imaging by introducing the UK Biobank Organs and Bones (UKBOB) dataset, which contains 51,761 MRI 3D samples and over 1.37 billion segmentation masks for 72 organs. The authors employ automatic labeling and a cleaning pipeline to ensure high-quality labels, while also validating the dataset with manual annotations of a subset of MRIs. They demonstrate the effectiveness of the dataset by achieving zero-shot generalization on other small labeled datasets and propose a new method, Entropy Test-time Adaptation (ETTA), to improve segmentation outputs. The foundation model, Swin-BOB, trained on UKBOB, sets new benchmarks in 3D medical image segmentation, showcasing significant improvements in established challenges.'}, 'zh': {'title': 'æ„å»ºå¤§è§„æ¨¡åŒ»å­¦å½±åƒæ•°æ®é›†çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'åœ¨åŒ»å­¦å½±åƒé¢†åŸŸï¼Œæ”¶é›†å¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é¢ä¸´éšç§ã€ç‰©æµå’Œé«˜æ ‡æ³¨æˆæœ¬ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†UK Biobank Organs and Bones (UKBOB)ï¼Œè¿™æ˜¯æœ€å¤§çš„èº«ä½“å™¨å®˜æ ‡æ³¨æ•°æ®é›†ï¼ŒåŒ…å«51,761ä¸ªMRI 3Dæ ·æœ¬å’Œè¶…è¿‡13.7äº¿ä¸ª2Dåˆ†å‰²æ©è†œã€‚æˆ‘ä»¬é‡‡ç”¨è‡ªåŠ¨æ ‡æ³¨å’Œè‡ªåŠ¨åŒ–æ ‡ç­¾æ¸…ç†æµç¨‹ï¼Œå¹¶æ‰‹åŠ¨æ ‡æ³¨300ä¸ªMRIä»¥éªŒè¯æ ‡ç­¾è´¨é‡ã€‚é€šè¿‡è®­ç»ƒåŸºç¡€æ¨¡å‹Swin-BOBï¼Œæˆ‘ä»¬åœ¨3DåŒ»å­¦å½±åƒåˆ†å‰²ä¸­å–å¾—äº†å¤šé¡¹åŸºå‡†æµ‹è¯•çš„æœ€æ–°æˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.05303', 'title': 'InteractVLM: 3D Interaction Reasoning from 2D Foundational Models', 'url': 'https://huggingface.co/papers/2504.05303', 'abstract': 'We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.', 'score': 0, 'issue_id': 3219, 'pub_date': '2025-04-07', 'pub_date_card': {'ru': '7 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 7', 'zh': '4æœˆ7æ—¥'}, 'hash': 'be4a2fea61b66424', 'authors': ['Sai Kumar Dwivedi', 'Dimitrije AntiÄ‡', 'Shashank Tripathi', 'Omid Taheri', 'Cordelia Schmid', 'Michael J. Black', 'Dimitrios Tzionas'], 'affiliations': ['Inria, Ecole normale superieure, CNRS, PSL Research University, France', 'Max Planck Institute for Intelligent Systems, Tubingen, Germany', 'University of Amsterdam (UvA), the Netherlands'], 'pdf_title_img': 'assets/pdf/title_img/2504.05303.jpg', 'data': {'categories': ['#cv', '#3d'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'InteractVLM: 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'InteractVLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ 3D-Ñ‚Ğ¾Ñ‡ĞµĞº ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ´ÑŒĞ¼Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾ĞºĞºĞ»ÑĞ·Ğ¸Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ´Ğ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Render-Localize-Lift Ğ´Ğ»Ñ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² 2D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ´ÑŠĞµĞ¼Ğ° Ğ² 3D. InteractVLM Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ² Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰ÑƒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Contact Estimation with InteractVLM', 'desc': 'InteractVLM is a new method designed to identify 3D contact points between humans and objects using just single images taken in everyday settings. It addresses challenges like occlusions and varying object shapes, which complicate accurate 3D reconstruction. Unlike previous methods that depend on costly 3D annotations, InteractVLM leverages the capabilities of Vision-Language Models (VLMs) fine-tuned with minimal 3D data. The approach includes a unique Render-Localize-Lift module that converts 3D surfaces into 2D, infers contacts in 2D, and then translates these findings back into 3D, enhancing the understanding of human-object interactions.'}, 'zh': {'title': 'InteractVLMï¼šä»å›¾åƒä¸­å®ç°3Dæ¥è§¦ç‚¹ä¼°è®¡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'InteractVLMæ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•å¼ è‡ªç„¶åœºæ™¯å›¾åƒä¸­ä¼°è®¡äººä½“å’Œç‰©ä½“çš„3Dæ¥è§¦ç‚¹ï¼Œä»è€Œå®ç°å‡†ç¡®çš„äººä½“-ç‰©ä½“è”åˆ3Dé‡å»ºã€‚è¿™é¡¹ä»»åŠ¡é¢ä¸´é®æŒ¡ã€æ·±åº¦æ¨¡ç³Šå’Œç‰©ä½“å½¢çŠ¶å¤šæ ·æ€§ç­‰æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæ˜‚è´µçš„è¿åŠ¨æ•æ‰ç³»ç»Ÿæˆ–ç¹ççš„æ‰‹åŠ¨æ ‡æ³¨æ¥æ”¶é›†3Dæ¥è§¦æ³¨é‡Šï¼Œé™åˆ¶äº†å…¶å¯æ‰©å±•æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚InteractVLMåˆ©ç”¨å¤§å‹è§†è§‰-è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è§†è§‰çŸ¥è¯†ï¼Œå¹¶é€šè¿‡æœ‰é™çš„3Dæ¥è§¦æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (5)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (8)', '#audio', '#benchmark (10)', '#cv (9)', '#data (1)', '#dataset (4)', '#diffusion (7)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (4)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (2)', '#training (9)', '#transfer_learning (2)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-14 22:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-14 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-14 22:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    