
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. January 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 января</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-01-22.html">⬅️ <span id="prev-date">22.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-01-24.html">➡️ <span id="next-date">24.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-01.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 января', 'en': 'January 23', 'zh': '1月23日'};
        let feedDateNext = {'ru': '24.01', 'en': '01/24', 'zh': '1月24日'};
        let feedDatePrev = {'ru': '22.01', 'en': '01/22', 'zh': '1月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2501.11425', 'title': 'Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training', 'url': 'https://huggingface.co/papers/2501.11425', 'abstract': "Large Language Models (LLMs) agents are increasingly pivotal for addressing complex tasks in interactive environments. Existing work mainly focuses on enhancing performance through behavior cloning from stronger experts, yet such approaches often falter in real-world applications, mainly due to the inability to recover from errors. However, step-level critique data is difficult and expensive to collect. Automating and dynamically constructing self-critique datasets is thus crucial to empowering models with intelligent agent capabilities. In this work, we propose an iterative self-training framework, Agent-R, that enables language Agent to Reflect on the fly. Unlike traditional methods that reward or penalize actions based on correctness, Agent-R leverages MCTS to construct training data that recover correct trajectories from erroneous ones. A key challenge of agent reflection lies in the necessity for timely revision rather than waiting until the end of a rollout. To address this, we introduce a model-guided critique construction mechanism: the actor model identifies the first error step (within its current capability) in a failed trajectory. Starting from it, we splice it with the adjacent correct path, which shares the same parent node in the tree. This strategy enables the model to learn reflection based on its current policy, therefore yielding better learning efficiency. To further explore the scalability of this self-improvement paradigm, we investigate iterative refinement of both error correction capabilities and dataset construction. Our findings demonstrate that Agent-R continuously improves the model's ability to recover from errors and enables timely error correction. Experiments on three interactive environments show that Agent-R effectively equips agents to correct erroneous actions while avoiding loops, achieving superior performance compared to baseline methods (+5.59%).", 'score': 61, 'issue_id': 1798, 'pub_date': '2025-01-20', 'pub_date_card': {'ru': '20 января', 'en': 'January 20', 'zh': '1月20日'}, 'hash': '96d073b4606b0493', 'authors': ['Siyu Yuan', 'Zehui Chen', 'Zhiheng Xi', 'Junjie Ye', 'Zhengyin Du', 'Jiecao Chen'], 'affiliations': ['ByteDance', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2501.11425.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#agents', '#training', '#agi'], 'emoji': '🤖', 'ru': {'title': 'Самообучающиеся ИИ-агенты: исправление ошибок на лету', 'desc': 'Статья представляет новый метод обучения языковых агентов на основе искусственного интеллекта под названием Agent-R. Этот подход использует самообучение и самокритику для улучшения способности модели исправлять ошибки в процессе выполнения задач. Agent-R применяет метод Монте-Карло для построения дерева поиска (MCTS) для создания обучающих данных, которые помогают агенту восстанавливаться после ошибочных действий. Эксперименты показывают, что Agent-R значительно повышает производительность агентов в интерактивных средах по сравнению с базовыми методами.'}, 'en': {'title': 'Empowering Language Agents with Real-Time Self-Critique', 'desc': "This paper introduces Agent-R, an iterative self-training framework designed to enhance the performance of Large Language Models (LLMs) in interactive environments. Unlike traditional methods that rely on static feedback, Agent-R utilizes Monte Carlo Tree Search (MCTS) to dynamically create training data that helps models recover from mistakes in real-time. The framework focuses on timely error correction by identifying the first error in a trajectory and splicing it with a correct path, allowing the model to learn from its current policy. Experimental results show that Agent-R significantly improves the model's error recovery capabilities and overall performance, outperforming baseline methods by 5.59%."}, 'zh': {'title': 'Agent-R：实时反思，提升学习效率', 'desc': '大型语言模型（LLMs）在复杂任务的交互环境中变得越来越重要。现有研究主要通过模仿更强专家的行为来提升性能，但这种方法在实际应用中常常失败，主要是因为无法从错误中恢复。为了解决这个问题，我们提出了一种迭代自我训练框架Agent-R，使语言代理能够实时反思。Agent-R通过构建训练数据来纠正错误轨迹，从而提高模型的学习效率和错误恢复能力。'}}}, {'id': 'https://huggingface.co/papers/2501.12380', 'title': 'MMVU: Measuring Expert-Level Multi-Discipline Video Understanding', 'url': 'https://huggingface.co/papers/2501.12380', 'abstract': 'We introduce MMVU, a comprehensive expert-level, multi-discipline benchmark for evaluating foundation models in video understanding. MMVU includes 3,000 expert-annotated questions spanning 27 subjects across four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. Compared to prior benchmarks, MMVU features three key advancements. First, it challenges models to apply domain-specific knowledge and perform expert-level reasoning to analyze specialized-domain videos, moving beyond the basic visual perception typically assessed in current video benchmarks. Second, each example is annotated by human experts from scratch. We implement strict data quality controls to ensure the high quality of the dataset. Finally, each example is enriched with expert-annotated reasoning rationals and relevant domain knowledge, facilitating in-depth analysis. We conduct an extensive evaluation of 32 frontier multimodal foundation models on MMVU. The latest System-2-capable models, o1 and Gemini 2.0 Flash Thinking, achieve the highest performance among the tested models. However, they still fall short of matching human expertise. Through in-depth error analyses and case studies, we offer actionable insights for future advancements in expert-level, knowledge-intensive video understanding for specialized domains.', 'score': 59, 'issue_id': 1797, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': 'dcb04aaca349cc32', 'authors': ['Yilun Zhao', 'Lujing Xie', 'Haowei Zhang', 'Guo Gan', 'Yitao Long', 'Zhiyuan Hu', 'Tongyan Hu', 'Weiyuan Chen', 'Chuhan Li', 'Junyang Song', 'Zhijian Xu', 'Chengye Wang', 'Weifeng Pan', 'Ziyao Shangguan', 'Xiangru Tang', 'Zhenwen Liang', 'Yixin Liu', 'Chen Zhao', 'Arman Cohan'], 'affiliations': ['Yale NLP'], 'pdf_title_img': 'assets/pdf/title_img/2501.12380.jpg', 'data': {'categories': ['#multimodal', '#science', '#benchmark', '#video', '#healthcare', '#reasoning'], 'emoji': '🎓', 'ru': {'title': 'Новый рубеж в понимании видео: от базового восприятия к экспертному анализу', 'desc': 'Статья представляет MMVU - многодисциплинарный экспертный бенчмарк для оценки фундаментальных моделей в понимании видео. MMVU включает 3000 вопросов по 27 предметам в четырех основных дисциплинах, требующих применения специализированных знаний и экспертного анализа. Бенчмарк отличается высоким качеством данных, аннотированных экспертами, и включает обоснования и релевантные знания для каждого примера. Оценка 32 мультимодальных моделей на MMVU показала, что даже лучшие модели пока не достигают уровня человека-эксперта в этой задаче.'}, 'en': {'title': 'MMVU: Elevating Video Understanding to Expert Levels', 'desc': 'The paper presents MMVU, a new benchmark designed to evaluate foundation models specifically in video understanding across various expert domains. It includes 3,000 questions that require advanced reasoning and domain-specific knowledge, moving beyond simple visual recognition tasks. Each question is meticulously annotated by human experts, ensuring high data quality and providing reasoning rationales to enhance analysis. The evaluation of 32 advanced multimodal models reveals that while some perform well, they still do not reach the level of human expertise, highlighting areas for future improvement in this field.'}, 'zh': {'title': 'MMVU：视频理解的新标准', 'desc': '我们介绍了MMVU，这是一个全面的专家级多学科基准，用于评估基础模型在视频理解方面的表现。MMVU包含3000个专家注释的问题，涵盖科学、医疗、人文学科与社会科学和工程四个核心学科。与之前的基准相比，MMVU在三个关键方面有所改进，包括要求模型应用领域特定知识进行专家级推理，确保数据集的高质量，以及为每个示例提供专家注释的推理依据和相关领域知识。我们对32个前沿多模态基础模型在MMVU上的表现进行了广泛评估，发现最新的系统2能力模型o1和Gemini 2.0 Flash Thinking在测试模型中表现最佳，但仍未能达到人类专家的水平。'}}}, {'id': 'https://huggingface.co/papers/2501.11873', 'title': 'Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models', 'url': 'https://huggingface.co/papers/2501.11873', 'abstract': 'This paper revisits the implementation of Load-balancing Loss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as N_E sum_{i=1}^{N_E} f_i p_i, where N_E is the total number of experts, f_i represents the frequency of expert i being selected, and p_i denotes the average gating score of the expert i. Existing MoE training frameworks usually employ the parallel training strategy so that f_i and the LBL are calculated within a micro-batch and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence (e.g., code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a global-batch to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize f_i across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to 42.8B total parameters and 400B tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.', 'score': 51, 'issue_id': 1797, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '370d057fec504963', 'authors': ['Zihan Qiu', 'Zeyu Huang', 'Bo Zheng', 'Kaiyue Wen', 'Zekun Wang', 'Rui Men', 'Ivan Titov', 'Dayiheng Liu', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Qwen Team, Alibaba Group', 'Stanford University', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2501.11873.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': '⚖️', 'ru': {'title': 'Глобальный подход к балансировке нагрузки экспертов в MoE моделях', 'desc': 'Статья предлагает новый подход к реализации функции потерь балансировки нагрузки (LBL) при обучении моделей Mixture-of-Experts (MoE). Авторы предлагают вычислять LBL на уровне глобального батча, а не микро-батча, что позволяет ослабить ограничения на распределение токенов между экспертами. Эксперименты на крупномасштабных языковых моделях показывают, что этот метод улучшает перплексию при предобучении и результаты на задачах downstream. Анализ также демонстрирует улучшение специализации экспертов по доменам.'}, 'en': {'title': 'Enhancing Expert Specialization with Global-Batch Load-Balancing', 'desc': 'This paper focuses on improving the Load-balancing Loss (LBL) in training Mixture-of-Experts (MoEs) models. The authors highlight that traditional methods use micro-batches, which limit the diversity of sequences and hinder expert specialization. They propose a new approach that utilizes global-batches, allowing for a broader range of sequences and better load balancing across the entire dataset. Experimental results show that this global-batch LBL method significantly enhances model performance and expert specialization in large language models.'}, 'zh': {'title': '全局批次提升混合专家模型的负载均衡与专业化', 'desc': '本文重新审视了在训练混合专家模型（MoEs）时的负载均衡损失（LBL）实现。我们提出使用全局批次来计算LBL，以打破微批次的严格约束，从而在语料库层面上促进负载均衡。通过在训练中引入额外的通信步骤来同步专家选择频率，实验结果显示全局批次LBL策略在预训练困惑度和下游任务中均显著提升了性能。我们的分析表明，全局批次LBL还大大改善了MoE专家的领域专业化。'}}}, {'id': 'https://huggingface.co/papers/2501.12224', 'title': 'TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space', 'url': 'https://huggingface.co/papers/2501.12224', 'abstract': "We present TokenVerse -- a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods. project's webpage in https://token-verse.github.io/", 'score': 32, 'issue_id': 1804, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '20dcd865e2d7bc5c', 'authors': ['Daniel Garibi', 'Shahar Yadin', 'Roni Paiss', 'Omer Tov', 'Shiran Zada', 'Ariel Ephrat', 'Tomer Michaeli', 'Inbar Mosseri', 'Tali Dekel'], 'affiliations': ['Google DeepMind', 'Technion', 'Tel Aviv University', 'Weizmann Institute'], 'pdf_title_img': 'assets/pdf/title_img/2501.12224.jpg', 'data': {'categories': ['#multimodal', '#cv', '#optimization', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Персонализация изображений с помощью семантического пространства модуляции', 'desc': 'TokenVerse - это метод многоконцептуальной персонализации, использующий предобученную модель диффузии текста в изображение. Он позволяет выделять сложные визуальные элементы и атрибуты даже из одного изображения, обеспечивая при этом возможность комбинировать концепты из нескольких изображений. TokenVerse использует модель DiT, где входной текст влияет на генерацию через внимание и модуляцию. Метод оптимизирует направления в пространстве модуляции для каждого слова, что позволяет генерировать новые изображения с желаемой комбинацией выученных концептов.'}, 'en': {'title': 'TokenVerse: Mastering Multi-Concept Image Personalization', 'desc': 'TokenVerse is a novel approach for personalizing images by using a pre-trained text-to-image diffusion model. It can separate and manipulate various visual elements from just one image, allowing for the creation of new images that combine concepts from multiple sources. Unlike previous methods, TokenVerse effectively manages multiple images with different concepts, covering a wide array of attributes such as objects, poses, and lighting. The framework utilizes a DiT-based model that enables precise control over image generation through semantic modulation, making it a powerful tool for complex personalization tasks.'}, 'zh': {'title': 'TokenVerse：多概念个性化的新方法', 'desc': 'TokenVerse是一种多概念个性化的方法，利用预训练的文本到图像扩散模型。该框架能够从单张图像中解耦复杂的视觉元素和属性，并支持从多张图像中提取概念的无缝组合生成。与现有方法不同，TokenVerse可以处理每张图像中包含多个概念的情况，并支持广泛的概念类型，包括物体、配件、材料、姿势和光照。我们的研究利用基于DiT的文本到图像模型，通过注意力和调制（偏移和缩放）来影响生成过程，从而实现对复杂概念的局部控制。'}}}, {'id': 'https://huggingface.co/papers/2501.12326', 'title': 'UI-TARS: Pioneering Automated GUI Interaction with Native Agents', 'url': 'https://huggingface.co/papers/2501.12326', 'abstract': 'This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.', 'score': 30, 'issue_id': 1797, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '1f98d8f49b073983', 'authors': ['Yujia Qin', 'Yining Ye', 'Junjie Fang', 'Haoming Wang', 'Shihao Liang', 'Shizuo Tian', 'Junda Zhang', 'Jiahao Li', 'Yunxin Li', 'Shijue Huang', 'Wanjun Zhong', 'Kuanye Li', 'Jiale Yang', 'Yu Miao', 'Woyu Lin', 'Longxiang Liu', 'Xu Jiang', 'Qianli Ma', 'Jingyu Li', 'Xiaojun Xiao', 'Kai Cai', 'Chuang Li', 'Yaowei Zheng', 'Chaolin Jin', 'Chen Li', 'Xiao Zhou', 'Minchao Wang', 'Haoli Chen', 'Zhaojian Li', 'Haihua Yang', 'Haifeng Liu', 'Feng Lin', 'Tao Peng', 'Xin Liu', 'Guang Shi'], 'affiliations': ['ByteDance Seed', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2501.12326.jpg', 'data': {'categories': ['#optimization', '#dataset', '#agents', '#training', '#reasoning'], 'emoji': '🖥️', 'ru': {'title': 'UI-TARS: Революция в мире GUI-агентов', 'desc': 'Статья представляет UI-TARS - модель графического агента, которая воспринимает только скриншоты и выполняет операции, подобные человеческим. UI-TARS превосходит существующие фреймворки агентов, достигая лучших результатов в более чем 10 бенчмарках для GUI-агентов. Модель включает в себя несколько ключевых инноваций: улучшенное восприятие, унифицированное моделирование действий, рассуждение по системе-2 и итеративное обучение с рефлексивными онлайн-трассами. UI-TARS постоянно учится на своих ошибках и адаптируется к непредвиденным ситуациям с минимальным вмешательством человека.'}, 'en': {'title': 'Revolutionizing GUI Interaction with UI-TARS: The End-to-End Agent Model', 'desc': 'UI-TARS is a novel GUI agent model that processes screenshots to perform tasks like a human would, using keyboard and mouse actions. Unlike existing models that rely on complex commercial frameworks and pre-defined prompts, UI-TARS operates end-to-end and shows superior performance in various benchmarks. It achieves state-of-the-art results in GUI task execution by utilizing enhanced perception, unified action modeling, and system-2 reasoning for better decision-making. Additionally, its iterative training approach allows it to learn from past interactions, improving its adaptability with minimal human input.'}, 'zh': {'title': 'UI-TARS：革新图形用户界面代理的全新模型', 'desc': '本文介绍了UI-TARS，这是一种原生的图形用户界面（GUI）代理模型，能够仅通过屏幕截图进行人类般的交互。与依赖复杂商业模型的现有代理框架不同，UI-TARS是一个端到端的模型，在多个GUI代理基准测试中表现优异，尤其在感知、定位和任务执行方面。UI-TARS通过增强感知、统一动作建模、系统-2推理和反思在线追踪等创新，显著提高了其性能。通过迭代训练和反思调优，UI-TARS能够不断学习并适应新的情况，减少对人类干预的需求。'}}}, {'id': 'https://huggingface.co/papers/2501.12368', 'title': 'InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model', 'url': 'https://huggingface.co/papers/2501.12368', 'abstract': 'Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at https://github.com/InternLM/InternLM-XComposer', 'score': 23, 'issue_id': 1804, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': 'd51d195276c2215d', 'authors': ['Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Ziyu Liu', 'Shengyuan Ding', 'Shenxi Wu', 'Yubo Ma', 'Haodong Duan', 'Wenwei Zhang', 'Kai Chen', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Fudan University', 'Nanjing University', 'Nanyang Technological University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.12368.jpg', 'data': {'categories': ['#rlhf', '#alignment', '#open_source', '#benchmark', '#training', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Улучшение LVLM с помощью мультимодальной модели вознаграждения', 'desc': 'В статье представлена мультимодальная модель вознаграждения InternLM-XComposer2.5-Reward (IXC-2.5-Reward) для улучшения качества генерации больших визуально-языковых моделей (LVLM). Модель обучена на высококачественном наборе данных, охватывающем различные домены и типы входных данных. IXC-2.5-Reward показывает отличные результаты на бенчмарках мультимодальных и текстовых моделей вознаграждения. Авторы демонстрируют три ключевых применения модели: обучение с подкреплением, выбор лучшего ответа из кандидатов и фильтрация шумных данных.'}, 'en': {'title': 'Bridging the Gap in Multi-Modal Reward Models for LVLMs', 'desc': 'This paper introduces InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a multi-modal reward model designed to enhance the performance of Large Vision Language Models (LVLMs) by aligning them with human preferences. The authors address the lack of publicly available multi-modal reward models by creating a comprehensive preference corpus that includes text, images, and videos across various domains. IXC-2.5-Reward demonstrates strong performance on multi-modal benchmarks and effectively supports reinforcement learning training, response selection, and data filtering. The model and its training methods are open-sourced to promote reproducibility and further research in the field.'}, 'zh': {'title': '提升视觉语言模型生成质量的多模态奖励模型', 'desc': '本文介绍了一种新的多模态奖励模型，名为InternLM-XComposer2.5-Reward（IXC-2.5-Reward），旨在提高大型视觉语言模型（LVLMs）的生成质量。该模型通过对文本、图像和视频等多种输入形式进行高质量的偏好学习，来对齐LVLMs与人类的偏好。IXC-2.5-Reward在最新的多模态奖励模型基准测试中表现优异，并在文本奖励模型基准测试中也展现了竞争力。我们还展示了IXC-2.5-Reward的三种关键应用，包括强化学习训练的监督信号、候选响应的最佳选择以及过滤噪声样本。'}}}, {'id': 'https://huggingface.co/papers/2501.11733', 'title': 'Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks', 'url': 'https://huggingface.co/papers/2501.11733', 'abstract': 'Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.', 'score': 19, 'issue_id': 1798, 'pub_date': '2025-01-20', 'pub_date_card': {'ru': '20 января', 'en': 'January 20', 'zh': '1月20日'}, 'hash': 'a9cddb8786536def', 'authors': ['Zhenhailong Wang', 'Haiyang Xu', 'Junyang Wang', 'Xi Zhang', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Heng Ji'], 'affiliations': ['Alibaba Group', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2501.11733.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#optimization', '#agents', '#multimodal', '#long_context'], 'emoji': '📱', 'ru': {'title': 'Мобильный ИИ-ассистент с самообучением для сложных задач', 'desc': 'Статья представляет Mobile-Agent-E - иерархическую мультиагентную систему для выполнения сложных задач на мобильных устройствах. Система включает Менеджера для планирования и четыре подчиненных агента для восприятия, выполнения действий, проверки ошибок и агрегации информации. Ключевой особенностью является модуль самоэволюции с долговременной памятью, содержащей Подсказки и Ярлыки для улучшения производительности. Эмпирические результаты показывают значительное улучшение по сравнению с предыдущими подходами на новом бенчмарке Mobile-Eval-E.'}, 'en': {'title': 'Empowering Mobile Agents with Self-Evolution for Enhanced Task Performance', 'desc': 'This paper presents Mobile-Agent-E, a hierarchical multi-agent framework designed to enhance mobile task performance by learning from past experiences. The framework separates high-level planning from low-level execution, utilizing a Manager for task decomposition and four specialized agents for perception, action, error checking, and information management. A key feature is the self-evolution module, which incorporates a long-term memory of Tips and Shortcuts to improve task efficiency and effectiveness. Experimental results demonstrate that Mobile-Agent-E significantly outperforms existing methods, achieving a 22% improvement in complex mobile tasks.'}, 'zh': {'title': '智能手机任务执行的新突破', 'desc': '本论文介绍了一种名为Mobile-Agent-E的层次化多智能体框架，旨在提升智能手机上的任务执行能力。该框架通过将高层规划与低层执行明确分离，包含一个管理者和四个子代理，分别负责视觉感知、动作执行、错误验证和信息聚合。Mobile-Agent-E还引入了自我进化模块，利用长期记忆中的提示和捷径来不断优化性能。实验结果表明，该框架在复杂移动任务中相较于现有方法有22%的绝对提升。'}}}, {'id': 'https://huggingface.co/papers/2501.11223', 'title': 'Reasoning Language Models: A Blueprint', 'url': 'https://huggingface.co/papers/2501.11223', 'abstract': 'Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI\'s o1 and o3, DeepSeek-V3, and Alibaba\'s QwQ, have redefined AI\'s problem-solving capabilities by extending large language models (LLMs) with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining Reinforcement Learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), and supervision schemes (Output-Based and Process-Based Supervision). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint\'s versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we outline how RLMs can integrate with a broader LLM ecosystem, including tools and databases. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between "rich AI" and "poor AI" by lowering barriers to RLM development and experimentation.', 'score': 17, 'issue_id': 1797, 'pub_date': '2025-01-20', 'pub_date_card': {'ru': '20 января', 'en': 'January 20', 'zh': '1月20日'}, 'hash': 'f554416ad9af3344', 'authors': ['Maciej Besta', 'Julia Barth', 'Eric Schreiber', 'Ales Kubicek', 'Afonso Catarino', 'Robert Gerstenberger', 'Piotr Nyczyk', 'Patrick Iff', 'Yueling Li', 'Sam Houliston', 'Tomasz Sternal', 'Marcin Copik', 'Grzegorz Kwaśniewski', 'Jürgen Müller', 'Łukasz Flis', 'Hannes Eberhard', 'Hubert Niewiadomski', 'Torsten Hoefler'], 'affiliations': ['BASF SE', 'Cledar', 'Cyfronet AGH', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2501.11223.jpg', 'data': {'categories': ['#rl', '#math', '#training', '#survey', '#reasoning', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Демократизация искусственного интеллекта: модульный подход к созданию моделей рассуждений', 'desc': 'Статья представляет комплексный подход к созданию моделей рассуждений (RLM), объединяющих языковые модели с механизмами продвинутых рассуждений. Авторы предлагают модульную структуру, включающую различные стратегии рассуждений, концепции обучения с подкреплением и схемы обучения. Они демонстрируют применимость этой структуры на примере существующих моделей и представляют x1 - модульную реализацию для быстрого прототипирования RLM. Исследование направлено на демократизацию возможностей продвинутых рассуждений в ИИ и снижение барьеров для разработки RLM.'}, 'en': {'title': 'Democratizing Advanced Reasoning in AI', 'desc': 'This paper introduces a modular framework for Reasoning Language Models (RLMs), which enhance traditional Large Language Models (LLMs) with advanced reasoning capabilities. The authors address the challenges of high costs and complex architectures by organizing RLM components into a comprehensive blueprint that includes various reasoning structures and strategies. They provide mathematical formulations and algorithmic specifications to facilitate easier implementation of RLMs. Additionally, the paper presents x1, a tool for rapid prototyping, and discusses how RLMs can be integrated into the larger LLM ecosystem to promote accessibility and innovation in AI development.'}, 'zh': {'title': '简化推理语言模型，促进AI创新', 'desc': '推理语言模型（RLMs）通过结合强化学习、搜索启发式和大型语言模型（LLMs），重新定义了人工智能的解决问题能力。尽管它们具有强大的推理机制，但高成本和复杂架构使得其可访问性和可扩展性面临挑战。为了解决这些问题，我们提出了一个模块化框架，组织RLM组件，并提供详细的数学公式和算法规范，以简化RLM的实现。我们的工作旨在降低RLM开发和实验的门槛，促进创新，缩小“富有AI”和“贫穷AI”之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2501.12202', 'title': 'Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation', 'url': 'https://huggingface.co/papers/2501.12202', 'abstract': 'We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets. This system includes two foundation components: a large-scale shape generation model -- Hunyuan3D-DiT, and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly aligns with a given condition image, laying a solid foundation for downstream applications. The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant texture maps for either generated or hand-crafted meshes. Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production platform that simplifies the re-creation process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models, including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps in the open-source 3D community for large-scale foundation generative models. The code and pre-trained weights of our models are available at: https://github.com/Tencent/Hunyuan3D-2', 'score': 15, 'issue_id': 1798, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': 'f95f069cba0bd83e', 'authors': ['Zibo Zhao', 'Zeqiang Lai', 'Qingxiang Lin', 'Yunfei Zhao', 'Haolin Liu', 'Shuhui Yang', 'Yifei Feng', 'Mingxin Yang', 'Sheng Zhang', 'Xianghui Yang', 'Huiwen Shi', 'Sicong Liu', 'Junta Wu', 'Yihang Lian', 'Fan Yang', 'Ruining Tang', 'Zebin He', 'Xinzhou Wang', 'Jian Liu', 'Xuhui Zuo', 'Zhuo Chen', 'Biwen Lei', 'Haohan Weng', 'Jing Xu', 'Yiling Zhu', 'Xinhai Liu', 'Lixin Xu', 'Changrong Hu', 'Tianyu Huang', 'Lifu Wang', 'Jihong Zhang', 'Meng Chen', 'Liang Dong', 'Yiwen Jia', 'Yulin Cai', 'Jiaao Yu', 'Yixuan Tang', 'Hao Zhang', 'Zheng Ye', 'Peng He', 'Runzhou Wu', 'Chao Zhang', 'Yonghao Tan', 'Jie Xiao', 'Yangyu Tao', 'Jianchen Zhu', 'Jinbao Xue', 'Kai Liu', 'Chongqing Zhao', 'Xinming Wu', 'Zhichao Hu', 'Lei Qin', 'Jianbing Peng', 'Zhan Li', 'Minghui Chen', 'Xipeng Zhang', 'Lin Niu', 'Paige Wang', 'Yingkai Wang', 'Haozhao Kuang', 'Zhongyi Fan', 'Xu Zheng', 'Weihao Zhuang', 'YingPing He', 'Tian Liu', 'Yong Yang', 'Di Wang', 'Yuhong Liu', 'Jie Jiang', 'Jingwei Huang', 'Chunchao Guo'], 'affiliations': ['Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2501.12202.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Революция в 3D-генерации: от формы к текстуре', 'desc': 'Hunyuan3D 2.0 - это продвинутая система для создания трехмерных текстурированных объектов высокого разрешения. Она состоит из двух основных компонентов: модели генерации форм Hunyuan3D-DiT и модели синтеза текстур Hunyuan3D-Paint. Модель генерации форм основана на масштабируемом диффузионном трансформере и создает геометрию, соответствующую заданному изображению. Модель синтеза текстур, используя геометрические и диффузионные праймы, создает высококачественные текстурные карты для сгенерированных или созданных вручную мешей.'}, 'en': {'title': 'Revolutionizing 3D Asset Creation with Hunyuan3D 2.0', 'desc': 'Hunyuan3D 2.0 is a sophisticated system designed for creating high-quality 3D models with detailed textures. It consists of two main components: Hunyuan3D-DiT for generating 3D shapes and Hunyuan3D-Paint for applying textures. The shape model uses a flow-based diffusion transformer to ensure that the generated geometry matches the input conditions, while the texture model leverages geometric and diffusion principles to create vibrant textures. This system not only enhances the quality of 3D assets but also provides an accessible platform for users to create and animate their models easily.'}, 'zh': {'title': 'Hunyuan3D 2.0：高效生成高质量3D资产的系统', 'desc': 'Hunyuan3D 2.0 是一个先进的大规模 3D 合成系统，能够生成高分辨率的纹理 3D 资产。该系统包含两个基础组件：Hunyuan3D-DiT 形状生成模型和 Hunyuan3D-Paint 纹理合成模型。形状生成模型基于可扩展的流式扩散变换器，旨在创建与给定条件图像相匹配的几何形状。纹理合成模型则利用强大的几何和扩散先验，为生成或手工制作的网格生成高分辨率的生动纹理图。'}}}, {'id': 'https://huggingface.co/papers/2501.12375', 'title': 'Video Depth Anything: Consistent Depth Estimation for Super-Long Videos', 'url': 'https://huggingface.co/papers/2501.12375', 'abstract': 'Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.', 'score': 14, 'issue_id': 1798, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '00640fb6adcf39e3', 'authors': ['Sili Chen', 'Hengkai Guo', 'Shengnan Zhu', 'Feihu Zhang', 'Zilong Huang', 'Jiashi Feng', 'Bingyi Kang'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2501.12375.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#small_models', '#video', '#cv', '#training'], 'emoji': '🎥', 'ru': {'title': 'Согласованная оценка глубины для сверхдлинных видео', 'desc': 'В статье представлен метод Video Depth Anything для оценки глубины в сверхдлинных видео с высоким качеством и временной согласованностью. Модель основана на Depth Anything V2 с новой пространственно-временной головой и использует эффективную функцию потерь для обеспечения временной согласованности. Предложенный подход позволяет обрабатывать видео произвольной длительности без ущерба для качества и обобщающей способности. Метод достигает наилучших результатов в задаче zero-shot оценки глубины видео на нескольких бенчмарках.'}, 'en': {'title': 'Achieving Consistent Depth Estimation in Long Videos', 'desc': 'This paper introduces Video Depth Anything, a model designed for accurate depth estimation in long videos, overcoming the limitations of previous methods that struggled with temporal consistency. The model builds on Depth Anything V2, enhancing it with a spatial-temporal head and a novel temporal consistency loss that focuses on the depth gradient over time. By training on a combined dataset of video depth and unlabeled images, the model achieves high-quality depth estimation without the need for complex geometric priors. The results demonstrate that Video Depth Anything can handle videos of any length while maintaining efficiency and setting new benchmarks in zero-shot video depth estimation.'}, 'zh': {'title': '超长视频深度估计的新突破', 'desc': '本文提出了一种名为Video Depth Anything的新模型，旨在解决单目深度估计在视频中的时间一致性问题。该模型能够在超长视频（超过几分钟）中实现高质量和一致性的深度估计，而不牺牲计算效率。我们通过设计一个简单有效的时间一致性损失，来约束时间深度梯度，从而避免了额外几何先验的需求。实验结果表明，该模型在多个视频基准测试中表现出色，设定了零-shot视频深度估计的新状态。'}}}, {'id': 'https://huggingface.co/papers/2501.10893', 'title': 'Learn-by-interact: A Data-Centric Framework for Self-Adaptive Agents in Realistic Environments', 'url': 'https://huggingface.co/papers/2501.10893', 'abstract': 'Autonomous agents powered by large language models (LLMs) have the potential to enhance human capabilities, assisting with digital tasks from sending emails to performing data analysis. The abilities of existing LLMs at such tasks are often hindered by the lack of high-quality agent data from the corresponding environments they interact with. We propose Learn-by-interact, a data-centric framework to adapt LLM agents to any given environments without human annotations. Learn-by-interact synthesizes trajectories of agent-environment interactions based on documentations, and constructs instructions by summarizing or abstracting the interaction histories, a process called backward construction. We assess the quality of our synthetic data by using them in both training-based scenarios and training-free in-context learning (ICL), where we craft innovative retrieval approaches optimized for agents. Extensive experiments on SWE-bench, WebArena, OSWorld and Spider2-V spanning across realistic coding, web, and desktop environments show the effectiveness of Learn-by-interact in various downstream agentic tasks -- baseline results are improved by up to 12.2\\% for ICL with Claude-3.5 and 19.5\\% for training with Codestral-22B. We further demonstrate the critical role of backward construction, which provides up to 14.0\\% improvement for training. Our ablation studies demonstrate the efficiency provided by our synthesized data in ICL and the superiority of our retrieval pipeline over alternative approaches like conventional retrieval-augmented generation (RAG). We expect that Learn-by-interact will serve as a foundation for agent data synthesis as LLMs are increasingly deployed at real-world environments.', 'score': 14, 'issue_id': 1798, 'pub_date': '2025-01-18', 'pub_date_card': {'ru': '18 января', 'en': 'January 18', 'zh': '1月18日'}, 'hash': 'b6ab4c9ac3809941', 'authors': ['Hongjin Su', 'Ruoxi Sun', 'Jinsung Yoon', 'Pengcheng Yin', 'Tao Yu', 'Sercan Ö. Arık'], 'affiliations': ['Google', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2501.10893.jpg', 'data': {'categories': ['#optimization', '#agents', '#synthetic', '#training', '#data', '#rag', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Обучение ИИ-агентов через синтетическое взаимодействие', 'desc': 'Статья представляет Learn-by-interact - фреймворк для адаптации агентов на основе больших языковых моделей (LLM) к различным средам без аннотаций человека. Метод синтезирует траектории взаимодействия агента со средой на основе документации и создает инструкции путем обобщения истории взаимодействий. Эксперименты показывают эффективность подхода в различных задачах, улучшая базовые результаты до 19.5% при обучении. Авторы демонстрируют критическую роль обратного конструирования и превосходство их метода над альтернативными подходами.'}, 'en': {'title': 'Empowering LLM Agents through Synthetic Interaction Data', 'desc': "This paper introduces Learn-by-interact, a framework designed to enhance the performance of large language model (LLM) agents in various environments without needing human-generated data. The framework generates synthetic data by simulating interactions between agents and their environments, using documentation to guide the process. A key innovation is the backward construction method, which summarizes interaction histories to create effective instructions for the agents. Experimental results show significant improvements in agent performance across multiple tasks, highlighting the framework's potential for real-world applications."}, 'zh': {'title': '通过交互学习，提升智能代理能力', 'desc': '本文提出了一种名为Learn-by-interact的数据中心框架，旨在使大型语言模型（LLMs）能够适应不同的环境，而无需人工标注。该框架通过文档生成代理与环境交互的轨迹，并通过总结或抽象交互历史来构建指令，这一过程称为反向构建。实验结果表明，Learn-by-interact在多种下游任务中显著提高了性能，尤其是在无监督学习和训练场景中。我们还展示了反向构建在训练中的重要性，进一步验证了合成数据的有效性和检索管道的优越性。'}}}, {'id': 'https://huggingface.co/papers/2501.08331', 'title': 'Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise', 'url': 'https://huggingface.co/papers/2501.08331', 'abstract': 'Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Video results are available on our webpage: https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code and model checkpoints are available on GitHub: https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.', 'score': 12, 'issue_id': 1798, 'pub_date': '2025-01-14', 'pub_date_card': {'ru': '14 января', 'en': 'January 14', 'zh': '1月14日'}, 'hash': 'c48e19ef08e8d758', 'authors': ['Ryan Burgert', 'Yuancheng Xu', 'Wenqi Xian', 'Oliver Pilarski', 'Pascal Clausen', 'Mingming He', 'Li Ma', 'Yitong Deng', 'Lingxiao Li', 'Mohsen Mousavi', 'Michael Ryoo', 'Paul Debevec', 'Ning Yu'], 'affiliations': ['Eyeline Studios', 'Netflix', 'Stanford University', 'Stony Brook University', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2501.08331.jpg', 'data': {'categories': ['#diffusion', '#video', '#data'], 'emoji': '🎬', 'ru': {'title': 'Контроль движения в видео-диффузии через структурированный шум', 'desc': 'Исследователи предложили метод улучшения видео-диффузионных моделей путем изменения структуры шумовых данных при обучении. Они разработали алгоритм искажения шума в реальном времени, который сохраняет пространственную гауссовость, но вводит временную корреляцию на основе оптического потока. Этот подход позволяет контролировать движение в генерируемых видео без изменения архитектуры модели. Эксперименты показали эффективность метода для управления локальным движением объектов, глобальным движением камеры и переносом движения.'}, 'en': {'title': 'Transforming Noise into Motion: Enhanced Control in Video Diffusion Models', 'desc': 'This paper presents an improvement in video diffusion models by introducing a method for controlling motion through structured latent noise sampling. The authors propose a novel noise warping algorithm that modifies the training data to replace random noise with correlated noise based on optical flow, enhancing temporal coherence while maintaining spatial quality. This approach allows for real-time processing and fine-tuning of existing video diffusion models without altering their architecture or training methods. The results show that this method effectively enables various motion control tasks, making it a versatile tool for video generation applications.'}, 'zh': {'title': '运动控制的新方法：扭曲噪声的力量', 'desc': '生成建模的目标是将随机噪声转化为结构化输出。本文通过结构化潜在噪声采样增强视频扩散模型，实现了运动控制。我们提出了一种新颖的噪声扭曲算法，能够实时运行，并用光流场导出的相关扭曲噪声替代随机时间高斯噪声，同时保持空间高斯性。我们的算法高效性使得在现代视频扩散基础模型中使用扭曲噪声进行微调成为可能，提供了用户友好的运动控制解决方案。'}}}, {'id': 'https://huggingface.co/papers/2501.12273', 'title': 'Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and Refinement', 'url': 'https://huggingface.co/papers/2501.12273', 'abstract': 'The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.', 'score': 12, 'issue_id': 1796, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '10499c8b820d5368', 'authors': ['Maosong Cao', 'Taolin Zhang', 'Mo Li', 'Chuyu Zhang', 'Yunxin Liu', 'Haodong Duan', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2501.12273.jpg', 'data': {'categories': ['#optimization', '#synthetic', '#data', '#dataset', '#training'], 'emoji': '🦅', 'ru': {'title': 'Condor: прорыв в создании синтетических данных для обучения языковых моделей', 'desc': 'В статье представлен Condor - новый фреймворк для генерации синтетических данных для обучения больших языковых моделей (LLM). Он использует дерево мировых знаний и самоанализ для создания высококачественных обучающих данных. Эксперименты показали, что модель, обученная на 20 тысячах сгенерированных Condor примеров, превосходит аналоги. Исследование также выявило потенциал для улучшения производительности LLM при масштабировании синтетических данных.'}, 'en': {'title': 'Unlocking LLM Potential with Synthetic Data Generation', 'desc': 'This paper addresses the challenge of obtaining high-quality Supervised Fine-Tuning (SFT) data for Large Language Models (LLMs). It presents Condor, a two-stage framework that generates synthetic training data using World Knowledge Tree and Self-Reflection Refinement techniques. The results show that models fine-tuned with just 20,000 samples from Condor outperform those trained with traditional methods. Additionally, the framework allows for iterative self-improvement, suggesting significant potential for enhancing LLM performance through synthetic data.'}, 'zh': {'title': '合成数据生成，提升对话能力的关键', 'desc': '本论文探讨了监督微调（SFT）数据的质量对大型语言模型（LLMs）对话能力的重要性。随着LLMs的进步，高质量的人类标注SFT数据变得稀缺，因此需要更多依赖合成训练数据。我们提出了一种名为Condor的两阶段合成数据生成框架，结合了世界知识树和自我反思精炼，以大规模生成高质量的SFT数据。实验结果表明，仅用20K个Condor生成的样本微调的基础模型，其性能优于其他模型，验证了我们方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2501.12390', 'title': 'GPS as a Control Signal for Image Generation', 'url': 'https://huggingface.co/papers/2501.12390', 'abstract': 'We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-image models through score distillation sampling, using GPS conditioning to constrain the appearance of the reconstruction from each viewpoint. Our evaluations suggest that our GPS-conditioned models successfully learn to generate images that vary based on location, and that GPS conditioning improves estimated 3D structure.', 'score': 10, 'issue_id': 1797, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '11d289e8a895bedd', 'authors': ['Chao Feng', 'Ziyang Chen', 'Aleksander Holynski', 'Alexei A. Efros', 'Andrew Owens'], 'affiliations': ['UC Berkeley', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2501.12390.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#dataset', '#diffusion', '#3d'], 'emoji': '🗺️', 'ru': {'title': 'GPS-метки открывают новые горизонты в генерации изображений и 3D-моделировании', 'desc': 'Исследователи демонстрируют, как GPS-метки в метаданных фотографий могут использоваться для улучшения генерации изображений. Они обучают модели диффузии, генерирующие изображения на основе GPS-координат и текста, что позволяет точно отображать особенности различных районов и достопримечательностей. Авторы также извлекают 3D-модели из 2D GPS-моделей с помощью методики score distillation sampling. Результаты показывают, что GPS-обусловленные модели успешно генерируют изображения, варьирующиеся в зависимости от местоположения, и улучшают оценку 3D-структуры.'}, 'en': {'title': 'Harnessing GPS Data for Location-Aware Image Generation', 'desc': 'This paper explores the use of GPS data embedded in photo metadata as a control signal for generating images. The authors develop GPS-to-image models, particularly a diffusion model, that can create images based on both GPS coordinates and textual descriptions. The model effectively captures the unique characteristics of various urban environments, such as neighborhoods and landmarks. Additionally, they demonstrate the ability to extract 3D models from these images, enhancing the accuracy of 3D reconstructions by using GPS information to guide the process.'}, 'zh': {'title': '利用GPS标签生成城市图像的创新方法', 'desc': '本文展示了照片元数据中的GPS标签可以作为图像生成的有用控制信号。我们训练了GPS到图像的模型，并将其应用于需要细致理解城市中图像变化的任务。特别地，我们训练了一个扩散模型，生成同时依赖于GPS和文本的图像。评估结果表明，我们的GPS条件模型成功学习了基于位置生成变化图像，并且GPS条件改善了估计的3D结构。'}}}, {'id': 'https://huggingface.co/papers/2501.10687', 'title': 'EMO2: End-Effector Guided Audio-Driven Avatar Video Generation', 'url': 'https://huggingface.co/papers/2501.10687', 'abstract': 'In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations.', 'score': 9, 'issue_id': 1798, 'pub_date': '2025-01-18', 'pub_date_card': {'ru': '18 января', 'en': 'January 18', 'zh': '1月18日'}, 'hash': '13c0931101eb51eb', 'authors': ['Linrui Tian', 'Siqi Hu', 'Qi Wang', 'Bang Zhang', 'Liefeng Bo'], 'affiliations': ['Institute for Intelligent Computing, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2501.10687.jpg', 'data': {'categories': ['#multimodal', '#audio', '#video', '#games', '#diffusion'], 'emoji': '🗣️', 'ru': {'title': 'Революция в анимации: от звука к выразительным жестам', 'desc': 'В статье предлагается новый метод создания говорящей головы на основе аудио, способный одновременно генерировать выразительные мимику и жесты рук. Авторы определяют задачу как двухэтапный процесс: сначала генерируются позы рук непосредственно из аудиовхода, затем применяется диффузионная модель для синтеза видеокадров. Экспериментальные результаты показывают, что предложенный метод превосходит современные подходы по качеству изображения и точности синхронизации. Работа предоставляет новый взгляд на генерацию жестов на основе аудио и надежную основу для создания выразительных и естественных анимаций говорящей головы.'}, 'en': {'title': 'Expressive Talking Heads: Bridging Audio and Gesture Generation', 'desc': 'This paper introduces a new method for creating talking head animations that are driven by audio. It focuses on generating both facial expressions and hand gestures, addressing the limitations of previous methods that often overlook the connection between audio and gestures. The approach is divided into two stages: first, it generates hand poses from audio signals, and then it uses a diffusion model to create video frames that combine these hand poses with realistic facial movements. The results show that this method is more effective than existing techniques, providing better visual quality and synchronization with the audio.'}, 'zh': {'title': '音频驱动的生动表情与手势生成新方法', 'desc': '本文提出了一种新颖的音频驱动的说话头方法，能够同时生成高度表现力的面部表情和手势。与现有方法不同，我们关注于共语手势生成的挑战，并识别音频特征与全身手势之间的弱对应关系。为了解决这个问题，我们将任务重新定义为两个阶段：第一阶段直接从音频输入生成手势，第二阶段使用扩散模型合成视频帧，结合第一阶段生成的手势，产生逼真的面部表情和身体动作。实验结果表明，该方法在视觉质量和同步精度方面优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2501.10057', 'title': 'MSTS: A Multimodal Safety Test Suite for Vision-Language Models', 'url': 'https://huggingface.co/papers/2501.10057', 'abstract': 'Vision-language models (VLMs), which process image and text inputs, are increasingly integrated into chat assistants and other consumer AI applications. Without proper safeguards, however, VLMs may give harmful advice (e.g. how to self-harm) or encourage unsafe behaviours (e.g. to consume drugs). Despite these clear hazards, little work so far has evaluated VLM safety and the novel risks created by multimodal inputs. To address this gap, we introduce MSTS, a Multimodal Safety Test Suite for VLMs. MSTS comprises 400 test prompts across 40 fine-grained hazard categories. Each test prompt consists of a text and an image that only in combination reveal their full unsafe meaning. With MSTS, we find clear safety issues in several open VLMs. We also find some VLMs to be safe by accident, meaning that they are safe because they fail to understand even simple test prompts. We translate MSTS into ten languages, showing non-English prompts to increase the rate of unsafe model responses. We also show models to be safer when tested with text only rather than multimodal prompts. Finally, we explore the automation of VLM safety assessments, finding even the best safety classifiers to be lacking.', 'score': 7, 'issue_id': 1802, 'pub_date': '2025-01-17', 'pub_date_card': {'ru': '17 января', 'en': 'January 17', 'zh': '1月17日'}, 'hash': '05ea9cad57d3e1e6', 'authors': ['Paul Röttger', 'Giuseppe Attanasio', 'Felix Friedrich', 'Janis Goldzycher', 'Alicia Parrish', 'Rishabh Bhardwaj', 'Chiara Di Bonaventura', 'Roman Eng', 'Gaia El Khoury Geagea', 'Sujata Goswami', 'Jieun Han', 'Dirk Hovy', 'Seogyeong Jeong', 'Paloma Jeretič', 'Flor Miriam Plaza-del-Arco', 'Donya Rooein', 'Patrick Schramowski', 'Anastassia Shaitarova', 'Xudong Shen', 'Richard Willats', 'Andrea Zugarini', 'Bertie Vidgen'], 'affiliations': ['Bocconi University', 'CERTAIN', 'Clarkson University', 'Contextual AI', 'DFKI', 'Expert.ai', 'Google DeepMind', 'Hessian.AI', 'Imperial College London', 'Instituto de Telecomunicações', 'KAIST', 'Kings College London', 'Lawrence Berkeley National Laboratory', 'National University of Singapore', 'TU Darmstadt', 'University of Pennsylvania', 'University of Zurich', 'Walled AI'], 'pdf_title_img': 'assets/pdf/title_img/2501.10057.jpg', 'data': {'categories': ['#security', '#dataset', '#benchmark', '#multimodal', '#ethics', '#multilingual'], 'emoji': '🔍', 'ru': {'title': 'Новый подход к оценке безопасности мультимодальных ИИ-моделей', 'desc': 'Статья представляет новый набор тестов MSTS для оценки безопасности мультимодальных моделей, работающих с изображениями и текстом. MSTS содержит 400 тестовых запросов в 40 категориях опасностей, где небезопасный смысл раскрывается только при сочетании текста и изображения. Исследование выявило проблемы безопасности в нескольких открытых мультимодальных моделях, а также показало, что некоторые модели безопасны случайно из-за непонимания даже простых запросов. Авторы также обнаружили, что модели менее безопасны при тестировании на других языках и с мультимодальными запросами по сравнению с только текстовыми.'}, 'en': {'title': 'Ensuring Safety in Vision-Language Models: A New Testing Approach', 'desc': 'This paper discusses the safety concerns associated with Vision-Language Models (VLMs) that combine image and text inputs. It introduces the Multimodal Safety Test Suite (MSTS), which includes 400 test prompts designed to evaluate the safety of VLMs across various hazard categories. The study reveals that many VLMs exhibit safety issues when processing multimodal inputs, while some are inadvertently safe due to their inability to comprehend simple prompts. Additionally, the research highlights the challenges in automating safety assessments for VLMs, indicating that even the most advanced safety classifiers have limitations.'}, 'zh': {'title': '确保视觉语言模型安全的关键测试', 'desc': '本文介绍了一种多模态安全测试套件（MSTS），用于评估视觉语言模型（VLMs）的安全性。MSTS包含400个测试提示，涵盖40个细分的危险类别，每个提示由文本和图像组合而成，以揭示其潜在的危险含义。研究发现，许多开放的VLM在安全性方面存在明显问题，而一些模型由于无法理解简单提示而意外地表现出安全性。此外，测试结果表明，单一文本提示的安全性高于多模态提示，且现有的安全分类器在自动化评估中仍存在不足。'}}}, {'id': 'https://huggingface.co/papers/2501.10573', 'title': 'The Geometry of Tokens in Internal Representations of Large Language Models', 'url': 'https://huggingface.co/papers/2501.10573', 'abstract': 'We investigate the relationship between the geometry of token embeddings and their role in the next token prediction within transformer models. An important aspect of this connection uses the notion of empirical measure, which encodes the distribution of token point clouds across transformer layers and drives the evolution of token representations in the mean-field interacting picture. We use metrics such as intrinsic dimension, neighborhood overlap, and cosine similarity to observationally probe these empirical measures across layers. To validate our approach, we compare these metrics to a dataset where the tokens are shuffled, which disrupts the syntactic and semantic structure. Our findings reveal a correlation between the geometric properties of token embeddings and the cross-entropy loss of next token predictions, implying that prompts with higher loss values have tokens represented in higher-dimensional spaces.', 'score': 5, 'issue_id': 1807, 'pub_date': '2025-01-17', 'pub_date_card': {'ru': '17 января', 'en': 'January 17', 'zh': '1月17日'}, 'hash': '1b34301e721ccccd', 'authors': ['Karthik Viswanathan', 'Yuri Gardinazzi', 'Giada Panerai', 'Alberto Cazzaniga', 'Matteo Biagetti'], 'affiliations': ['Area Science Park, Trieste, Italy', 'University of Amsterdam, Amsterdam, the Netherlands', 'University of Trieste, Trieste, Italy'], 'pdf_title_img': 'assets/pdf/title_img/2501.10573.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#dataset', '#interpretability', '#data', '#architecture'], 'emoji': '🧮', 'ru': {'title': 'Геометрия вложений токенов раскрывает тайны предсказания в трансформерах', 'desc': 'Исследование посвящено связи между геометрией вложений токенов и их ролью в предсказании следующего токена в трансформерных моделях. Авторы используют понятие эмпирической меры для анализа распределения облаков точек токенов по слоям трансформера. Они применяют метрики, такие как внутренняя размерность, перекрытие окрестностей и косинусное сходство, для изучения этих эмпирических мер. Результаты показывают корреляцию между геометрическими свойствами вложений токенов и кросс-энтропийной функцией потерь при предсказании следующего токена.'}, 'en': {'title': 'Geometry Matters: Token Embeddings Shape Prediction Success', 'desc': 'This paper explores how the shape and arrangement of token embeddings affect the ability of transformer models to predict the next token in a sequence. It introduces the concept of empirical measure to analyze how token representations change across different layers of the model. By examining metrics like intrinsic dimension and cosine similarity, the authors investigate the geometric properties of these embeddings. The results show that tokens associated with higher prediction errors are represented in more complex, higher-dimensional spaces, highlighting the importance of geometry in language modeling.'}, 'zh': {'title': '标记嵌入几何与预测损失的关系', 'desc': '本文研究了在变换器模型中，标记嵌入的几何形状与下一个标记预测之间的关系。我们使用经验测度的概念来编码标记点云在变换器层中的分布，并驱动标记表示的演变。通过内在维度、邻域重叠和余弦相似度等指标，我们观察了这些经验测度在各层之间的变化。研究结果表明，标记嵌入的几何特性与下一个标记预测的交叉熵损失之间存在相关性，损失值较高的提示对应的标记在更高维空间中表示。'}}}, {'id': 'https://huggingface.co/papers/2501.12389', 'title': 'Taming Teacher Forcing for Masked Autoregressive Video Generation', 'url': 'https://huggingface.co/papers/2501.12389', 'abstract': 'We introduce MAGI, a hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation. Our key innovation, Complete Teacher Forcing (CTF), conditions masked frames on complete observation frames rather than masked ones (namely Masked Teacher Forcing, MTF), enabling a smooth transition from token-level (patch-level) to frame-level autoregressive generation. CTF significantly outperforms MTF, achieving a +23% improvement in FVD scores on first-frame conditioned video prediction. To address issues like exposure bias, we employ targeted training strategies, setting a new benchmark in autoregressive video generation. Experiments show that MAGI can generate long, coherent video sequences exceeding 100 frames, even when trained on as few as 16 frames, highlighting its potential for scalable, high-quality video generation.', 'score': 3, 'issue_id': 1813, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': '43a9c17394f0d637', 'authors': ['Deyu Zhou', 'Quan Sun', 'Yuang Peng', 'Kun Yan', 'Runpei Dong', 'Duomin Wang', 'Zheng Ge', 'Nan Duan', 'Xiangyu Zhang', 'Lionel M. Ni', 'Heung-Yeung Shum'], 'affiliations': ['HKUST', 'HKUST(GZ)', 'StepFun', 'THU', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2501.12389.jpg', 'data': {'categories': ['#training', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'MAGI: Революция в автоматической генерации видео', 'desc': 'MAGI - это гибридная система генерации видео, объединяющая маскированное моделирование для внутрикадровой генерации и каузальное моделирование для генерации следующего кадра. Ключевое нововведение - Complete Teacher Forcing (CTF), которое обусловливает маскированные кадры полными наблюдаемыми кадрами, а не маскированными. CTF значительно превосходит Masked Teacher Forcing (MTF), улучшая показатели FVD на 23% при прогнозировании видео на основе первого кадра. MAGI способна генерировать длинные, связные видеопоследовательности, превышающие 100 кадров, даже при обучении на всего 16 кадрах.'}, 'en': {'title': 'MAGI: Revolutionizing Video Generation with Complete Teacher Forcing', 'desc': 'MAGI is a new framework for generating videos that uses two main techniques: masked modeling for creating individual frames and causal modeling for predicting the next frame. The innovative approach called Complete Teacher Forcing (CTF) improves the process by using fully observed frames to guide the generation, rather than just partially masked frames. This method leads to a significant performance boost, as evidenced by a 23% increase in FVD scores compared to previous methods. Additionally, MAGI can produce long and coherent video sequences, demonstrating its effectiveness even with limited training data.'}, 'zh': {'title': 'MAGI：高效视频生成的新突破', 'desc': '本文介绍了一种名为MAGI的混合视频生成框架，它结合了掩码建模用于帧内生成和因果建模用于下一帧生成。我们提出的关键创新是完整教师强制（CTF），它基于完整观察帧而非掩码帧来条件化掩码帧，从而实现从标记级到帧级自回归生成的平滑过渡。CTF在第一帧条件视频预测中显著优于掩码教师强制（MTF），FVD分数提高了23%。实验表明，MAGI能够生成超过100帧的长时间连贯视频序列，即使在仅用16帧训练的情况下，也展现了其可扩展性和高质量视频生成的潜力。'}}}, {'id': 'https://huggingface.co/papers/2501.11900', 'title': 'Panoramic Interests: Stylistic-Content Aware Personalized Headline Generation', 'url': 'https://huggingface.co/papers/2501.11900', 'abstract': "Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal personalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines.", 'score': 3, 'issue_id': 1805, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': 'af7a432a54575398', 'authors': ['Junhong Lian', 'Xiang Ao', 'Xinyu Liu', 'Yang Liu', 'Qing He'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)'], 'pdf_title_img': 'assets/pdf/title_img/2501.11900.jpg', 'data': {'categories': ['#multimodal', '#training', '#story_generation', '#dataset'], 'emoji': '📰', 'ru': {'title': 'SCAPE: персонализация заголовков с учетом стиля и содержания', 'desc': 'Эта статья представляет новый подход к генерации персонализированных заголовков новостей, называемый SCAPE. Фреймворк SCAPE учитывает как содержательные, так и стилистические предпочтения пользователей с помощью большой языковой модели. Он адаптивно интегрирует долгосрочные и краткосрочные интересы пользователей через иерархическую сеть слияния на основе контрастного обучения. Эксперименты на реальном датасете PENS демонстрируют превосходство SCAPE над базовыми методами.'}, 'en': {'title': 'Tailored Headlines: Merging Style and Content for Personalization', 'desc': "This paper introduces a new framework called SCAPE for generating personalized news headlines that cater to both content and stylistic preferences of users. Unlike previous methods that primarily focus on content, SCAPE recognizes the importance of diverse stylistic choices in enhancing personalization. The framework utilizes large language models to extract relevant features and employs a contrastive learning-based hierarchical fusion network to integrate users' interests over time. Experimental results on the PENS dataset show that SCAPE outperforms existing approaches in generating more appealing and tailored headlines."}, 'zh': {'title': '个性化标题生成的新视角：风格与内容的结合', 'desc': '个性化新闻标题生成旨在为用户提供吸引眼球的标题，符合他们的偏好。现有方法主要关注用户的内容偏好，但往往忽视了用户多样化的风格偏好，这导致个性化效果不佳。为此，我们提出了一种新颖的风格内容感知个性化标题生成框架（SCAPE）。SCAPE通过大型语言模型提取标题的内容和风格特征，并通过对比学习的层次融合网络自适应整合用户的长期和短期兴趣，从而在生成过程中反映用户的风格内容偏好。'}}}, {'id': 'https://huggingface.co/papers/2501.12206', 'title': 'Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model', 'url': 'https://huggingface.co/papers/2501.12206', 'abstract': 'Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and describing visual content, achieving state-of-the-art performance across various vision-language tasks. However, these models frequently exhibit hallucination behavior, where they generate descriptions containing objects or details absent in the input image. Our work investigates this phenomenon by analyzing attention patterns across transformer layers and heads, revealing that hallucinations often stem from progressive degradation of visual grounding in deeper layers. We propose a novel attention modification approach that combines selective token emphasis and head-specific modulation to maintain visual grounding throughout the generation process. Our method introduces two key components: (1) a dual-stream token selection mechanism that identifies and prioritizes both locally informative and spatially significant visual tokens, and (2) an attention head-specific modulation strategy that differentially amplifies visual information processing based on measured visual sensitivity of individual attention heads. Through extensive experimentation on the MSCOCO dataset, we demonstrate that our approach reduces hallucination rates by up to 62.3\\% compared to baseline models while maintaining comparable task performance. Our analysis reveals that selectively modulating tokens across attention heads with varying levels of visual sensitivity can significantly improve visual grounding without requiring model retraining.', 'score': 0, 'issue_id': 1812, 'pub_date': '2025-01-21', 'pub_date_card': {'ru': '21 января', 'en': 'January 21', 'zh': '1月21日'}, 'hash': 'd37fc59e414ab903', 'authors': ['Kazi Hasan Ibn Arif', 'Sajib Acharjee Dip', 'Khizar Hussain', 'Lang Zhang', 'Chris Thomas'], 'affiliations': ['Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2501.12206.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#interpretability', '#architecture', '#cv', '#hallucinations'], 'emoji': '👁️', 'ru': {'title': 'Улучшение визуальной привязки для снижения галлюцинаций в LVLM', 'desc': 'Данная статья исследует проблему галлюцинаций в крупных визуально-языковых моделях (LVLM) при описании изображений. Авторы анализируют паттерны внимания в слоях трансформера и обнаруживают, что галлюцинации часто возникают из-за ослабления визуальной привязки в глубоких слоях. Предлагается новый подход модификации внимания, сочетающий выборочное усиление токенов и модуляцию головок внимания для сохранения визуальной привязки. Эксперименты показывают, что метод снижает уровень галлюцинаций на 62.3% по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing Visual Grounding to Combat Hallucinations in LVLMs', 'desc': "This paper addresses the issue of hallucination in Large Vision Language Models (LVLMs), where the models generate incorrect descriptions that include non-existent objects. The authors analyze attention patterns in transformer layers to understand how visual grounding deteriorates in deeper layers, leading to these hallucinations. They propose a new method that enhances attention by focusing on important visual tokens and adjusting how different attention heads process visual information. Their experiments show that this approach can significantly reduce hallucination rates while keeping the model's performance on tasks intact."}, 'zh': {'title': '减少幻觉，提升视觉理解！', 'desc': '大型视觉语言模型（LVLMs）在理解和描述视觉内容方面表现出色，但它们常常会产生幻觉行为，即生成的描述中包含输入图像中不存在的对象或细节。我们的研究分析了变换器层和头部的注意力模式，发现幻觉通常源于深层次的视觉基础逐渐退化。我们提出了一种新的注意力修改方法，结合选择性标记强调和头部特定调制，以在生成过程中保持视觉基础。通过在MSCOCO数据集上的广泛实验，我们的方法将幻觉率降低了多达62.3%，同时保持了相似的任务性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi (1)', '#alignment (1)', '#architecture (4)', '#audio (1)', '#benchmark (7)', '#cv (4)', '#data (4)', '#dataset (8)', '#diffusion (5)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (9)', '#open_source (2)', '#optimization (9)', '#plp', '#rag (1)', '#reasoning (5)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation (1)', '#survey (1)', '#synthetic (3)', '#training (11)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-01-23 02:08',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-01-23 02:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-01-23 02:08')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    