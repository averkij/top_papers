
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. January 22.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">22 ÑĞ½Ğ²Ğ°Ñ€Ñ</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2026-01-21.html">â¬…ï¸ <span id="prev-date">21.01</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2026-01-23.html">â¡ï¸ <span id="next-date">23.01</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2026-01.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '22 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 22', 'zh': '1æœˆ22æ—¥'};
        let feedDateNext = {'ru': '23.01', 'en': '01/23', 'zh': '1æœˆ23æ—¥'};
        let feedDatePrev = {'ru': '21.01', 'en': '01/21', 'zh': '1æœˆ21æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2601.12538', 'title': 'Agentic Reasoning for Large Language Models', 'url': 'https://huggingface.co/papers/2601.12538', 'abstract': 'Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.', 'score': 52, 'issue_id': 705, 'pub_date': '2026-01-18', 'pub_date_card': {'ru': '18 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 18', 'zh': '1æœˆ18æ—¥'}, 'hash': '889c44cfdc0d7221', 'authors': ['Tianxin Wei', 'Ting-Wei Li', 'Zhining Liu', 'Xuying Ning', 'Ze Yang', 'Jiaru Zou', 'Zhichen Zeng', 'Ruizhong Qiu', 'Xiao Lin', 'Dongqi Fu', 'Zihao Li', 'Mengting Ai', 'Duo Zhou', 'Wenxuan Bao', 'Yunzhe Li', 'Gaotang Li', 'Cheng Qian', 'Yu Wang', 'Xiangru Tang', 'Yin Xiao', 'Liri Fang', 'Hui Liu', 'Xianfeng Tang', 'Yuji Zhang', 'Chi Wang', 'Jiaxuan You', 'Heng Ji', 'Hanghang Tong', 'Jingrui He'], 'affiliations': ['Amazon', 'Google Deepmind', 'Meta', 'UCSD', 'University of Illinois Urbana-Champaign', 'Yale'], 'pdf_title_img': 'assets/pdf/title_img/2601.12538.jpg', 'data': {'categories': ['#robotics', '#healthcare', '#training', '#survey', '#math', '#benchmark', '#science', '#rl', '#reasoning', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ: LLM ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ°Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğº Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹Ğ´ĞµĞ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ (Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ), ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ (Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ) Ğ¸ ĞºĞ¾Ğ»Ğ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ (Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡). Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ (Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ) Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ°Ğ¿Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· reinforcement learning. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ½Ğ°ÑƒĞºĞ¸, Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering LLMs as Autonomous Agents through Agentic Reasoning', 'desc': 'This paper introduces agentic reasoning, which transforms large language models (LLMs) into autonomous agents that can plan, act, and learn in changing environments. It categorizes agentic reasoning into three levels: foundational capabilities for single agents, self-evolving abilities that adapt through experience, and collective reasoning for multi-agent collaboration. The authors differentiate between in-context reasoning, which enhances interaction during use, and post-training reasoning, which improves performance through learning techniques. The survey also highlights practical applications and future challenges in deploying these advanced reasoning frameworks in real-world scenarios.'}, 'zh': {'title': 'ä»£ç†æ¨ç†ï¼šè®©è¯­è¨€æ¨¡å‹æˆä¸ºè‡ªä¸»æ™ºèƒ½ä½“', 'desc': 'ä»£ç†æ¨ç†é‡æ–°å®šä¹‰äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶æˆä¸ºèƒ½å¤Ÿåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè§„åˆ’ã€è¡ŒåŠ¨å’Œå­¦ä¹ çš„è‡ªä¸»æ™ºèƒ½ä½“ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å°é—­ä¸–ç•Œä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¼€æ”¾å’ŒåŠ¨æ€ç¯å¢ƒä¸­å´é¢ä¸´æŒ‘æˆ˜ã€‚ä»£ç†æ¨ç†é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºè‡ªä¸»æ™ºèƒ½ä½“ï¼Œæ ‡å¿—ç€ä¸€ä¸ªèŒƒå¼è½¬å˜ï¼Œå¼ºè°ƒæŒç»­äº’åŠ¨çš„é‡è¦æ€§ã€‚æœ¬æ–‡ç»„ç»‡äº†ä»£ç†æ¨ç†çš„ä¸‰ä¸ªäº’è¡¥ç»´åº¦ï¼ŒåŒ…æ‹¬åŸºç¡€ä»£ç†æ¨ç†ã€è‡ªæˆ‘æ¼”åŒ–ä»£ç†æ¨ç†å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼Œæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æ€è·¯ä¸è¡ŒåŠ¨çš„è·¯çº¿å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14171', 'title': 'Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance', 'url': 'https://huggingface.co/papers/2601.14171', 'abstract': 'RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.  \t\t\t\t\tAI-generated summary \t\t\t\t Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.', 'score': 3, 'issue_id': 705, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'a4a9f4fe5907584a', 'authors': ['Qianli Ma', 'Chang Guo', 'Zhiheng Tian', 'Siyu Wang', 'Jipeng Xiao', 'Yuanhao Yue', 'Zhipeng Zhang'], 'affiliations': ['AutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2601.14171.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#hallucinations', '#reasoning', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ”Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'RebuttalAgent â€” ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ñ Ñ€ĞµÑ†ĞµĞ½Ğ·ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ‹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ñ€ĞµĞ·ÑĞ¼Ğµ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹. ĞŸĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½, Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒÑ, Ñ‡Ñ‚Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğµ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğµ Ğ¿Ğ¾ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ².'}, 'en': {'title': 'RebuttalAgent: Transforming Rebuttal Generation with Evidence-Centric Planning', 'desc': 'RebuttalAgent is a novel multi-agent framework designed to enhance the process of generating rebuttals in academic peer reviews. It shifts the focus from simple text generation to an evidence-centric planning approach, which helps in addressing critiques more effectively. By breaking down complex feedback into manageable parts and utilizing external literature for support, it improves the accuracy and reliability of the rebuttals. The framework has been validated against existing methods, showing superior performance in terms of coverage, faithfulness, and strategic coherence.'}, 'zh': {'title': 'ä»¥è¯æ®ä¸ºä¸­å¿ƒçš„åé©³ç”Ÿæˆæ¡†æ¶', 'desc': 'RebuttalAgentæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œå°†åé©³ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä»¥è¯æ®ä¸ºä¸­å¿ƒçš„è§„åˆ’ä»»åŠ¡ï¼Œä»è€Œæé«˜å­¦æœ¯åŒè¡Œè¯„å®¡ä¸­çš„è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§ã€‚è¯¥ç³»ç»Ÿå°†å¤æ‚çš„åé¦ˆåˆ†è§£ä¸ºåŸºæœ¬é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆæˆå‹ç¼©æ‘˜è¦ä¸é«˜ä¿çœŸæ–‡æœ¬åŠ¨æ€æ„å»ºæ··åˆä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶é›†æˆä¸€ä¸ªè‡ªä¸»çš„å¤–éƒ¨æœç´¢æ¨¡å—ï¼Œä»¥è§£å†³éœ€è¦å¤–éƒ¨æ–‡çŒ®çš„å…³æ³¨ç‚¹ã€‚é€šè¿‡åœ¨èµ·è‰ä¹‹å‰ç”Ÿæˆå¯æ£€æŸ¥çš„å“åº”è®¡åˆ’ï¼ŒRebuttalAgentç¡®ä¿æ¯ä¸ªè®ºç‚¹éƒ½æ˜ç¡®åŸºäºå†…éƒ¨æˆ–å¤–éƒ¨è¯æ®ã€‚æˆ‘ä»¬çš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨RebuttalBenchä¸Šçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨è¦†ç›–ç‡ã€çœŸå®æ€§å’Œæˆ˜ç•¥ä¸€è‡´æ€§æ–¹é¢ä¼˜äºå¼ºåŸºçº¿ï¼Œæä¾›äº†ä¸€ä¸ªé€æ˜ä¸”å¯æ§çš„åŒè¡Œè¯„å®¡åŠ©æ‰‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14750', 'title': 'Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning', 'url': 'https://huggingface.co/papers/2601.14750', 'abstract': 'Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT', 'score': 2, 'issue_id': 705, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'}, 'hash': 'fa7ddd84d3506d5c', 'authors': ['Yifan Wang', 'Shiyu Li', 'Peiming Li', 'Xiaochen Yang', 'Yang Tang', 'Zheng Wei'], 'affiliations': ['School of Electronic and Computer Engineering, Peking University', 'School of Mathematics and Statistics, University of Glasgow', 'Tencent BAC', 'Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2601.14750.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ LLM', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Render-of-Thought (RoT) â€” Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºÑƒ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ²Ğ½Ğ¾Ğ¹ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ°Ğ»Ğ¾Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Chain-of-Thought Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 3-4-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ²Ğ½Ğ¾Ğ¹ CoT, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Visualizing Reasoning for Enhanced Clarity and Efficiency', 'desc': 'The Render-of-Thought (RoT) framework transforms reasoning steps from text into images using vision-language models, enhancing the clarity and efficiency of reasoning processes. This approach addresses the limitations of Chain-of-Thought (CoT) prompting, which, while effective, can be verbose and computationally expensive. By visualizing the reasoning chain, RoT makes the underlying rationale more explicit and easier to analyze. Experimental results show that RoT significantly reduces token usage and speeds up inference while maintaining strong performance on reasoning tasks.'}, 'zh': {'title': 'å°†æ¨ç†æ­¥éª¤å¯è§†åŒ–ï¼Œæé«˜æ•ˆç‡ä¸å¯è¿½æº¯æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºRender-of-Thoughtï¼ˆRoTï¼‰ï¼Œæ—¨åœ¨å°†æ–‡æœ¬æ¨ç†æ­¥éª¤è½¬æ¢ä¸ºå›¾åƒï¼Œä»¥æé«˜æ¨ç†çš„å¯è¿½æº¯æ€§å’Œæ•ˆç‡ã€‚RoTåˆ©ç”¨ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è§†è§‰ç¼–ç å™¨ï¼Œå°†è§†è§‰åµŒå…¥ä¸æ–‡æœ¬ç©ºé—´å¯¹é½ï¼Œä»è€Œä½¿æ½œåœ¨çš„æ¨ç†é“¾æ›´åŠ æ˜ç¡®ã€‚ä¸ä¼ ç»Ÿçš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºç›¸æ¯”ï¼ŒRoTåœ¨ä¿æŒç«äº‰æ€§æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†3-4å€çš„ä»¤ç‰Œå‹ç¼©å’Œæ˜¾è‘—çš„æ¨ç†åŠ é€Ÿã€‚è¯¥æ–¹æ³•çš„å®ç°æ— éœ€é¢å¤–çš„é¢„è®­ç»ƒå¼€é”€ï¼Œå±•ç¤ºäº†å…¶åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†ä¸Šçš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.15282', 'title': 'Rethinking Video Generation Model for the Embodied World', 'url': 'https://huggingface.co/papers/2601.15282', 'abstract': 'A comprehensive robotics benchmark evaluates video generation models across multiple task domains and embodiments, revealing deficiencies in physical realism and introducing a large-scale dataset to address training data shortages.  \t\t\t\t\tAI-generated summary \t\t\t\t Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.', 'score': 1, 'issue_id': 705, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'}, 'hash': 'cb5ee149b1d3627f', 'authors': ['Yufan Deng', 'Zilin Pan', 'Hongyu Zhang', 'Xiaojie Li', 'Ruoqing Hu', 'Yufei Ding', 'Yiming Zou', 'Yan Zeng', 'Daquan Zhou'], 'affiliations': ['ByteDance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2601.15282.jpg', 'data': {'categories': ['#robotics', '#video', '#benchmark', '#dataset', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼: Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ RBench â€” ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ, Ğ¿Ğ¾ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿ÑÑ‚ÑŒ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ñ‚Ğ¸Ğ¿Ğ° Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ². Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ĞºĞ°Ğº ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ»Ğ°ÑƒÑĞ¸Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ RoVid-X â€” Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ»Ğ¸Ğ¿Ğ¾Ğ² Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ­Ñ‚Ğ° ÑĞºĞ¾ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ˜Ğ˜.'}, 'en': {'title': 'RBench: Elevating Video Generation for Robotics with Comprehensive Evaluation and Data', 'desc': 'This paper presents RBench, a comprehensive benchmark for evaluating video generation models in robotics across various tasks and embodiments. It identifies significant shortcomings in the physical realism of generated videos and proposes a refined data pipeline to create RoVid-X, a large-scale dataset with 4 million annotated video clips. The benchmark assesses models based on task correctness and visual fidelity, achieving a high correlation with human evaluations. By combining rigorous evaluation with a rich dataset, the study aims to enhance the training of video generation models and advance embodied AI capabilities.'}, 'zh': {'title': 'å…¨é¢è¯„ä¼°æœºå™¨äººè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„åŸºå‡†æµ‹è¯•', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªå…¨é¢çš„æœºå™¨äººåŸºå‡†æµ‹è¯•RBenchï¼Œç”¨äºè¯„ä¼°è§†é¢‘ç”Ÿæˆæ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡é¢†åŸŸå’Œä¸åŒä½“ç°ä¸Šçš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨ç”Ÿæˆç‰©ç†çœŸå®æ„Ÿæ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œå¹¶ä¸”ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†é™åˆ¶äº†å…¬å¹³æ¯”è¾ƒå’Œè¿›å±•ã€‚ä¸ºäº†è§£å†³è®­ç»ƒæ•°æ®çŸ­ç¼ºçš„é—®é¢˜ï¼Œè®ºæ–‡æå‡ºäº†RoVid-Xï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«400ä¸‡ä¸ªæ ‡æ³¨è§†é¢‘ç‰‡æ®µçš„å¤§å‹å¼€æºæ•°æ®é›†ï¼Œæ¶µç›–äº†æ•°åƒä¸ªä»»åŠ¡ï¼Œå¹¶é™„æœ‰å…¨é¢çš„ç‰©ç†å±æ€§æ³¨é‡Šã€‚é€šè¿‡è¯„ä¼°å’Œæ•°æ®çš„ååŒä½œç”¨ï¼Œå»ºç«‹äº†ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œä»¥ä¿ƒè¿›è§†é¢‘æ¨¡å‹çš„ä¸¥æ ¼è¯„ä¼°å’Œå¯æ‰©å±•è®­ç»ƒï¼Œæ¨åŠ¨å…·èº«äººå·¥æ™ºèƒ½å‘é€šç”¨æ™ºèƒ½çš„æ¼”è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14245', 'title': 'XR: Cross-Modal Agents for Composed Image Retrieval', 'url': 'https://huggingface.co/papers/2601.14245', 'abstract': 'A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.  \t\t\t\t\tAI-generated summary \t\t\t\t Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.', 'score': 1, 'issue_id': 705, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'e41f2cd4d64c71b2', 'authors': ['Zhongyu Yang', 'Wei Pang', 'Yingfang Yuan'], 'affiliations': ['BCML, Heriot-Watt University Edinburgh, UK'], 'pdf_title_img': 'assets/pdf/title_img/2601.14245.jpg', 'data': {'categories': ['#rag', '#multimodal', '#agents'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº XR Ğ´Ğ»Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²: Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒÑÑ‚ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‚ Ğ³Ñ€ÑƒĞ±ÑƒÑ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ñ‹Ğ²Ğ°Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… FashionIQ, CIRR Ğ¸ CIRCO Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 38% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Revolutionizing Image Retrieval with Multi-Agent Coordination', 'desc': 'This paper presents a multi-agent framework called XR for compositional image retrieval, which enhances the process of matching images with textual descriptions. It utilizes three types of specialized agents: imagination agents that generate target representations, similarity agents that filter results based on hybrid matching, and question agents that verify the accuracy of the results. By coordinating these agents, XR improves the retrieval process to better meet both semantic and visual requirements. The framework shows significant performance improvements over existing methods, demonstrating the importance of each agent in achieving effective retrieval outcomes.'}, 'zh': {'title': 'å¤šæ™ºèƒ½ä½“æ¡†æ¶æå‡å›¾åƒæ£€ç´¢æ•ˆæœ', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºç»„åˆå›¾åƒæ£€ç´¢ï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰å’Œè§†è§‰æŸ¥è¯¢åŒ¹é…çš„æ•ˆæœã€‚è¯¥æ¡†æ¶ä½¿ç”¨ä¸‰ç§ä¸“é—¨çš„æ™ºèƒ½ä½“ï¼šæƒ³è±¡æ™ºèƒ½ä½“ç”Ÿæˆç›®æ ‡è¡¨ç¤ºï¼Œç›¸ä¼¼æ€§æ™ºèƒ½ä½“è¿›è¡Œç²—ç•¥è¿‡æ»¤ï¼Œé—®é¢˜æ™ºèƒ½ä½“é€šè¿‡é’ˆå¯¹æ€§æ¨ç†éªŒè¯äº‹å®ä¸€è‡´æ€§ã€‚ä¸ä¼ ç»Ÿçš„ç›¸ä¼¼æ€§åŸºç¡€æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è·¨æ¨¡æ€çš„ç†è§£å’Œæ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æ¯ç§æ™ºèƒ½ä½“åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14152', 'title': 'Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models', 'url': 'https://huggingface.co/papers/2601.14152', 'abstract': 'Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.', 'score': 1, 'issue_id': 705, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'ebb86e7319481dfb', 'authors': ['Hyunjong Ok', 'Jaeho Lee'], 'affiliations': ['HJ AILAB', 'POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2601.14152.jpg', 'data': {'categories': [], 'emoji': 'ğŸ”—', 'ru': {'title': 'ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸: Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ² Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑƒĞ·ĞºĞ¸Ğµ Ğ¼ĞµÑÑ‚Ğ° Ğ¿Ñ€Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¿ÑÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‡Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğº Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ°Ğ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ QOC. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ-Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ (CQO) Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 14 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Causal Attention: The Key to Unlocking Prompt Performance', 'desc': 'This paper investigates how the order of prompts affects the performance of large language models in multiple-choice question answering. It finds that placing context before questions and options (CQO) significantly improves performance compared to the reverse order (QOC), with a drop of over 14 percentage points in the latter. The authors attribute this performance gap to causal attention mechanisms, which restrict option tokens from accessing context in QOC prompts, leading to an information bottleneck. By analyzing various models and datasets, the study highlights the importance of prompt structure in optimizing model performance.'}, 'zh': {'title': 'å› æœæ³¨æ„åŠ›å½±å“å¤šé€‰é¢˜è¡¨ç°', 'desc': 'æœ¬ç ”ç©¶æ­ç¤ºäº†è¯­è¨€æ¨¡å‹ä¸­çš„å› æœæ³¨æ„åŠ›åœ¨å¤šé€‰é¢˜å›ç­”ä¸­é€ æˆä¿¡æ¯ç“¶é¢ˆçš„é—®é¢˜ã€‚å½“ä¸Šä¸‹æ–‡åœ¨é—®é¢˜å’Œé€‰é¡¹ä¹‹å‰æ—¶ï¼ˆCQOï¼‰ï¼Œæ¨¡å‹çš„è¡¨ç°æ¯”åå‘é¡ºåºï¼ˆQOCï¼‰é«˜å‡ºè¶…è¿‡14ä¸ªç™¾åˆ†ç‚¹ã€‚é€šè¿‡ç³»ç»Ÿçš„æ¶æ„åˆ†æï¼Œæˆ‘ä»¬å‘ç°å› æœæ³¨æ„åŠ›æ˜¯æ ¸å¿ƒæœºåˆ¶ï¼šåœ¨QOCæç¤ºä¸­ï¼Œå› æœæ©ç é˜»æ­¢é€‰é¡¹æ ‡è®°å…³æ³¨ä¸Šä¸‹æ–‡ï¼Œä»è€Œå¯¼è‡´ä¸Šä¸‹æ–‡å¯¹é€‰é¡¹ä¸å¯è§ã€‚è¿™ä¸ªå‘ç°ä¸ºç†è§£å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•æ„Ÿæ€§æä¾›äº†æ–°çš„è§†è§’ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.15100', 'title': 'Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek', 'url': 'https://huggingface.co/papers/2601.15100', 'abstract': "WebSeek is a mixed-initiative browser extension that enables interactive web data extraction and analysis with AI-assisted guidance and automation.  \t\t\t\t\tAI-generated summary \t\t\t\t Web AI agents such as ChatGPT Agent and GenSpark are increasingly used for routine web-based tasks, yet they still rely on text-based input prompts, lack proactive detection of user intent, and offer no support for interactive data analysis and decision making. We present WebSeek, a mixed-initiative browser extension that enables users to discover and extract information from webpages to then flexibly build, transform, and refine tangible data artifacts-such as tables, lists, and visualizations-all within an interactive canvas. Within this environment, users can perform analysis-including data transformations such as joining tables or creating visualizations-while an in-built AI both proactively offers context-aware guidance and automation, and reactively responds to explicit user requests. An exploratory user study (N=15) with WebSeek as a probe reveals participants' diverse analysis strategies, underscoring their desire for transparency and control during human-AI collaboration.", 'score': 0, 'issue_id': 705, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'}, 'hash': '8a15e6703617633f', 'authors': ['Yanwei Huang', 'Arpit Narechania'], 'affiliations': ['The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2601.15100.jpg', 'data': {'categories': ['#multimodal', '#agents'], 'emoji': 'ğŸ•·ï¸', 'ru': {'title': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²ĞµĞ±-ÑĞºÑ€ĞµĞ¿Ğ¸Ğ½Ğ³ Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ AI', 'desc': 'WebSeek â€” ÑÑ‚Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ²ĞµĞ±-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞµÑ‘ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ (Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹, ÑĞ¿Ğ¸ÑĞºĞ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸) Ğ¸ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ. AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ´Ğ²ÑƒÑ… Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ñ…: Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾-Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒ Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞ²Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ‹Ñ‚Ğ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ»ÑĞ´Ğ¸ Ñ†ĞµĞ½ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ¿Ñ€Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ˜Ğ˜ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹.'}, 'en': {'title': 'Empowering Web Data Analysis with AI Collaboration', 'desc': 'WebSeek is a browser extension that combines human and AI efforts to help users extract and analyze data from the web. It allows users to create and modify data artifacts like tables and visualizations in an interactive workspace. The AI component not only provides guidance based on user actions but also automates certain tasks to enhance the analysis process. A user study showed that participants appreciated having control and clarity while working alongside the AI.'}, 'zh': {'title': 'WebSeekï¼šæ™ºèƒ½åŒ–çš„ç½‘é¡µæ•°æ®æå–ä¸åˆ†æå·¥å…·', 'desc': 'WebSeek æ˜¯ä¸€ä¸ªæ··åˆä¸»åŠ¨çš„æµè§ˆå™¨æ‰©å±•ï¼Œæ—¨åœ¨é€šè¿‡ AI è¾…åŠ©çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–æ¥å®ç°äº’åŠ¨å¼ç½‘é¡µæ•°æ®æå–å’Œåˆ†æã€‚ç”¨æˆ·å¯ä»¥åœ¨ä¸€ä¸ªäº’åŠ¨ç”»å¸ƒä¸­å‘ç°å’Œæå–ç½‘é¡µä¿¡æ¯ï¼Œçµæ´»æ„å»ºã€è½¬æ¢å’Œå®Œå–„æ•°æ®å·¥ä»¶ï¼Œå¦‚è¡¨æ ¼ã€åˆ—è¡¨å’Œå¯è§†åŒ–ã€‚è¯¥ç³»ç»Ÿä¸ä»…æä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æŒ‡å¯¼å’Œè‡ªåŠ¨åŒ–ï¼Œè¿˜èƒ½æ ¹æ®ç”¨æˆ·çš„æ˜ç¡®è¯·æ±‚åšå‡ºååº”ã€‚ç”¨æˆ·ç ”ç©¶æ˜¾ç¤ºï¼Œå‚ä¸è€…åœ¨åˆ†æè¿‡ç¨‹ä¸­é‡‡ç”¨äº†å¤šæ ·çš„ç­–ç•¥ï¼Œå¼ºè°ƒäº†åœ¨ä¸ AI åä½œæ—¶å¯¹é€æ˜åº¦å’Œæ§åˆ¶çš„éœ€æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14681', 'title': 'FARE: Fast-Slow Agentic Robotic Exploration', 'url': 'https://huggingface.co/papers/2601.14681', 'abstract': 'FARE is a hierarchical exploration framework that combines large language model reasoning with reinforcement learning control to enable efficient autonomous robot navigation in complex environments.  \t\t\t\t\tAI-generated summary \t\t\t\t This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale 200mtimes130m building environment.', 'score': 0, 'issue_id': 705, 'pub_date': '2026-01-21', 'pub_date_card': {'ru': '21 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 21', 'zh': '1æœˆ21æ—¥'}, 'hash': 'b87cdcf418e36496', 'authors': ['Shuhao Liao', 'Xuxin Lv', 'Jeric Lew', 'Shizhe Zhang', 'Jingsong Liang', 'Peizhuo Li', 'Yuhong Cao', 'Wenjun Wu', 'Guillaume Sartoretti'], 'affiliations': ['Beihang University, China', 'Department of Mechanical Engineering, National University of Singapore, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2601.14681.jpg', 'data': {'categories': ['#robotics', '#graphs', '#training', '#rl', '#reasoning', '#agents'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°: LLM Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸, RL Ğ´Ğ»Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'FARE â€” ÑÑ‚Ğ¾ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ: Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ LLM Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¸Ğ·Ğ²ĞµĞ´Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ° Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ RL-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒÑÑÑŒ Ğ½Ğ° ÑÑ‚Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° ĞºĞ°Ğº Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒÑ 200Ã—130 Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ² Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'FARE: Smart Navigation for Robots with Language and Learning', 'desc': 'FARE is a novel framework that enhances robot navigation by combining large language model (LLM) reasoning with reinforcement learning (RL) control. It operates on a fast-slow thinking paradigm, where the LLM interprets environmental descriptions to create exploration strategies, while the RL module executes these strategies based on real-time observations. The framework uses a modularity-based pruning mechanism to streamline the exploration graph, improving reasoning efficiency. Experimental results demonstrate that FARE significantly outperforms existing methods in both simulated and real-world environments, showcasing its effectiveness in complex navigation tasks.'}, 'zh': {'title': 'FAREï¼šé«˜æ•ˆè‡ªä¸»æ¢ç´¢çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'FAREæ˜¯ä¸€ä¸ªå±‚æ¬¡åŒ–çš„æ¢ç´¢æ¡†æ¶ï¼Œç»“åˆäº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ çš„æ§åˆ¶æ–¹æ³•ï¼Œä»¥å®ç°å¤æ‚ç¯å¢ƒä¸­çš„é«˜æ•ˆè‡ªä¸»æœºå™¨äººå¯¼èˆªã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¿«æ…¢æ€ç»´æ¨¡å¼ï¼Œæ…¢æ€ç»´çš„LLMæ¨¡å—è§£ææœªçŸ¥ç¯å¢ƒçš„æ–‡æœ¬æè¿°ï¼Œå¹¶ç”Ÿæˆæ¢ç´¢ç­–ç•¥ï¼Œéšåé€šè¿‡æ‹“æ‰‘å›¾è½¬åŒ–ä¸ºå…¨å±€è·¯å¾„ç‚¹ã€‚å¿«é€Ÿæ€ç»´çš„RLæ¨¡å—æ ¹æ®å±€éƒ¨è§‚å¯Ÿæ‰§è¡Œæ¢ç´¢ï¼ŒåŒæ—¶å—åˆ°LLMç”Ÿæˆçš„å…¨å±€è·¯å¾„ç‚¹çš„æŒ‡å¯¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFAREåœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­çš„æ¢ç´¢æ•ˆç‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶åœ¨å®é™…ç¡¬ä»¶ä¸ŠéªŒè¯äº†å…¶åœ¨å¤æ‚å¤§è§„æ¨¡ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2601.14352', 'title': 'RoboBrain 2.5: Depth in Sight, Time in Mind', 'url': 'https://huggingface.co/papers/2601.14352', 'abstract': 'RoboBrain 2.5 enhances embodied AI through improved 3D spatial reasoning and temporal value estimation for more precise manipulation tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io', 'score': 0, 'issue_id': 705, 'pub_date': '2026-01-20', 'pub_date_card': {'ru': '20 ÑĞ½Ğ²Ğ°Ñ€Ñ', 'en': 'January 20', 'zh': '1æœˆ20æ—¥'}, 'hash': 'a4c2212b43fc1018', 'authors': ['Huajie Tan', 'Enshen Zhou', 'Zhiyu Li', 'Yijie Xu', 'Yuheng Ji', 'Xiansheng Chen', 'Cheng Chi', 'Pengwei Wang', 'Huizhu Jia', 'Yulong Ao', 'Mingyu Cao', 'Sixiang Chen', 'Zhe Li', 'Mengzhen Liu', 'Zixiao Wang', 'Shanyu Rong', 'Yaoxu Lyu', 'Zhongxia Zhao', 'Peterson Co', 'Yibo Li', 'Yi Han', 'Shaoxuan Xie', 'Guocai Yao', 'Songjing Wang', 'Leiduo Zhang', 'Xi Yang', 'Yance Jiao', 'Donghai Shi', 'Kunchang Xie', 'Shaokai Nie', 'Chunlei Men', 'Yonghua Lin', 'Zhongyuan Wang', 'Tiejun Huang', 'Shanghang Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2601.14352.jpg', 'data': {'categories': ['#architecture', '#3d', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¢Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'RoboBrain 2.5 â€” ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ“Ğ»Ğ°Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğµ Ğ¾Ñ‚ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°Ñ… Ğº Ñ‚Ñ€Ñ‘Ñ…Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞ¼Ñƒ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ². ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒĞ³Ğ»Ğ¾Ğ² Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ñ‡Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ­Ñ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ÑÑ‚ Ğ²Ğ¾Ğ¿Ğ»Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡.'}, 'en': {'title': 'Empowering AI with Enhanced 3D Reasoning and Temporal Awareness', 'desc': 'RoboBrain 2.5 is an advanced embodied AI model that improves how machines understand and interact with 3D environments. It enhances spatial reasoning by using depth-aware predictions instead of just 2D images, allowing for more accurate manipulation of objects. Additionally, it introduces a method for estimating the value of actions over time, which helps the AI predict and understand its progress during tasks. These improvements make RoboBrain 2.5 better at performing complex tasks that require precise movements and awareness of its surroundings.'}, 'zh': {'title': 'æå‡å…·èº«æ™ºèƒ½çš„ç²¾ç¡®æ“ä½œèƒ½åŠ›', 'desc': 'RoboBrain 2.5 æ˜¯ä¸€ç§å…ˆè¿›çš„å…·èº«äººå·¥æ™ºèƒ½åŸºç¡€æ¨¡å‹ï¼Œæå‡äº†å…¶åœ¨ä¸‰ç»´ç©ºé—´æ¨ç†å’Œæ—¶é—´ä»·å€¼ä¼°è®¡æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡é«˜è´¨é‡çš„æ—¶ç©ºç›‘ç£è®­ç»ƒï¼Œè¯¥æ¨¡å‹å®ç°äº†ç²¾ç¡®çš„ä¸‰ç»´ç©ºé—´æ¨ç†ï¼Œèƒ½å¤Ÿç”Ÿæˆå®Œæ•´çš„ä¸‰ç»´æ“ä½œè½¨è¿¹ã€‚å®ƒè¿˜å»ºç«‹äº†å¯†é›†çš„æ—¶é—´ä»·å€¼ä¼°è®¡ï¼Œæä¾›äº†å¯¹æ‰§è¡ŒçŠ¶æ€çš„ç†è§£å’Œè¿›åº¦é¢„æµ‹ã€‚è¿™äº›å‡çº§ä½¿å¾—æ¨¡å‹åœ¨å¤æ‚çš„ç²¾ç»†æ“ä½œä¸­æ›´åŠ ç¨³å®šå’Œæ™ºèƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (5)', '#agi', '#alignment', '#architecture (1)', '#audio', '#benchmark (3)', '#cv', '#data', '#dataset (2)', '#diffusion', '#ethics', '#games', '#graphs (1)', '#hallucinations (1)', '#healthcare (1)', '#inference', '#interpretability (1)', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (3)', '#open_source (2)', '#optimization', '#plp', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf', '#robotics (4)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (2)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2026-01-22 03:49',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2026-01-22 03:49')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2026-01-22 03:49')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    