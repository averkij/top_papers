
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. February 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 февраля</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-07.html">⬅️ <span id="prev-date">07.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-11.html">➡️ <span id="next-date">11.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 февраля', 'en': 'February 10', 'zh': '2月10日'};
        let feedDateNext = {'ru': '11.02', 'en': '02/11', 'zh': '2月11日'};
        let feedDatePrev = {'ru': '07.02', 'en': '02/07', 'zh': '2月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.05173', 'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?', 'url': 'https://huggingface.co/papers/2502.05173', 'abstract': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.', 'score': 45, 'issue_id': 2118, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ba284ed1a62b3c2c', 'authors': ['Xilin Wei', 'Xiaoran Liu', 'Yuhang Zang', 'Xiaoyi Dong', 'Pan Zhang', 'Yuhang Cao', 'Jian Tong', 'Haodong Duan', 'Qipeng Guo', 'Jiaqi Wang', 'Xipeng Qiu', 'Dahua Lin'], 'affiliations': ['Fudan University, Shanghai, China', 'Shanghai AI Laboratory, Shanghai, China', 'Shanghai Innovation Institute, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.05173.jpg', 'data': {'categories': ['#hallucinations', '#3d', '#architecture', '#video', '#long_context'], 'emoji': '🎥', 'ru': {'title': 'VideoRoPE: Эффективное позиционное кодирование для глубокого обучения на видео', 'desc': 'Статья представляет VideoRoPE - новый метод позиционного кодирования для видео, основанный на Rotary Position Embedding. Авторы провели анализ и выявили 4 ключевые характеристики для эффективной адаптации RoPE к видео. Они предложили сложную задачу V-NIAH-D для демонстрации недостатков существующих вариантов RoPE. VideoRoPE имеет 3D-структуру, сохраняющую пространственно-временные отношения, и превосходит предыдущие варианты RoPE в различных задачах обработки видео.'}, 'en': {'title': 'Enhancing Video Understanding with VideoRoPE', 'desc': 'This paper addresses the challenge of adapting Rotary Position Embedding (RoPE) for video data, which has a complex spatio-temporal structure. The authors identify four key characteristics necessary for this adaptation and introduce a new task, V-NIAH-D, to highlight the limitations of existing RoPE variants when faced with distractors. They propose VideoRoPE, a 3D structure that effectively maintains spatio-temporal relationships and improves performance by using low-frequency temporal allocation and a diagonal layout. VideoRoPE outperforms previous methods in various video-related tasks, demonstrating its effectiveness in handling long-context video data.'}, 'zh': {'title': 'VideoRoPE：视频中的旋转位置嵌入新突破', 'desc': '本文探讨了如何将旋转位置嵌入（RoPE）有效地扩展到视频数据中。研究分析了四个关键特性，这些特性对于RoPE在视频中的适应性至关重要。我们提出了一个新的任务V-NIAH-D，展示了现有RoPE变体在处理视频时容易受到干扰的缺陷。基于这些分析，我们提出了VideoRoPE，它通过3D结构来保持时空关系，并在多个下游任务中表现优于之前的RoPE变体。'}}}, {'id': 'https://huggingface.co/papers/2502.04507', 'title': 'Fast Video Generation with Sliding Tile Attention', 'url': 'https://huggingface.co/papers/2502.04507', 'abstract': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.', 'score': 34, 'issue_id': 2120, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'dcbf1070dac1b391', 'authors': ['Peiyuan Zhang', 'Yongqi Chen', 'Runlong Su', 'Hangliang Ding', 'Ion Stoica', 'Zhenghong Liu', 'Hao Zhang'], 'affiliations': ['Mohamed bin Zayed University of Artificial Intelligence', 'Tsinghua University', 'University of California, Berkeley', 'University of California, San Diego', 'University of Michigan, Ann Arbor'], 'pdf_title_img': 'assets/pdf/title_img/2502.04507.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации видео с помощью скользящего плиточного внимания', 'desc': 'Статья представляет метод скользящего плиточного внимания (STA) для ускорения генерации видео с помощью диффузионных трансформеров. STA использует наблюдение, что оценки внимания в предобученных моделях диффузии видео в основном концентрируются в локализованных 3D-окнах. Этот подход устраняет избыточность полного внимания, сохраняя выразительность и эффективность на аппаратном уровне. STA ускоряет внимание в 2.8-17 раз по сравнению с FlashAttention-2 и в 1.6-10 раз по сравнению с FlashAttention-3, значительно сокращая время генерации видео.'}, 'en': {'title': 'Efficient Video Generation with Sliding Tile Attention', 'desc': 'This paper presents a new method called sliding tile attention (STA) to improve the efficiency of video generation using Diffusion Transformers (DiTs). Traditional full attention mechanisms are computationally expensive, especially for generating high-resolution videos, leading to long inference times. STA reduces this cost by focusing on localized 3D windows, allowing for faster processing without sacrificing the quality of the generated videos. The implementation of STA achieves significant speedups in attention computation, making it a promising solution for real-time video generation tasks.'}, 'zh': {'title': '滑动瓦片注意力：高效视频生成的新突破', 'desc': '本论文介绍了一种新的滑动瓦片注意力机制（STA），旨在提高视频生成的效率。传统的扩散变换器在生成视频时计算成本高，而STA通过关注局部的时空区域来减少冗余计算。与传统的滑动窗口注意力不同，STA采用了硬件友好的设计，逐块处理，保持了表达能力的同时提高了计算效率。经过优化，STA在视频生成任务中显著加速了注意力计算，降低了延迟，同时不影响生成质量。'}}}, {'id': 'https://huggingface.co/papers/2502.05003', 'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations', 'url': 'https://huggingface.co/papers/2502.05003', 'abstract': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.   We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the "true" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.', 'score': 24, 'issue_id': 2122, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c011c3548ad7a5dd', 'authors': ['Andrei Panferov', 'Jiale Chen', 'Soroush Tabesh', 'Roberto L. Castro', 'Mahdi Nikdan', 'Dan Alistarh'], 'affiliations': ['ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05003.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное квантование для обучения языковых моделей', 'desc': 'Статья представляет новый метод QuEST для обучения больших языковых моделей с использованием квантования. QuEST позволяет обучать модели с весами и активациями в 4 бита или меньше, сохраняя конкурентоспособную точность по сравнению с FP16. Метод улучшает квантование распределений весов и активаций, а также вводит новый оценщик градиента доверия. Эксперименты показывают, что QuEST обеспечивает стабильные законы масштабирования для различных уровней точности.'}, 'en': {'title': 'QuEST: Revolutionizing Quantization for Efficient Language Models', 'desc': 'This paper introduces QuEST, a novel method for training large language models (LLMs) using quantization-aware training (QAT) with significantly reduced bit-widths. QuEST achieves competitive accuracy with traditional FP16 precision while utilizing weights and activations as low as 4 bits, and even supports stable training with 1-bit representations. The method enhances QAT by employing Hadamard normalization for better quantization and a new trust gradient estimator to minimize errors in gradient calculations. Experiments demonstrate that QuEST maintains stable performance across various hardware-supported precisions and can be efficiently executed with provided GPU kernel support.'}, 'zh': {'title': 'QuEST：低精度高效训练大语言模型的创新方法', 'desc': '本文提出了一种名为QuEST的新方法，旨在通过量化感知训练（QAT）来提高大语言模型的训练效率。QuEST能够在4位或更低的精度下训练模型，同时保持与FP16精度相当的准确性。该方法通过改进权重和激活的量化过程，以及引入新的信任梯度估计器，来实现更稳定的训练。实验结果表明，QuEST在各种硬件支持的精度范围内都能实现稳定的扩展性，并且可以有效执行。'}}}, {'id': 'https://huggingface.co/papers/2502.04896', 'title': 'Goku: Flow Based Video Generative Foundation Models', 'url': 'https://huggingface.co/papers/2502.04896', 'abstract': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.', 'score': 23, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ad6ef6eed233cc90', 'authors': ['Shoufa Chen', 'Chongjian Ge', 'Yuqi Zhang', 'Yida Zhang', 'Fengda Zhu', 'Hao Yang', 'Hongxiang Hao', 'Hui Wu', 'Zhichao Lai', 'Yifei Hu', 'Ting-Che Lin', 'Shilong Zhang', 'Fu Li', 'Chuan Li', 'Xing Wang', 'Yanghua Peng', 'Peize Sun', 'Ping Luo', 'Yi Jiang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['Bytedance Inc', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04896.jpg', 'data': {'categories': ['#cv', '#training', '#video', '#architecture', '#data', '#benchmark', '#dataset'], 'emoji': '🐉', 'ru': {'title': 'Goku: Новый уровень генерации изображений и видео', 'desc': 'Статья представляет семейство моделей Goku для совместной генерации изображений и видео. Модели используют трансформеры с выпрямленным потоком для достижения передовых результатов. Авторы описывают ключевые элементы, включая подготовку данных, архитектуру модели и инфраструктуру для эффективного обучения. Goku демонстрирует превосходную производительность в качественных и количественных оценках, устанавливая новые стандарты в основных задачах генерации изображений и видео.'}, 'en': {'title': 'Goku: Revolutionizing Image and Video Generation with Transformers', 'desc': 'This paper presents Goku, a cutting-edge model for generating images and videos using rectified flow Transformers. It discusses key components that contribute to its high-quality output, such as the data curation process and the design of the model architecture. Goku sets new performance records in various tasks, achieving impressive scores on benchmarks for both text-to-image and text-to-video generation. The authors aim to offer insights and advancements that will benefit researchers working on similar generation models.'}, 'zh': {'title': 'Goku：图像与视频生成的新标杆', 'desc': '本文介绍了Goku，这是一种先进的联合图像和视频生成模型，利用了修正流Transformer以实现行业领先的性能。我们详细阐述了高质量视觉生成的基础要素，包括数据整理流程、模型架构设计、流的公式化以及高效稳健的大规模训练基础设施。Goku模型在定性和定量评估中表现优越，为主要任务设定了新的基准。具体而言，Goku在文本到图像生成中达到了0.76的GenEval和83.65的DPG-Bench，在文本到视频任务中达到了84.85的VBench。'}}}, {'id': 'https://huggingface.co/papers/2502.05176', 'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting', 'url': 'https://huggingface.co/papers/2502.05176', 'abstract': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{\\deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{\\deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.', 'score': 22, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '9b52f2788f53c3c0', 'authors': ['Chung-Ho Wu', 'Yang-Jung Chen', 'Ying-Huan Chen', 'Jie-Ying Lee', 'Bo-Hsu Ke', 'Chun-Wei Tuan Mu', 'Yi-Chuan Huang', 'Chin-Yang Lin', 'Min-Hung Chen', 'Yen-Yu Lin', 'Yu-Lun Liu'], 'affiliations': ['NVIDIA', 'National Yang Ming Chiao Tung University'], 'pdf_title_img': 'assets/pdf/title_img/2502.05176.jpg', 'data': {'categories': ['#dataset', '#3d'], 'emoji': '🌐', 'ru': {'title': 'Революция в 3D-реконструкции: AuraFusion360 для безупречного восстановления сцен', 'desc': 'AuraFusion360 - это новый метод восстановления трехмерных сцен на основе Gaussian Splatting. Он использует генерацию масок невидимых областей с учетом глубины, адаптивную диффузию глубины и улучшение деталей на основе SDEdit для создания высококачественных результатов. Метод превосходит существующие подходы по качеству восприятия и геометрической точности при изменении точки обзора. Авторы также представили первый набор данных 360-USID для оценки методов восстановления сцен с охватом 360 градусов.'}, 'en': {'title': 'Revolutionizing 3D Scene Inpainting with AuraFusion360', 'desc': 'AuraFusion360 is a new method for filling in missing parts of 3D scenes, which is important for applications like virtual reality. It uses Gaussian Splatting to represent scenes and introduces techniques for better identifying what should be filled in, ensuring that the filled areas look realistic from different angles. The method also places points accurately without needing extra training and enhances details to keep the views consistent. Additionally, it comes with a new dataset for testing these 3D inpainting methods, showing that AuraFusion360 performs better than previous techniques in both quality and accuracy.'}, 'zh': {'title': 'AuraFusion360：三维场景修复的新突破', 'desc': '三维场景修复在虚拟现实和建筑可视化等应用中非常重要，但现有方法在360度无界场景中面临视图一致性和几何精度的挑战。我们提出了AuraFusion360，这是一种新颖的基于参考的方法，能够在高质量的3D场景中进行物体移除和孔填充。该方法引入了深度感知的未见掩码生成、适应性引导深度扩散和基于SDEdit的细节增强，确保多视图的一致性。通过大量实验，AuraFusion360在感知质量和几何精度方面显著优于现有方法，能够在剧烈视角变化中保持高质量的修复效果。'}}}, {'id': 'https://huggingface.co/papers/2502.05171', 'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach', 'url': 'https://huggingface.co/papers/2502.05171', 'abstract': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.', 'score': 20, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': '4386159312d9856b', 'authors': ['Jonas Geiping', 'Sean McLeish', 'Neel Jain', 'John Kirchenbauer', 'Siddharth Singh', 'Brian R. Bartoldson', 'Bhavya Kailkhura', 'Abhinav Bhatele', 'Tom Goldstein'], 'affiliations': ['ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center', 'Lawrence Livermore National Laboratory', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2502.05171.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Глубокие рассуждения в латентном пространстве: новый подход к языковым моделям', 'desc': 'В статье представлена новая архитектура языковой модели, способная масштабировать вычисления во время тестирования путем неявных рассуждений в латентном пространстве. Модель работает путем итерации рекуррентного блока, разворачиваясь до произвольной глубины во время тестирования. В отличие от подходов, основанных на цепочке рассуждений, этот метод не требует специализированных обучающих данных и может работать с небольшими контекстными окнами. Авторы масштабировали экспериментальную модель до 3,5 миллиардов параметров и 800 миллиардов токенов, показав значительное улучшение производительности на тестах рассуждений.'}, 'en': {'title': 'Scaling Reasoning with Latent Space Computation', 'desc': 'This paper presents a new language model architecture that enhances reasoning capabilities by performing computations in a latent space during test-time. Instead of generating more tokens to scale reasoning, the model uses a recurrent block that allows it to unroll computations to any depth. This method does not rely on specialized training data and can effectively handle small context windows, enabling it to capture complex reasoning patterns. The authors demonstrate that their model, with 3.5 billion parameters, can achieve significant improvements on reasoning tasks, comparable to models with much larger parameter counts.'}, 'zh': {'title': '隐式推理，提升语言模型的计算能力', 'desc': '我们研究了一种新颖的语言模型架构，该架构能够通过在潜在空间中隐式推理来扩展测试时的计算能力。我们的模型通过迭代递归块工作，从而在测试时可以展开到任意深度。这与主流推理模型不同，后者通过生成更多的标记来增加计算量。我们的模型不需要特殊的训练数据，能够处理小的上下文窗口，并且能够捕捉不易用语言表示的推理类型。'}}}, {'id': 'https://huggingface.co/papers/2502.05163', 'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails', 'url': 'https://huggingface.co/papers/2502.05163', 'abstract': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model \\ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.', 'score': 14, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ae863d89ab71ab51', 'authors': ['Yihe Deng', 'Yu Yang', 'Junkai Zhang', 'Wei Wang', 'Bo Li'], 'affiliations': ['University of California, Los Angeles', 'University of Illinois at Urbana-Champaign', 'VirtueAI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05163.jpg', 'data': {'categories': ['#low_resource', '#inference', '#synthetic', '#dataset', '#open_source', '#multilingual', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Улучшение многоязычной безопасности LLM через совместное обучение генератора и ограничителя', 'desc': 'Статья представляет новый подход к созданию многоязычных моделей-ограничителей для обеспечения безопасности больших языковых моделей (LLM). Авторы предлагают framework с двумя игроками на основе обучения с подкреплением, где генератор и модель-ограничитель развиваются совместно для создания синтетических данных. Теоретически это взаимодействие формализовано как игра двух игроков с доказанной сходимостью к равновесию Нэша. Эмпирические оценки показывают, что предложенная модель превосходит современные аналоги, особенно для языков с меньшими ресурсами.'}, 'en': {'title': 'Enhancing Multilingual Safety in LLMs with Synthetic Data Generation', 'desc': "This paper introduces a new method for creating guardrail models that help ensure the safe use of large language models (LLMs) by detecting harmful content in multiple languages. The authors propose a two-player Reinforcement Learning framework where a generator and a guardrail model work together in a competitive manner to create high-quality synthetic safety data. They demonstrate that this approach not only improves performance on English safety benchmarks but also significantly enhances the model's ability to handle lower-resource languages. The results show that their method is faster and more efficient, making it a promising solution for developing multilingual safety measures in LLMs."}, 'zh': {'title': '多语言护栏模型的创新进展', 'desc': '随着大型语言模型（LLMs）的快速发展，确保其负责任使用的护栏模型需求增加，尤其是在检测不安全和非法内容方面。虽然英语的安全数据相对丰富，但由于其他语言开放源代码安全数据的稀缺，多语言护栏建模仍然未被充分探索。为了解决这一问题，我们提出了一种新颖的双玩家强化学习框架，其中生成器和护栏模型对抗性地共同进化，以生成高质量的合成数据用于多语言护栏训练。我们的模型在多语言安全任务中取得了显著进展，特别是在处理低资源语言的不平衡问题上。'}}}, {'id': 'https://huggingface.co/papers/2502.04403', 'title': 'Agency Is Frame-Dependent', 'url': 'https://huggingface.co/papers/2502.04403', 'abstract': "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.", 'score': 11, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '32ceb8df4d77794a', 'authors': ['David Abel', 'André Barreto', 'Michael Bowling', 'Will Dabney', 'Shi Dong', 'Steven Hansen', 'Anna Harutyunyan', 'Khimya Khetarpal', 'Clare Lyle', 'Razvan Pascanu', 'Georgios Piliouras', 'Doina Precup', 'Jonathan Richens', 'Mark Rowland', 'Tom Schaul', 'Satinder Singh'], 'affiliations': ['Amii, University of Alberta', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2502.04403.jpg', 'data': {'categories': ['#rl', '#agi', '#reasoning', '#math'], 'emoji': '🤖', 'ru': {'title': 'Агентность: все зависит от точки зрения', 'desc': 'Статья рассматривает концепцию агентности в контексте обучения с подкреплением. Авторы утверждают, что агентность фундаментально зависит от системы отсчета. Они поддерживают этот тезис, анализируя ключевые свойства агентности, предложенные в предыдущих исследованиях. Статья подчеркивает необходимость учета зависимости от системы отсчета в изучении агентности и обсуждает последствия для обучения с подкреплением.'}, 'en': {'title': 'Agency in Reinforcement Learning: A Frame-Dependent Perspective', 'desc': 'This paper explores the concept of agency in systems, particularly in the context of reinforcement learning. It argues that agency is not an absolute trait but is dependent on the reference frame used to evaluate it. The authors present a philosophical argument showing that key properties of agency are influenced by the perspective from which they are assessed. They conclude that understanding agency in a scientific manner necessitates acknowledging its frame-dependent nature, which has significant implications for the field of reinforcement learning.'}, 'zh': {'title': '能动性：依赖于框架的系统能力', 'desc': '本文探讨了系统的能动性，特别是在强化学习的背景下。能动性是指系统朝着目标引导结果的能力，但判断一个系统是否具备能动性是一个复杂的问题。我们认为，能动性是依赖于参考框架的，任何对系统能动性的测量都必须相对于某个参考框架进行。通过哲学论证，我们支持这一观点，并讨论了这一结论对强化学习的影响。'}}}, {'id': 'https://huggingface.co/papers/2502.04728', 'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language Models', 'url': 'https://huggingface.co/papers/2502.04728', 'abstract': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.', 'score': 9, 'issue_id': 2119, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'cd97668cd0eee0ee', 'authors': ['Zhouliang Yu', 'Yuhuan Yuan', 'Tim Z. Xiao', 'Fuxiang Frank Xia', 'Jie Fu', 'Ge Zhang', 'Ge Lin', 'Weiyang Liu'], 'affiliations': ['Environmental Systems Research Institute, Inc.', 'Max Planck Institute for Intelligent Systems, Tübingen', 'SEED, Bytedance', 'Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2502.04728.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#reasoning', '#data', '#benchmark', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности планирования с помощью PDDL и LLM', 'desc': 'Статья представляет новый подход к решению сложных задач планирования с использованием больших языковых моделей (LLM). Авторы предлагают использовать язык определения планирования (PDDL) для создания точной символической модели мира. Метод включает алгоритм Best-of-N для улучшения качества начального решения и последующее уточнение с помощью вербализованного машинного обучения. Результаты показывают значительное превосходство над существующими методами в генерации доменов PDDL и решении задач планирования высокого уровня.'}, 'en': {'title': 'Enhancing LLMs for Optimal Planning with PDDL', 'desc': "This paper addresses the challenge of using Large Language Models (LLMs) for complex planning problems by introducing a method to generate Planning Domain Definition Language (PDDL) domains. PDDL serves as a formal language that helps in creating precise state descriptions, which is crucial for avoiding rule violations and ensuring optimal planning. The authors propose a novel algorithm that enhances LLMs' reasoning capabilities through a Best-of-N sampling approach, followed by fine-grained refinement using verbalized machine learning techniques. Their approach significantly improves the generation of PDDL domains, achieving over 50% success in generating high-quality plans from natural language descriptions without additional training."}, 'zh': {'title': '利用PDDL提升规划问题解决能力', 'desc': '本论文探讨了如何利用大型语言模型（LLMs）解决复杂的规划问题。为了避免规则违反和确保最优性，研究者们引入了规划领域定义语言（PDDL），作为一种精确的状态描述工具。通过PDDL，可以生成符号世界模型，并应用经典搜索算法（如A*）来寻找最优计划。本文提出了一种简单有效的算法，通过Best-of-N采样和细致的机器学习优化，显著提高了PDDL领域的生成质量，成功率超过50%。'}}}, {'id': 'https://huggingface.co/papers/2502.05179', 'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation', 'url': 'https://huggingface.co/papers/2502.05179', 'abstract': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .', 'score': 8, 'issue_id': 2120, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'c3147244e03af4a6', 'authors': ['Shilong Zhang', 'Wenbo Li', 'Shoufa Chen', 'Chongjian Ge', 'Peize Sun', 'Yida Zhang', 'Yi Jiang', 'Zehuan Yuan', 'Binyue Peng', 'Ping Luo'], 'affiliations': ['ByteDance', 'The Chinese University of Hong Kong', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.05179.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#video', '#diffusion'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео высокого разрешения с предпросмотром', 'desc': 'Статья представляет новый двухэтапный подход к генерации видео на основе текста под названием FlashVideo. На первом этапе модель фокусируется на соответствии промпту, генерируя видео низкого разрешения. Второй этап использует сопоставление потоков для эффективного создания деталей высокого разрешения. Этот метод позволяет достичь высокого качества генерации видео при меньших вычислительных затратах по сравнению с существующими подходами. Кроме того, пользователи могут предварительно просмотреть результат перед полной генерацией, что повышает коммерческую привлекательность технологии.'}, 'en': {'title': 'FlashVideo: Efficient Text-to-Video Generation with Two-Stage Framework', 'desc': 'The paper introduces FlashVideo, a two-stage framework for text-to-video generation that improves efficiency and quality. In the first stage, it focuses on generating low-resolution videos with high fidelity to text prompts, using large model parameters and sufficient function evaluations. The second stage enhances the video by matching flow between low and high resolutions, adding fine details while minimizing computational demands. This approach not only achieves high-resolution outputs with better efficiency but also allows users to preview results before full generation, making it more practical for commercial use.'}, 'zh': {'title': 'FlashVideo：高效生成高分辨率视频的创新框架', 'desc': 'DiT扩散模型在文本到视频生成方面取得了显著成功，但高内容和运动保真度通常需要大量模型参数和函数评估。为了解决这些计算需求，我们提出了一种新的两阶段框架FlashVideo，旨在平衡生成的保真度和质量。在第一阶段，通过低分辨率生成过程优先考虑提示保真度，利用大参数和足够的函数评估提高计算效率。第二阶段则在低分辨率和高分辨率之间建立流匹配，有效生成细节，且所需的函数评估最小化。'}}}, {'id': 'https://huggingface.co/papers/2502.04959', 'title': 'No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces', 'url': 'https://huggingface.co/papers/2502.04959', 'abstract': 'Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging .', 'score': 7, 'issue_id': 2127, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a73679e79ff14b7c', 'authors': ['Daniel Marczak', 'Simone Magistri', 'Sebastian Cygert', 'Bartłomiej Twardowski', 'Andrew D. Bagdanov', 'Joost van de Weijer'], 'affiliations': ['Computer Vision Center, Barcelona, Spain', 'Department of Computer Science, Universitat Autonoma de Barcelona, Spain', 'Department of Information Engineering, University of Florence, Italy', 'Gdansk University of Technology, Poland', 'IDEAS NCBR, Warsaw, Poland', 'Warsaw University of Technology, Poland'], 'pdf_title_img': 'assets/pdf/title_img/2502.04959.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': '🔀', 'ru': {'title': 'Эффективное объединение моделей машинного обучения без дополнительного обучения', 'desc': 'Статья посвящена объединению весов нескольких моделей, обученных на конкретных задачах, в одну многозадачную модель. Авторы исследуют ключевые характеристики матриц задач, которые позволяют эффективно объединять модели. Они предлагают изотропный фреймворк для объединения, который выравнивает спектр сингулярных значений матриц задач и улучшает их выравнивание. Предложенный подход достигает наилучших результатов в различных сценариях, включая разные наборы задач и масштабы моделей.'}, 'en': {'title': 'Bridging the Performance Gap in Model Merging', 'desc': 'This paper focuses on model merging, which combines the weights of different models designed for specific tasks into one model that can handle multiple tasks. The authors identify that there is often a performance gap when comparing the merged model to single-task models. They explore the importance of task matrices, which are weight update matrices, and find that better alignment between the components of these matrices leads to improved performance. To address the performance gap, they introduce an isotropic merging framework that optimizes the alignment of task matrices and incorporates both shared and unique features of tasks, achieving top performance across various scenarios.'}, 'zh': {'title': '有效的模型合并方法提升多任务性能', 'desc': '模型合并是将多个特定任务模型的权重整合为一个多任务模型的过程。尽管这一问题受到越来越多的关注，但合并模型与单任务模型之间仍存在显著的性能差距。本文研究了任务矩阵的关键特性，这些矩阵是应用于预训练模型的权重更新矩阵，发现任务特定矩阵与合并矩阵之间的对齐程度与性能提升密切相关。我们提出了一种各向同性合并框架，通过平坦化任务矩阵的奇异值谱，增强对齐性，从而缩小性能差距，并在多个场景中实现了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2502.04520', 'title': "Linear Correlation in LM's Compositional Generalization and Hallucination", 'url': 'https://huggingface.co/papers/2502.04520', 'abstract': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., "X lives in the city of" rightarrow "X lives in the country of" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.', 'score': 6, 'issue_id': 2119, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '41ef9027d1533f06', 'authors': ['Letian Peng', 'Chenyang An', 'Shibo Hao', 'Chengyu Dong', 'Jingbo Shang'], 'affiliations': ['University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2502.04520.jpg', 'data': {'categories': ['#interpretability', '#training', '#architecture', '#agi', '#data', '#hallucinations'], 'emoji': '🧠', 'ru': {'title': 'Линейность как ключ к обобщению языковых моделей', 'desc': 'Статья исследует феномен линейных корреляций в языковых моделях при композиции знаний. Авторы обнаружили, что существует линейное преобразование между связанными знаниями, которое отображает логиты предсказания следующего токена от одного промпта к другому. Это явление устойчиво к масштабному дообучению и может служить потенциальным идентификатором обобщения языковой модели. Исследование показывает, что такие линейные корреляции могут быть изучены с помощью одной полносвязной нейронной сети и предобученных представлений словаря.'}, 'en': {'title': 'Unlocking Language Models: The Power of Linear Correlations in Knowledge Composition', 'desc': 'This paper explores how language models (LMs) handle knowledge composition, revealing that they exhibit linear correlations when predicting the next token. It shows that there is a linear transformation that connects related knowledge, allowing LMs to generalize information effectively, similar to how humans relate concepts. The study finds that while these linear transformations can adapt to new information through fine-tuning, they can also lead to incorrect outputs, or hallucinations, when the relationships are not aligned with reality. Overall, the research suggests that understanding these linear correlations can help identify how well LMs generalize knowledge.'}, 'zh': {'title': '语言模型的线性相关性与知识组合', 'desc': '这篇论文探讨了语言模型（LM）在知识组合中的线性相关现象。研究发现，某些相关知识之间存在线性变换，这种变换可以将一个提示的下一个标记预测从一个映射到另一个。比如，从"X 住在城市"可以转变为"X 住在国家"。结果表明，线性变换在大规模微调中具有韧性，并且当与现实世界关系一致时能够推广更新的知识，但当偏离时则会导致幻觉。'}}}, {'id': 'https://huggingface.co/papers/2502.04416', 'title': 'CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference', 'url': 'https://huggingface.co/papers/2502.04416', 'abstract': 'Large language models (LLMs) achieve impressive performance by scaling model parameters, but this comes with significant inference overhead. Feed-forward networks (FFNs), which dominate LLM parameters, exhibit high activation sparsity in hidden neurons. To exploit this, researchers have proposed using a mixture-of-experts (MoE) architecture, where only a subset of parameters is activated. However, existing approaches often require extensive training data and resources, limiting their practicality. We propose CMoE (Carved MoE), a novel framework to efficiently carve MoE models from dense models. CMoE achieves remarkable performance through efficient expert grouping and lightweight adaptation. First, neurons are grouped into shared and routed experts based on activation rates. Next, we construct a routing mechanism without training from scratch, incorporating a differentiable routing process and load balancing. Using modest data, CMoE produces a well-designed, usable MoE from a 7B dense model within five minutes. With lightweight fine-tuning, it achieves high-performance recovery in under an hour. We make our code publicly available at https://github.com/JarvisPei/CMoE.', 'score': 5, 'issue_id': 2125, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': '0b4520b0860c835c', 'authors': ['Zehua Pei', 'Lancheng Zou', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2502.04416.jpg', 'data': {'categories': ['#architecture', '#optimization', '#open_source', '#training', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Эффективное превращение плотных LLM в разреженные MoE модели', 'desc': 'Статья представляет новый метод CMoE (Carved MoE) для эффективного создания моделей Mixture-of-Experts из плотных моделей больших языковых моделей (LLM). CMoE группирует нейроны в общие и маршрутизируемые экспертные группы на основе уровней активации. Метод включает механизм маршрутизации с дифференцируемым процессом и балансировкой нагрузки. CMoE позволяет быстро создать эффективную MoE модель из плотной модели размером 7 миллиардов параметров, используя небольшой объем данных.'}, 'en': {'title': 'Efficiently Carving Mixture-of-Experts for Large Language Models', 'desc': 'This paper introduces CMoE (Carved Mixture-of-Experts), a new framework designed to enhance the efficiency of large language models (LLMs) by utilizing a mixture-of-experts architecture. CMoE takes advantage of the high activation sparsity found in feed-forward networks by activating only a subset of parameters, which reduces inference overhead. The framework groups neurons into shared and routed experts based on their activation rates and employs a differentiable routing mechanism that avoids the need for extensive retraining. Remarkably, CMoE can create a functional MoE model from a dense model in just five minutes using limited data, achieving high performance with minimal fine-tuning.'}, 'zh': {'title': '高效雕刻混合专家模型的创新框架', 'desc': '大型语言模型（LLMs）通过增加模型参数实现了出色的性能，但这也带来了显著的推理开销。前馈网络（FFNs）在LLM参数中占主导地位，隐藏神经元的激活稀疏性很高。为此，研究人员提出了混合专家（MoE）架构，仅激活一部分参数。然而，现有方法通常需要大量的训练数据和资源，限制了其实用性。我们提出了CMoE（Carved MoE），一个新颖的框架，可以高效地从稠密模型中雕刻出MoE模型。'}}}, {'id': 'https://huggingface.co/papers/2502.03738', 'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More', 'url': 'https://huggingface.co/papers/2502.03738', 'abstract': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.', 'score': 5, 'issue_id': 2122, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'aa76478090a36c04', 'authors': ['Feng Wang', 'Yaodong Yu', 'Guoyizhe Wei', 'Wei Shao', 'Yuyin Zhou', 'Alan Yuille', 'Cihang Xie'], 'affiliations': ['Johns Hopkins University', 'UC Berkeley', 'UC Santa Cruz', 'University of Florida'], 'pdf_title_img': 'assets/pdf/title_img/2502.03738.jpg', 'data': {'categories': ['#benchmark', '#architecture', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Пиксельная токенизация превосходит патчи в vision-моделях', 'desc': 'Исследование посвящено анализу влияния размера патчей в Vision Transformer (ViT) на качество распознавания изображений. Авторы обнаружили, что уменьшение размера патчей до 1x1 пикселя (пиксельная токенизация) улучшает предсказательную способность модели. Этот эффект наблюдается для различных задач компьютерного зрения, масштабов входных данных и архитектур, включая ViT и Mamba. Эксперименты показали, что модель базового размера с длиной визуальной последовательности 50 176 токенов достигает точности 84,6% на наборе данных ImageNet-1k.'}, 'en': {'title': 'Unlocking Visual Potential: The Power of Smaller Patches in Vision Transformers', 'desc': 'This paper investigates the impact of patchification, a method of dividing images into smaller sections, on the performance of Vision Transformers (ViT) in visual tasks. The authors find that reducing the size of these patches leads to better predictive performance, with the optimal size being 1x1 pixels, which represents pixel tokenization. They conduct experiments that demonstrate this scaling law across various architectures and tasks, revealing that smaller patches diminish the need for complex decoder heads in dense prediction tasks. The study achieves a significant milestone by processing a visual sequence of 50,176 tokens, achieving a competitive accuracy on the ImageNet-1k benchmark, and aims to inform future developments in non-compressive vision models.'}, 'zh': {'title': '小块更优，视觉理解更强！', 'desc': '本文研究了视觉变换器（ViT）中图像分块（patchification）对信息损失的影响。通过缩小图像的空间大小，分块方法可以有效减少令牌序列的长度，从而降低计算成本。我们发现，随着分块大小的减小，模型的预测性能持续提高，直到达到最小的1x1像素分块。该研究结果适用于多种视觉任务和不同的模型架构，为未来构建非压缩视觉模型提供了理论基础。'}}}, {'id': 'https://huggingface.co/papers/2502.04363', 'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices', 'url': 'https://huggingface.co/papers/2502.04363', 'abstract': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.', 'score': 5, 'issue_id': 2119, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '339b45dee733174c', 'authors': ['Bosung Kim', 'Kyuhwan Lee', 'Isu Jeong', 'Jungmin Cheon', 'Yeojin Lee', 'Seulki Lee'], 'affiliations': ['Ulsan National Institute of Science and Technology South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2502.04363.jpg', 'data': {'categories': ['#inference', '#video', '#open_source', '#diffusion', '#architecture', '#low_resource'], 'emoji': '📱', 'ru': {'title': 'Генерация видео по тексту прямо на вашем смартфоне', 'desc': 'On-device Sora представляет собой инновационное решение для генерации видео на основе текста с использованием диффузионных моделей, работающее на смартфонах. Система применяет три новые техники: Linear Proportional Leap (LPL) для сокращения шагов денойзинга, Temporal Dimension Token Merging (TDTM) для оптимизации вычислений в слоях внимания, и Concurrent Inference with Dynamic Loading (CI-DL) для эффективного использования ограниченной памяти устройства. Эксперименты на iPhone 15 Pro показали, что On-device Sora способна генерировать видео высокого качества, сравнимые с результатами Open-Sora на мощных GPU.'}, 'en': {'title': 'Empowering Video Creation on Your Smartphone!', 'desc': 'On-device Sora is a groundbreaking solution for generating videos from text using diffusion models directly on smartphones. It introduces three innovative techniques: Linear Proportional Leap (LPL) to reduce denoising steps, Temporal Dimension Token Merging (TDTM) to optimize token processing in attention layers, and Concurrent Inference with Dynamic Loading (CI-DL) to manage memory efficiently. These advancements allow the system to produce high-quality videos comparable to those generated by powerful GPUs, all while operating within the constraints of mobile devices. This work not only enhances accessibility to advanced generative technologies but also prioritizes user privacy and reduces reliance on cloud services.'}, 'zh': {'title': '移动设备上的高效视频生成新突破', 'desc': '我们提出了On-device Sora，这是首个基于扩散模型的移动设备文本到视频生成解决方案，能够高效地在智能手机上运行。该系统采用了三种新技术来解决移动设备在计算和内存方面的限制。首先，线性比例跳跃（LPL）通过高效的跳跃方法减少了视频扩散中所需的去噪步骤。其次，时间维度令牌合并（TDTM）通过沿时间维度合并连续令牌，降低了注意力层中密集的令牌处理计算。'}}}, {'id': 'https://huggingface.co/papers/2502.04404', 'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models', 'url': 'https://huggingface.co/papers/2502.04404', 'abstract': "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.", 'score': 5, 'issue_id': 2118, 'pub_date': '2025-02-06', 'pub_date_card': {'ru': '6 февраля', 'en': 'February 6', 'zh': '2月6日'}, 'hash': 'a7bd201755c7ea1d', 'authors': ['Xiao-Wen Yang', 'Xuan-Yi Zhu', 'Wen-Da Wei', 'Ding-Chu Zhang', 'Jie-Jing Shao', 'Zhi Zhou', 'Lan-Zhe Guo', 'Yu-Feng Li'], 'affiliations': ['National Key Laboratory for Novel Software Technology, Nanjing University, China', 'School of Artificial Intelligence, Nanjing University, China', 'School of Intelligence Science and Technology, Nanjing University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04404.jpg', 'data': {'categories': ['#agi', '#training', '#inference', '#agents', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Самостоятельный возврат: путь к более разумным ИИ', 'desc': 'Статья представляет новый механизм самостоятельного возврата (self-backtracking) для больших языковых моделей (LLM). Этот механизм позволяет LLM автономно определять, когда и где нужно вернуться назад в процессе рассуждений. Авторы утверждают, что это улучшает способности LLM к рассуждению и повышает эффективность, превращая медленное мышление в быстрое через самосовершенствование. Эмпирические оценки показывают значительное улучшение возможностей рассуждения LLM с использованием этого подхода.'}, 'en': {'title': 'Empowering LLMs with Self-Backtracking for Enhanced Reasoning', 'desc': 'This paper discusses how adding slow-thinking processes to large language models (LLMs) can help them become better at reasoning, moving towards Level 2 AGI. It identifies problems like inefficient overthinking and dependence on external reward systems as obstacles to effective reasoning. The authors propose a self-backtracking mechanism that allows LLMs to independently decide when to revisit previous decisions, improving their reasoning and efficiency. Their experiments show that this approach significantly boosts LLM performance, achieving over a 40% improvement compared to traditional training methods.'}, 'zh': {'title': '自我回溯机制：提升语言模型推理能力的关键', 'desc': '将慢思考机制整合到大型语言模型（LLMs）中，为实现二级AGI推理器提供了有希望的途径。当前的挑战包括低效的过度思考和对辅助奖励模型的过度依赖。我们指出，这些限制源于LLMs无法内化搜索过程，而搜索过程是有效推理的关键组成部分。我们提出了一种自我回溯机制，使LLMs能够在训练和推理过程中自主决定何时以及如何回溯，从而显著提升推理能力和效率。'}}}, {'id': 'https://huggingface.co/papers/2502.05092', 'title': 'Lost in Time: Clock and Calendar Understanding Challenges in Multimodal LLMs', 'url': 'https://huggingface.co/papers/2502.05092', 'abstract': "Understanding time from visual representations is a fundamental cognitive skill, yet it remains a challenge for multimodal large language models (MLLMs). In this work, we investigate the capabilities of MLLMs in interpreting time and date through analogue clocks and yearly calendars. To facilitate this, we curated a structured dataset comprising two subsets: 1) ClockQA, which comprises various types of clock styles-standard, black-dial, no-second-hand, Roman numeral, and arrow-hand clocks-paired with time related questions; and 2) CalendarQA, which consists of yearly calendar images with questions ranging from commonly known dates (e.g., Christmas, New Year's Day) to computationally derived ones (e.g., the 100th or 153rd day of the year). We aim to analyse how MLLMs can perform visual recognition, numerical reasoning, and temporal inference when presented with time-related visual data. Our evaluations show that despite recent advancements, reliably understanding time remains a significant challenge for MLLMs.", 'score': 4, 'issue_id': 2128, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'ea9f14d34d4cbb60', 'authors': ['Rohit Saxena', 'Aryo Pradipta Gema', 'Pasquale Minervini'], 'affiliations': ['ILCC, School of Informatics, University of Edinburgh', 'Miniml.AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.05092.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#multimodal', '#cv', '#interpretability'], 'emoji': '⏰', 'ru': {'title': 'Время бросает вызов искусственному интеллекту', 'desc': 'Статья исследует способности мультимодальных больших языковых моделей (MLLM) в интерпретации времени и дат через аналоговые часы и календари. Авторы создали структурированный набор данных, включающий ClockQA с различными стилями часов и CalendarQA с календарными изображениями и вопросами. Цель исследования - проанализировать, как MLLM выполняют визуальное распознавание, числовые рассуждения и временные выводы. Результаты показывают, что надежное понимание времени остается значительной проблемой для MLLM, несмотря на недавние достижения.'}, 'en': {'title': 'Unlocking Time: Challenges for Multimodal Language Models', 'desc': "This paper explores how multimodal large language models (MLLMs) understand time through visual aids like clocks and calendars. The authors created a dataset with two parts: ClockQA, which includes different clock styles and related questions, and CalendarQA, featuring yearly calendars with various date-related queries. The study assesses the models' abilities in visual recognition, numerical reasoning, and temporal inference when interpreting these time-related images. Despite progress in MLLMs, the findings indicate that accurately grasping time concepts continues to be a major hurdle."}, 'zh': {'title': '理解时间的挑战：多模态语言模型的局限性', 'desc': '本研究探讨了多模态大型语言模型（MLLMs）在理解时间和日期方面的能力，特别是通过模拟时钟和年度日历的视觉表示。我们创建了一个结构化的数据集，包括两部分：ClockQA，包含不同风格的时钟及相关时间问题；CalendarQA，包含年度日历图像及常见日期问题。我们的目标是分析MLLMs在处理与时间相关的视觉数据时的视觉识别、数值推理和时间推断能力。尽管最近取得了一些进展，但MLLMs在可靠理解时间方面仍面临重大挑战。'}}}, {'id': 'https://huggingface.co/papers/2502.05178', 'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2502.05178', 'abstract': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.', 'score': 4, 'issue_id': 2121, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'bbc1f1a0b7f5423f', 'authors': ['Yue Zhao', 'Fuzhao Xue', 'Scott Reed', 'Linxi Fan', 'Yuke Zhu', 'Jan Kautz', 'Zhiding Yu', 'Philipp Krähenbühl', 'De-An Huang'], 'affiliations': ['NVIDIA', 'UT Austin'], 'pdf_title_img': 'assets/pdf/title_img/2502.05178.jpg', 'data': {'categories': ['#multimodal', '#cv', '#training', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'QLIP: Единый подход к пониманию и генерации изображений', 'desc': 'QLIP - это метод визуальной токенизации, объединяющий высококачественную реконструкцию изображений с пониманием изображений без предварительного обучения. Он использует автоэнкодер с бинарно-сферическим квантованием и оптимизирует одновременно реконструкцию и выравнивание языка и изображений. Авторы предлагают двухэтапный процесс обучения для эффективного сочетания требований к большим батчам и ограничений по памяти. QLIP может заменить визуальный энкодер в мультимодальных моделях и токенизатор изображений в генеративных моделях, показывая сопоставимую или лучшую производительность.'}, 'en': {'title': 'QLIP: Bridging Visual and Language Understanding with Efficient Tokenization', 'desc': 'The paper presents Quantized Language-Image Pretraining (QLIP), a novel method for visual tokenization that achieves high-quality image reconstruction while enhancing zero-shot image understanding. QLIP employs a binary-spherical-quantization-based autoencoder, integrating both reconstruction and language-image alignment objectives in a harmonious manner. By dynamically balancing these objectives during training, the method effectively addresses the challenges of large-batch image-language pre-training and memory constraints. The results show that QLIP can replace existing visual encoders and image tokenizers in models like LLaVA and LlamaGen, improving performance in multimodal understanding and text-conditioned image generation.'}, 'zh': {'title': '量化语言-图像预训练：多模态理解的新突破', 'desc': '我们介绍了一种量化语言-图像预训练方法（QLIP），这是一种视觉标记化方法，结合了最先进的重建质量和零-shot图像理解能力。QLIP使用基于二元球面量化的自编码器进行训练，同时优化重建和语言-图像对齐目标。我们首次证明这两个目标可以动态平衡，而不是相互对立。QLIP在多模态理解和文本条件图像生成方面表现出色，可以作为LLaVA的视觉编码器和LlamaGen的图像标记器的替代方案，性能相当或更好。'}}}, {'id': 'https://huggingface.co/papers/2502.04350', 'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance', 'url': 'https://huggingface.co/papers/2502.04350', 'abstract': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.', 'score': 4, 'issue_id': 2118, 'pub_date': '2025-02-04', 'pub_date_card': {'ru': '4 февраля', 'en': 'February 4', 'zh': '2月4日'}, 'hash': 'ad7829c09c28de41', 'authors': ['Yongchao Chen', 'Yilun Hao', 'Yueying Liu', 'Yang Zhang', 'Chuchu Fan'], 'affiliations': ['Harvard University, Boston, MA, USA', 'MIT-IBM Watson AI Lab, Boston, MA, USA', 'Massachusetts Institute of Technology, Boston, MA, USA', 'University of Illinois Urbana-Champaign, Urbana, IL, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.04350.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#rlhf', '#open_source', '#optimization', '#reasoning'], 'emoji': '🧭', 'ru': {'title': 'CodeSteer: Умное управление для раскрытия потенциала LLM в символьных вычислениях', 'desc': 'CodeSteer - это новый метод для эффективного управления генерацией кода и текста в больших языковых моделях (LLM). Исследователи создали комплексный бенчмарк SymBench и синтезировали наборы данных для обучения модели. Они дообучили модель Llama-3-8B с использованием многораундового обучения с учителем и прямой оптимизации предпочтений. Результирующая модель CodeSteerLLM значительно улучшает производительность других LLM в задачах символьных вычислений, превосходя даже лучшие существующие модели.'}, 'en': {'title': 'CodeSteer: Guiding LLMs to Master Code and Reasoning!', 'desc': 'This paper presents CodeSteer, a novel method designed to enhance the performance of Large Language Models (LLMs) in both textual reasoning and code generation. It introduces a benchmark called SymBench, which includes 37 symbolic tasks of varying complexity, and provides extensive datasets for training and evaluation. The authors fine-tune the Llama-3-8B model using multi-round supervised fine-tuning and direct preference optimization, resulting in the CodeSteerLLM. This model significantly improves the performance of existing LLMs, demonstrating a remarkable ability to leverage symbolic computing for complex tasks.'}, 'zh': {'title': 'CodeSteer：引导LLM实现符号计算的突破', 'desc': '现有的方法无法有效引导大型语言模型（LLMs）在文本推理和代码生成之间切换，导致符号计算能力未得到充分利用。我们提出了一种名为CodeSteer的方法，能够有效指导LLM的代码和文本生成。我们构建了一个全面的基准SymBench，包含37个具有可调复杂度的符号任务，并合成了包含1.2万多轮指导/生成轨迹和5500对指导比较的数据集。通过对Llama-3-8B模型进行多轮监督微调（SFT）和直接偏好优化（DPO），我们得到的CodeSteerLLM模型能够有效引导更大模型的代码/文本生成。'}}}, {'id': 'https://huggingface.co/papers/2502.03512', 'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment', 'url': 'https://huggingface.co/papers/2502.03512', 'abstract': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.   We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.', 'score': 3, 'issue_id': 2122, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': 'ebf6482c46f8fe2f', 'authors': ['Amitava Das', 'Yaswanth Narsupalli', 'Gurpreet Singh', 'Vinija Jain', 'Vasu Sharma', 'Suranjana Trivedy', 'Aman Chadha', 'Amit Sheth'], 'affiliations': ['Amazon AI, USA', 'Artificial Intelligence Institute, University of South Carolina, USA', 'Meta AI, USA'], 'pdf_title_img': 'assets/pdf/title_img/2502.03512.jpg', 'data': {'categories': ['#benchmark', '#rag', '#rlhf', '#ethics', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'Баланс противоречий в генерации изображений: YinYangAlign на страже точности', 'desc': 'Статья представляет YinYangAlign - передовую систему оценки для измерения точности выравнивания текстово-изобразительных (T2I) моделей. Она фокусируется на шести фундаментальных и противоречивых целях дизайна, отражающих ключевые напряжения в генерации изображений. YinYangAlign включает наборы данных с человеческими запросами, выровненными и невыровненными ответами ИИ, а также объяснениями противоречий. Система направлена на улучшение надежности и точности генерации изображений, применяя методы выравнивания, успешно используемые в больших языковых моделях.'}, 'en': {'title': 'Enhancing Image Generation Fidelity with YinYangAlign', 'desc': 'This paper discusses the importance of precise alignment in Text-to-Image (T2I) systems to ensure that generated images meet user expectations and ethical standards. It highlights past failures, like the Google Gemini incident, which demonstrate the need for better alignment strategies. The authors propose YinYangAlign, a new benchmarking framework that evaluates T2I systems based on six conflicting design goals, such as user prompt adherence versus creative freedom. By using detailed datasets that include human prompts and AI outputs, YinYangAlign aims to improve the fidelity and reliability of image generation.'}, 'zh': {'title': '提升文本到图像系统的对齐精度', 'desc': '本文探讨了文本到图像（T2I）系统中精确对齐的重要性，以确保生成的图像既能准确反映用户意图，又符合伦理和美学标准。研究表明，像Google Gemini这样的事件凸显了强大对齐机制的必要性。与此相比，大型语言模型（LLMs）在对齐方面取得了显著成功，研究人员希望将类似的对齐技术应用于T2I系统，以提高图像生成的准确性和可靠性。我们提出了YinYangAlign，这是一个先进的基准框架，系统地量化T2I系统的对齐保真度，解决了六个基本且内在矛盾的设计目标。'}}}, {'id': 'https://huggingface.co/papers/2502.04689', 'title': 'ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning', 'url': 'https://huggingface.co/papers/2502.04689', 'abstract': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance ("think step by step"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.', 'score': 2, 'issue_id': 2123, 'pub_date': '2025-02-07', 'pub_date_card': {'ru': '7 февраля', 'en': 'February 7', 'zh': '2月7日'}, 'hash': 'a052e3be1fe147cd', 'authors': ['Yuwei Yin', 'Giuseppe Carenini'], 'affiliations': ['University of British Columbia'], 'pdf_title_img': 'assets/pdf/title_img/2502.04689.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ARR: Новый шаг в улучшении рассуждений языковых моделей', 'desc': 'Статья представляет новый метод промптинга для больших языковых моделей под названием ARR. Этот метод включает три ключевых шага: анализ намерения вопроса, извлечение релевантной информации и пошаговое рассуждение. ARR показывает лучшие результаты по сравнению с базовым подходом и методом Chain-of-Thought на различных задачах вопросно-ответного типа. Эксперименты подтверждают эффективность и обобщаемость ARR для разных размеров и типов языковых моделей.'}, 'en': {'title': 'ARR: A Structured Approach to Boost LLM Reasoning', 'desc': "This paper presents a new prompting method called ARR for large language models (LLMs) that enhances their performance on multiple-choice question-answering tasks. ARR stands for Analyze, Retrieve, and Reason, which are the three critical steps it incorporates to improve reasoning capabilities. The method outperforms traditional zero-shot Chain-of-Thought prompting by providing clearer guidance on how to approach questions. Experiments show that each component of ARR contributes positively to the overall effectiveness, especially the analysis of the question's intent."}, 'zh': {'title': 'ARR：提升问答推理的新方法', 'desc': '本文介绍了一种新的零-shot提示方法ARR，旨在提高大型语言模型（LLMs）在多项选择问答任务中的推理能力。ARR明确包含三个关键步骤：分析问题意图、检索相关信息和逐步推理。通过在多种复杂问答任务上的实验，ARR consistently outperform了传统的基线方法和Chain-of-Thought（CoT）提示。研究表明，意图分析在ARR中起着至关重要的作用，进一步验证了每个组成部分的积极贡献。'}}}, {'id': 'https://huggingface.co/papers/2502.04376', 'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf', 'url': 'https://huggingface.co/papers/2502.04376', 'abstract': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60\\% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.', 'score': 1, 'issue_id': 2121, 'pub_date': '2025-02-05', 'pub_date_card': {'ru': '5 февраля', 'en': 'February 5', 'zh': '2月5日'}, 'hash': '049ab6200a9d2eae', 'authors': ['Lingxiang Hu', 'Shurun Yuan', 'Xiaoting Qin', 'Jue Zhang', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['Microsoft', 'Northeastern University, China', 'Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2502.04376.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#agi', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'LLM как виртуальные делегаты: будущее эффективных совещаний', 'desc': 'Исследователи разработали прототип системы делегирования участия в совещаниях на основе больших языковых моделей (LLM). Они создали комплексный бенчмарк, используя реальные стенограммы совещаний, для оценки эффективности различных LLM в роли делегатов. Результаты показали, что около 60% ответов моделей затрагивают как минимум один ключевой момент из эталонных данных, но требуется улучшение в снижении нерелевантного контента и повышении устойчивости к ошибкам транскрипции. Исследование подчеркивает потенциал и проблемы использования LLM в качестве делегатов на совещаниях, предлагая ценные выводы для их практического применения.'}, 'en': {'title': 'Empowering Meetings with LLMs: Balancing Engagement and Efficiency', 'desc': 'This paper investigates the use of Large Language Models (LLMs) as meeting delegates to improve the efficiency of workplace meetings. A prototype system was developed and evaluated using real meeting transcripts, revealing that different LLMs exhibit varying engagement strategies during discussions. The results indicate that while some models effectively address key points, there is a need for improvement in handling irrelevant content and transcription errors. The study highlights both the potential benefits and challenges of integrating LLMs into meeting processes, providing insights for future applications.'}, 'zh': {'title': '利用大型语言模型优化会议参与', 'desc': '在现代工作场所，会议是交流思想和确保团队一致性的重要环节，但常常面临时间消耗、日程冲突和参与效率低下等挑战。本文探讨了大型语言模型（LLMs）在会议中有效分配参与者的能力，开发了一个基于LLM的会议代理系统原型，并使用真实会议记录创建了全面的基准测试。评估结果显示，GPT-4/4o在积极和谨慎的参与策略之间保持了平衡，而Gemini 1.5 Pro则更倾向于谨慎，Gemini 1.5 Flash和Llama3-8B/70B则表现出更积极的倾向。尽管约60%的回应涵盖了至少一个关键点，但仍需改进以减少无关或重复内容，并提高对真实场景中转录错误的容忍度。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi (4)', '#alignment (1)', '#architecture (14)', '#audio', '#benchmark (8)', '#cv (4)', '#data (3)', '#dataset (6)', '#diffusion (3)', '#ethics (1)', '#games', '#graphs', '#hallucinations (2)', '#healthcare', '#inference (6)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (4)', '#open_source (4)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (2)', '#rlhf (2)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (13)', '#transfer_learning', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-02-10 17:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-10 17:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-10 17:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    