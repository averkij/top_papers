
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. July 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 Ğ¸ÑĞ»Ñ</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-24.html">â¬…ï¸ <span id="prev-date">24.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-28.html">â¡ï¸ <span id="next-date">28.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 Ğ¸ÑĞ»Ñ', 'en': 'July 25', 'zh': '7æœˆ25æ—¥'};
        let feedDateNext = {'ru': '28.07', 'en': '07/28', 'zh': '7æœˆ28æ—¥'};
        let feedDatePrev = {'ru': '24.07', 'en': '07/24', 'zh': '7æœˆ24æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.13546', 'title': 'nablaNABLA: Neighborhood Adaptive Block-Level Attention', 'url': 'https://huggingface.co/papers/2507.13546', 'abstract': "NABLA, a dynamic block-level attention mechanism, improves video diffusion transformers by enhancing computational efficiency without sacrificing generative quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: https://github.com/gen-ai-team/Wan2.1-NABLA", 'score': 57, 'issue_id': 5019, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '4a4705d5ba2d25e6', 'authors': ['Dmitrii Mikhailov', 'Aleksey Letunovskiy', 'Maria Kovaleva', 'Vladimir Arkhipkin', 'Vladimir Korviakov', 'Vladimir Polovnikov', 'Viacheslav Vasilev', 'Evelina Sidorova', 'Denis Dimitrov'], 'affiliations': ['Artificial Intelligence Research Institute (AIRI), Moscow, Russia', 'Lomonosov Moscow State University (MSU), Moscow, Russia', 'Moscow Institute of Physics and Technology (MIPT), Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.13546.jpg', 'data': {'categories': ['#open_source', '#training', '#video', '#optimization', '#diffusion', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'NABLA: Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'NABLA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ². ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. NABLA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ»ĞµĞ³ĞºĞ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 2.7 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'NABLA: Speeding Up Video Generation with Smart Attention', 'desc': 'This paper introduces NABLA, a new attention mechanism designed to enhance video diffusion transformers by improving their computational efficiency. NABLA utilizes a Neighborhood Adaptive Block-Level Attention approach that adjusts to the sparsity patterns found in video data, allowing for faster processing without losing quality. The method significantly reduces the computational burden associated with traditional full attention mechanisms, especially for high-resolution and lengthy video sequences. Experimental results show that NABLA can accelerate training and inference times by up to 2.7 times while maintaining high generative quality as measured by various evaluation metrics.'}, 'zh': {'title': 'NABLAï¼šæå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡çš„åˆ›æ–°æœºåˆ¶', 'desc': 'NABLAæ˜¯ä¸€ç§åŠ¨æ€å—çº§æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„è®¡ç®—æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè´¨é‡ã€‚ä¼ ç»Ÿçš„å…¨æ³¨æ„åŠ›æœºåˆ¶åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å’Œé•¿æ—¶åºè§†é¢‘æ—¶ï¼Œè®¡ç®—å¤æ‚åº¦è¿‡é«˜ï¼Œæˆä¸ºç“¶é¢ˆã€‚NABLAé€šè¿‡é‚»åŸŸè‡ªé€‚åº”å—çº§æ³¨æ„åŠ›ï¼ŒåŠ¨æ€é€‚åº”è§†é¢‘ä¸­çš„ç¨€ç–æ¨¡å¼ï¼Œä»è€Œå‡å°‘è®¡ç®—å¼€é”€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNABLAåœ¨è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ä¸Šæ¯”åŸºçº¿å¿«2.7å€ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“ç”Ÿæˆè´¨é‡å’Œè¯„ä¼°æŒ‡æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18071', 'title': 'Group Sequence Policy Optimization', 'url': 'https://huggingface.co/papers/2507.18071', 'abstract': 'This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.', 'score': 42, 'issue_id': 5014, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '0eb25e9d4ec9527e', 'authors': ['Chujie Zheng', 'Shixuan Liu', 'Mingze Li', 'Xiong-Hui Chen', 'Bowen Yu', 'Chang Gao', 'Kai Dang', 'Yuqiong Liu', 'Rui Men', 'An Yang', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2507.18071.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#games'], 'emoji': 'ğŸš€', 'ru': {'title': 'GSPO: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ - Group Sequence Policy Optimization (GSPO). Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², GSPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ GSPO Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ GRPO Ğ¿Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, GSPO ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Mixture-of-Experts Ğ¸ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ RL.'}, 'en': {'title': 'Revolutionizing RL Training with GSPO for Language Models', 'desc': 'This paper presents Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm designed for training large language models. GSPO improves upon previous methods by using sequence likelihood to define importance ratios, rather than focusing on individual tokens. It also incorporates sequence-level clipping and optimization, which enhances training stability and efficiency. The results show that GSPO outperforms the GRPO algorithm and significantly benefits the training of Mixture-of-Experts models, leading to advancements in the Qwen3 models.'}, 'zh': {'title': 'ç¾¤åºåˆ—ç­–ç•¥ä¼˜åŒ–ï¼šæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡çš„åˆ›æ–°ç®—æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œç§°ä¸ºç¾¤åºåˆ—ç­–ç•¥ä¼˜åŒ–ï¼ˆGSPOï¼‰ï¼Œæ—¨åœ¨é«˜æ•ˆä¸”ç¨³å®šåœ°è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä¸ä»¥å¾€åŸºäºä»¤ç‰Œçº§é‡è¦æ€§æ¯”ç‡çš„ç®—æ³•ä¸åŒï¼ŒGSPOåŸºäºåºåˆ—çš„ä¼¼ç„¶æ€§å®šä¹‰é‡è¦æ€§æ¯”ç‡ï¼Œå¹¶è¿›è¡Œåºåˆ—çº§çš„è£å‰ªã€å¥–åŠ±å’Œä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒGSPOåœ¨è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºGRPOç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶æœ‰æ½œåŠ›ç®€åŒ–å¼ºåŒ–å­¦ä¹ åŸºç¡€è®¾æ–½çš„è®¾è®¡ã€‚GSPOçš„è¿™äº›ä¼˜ç‚¹ä¸ºæœ€æ–°çš„Qwen3æ¨¡å‹å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.14958', 'title': 'MUR: Momentum Uncertainty guided Reasoning for Large Language Models', 'url': 'https://huggingface.co/papers/2507.14958', 'abstract': 'Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinking, wasting tokens on redundant computations. This work investigates how to efficiently and adaptively guide LLM test-time scaling without additional training. Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating stepwise uncertainty over time. To support flexible inference-time control, we introduce gamma-control, a simple mechanism that tunes the reasoning budget via a single hyperparameter. We provide in-depth theoretical proof to support the superiority of MUR in terms of stability and biases. MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%.', 'score': 27, 'issue_id': 5011, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ»Ñ', 'en': 'July 20', 'zh': '7æœˆ20æ—¥'}, 'hash': 'bf11cf5df484123c', 'authors': ['Hang Yan', 'Fangzhi Xu', 'Rongman Xu', 'Yifei Li', 'Jian Zhang', 'Haoran Luo', 'Xiaobao Wu', 'Luu Anh Tuan', 'Haiteng Zhao', 'Qika Lin', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Peking University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.14958.jpg', 'data': {'categories': ['#inference', '#reasoning', '#benchmark', '#architecture', '#training', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² LLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹, Ğ²Ñ‹ÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Momentum Uncertainty-guided Reasoning (MUR) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). MUR Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ‚Ğ°Ğ¿Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ³Ğ°Ğ¼Ğ¼Ğ°-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ´Ğ¸Ğ½ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ MUR ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 50% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 0.62-3.37%.'}, 'en': {'title': 'Optimize Reasoning Budgets with MUR!', 'desc': 'Momentum Uncertainty-guided Reasoning (MUR) is a novel approach that enhances the efficiency of Large Language Models (LLMs) during inference by optimizing their reasoning budgets. It addresses the challenge of overthinking in LLMs, which can lead to unnecessary computations and wasted tokens. By utilizing a concept from physics, MUR tracks and aggregates uncertainty over time to dynamically allocate reasoning resources to the most critical steps. The method is validated through extensive testing on various benchmarks, showing significant reductions in computation and improvements in accuracy compared to existing methods.'}, 'zh': {'title': 'åŠ¨é‡ä¸ç¡®å®šæ€§å¼•å¯¼æ¨ç†ï¼šä¼˜åŒ–æ¨ç†æ•ˆç‡çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŠ¨é‡ä¸ç¡®å®šæ€§å¼•å¯¼æ¨ç†ï¼ˆMURï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨åŠ¨æ€ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—é¢„ç®—ã€‚MURé€šè¿‡è·Ÿè¸ªå’Œèšåˆé€æ­¥çš„ä¸ç¡®å®šæ€§ï¼Œçµæ´»åœ°åˆ†é…æ€è€ƒé¢„ç®—ï¼Œä»è€Œæé«˜æ¨ç†æ•ˆç‡å¹¶å‡å°‘å†—ä½™è®¡ç®—ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¼½é©¬æ§åˆ¶æœºåˆ¶ï¼Œé€šè¿‡ä¸€ä¸ªè¶…å‚æ•°è°ƒèŠ‚æ¨ç†é¢„ç®—ï¼Œæ”¯æŒçµæ´»çš„æ¨ç†æ—¶é—´æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMURåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†è®¡ç®—é‡ï¼ŒåŒæ—¶æé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15758', 'title': 'LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2507.15758', 'abstract': "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.", 'score': 23, 'issue_id': 5006, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': 'ce0e5d782e94f5ee', 'authors': ['Xingyu Wu', 'Yuchen Yan', 'Shangke Lyu', 'Linjuan Wu', 'Yiwen Qiu', 'Yongliang Shen', 'Weiming Lu', 'Jian Shao', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15758.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#math', '#rl', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ¼ĞµĞ½ÑŒÑˆĞµ ÑĞ»Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ÑĞ¼Ñ‹ÑĞ»Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LAPO (Length-Adaptive Policy Optimization). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. LAPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 40.9%, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 2.3% Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LAPO, Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Smart Reasoning: Less is More with LAPO', 'desc': 'This paper introduces Length-Adaptive Policy Optimization (LAPO), a new method for improving reasoning models in machine learning. LAPO allows models to learn how much reasoning is needed for different problems, rather than setting strict limits on reasoning length. It uses a two-stage reinforcement learning process where models first learn successful reasoning patterns and then apply this knowledge during inference. The results show that LAPO can significantly reduce the number of tokens used while also increasing the accuracy of the models.'}, 'zh': {'title': 'æ™ºèƒ½æ¨ç†ï¼Œçµæ´»æ§åˆ¶ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºé•¿åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆLAPOï¼‰ï¼Œæ—¨åœ¨æ”¹å–„æ¨ç†æ¨¡å‹çš„æ•ˆç‡ã€‚LAPOé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå†…åœ¨ç†è§£é€‚å½“çš„æ¨ç†æ·±åº¦ï¼Œä»è€Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚ç¬¬ä¸€é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ æˆåŠŸè§£å†³æ–¹æ¡ˆé•¿åº¦çš„ç»Ÿè®¡åˆ†å¸ƒï¼›ç¬¬äºŒé˜¶æ®µï¼Œæ¨¡å‹å°†è¿™äº›æ¨¡å¼ä½œä¸ºå…ƒè®¤çŸ¥æŒ‡å¯¼ï¼ŒåµŒå…¥æ¨ç†ä¸Šä¸‹æ–‡ä¸­ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLAPOåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å°‘äº†å¤šè¾¾40.9%çš„æ ‡è®°ä½¿ç”¨ï¼ŒåŒæ—¶æé«˜äº†2.3%çš„å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18634', 'title': 'Captain Cinema: Towards Short Movie Generation', 'url': 'https://huggingface.co/papers/2507.18634', 'abstract': 'Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai', 'score': 16, 'issue_id': 5006, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': 'a52dc2cc9f063733', 'authors': ['Junfei Xiao', 'Ceyuan Yang', 'Lvmin Zhang', 'Shengqu Cai', 'Yang Zhao', 'Yuwei Guo', 'Gordon Wetzstein', 'Maneesh Agrawala', 'Alan Yuille', 'Lu Jiang'], 'affiliations': ['ByteDance Seed', 'CUHK Project Lead', 'Johns Hopkins University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.18634.jpg', 'data': {'categories': ['#diffusion', '#video', '#long_context', '#story_generation', '#multimodal', '#dataset'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ° Ğº ĞºĞ¸Ğ½Ğ¾: Ğ˜Ğ˜-Ñ€ĞµĞ¶Ğ¸ÑÑĞµÑ€ Captain Cinema', 'desc': 'Captain Cinema - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² ÑĞ²ĞµÑ€Ñ…Ñƒ Ğ²Ğ½Ğ¸Ğ·, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ¸Ğ´ĞµĞ¾ ÑĞ½Ğ¸Ğ·Ñƒ Ğ²Ğ²ĞµÑ€Ñ…. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ„Ğ¸Ğ»ÑŒĞ¼Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Transforming Text to Cinematic Reality with Captain Cinema', 'desc': "Captain Cinema is a framework designed to create short movies from text descriptions. It uses a two-step process: first, it generates keyframes that outline the movie's storyline, ensuring that the visuals and narrative are coherent. Next, it synthesizes video by connecting these keyframes, allowing for smooth transitions and dynamic scenes. The model employs Multimodal Diffusion Transformers with a unique training strategy to enhance the quality and efficiency of the movie generation process."}, 'zh': {'title': 'ç”¨æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çŸ­ç‰‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'Captain Cinema æ˜¯ä¸€ä¸ªçŸ­ç‰‡ç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æè¿°ç”Ÿæˆé«˜è´¨é‡çš„çŸ­ç”µå½±ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡è‡ªä¸Šè€Œä¸‹çš„å…³é”®å¸§è§„åˆ’ç”Ÿæˆä¸€ç³»åˆ—å…³é”®å¸§ï¼Œä»¥ç¡®ä¿æ•…äº‹æƒ…èŠ‚å’Œè§†è§‰å¤–è§‚çš„ä¸€è‡´æ€§ã€‚æ¥ç€ï¼Œè¿™äº›å…³é”®å¸§ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œä¾›è§†é¢‘åˆæˆæ¨¡å‹ä½¿ç”¨ï¼Œä»è€Œç”Ÿæˆåœºæ™¯ä¹‹é—´çš„æ—¶ç©ºåŠ¨æ€ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§äº¤é”™è®­ç»ƒç­–ç•¥ï¼Œä¸“é—¨ä¸ºé•¿ä¸Šä¸‹æ–‡è§†é¢‘æ•°æ®è®¾è®¡çš„å¤šæ¨¡æ€æ‰©æ•£å˜æ¢å™¨ï¼Œä»¥æ”¯æŒå¤šåœºæ™¯é•¿å™äº‹ç”µå½±çš„ç¨³å®šé«˜æ•ˆç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18537', 'title': 'TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation', 'url': 'https://huggingface.co/papers/2507.18537', 'abstract': "TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR.", 'score': 10, 'issue_id': 5005, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': 'acf3b32f27e7342b', 'authors': ['Zhekai Chen', 'Ruihang Chu', 'Yukang Chen', 'Shiwei Zhang', 'Yujie Wei', 'Yingya Zhang', 'Xihui Liu'], 'affiliations': ['CUHK', 'HKU MMLab', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.18537.jpg', 'data': {'categories': ['#cv', '#training', '#optimization'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'TTS-VAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµÑÑĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³Ğ°. TTS-VAR Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿ÑƒÑ‚Ğ¸, Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒÑ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ GenEval Ğ½Ğ° 8.7% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Infinity.'}, 'en': {'title': 'Dynamic Scaling for Enhanced Visual Generation Quality', 'desc': 'TTS-VAR is a novel framework designed to enhance the quality of visual auto-regressive (VAR) models during the generation process. It introduces a dynamic batch size adjustment strategy that balances computational efficiency with the ability to explore diverse outputs. The framework employs clustering techniques at coarse scales to maintain structural diversity and resampling methods at fine scales to prioritize high-potential candidates based on their generation history. Experiments demonstrate a significant improvement in generation quality, highlighting the importance of early-stage features in the overall output.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ï¼Œæå‡ç”Ÿæˆè´¨é‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'TTS-VARæ˜¯ä¸€ä¸ªç”¨äºè§†è§‰è‡ªå›å½’æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°å’Œä½¿ç”¨èšç±»ä¸é‡é‡‡æ ·æŠ€æœ¯æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚è¯¥æ¡†æ¶å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜ï¼Œæ—¨åœ¨å¹³è¡¡è®¡ç®—æ•ˆç‡ä¸æ¢ç´¢èƒ½åŠ›ã€‚å®ƒåœ¨ç²—å°ºåº¦ä¸Šé‡‡ç”¨åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ï¼Œä»¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼Œå¹¶åœ¨ç»†å°ºåº¦ä¸Šé€šè¿‡é‡é‡‡æ ·ä¼˜å…ˆé€‰æ‹©æ½œåœ¨çš„ä¼˜è´¨æ ·æœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTTS-VARåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šå®ç°äº†8.7%çš„GenEvalåˆ†æ•°æå‡ï¼Œæ˜¾ç¤ºå‡ºæ—©æœŸç»“æ„ç‰¹å¾å¯¹æœ€ç»ˆè´¨é‡çš„æœ‰æ•ˆå½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15844', 'title': 'Hierarchical Budget Policy Optimization for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2507.15844', 'abstract': 'Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.', 'score': 9, 'issue_id': 5006, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': 'c5c45ecb7b33f3cb', 'authors': ['Shangke Lyu', 'Linjuan Wu', 'Yuchen Yan', 'Xingyu Wu', 'Hao Li', 'Yongliang Shen', 'Peisheng Jiang', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['SF Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15844.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Hierarchical Budget Policy Optimization (HBPO) Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. HBPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³Ğ»Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ HBPO ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° 60.6% Ğ¿Ñ€Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 3.14% Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ, Ñ‚Ğ°Ğº Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing Reasoning Efficiency with Adaptive Depth', 'desc': 'This paper introduces Hierarchical Budget Policy Optimization (HBPO), a new reinforcement learning framework designed to improve the efficiency of reasoning models. HBPO allows models to adapt their reasoning depth based on the complexity of the problem, rather than using a one-size-fits-all approach. By implementing hierarchical budget exploration and differentiated reward mechanisms, the framework encourages models to allocate resources effectively while maintaining their performance. The results show that HBPO can significantly reduce token usage and enhance accuracy, demonstrating that efficiency and capability can be optimized together.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œèƒ½åŠ›ä¸æ•ˆç‡å¹¶å­˜', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå±‚æ¬¡é¢„ç®—ç­–ç•¥ä¼˜åŒ–ï¼ˆHBPOï¼‰çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ¨ç†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚HBPOé€šè¿‡åˆ†å±‚é¢„ç®—æ¢ç´¢ï¼Œå°†æ ·æœ¬åˆ†æˆå¤šä¸ªå…·æœ‰ä¸åŒä»¤ç‰Œé¢„ç®—çš„å­ç»„ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„èµ„æºåˆ†é…ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†å·®å¼‚åŒ–å¥–åŠ±æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚æ€§è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œé¿å…äº†é•¿è¾“å‡ºé•¿åº¦çš„æƒ©ç½šå¯¹æ¨¡å‹çš„è´Ÿé¢å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHBPOåœ¨å››ä¸ªæ¨ç†åŸºå‡†ä¸Šå¹³å‡å‡å°‘äº†60.6%çš„ä»¤ç‰Œä½¿ç”¨ï¼ŒåŒæ—¶æé«˜äº†3.14%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†æ¨ç†æ•ˆç‡ä¸èƒ½åŠ›å¯ä»¥åŒæ—¶ä¼˜åŒ–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16535', 'title': 'EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion', 'url': 'https://huggingface.co/papers/2507.16535', 'abstract': "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/", 'score': 8, 'issue_id': 5005, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '7ee137161bbe047e', 'authors': ['Shang Liu', 'Chenjie Cao', 'Chaohui Yu', 'Wen Qian', 'Jing Wang', 'Fan Wang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.16535.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#synthetic', '#3d', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'EarthCrafter: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¼ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ—ĞµĞ¼Ğ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Aerial-Earth3D - ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… 3D Ğ°ÑÑ€Ğ¾ÑÑŠĞµĞ¼ĞºĞ¸, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 50 Ñ‚Ñ‹ÑÑÑ‡ ÑÑ†ĞµĞ½ Ğ¿Ğ¾ Ğ²ÑĞµĞ¹ Ñ‚ĞµÑ€Ñ€Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¡Ğ¨Ğ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº EarthCrafter Ğ´Ğ»Ñ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·ĞµĞ¼Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° EarthCrafter Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… 3D-ÑÑ†ĞµĞ½ Ğ¸ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'Revolutionizing Large-Scale 3D Earth Generation', 'desc': "This paper presents a solution to the challenge of generating large-scale 3D models of Earth's surface by introducing a new dataset and a novel model architecture. The Aerial-Earth3D dataset is the largest of its kind, containing 50,000 scenes with detailed annotations that support diverse terrain representation. The EarthCrafter framework utilizes a dual approach with sparse-decoupled latent diffusion, allowing for efficient generation of 3D structures and textures while managing computational costs. The results show significant improvements in generating realistic and plausible large-scale 3D environments, with applications in urban planning and terrain synthesis."}, 'zh': {'title': 'å¤§è§„æ¨¡3Dåœ°çƒç”Ÿæˆçš„æ–°çªç ´', 'desc': 'å°½ç®¡æœ€è¿‘çš„3Dç”ŸæˆæŠ€æœ¯å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å°†è¿™äº›æ–¹æ³•æ‰©å±•åˆ°å¤§è§„æ¨¡åœ°ç†èŒƒå›´ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬é€šè¿‡æ•°æ®åŸºç¡€è®¾æ–½å’Œæ¨¡å‹æ¶æ„çš„åŒé‡åˆ›æ–°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬ä»‹ç»äº†Aerial-Earth3Dï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„3Dèˆªç©ºæ•°æ®é›†ï¼ŒåŒ…å«50,000ä¸ªåœºæ™¯ï¼Œæ”¯æŒå¤§è§„æ¨¡çš„3Dåœ°çƒç”Ÿæˆã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†EarthCrafteræ¡†æ¶ï¼Œé€šè¿‡ç¨€ç–è§£è€¦çš„æ½œåœ¨æ‰©æ•£æŠ€æœ¯ï¼Œå®ç°äº†é«˜æ•ˆçš„åœ°å½¢ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18464', 'title': 'DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts', 'url': 'https://huggingface.co/papers/2507.18464', 'abstract': "DriftMoE, an online Mixture-of-Experts architecture with a compact neural router, achieves competitive results in adapting to concept drift in data streams through a symbiotic learning loop.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using a multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with a clear training signal while accelerating expert specialization. We evaluate DriftMoE's performance across nine state-of-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-of-the-art stream learning adaptive ensembles, offering a principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.", 'score': 5, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '8de2367db0f263c3', 'authors': ['Miguel Aspis', 'SebastiÃ¡n A. Cajas OrdÃ³nez', 'AndrÃ©s L. SuÃ¡rez-Cetrulo', 'Ricardo SimÃ³n Carbajo'], 'affiliations': ['Irelands Centre for Applied AI (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9, Ireland', 'Irelands National Centre for Artificial Intelligence (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9'], 'pdf_title_img': 'assets/pdf/title_img/2507.18464.jpg', 'data': {'categories': ['#open_source', '#architecture', '#data', '#optimization', '#benchmark'], 'emoji': 'ğŸŒŠ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ DriftMoE: ÑƒĞºÑ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'DriftMoE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ´Ñ€ĞµĞ¹Ñ„Ñƒ Ğ² Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ Ğ¿ÑƒĞ»Ğ¾Ğ¼ Ğ¸Ğ½ĞºÑ€ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´ĞµÑ€ĞµĞ²ÑŒĞµĞ² Ğ¥ĞµÑ„Ğ´Ğ¸Ğ½Ğ³Ğ°. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¸Ğ¼Ğ±Ğ¸Ğ¾Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ñ†Ğ¸ĞºĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ, Ğ° Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ñƒ - ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². DriftMoE Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´ĞµĞ²ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': "Adapting to Change: DriftMoE's Smart Learning Loop", 'desc': 'DriftMoE is an innovative online Mixture-of-Experts architecture designed to effectively handle concept drift in data streams. It utilizes a compact neural router that works in tandem with incremental Hoeffding tree experts, allowing for real-time adaptation to changing data patterns. The architecture features a symbiotic learning loop where the router selects the best expert for predictions, and experts update their knowledge based on true labels, enhancing their specialization. This approach not only improves prediction accuracy but also maintains resource efficiency, making DriftMoE a competitive solution in the realm of adaptive ensemble methods.'}, 'zh': {'title': 'DriftMoEï¼šé«˜æ•ˆåº”å¯¹æ¦‚å¿µæ¼‚ç§»çš„åœ¨çº¿ä¸“å®¶æ¶æ„', 'desc': 'DriftMoEæ˜¯ä¸€ç§åœ¨çº¿æ··åˆä¸“å®¶æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹æ•°æ®æµä¸­çš„æ¦‚å¿µæ¼‚ç§»ã€‚å®ƒé€šè¿‡ä¸€ç§æ–°é¢–çš„å…±è®­ç»ƒæ¡†æ¶ï¼Œç»“åˆç´§å‡‘çš„ç¥ç»è·¯ç”±å™¨å’Œå¢é‡Hoeffdingæ ‘ä¸“å®¶ï¼Œå®ç°äº†ä¸“å®¶çš„ä¸“ä¸šåŒ–é€‰æ‹©ã€‚è¯¥æ¶æ„çš„å…³é”®åˆ›æ–°åœ¨äºå…¶å…±ç”Ÿå­¦ä¹ å¾ªç¯ï¼Œä½¿å¾—è·¯ç”±å™¨èƒ½å¤Ÿé€‰æ‹©æœ€åˆé€‚çš„ä¸“å®¶è¿›è¡Œé¢„æµ‹ï¼Œå¹¶é€šè¿‡çœŸå®æ ‡ç­¾çš„å¢é‡æ›´æ–°æ¥æå‡ä¸“å®¶çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDriftMoEåœ¨å¤šä¸ªæ•°æ®æµå­¦ä¹ åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæä¾›äº†ä¸€ç§é«˜æ•ˆçš„æ¦‚å¿µæ¼‚ç§»é€‚åº”æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18013', 'title': 'Technical Report of TeleChat2, TeleChat2.5 and T1', 'url': 'https://huggingface.co/papers/2507.18013', 'abstract': "The TeleChat2, TeleChat2.5, and T1 models enhance language capabilities through advanced training strategies, including Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning, achieving superior performance in reasoning and speed compared to previous models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the latest series of TeleChat models: TeleChat2, TeleChat2.5, and T1, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with TeleChat2, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. TeleChat2.5 and T1 expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, TeleChat2.5 prioritizes speed, delivering rapid inference. Both flagship models of T1 and TeleChat2.5 are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, T1-115B outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2, TeleChat2.5 and T1, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.", 'score': 4, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '5ea71fd57b80ec78', 'authors': ['Zihan Wang', 'Xinzhang Liu', 'Yitong Yao', 'Chao Wang', 'Yu Zhao', 'Zhihao Yang', 'Wenmin Deng', 'Kaipeng Jia', 'Jiaxin Peng', 'Yuyao Huang', 'Sishi Xiong', 'Zhuo Jiang', 'Kaidong Yu', 'Xiaohui Hu', 'Fubei Yao', 'Ruiyu Fang', 'Zhuoru Jiang', 'Ruiting Song', 'Qiyi Xie', 'Rui Xue', 'Xuewei He', 'Yanlei Xue', 'Zhu Yuan', 'Zhaoxi Zhang', 'Zilu Huang', 'Shiquan Wang', 'Xin Wang', 'Hanming Wu', 'Mingyuan Wang', 'Xufeng Zhan', 'Yuhan Sun', 'Zhaohu Xing', 'Yuhao Jiang', 'Bingkai Yang', 'Shuangyong Song', 'Yongxiang Li', 'Zhongjiang He', 'Xuelong Li'], 'affiliations': ['China Telecom'], 'pdf_title_img': 'assets/pdf/title_img/2507.18013.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#open_source', '#architecture', '#training', '#optimization', '#rl'], 'emoji': 'ğŸš€', 'ru': {'title': 'TeleChat: Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'ĞĞ¾Ğ²Ğ°Ñ ÑĞµÑ€Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TeleChat (TeleChat2, TeleChat2.5 Ğ¸ T1) Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´ÑˆĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸. Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ñ‹ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 10 Ñ‚Ñ€Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Supervised Fine-Tuning Ğ¸ Direct Preference Optimization. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ TeleChat2.5 Ğ¸ T1 Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ¤Ğ»Ğ°Ğ³Ğ¼Ğ°Ğ½ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ T1 Ğ¸ TeleChat2.5 Ñ 115 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº OpenAI o1-mini Ğ¸ GPT-4o, Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Language Models with TeleChat Series', 'desc': "The TeleChat2, TeleChat2.5, and T1 models represent a significant advancement in language processing capabilities through innovative training techniques. These models utilize Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning to enhance their performance in reasoning and speed. TeleChat2 is pretrained on a vast dataset of 10 trillion tokens, while TeleChat2.5 and T1 incorporate continual pretraining with domain-specific data to excel in tasks like code generation and mathematical reasoning. The T1 model is particularly designed for complex reasoning tasks, demonstrating notable improvements over previous models, including proprietary ones like OpenAI's GPT-4o."}, 'zh': {'title': 'æå‡è¯­è¨€èƒ½åŠ›çš„å…ˆè¿›æ¨¡å‹ç³»åˆ—', 'desc': 'TeleChat2ã€TeleChat2.5å’ŒT1æ¨¡å‹é€šè¿‡å…ˆè¿›çš„è®­ç»ƒç­–ç•¥æå‡äº†è¯­è¨€èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒã€ç›´æ¥åå¥½ä¼˜åŒ–å’Œå¼ºåŒ–å­¦ä¹ ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨ç†å’Œé€Ÿåº¦æ–¹é¢ç›¸è¾ƒäºä¹‹å‰çš„æ¨¡å‹è¡¨ç°æ›´ä¸ºå‡ºè‰²ã€‚TeleChat2åœ¨10ä¸‡äº¿é«˜è´¨é‡å¤šæ ·åŒ–çš„æ ‡è®°ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œéšåé€šè¿‡ç›‘ç£å¾®è°ƒå’Œç›´æ¥åå¥½ä¼˜åŒ–è¿›ä¸€æ­¥å¢å¼ºèƒ½åŠ›ã€‚T1æ¨¡å‹ä¸“æ³¨äºå¤æ‚æ¨ç†ï¼Œæ”¯æŒé•¿é“¾æ€ç»´æ¨ç†ï¼Œå¹¶åœ¨æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.14988', 'title': 'DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis', 'url': 'https://huggingface.co/papers/2507.14988', 'abstract': 'DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/.', 'score': 4, 'issue_id': 5005, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 Ğ¸ÑĞ»Ñ', 'en': 'July 20', 'zh': '7æœˆ20æ—¥'}, 'hash': '27df664b6cc093b4', 'authors': ['Yinghao Aaron Li', 'Xilin Jiang', 'Fei Tao', 'Cheng Niu', 'Kaifeng Xu', 'Juntong Song', 'Nima Mesgarani'], 'affiliations': ['Columbia University', 'NewsBreak'], 'pdf_title_img': 'assets/pdf/title_img/2507.14988.jpg', 'data': {'categories': ['#diffusion', '#training', '#rl', '#audio', '#optimization'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ° Ğ²ÑĞµÑ… ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ…', 'desc': 'DMOSpeech 2 - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ (GRPO) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ…Ğ¾Ğ¶ĞµÑÑ‚Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ° Ğ´Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ° Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. DMOSpeech 2 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Optimizing Duration for Diverse Speech Synthesis', 'desc': 'DMOSpeech 2 enhances speech synthesis by optimizing duration prediction, which was previously unaddressed. It employs a reinforcement learning strategy with group relative preference optimization (GRPO) to improve the duration predictor using metrics like speaker similarity and word error rate. Additionally, the paper introduces teacher-guided sampling, which combines a teacher model for initial processing with a student model for efficiency, leading to greater output diversity. Overall, DMOSpeech 2 achieves superior performance in speech synthesis while reducing the number of sampling steps needed, marking a significant advancement in metric-optimized TTS systems.'}, 'zh': {'title': 'ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œæå‡å¤šæ ·æ€§ä¸æ•ˆç‡', 'desc': 'DMOSpeech 2 æ˜¯ä¸€ç§ä¼˜åŒ–è¯­éŸ³åˆæˆä¸­æŒç»­æ—¶é—´é¢„æµ‹çš„æ–°æ–¹æ³•ï¼Œé‡‡ç”¨äº†å¼ºåŒ–å­¦ä¹ çš„ç­–ç•¥æ¥æå‡åˆæˆæ•ˆæœã€‚è¯¥ç³»ç»Ÿå¼•å…¥äº†åŸºäºç»„ç›¸å¯¹åå¥½çš„ä¼˜åŒ–æ¡†æ¶ï¼Œåˆ©ç”¨è¯´è¯è€…ç›¸ä¼¼æ€§å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œä»è€Œä¼˜åŒ–äº†ä¹‹å‰æœªä¼˜åŒ–çš„æŒç»­æ—¶é—´é¢„æµ‹ç»„ä»¶ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒDMOSpeech 2 è¿˜é‡‡ç”¨äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·çš„æ–¹æ³•ï¼Œé€šè¿‡æ•™å¸ˆæ¨¡å‹è¿›è¡Œåˆæ­¥å»å™ªï¼Œå†è½¬å‘å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ã€‚ç»¼åˆè¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼ŒåŒæ—¶å°†é‡‡æ ·æ­¥éª¤å‡å°‘äº†ä¸€åŠï¼Œä¸”æ²¡æœ‰é™ä½è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18103', 'title': 'A New Pair of GloVes', 'url': 'https://huggingface.co/papers/2507.18103', 'abstract': 'New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.', 'score': 3, 'issue_id': 5010, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '6dc651e0c2dd6898', 'authors': ['Riley Carlson', 'John Bauer', 'Christopher D. Manning'], 'affiliations': ['Stanford NLP Group, Stanford University, 353 Jane Stanford Way, Stanford CA 94305-9035, U.S.A.'], 'pdf_title_img': 'assets/pdf/title_img/2507.18103.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#transfer_learning', '#benchmark'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GloVe: ÑˆĞ°Ğ³ Ğ²Ğ¿ĞµÑ€ĞµĞ´ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GloVe 2024 Ğ³Ğ¾Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ²ĞµÑ€ÑĞ¸Ğ¸ 2014 Ğ³Ğ¾Ğ´Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Wikipedia, Gigaword Ğ¸ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Dolma. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ 2024 Ğ³Ğ¾Ğ´Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾ Ğ¸ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ NER Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Evolving GloVe: Enhanced Word Vectors for Modern Language Understanding', 'desc': 'The 2024 GloVe models enhance the original 2014 versions by utilizing updated datasets that reflect current language use. These models are specifically designed to improve Named Entity Recognition (NER) tasks, particularly in culturally and temporally relevant contexts. The paper details the training process of the new word embeddings using sources like Wikipedia and Gigaword, ensuring thorough documentation of the data and preprocessing methods. Evaluation results indicate that the new models not only maintain performance on traditional tasks but also excel in recognizing contemporary and diverse entities in language.'}, 'zh': {'title': 'æ›´æ–°GloVeæ¨¡å‹ï¼Œæå‡è¯­è¨€ç†è§£èƒ½åŠ›', 'desc': '2024å¹´çš„GloVeæ¨¡å‹åœ¨2014å¹´ç‰ˆæœ¬çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œé‡‡ç”¨äº†æ›´æ–°çš„æ•°æ®é›†ï¼Œå¹¶åœ¨ä¸æ–‡åŒ–å’Œæ—¶é—´ç›¸å…³çš„å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚è¿™äº›æ–°æ¨¡å‹é€šè¿‡ä½¿ç”¨ç»´åŸºç™¾ç§‘ã€Gigawordå’ŒDolmaçš„å­é›†è¿›è¡Œè®­ç»ƒï¼Œè§£å†³äº†2014å¹´æ¨¡å‹åœ¨æ•°æ®ç‰ˆæœ¬å’Œé¢„å¤„ç†æ–¹é¢æ–‡æ¡£ä¸å…¨çš„é—®é¢˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œ2024å¹´çš„è¯å‘é‡åœ¨è¯æ±‡æ¯”è¾ƒã€ç›´æ¥æµ‹è¯•å’Œå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°åæ˜ å½“å‰çš„è¯­è¨€å’Œæ–‡åŒ–å˜åŒ–ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›æ”¹è¿›ä½¿å¾—GloVeæ¨¡å‹åœ¨å¤„ç†ç°ä»£è¯­è¨€ä»»åŠ¡æ—¶æ›´åŠ æœ‰æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18546', 'title': 'GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface', 'url': 'https://huggingface.co/papers/2507.18546', 'abstract': 'GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.', 'score': 2, 'issue_id': 5011, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '1145b7615152d229', 'authors': ['Urchade Zaratiana', 'Gil Pasternak', 'Oliver Boyd', 'George Hurn-Maloney', 'Ash Lewis'], 'affiliations': ['Fastino AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.18546.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#open_source', '#data', '#architecture', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'GLiNER2 - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. GLiNER2 Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ½Ğ° CPU Ğ¿Ñ€Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'GLiNER2: One Model, Many NLP Tasks!', 'desc': 'GLiNER2 is a versatile framework designed for various natural language processing (NLP) tasks, allowing users to perform named entity recognition, text classification, and data extraction with a single transformer model. This approach reduces the need for multiple specialized models and minimizes the computational demands typically associated with large language models. By utilizing a pretrained transformer encoder, GLiNER2 achieves efficiency in both CPU usage and model size while offering a user-friendly schema-based interface for multi-task operations. The framework has shown strong performance in experiments, making it a practical choice for developers looking for accessible NLP solutions.'}, 'zh': {'title': 'GLiNER2ï¼šé«˜æ•ˆç»Ÿä¸€çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¡†æ¶', 'desc': 'GLiNER2æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä½¿ç”¨å•ä¸€é«˜æ•ˆçš„å˜æ¢å™¨æ¨¡å‹ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²å¯åŠæ€§ã€‚ä¿¡æ¯æå–æ˜¯è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨çš„åŸºç¡€ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦ä¸ºä¸åŒä»»åŠ¡è®¾è®¡ä¸“é—¨æ¨¡å‹ï¼Œæˆ–è€…ä¾èµ–è®¡ç®—æˆæœ¬é«˜çš„å¤§è¯­è¨€æ¨¡å‹ã€‚GLiNER2å¢å¼ºäº†åŸæœ‰çš„GLiNERæ¶æ„ï¼Œæ”¯æŒå‘½åå®ä½“è¯†åˆ«ã€æ–‡æœ¬åˆ†ç±»å’Œå±‚æ¬¡ç»“æ„æ•°æ®æå–ï¼Œä¸”åœ¨ä¸€ä¸ªé«˜æ•ˆæ¨¡å‹ä¸­å®ç°å¤šä»»åŠ¡ç»„åˆã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒGLiNER2åœ¨æå–å’Œåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå¹¶åœ¨éƒ¨ç½²å¯åŠæ€§ä¸Šç›¸æ¯”åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆæœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18192', 'title': 'TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance', 'url': 'https://huggingface.co/papers/2507.18192', 'abstract': "TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.", 'score': 2, 'issue_id': 5006, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': 'c5028e9aad7b0964', 'authors': ['Minghao Fu', 'Guo-Hua Wang', 'Xiaohao Chen', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce Group', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2507.18192.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#inference', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'TeEFusion: Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'TeEFusion - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. ĞĞ½ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸ Ğ±ĞµĞ·ÑƒÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ classifier-free guidance Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ñ€Ğ¾Ğ´Ğµ SD3 Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ 6 Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'TeEFusion: Fast and Efficient Text-to-Image Synthesis', 'desc': 'TeEFusion is a new method that improves text-to-image synthesis by integrating classifier-free guidance directly into text embeddings. This approach reduces the need for multiple forward passes, which are costly in terms of computation, while still producing high-quality images. By fusing conditional and unconditional text embeddings through simple linear operations, TeEFusion allows a student model to learn from a more complex teacher model without increasing the number of parameters. As a result, the student model can generate images up to six times faster than the teacher model, while maintaining similar image quality.'}, 'zh': {'title': 'TeEFusionï¼šé«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹æ³•', 'desc': 'TeEFusionæ˜¯ä¸€ç§æ–°é¢–çš„æ–‡æœ¬åµŒå…¥èåˆæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒåˆæˆçš„æ•ˆç‡ã€‚å®ƒé€šè¿‡å°†æ— åˆ†ç±»å™¨å¼•å¯¼ç›´æ¥èå…¥æ–‡æœ¬åµŒå…¥ä¸­ï¼Œå‡å°‘äº†æ¨ç†æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¿æ€§æ“ä½œç®€å•åœ°èåˆæ¡ä»¶å’Œæ— æ¡ä»¶æ–‡æœ¬åµŒå…¥ï¼Œé¿å…äº†é¢å¤–å‚æ•°çš„å¢åŠ ã€‚å®éªŒè¡¨æ˜ï¼ŒTeEFusionä½¿å¾—å­¦ç”Ÿæ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šæ¯”æ•™å¸ˆæ¨¡å‹å¿«6å€ï¼ŒåŒæ—¶å›¾åƒè´¨é‡ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.15595', 'title': 'SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging', 'url': 'https://huggingface.co/papers/2507.15595', 'abstract': 'SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image segmentation is crucial for many healthcare tasks, including disease diagnosis and treatment planning. One key area is the segmentation of skin lesions, which is vital for diagnosing skin cancer and monitoring patients. In this context, this paper introduces SegDT, a new segmentation model based on diffusion transformer (DiT). SegDT is designed to work on low-cost hardware and incorporates Rectified Flow, which improves the generation quality at reduced inference steps and maintains the flexibility of standard diffusion models. Our method is evaluated on three benchmarking datasets and compared against several existing works, achieving state-of-the-art results while maintaining fast inference speeds. This makes the proposed model appealing for real-world medical applications. This work advances the performance and capabilities of deep learning models in medical image analysis, enabling faster, more accurate diagnostic tools for healthcare professionals. The code is made publicly available at https://github.com/Bekhouche/SegDT{GitHub}.', 'score': 2, 'issue_id': 5011, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': '8ea1ce4e3edb7f0b', 'authors': ['Salah Eddine Bekhouche', 'Gaby Maroun', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.15595.jpg', 'data': {'categories': ['#inference', '#cv', '#diffusion', '#healthcare', '#open_source', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'SegDT: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¶Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SegDT - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ğ¾Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¶Ğ¸. SegDT Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ‚Ñ€ĞµÑ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Rectified Flow Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. SegDT Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'SegDT: Fast and Accurate Skin Lesion Segmentation for Real-World Healthcare', 'desc': 'SegDT is a novel segmentation model that utilizes a diffusion transformer architecture to enhance the segmentation of skin lesions, which is essential for skin cancer diagnosis. The model incorporates Rectified Flow to improve the quality of generated segmentations while reducing the number of inference steps required. Evaluated on three benchmark datasets, SegDT outperforms existing models, achieving state-of-the-art results with rapid inference speeds. This advancement in deep learning for medical image analysis makes SegDT a practical tool for healthcare professionals in real-world applications.'}, 'zh': {'title': 'SegDTï¼šå¿«é€Ÿé«˜æ•ˆçš„çš®è‚¤ç—…å˜åˆ†å‰²æ¨¡å‹', 'desc': 'SegDTæ˜¯ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„åˆ†å‰²æ¨¡å‹ï¼Œä¸“æ³¨äºçš®è‚¤ç—…å˜çš„åˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ¨¡å‹åœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šè¿è¡Œï¼Œå…·æœ‰å¿«é€Ÿæ¨ç†é€Ÿåº¦ï¼Œé€‚åˆå®é™…åŒ»ç–—åº”ç”¨ã€‚é€šè¿‡å¼•å…¥ä¿®æ­£æµï¼ˆRectified Flowï¼‰ï¼ŒSegDTåœ¨å‡å°‘æ¨ç†æ­¥éª¤çš„åŒæ—¶æé«˜äº†ç”Ÿæˆè´¨é‡ã€‚ç»è¿‡åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¯„ä¼°ï¼ŒSegDTåœ¨å¤šä¸ªç°æœ‰æ–¹æ³•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ¨åŠ¨äº†åŒ»ç–—å›¾åƒåˆ†æä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å’Œèƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18565', 'title': 'Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age\n  Estimation and Gender Classification for Targeted Advertisement', 'url': 'https://huggingface.co/papers/2507.18565', 'abstract': 'A custom CNN architecture simultaneously classifies age and gender from facial images, improving performance by learning shared representations and achieving high accuracy and low mean absolute error.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a novel deep learning-based approach for simultaneous age and gender classification from facial images, designed to enhance the effectiveness of targeted advertising campaigns. We propose a custom Convolutional Neural Network (CNN) architecture, optimized for both tasks, which leverages the inherent correlation between age and gender information present in facial features. Unlike existing methods that often treat these tasks independently, our model learns shared representations, leading to improved performance. The network is trained on a large, diverse dataset of facial images, carefully pre-processed to ensure robustness against variations in lighting, pose, and image quality. Our experimental results demonstrate a significant improvement in gender classification accuracy, achieving 95%, and a competitive mean absolute error of 5.77 years for age estimation. Critically, we analyze the performance across different age groups, identifying specific challenges in accurately estimating the age of younger individuals. This analysis reveals the need for targeted data augmentation and model refinement to address these biases. Furthermore, we explore the impact of different CNN architectures and hyperparameter settings on the overall performance, providing valuable insights for future research.', 'score': 1, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '7ecd81e416dd179d', 'authors': ['Muhammad Imran Zaman', 'Nisar Ahmed'], 'affiliations': ['Department of Computer Science (New Campus), University of Engineering and Technology Lahore, Pakistan', 'Department of Computer Science, COMSATS University Islamabad Lahore Campus, Lahore, Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2507.18565.jpg', 'data': {'categories': ['#cv', '#dataset', '#architecture', '#ethics', '#optimization', '#training'], 'emoji': 'ğŸ‘¤', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ CNN Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ»Ğ° Ğ¿Ğ¾ Ğ»Ğ¸Ñ†Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ¸ Ğ¿Ğ¾Ğ»Ğ° Ğ¿Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ»Ğ¸Ñ† Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ (CNN), Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ³ĞµĞ½Ğ´ĞµÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 95% Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ° Ğ¸ ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ 5,77 Ğ»ĞµÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ°. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ñ€Ğ°ÑÑ‚Ğ° Ğ¼Ğ¾Ğ»Ğ¾Ğ´Ñ‹Ñ… Ğ»ÑĞ´ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Simultaneous Age and Gender Classification with Custom CNN', 'desc': "This paper introduces a custom Convolutional Neural Network (CNN) designed to classify both age and gender from facial images simultaneously. By learning shared representations, the model improves performance compared to traditional methods that treat these tasks separately. The network is trained on a large dataset, ensuring it can handle variations in lighting and image quality effectively. Results show high accuracy in gender classification and a low mean absolute error for age estimation, highlighting the model's potential for applications like targeted advertising."}, 'zh': {'title': 'é¢éƒ¨å›¾åƒçš„å¹´é¾„ä¸æ€§åˆ«åŒé‡åˆ†ç±»æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡é¢éƒ¨å›¾åƒåŒæ—¶åˆ†ç±»å¹´é¾„å’Œæ€§åˆ«ï¼Œæ—¨åœ¨æé«˜é’ˆå¯¹æ€§å¹¿å‘Šæ´»åŠ¨çš„æ•ˆæœã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å®šåˆ¶çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„ï¼Œä¼˜åŒ–äº†è¿™ä¸¤ä¸ªä»»åŠ¡ï¼Œåˆ©ç”¨é¢éƒ¨ç‰¹å¾ä¸­å¹´é¾„å’Œæ€§åˆ«ä¿¡æ¯çš„å†…åœ¨å…³è”ã€‚ä¸ç°æœ‰æ–¹æ³•é€šå¸¸ç‹¬ç«‹å¤„ç†è¿™äº›ä»»åŠ¡ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å…±äº«è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ€§åˆ«åˆ†ç±»å‡†ç¡®ç‡è¾¾åˆ°95%ï¼Œå¹´é¾„ä¼°è®¡çš„å¹³å‡ç»å¯¹è¯¯å·®ä¸º5.77å²ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨ä¸åŒå¹´é¾„ç»„çš„è¡¨ç°å…·æœ‰æ˜¾è‘—æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16802', 'title': 'Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning', 'url': 'https://huggingface.co/papers/2507.16802', 'abstract': 'The Agentar-Fin-R1 series of financial large language models enhances reasoning, reliability, and domain specialization through a trustworthiness assurance framework and achieves state-of-the-art performance on financial and general reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.', 'score': 1, 'issue_id': 5014, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 Ğ¸ÑĞ»Ñ', 'en': 'July 22', 'zh': '7æœˆ22æ—¥'}, 'hash': '227d9ddcf1258003', 'authors': ['Yanjun Zheng', 'Xiyang Du', 'Longfei Liao', 'Xiaoke Zhao', 'Zhaowen Zhou', 'Jingze Song', 'Bo Zhang', 'Jiawei Liu', 'Xiang Qi', 'Zhe Li', 'Zhiqiang Zhang', 'Wei Wang', 'Peng Zhang'], 'affiliations': ['@antgroup.com'], 'pdf_title_img': 'assets/pdf/title_img/2507.16802.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#training', '#optimization', '#science', '#agents', '#reasoning'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'ĞĞ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Agentar-Fin-R1, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen3. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸. Agentar-Fin-R1 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Finova Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼.'}, 'en': {'title': 'Empowering Financial Intelligence with Trustworthy LLMs', 'desc': 'The Agentar-Fin-R1 series of financial large language models (LLMs) is designed to improve reasoning, reliability, and specialization in financial contexts. It utilizes a trustworthiness assurance framework that includes high-quality knowledge engineering and rigorous data validation. By employing a systematic labeling system and a two-stage training pipeline, these models achieve enhanced training efficiency and performance. The models have been evaluated on various financial and general reasoning benchmarks, demonstrating state-of-the-art results and strong capabilities for real-world financial applications.'}, 'zh': {'title': 'é‡‘èé¢†åŸŸçš„ä¿¡ä»»ä¸æ¨ç†æ–°æ ‡å‡†', 'desc': 'Agentar-Fin-R1ç³»åˆ—é‡‘èå¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡ä¿¡ä»»ä¿éšœæ¡†æ¶å¢å¼ºäº†æ¨ç†èƒ½åŠ›ã€å¯é æ€§å’Œé¢†åŸŸä¸“ä¸šåŒ–ï¼Œè¾¾åˆ°äº†é‡‘èå’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹åŸºäºQwen3åŸºç¡€æ¨¡å‹è®¾è®¡ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ¨¡å‹åœ¨å¤æ‚æ¨ç†å’Œä¿¡ä»»æ ‡å‡†æ–¹é¢çš„å±€é™æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨é«˜è´¨é‡çš„é‡‘èä»»åŠ¡æ ‡ç­¾ç³»ç»Ÿå’Œå¤šå±‚æ¬¡çš„ä¿¡ä»»ä¿éšœæ¡†æ¶ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAgentar-Fin-R1åœ¨é‡‘èä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨ä¸€èˆ¬æ¨ç†èƒ½åŠ›ä¸Šä¹Ÿè¡¨ç°ä¼˜å¼‚ï¼Œè¯æ˜äº†å…¶åœ¨é«˜é£é™©é‡‘èåº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.16038', 'title': 'Discovering and using Spelke segments', 'url': 'https://huggingface.co/papers/2507.16038', 'abstract': 'A visual world model called SpelkeNet outperforms existing methods in identifying Spelke objects in images, improving performance in tasks like physical object manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objects--groupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains a wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, a class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under a poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for "statistical counterfactual probing", where diverse "virtual pokes" are applied on regions of high motion-affordance, and the resultant expected displacement maps are used define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in a variety of off-the-shelf object manipulation models.', 'score': 1, 'issue_id': 5012, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 Ğ¸ÑĞ»Ñ', 'en': 'July 21', 'zh': '7æœˆ21æ—¥'}, 'hash': '23df9dd6fca8cb24', 'authors': ['Rahul Venkatesh', 'Klemen Kotar', 'Lilian Naing Chen', 'Seungwoo Kim', 'Luca Thomas Wheeler', 'Jared Watrous', 'Ashley Xu', 'Gia Ancone', 'Wanhee Lee', 'Honglin Chen', 'Daniel Bear', 'Stefan Stojanov', 'Daniel Yamins'], 'affiliations': ['Noetik Inc.', 'OpenAI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16038.jpg', 'data': {'categories': ['#multimodal', '#games', '#cv', '#dataset', '#3d', '#optimization', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SpelkeNet: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': "SpelkeNet - ÑÑ‚Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¡Ğ¿ĞµĞ»ĞºĞµ Ğ½Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ñ… ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹. SpelkeNet Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ 'ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ·Ğ¾Ğ½Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ' Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¡Ğ¿ĞµĞ»ĞºĞµ. Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SpelkeBench Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸."}, 'en': {'title': 'Revolutionizing Object Recognition with SpelkeNet', 'desc': 'This paper introduces SpelkeNet, a visual world model that excels in identifying Spelke objects in images, which are groupings of physical entities that move together due to physical forces. Unlike traditional methods that rely on specific categories, SpelkeNet leverages category-agnostic causal relationships to enhance tasks such as object manipulation and planning. The authors present the SpelkeBench dataset to benchmark the Spelke object concept and demonstrate that SpelkeNet outperforms existing models like SegmentAnything. Additionally, the Spelke concept proves beneficial for practical applications, improving performance in physical object manipulation tasks.'}, 'zh': {'title': 'SpelkeNetï¼šè¶…è¶Šä¼ ç»Ÿçš„è§†è§‰å¯¹è±¡è¯†åˆ«', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSpelkeNetçš„è§†è§‰ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒåœ¨è¯†åˆ«å›¾åƒä¸­çš„Spelkeå¯¹è±¡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰©ç†å¯¹è±¡æ“ä½œä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚Spelkeå¯¹è±¡æ˜¯åŸºäºç‰©ç†è¿åŠ¨å…³ç³»çš„åˆ†ç»„ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ”¯æŒæ“ä½œå’Œè§„åˆ’ä»»åŠ¡ã€‚æˆ‘ä»¬æ„å»ºäº†SpelkeBenchæ•°æ®é›†ï¼Œä»¥åŸºå‡†æµ‹è¯•Spelkeå¯¹è±¡çš„æ¦‚å¿µï¼Œå¹¶é€šè¿‡SpelkeNetç®—æ³•æå–å›¾åƒä¸­çš„Spelkeæ®µã€‚ç ”ç©¶è¡¨æ˜ï¼ŒSpelkeNetåœ¨å¤šä¸ªä¸‹æ¸¸åº”ç”¨ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ç‰©ç†å¯¹è±¡æ“ä½œçš„3DEditBenchåŸºå‡†æµ‹è¯•ä¸­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.18405', 'title': 'Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows', 'url': 'https://huggingface.co/papers/2507.18405', 'abstract': "Iwin Transformer, a hierarchical vision transformer without position embeddings, combines interleaved window attention and depthwise separable convolution for efficient global information exchange, achieving competitive performance in image classification, semantic segmentation, and video action recognition.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.", 'score': 0, 'issue_id': 5019, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 Ğ¸ÑĞ»Ñ', 'en': 'July 24', 'zh': '7æœˆ24æ—¥'}, 'hash': '093938764e40d430', 'authors': ['Simin Huo', 'Ning Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.18405.jpg', 'data': {'categories': ['#open_source', '#video', '#optimization', '#architecture', '#cv'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Iwin Transformer - Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰ĞµĞµÑÑ Ğ¾ĞºĞ¾Ğ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğ½Ğ¾-Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼ÑƒÑ ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹. Iwin Transformer Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Iwin Transformer: Efficient Global Information Exchange in Vision Tasks', 'desc': 'The Iwin Transformer is a new type of vision transformer that does not use position embeddings, allowing it to efficiently process images and videos. It combines interleaved window attention, which connects distant parts of the image, with depthwise separable convolution, which focuses on nearby areas. This design enables the model to exchange global information effectively within a single module, improving upon previous models like the Swin Transformer. The Iwin Transformer shows strong performance in various tasks, including image classification and semantic segmentation, and can also enhance class-conditional image generation.'}, 'zh': {'title': 'Iwin Transformerï¼šé«˜æ•ˆçš„è§†è§‰å˜æ¢å™¨æ–°é€‰æ‹©', 'desc': 'Iwin Transformeræ˜¯ä¸€ç§æ–°å‹çš„åˆ†å±‚è§†è§‰å˜æ¢å™¨ï¼Œä¸ä½¿ç”¨ä½ç½®åµŒå…¥ã€‚å®ƒé€šè¿‡äº¤é”™çª—å£æ³¨æ„åŠ›å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„ç»“åˆï¼Œå®ç°äº†é«˜æ•ˆçš„å…¨å±€ä¿¡æ¯äº¤æ¢ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒåˆ†ç±»ã€è¯­ä¹‰åˆ†å‰²å’Œè§†é¢‘åŠ¨ä½œè¯†åˆ«ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶åœ¨ImageNet-1Kä¸Šè¾¾åˆ°äº†87.4çš„é¡¶çº§å‡†ç¡®ç‡ã€‚Iwin Transformerçš„æ ¸å¿ƒç»„ä»¶å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å—ï¼Œæ›¿ä»£è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå…·æœ‰å¹¿æ³›çš„ç ”ç©¶æ½œåŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment', '#architecture (8)', '#audio (1)', '#benchmark (8)', '#cv (6)', '#data (3)', '#dataset (7)', '#diffusion (5)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (3)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (8)', '#optimization (14)', '#plp', '#rag', '#reasoning (5)', '#rl (5)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation (1)', '#survey', '#synthetic (1)', '#training (12)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-25 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-25 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-25 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    