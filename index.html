
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. July 23.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">23 июля</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-22.html">⬅️ <span id="prev-date">22.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-24.html">➡️ <span id="next-date">24.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '23 июля', 'en': 'July 23', 'zh': '7月23日'};
        let feedDateNext = {'ru': '24.07', 'en': '07/24', 'zh': '7月24日'};
        let feedDatePrev = {'ru': '22.07', 'en': '07/22', 'zh': '7月22日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.16784', 'title': 'Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning', 'url': 'https://huggingface.co/papers/2507.16784', 'abstract': 'A Thread Inference Model (TIM) and its runtime (TIMRUN) enable long-horizon reasoning in LLMs by using reasoning trees and key-value state retention, overcoming context and memory limitations.  \t\t\t\t\tAI-generated summary \t\t\t\t To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.', 'score': 36, 'issue_id': 4961, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '9f1ac89cdfedc134', 'authors': ['Hongyin Luo', 'Nathaniel Morgan', 'Tina Li', 'Derek Zhao', 'Ai Vy Ngo', 'Philip Schroeder', 'Lijie Yang', 'Assaf Ben-Kish', "Jack O'Brien", 'James Glass'], 'affiliations': ['MIT CSAIL', 'Princeton University', 'Subconscious Systems Technologies, Inc.', 'Tel Aviv University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16784.jpg', 'data': {'categories': ['#inference', '#architecture', '#long_context', '#reasoning', '#math'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений контекста в LLM с помощью древовидных рассуждений', 'desc': 'Эта статья представляет Thread Inference Model (TIM) и его среду выполнения TIMRUN, которые позволяют большим языковым моделям (LLM) осуществлять долгосрочные рассуждения. TIM использует деревья рассуждений и сохранение ключевых состояний для преодоления ограничений контекста и памяти. Система поддерживает практически неограниченную рабочую память и многоэтапные вызовы инструментов в рамках одного вывода языковой модели. Экспериментальные результаты показывают высокую пропускную способность вывода и точные рассуждения для задач, требующих долгосрочного анализа.'}, 'en': {'title': 'Unlocking Long-Horizon Reasoning in LLMs with TIM', 'desc': 'The Thread Inference Model (TIM) introduces a new approach for large language models (LLMs) to enhance their reasoning capabilities over extended contexts. By utilizing reasoning trees and a key-value state retention mechanism, TIM allows for recursive problem solving and efficient management of memory during inference. This model overcomes traditional limitations of LLMs, such as output constraints and GPU memory bottlenecks, enabling complex multi-hop reasoning tasks. Experimental results demonstrate that TIM can maintain high throughput while accurately performing mathematical reasoning and information retrieval tasks that require long-horizon thinking.'}, 'zh': {'title': '突破上下文限制，实现长远推理', 'desc': '本文提出了一种线程推理模型（TIM）及其运行时（TIMRUN），旨在解决大型语言模型（LLMs）在推理准确性和效率上的上下文限制。TIM通过使用推理树和关键值状态保留，支持递归和分解问题的解决，从而实现长时间跨度的推理。该模型能够在单次推理中支持几乎无限的工作记忆和多跳工具调用，克服了输出限制和GPU内存瓶颈。实验结果表明，该系统在处理数学任务和信息检索挑战时，能够保持高推理吞吐量，并提供准确的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2507.16632', 'title': 'Step-Audio 2 Technical Report', 'url': 'https://huggingface.co/papers/2507.16632', 'abstract': 'This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.', 'score': 24, 'issue_id': 4960, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '09237169b94ecf48', 'authors': ['Boyong Wu', 'Chao Yan', 'Chen Hu', 'Cheng Yi', 'Chengli Feng', 'Fei Tian', 'Feiyu Shen', 'Gang Yu', 'Haoyang Zhang', 'Jingbei Li', 'Mingrui Chen', 'Peng Liu', 'Wang You', 'Xiangyu Tony Zhang', 'Xingyuan Li', 'Xuerui Yang', 'Yayue Deng', 'Yechang Huang', 'Yuxin Li', 'Yuxin Zhang', 'Zhao You', 'Brian Li', 'Changyi Wan', 'Hanpeng Hu', 'Jiangjie Zhen', 'Siyu Chen', 'Song Yuan', 'Xuelin Zhang', 'Yimin Jiang', 'Yu Zhou', 'Yuxiang Yang', 'Bingxin Li', 'Buyun Ma', 'Changhe Song', 'Dongqing Pang', 'Guoqiang Hu', 'Haiyang Sun', 'Kang An', 'Na Wang', 'Shuli Gao', 'Wei Ji', 'Wen Li', 'Wen Sun', 'Xuan Wen', 'Yong Ren', 'Yuankai Ma', 'Yufan Lu', 'Bin Wang', 'Bo Li', 'Changxin Miao', 'Che Liu', 'Chen Xu', 'Dapeng Shi', 'Dingyuan Hu', 'Donghang Wu', 'Enle Liu', 'Guanzhe Huang', 'Gulin Yan', 'Han Zhang', 'Hao Nie', 'Haonan Jia', 'Hongyu Zhou', 'Jianjian Sun', 'Jiaoren Wu', 'Jie Wu', 'Jie Yang', 'Jin Yang', 'Junzhe Lin', 'Kaixiang Li', 'Lei Yang', 'Liying Shi', 'Li Zhou', 'Longlong Gu', 'Ming Li', 'Mingliang Li', 'Mingxiao Li', 'Nan Wu', 'Qi Han', 'Qinyuan Tan', 'Shaoliang Pang', 'Shengjie Fan', 'Siqi Liu', 'Tiancheng Cao', 'Wanying Lu', 'Wenqing He', 'Wuxun Xie', 'Xu Zhao', 'Xueqi Li', 'Yanbo Yu', 'Yang Yang', 'Yi Liu', 'Yifan Lu', 'Yilei Wang', 'Yuanhao Ding', 'Yuanwei Liang', 'Yuanwei Lu', 'Yuchu Luo', 'Yuhe Yin', 'Yumeng Zhan', 'Yuxiang Zhang', 'Zidong Yang', 'Zixin Zhang', 'Binxing Jiao', 'Daxin Jiang', 'Heung-Yeung Shum', 'Jiansheng Chen', 'Jing Li', 'Xiangyu Zhang', 'Yibo Zhu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.16632.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#agi', '#hallucinations', '#reasoning', '#rl', '#audio', '#open_source', '#rag'], 'emoji': '🎙️', 'ru': {'title': 'Интеллектуальное понимание аудио и речи с помощью мультимодальной языковой модели', 'desc': 'Step-Audio 2 - это мультимодальная языковая модель для понимания аудио и речевого общения. Она использует латентный аудиокодировщик и обучение с подкреплением для улучшения распознавания речи и понимания аудио. Модель интегрирует генерацию дискретных аудиотокенов в языковое моделирование, что позволяет учитывать паралингвистическую информацию. Step-Audio 2 также применяет генерацию с использованием извлечения информации и может обращаться к внешним инструментам для уменьшения галлюцинаций.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Step-Audio 2', 'desc': 'Step-Audio 2 is a powerful multi-modal large language model that excels in understanding audio and facilitating speech conversations. It uses a latent audio encoder combined with reinforcement learning to improve automatic speech recognition and audio comprehension. The model enhances its ability to respond to different speaking styles and emotions by generating discrete audio tokens within its language framework. Additionally, it employs retrieval-augmented generation to access external information, reducing errors and improving its performance in real-world applications.'}, 'zh': {'title': 'Step-Audio 2：音频理解与对话的未来', 'desc': '本文介绍了Step-Audio 2，这是一种端到端的多模态大型语言模型，旨在实现工业级的音频理解和语音对话。通过集成潜在音频编码器和以推理为中心的强化学习，Step-Audio 2在自动语音识别（ASR）和音频理解方面取得了良好的性能。为了实现真正的端到端语音对话，Step-Audio 2将离散音频标记的生成纳入语言建模中，显著增强了对说话风格和情感等副语言信息的响应能力。经过数百万小时的语音和音频数据训练，Step-Audio 2在各种对话场景中展现出智能和表现力。'}}}, {'id': 'https://huggingface.co/papers/2507.08422', 'title': 'Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2507.08422', 'abstract': 'Diffusion transformers have emerged as an alternative to U-net-based diffusion models for high-fidelity image and video generation, offering superior scalability. However, their heavy computation remains a major obstacle to real-world deployment. Existing acceleration methods primarily exploit the temporal dimension such as reusing cached features across diffusion timesteps. Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free framework that accelerates inference along spatial dimension. RALU performs mixed-resolution sampling across three stages: 1) low-resolution denoising latent diffusion to efficiently capture global semantic structure, 2) region-adaptive upsampling on specific regions prone to artifacts at full-resolution, and 3) all latent upsampling at full-resolution for detail refinement. To stabilize generations across resolution transitions, we leverage noise-timestep rescheduling to adapt the noise level across varying resolutions. Our method significantly reduces computation while preserving image quality by achieving up to 7.0times speed-up on FLUX and 3.0times on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is complementary to existing temporal accelerations such as caching methods, thus can be seamlessly integrated to further reduce inference latency without compromising generation quality.', 'score': 21, 'issue_id': 4964, 'pub_date': '2025-07-11', 'pub_date_card': {'ru': '11 июля', 'en': 'July 11', 'zh': '7月11日'}, 'hash': '63043ce148fb06e1', 'authors': ['Wongi Jeong', 'Kyungryeol Lee', 'Hoigi Seo', 'Se Young Chun'], 'affiliations': ['Dept. of Electrical and Computer Engineering, Seoul National University, Republic of Korea', 'IPAI & INMC, Seoul National University, Republic of Korea'], 'pdf_title_img': 'assets/pdf/title_img/2507.08422.jpg', 'data': {'categories': ['#inference', '#diffusion', '#optimization', '#cv'], 'emoji': '🚀', 'ru': {'title': 'Ускорение диффузионных трансформеров без потери качества', 'desc': 'Статья представляет новый метод ускорения генерации изображений с помощью диффузионных трансформеров - Region-Adaptive Latent Upsampling (RALU). RALU использует смешанное разрешение на разных этапах генерации, что позволяет значительно сократить вычислительные затраты. Метод включает три стадии: низкоразрешенную денойзинг латентную диффузию, адаптивное повышение разрешения в проблемных областях и финальное повышение разрешения для всего изображения. RALU позволяет достичь ускорения до 7 раз для модели FLUX и до 3 раз для Stable Diffusion 3 при минимальной потере качества.'}, 'en': {'title': 'Accelerating Diffusion Transformers with RALU for High-Quality Generation', 'desc': 'This paper introduces Region-Adaptive Latent Upsampling (RALU), a novel framework designed to enhance the efficiency of diffusion transformers in image and video generation. RALU accelerates inference by focusing on the spatial dimension, utilizing a three-stage process that includes low-resolution denoising, region-specific upsampling, and full-resolution detail refinement. The method also incorporates noise-timestep rescheduling to maintain stability during resolution transitions, ensuring high-quality outputs. Overall, RALU achieves significant computation speed-ups while preserving image fidelity, making it a valuable addition to existing acceleration techniques.'}, 'zh': {'title': '区域自适应潜在上采样：加速图像生成的新方法', 'desc': '扩散变换器作为高保真图像和视频生成的替代方案，提供了更好的可扩展性，但其计算量大仍然是实际应用的障碍。我们提出了一种名为区域自适应潜在上采样（RALU）的框架，旨在沿空间维度加速推理，而无需训练。RALU通过三个阶段进行混合分辨率采样，首先在低分辨率下去噪以捕捉全局语义结构，然后在特定区域进行自适应上采样，最后在全分辨率下进行细节优化。我们的研究表明，RALU在保持图像质量的同时，显著减少了计算量，能够实现高达7倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2507.16812', 'title': 'MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning', 'url': 'https://huggingface.co/papers/2507.16812', 'abstract': 'MegaScience, a large-scale dataset of scientific reasoning questions, enhances the performance and training efficiency of AI models compared to existing datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.', 'score': 19, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'e6653e3f0a1b904f', 'authors': ['Run-Ze Fan', 'Zengzhi Wang', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University, SII, GAIR Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.16812.jpg', 'data': {'categories': ['#benchmark', '#science', '#data', '#open_source', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'MegaScience: прорыв в научном мышлении ИИ', 'desc': 'MegaScience - это крупномасштабный набор данных научных рассуждений, улучшающий производительность и эффективность обучения моделей ИИ по сравнению с существующими наборами данных. Он включает в себя 1,25 миллиона примеров из 7 научных дисциплин, полученных из учебников университетского уровня и других высококачественных источников. Авторы разработали комплексную систему оценки, охватывающую различные предметы и типы вопросов по 15 эталонным тестам. Эксперименты показывают, что модели, обученные на MegaScience, значительно превосходят официальные инструктивные модели по средней производительности.'}, 'en': {'title': 'Empowering AI with MegaScience for Superior Scientific Reasoning', 'desc': "This paper introduces MegaScience, a large-scale dataset designed to improve AI models' performance in scientific reasoning tasks. It addresses the lack of high-quality, open-source datasets in the scientific domain by providing 1.25 million instances of reasoning questions derived from university-level textbooks. The authors conducted systematic studies to select the best data subsets, ensuring that the dataset is both comprehensive and effective for training AI models. Their experiments show that models trained on MegaScience outperform existing models, highlighting its potential to enhance scientific reasoning capabilities in AI."}, 'zh': {'title': 'MegaScience：推动科学推理的未来', 'desc': 'MegaScience是一个大规模的科学推理问题数据集，旨在提高人工智能模型的性能和训练效率。该数据集包含从12000本大学级科学教科书中提取的真实参考答案，涵盖650000个推理问题，涉及7个科学学科。通过系统的消融研究，我们开发了1.25百万实例的高质量开放源数据集，并建立了全面的评估系统，以确保准确的评估指标。我们的实验表明，MegaScience在训练效率和响应长度上优于现有的开放源科学数据集，特别适合更大和更强的模型。'}}}, {'id': 'https://huggingface.co/papers/2507.16814', 'title': 'Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning', 'url': 'https://huggingface.co/papers/2507.16814', 'abstract': 'Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training.', 'score': 16, 'issue_id': 4961, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'cbe613b2436dd0f8', 'authors': ['Junhao Shen', 'Haiteng Zhao', 'Yuzhe Gu', 'Songyang Gao', 'Kuikun Liu', 'Haian Huang', 'Jianfei Gao', 'Dahua Lin', 'Wenwei Zhang', 'Kai Chen'], 'affiliations': ['MMLab, The Chinese University of Hong Kong', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16814.jpg', 'data': {'categories': ['#training', '#open_source', '#benchmark', '#multimodal', '#reasoning', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'SOPHIA: Революция в визуальном мышлении искусственного интеллекта', 'desc': 'Статья представляет SOPHIA - новый метод обучения с подкреплением для улучшения способностей крупных мультимодальных моделей к визуальному рассуждению. SOPHIA сочетает в себе визуальное понимание из обучаемой модели LVLM с медленным рассуждением из языковой модели, используя полу-офф-полисный подход. Метод показал значительное улучшение производительности на различных тестах мультимодального рассуждения, превзойдя некоторые закрытые модели. Анализ демонстрирует преимущества SOPHIA перед обучением с учителем и прямыми он-полисными методами обучения с подкреплением.'}, 'en': {'title': 'SOPHIA: Elevating LVLMs with Semi-Off-Policy Reasoning', 'desc': "This paper introduces SOPHIA, a novel Semi-Off-Policy Reinforcement Learning (RL) approach designed to enhance large vision-language models (LVLMs) by incorporating slow-thinking reasoning capabilities. The challenge with traditional on-policy RL is that it limits the model's ability to explore beyond its initial training, while off-policy RL can lead to visual inconsistencies when using external models. SOPHIA effectively combines on-policy visual understanding from LVLMs with off-policy reasoning from language models, allowing for better learning through outcome-based rewards. Experimental results demonstrate that SOPHIA significantly improves performance on multimodal reasoning tasks, surpassing both open-source and some closed-source models."}, 'zh': {'title': 'SOPHIA：提升视觉语言模型的慢思维推理能力', 'desc': '本文提出了一种名为SOPHIA的半离线强化学习方法，用于增强大型视觉语言模型（LVLM）的慢思维推理能力。SOPHIA结合了来自可训练LVLM的在线视觉理解和来自语言模型的离线慢思维推理，利用结果导向的奖励来促进推理过程。通过这种方法，LVLM能够从获得的推理轨迹中学习慢思维推理能力，并通过离线强化学习算法传播奖励。实验结果表明，SOPHIA在多个多模态推理基准上显著提高了模型性能，超越了许多现有的开源和闭源模型。'}}}, {'id': 'https://huggingface.co/papers/2507.16815', 'title': 'ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning', 'url': 'https://huggingface.co/papers/2507.16815', 'abstract': 'ThinkAct, a dual-system framework, uses reinforced visual latent planning to enable high-level reasoning and robust action execution in vision-language-action tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.', 'score': 11, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': 'a0c0c8cc661a52b5', 'authors': ['Chi-Pin Huang', 'Yueh-Hua Wu', 'Min-Hung Chen', 'Yu-Chiang Frank Wang', 'Fu-En Yang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16815.jpg', 'data': {'categories': ['#agents', '#optimization', '#robotics', '#training', '#multimodal', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Думай и действуй: интеллектуальное планирование для воплощенного ИИ', 'desc': 'ThinkAct - это двухсистемная архитектура для задач визуально-языкового взаимодействия. Она использует подкрепленное визуальное латентное планирование для высокоуровневых рассуждений и надежного выполнения действий. ThinkAct обучает мультимодальную языковую модель генерировать планы рассуждений, основываясь на визуальных наградах, связанных с действиями. Эти планы сжимаются в визуальный латентный план, который используется для управления моделью действий в целевой среде.'}, 'en': {'title': 'ThinkAct: Bridging Reasoning and Action in AI Tasks', 'desc': 'ThinkAct is a novel framework designed for vision-language-action tasks that combines high-level reasoning with low-level action execution. It utilizes reinforced visual latent planning to create effective plans that guide agents in dynamic environments. By training a multimodal large language model (LLM), ThinkAct generates reasoning plans that are optimized through visual rewards, ensuring actions align with goals. The framework shows significant improvements in few-shot adaptation and long-horizon planning, making it suitable for complex AI tasks like robot manipulation.'}, 'zh': {'title': 'ThinkAct：高效推理与动作执行的双系统框架', 'desc': 'ThinkAct是一个双系统框架，旨在通过强化视觉潜在规划实现高水平推理和稳健的动作执行。该框架能够处理视觉-语言-动作（VLA）任务，帮助智能体理解多模态指令并进行长远规划。与传统的端到端训练方法不同，ThinkAct通过生成与动作对齐的视觉奖励来指导推理计划，从而提高了智能体在复杂环境中的适应能力。实验结果表明，ThinkAct在少量样本适应、长远规划和自我修正行为方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2507.16813', 'title': 'HOComp: Interaction-Aware Human-Object Composition', 'url': 'https://huggingface.co/papers/2507.16813', 'abstract': 'HOComp uses MLLMs and attention mechanisms to achieve seamless human-object interactions with consistent appearances in image compositing.  \t\t\t\t\tAI-generated summary \t\t\t\t While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively.', 'score': 9, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '5a513d80af038052', 'authors': ['Dong Liang', 'Jinyuan Jia', 'Yuhao Liu', 'Rynson W. H. Lau'], 'affiliations': ['CityUHK', 'HKUST(GZ)', 'Tongji University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16813.jpg', 'data': {'categories': ['#cv', '#synthetic', '#games', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Гармоничная композиция человека и объекта с помощью искусственного интеллекта', 'desc': 'HOComp - это новый подход к композиции изображений, обеспечивающий гармоничное взаимодействие между объектом переднего плана и человеком на фоновом изображении. Он использует многоязычные языковые модели (MLLM) для определения региона и типа взаимодействия, а также механизмы внимания для сохранения деталей и согласованности внешнего вида. HOComp превосходит существующие методы в создании естественных композиций с взаимодействием человека и объекта. Авторы также представили новый набор данных IHOC для этой задачи.'}, 'en': {'title': 'Seamless Human-Object Interaction in Image Compositing', 'desc': 'HOComp is a new method that enhances how foreground objects interact with people in images, ensuring they blend naturally into the scene. It uses Multi-Layered Language Models (MLLMs) to guide the placement and pose of objects based on the type of interaction, like holding or lifting. Additionally, it employs a technique called Detail-Consistent Appearance Preservation (DCAP) to maintain the visual consistency of shapes and textures between the foreground and background. The paper also introduces a new dataset, Interaction-aware Human-Object Composition (IHOC), to evaluate the effectiveness of HOComp, which shows significant improvements over existing methods.'}, 'zh': {'title': '无缝人机互动的图像合成新方法', 'desc': 'HOComp是一种新颖的方法，旨在将前景物体无缝地合成到以人为中心的背景图像中。该方法利用多语言大模型（MLLMs）和注意力机制，确保前景物体与背景人物之间的和谐互动及一致的外观。其核心设计包括基于区域的姿态引导和细节一致的外观保留，前者帮助识别互动区域和类型，后者确保前景和背景的一致性。实验结果表明，HOComp在生成自然的人物与物体互动方面表现优异，超越了相关方法。'}}}, {'id': 'https://huggingface.co/papers/2507.16746', 'title': 'Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning', 'url': 'https://huggingface.co/papers/2507.16746', 'abstract': "Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.", 'score': 8, 'issue_id': 4959, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '195867d3f8c130bf', 'authors': ['Ang Li', 'Charles Wang', 'Kaiyu Yue', 'Zikui Cai', 'Ollie Liu', 'Deqing Fu', 'Peng Guo', 'Wang Bill Zhu', 'Vatsal Sharan', 'Robin Jia', 'Willie Neiswanger', 'Furong Huang', 'Tom Goldstein', 'Micah Goldblum'], 'affiliations': ['Columbia University', 'New York University', 'University of Maryland', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2507.16746.jpg', 'data': {'categories': ['#rl', '#benchmark', '#games', '#optimization', '#multimodal', '#open_source', '#cv', '#dataset'], 'emoji': '🦓', 'ru': {'title': 'Zebra-CoT: Прорыв в обучении ИИ визуальному мышлению', 'desc': 'Статья представляет Zebra-CoT - крупномасштабный набор данных для обучения мультимодальных моделей визуальному рассуждению. Набор содержит 182,384 образца с логически связанными текстово-визуальными цепочками рассуждений для различных задач, включая научные вопросы, 2D и 3D визуальное мышление, и стратегические игры. Файнтюнинг модели Anole-7B на этом корпусе улучшил точность на 12% в тестовом наборе и до 13% в стандартных бенчмарках VLM. Авторы открыли доступ к набору данных и моделям для поддержки разработки и оценки визуального цепочечного мышления.'}, 'en': {'title': 'Zebra-CoT: Enhancing Visual Reasoning with a Rich Dataset', 'desc': "This paper presents Zebra-CoT, a large dataset designed to enhance multimodal models' ability to perform visual reasoning tasks. It addresses the challenges of poor performance in existing visual chain of thought (CoT) models and the scarcity of quality training data. The dataset includes 182,384 samples that combine text and images for various reasoning tasks, such as geometry and strategic games. Fine-tuning models like Anole-7B and Bagel-7B on this dataset significantly improves their accuracy and ability to generate coherent visual reasoning chains."}, 'zh': {'title': 'Zebra-CoT：提升多模态推理能力的关键数据集', 'desc': '本论文介绍了一种名为Zebra-CoT的大规模数据集，包含182,384个样本，旨在帮助多模态模型进行视觉推理。该数据集提供了逻辑连贯的文本-图像推理链，适用于科学问题、2D和3D推理任务以及视觉逻辑问题等多种任务。通过对Anole-7B模型进行微调，测试集准确率提高了12%，并在标准VLM基准评估中获得了高达13%的性能提升。我们开源了该数据集和模型，以支持视觉推理能力的开发和评估。'}}}, {'id': 'https://huggingface.co/papers/2507.15024', 'title': 'RefCritic: Training Long Chain-of-Thought Critic Models with Refinement\n  Feedback', 'url': 'https://huggingface.co/papers/2507.15024', 'abstract': "With the rapid advancement of Large Language Models (LLMs), developing effective critic modules for precise guidance has become crucial yet challenging. In this paper, we initially demonstrate that supervised fine-tuning for building critic modules (which is widely adopted in current solutions) fails to genuinely enhance models' critique abilities, producing superficial critiques with insufficient reflections and verifications. To unlock the unprecedented critique capabilities, we propose RefCritic, a long-chain-of-thought critic module based on reinforcement learning with dual rule-based rewards: (1) instance-level correctness of solution judgments and (2) refinement accuracies of the policy model based on critiques, aiming to generate high-quality evaluations with actionable feedback that effectively guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement settings, RefCritic demonstrates consistent advantages across all benchmarks, e.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably, under majority voting, policy models filtered by RefCritic show superior scaling with increased voting numbers. Moreover, despite training on solution-level supervision, RefCritic outperforms step-level supervised approaches on ProcessBench, a benchmark to identify erroneous steps in mathematical reasoning.", 'score': 5, 'issue_id': 4960, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'd7fe33ada300b4f0', 'authors': ['Qiaoyu Tang', 'Hao Xiang', 'Le Yu', 'Bowen Yu', 'Hongyu Lin', 'Yaojie Lu', 'Xianpei Han', 'Le Sun', 'Junyang Lin'], 'affiliations': ['Alibaba Group', 'Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.15024.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#reasoning', '#rl', '#rlhf'], 'emoji': '🔍', 'ru': {'title': 'RefCritic: Революция в критике и улучшении языковых моделей', 'desc': 'В статье представлен новый подход к разработке критических модулей для больших языковых моделей (LLM) - RefCritic. В отличие от традиционных методов обучения с учителем, RefCritic использует обучение с подкреплением и двойную систему вознаграждений. Этот метод позволяет генерировать высококачественные оценки с действенной обратной связью для эффективного улучшения моделей. Эксперименты показали превосходство RefCritic над существующими подходами на нескольких бенчмарках, включая задачи критики, уточнения и пошагового анализа математических рассуждений.'}, 'en': {'title': 'Unlocking True Critique Power with RefCritic', 'desc': 'This paper addresses the limitations of current critic modules in Large Language Models (LLMs) that rely on supervised fine-tuning, which often leads to shallow critiques. The authors introduce RefCritic, a novel critic module that utilizes reinforcement learning and dual rule-based rewards to enhance critique quality. By focusing on both the correctness of solution judgments and the refinement of the policy model, RefCritic aims to provide actionable feedback for model improvement. Evaluation results show that RefCritic consistently outperforms existing methods across multiple benchmarks, demonstrating its effectiveness in generating high-quality evaluations.'}, 'zh': {'title': '提升批评能力，重塑模型指导！', 'desc': '随着大型语言模型（LLMs）的快速发展，开发有效的批评模块以提供精确指导变得至关重要且具有挑战性。本文首先表明，当前广泛采用的监督微调方法并未真正提升模型的批评能力，产生的批评往往表面化，缺乏深入的反思和验证。为了解锁前所未有的批评能力，我们提出了RefCritic，这是一种基于强化学习的长链思维批评模块，采用双重规则奖励机制，旨在生成高质量的评估和可操作的反馈，从而有效指导模型的改进。我们在多个基准测试中评估了RefCritic，结果显示其在批评和改进设置上均表现出一致的优势。'}}}, {'id': 'https://huggingface.co/papers/2507.15245', 'title': 'SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced\n  Academic Search', 'url': 'https://huggingface.co/papers/2507.15245', 'abstract': 'Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR', 'score': 4, 'issue_id': 4959, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'a01817799751346b', 'authors': ['Xiaofeng Shi', 'Yuduo Li', 'Qian Kou', 'Longbin Yu', 'Jinxin Xie', 'Hua Zhou'], 'affiliations': ['Beijing Academy of Artificial Intelligence (BAAI)', 'Beijing Jiaotong University (BJTU)'], 'pdf_title_img': 'assets/pdf/title_img/2507.15245.jpg', 'data': {'categories': ['#agents', '#benchmark', '#survey', '#interpretability', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'SPAR: Умный поиск научной литературы с помощью ИИ', 'desc': 'SPAR - это мультиагентная система для поиска научной литературы, использующая большие языковые модели. Она применяет декомпозицию и эволюцию запросов на основе RefChain для более гибкого и эффективного поиска. Авторы также создали бенчмарк SPARBench с экспертной разметкой для оценки таких систем. Эксперименты показали, что SPAR значительно превосходит сильные базовые модели, достигая улучшения F1-меры до 56% на AutoScholar и 23% на SPARBench.'}, 'en': {'title': 'Revolutionizing Academic Search with SPAR', 'desc': 'This paper presents SPAR, a novel multi-agent framework designed to enhance academic literature retrieval using large language models. SPAR utilizes RefChain-based query decomposition and evolution, allowing for more adaptable and effective search strategies compared to traditional rigid systems. The authors also introduce SPARBench, a benchmark with expert-annotated relevance labels to systematically evaluate retrieval performance. Experimental results show that SPAR significantly improves retrieval accuracy, outperforming existing methods by notable margins on both AutoScholar and SPARBench datasets.'}, 'zh': {'title': 'SPAR：提升学术检索的灵活性与效果', 'desc': '本文介绍了一种名为SPAR的多智能体框架，旨在提高学术文献检索的灵活性和有效性。SPAR通过基于RefChain的查询分解和查询演变技术，克服了现有系统的局限性。为了系统评估，我们构建了SPARBench，这是一个具有专家标注相关性标签的挑战性基准。实验结果表明，SPAR在AutoScholar和SPARBench上分别比最佳基线提高了56%和23%的F1分数，展示了其优越的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.15974', 'title': 'Does More Inference-Time Compute Really Help Robustness?', 'url': 'https://huggingface.co/papers/2507.15974', 'abstract': 'Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications.', 'score': 2, 'issue_id': 4961, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'e1f3efb4b484fe5f', 'authors': ['Tong Wu', 'Chong Xiang', 'Jiachen T. Wang', 'Weichen Yu', 'Chawin Sitawarin', 'Vikash Sehwag', 'Prateek Mittal'], 'affiliations': ['Carnegie Mellon University', 'Google DeepMind', 'NVIDIA', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15974.jpg', 'data': {'categories': ['#security', '#inference', '#reasoning'], 'emoji': '🛡️', 'ru': {'title': 'Масштабирование вывода: палка о двух концах для безопасности ИИ', 'desc': 'Исследование показывает, что увеличение вычислений во время вывода улучшает устойчивость не только крупных проприетарных языковых моделей, но и небольших моделей с открытым исходным кодом. Однако авторы обнаружили, что если промежуточные шаги рассуждений становятся доступными, увеличение вычислений снижает устойчивость модели. Это создает риски безопасности для моделей с интегрированными инструментами рассуждений и при атаках на извлечение рассуждений. Исследователи призывают тщательно оценивать компромиссы перед применением масштабирования вывода в чувствительных к безопасности приложениях.'}, 'en': {'title': 'Inference-Time Scaling: A Double-Edged Sword for Model Robustness', 'desc': 'This paper explores how increasing computation during inference can enhance the robustness of smaller, open-source language models, similar to findings in larger proprietary models. The authors challenge the assumption that intermediate reasoning steps in these models are hidden from adversaries, revealing a security risk when these steps are made accessible. They introduce the concept of an inverse scaling law, showing that more computation can actually decrease robustness if adversaries can see the reasoning process. The paper emphasizes the need for careful consideration of these trade-offs in security-sensitive applications, especially when models are integrated with tools or face advanced attack methods.'}, 'zh': {'title': '推理时间扩展的鲁棒性与安全风险', 'desc': '最近，Zaremba等人展示了增加推理时间计算可以提高大型专有推理大语言模型的鲁棒性。本文首先表明，小规模的开源模型（如DeepSeek R1、Qwen3、Phi-reasoning）也可以通过简单的预算强制策略受益于推理时间的扩展。更重要的是，我们揭示并批判性地审视了先前工作的一个隐含假设：中间推理步骤对对手是隐藏的。通过放宽这一假设，我们识别出一个重要的安全风险，即反向缩放法则：如果中间推理步骤变得显式可访问，增加的推理时间计算会持续降低模型的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2507.16795', 'title': 'Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2507.16795', 'abstract': "Concept Ablation Fine-Tuning (CAFT) uses interpretability tools to steer LLM generalization away from unintended concepts without altering training data.  \t\t\t\t\tAI-generated summary \t\t\t\t Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLM's latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.", 'score': 1, 'issue_id': 4962, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '18824f37cd717c2f', 'authors': ['Helena Casademunt', 'Caden Juang', 'Adam Karvonen', 'Samuel Marks', 'Senthooran Rajamanoharan', 'Neel Nanda'], 'affiliations': ['Anthropic', 'Harvard University', 'Independent', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16795.jpg', 'data': {'categories': ['#training', '#interpretability', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Управление обобщением языковых моделей без изменения данных', 'desc': 'Метод Concept Ablation Fine-Tuning (CAFT) использует инструменты интерпретируемости для управления обобщением больших языковых моделей (LLM) без изменения обучающих данных. CAFT применяет линейные проекции для удаления нежелательных концепций в латентном пространстве модели во время дообучения. Этот подход успешно применен к трем задачам дообучения, включая проблему возникающего рассогласования. CAFT значительно снижает количество нежелательных ответов без ухудшения производительности на обучающем распределении.'}, 'en': {'title': 'Steering LLMs Away from Unwanted Concepts Without Data Changes', 'desc': "Concept Ablation Fine-Tuning (CAFT) is a method designed to improve how large language models (LLMs) generalize after fine-tuning. Instead of changing the training data, CAFT uses interpretability tools to identify and remove unwanted concepts from the model's understanding. By applying linear projections in the model's latent space, CAFT effectively steers the model away from making unintended generalizations. This technique has been shown to significantly reduce misaligned responses in LLMs while maintaining their performance on the original training tasks."}, 'zh': {'title': '概念消融微调：引导模型避免错误泛化', 'desc': '概念消融微调（CAFT）是一种新技术，旨在控制大型语言模型（LLM）的泛化能力，避免其产生不必要的概念。传统方法通常需要修改训练数据，但这并不总是可行。CAFT利用可解释性工具，通过在微调过程中对不希望的概念进行消融，来引导模型远离错误的泛化。实验表明，CAFT在不改变训练数据的情况下，能够显著减少模型的错误响应，同时保持在训练数据上的性能。'}}}, {'id': 'https://huggingface.co/papers/2507.15454', 'title': 'ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via\n  Gaussian Splatting', 'url': 'https://huggingface.co/papers/2507.15454', 'abstract': 'ObjectGS combines 3D scene reconstruction with semantic understanding by modeling individual objects as neural Gaussians, achieving superior performance in segmentation and integration with applications like mesh extraction and scene editing.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and real-time novel view synthesis, yet its lack of semantic understanding limits object-level perception. In this work, we propose ObjectGS, an object-aware framework that unifies 3D scene reconstruction with semantic understanding. Instead of treating the scene as a unified whole, ObjectGS models individual objects as local anchors that generate neural Gaussians and share object IDs, enabling precise object-level reconstruction. During training, we dynamically grow or prune these anchors and optimize their features, while a one-hot ID encoding with a classification loss enforces clear semantic constraints. We show through extensive experiments that ObjectGS not only outperforms state-of-the-art methods on open-vocabulary and panoptic segmentation tasks, but also integrates seamlessly with applications like mesh extraction and scene editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page', 'score': 1, 'issue_id': 4960, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '98f5b463073c9876', 'authors': ['Ruijie Zhu', 'Mulin Yu', 'Linning Xu', 'Lihan Jiang', 'Yixuan Li', 'Tianzhu Zhang', 'Jiangmiao Pang', 'Bo Dai'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong', 'The University of Hong Kong', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.15454.jpg', 'data': {'categories': ['#3d', '#games', '#optimization', '#cv'], 'emoji': '🧠', 'ru': {'title': 'Объектно-ориентированная 3D-реконструкция с семантическим пониманием', 'desc': 'ObjectGS - это новый подход к 3D-реконструкции сцен, объединяющий геометрическое моделирование с семантическим пониманием. Метод использует нейронные гауссианы для представления отдельных объектов, что позволяет достичь высокой точности сегментации. ObjectGS превосходит современные методы в задачах сегментации с открытым словарем и панорамной сегментации. Технология хорошо интегрируется с приложениями для извлечения меш-моделей и редактирования сцен.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Object-Level Semantic Understanding', 'desc': 'ObjectGS is a novel framework that enhances 3D scene reconstruction by incorporating semantic understanding of individual objects. It models each object as a neural Gaussian, allowing for precise segmentation and reconstruction at the object level. The framework dynamically adjusts object anchors during training, optimizing their features while enforcing semantic clarity through a classification loss. Extensive experiments demonstrate that ObjectGS surpasses existing methods in segmentation tasks and integrates effectively with applications like mesh extraction and scene editing.'}, 'zh': {'title': '对象感知的3D场景重建新方法', 'desc': 'ObjectGS 是一个结合了 3D 场景重建和语义理解的框架。它通过将每个对象建模为局部锚点，生成神经高斯分布，从而实现精确的对象级重建。训练过程中，ObjectGS 动态调整这些锚点的数量和特征，并通过一热编码和分类损失来强化语义约束。实验结果表明，ObjectGS 在开放词汇和全景分割任务上超越了现有的最先进方法，并能与网格提取和场景编辑等应用无缝集成。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (2)', '#agi (1)', '#alignment (1)', '#architecture (1)', '#audio (1)', '#benchmark (6)', '#cv (4)', '#data (1)', '#dataset (3)', '#diffusion (1)', '#ethics', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (6)', '#plp', '#rag (1)', '#reasoning (8)', '#rl (4)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (4)', '#transfer_learning', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-23 09:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-23 09:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-23 09:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    