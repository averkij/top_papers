
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 72 papers. May 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 мая</span> | <span id="title-articles-count">72 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-27.html">⬅️ <span id="prev-date">27.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-29.html">➡️ <span id="next-date">29.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 мая', 'en': 'May 28', 'zh': '5月28日'};
        let feedDateNext = {'ru': '29.05', 'en': '05/29', 'zh': '5月29日'};
        let feedDatePrev = {'ru': '27.05', 'en': '05/27', 'zh': '5月27日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.19897', 'title': 'ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows', 'url': 'https://huggingface.co/papers/2505.19897', 'abstract': "ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.", 'score': 79, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '7903b97e8a51b382', 'authors': ['Qiushi Sun', 'Zhoumianze Liu', 'Chang Ma', 'Zichen Ding', 'Fangzhi Xu', 'Zhangyue Yin', 'Haiteng Zhao', 'Zhenyu Wu', 'Kanzhi Cheng', 'Zhaoyang Liu', 'Jianing Wang', 'Qintong Li', 'Xiangru Tang', 'Tianbao Xie', 'Xiachong Feng', 'Xiang Li', 'Ben Kao', 'Wenhai Wang', 'Biqing Qi', 'Lingpeng Kong', 'Zhiyong Wu'], 'affiliations': ['East China Normal University', 'Fudan University', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19897.jpg', 'data': {'categories': ['#science', '#multimodal', '#agents', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'ScienceBoard: реалистичная среда для оценки ИИ-агентов в научных задачах', 'desc': 'ScienceBoard представляет собой реалистичную среду для научных рабочих процессов и бенчмарк для оценки производительности агентов на основе больших языковых моделей (LLM). Она включает в себя динамичные научные рабочие процессы с интегрированным профессиональным программным обеспечением в различных областях, таких как биохимия, астрономия и геоинформатика. Бенчмарк содержит 169 тщательно проверенных задач из реального мира. Тестирование современных агентов показало, что они достигают лишь 15% общего уровня успешности, демонстрируя текущие ограничения в сложных научных задачах.'}, 'en': {'title': 'Unlocking the Potential of LLMs in Scientific Workflows', 'desc': 'The paper introduces ScienceBoard, a platform designed to evaluate the performance of Large Language Model (LLM)-based agents in scientific workflows. It features a realistic environment with dynamic tasks across various scientific domains, allowing agents to interact with professional software. Despite the advancements in LLMs, evaluations reveal that these agents struggle with complex tasks, achieving only a 15% success rate. The study highlights the need for improved design principles to enhance the capabilities of these agents in aiding scientific discovery.'}, 'zh': {'title': '科学发现的新助手：ScienceBoard', 'desc': 'ScienceBoard是一个现实的科学工作流程环境和基准，用于评估基于大型语言模型（LLM）代理的性能，展示它们在复杂科学任务中的局限性。该平台提供了一个多领域的动态环境，允许代理通过不同的接口自主互动，以加速复杂的研究任务和实验。尽管一些代理在评估中表现出一定的潜力，但它们在复杂工作流程中的成功率仅为15%，显示出仍需改进。通过深入分析，本文为解决当前代理的局限性和设计更有效的代理提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.21327', 'title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2505.21327', 'abstract': "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  \t\t\t\t\tAI-generated summary \t\t\t\t Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.", 'score': 70, 'issue_id': 3991, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a022fb524c5969bf', 'authors': ['Jiakang Yuan', 'Tianshuo Peng', 'Yilei Jiang', 'Yiting Lu', 'Renrui Zhang', 'Kaituo Feng', 'Chaoyou Fu', 'Tao Chen', 'Lei Bai', 'Bo Zhang', 'Xiangyu Yue'], 'affiliations': ['Fudan University', 'MMLab, The Chinese University of Hong Kong', 'Nanjing University', 'Shanghai AI Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21327.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая пробелы в логике искусственного интеллекта', 'desc': "MME-Reasoning - это новый комплексный бенчмарк для оценки способностей мультимодальных больших языковых моделей (MLLM) к логическому рассуждению. Он охватывает индуктивное, дедуктивное и абдуктивное рассуждения, фокусируясь именно на логике, а не на восприятии или знаниях. Оценка показала существенные ограничения современных MLLM в комплексных логических рассуждениях, с заметными различиями в производительности для разных типов рассуждений. Анализ также выявил ограниченную эффективность популярных подходов, таких как 'режим мышления' и обучение с подкреплением на основе правил, для улучшения способностей к рассуждению."}, 'en': {'title': 'Unveiling the Reasoning Gaps in Multimodal AI', 'desc': 'The paper introduces MME-Reasoning, a benchmark designed to assess the logical reasoning abilities of multimodal large language models (MLLMs). It categorizes reasoning into three types: inductive, deductive, and abductive, addressing gaps in existing evaluations that often overlook these distinctions. The study reveals that even advanced MLLMs struggle with logical reasoning tasks, showing significant performance imbalances across the different reasoning types. Additionally, the paper analyzes common methods aimed at improving reasoning, highlighting the persistent limitations of current MLLMs in effectively handling diverse logical reasoning challenges.'}, 'zh': {'title': '评估多模态模型的逻辑推理能力', 'desc': 'MME-Reasoning 是一个评估多模态大型语言模型（MLLMs）逻辑推理能力的基准，揭示了在归纳、演绎和溯因推理类型上的显著局限性和性能不平衡。尽管多模态推理取得了显著进展，但现有基准未能全面评估其推理能力，缺乏对逻辑推理类型的明确分类。我们设计了 MME-Reasoning，涵盖所有三种推理类型的问题，确保每个问题有效评估推理能力，而非感知技能或知识广度。评估结果显示，当前最先进的 MLLMs 在全面的逻辑推理评估中表现有限，且在不同推理类型之间存在明显的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers', 'url': 'https://huggingface.co/papers/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.", 'score': 63, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '7f740f76be754bce', 'authors': ['Wei Pang', 'Kevin Qinghong Lin', 'Xiangru Jian', 'Xi He', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.21497.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#agents', '#science'], 'emoji': '🖼️', 'ru': {'title': 'Автоматическая генерация научных постеров: от статьи к визуализации', 'desc': 'Эта статья представляет первый эталонный тест и набор метрик для генерации академических постеров, сопоставляя недавние научные статьи с постерами, созданными авторами. Исследователи предлагают PosterAgent - многоагентный конвейер, который включает в себя Parser для извлечения ключевой информации, Planner для создания структуры постера и Painter-Commenter для визуального оформления. Оценка показывает, что открытые модели на основе Qwen-2.5 превосходят существующие системы по большинству метрик, используя на 87% меньше токенов. Исследование открывает путь к следующему поколению полностью автоматизированных моделей для создания постеров.'}, 'en': {'title': 'Revolutionizing Academic Poster Generation with PosterAgent', 'desc': 'This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation.'}, 'zh': {'title': '自动化学术海报生成的新纪元', 'desc': '本论文介绍了一种新的学术海报生成基准和评估指标，旨在将长篇文档压缩为视觉上连贯的单页海报。我们提出了PosterAgent，一个多代理管道，能够有效地解析、规划和绘制海报内容。通过对比不同模型的输出，我们发现人类设计的海报在视觉语义上更具吸引力，而GPT-4o模型虽然外观美观，但文本质量和信息传达能力较差。我们的开源变体在多个指标上超越了现有系统，并且显著减少了所需的计算资源。'}}}, {'id': 'https://huggingface.co/papers/2505.18445', 'title': 'OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data', 'url': 'https://huggingface.co/papers/2505.18445', 'abstract': "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.", 'score': 57, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '0a5e56835e542da2', 'authors': ['Yiren Song', 'Cheng Liu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.18445.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#diffusion', '#cv', '#training'], 'emoji': '🎨', 'ru': {'title': 'Универсальная согласованность стиля в генерации изображений', 'desc': 'OmniConsistency - это универсальный плагин для улучшения согласованности стилизации в задачах преобразования изображений. Он использует крупномасштабные Диффузионные Трансформеры (DiTs) и обучается на парах выровненных изображений для лучшей генерализации. Плагин применяет двухэтапную стратегию прогрессивного обучения, разделяющую изучение стиля и сохранение согласованности. OmniConsistency совместим с произвольными стилевыми LoRA и значительно повышает визуальную согласованность и эстетическое качество изображений.'}, 'en': {'title': 'Achieving Consistent and High-Quality Image Stylization with OmniConsistency', 'desc': 'OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models.'}, 'zh': {'title': 'OmniConsistency：提升图像风格一致性的创新方案', 'desc': 'OmniConsistency 是一种利用大规模扩散变换器（Diffusion Transformers）来增强图像到图像管道中的风格一致性和泛化能力的方法。该方法解决了在复杂场景中保持一致风格和防止风格退化的两个主要挑战。OmniConsistency 提供了一种基于对齐图像对的上下文一致性学习框架，并采用两阶段的渐进学习策略来分离风格学习与一致性保持。实验结果表明，OmniConsistency 显著提高了视觉连贯性和美学质量，性能可与商业最先进模型 GPT-4o 相媲美。'}}}, {'id': 'https://huggingface.co/papers/2505.20292', 'title': 'OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20292', 'abstract': "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.", 'score': 49, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'e2a8d12789199cde', 'authors': ['Shenghai Yuan', 'Xianyi He', 'Yufan Deng', 'Yang Ye', 'Jinfa Huang', 'Bin Lin', 'Chongyang Ma', 'Jiebo Luo', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20292.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#synthetic', '#video'], 'emoji': '🎬', 'ru': {'title': 'OpenS2V-Nexus: Революция в генерации видео на основе заданного содержания', 'desc': 'Статья представляет OpenS2V-Nexus - инфраструктуру для генерации видео на основе заданного содержания (Subject-to-Video, S2V). Она включает в себя OpenS2V-Eval - детальный бенчмарк для оценки качества генерируемых видео, и OpenS2V-5M - крупномасштабный датасет из 5 миллионов триплетов субъект-текст-видео. Авторы предлагают три автоматические метрики для оценки согласованности субъекта, естественности и релевантности текста в сгенерированных видео. Проведена комплексная оценка 16 репрезентативных S2V моделей, выявляющая их сильные и слабые стороны.'}, 'en': {'title': 'Revolutionizing Video Generation with Subject Fidelity', 'desc': 'The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models.'}, 'zh': {'title': '构建视频生成的新基准与数据集', 'desc': '本论文提出了Subject-to-Video (S2V) 生成的基础设施OpenS2V-Nexus，旨在创建忠实于参考内容的视频。我们引入了OpenS2V-Eval，一个细粒度的基准，专注于生成具有自然外观和身份保真度的一致视频。为了评估生成视频的质量，我们设计了三种自动化指标，分别量化主题一致性、自然性和文本相关性。最后，我们创建了一个包含五百万个高质量720P主题-文本-视频三元组的开放源代码数据集OpenS2V-5M，以支持未来的S2V生成研究。'}}}, {'id': 'https://huggingface.co/papers/2505.19641', 'title': 'SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond', 'url': 'https://huggingface.co/papers/2505.19641', 'abstract': 'SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.', 'score': 41, 'issue_id': 3996, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '4c75ba0f126de4b9', 'authors': ['Junteng Liu', 'Yuanxiang Fan', 'Zhuo Jiang', 'Han Ding', 'Yongyi Hu', 'Chi Zhang', 'Yiqi Shi', 'Shitong Weng', 'Aili Chen', 'Shiqi Chen', 'Yunan Huang', 'Mozhi Zhang', 'Pengyu Zhao', 'Junjie Yan', 'Junxian He'], 'affiliations': ['MiniMax', 'The City University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.19641.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#reasoning', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SynLogic: прорыв в обучении ИИ логическому мышлению', 'desc': 'SynLogic - это фреймворк для синтеза данных, который улучшает способности больших языковых моделей (LLM) к логическому рассуждению с помощью обучения с подкреплением. Он генерирует разнообразные задачи по логическому мышлению, которые можно легко проверить, что делает их идеальными для обучения с подкреплением. Эксперименты показали, что модели, обученные на данных SynLogic, достигают наилучших результатов в задачах логического рассуждения среди открытых датасетов. Более того, комбинирование данных SynLogic с математическими задачами и задачами по программированию улучшает общие способности моделей к рассуждению.'}, 'en': {'title': 'Enhancing AI Reasoning with SynLogic Framework', 'desc': 'SynLogic is a framework designed to improve the logical reasoning skills of Large Language Models (LLMs) using Reinforcement Learning (RL). It generates a diverse set of logical reasoning tasks, allowing for controlled data synthesis that can be adjusted in difficulty and quantity. The framework not only enhances the reasoning capabilities of LLMs but also improves their generalization across different domains by mixing logical reasoning data with mathematical and coding tasks. The results show that models trained with SynLogic achieve state-of-the-art performance, demonstrating its potential as a valuable resource for advancing AI reasoning.'}, 'zh': {'title': 'SynLogic：提升大型语言模型推理能力的关键', 'desc': 'SynLogic是一个数据合成框架，旨在通过强化学习（RL）提升大型语言模型（LLMs）的逻辑推理能力。该框架生成多样化的逻辑推理数据，涵盖35个不同的推理任务，并允许根据难度和数量进行控制合成。实验表明，使用SynLogic数据进行RL训练，能够显著提高推理性能，并在多个基准测试中超越现有的开源数据集。通过将SynLogic数据与数学和编码任务混合训练，进一步提升了这些领域的训练效率和推理泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20325', 'title': 'Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence', 'url': 'https://huggingface.co/papers/2505.20325', 'abstract': 'The Guided by Gut (GG) framework enhances LLM reasoning efficiently using intrinsic signals and token-level confidence, outperforming PRM-based methods with faster inference and lower memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.', 'score': 39, 'issue_id': 4006, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '3055ff28f0a7f0f6', 'authors': ['Amirhosein Ghasemabadi', 'Keith G. Mills', 'Baochun Li', 'Di Niu'], 'affiliations': ['ECE Department, University of Alberta', 'ECE Department, University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.20325.jpg', 'data': {'categories': ['#training', '#rl', '#math', '#optimization', '#small_models', '#inference', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное улучшение рассуждений LLM без внешних верификаторов', 'desc': 'Статья представляет новый фреймворк Guided by Gut (GG) для улучшения рассуждений больших языковых моделей (LLM). GG использует внутренние сигналы LLM и уверенность на уровне токенов, что позволяет достичь производительности методов на основе PRM, но с более быстрым выводом и меньшим использованием памяти. Фреймворк применяет легковесный поиск по дереву, управляемый только внутренними сигналами LLM. Эмпирические оценки показывают, что GG позволяет меньшим моделям достигать точности значительно больших моделей при снижении использования памяти GPU до 10 раз.'}, 'en': {'title': 'Efficient Reasoning with Guided by Gut: Smarter, Faster, Smaller!', 'desc': 'The Guided by Gut (GG) framework improves the reasoning capabilities of Large Language Models (LLMs) by using internal signals and confidence levels at the token level. Unlike traditional Test-Time Scaling (TTS) methods that depend on external Process Reward Models (PRMs), GG operates efficiently without these costly verifiers. It utilizes a lightweight tree search and enhances internal confidence through reinforcement learning fine-tuning. Empirical results show that GG allows smaller models to achieve high accuracy while significantly reducing memory usage and increasing inference speed compared to PRM-based approaches.'}, 'zh': {'title': '高效推理，超越规模的GG框架', 'desc': '本文提出了一种名为"Guided by Gut (GG)"的框架，旨在提高大型语言模型（LLM）的推理效率。GG框架通过利用内在信号和令牌级置信度，避免了对外部过程奖励模型（PRM）的依赖，从而实现了更快的推理速度和更低的内存使用。通过轻量级的树搜索和强化学习微调，GG能够在较小的模型上达到与更大模型相当的准确性，同时显著降低GPU内存消耗。实验证明，GG在数学推理基准测试中表现优异，展示了其在实际应用中的高效性。'}}}, {'id': 'https://huggingface.co/papers/2505.17813', 'title': "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning", 'url': 'https://huggingface.co/papers/2505.17813', 'abstract': 'Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.', 'score': 38, 'issue_id': 3992, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'd8d9d938b84c5ea6', 'authors': ['Michael Hassid', 'Gabriel Synnaeve', 'Yossi Adi', 'Roy Schwartz'], 'affiliations': ['FAIR Team, Meta', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2505.17813.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training'], 'emoji': '⚡', 'ru': {'title': 'Короче мысль - быстрее вывод: оптимизация рассуждений в LLM', 'desc': 'Статья исследует эффективность коротких цепочек рассуждений в крупных языковых моделях (LLM) для задач рассуждения. Авторы предлагают метод short-m@k, который выполняет параллельные генерации и выбирает ответ голосованием среди первых m завершенных процессов. Эксперименты показывают, что короткие цепочки часто дают более точные результаты, чем длинные, при меньших вычислительных затратах. Дополнительно, авторы обнаружили, что обучение на коротких цепочках рассуждений приводит к лучшей производительности модели.'}, 'en': {'title': 'Shorter Chains, Smarter Reasoning!', 'desc': 'This paper investigates the effectiveness of reasoning in large language models (LLMs) by comparing long and short reasoning chains. The authors find that shorter reasoning chains can yield significantly more accurate answers, with improvements of up to 34.5% compared to longer chains. They introduce a new method called short-m@k, which allows for parallel processing of multiple reasoning chains and selects the final answer based on majority voting. Their results indicate that shorter reasoning processes not only enhance performance but also reduce computational costs and inference time, challenging the traditional belief that longer reasoning leads to better outcomes.'}, 'zh': {'title': '短思维链，提升推理能力！', 'desc': '这篇论文探讨了大型语言模型（LLMs）在推理任务中依赖长思维链的假设。研究表明，较短的推理链在回答问题时更可能产生正确答案，准确率比最长链高出34.5%。基于此，提出了一种新的推理方法short-m@k，通过并行生成k个独立的思维过程，并在第一个m个完成后停止计算，最终答案通过多数投票选出。研究结果表明，短思维链的训练可以提高模型性能，挑战了长思维链必然带来更好推理能力的传统观念。'}}}, {'id': 'https://huggingface.co/papers/2505.21189', 'title': 'Exploring the Latent Capacity of LLMs for One-Step Text Generation', 'url': 'https://huggingface.co/papers/2505.21189', 'abstract': 'LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  \t\t\t\t\tAI-generated summary \t\t\t\t A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.', 'score': 37, 'issue_id': 3996, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '993d2720f6fed612', 'authors': ['Gleb Mezentsev', 'Ivan Oseledets'], 'affiliations': ['AIRI Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21189.jpg', 'data': {'categories': ['#data', '#long_context', '#architecture', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'Мгновенная генерация длинных текстов с помощью LLM', 'desc': 'Исследование показывает, что большие языковые модели (LLM) способны генерировать длинные тексты в один проход, используя только два специально обученных эмбеддинга. Это демонстрирует неожиданную способность LLM к многотокенной генерации без итеративного декодирования. Авторы изучают свойства этих эмбеддингов и информацию, которую они кодируют. Исследование также выявляет, что эти представления формируют связные локальные области в пространстве эмбеддингов.'}, 'en': {'title': 'Unlocking Multi-Token Generation in LLMs Without Autoregression', 'desc': 'This paper investigates the ability of large language models (LLMs) to generate long text segments in a single forward pass using learned embeddings. It demonstrates that LLMs can produce hundreds of accurate tokens without relying on autoregressive methods, which typically require iterative decoding. The study reveals that by using just two learned embeddings, LLMs can reconstruct extensive text, showcasing a previously underexplored capability. Additionally, the authors analyze the properties of these embeddings, suggesting that they encode meaningful information and could lead to the development of a dedicated encoder for improved text generation.'}, 'zh': {'title': '探索LLMs的多标记生成能力', 'desc': '这篇论文探讨了大型语言模型（LLMs）在没有自回归的情况下生成长文本的能力。研究表明，冻结的LLMs可以仅通过两个学习到的嵌入，在一次前向传播中生成数百个准确的标记。这揭示了LLMs在多标记生成方面的潜力，超出了传统的迭代解码方法。我们还分析了这些嵌入的行为，并提供了它们所编码信息的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.19000', 'title': 'VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied\n  Iterative Policy Optimization', 'url': 'https://huggingface.co/papers/2505.19000', 'abstract': "A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.", 'score': 35, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '058fcf46b0f20cc6', 'authors': ['Yunxin Li', 'Xinyu Chen', 'Zitao Li', 'Zhenyu Liu', 'Longyue Wang', 'Wenhan Luo', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Group', 'Division of AMC and Department of ECE, HKUST', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19000.jpg', 'data': {'categories': ['#reasoning', '#training', '#video', '#optimization', '#rl', '#rlhf'], 'emoji': '🎥', 'ru': {'title': 'VerIPO: Улучшение рассуждений видео-LLM с помощью верификатора', 'desc': 'Статья представляет метод VerIPO для улучшения способностей видео-LLM к рассуждениям. Метод использует Verifier между фазами GRPO и DPO для оптимизации генерации длинных цепочек рассуждений. VerIPO позволяет создавать качественные контрастные данные, что ускоряет обучение в 7 раз по сравнению с GRPO. Эксперименты показывают превосходство VerIPO над существующими методами в задачах видео-рассуждений.'}, 'en': {'title': 'Enhancing Video Reasoning with Verifier-guided Optimization', 'desc': 'This paper introduces VerIPO, a new method for improving Video Large Language Models (Video-LLMs) using a Verifier-guided Iterative Policy Optimization approach. It addresses the limitations of existing Reinforcement Fine-Tuning methods by incorporating a Rollout-Aware Verifier that enhances the quality of reasoning chains during training. By creating high-quality contrastive data, this method allows for faster and more effective optimization, achieving results that are significantly better than traditional methods. Experimental findings show that VerIPO not only speeds up the training process but also improves the contextual consistency and length of reasoning outputs in video tasks.'}, 'zh': {'title': '验证者引导的迭代优化，提升视频推理能力', 'desc': '本文提出了一种名为VerIPO的验证者引导迭代策略优化方法，旨在提升视频大型语言模型（Video-LLMs）的推理能力。该方法通过在GRPO和DPO阶段之间引入一个回滚感知验证器，形成GRPO-验证器-DPO训练循环，从而实现更快且更有效的优化。验证器利用小型语言模型评估推理逻辑，生成高质量的对比数据，促进了长链推理的生成。实验结果表明，VerIPO在优化速度和推理质量上均显著优于传统的GRPO方法。'}}}, {'id': 'https://huggingface.co/papers/2505.16459', 'title': 'MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.16459', 'abstract': 'The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.', 'score': 35, 'issue_id': 3991, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'd18f80036a817d7c', 'authors': ['Guiyao Tie', 'Xueyang Zhou', 'Tianhe Gu', 'Ruihang Zhang', 'Chaoran Hu', 'Sizhe Zhang', 'Mengqu Sun', 'Yan Zhang', 'Pan Zhou', 'Lichao Sun'], 'affiliations': ['Huazhong University of Science and Technology', 'Lehigh University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16459.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'MMMR: Новый стандарт оценки мультимодального мышления ИИ', 'desc': 'Статья представляет новый бенчмарк MMMR для оценки мультимодального рассуждения в крупных языковых моделях (MLLM). MMMR включает набор данных из 1083 сложных вопросов, охватывающих шесть типов рассуждений, и конвейер оценки качества рассуждений (RTEP). Результаты показывают, что MLLM с промежуточными этапами мышления превосходят модели без них, но даже лучшие модели страдают от несогласованности и чрезмерного анализа. Бенчмарк выявляет разрыв между точностью и качеством рассуждений, предоставляя основу для улучшения мультимодальных систем рассуждений.'}, 'en': {'title': 'Evaluating Multi-Modal Reasoning: The MMMR Benchmark', 'desc': 'The MMMR benchmark is designed to evaluate the reasoning abilities of Multi-Modal Large Language Models (MLLMs) by focusing on their thinking quality across various reasoning types. It includes a challenging dataset with 1,083 questions that require complex reasoning, and a modular evaluation pipeline to assess reasoning quality beyond just accuracy. The study finds that while MLLMs with intermediate thinking traces perform better than those without, they still exhibit issues like inconsistency and overthinking. This benchmark aims to bridge the gap between accuracy and reasoning quality, providing a structured approach for future advancements in multi-modal reasoning systems.'}, 'zh': {'title': '多模态推理的新基准：MMMR', 'desc': 'MMMR基准测试评估多模态大语言模型（MLLMs）的推理能力，重点在于通过多样的推理类型和模块化评估流程来评估思维质量。尽管MLLMs在语言、视觉和结构化输入的统一处理上取得了进展，但其推理能力仍然不够清晰，缺乏标准化的评估基准。MMMR包含一个高难度的数据集和一个推理追踪评估管道，旨在超越准确性评估，关注推理的相关性、一致性和结构化错误注释。通过实证结果，MMMR揭示了准确性与推理质量之间的差距，为未来模型的发展提供了可操作的评估框架。'}}}, {'id': 'https://huggingface.co/papers/2505.21496', 'title': 'UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents', 'url': 'https://huggingface.co/papers/2505.21496', 'abstract': 'In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.', 'score': 33, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'db6e0d226a2c500e', 'authors': ['Han Xiao', 'Guozhi Wang', 'Yuxiang Chai', 'Zimu Lu', 'Weifeng Lin', 'Hao He', 'Lue Fan', 'Liuyang Bian', 'Rui Hu', 'Liang Liu', 'Shuai Ren', 'Yafei Wen', 'Xiaoxin Chen', 'Aojun Zhou', 'Hongsheng Li'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21496.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#synthetic', '#open_source', '#dataset', '#agents', '#training', '#data'], 'emoji': '🧞', 'ru': {'title': 'UI-Genie: самообучающийся ИИ-помощник для графических интерфейсов', 'desc': 'UI-Genie - это самосовершенствующаяся система для агентов графического интерфейса, решающая проблемы верификации результатов и масштабирования качественных обучающих данных. Система включает модель вознаграждения UI-Genie-RM с архитектурой, объединяющей изображения и текст, и конвейер самосовершенствования для расширения решаемых задач. Для обучения модели созданы наборы данных UI-Genie-RM-517k и UI-Genie-Agent-16k без ручной разметки. Экспериментальные результаты показывают, что UI-Genie достигает наилучших показателей в нескольких эталонных тестах для агентов графического интерфейса.'}, 'en': {'title': 'UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models', 'desc': 'This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach.'}, 'zh': {'title': 'UI-Genie：自我改进的GUI代理框架', 'desc': '本文介绍了UI-Genie，这是一个自我改进的框架，旨在解决GUI代理中的两个主要挑战：轨迹结果的验证困难和高质量训练数据的可扩展性。我们通过奖励模型和自我改进管道来解决这些问题。奖励模型UI-Genie-RM采用图像-文本交错架构，有效处理历史上下文，并统一了动作级和任务级奖励。自我改进管道通过奖励引导探索和动态环境中的结果验证，逐步扩展可解决的复杂GUI任务，从而提升代理和奖励模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.18875', 'title': 'Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation', 'url': 'https://huggingface.co/papers/2505.18875', 'abstract': 'SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.', 'score': 32, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'bc68e232d8897ad4', 'authors': ['Shuo Yang', 'Haocheng Xi', 'Yilong Zhao', 'Muyang Li', 'Jintao Zhang', 'Han Cai', 'Yujun Lin', 'Xiuyu Li', 'Chenfeng Xu', 'Kelly Peng', 'Jianfei Chen', 'Song Han', 'Kurt Keutzer', 'Ion Stoica'], 'affiliations': ['MIT', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.18875.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#optimization'], 'emoji': '🎞️', 'ru': {'title': 'Семантическая оптимизация для быстрой и качественной генерации видео', 'desc': 'SVG2 - это фреймворк для улучшения эффективности и качества генерации видео без дополнительного обучения. Он использует семантически-ориентированную перестановку для точной идентификации и обработки критических токенов. SVG2 применяет кластеризацию k-means для группировки токенов по семантическому сходству, что повышает точность представления. Фреймворк также включает динамический контроль бюджета top-p и оптимизированные ядра, достигая ускорения до 2.30x при сохранении высокого качества генерации.'}, 'en': {'title': 'Maximizing Video Generation Efficiency with SVG2', 'desc': 'SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency.'}, 'zh': {'title': 'SVG2：提升视频生成效率与质量的创新框架', 'desc': 'SVG2是一个无需训练的框架，通过准确识别和处理关键标记，提升视频生成的效率和质量。它采用语义感知的排列和动态预算控制，解决了现有方法在计算预算下生成质量不佳的问题。SVG2通过k-means聚类和重新排列标记，确保了精确的聚类表示，从而提高了识别准确性，并减少了计算浪费。该框架在保持生成质量的同时，实现了高达2.30倍的加速。'}}}, {'id': 'https://huggingface.co/papers/2505.21333', 'title': 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios', 'url': 'https://huggingface.co/papers/2505.21333', 'abstract': 'MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.', 'score': 29, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '25639980b7f7add5', 'authors': ['Yang Shi', 'Huanqian Wang', 'Wulin Xie', 'Huanyao Zhang', 'Lijie Zhao', 'Yi-Fan Zhang', 'Xinfeng Li', 'Chaoyou Fu', 'Zhuoer Wen', 'Wenting Liu', 'Zhuoran Zhang', 'Xinlong Chen', 'Bohan Zeng', 'Sihan Yang', 'Yuanxing Zhang', 'Pengfei Wan', 'Haotian Wang', 'Wenjing Yang'], 'affiliations': ['CASIA', 'CUHKSZ', 'Kuaishou', 'NTU', 'PKU', 'THU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.21333.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#reasoning', '#video'], 'emoji': '🎥', 'ru': {'title': 'Ограничения мультимодальных моделей в задаче OCR на видео', 'desc': 'Мультимодальные большие языковые модели (MLLM) показывают невысокую точность в задаче оптического распознавания символов (OCR) на видео из-за размытия при движении, временных вариаций и визуальных эффектов. Авторы представляют бенчмарк MME-VideoOCR, включающий 10 категорий задач и 25 отдельных заданий для оценки возможностей MLLM в видео OCR. Эксперименты на 18 современных MLLM выявили ограничения в пространственно-временном рассуждении и языковых предубеждениях моделей. Исследование подчеркивает важность высокого разрешения и достаточного временного охвата для надежного OCR в динамических видеосценариях.'}, 'en': {'title': 'Enhancing Video OCR: Bridging the Gap in MLLM Performance', 'desc': 'This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames.'}, 'zh': {'title': '提升视频OCR的多模态基准挑战', 'desc': '多模态大型语言模型（MLLMs）在静态图像的光学字符识别（OCR）中表现良好，但在视频OCR中效果显著下降。这是由于视频内容中的运动模糊、时间变化和视觉效果等因素影响。为了解决这些问题，我们提出了MME-VideoOCR基准，涵盖了多种视频OCR应用场景，包含10个任务类别和25个具体任务。我们的研究表明，现有的MLLMs在处理需要整体视频理解的任务时能力有限，尤其是在时空推理和跨帧信息整合方面。'}}}, {'id': 'https://huggingface.co/papers/2505.20355', 'title': 'GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.20355', 'abstract': "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git", 'score': 28, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'd4035428ea14ce6b', 'authors': ['Yeonjoon Jung', 'Daehyun Ahn', 'Hyungjun Kim', 'Taesu Kim', 'Eunhyeok Park'], 'affiliations': ['POSTECH', 'SqueezeBits'], 'pdf_title_img': 'assets/pdf/title_img/2505.20355.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#optimization'], 'emoji': '🧩', 'ru': {'title': 'GraLoRA: Гранулярная низкоранговая адаптация для эффективной настройки генеративных моделей', 'desc': 'Статья представляет новый метод адаптации моделей машинного обучения под названием Granular Low-Rank Adaptation (GraLoRA). GraLoRA преодолевает ограничения популярного метода Low-Rank Adaptation (LoRA), связанные с переобучением при увеличении ранга. Метод разбивает весовые матрицы на подблоки, каждый со своим низкоранговым адаптером, что позволяет эффективно увеличить репрезентативную способность модели. Эксперименты показывают, что GraLoRA превосходит LoRA и другие базовые методы в задачах генерации кода и здравого смысла, достигая улучшения до 8.5% в метрике Pass@1 на датасете HumanEval+.'}, 'en': {'title': 'GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation', 'desc': 'This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs.'}, 'zh': {'title': '颗粒低秩适应：超越LoRA的高效微调', 'desc': '低秩适应（LoRA）是一种流行的参数高效微调方法，因其简单有效而受到重视。尽管最近有所改进，LoRA仍然面临一个根本性限制：当瓶颈加宽时容易过拟合。我们提出了一种新结构，称为颗粒低秩适应（GraLoRA），它将权重矩阵划分为子块，每个子块都有自己的低秩适配器，从而克服了LoRA的局限性。实验表明，GraLoRA在代码生成和常识推理基准上表现优于LoRA，具有更强的可扩展性和鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2505.21374', 'title': 'Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?', 'url': 'https://huggingface.co/papers/2505.21374', 'abstract': 'Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.', 'score': 26, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '0683137770e562ab', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Teng Wang', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.21374.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#video'], 'emoji': '🕵️', 'ru': {'title': 'Шерлок Холмс для ИИ: новый вызов в понимании видео', 'desc': 'Video-Holmes - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к сложным рассуждениям на основе видео. Он использует короткометражные фильмы-саспенс и состоит из 1837 вопросов по 270 аннотированным видео, охватывающих 7 специально разработанных задач. Бенчмарк выявил значительные трудности современных моделей в интеграции информации и поиске ключевых подсказок по сравнению с экспертами-людьми. Лучшая модель Gemini-2.5-Pro достигла точности всего 45%, что подчеркивает сложность задачи и необходимость дальнейших исследований в этой области.'}, 'en': {'title': 'Video-Holmes: A New Benchmark for Complex Video Reasoning', 'desc': 'The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning.'}, 'zh': {'title': 'Video-Holmes：激励模型更像人类推理的基准测试', 'desc': 'Video-Holmes基准测试评估了多模态大语言模型（MLLMs）在复杂视频推理方面的能力，特别是通过悬疑短片来揭示与人类专家相比的信息整合挑战。该基准包含来自270部手动注释的悬疑短片的1837个问题，设计了七个任务，要求模型主动寻找和连接分散在不同视频片段中的多个相关视觉线索。尽管现有的MLLMs在视觉感知方面表现良好，但在信息整合上却面临重大困难，许多模型的准确率低于40%。我们希望Video-Holmes能够激励模型更像人类进行推理，并强调这一领域的持续挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.21297', 'title': 'rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset', 'url': 'https://huggingface.co/papers/2505.21297', 'abstract': 'A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.', 'score': 22, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '1fdb1a9edd20c73b', 'authors': ['Yifei Liu', 'Li Lyna Zhang', 'Yi Zhu', 'Bingcheng Dong', 'Xudong Zhou', 'Ning Shang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Dalian University of Technology', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21297.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#reasoning', '#data'], 'emoji': '🧠', 'ru': {'title': 'rStar-Coder: прорыв в обучении языковых моделей рассуждениям о коде', 'desc': 'Исследователи представили rStar-Coder - крупномасштабный датасет для улучшения способностей языковых моделей (LLM) в рассуждениях о коде. Датасет содержит 418 тысяч задач по программированию соревновательного уровня, 580 тысяч подробных решений и тестовые примеры различной сложности. Использование rStar-Coder значительно повысило производительность моделей Qwen на различных бенчмарках, позволив им достичь результатов, сравнимых с передовыми LLM для рассуждений о коде, но при гораздо меньших размерах моделей. На бенчмарке LiveCodeBench модель Qwen2.5-14B улучшила свой результат с 23.3% до 62.5% после обучения на rStar-Coder.'}, 'en': {'title': 'Unlocking Code Reasoning with rStar-Coder', 'desc': 'The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks.'}, 'zh': {'title': '提升代码推理能力的rStar-Coder数据集', 'desc': 'rStar-Coder是一个大规模的数据集，旨在提升大语言模型（LLMs）在代码推理方面的能力。该数据集包含418,000个竞争级别的代码问题和580,000个长推理解决方案，配备丰富的测试用例，涵盖不同难度。通过三项核心贡献，rStar-Coder提供了可验证的输入输出测试案例，确保了解决方案的有效性。实验结果显示，使用rStar-Coder的数据集，Qwen模型在多个代码推理基准测试中表现优异，显著提高了模型的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.18943', 'title': 'MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems', 'url': 'https://huggingface.co/papers/2505.18943', 'abstract': "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.", 'score': 20, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '718f1062d34a47a7', 'authors': ['Xuanming Zhang', 'Yuxuan Chen', 'Min-Hsuan Yeh', 'Yixuan Li'], 'affiliations': ['Tsinghua University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.18943.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agents', '#reasoning', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'MetaMind: Искусственный интеллект с человеческим социальным пониманием', 'desc': 'MetaMind - это многоагентная система, улучшающая способность больших языковых моделей выполнять задачи теории сознания. Она разбивает социальное понимание на генерацию гипотез, их уточнение и генерацию ответов. MetaMind достигает уровня человека в ключевых задачах теории сознания, показывая улучшение на 35.7% в реальных социальных сценариях. Система использует три агента: агент теории сознания, доменный агент и агент ответов, что позволяет балансировать контекстуальную правдоподобность, социальную уместность и адаптацию к пользователю.'}, 'en': {'title': 'Empowering AI with Human-like Social Intelligence', 'desc': 'MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions.'}, 'zh': {'title': 'MetaMind：提升AI的社会智能', 'desc': 'MetaMind是一个多智能体框架，灵感来源于元认知，旨在提升大型语言模型（LLMs）在心智理论任务中的表现。该框架将社会理解分解为三个协作阶段：首先，心智理论代理生成用户心理状态的假设；其次，领域代理利用文化规范和伦理约束来细化这些假设；最后，响应代理生成符合上下文的适当回应。通过这种方式，MetaMind在三个具有挑战性的基准测试中实现了最先进的性能，首次使LLMs在关键的心智理论任务上达到人类水平。'}}}, {'id': 'https://huggingface.co/papers/2505.21334', 'title': 'HoliTom: Holistic Token Merging for Fast Video Large Language Models', 'url': 'https://huggingface.co/papers/2505.21334', 'abstract': "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.", 'score': 17, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'b07c0c110f38f89c', 'authors': ['Kele Shao', 'Keda Tao', 'Can Qin', 'Haoxuan You', 'Yang Sui', 'Huan Wang'], 'affiliations': ['Columbia University', 'Rice University', 'Salesforce AI Research', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21334.jpg', 'data': {'categories': ['#inference', '#video', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'HoliTom: Революционное сжатие токенов для эффективных видео-LLM', 'desc': 'HoliTom - это новый фреймворк для эффективного сжатия токенов в видео-LLM моделях. Он сочетает внешнюю обрезку LLM через глобальную временную сегментацию с внутренним объединением токенов на основе сходства. Такой подход позволяет значительно снизить вычислительные затраты без ущерба для производительности. В результате удается сократить количество операций с плавающей запятой до 6,9% от исходного, сохранив 99,1% производительности.'}, 'en': {'title': 'HoliTom: Efficient Video LLMs through Smart Token Pruning', 'desc': 'HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference.'}, 'zh': {'title': 'HoliTom：高效视频LLM的全新剪枝策略', 'desc': 'HoliTom是一种新颖的训练无关的整体令牌合并框架，旨在通过全球时间分割进行外部LLM剪枝，显著减少视频大语言模型（视频LLM）的计算效率问题。该方法结合了外部LLM剪枝和内部LLM基于令牌相似性的合并，能够在不牺牲性能的情况下，减少视觉令牌超过90%。通过这种方式，HoliTom有效缓解了LLM的计算负担，同时保持了99.1%的原始性能。评估结果显示，该方法在计算成本和解码速度上均有显著提升，展示了其在高效视频LLM推理中的实际应用价值。'}}}, {'id': 'https://huggingface.co/papers/2505.17952', 'title': 'Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL', 'url': 'https://huggingface.co/papers/2505.17952', 'abstract': 'Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.', 'score': 17, 'issue_id': 3999, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6dbd0f852449229e', 'authors': ['Che Liu', 'Haozhe Wang', 'Jiazhen Pan', 'Zhongwei Wan', 'Yong Dai', 'Fangzhen Lin', 'Wenjia Bai', 'Daniel Rueckert', 'Rossella Arcucci'], 'affiliations': ['Fudan University', 'HKUST', 'Imperial College London', 'Ohio State University', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.17952.jpg', 'data': {'categories': ['#healthcare', '#reasoning', '#benchmark', '#rl', '#dataset', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Революция в обучении медицинских ИИ: рассуждение без явных инструкций', 'desc': 'Статья представляет AlphaMed - первую медицинскую модель большого языка (LLM), демонстрирующую способность к рассуждению, развитую исключительно через обучение с подкреплением (RL). Модель достигает лучших результатов на шести медицинских тестах вопросов и ответов, превосходя модели, обученные традиционными методами. Исследование показывает, что информативность датасета является ключевым фактором в развитии способности к рассуждению. Авторы также отмечают необходимость разработки более сложных тестов для оценки медицинских LLM.'}, 'en': {'title': 'Reinforcement Learning Powers Medical Reasoning Without Supervision', 'desc': 'This paper introduces AlphaMed, a medical large language model (LLM) that enhances reasoning capabilities through reinforcement learning (RL) without the need for supervised fine-tuning (SFT) or costly chain-of-thought (CoT) data. AlphaMed utilizes minimalist rule-based rewards on publicly available multiple-choice question-answering datasets, achieving state-of-the-art performance on various medical QA benchmarks. The study reveals that the quality and diversity of the dataset significantly influence reasoning performance, demonstrating that informative datasets can effectively promote reasoning skills. Additionally, the authors highlight the limitations of current evaluation methods and advocate for the development of more challenging benchmarks to better assess reasoning in medical contexts.'}, 'zh': {'title': '通过强化学习提升医学推理能力', 'desc': '本研究提出了AlphaMed，这是首个通过强化学习（RL）实现推理能力的医学大型语言模型（LLM），无需监督微调（SFT）或闭源模型的链式思维（CoT）数据。AlphaMed在六个医学问答基准测试中取得了最先进的结果，超越了传统的SFT+RL训练模型。研究表明，数据集的信息量是推理性能的关键驱动因素，使用信息丰富的多项选择问答数据进行简约的RL能够有效诱导推理能力。我们还发现，当前评估方法存在局限性，需要更具挑战性和推理导向的医学问答基准。'}}}, {'id': 'https://huggingface.co/papers/2505.14064', 'title': 'NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI', 'url': 'https://huggingface.co/papers/2505.14064', 'abstract': 'NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously unknown categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.   We therefore present NOVA, a challenging, real-life evaluation-only benchmark of sim900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an extreme stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.', 'score': 16, 'issue_id': 3999, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '168ea71152c0b54f', 'authors': ['Cosmin I. Bercea', 'Jun Li', 'Philipp Raffler', 'Evamaria O. Riedel', 'Lena Schmitzer', 'Angela Kurz', 'Felix Bitzer', 'Paula Roßmüller', 'Julian Canisius', 'Mirjam L. Beyrle', 'Che Liu', 'Wenjia Bai', 'Bernhard Kainz', 'Julia A. Schnabel', 'Benedikt Wiestler'], 'affiliations': ['FAU Erlangen-Nürnberg', 'Helmholtz Center Munich', 'Imperial College London', 'Kings College London', 'Klinikum Rechts der Isar', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.14064.jpg', 'data': {'categories': ['#healthcare', '#cv', '#reasoning', '#benchmark', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'NOVA: Экстремальный тест на обобщение ИИ в медицинской визуализации', 'desc': 'NOVA - это новый бенчмарк для оценки мультимодальных моделей на редких патологиях МРТ головного мозга. Он включает 900 снимков с 281 редкой патологией и позволяет оценить способность моделей к обнаружению аномалий, визуальному описанию и диагностическому рассуждению. NOVA служит экстремальным стресс-тестом для обобщения вне распределения, так как модели должны преодолеть разрыв как во внешнем виде образцов, так и в семантическом пространстве. Базовые результаты ведущих мультимодальных моделей показывают значительное снижение производительности по всем задачам.'}, 'en': {'title': 'NOVA: Stress-Testing AI for Rare MRI Pathologies', 'desc': 'NOVA is a benchmark designed to evaluate vision-language models specifically on rare MRI pathologies that are clinically relevant. It focuses on out-of-distribution detection and open-world recognition, which are crucial for models to handle unseen data effectively. By using a dataset of 900 brain MRI scans with 281 rare conditions, NOVA tests models on their ability to generalize beyond their training data. The results from leading models show significant performance drops, highlighting the need for improved capabilities in detecting and reasoning about unknown anomalies in medical imaging.'}, 'zh': {'title': 'NOVA：挑战视觉-语言模型的稀有病理识别', 'desc': 'NOVA是一个用于评估视觉-语言模型在稀有临床相关MRI病理方面的基准，挑战模型在分布外和开放世界识别能力。该基准包含900个脑部MRI扫描，涵盖281种稀有病理和不同的获取协议。每个案例都包括丰富的临床叙述和双盲专家的边界框注释，支持异常定位、视觉描述和诊断推理的联合评估。NOVA从未用于训练，因此它是对模型在分布外泛化能力的极限压力测试，揭示了当前模型在处理未知异常时的性能下降。'}}}, {'id': 'https://huggingface.co/papers/2505.21505', 'title': "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective", 'url': 'https://huggingface.co/papers/2505.21505', 'abstract': "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.", 'score': 15, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'b77701cf90420b4c', 'authors': ['Shimao Zhang', 'Zhejian Lai', 'Xiang Liu', 'Shuaijie She', 'Xiao Liu', 'Yeyun Gong', 'Shujian Huang', 'Jiajun Chen'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21505.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#alignment'], 'emoji': '🌐', 'ru': {'title': 'Раскрытие тайн многоязычности в нейронах LLM', 'desc': 'Исследование предлагает алгоритм более точной идентификации нейронов для обнаружения языково-специфичных и языково-агностических нейронов в больших языковых моделях (LLM). Авторы анализируют влияние этого подхода на многоязычное выравнивание и возможности моделей. Исследование рассматривает многоязычное понимание, общее семантическое рассуждение, многоязычное преобразование выходных данных и вывод в пространстве словаря. Работа предоставляет эмпирические результаты и ценные выводы для лучшего понимания многоязычного выравнивания и многоязычных возможностей LLM.'}, 'en': {'title': 'Enhancing Multilingual Capabilities in LLMs through Neuron Identification', 'desc': "This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities."}, 'zh': {'title': '细粒度神经元识别，提升多语言能力', 'desc': '本研究提出了一种更细粒度的神经元识别算法，用于检测大型语言模型（LLMs）中的语言特定神经元和语言无关神经元。我们分析了多语言理解、共享语义推理、多语言输出转换和词汇空间输出等方面对多语言对齐和能力的影响。研究表明，存在在处理不同语言时选择性激活的语言特定神经元，这为深入理解LLMs在多语言场景中的机制提供了新视角。通过对不同类型神经元的系统分析，我们为更好地理解LLMs的多语言对齐和能力提供了实证结果和有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.20275', 'title': 'ImgEdit: A Unified Image Editing Dataset and Benchmark', 'url': 'https://huggingface.co/papers/2505.20275', 'abstract': 'Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.', 'score': 14, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ab1a7940b7d4c31c', 'authors': ['Yang Ye', 'Xianyi He', 'Zongjian Li', 'Bin Lin', 'Shenghai Yuan', 'Zhiyuan Yan', 'Bohan Hou', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20275.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#cv', '#data'], 'emoji': '🖼️', 'ru': {'title': 'ImgEdit: прорыв в редактировании изображений с помощью ИИ', 'desc': 'Исследователи представили ImgEdit - крупномасштабный набор данных для редактирования изображений, содержащий 1,2 миллиона тщательно отобранных пар изображений до и после редактирования. На основе этого набора данных была обучена модель ImgEdit-E1, использующая мультимодальную языковую модель для обработки изображений и текстовых инструкций. Авторы также разработали бенчмарк ImgEdit-Bench для оценки моделей редактирования изображений. Результаты показывают, что ImgEdit-E1 превосходит существующие открытые модели по нескольким задачам редактирования.'}, 'en': {'title': 'Empowering Open-Source Image Editing with ImgEdit Dataset', 'desc': 'This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks.'}, 'zh': {'title': 'ImgEdit：高质量图像编辑的突破', 'desc': '最近生成模型的进展使得高保真文本到图像的生成成为可能。然而，开源图像编辑模型仍然落后于专有模型，主要是由于高质量数据的缺乏和基准测试不足。为了解决这些问题，我们推出了ImgEdit，这是一个大规模的高质量图像编辑数据集，包含120万个精心策划的编辑对，涵盖新颖和复杂的单轮编辑以及具有挑战性的多轮任务。我们使用多阶段流程确保数据质量，整合了先进的视觉语言模型、检测模型、分割模型以及特定任务的修复程序和严格的后处理。'}}}, {'id': 'https://huggingface.co/papers/2505.20322', 'title': 'Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms', 'url': 'https://huggingface.co/papers/2505.20322', 'abstract': 'Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.', 'score': 13, 'issue_id': 3990, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '63cd6df71ddaefa4', 'authors': ['Mengru Wang', 'Ziwen Xu', 'Shengyu Mao', 'Shumin Deng', 'Zhaopeng Tu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['NUS-NCS Joint Lab, Singapore', 'National University of Singapore', 'Tencent AI Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20322.jpg', 'data': {'categories': ['#rl', '#reasoning', '#security', '#training', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'Точное управление языковыми моделями через атомарные компоненты знаний', 'desc': 'Статья представляет новый метод под названием Steering Target Atoms (STA) для точного контроля над генерацией языковых моделей. Метод использует разреженные автоэнкодеры для выделения атомарных компонентов знаний в высокоразмерных пространствах. Эксперименты показывают эффективность STA в повышении безопасности и надежности языковых моделей. Метод демонстрирует особую устойчивость и гибкость в сценариях состязательного машинного обучения.'}, 'en': {'title': 'Enhancing Control in Language Models with Steering Target Atoms', 'desc': 'This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models.'}, 'zh': {'title': '提升语言模型安全性的创新方法', 'desc': '本文提出了一种新的方法，称为引导目标原子（STA），旨在提高语言模型生成的安全性和可靠性。通过使用稀疏自编码器（SAE），该方法能够分离和操控高维空间中的知识组件，从而增强对模型行为的控制。实验结果表明，STA在对抗性场景中表现出更强的鲁棒性和灵活性。此外，我们还将这一引导策略应用于大型推理模型，验证了其在精确推理控制中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.17332', 'title': 'SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for\n  Enterprise Use', 'url': 'https://huggingface.co/papers/2505.17332', 'abstract': "SweEval is a benchmark for evaluating Large Language Models' compliance with ethical guidelines and cultural nuances when instructed to include offensive language.  \t\t\t\t\tAI-generated summary \t\t\t\t Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity.", 'score': 13, 'issue_id': 4006, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'faf1d1381f867cc1', 'authors': ['Hitesh Laxmichand Patel', 'Amit Agarwal', 'Arion Das', 'Bhargava Kumar', 'Srikant Panda', 'Priyaranjan Pattnayak', 'Taki Hasan Rafi', 'Tejaswini Kumar', 'Dong-Kyu Chae'], 'affiliations': ['Columbia University', 'Hanyang University', 'Indian Institute of Information Technology Ranchi', 'Oracle AI', 'TD Securities'], 'pdf_title_img': 'assets/pdf/title_img/2505.17332.jpg', 'data': {'categories': ['#alignment', '#multilingual', '#open_source', '#ethics', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Этичные языковые модели: оценка и контроль нецензурной лексики', 'desc': 'SweEval - это бенчмарк для оценки способности больших языковых моделей (LLM) соблюдать этические нормы и учитывать культурные особенности при генерации текста с нецензурной лексикой. Он симулирует реальные сценарии с различными тонами и контекстами, намеренно инструктируя модель использовать ругательства. Бенчмарк оценивает, насколько LLM следуют или сопротивляются таким неуместным инструкциям. SweEval помогает создавать этически выверенные системы искусственного интеллекта для корпоративного использования.'}, 'en': {'title': 'Evaluating Ethical Compliance in Language Models with SweEval', 'desc': "SweEval is a benchmark designed to evaluate how well Large Language Models (LLMs) adhere to ethical guidelines when faced with prompts that include offensive language. It simulates real-world scenarios by varying the tone and context of the instructions, allowing researchers to assess the models' responses to inappropriate requests. The benchmark focuses on the models' ability to recognize and handle unsafe language while maintaining cultural sensitivity and compliance with ethical standards. By providing a dataset and code, SweEval aims to foster the development of AI systems that are both effective and ethically responsible in enterprise applications."}, 'zh': {'title': 'SweEval：评估语言模型的伦理合规性', 'desc': 'SweEval是一个基准测试，用于评估大型语言模型在处理冒犯性语言时的伦理合规性和文化细微差别。随着企业客户越来越多地采用大型语言模型进行重要的沟通任务，确保这些模型能够理解不同的文化和语言背景变得至关重要。该基准通过模拟真实场景，评估模型在面对不当指令时的反应，以及它们在伦理框架和语言理解能力方面的表现。我们发布了数据集和代码，以推动构建符合伦理的人工智能系统的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.21493', 'title': 'Reinforcing General Reasoning without Verifiers', 'url': 'https://huggingface.co/papers/2505.21493', 'abstract': 'The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.', 'score': 11, 'issue_id': 4003, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'f16e69476bc70236', 'authors': ['Xiangxin Zhou', 'Zichen Liu', 'Anya Sims', 'Haonan Wang', 'Tianyu Pang', 'Chongxuan Li', 'Liang Wang', 'Min Lin', 'Chao Du'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'Renmin University of China', 'Sea AI Lab, Singapore', 'University of Chinese Academy of Sciences', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.21493.jpg', 'data': {'categories': ['#optimization', '#math', '#rl', '#benchmark', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'VeriFree: обучение LLM без верификатора для универсальных рассуждений', 'desc': 'Статья представляет новый метод обучения больших языковых моделей (LLM) под названием VeriFree. Этот метод использует обучение с подкреплением для максимизации вероятности генерации эталонного ответа, избегая необходимости в отдельной модели-верификаторе. VeriFree показывает результаты, сопоставимые или превосходящие методы с верификатором на различных тестах, включая MMLU-Pro и GPQA. Метод позволяет расширить обучение в стиле DeepSeek-R1-Zero на общие области рассуждений, такие как химия, медицина и экономика.'}, 'en': {'title': 'VeriFree: Reinventing Reinforcement Learning for Language Models Without Verifiers', 'desc': 'This paper introduces a new method called VeriFree for training large language models (LLMs) without the need for a separate verifier. Traditional reinforcement learning (RL) approaches rely on rule-based verification, which limits their application in complex real-world domains. VeriFree simplifies the process by directly maximizing the likelihood of generating correct answers, eliminating the need for an additional verifier model. The results show that VeriFree not only reduces computational demands but also performs as well or better than existing verifier-based methods across various benchmarks.'}, 'zh': {'title': '无验证器的强化学习新方法', 'desc': '最近，使用DeepSeek-R1-Zero风格的强化学习（RL）训练大型语言模型（LLMs）在代码和数学推理方面取得了显著进展。然而，这种方法仅限于可以进行基于规则的答案验证的任务，无法自然扩展到化学、医疗、工程、法律、生物、商业和经济等现实世界领域。目前的解决方案是使用额外的LLM作为模型验证器，但这会引入对强验证器LLM的依赖、奖励黑客攻击的风险以及在训练期间维护验证器模型的实际负担。为了解决这些问题并将DeepSeek-R1-Zero风格的训练扩展到一般推理领域，我们提出了一种无验证器的方法（VeriFree），该方法绕过答案验证，直接使用RL最大化生成参考答案的概率。'}}}, {'id': 'https://huggingface.co/papers/2505.21491', 'title': 'Frame In-N-Out: Unbounded Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.21491', 'abstract': 'Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.', 'score': 11, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'cf3b062e968f421f', 'authors': ['Boyang Wang', 'Xuweiyi Chen', 'Matheus Gadelha', 'Zezhou Cheng'], 'affiliations': ['Adobe Research', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.21491.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#diffusion', '#architecture', '#video'], 'emoji': '🎬', 'ru': {'title': 'Управляемая генерация видео: новый уровень контроля над объектами в кадре', 'desc': 'Статья посвящена улучшению контролируемости, временной согласованности и детализации в генерации видео. Авторы предлагают новый подход к технике Frame In и Frame Out, позволяющий пользователям управлять объектами, входящими в кадр или выходящими из него. Для решения этой задачи был создан новый датасет, разработан протокол оценки и предложена архитектура Diffusion Transformer для генерации видео с сохранением идентичности объектов. Результаты показывают значительное превосходство предложенного метода над существующими базовыми подходами.'}, 'en': {'title': 'Mastering Video Generation with Motion Control', 'desc': 'This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models.'}, 'zh': {'title': '提升视频生成的可控性与一致性', 'desc': '本论文关注视频生成中的可控性、时间一致性和细节合成等关键挑战。我们提出了一种名为“帧进帧出”的电影技术，允许用户控制图像中的对象自然地离开或进入场景。为支持这一任务，我们引入了一个半自动策划的新数据集，并制定了针对该设置的综合评估协议。我们的评估结果表明，所提出的方法在性能上显著优于现有基线。'}}}, {'id': 'https://huggingface.co/papers/2505.21457', 'title': 'Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO', 'url': 'https://huggingface.co/papers/2505.21457', 'abstract': "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.", 'score': 11, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '6d338aff50466f43', 'authors': ['Muzhi Zhu', 'Hao Zhong', 'Canyu Zhao', 'Zongze Du', 'Zheng Huang', 'Mingyu Liu', 'Hao Chen', 'Cheng Zou', 'Jingdong Chen', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21457.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': '👁️', 'ru': {'title': 'ACTIVE-O3: Наделение MLLM активным восприятием для эффективного принятия решений', 'desc': 'Статья представляет ACTIVE-O3 - фреймворк обучения с подкреплением для наделения мультимодальных больших языковых моделей (MLLM) возможностями активного восприятия. Авторы определяют задачи активного восприятия для MLLM и создают комплексный набор тестов для оценки ACTIVE-O3 в различных сценариях. Фреймворк демонстрирует улучшенную эффективность поиска и точность выбора регионов по сравнению с предыдущими подходами. ACTIVE-O3 также показывает сильные способности к рассуждениям с нулевым обучением на эталонном тесте V*.'}, 'en': {'title': 'Empowering MLLMs with Active Perception for Smarter Decision-Making', 'desc': 'This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data.'}, 'zh': {'title': '提升机器人主动感知能力的创新框架', 'desc': '主动视觉，也称为主动感知，是指主动选择观察的方式和位置，以获取与任务相关的信息。本文探讨了多模态大型语言模型（MLLMs）在机器人系统中的主动感知能力，提出了一种基于强化学习的训练框架ACTIVE-O3。我们定义了MLLMs的主动感知任务，并指出现有模型在搜索效率和区域选择上存在不足。通过建立综合基准测试套件，ACTIVE-O3在多个任务中展示了强大的零-shot推理能力，推动了主动感知的研究。'}}}, {'id': 'https://huggingface.co/papers/2505.16901', 'title': 'Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks', 'url': 'https://huggingface.co/papers/2505.16901', 'abstract': "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", 'score': 11, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'a91f0c74f1c3c191', 'authors': ['Hongyuan Tao', 'Ying Zhang', 'Zhenhao Tang', 'Hongen Peng', 'Xukun Zhu', 'Bingchang Liu', 'Yingguang Yang', 'Ziyin Zhang', 'Zhaogui Xu', 'Haipeng Zhang', 'Linchao Zhu', 'Rui Wang', 'Hang Yu', 'Jianguo Li', 'Peng Di'], 'affiliations': ['Ant Group, Hangzhou, China', 'Shanghai Jiaotong University, Shanghai, China', 'ShanghaiTech University, Shanghai, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16901.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#games', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Графовые модели кода: новый уровень генерации в открытых ИИ-системах', 'desc': 'Статья представляет Code Graph Models (CGM) - новый подход к генерации кода на уровне репозитория. CGM интегрируют структуры графов кода в механизм внимания языковых моделей, улучшая понимание функций и файлов в кодовой базе. Эта техника позволяет открытым языковым моделям эффективно решать задачи на уровне репозитория без использования агентных подходов. В сочетании с безагентным фреймворком graph RAG, метод достигает высоких результатов на бенчмарке SWE-bench Lite, превосходя предыдущие открытые модели.'}, 'en': {'title': 'Empowering Code Generation with Open-Source Graph Models', 'desc': 'This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark.'}, 'zh': {'title': '开源代码图模型提升代码生成性能', 'desc': '本论文提出了一种开源代码图模型（Code Graph Models, CGMs），旨在提升代码生成任务的性能。通过将代码图结构整合到大型语言模型（LLMs）的注意力机制中，CGMs能够更好地理解代码库中的函数和文件。与依赖代理的传统方法不同，我们的方法不需要代理，确保了数据隐私和模型定制的灵活性。实验结果表明，使用开源Qwen2.5-72B模型，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，表现优于其他开源模型。'}}}, {'id': 'https://huggingface.co/papers/2505.21500', 'title': 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.21500', 'abstract': "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.", 'score': 10, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'c8cec2407a7def0e', 'authors': ['Dingming Li', 'Hongxing Li', 'Zixuan Wang', 'Yuchen Yan', 'Hang Zhang', 'Siqi Chen', 'Guiyang Hou', 'Shengpei Jiang', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21500.jpg', 'data': {'categories': ['#benchmark', '#cv', '#reasoning', '#3d', '#games'], 'emoji': '🧠', 'ru': {'title': 'Новый рубеж в пространственном интеллекте моделей компьютерного зрения', 'desc': 'Статья представляет новый бенчмарк ViewSpatial-Bench для оценки способностей моделей компьютерного зрения к пространственному рассуждению с разных точек зрения. Авторы выявили, что существующие vision-language модели хорошо справляются с эгоцентрическим пространственным мышлением, но испытывают трудности с аллоцентрическими задачами. Бенчмарк включает пять типов задач и использует автоматизированный 3D-конвейер аннотаций. Тонкая настройка моделей на многоракурсном пространственном датасете позволила улучшить общую производительность на 46.24%.'}, 'en': {'title': 'Enhancing VLMs with Multi-Viewpoint Spatial Reasoning', 'desc': "This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities."}, 'zh': {'title': '提升视觉语言模型的空间推理能力', 'desc': '视觉语言模型（VLMs）在理解和推理视觉内容方面表现出色，但在需要跨视角理解和空间推理的任务中仍面临重大挑战。我们发现当前的VLMs主要在自我中心的空间推理上表现良好，但在需要采用其他实体的空间参考框架时，无法很好地推广到他心中心视角。为此，我们提出了ViewSpatial-Bench，这是第一个专门为多视角空间定位识别评估设计的综合基准，支持自动化的3D标注流程生成精确的方向标签。通过在我们的多视角空间数据集上微调VLMs，我们在各项任务上实现了46.24%的整体性能提升，证明了我们方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.21473', 'title': 'DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction', 'url': 'https://huggingface.co/papers/2505.21473', 'abstract': "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.", 'score': 9, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'd4df44fe3325795e', 'authors': ['Yiheng Liu', 'Liao Qu', 'Huichao Zhang', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Xian Li', 'Shuai Wang', 'Daniel K. Du', 'Shu Cheng', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.21473.jpg', 'data': {'categories': ['#benchmark', '#cv', '#training', '#architecture', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Эффективная генерация изображений от общего к частному', 'desc': 'DetailFlow - это новый метод генерации изображений, использующий авторегрессионный подход с последовательным уточнением деталей. Модель обучается на токенизированных последовательностях изображений с прогрессивно ухудшающимся разрешением, что позволяет начинать генерацию с глобальной структуры и постепенно добавлять детали. Этот подход позволяет достичь высокого качества синтеза изображений при значительно меньшем количестве токенов по сравнению с предыдущими методами. Авторы также предлагают механизм параллельного вывода с самокоррекцией, который ускоряет генерацию примерно в 8 раз.'}, 'en': {'title': 'DetailFlow: Efficient 1D Image Generation with Coarse-to-Fine Refinement', 'desc': 'This paper introduces DetailFlow, a new method for generating images using a 1D autoregressive approach. It employs a next-detail prediction strategy that allows the model to start with a broad image structure and gradually add finer details. By using a resolution-aware token sequence and a parallel inference mechanism, DetailFlow significantly improves generation speed and reduces the number of tokens needed for high-quality image synthesis. The results show that DetailFlow outperforms existing models in both image quality and efficiency, achieving better performance with fewer resources.'}, 'zh': {'title': 'DetailFlow：高效的自回归图像生成方法', 'desc': '本文提出了一种名为DetailFlow的粗到细的1D自回归图像生成方法，采用了一种新颖的下一个细节预测策略来建模图像。通过学习一个分辨率感知的标记序列，并使用逐步降级的图像进行监督，DetailFlow使生成过程能够从全局结构开始，逐步细化细节。该粗到细的1D标记序列与自回归推理机制很好地对齐，为自回归模型生成复杂视觉内容提供了一种更自然和高效的方法。实验结果表明，DetailFlow在图像合成质量和效率上优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2505.19099', 'title': 'SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.19099', 'abstract': "SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.", 'score': 8, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'e7a5589d299cea0e', 'authors': ['Kun Xiang', 'Heng Li', 'Terry Jingchen Zhang', 'Yinya Huang', 'Zirong Liu', 'Peixin Qu', 'Jixi He', 'Jiaqi Chen', 'Yu-Jie Yuan', 'Jianhua Han', 'Hang Xu', 'Hanhui Li', 'Mrinmaya Sachan', 'Xiaodan Liang'], 'affiliations': ['ETH Zurich', 'Huawei Noahs Ark Lab', 'Sun Yat-sen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#benchmark', '#cv', '#multimodal'], 'emoji': '🔬', 'ru': {'title': 'SeePhys: выявление ограничений визуального мышления LLM в физике', 'desc': 'SeePhys - это новый мультимодальный бенчмарк для оценки способностей больших языковых моделей (LLM) в области визуального мышления и решения задач по физике. Он охватывает 7 фундаментальных разделов физики и включает 21 категорию разнородных диаграмм. Бенчмарк содержит 75% задач, требующих обязательной интерпретации визуальной информации. Тестирование показало, что даже самые продвинутые модели визуального мышления достигают точности менее 60% на этом бенчмарке.'}, 'en': {'title': 'SeePhys: Bridging Visual Reasoning and Physics in LLMs', 'desc': 'SeePhys is a new benchmark designed to test how well large language models (LLMs) can solve physics problems that involve visual reasoning. It includes a wide range of questions, from middle school to PhD level, and features many different types of diagrams that are crucial for finding the right answers. Unlike previous benchmarks, SeePhys requires models to extract visual information for 75% of the problems, making it essential for them to interpret diagrams accurately. The results show that even the best models struggle with these tasks, highlighting significant gaps in their ability to connect visual data with physics reasoning without relying heavily on text.'}, 'zh': {'title': 'SeePhys：挑战视觉推理与物理问题解决的基准', 'desc': 'SeePhys是一个大型的多模态基准，旨在评估大型语言模型（LLM）在物理问题上的推理能力，涵盖从中学到博士资格考试的范围。该基准涉及物理学的7个基本领域，并包含21类高度异质的图表。与以往的研究不同，SeePhys中75%的问题需要提取视觉信息才能得出正确答案，显示出视觉信息在解决问题中的重要性。评估结果表明，即使是最先进的视觉推理模型，其准确率也未能超过60%，揭示了当前大型语言模型在视觉理解方面的根本挑战。'}}}, {'id': 'https://huggingface.co/papers/2505.20287', 'title': 'MotionPro: A Precise Motion Controller for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20287', 'abstract': 'MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  \t\t\t\t\tAI-generated summary \t\t\t\t Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.', 'score': 7, 'issue_id': 4004, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '032f4425f4fd7152', 'authors': ['Zhongwei Zhang', 'Fuchen Long', 'Zhaofan Qiu', 'Yingwei Pan', 'Wu Liu', 'Ting Yao', 'Tao Mei'], 'affiliations': ['HiDream.ai Inc.', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20287.jpg', 'data': {'categories': ['#benchmark', '#video', '#games', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Точный контроль движения при генерации видео из изображений', 'desc': 'MotionPro - это новый метод генерации видео из изображений, использующий траектории движения по регионам и маски движения. Он позволяет более точно контролировать движение и разделять движение объектов и камеры. MotionPro оценивает карты потока для каждого обучающего видео и использует траектории по регионам вместо больших гауссовых ядер. Метод также включает маску движения для захвата общей динамики движущихся областей.'}, 'en': {'title': 'Precision in Motion Control for Image-to-Video Generation', 'desc': 'MotionPro is a novel approach for image-to-video generation that enhances motion control by using region-wise trajectories and motion masks. This method allows for precise regulation of motion synthesis, effectively distinguishing between object and camera movements. Instead of relying on large Gaussian kernels, MotionPro utilizes localized trajectories to achieve fine-grained control over motion dynamics. Additionally, it incorporates a motion mask to improve video denoising and has been evaluated using a newly created benchmark dataset, MC-Bench, demonstrating its effectiveness in motion control tasks.'}, 'zh': {'title': '精确运动控制，提升图像到视频生成', 'desc': 'MotionPro 是一种精确的运动控制器，利用区域轨迹和运动掩码来改善图像到视频的生成。它通过估计每个训练视频的流动图，并从中采样区域轨迹，来实现细粒度的运动合成。与传统方法不同，MotionPro 直接使用局部区域内的轨迹，从而实现更精确的运动控制。该方法还通过特征调制增强视频去噪，构建了一个基准数据集 MC-Bench，用于评估细粒度和对象级的运动控制效果。'}}}, {'id': 'https://huggingface.co/papers/2505.21494', 'title': 'Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment', 'url': 'https://huggingface.co/papers/2505.21494', 'abstract': "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.", 'score': 6, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '9308405d203b4ba9', 'authors': ['Xiaojun Jia', 'Sensen Gao', 'Simeng Qin', 'Tianyu Pang', 'Chao Du', 'Yihao Huang', 'Xinfeng Li', 'Yiming Li', 'Bo Li', 'Yang Liu'], 'affiliations': ['MBZUAI, United Arab Emirates', 'Nanyang Technological University, Singapore', 'Sea AI Lab, Singapore', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.21494.jpg', 'data': {'categories': ['#multimodal', '#security', '#training'], 'emoji': '🎯', 'ru': {'title': 'Усовершенствованная атака на мультимодальные языковые модели через оптимальное выравнивание признаков', 'desc': 'Статья представляет новый метод атаки на мультимодальные языковые модели, называемый FOA-Attack. Он использует оптимальное выравнивание признаков на глобальном и локальном уровнях для улучшения переносимости состязательных примеров. Метод включает глобальное выравнивание с помощью косинусного сходства и локальное выравнивание с использованием кластеризации и оптимального транспорта. Также предлагается динамическая стратегия взвешивания ансамбля моделей для адаптивной балансировки влияния нескольких моделей при генерации состязательных примеров.'}, 'en': {'title': 'Enhancing Adversarial Transferability with FOA-Attack', 'desc': 'This paper addresses the vulnerability of multimodal large language models (MLLMs) to transferable adversarial examples. The authors introduce a new attack method called FOA-Attack, which focuses on aligning both global and local features to enhance the transferability of adversarial samples. By utilizing cosine similarity for global feature alignment and optimal transport for local feature alignment, the method improves the effectiveness of attacks on closed-source models. The proposed approach outperforms existing methods in various experiments, demonstrating its robustness in generating transferable adversarial examples.'}, 'zh': {'title': '提升对抗样本转移能力的FOA-Attack方法', 'desc': '多模态大型语言模型（MLLMs）容易受到可转移的对抗样本攻击。现有方法通常通过对齐全局特征来实现目标攻击，但忽视了局部信息的丰富性。为了解决这个问题，我们提出了一种基于特征最优对齐的针对性可转移对抗攻击方法，称为FOA-Attack。通过引入全局特征损失和局部聚类最优传输损失，我们显著提高了对抗样本的转移能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20793', 'title': 'Rendering-Aware Reinforcement Learning for Vector Graphics Generation', 'url': 'https://huggingface.co/papers/2505.20793', 'abstract': 'RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  \t\t\t\t\tAI-generated summary \t\t\t\t Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.', 'score': 6, 'issue_id': 4004, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'bba0f264ad66c8c3', 'authors': ['Juan A. Rodriguez', 'Haotian Zhang', 'Abhay Puri', 'Aarash Feizi', 'Rishav Pramanik', 'Pascal Wichmann', 'Arnab Mondal', 'Mohammad Reza Samsami', 'Rabiul Awal', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar', 'David Vazquez', 'Christopher Pal', 'Marco Pedersoli'], 'affiliations': ['Apple', 'Canada CIFAR AI Chair', 'Columbia University', 'Google Research', 'Independent Scholar', 'McGill University', 'Mila', 'Polytechnique Montréal', 'ServiceNow Research', 'Stony Brook University', 'ÉTS Montréal'], 'pdf_title_img': 'assets/pdf/title_img/2505.20793.jpg', 'data': {'categories': ['#cv', '#rag', '#transfer_learning', '#rl', '#games'], 'emoji': '🎨', 'ru': {'title': 'RLRF: Улучшение генерации SVG через обратную связь от рендеринга', 'desc': 'Метод RLRF использует обучение с подкреплением для улучшения генерации SVG в моделях компьютерного зрения и языка (VLM). Он сравнивает отрендеренные SVG с исходными изображениями для вычисления награды. Это позволяет модели создавать более точные, эффективные и семантически согласованные SVG. RLRF значительно превосходит обычное обучение с учителем, решая распространенные проблемы и обеспечивая высококачественную генерацию SVG.'}, 'en': {'title': 'Enhancing SVG Generation with Reinforcement Learning from Rendering Feedback', 'desc': "The paper presents RLRF, a novel reinforcement learning method that improves the generation of Scalable Vector Graphics (SVG) using rendering feedback. By comparing rendered SVG outputs to original images, RLRF provides evaluative feedback that enhances the model's ability to produce accurate and efficient SVGs. This approach leverages the strengths of vision-language models (VLMs) to capture both global semantics and detailed visual patterns, addressing limitations of previous methods that lacked direct rendering feedback. Ultimately, RLRF demonstrates superior performance over traditional supervised fine-tuning, leading to high-quality SVG generation with better structural understanding and generalization capabilities."}, 'zh': {'title': '利用渲染反馈提升SVG生成的强化学习方法', 'desc': 'RLRF是一种利用渲染反馈的强化学习方法，旨在提高视觉语言模型（VLMs）生成可缩放矢量图形（SVG）的能力。通过将生成的SVG与原始图像进行比较，RLRF能够提供视觉保真度反馈，从而指导模型生成更准确和高效的SVG。该方法解决了现有VLM在训练过程中未观察渲染图像的问题，显著提升了SVG生成的质量和结构理解能力。RLRF在性能上超越了传统的监督微调方法，展现出更强的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2505.17613', 'title': 'MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation', 'url': 'https://huggingface.co/papers/2505.17613', 'abstract': 'MMMG is a comprehensive benchmark for multimodal generation, offering 49 tasks and 937 instructions to align automatic evaluation with human judgment, revealing areas for improvement in reasoning and audio generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 49 tasks (including 29 newly developed ones), each with a carefully designed evaluation pipeline, and 937 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.3%. Benchmarking results on 24 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 78.3% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable headroom for improvement in audio generation, highlighting an important direction for future research.', 'score': 6, 'issue_id': 4006, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'd49545e0846c379a', 'authors': ['Jihan Yao', 'Yushi Hu', 'Yujie Yi', 'Bin Han', 'Shangbin Feng', 'Guang Yang', 'Bingbing Wen', 'Ranjay Krishna', 'Lucy Lu Wang', 'Yulia Tsvetkov', 'Noah A. Smith', 'Banghua Zhu'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.17613.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#multimodal'], 'emoji': '🎭', 'ru': {'title': 'MMMG: Новый стандарт оценки мультимодальных генеративных моделей', 'desc': 'MMMG - это комплексный бенчмарк для мультимодальной генерации, включающий 49 задач и 937 инструкций. Он направлен на согласование автоматической оценки с человеческим суждением в области генерации контента разных модальностей. Бенчмарк охватывает генерацию изображений, аудио, а также чередующихся текста с изображениями и текста с аудио. Результаты тестирования 24 мультимодальных моделей показывают, что даже лучшие из них имеют значительный потенциал для улучшения, особенно в сфере рассуждений и генерации аудио.'}, 'en': {'title': 'MMMG: Bridging the Gap Between Human and Machine Evaluation in Multimodal Generation', 'desc': 'MMMG is a new benchmark designed to evaluate multimodal generation tasks, which involve combining different types of data like images, audio, and text. It includes 49 tasks and 937 instructions that help ensure automatic evaluations match human judgments, especially in complex scenarios. The benchmark focuses on assessing key capabilities such as reasoning and controllability in generation models. Results show that while some models perform well in specific areas, there is still significant room for improvement, particularly in audio generation.'}, 'zh': {'title': '多模态生成的全面基准与评估', 'desc': 'MMMG是一个全面的多模态生成基准，提供49个任务和937条指令，旨在将自动评估与人类判断对齐，揭示推理和音频生成的改进空间。该基准涵盖四种模态组合（图像、音频、交错文本和图像、交错文本和音频），专注于对生成模型提出重大挑战的任务，同时通过模型和程序的结合实现可靠的自动评估。MMMG的评估管道经过精心设计，能够系统地评估多模态生成模型的推理、可控性等关键能力。广泛的验证表明，MMMG与人类评估高度一致，平均一致性达到94.3%。'}}}, {'id': 'https://huggingface.co/papers/2505.21178', 'title': 'Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21178', 'abstract': 'As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model\'s reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.', 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e8fe10bbb758cc6a', 'authors': ['Mingyang Song', 'Mao Zheng'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.21178.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#rl', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Краткость - сестра таланта: новый метод для улучшения рассуждений ИИ', 'desc': 'Исследователи предложили новый метод обучения языковых моделей для улучшения их способности к рассуждению. Метод ConciseR использует двухэтапное обучение с подкреплением для достижения лаконичных и эффективных рассуждений в больших языковых моделях (LLM). Первый этап усиливает способности модели к рассуждению, а второй этап оптимизирует краткость ответов. Эксперименты показали, что ConciseR превосходит современные модели рассуждений на различных математических тестах.'}, 'en': {'title': 'ConciseR: Streamlining Reasoning in Large Language Models', 'desc': 'This paper introduces ConciseR, a two-stage reinforcement learning framework designed to improve the reasoning capabilities of Large Language Models (LLMs) by generating more concise Chain-of-Thought (CoT) responses. The first stage enhances reasoning through Group Relative Policy Optimization with dynamic sampling, while the second stage focuses on enforcing conciseness using Length-aware Group Relative Policy Optimization. The approach addresses the issue of overthinking in LLMs, which often leads to redundant responses in long CoT outputs. Experimental results show that ConciseR outperforms existing state-of-the-art models across various reasoning benchmarks, demonstrating its effectiveness in producing efficient and concise reasoning.'}, 'zh': {'title': '简洁推理，提升LLMs性能', 'desc': '本文提出了一种名为ConciseR的两阶段强化学习框架，旨在提高大型语言模型（LLMs）的推理能力并实现简洁的推理。第一阶段通过使用更多的训练步骤，采用改进的组相对策略优化（GRPO++）来激励模型的推理能力。第二阶段则通过长度感知的组相对策略优化（L-GRPO）来强制执行简洁性，提高效率。实验结果表明，ConciseR在多个基准测试中生成的推理响应更为简洁，超越了最新的推理模型。'}}}, {'id': 'https://huggingface.co/papers/2505.20426', 'title': 'MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness', 'url': 'https://huggingface.co/papers/2505.20426', 'abstract': "Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/", 'score': 5, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '29aacf61c54d1cda', 'authors': ['Yunlong Tang', 'Pinxin Liu', 'Mingqian Feng', 'Zhangyun Tan', 'Rui Mao', 'Chao Huang', 'Jing Bi', 'Yunzhong Xiao', 'Susan Liang', 'Hang Hua', 'Ali Vosoughi', 'Luchuan Song', 'Zeliang Zhang', 'Chenliang Xu'], 'affiliations': ['Carnegie Mellon University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20426.jpg', 'data': {'categories': ['#3d', '#cv', '#interpretability', '#multimodal', '#reasoning', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'MMPerspective: новый рубеж в оценке пространственного понимания ИИ', 'desc': 'Статья представляет MMPerspective - первый бенчмарк для оценки понимания перспективы мультимодальными большими языковыми моделями (MLLM). Бенчмарк включает 10 задач в трех измерениях: восприятие перспективы, рассуждение и устойчивость, с использованием 2,711 реальных и синтетических изображений. Оценка 43 современных MLLM выявила значительные ограничения в понимании пространственных отношений и композиционном мышлении. Исследование также показало связь между архитектурой модели, ее масштабом и способностями в области перспективы.'}, 'en': {'title': 'Evaluating Perspective Understanding in Multimodal Models', 'desc': 'This paper introduces MMPerspective, a benchmark designed to evaluate how well multimodal large language models (MLLMs) understand perspective geometry. It includes 10 tasks that assess three key areas: how models perceive perspective, their reasoning abilities, and their robustness to changes. The benchmark features over 2,700 images and more than 5,000 question-answer pairs that test various skills like recognizing vanishing points and understanding 3D spatial relationships. The study finds that while MLLMs perform well on basic tasks, they struggle with complex reasoning and maintaining spatial accuracy when faced with transformations, revealing important insights into their limitations and potential improvements.'}, 'zh': {'title': 'MMPerspective：评估视角理解的基准', 'desc': '理解视角对人类视觉感知至关重要，但多模态大型语言模型（MLLMs）在视角几何方面的理解程度尚不明确。我们提出了MMPerspective，这是第一个专门设计的基准，旨在通过10个精心设计的任务系统地评估MLLMs对视角的理解。基准包含2711个真实和合成图像实例，以及5083个问题-答案对，探讨关键能力，如消失点感知、视角类型推理和3D空间中的线关系理解等。通过对43个最先进的MLLMs进行全面评估，我们发现模型在表面感知任务上表现良好，但在组合推理和保持空间一致性方面存在显著局限。'}}}, {'id': 'https://huggingface.co/papers/2505.19433', 'title': 'Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic\n  Capabilities in LLM Compression', 'url': 'https://huggingface.co/papers/2505.19433', 'abstract': "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.", 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '1fb4272abd1ee9c8', 'authors': ['Peijie Dong', 'Zhenheng Tang', 'Xiang Liu', 'Lujun Li', 'Xiaowen Chu', 'Bo Li'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19433.jpg', 'data': {'categories': ['#agents', '#optimization', '#inference', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'ACBench: первый комплексный бенчмарк для оценки агентных способностей сжатых LLM', 'desc': 'Статья представляет новый бенчмарк ACBench для оценки влияния сжатия на агентные способности больших языковых моделей (LLM). ACBench включает 12 задач по 4 направлениям, различные методы квантизации и прунинга, а также 15 моделей разного размера. Эксперименты показывают, что 4-битная квантизация сохраняет способности к генерации рабочих процессов и использованию инструментов, но снижает точность в реальных приложениях на 10-15%. Авторы вводят новые метрики ERank, Top-k Ranking Correlation и Energy для систематизации анализа.'}, 'en': {'title': 'Optimizing LLM Compression for Real-World Agentic Tasks', 'desc': 'This paper presents the Agent Compression Benchmark (ACBench), which evaluates how post-training compression affects the agentic capabilities of large language models (LLMs). Unlike existing benchmarks that focus solely on language modeling and understanding, ACBench assesses 12 tasks across four key capabilities, including workflow generation and long-context retrieval. The study explores various compression techniques, such as quantization and pruning, across multiple LLMs of different sizes. Results indicate that while some compression methods maintain performance in certain tasks, they can significantly reduce accuracy in real-world applications, highlighting the need for careful optimization in LLM deployment.'}, 'zh': {'title': '代理压缩基准：优化LLM的能力与效率', 'desc': '后训练压缩技术可以减少大型语言模型（LLMs）的计算和内存成本，从而实现更高效的资源部署。现有的压缩基准主要关注语言建模和自然语言理解任务，忽视了模型在工作流、工具使用、长上下文理解和实际应用等方面的能力。我们提出了代理压缩基准（ACBench），这是第一个全面评估压缩对LLMs代理能力影响的基准。实验结果显示，4位量化在工作流生成和工具使用方面保持良好性能，但在实际应用准确性上下降了10%-15%。'}}}, {'id': 'https://huggingface.co/papers/2505.18134', 'title': 'VideoGameBench: Can Vision-Language Models complete popular video games?', 'url': 'https://huggingface.co/papers/2505.18134', 'abstract': "VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.", 'score': 5, 'issue_id': 4002, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '8dbd2cb973419464', 'authors': ['Alex L. Zhang', 'Thomas L. Griffiths', 'Karthik R. Narasimhan', 'Ofir Press'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.18134.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#benchmark', '#video'], 'emoji': '🎮', 'ru': {'title': 'Видеоигры как вызов для искусственного интеллекта', 'desc': 'VideoGameBench - это новый бенчмарк для оценки способностей моделей машинного зрения и обработки естественного языка в контексте взаимодействия с видеоиграми в реальном времени. Он включает 10 популярных игр 1990-х годов и оценивает способность моделей выполнять задачи, используя только визуальные входные данные и высокоуровневое описание целей и управления. Эксперименты показывают, что современные модели испытывают трудности даже на начальных этапах игр. Бенчмарк призван стимулировать исследования в области восприятия, пространственной навигации и управления памятью в контексте искусственного интеллекта.'}, 'en': {'title': 'Evaluating Human-Like Skills in AI through Video Games', 'desc': "VideoGameBench is a new benchmark designed to assess the capabilities of vision-language models (VLMs) in real-time video game interactions using only visual inputs and high-level objectives. It highlights the gap in VLMs' performance on tasks that require human-like skills such as perception and spatial navigation, which are essential for mastering video games. The benchmark includes 10 classic video games from the 1990s, challenging models to complete them without additional game-specific information. Results show that even advanced models struggle significantly, indicating the need for further research to enhance VLMs' abilities in these areas."}, 'zh': {'title': '评估视觉-语言模型的游戏能力', 'desc': 'VideoGameBench 是一个评估视觉-语言模型（VLM）在实时视频游戏互动中能力的基准。该基准使用仅有的视觉输入和高层次目标，强调了人类技能的挑战。尽管 VLM 在编码和数学基准上表现出色，但在感知、空间导航和记忆管理等人类自然任务上的能力仍然缺乏研究。我们的实验表明，前沿的 VLM 在游戏的开始阶段就遇到了困难，推理延迟是其在实时环境中的主要限制。'}}}, {'id': 'https://huggingface.co/papers/2505.11277', 'title': 'Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs', 'url': 'https://huggingface.co/papers/2505.11277', 'abstract': "AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.", 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'ec7d9a17f0e241f5', 'authors': ['Yaorui Shi', 'Shihan Li', 'Chang Wu', 'Zhiyuan Liu', 'Junfeng Fang', 'Hengxing Cai', 'An Zhang', 'Xiang Wang'], 'affiliations': ['DP Technology', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.11277.jpg', 'data': {'categories': ['#optimization', '#rag', '#rl', '#reasoning', '#benchmark'], 'emoji': '🔍', 'ru': {'title': 'AutoRefine: Умное уточнение знаний для улучшения рассуждений ИИ', 'desc': 'AutoRefine - это система обучения с подкреплением для больших языковых моделей, которая улучшает рассуждения на основе извлечения информации. Она итеративно уточняет знания и оптимизирует поиски, что приводит к повышению производительности в сложных задачах вопросно-ответного анализа. AutoRefine вводит этапы уточнения знаний между последовательными вызовами поиска, позволяя модели фильтровать, дистиллировать и организовывать доказательства перед генерацией ответа. Эксперименты показывают, что AutoRefine значительно превосходит существующие подходы, особенно в сложных сценариях многоэтапных рассуждений.'}, 'en': {'title': 'Refine Your Search, Enhance Your Answers!', 'desc': "AutoRefine is a reinforcement learning framework designed to improve the performance of large language models (LLMs) in complex question-answering tasks. It enhances retrieval-augmented reasoning by allowing LLMs to iteratively refine their knowledge and optimize their search processes. The framework introduces a 'search-and-refine-during-think' approach, where the model filters and organizes information before generating answers. Experiments show that AutoRefine outperforms existing methods, especially in scenarios requiring multi-hop reasoning, by producing higher-quality searches and synthesizing evidence more effectively."}, 'zh': {'title': 'AutoRefine：提升推理能力的强化学习框架', 'desc': 'AutoRefine是一种用于大型语言模型的强化学习框架，旨在通过迭代优化知识和搜索过程来增强检索增强推理。该框架采用了“思考时搜索与精炼”的新范式，在连续的搜索调用之间引入了明确的知识精炼步骤，使模型能够在生成答案之前过滤、提炼和组织证据。通过结合特定的检索奖励和答案正确性奖励，AutoRefine在复杂的多跳问答任务中表现出色，显著超越了现有方法。实验结果表明，AutoRefine能够进行更频繁且高质量的搜索，有效合成证据，从而提高推理的准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.21097', 'title': 'Thinker: Learning to Think Fast and Slow', 'url': 'https://huggingface.co/papers/2505.21097', 'abstract': 'A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.', 'score': 4, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '90232cf610bff733', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21097.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений LLM через разделение интуиции и обдумывания', 'desc': 'Статья представляет модификацию задачи вопросно-ответной системы для улучшения рассуждений больших языковых моделей (LLM) в математике и программировании. Подход основан на теории двойного процесса из психологии и включает четыре этапа: быстрое мышление, проверку, медленное мышление и обобщение. Эксперименты показали повышение точности с 24.9% до 27.9% для модели Qwen2.5-1.5B и с 45.9% до 49.8% для DeepSeek-R1-Qwen-1.5B. Результаты свидетельствуют, что интуитивное и deliberative рассуждение - это различные, но дополняющие друг друга системы, которые выигрывают от целенаправленного обучения.'}, 'en': {'title': 'Enhancing LLM Accuracy through Structured Reasoning', 'desc': 'This paper presents a novel four-stage modification to question-answering tasks for Large Language Models (LLMs), inspired by Dual Process Theory. The stages include Fast Thinking, Verification, Slow Thinking, and Summarization, which help separate intuitive responses from more deliberate reasoning. By implementing this structured approach, the authors demonstrate significant improvements in accuracy for math and coding tasks, with notable efficiency in token usage. The results indicate that enhancing both intuitive and deliberative reasoning can lead to better performance in LLMs.'}, 'zh': {'title': '优化问答任务，提升模型推理能力', 'desc': '本研究提出了一种基于双重过程理论的四阶段问答任务修改，旨在提高大型语言模型（LLMs）在数学和编程领域的准确性。该方法将思维过程分为快速思考、验证、慢速思考和总结四个阶段，以改善模型的推理能力。通过这种方式，模型能够更好地评估和精炼其初始回答，从而减少冗余和不确定性。实验结果表明，该方法显著提高了模型的平均准确率，证明了直觉和深思熟虑的推理是互补的系统。'}}}, {'id': 'https://huggingface.co/papers/2505.21070', 'title': 'Minute-Long Videos with Dual Parallelisms', 'url': 'https://huggingface.co/papers/2505.21070', 'abstract': 'Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.', 'score': 4, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'c777be11d4844462', 'authors': ['Zeqing Wang', 'Bowen Zheng', 'Xingyi Yang', 'Yuecong Xu', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21070.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': '🎞️', 'ru': {'title': 'Ускорение генерации длинных видео с помощью распределенного вывода', 'desc': 'Статья представляет новую стратегию распределенного вывода для видео-диффузионных моделей на основе Diffusion Transformer (DiT), называемую DualParal. Основная идея заключается в параллелизации как временных кадров, так и слоев модели между GPU для снижения задержки обработки и затрат памяти при генерации длинных видео. Авторы используют блочную схему шумоподавления и кэширование признаков для оптимизации производительности. Метод позволяет эффективно генерировать видео длиной 1025 кадров с до 6,54 раз меньшей задержкой и 1,48 раз меньшими затратами памяти на 8 GPU RTX 4090.'}, 'en': {'title': 'Revolutionizing Video Generation with DualPar Efficiency!', 'desc': 'The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts.'}, 'zh': {'title': '高效生成无限长视频的创新策略', 'desc': '本论文提出了一种基于扩散变换器（DiT）的视频扩散模型，旨在解决长视频生成时的处理延迟和内存成本问题。我们提出了一种新的分布式推理策略，称为DualParal，通过在多个GPU上并行处理时间帧和模型层来提高效率。为了克服扩散模型对噪声水平同步的要求，我们采用了块级去噪方案，使得每个GPU处理特定的帧块和层，同时实现异步计算和通信。最终，我们的方法在生成高质量视频时显著降低了延迟和内存消耗，能够高效生成无限长的视频。'}}}, {'id': 'https://huggingface.co/papers/2505.20561', 'title': 'Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.20561', 'abstract': 'BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.', 'score': 4, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '4b7b0470932e5c49', 'authors': ['Shenao Zhang', 'Yaqing Wang', 'Yinxiao Liu', 'Tianqi Liu', 'Peter Grabowski', 'Eugene Ie', 'Zhaoran Wang', 'Yunxuan Li'], 'affiliations': ['Google', 'Google DeepMind', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20561.jpg', 'data': {'categories': ['#rl', '#training', '#rlhf', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'BARL: Умнее исследуем, эффективнее рассуждаем', 'desc': 'BARL - это новая система Байесовского адаптивного обучения с подкреплением для улучшения работы больших языковых моделей. Она интегрирует рефлексивное рассуждение и эффективное исследование, что приводит к лучшей эффективности использования токенов. BARL инструктирует языковую модель переключать стратегии на основе наблюдаемых результатов. Эмпирические результаты показывают, что BARL превосходит стандартные марковские подходы обучения с подкреплением при тестировании.'}, 'en': {'title': 'Enhancing LLMs with Reflective Exploration through BARL', 'desc': 'The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods.'}, 'zh': {'title': 'BARL：提升LLM性能的贝叶斯自适应强化学习框架', 'desc': 'BARL是一种贝叶斯自适应强化学习框架，通过整合反思推理和高效探索，提升了大型语言模型（LLM）的性能。传统的马尔可夫强化学习限制了探索过程，仅依赖当前状态的历史上下文来学习最优策略，导致反思推理的出现和其在测试时的好处不明确。BARL框架通过优化马尔可夫决策过程的后验分布，鼓励模型在奖励最大化和信息收集之间进行平衡。实验结果表明，BARL在合成和数学推理任务中优于传统的马尔可夫强化学习方法，展现出更高的令牌效率和更有效的探索能力。'}}}, {'id': 'https://huggingface.co/papers/2505.20289', 'title': 'VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual\n  Tool Selection', 'url': 'https://huggingface.co/papers/2505.20289', 'abstract': "VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems.", 'score': 4, 'issue_id': 3991, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '06184525a85012f4', 'authors': ['Zeyi Huang', 'Yuyang Ji', 'Anirudh Sundara Rajan', 'Zefan Cai', 'Wen Xiao', 'Junjie Hu', 'Yong Jae Lee'], 'affiliations': ['Microsoft', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.20289.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#optimization', '#cv', '#rl'], 'emoji': '🧠', 'ru': {'title': 'VisTA: Автономное улучшение визуального мышления с помощью обучения с подкреплением', 'desc': 'VisTA - это новая система обучения с подкреплением для улучшения визуального мышления. Она позволяет агентам автономно выбирать и комбинировать инструменты из разнообразной библиотеки на основе эмпирической производительности. В отличие от существующих методов, VisTA использует сквозное обучение с подкреплением для итеративного улучшения стратегий выбора инструментов. Эксперименты показывают, что VisTA достигает значительных улучшений производительности по сравнению с базовыми моделями, особенно на примерах вне распределения обучающей выборки.'}, 'en': {'title': 'Empowering Visual Agents with Autonomous Tool Selection', 'desc': 'VisTA is a reinforcement learning framework designed to improve visual reasoning by allowing agents to autonomously select and combine tools from a diverse library. Unlike traditional methods that either require extensive human supervision or lack active exploration, VisTA uses end-to-end reinforcement learning to refine tool selection strategies based on task outcomes. The framework employs Group Relative Policy Optimization (GRPO) to enable agents to discover effective pathways for tool selection without explicit reasoning guidance. Experiments show that VisTA significantly outperforms existing methods, particularly in challenging scenarios, demonstrating its potential for enhancing generalization and adaptability in visual reasoning tasks.'}, 'zh': {'title': 'VisTA：自主选择工具的视觉推理新框架', 'desc': 'VisTA是一个强化学习框架，旨在通过自主选择和组合多样化工具来增强视觉推理能力。与现有方法不同，VisTA不依赖于训练前提示或大规模微调，而是通过端到端的强化学习来优化工具选择策略。该框架利用任务结果作为反馈信号，允许智能体自主发现有效的工具选择路径。实验结果表明，VisTA在多个基准测试中表现优异，特别是在处理分布外示例时，显示出其在增强泛化能力和灵活利用多样化工具方面的优势。'}}}, {'id': 'https://huggingface.co/papers/2505.19314', 'title': 'SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline', 'url': 'https://huggingface.co/papers/2505.19314', 'abstract': "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.", 'score': 4, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'd58bb66dfe2fc291', 'authors': ['Helin Wang', 'Jiarui Hai', 'Dongchao Yang', 'Chen Chen', 'Kai Li', 'Junyi Peng', 'Thomas Thebaud', 'Laureano Moro Velazquez', 'Jesus Villalba', 'Najim Dehak'], 'affiliations': ['Brno University of Technology', 'Johns Hopkins University', 'Nanyang Technological University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19314.jpg', 'data': {'categories': ['#audio'], 'emoji': '🎙️', 'ru': {'title': 'SoloSpeech: генеративное извлечение целевой речи нового поколения', 'desc': 'SoloSpeech - это новый генеративный подход к извлечению целевой речи из смеси голосов. Он использует каскадный пайплайн, включающий сжатие, извлечение, реконструкцию и коррекцию. Ключевая особенность - экстрактор целевой речи, работающий без эмбеддингов говорящего и использующий латентное пространство вспомогательного аудио. SoloSpeech достигает наилучших результатов по разборчивости и качеству на датасете Libri2Mix, демонстрируя отличную обобщающую способность.'}, 'en': {'title': 'SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques', 'desc': "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."}, 'zh': {'title': 'SoloSpeech：提升目标语音提取的新方法', 'desc': '目标语音提取（TSE）旨在从多个说话者的混合音频中分离出目标说话者的声音，通常利用作为辅助音频的说话者特征线索。尽管近期的TSE进展主要采用了高感知质量的判别模型，但这些模型常常引入不必要的伪影，降低自然性，并对训练和测试环境之间的差异敏感。另一方面，生成模型在感知质量和可懂性方面滞后。为了解决这些问题，我们提出了SoloSpeech，这是一种新颖的级联生成管道，集成了压缩、提取、重建和修正过程，能够在目标语音提取和语音分离任务中实现新的最先进的可懂性和质量。'}}}, {'id': 'https://huggingface.co/papers/2505.20321', 'title': 'BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases', 'url': 'https://huggingface.co/papers/2505.20321', 'abstract': 'BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.', 'score': 4, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '3f6bd45ed0456607', 'authors': ['Mathew J. Koretsky', 'Maya Willey', 'Adi Asija', 'Owen Bianchi', 'Chelsea X. Alvarado', 'Tanay Nayak', 'Nicole Kuznetsov', 'Sungwon Kim', 'Mike A. Nalls', 'Daniel Khashabi', 'Faraz Faghri'], 'affiliations': ['Center for Alzheimers Disease and Related Dementias, NIA, NIH', 'DataTecnica LLC', 'Johns Hopkins University', 'Laboratory of Neurogenetics, NIA, NIH'], 'pdf_title_img': 'assets/pdf/title_img/2505.20321.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#benchmark', '#reasoning'], 'emoji': '🧬', 'ru': {'title': 'Оценка научного мышления в тексто-SQL задачах для биомедицины', 'desc': 'BiomedSQL - это новый эталонный тест для оценки научного мышления в задачах преобразования текста в SQL, использующий большую биомедицинскую базу знаний. Он включает 68 000 триплетов вопрос/SQL-запрос/ответ, основанных на интегрированной базе знаний BigQuery. Каждый вопрос требует от моделей вывода специфичных для предметной области критериев, а не просто синтаксического перевода. Результаты показывают значительный разрыв в производительности между существующими моделями и экспертным уровнем.'}, 'en': {'title': 'Bridging the Gap in Biomedical Text-to-SQL Reasoning', 'desc': 'BiomedSQL is a benchmark designed to assess scientific reasoning in text-to-SQL tasks specifically within the biomedical domain. It highlights the challenges faced by existing models in translating qualitative scientific questions into executable SQL queries, especially when implicit reasoning is required. The benchmark includes 68,000 question/SQL query/answer triples based on a comprehensive biomedical knowledge base, which necessitates understanding complex domain-specific criteria. Evaluation of various large language models (LLMs) shows significant performance gaps, indicating the need for improved systems that can effectively support scientific discovery through advanced reasoning capabilities.'}, 'zh': {'title': 'BiomedSQL：推动生物医学领域的科学推理', 'desc': 'BiomedSQL是一个新的基准，旨在评估文本到SQL生成中的科学推理能力，特别是在生物医学领域。它包含68,000个问题、SQL查询和答案的三元组，基于一个整合了基因-疾病关联、组学数据因果推断和药物批准记录的知识库。现有的文本到SQL系统在将科学问题转化为可执行的SQL时，尤其是在需要隐含领域推理时，表现不佳。通过对多种大型语言模型的评估，BiomedSQL揭示了显著的性能差距，为支持科学发现的文本到SQL系统的进步奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2505.17005', 'title': 'R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17005', 'abstract': "R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus.", 'score': 4, 'issue_id': 3996, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'c185506804508a79', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Wenqing Tian', 'Zhipeng Chen', 'Yuhuan Wu', 'Jiahao Zhao', 'Yingqian Min', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['Beijing Institute of Technology', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17005.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#reasoning', '#rag'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение LLM: объединяем внутренние и внешние знания', 'desc': 'R1-Searcher++ - это новая система, улучшающая работу больших языковых моделей (LLM) путем адаптивной интеграции внутренних и внешних знаний. Система использует двухэтапное обучение: начальное обучение с учителем и затем обучение с подкреплением для динамического приобретения знаний. R1-Searcher++ эффективно сочетает внутренние знания модели с внешним поиском, постоянно улучшая свои возможности. Эксперименты показывают, что система превосходит предыдущие методы извлечения и аугментации для рассуждений.'}, 'en': {'title': 'Enhancing LLMs with Adaptive Knowledge Integration', 'desc': "R1-Searcher++ is a new framework that enhances Large Language Models (LLMs) by combining internal knowledge with external information through a two-stage training process. The first stage, called SFT Cold-start, helps the model learn basic formats, while the second stage uses Reinforcement Learning (RL) to improve knowledge acquisition dynamically. This RL phase encourages the model to explore and rewards it for effectively using its internal knowledge, while also allowing it to memorize and integrate new information from external sources. As a result, R1-Searcher++ significantly boosts the model's reasoning efficiency and performance compared to existing methods."}, 'zh': {'title': 'R1-Searcher++：智能整合内外部知识的框架', 'desc': 'R1-Searcher++是一个新颖的框架，通过两阶段训练，增强了大型语言模型（LLMs）的能力，能够自适应地整合内部和外部知识。该框架首先进行冷启动的初始格式学习，然后通过强化学习（RL）进行动态知识获取。强化学习阶段采用结果监督，鼓励模型探索，同时引入奖励机制以促进内部知识的利用，并结合记忆机制不断吸收检索到的信息。实验结果表明，R1-Searcher++在检索增强推理效率和性能上优于之前的RAG和推理方法。'}}}, {'id': 'https://huggingface.co/papers/2505.21471', 'title': 'Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2505.21471', 'abstract': 'The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, ExtAgents, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $boldsymbol{inftyBench+}, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls within or exceeds the context window$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.', 'score': 3, 'issue_id': 4001, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3daf58c369674282', 'authors': ['Zijun Liu', 'Zhennan Wan', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.21471.jpg', 'data': {'categories': ['#long_context', '#inference', '#agents', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'ExtAgents: Расширение возможностей LLM без увеличения контекстного окна', 'desc': 'Статья представляет новую систему ExtAgents, которая улучшает масштабируемость интеграции знаний во время вывода в больших языковых моделях (LLM). Эта мультиагентная система позволяет обрабатывать большие объемы внешних знаний, преодолевая ограничения контекстного окна LLM. ExtAgents показывает значительное улучшение производительности на различных задачах, включая многоэтапные вопросно-ответные системы и генерацию длинных обзоров. Система сохраняет высокую эффективность благодаря параллелизму и не требует дополнительного обучения модели.'}, 'en': {'title': 'Scalable Knowledge Integration with ExtAgents Framework', 'desc': 'The ExtAgents framework improves how large language models (LLMs) integrate external knowledge during inference without needing to extend their context window. It addresses key challenges in knowledge synchronization and reasoning that limit the effectiveness of existing methods. By using a multi-agent approach, ExtAgents allows for better scalability and performance in tasks that require significant external information. This method not only enhances results in multi-hop question answering but also maintains efficiency through parallel processing.'}, 'zh': {'title': 'ExtAgents：提升大语言模型知识整合的可扩展性', 'desc': 'ExtAgents是一个多智能体框架，旨在提高大语言模型在推理时整合外部知识的可扩展性。该框架解决了现有知识同步和推理过程中的两个核心瓶颈，从而在不增加上下文窗口的情况下，提升了性能。通过在多跳问答测试和其他公共测试集上的基准测试，ExtAgents显著提高了在相同外部知识输入下的表现。该方法还因其高并行性而保持了高效率，未来在协调LLM智能体以增加外部知识输入方面的研究将有助于实际应用。'}}}, {'id': 'https://huggingface.co/papers/2505.21205', 'title': 'Sci-Fi: Symmetric Constraint for Frame Inbetweening', 'url': 'https://huggingface.co/papers/2505.21205', 'abstract': "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.", 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a20a9eb43b42208f', 'authors': ['Liuhan Chen', 'Xiaodong Cun', 'Xiaoyu Li', 'Xianyi He', 'Shenghai Yuan', 'Jie Chen', 'Ying Shan', 'Li Yuan'], 'affiliations': ['ARC Lab, Tencent PCG', 'GVC Lab, Great Bay University', 'Rabbitpre Intelligence', 'Shenzhen Graduate School, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21205.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'Симметричное внедрение граничных кадров для улучшенного синтеза видео', 'desc': 'Статья представляет новый подход к синтезу промежуточных видеокадров между заданными начальным и конечным кадрами. Авторы предлагают фреймворк Sci-Fi, который решает проблему асимметричного влияния начального и конечного кадров в существующих методах, основанных на диффузионных моделях преобразования изображения в видео (I2V-DM). Sci-Fi вводит специальный модуль EF-Net для более эффективного внедрения информации о конечном кадре. Эксперименты показывают превосходство предложенного метода над существующими подходами в создании плавных и согласованных переходов между кадрами.'}, 'en': {'title': 'Achieving Harmony in Video Frame Synthesis with Sci-Fi', 'desc': 'This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques.'}, 'zh': {'title': '实现起始帧与结束帧的对称约束', 'desc': '本文提出了一种新的框架，称为Sci-Fi，用于在给定的起始帧和结束帧之间合成中间视频序列。现有的方法主要依赖于大型预训练的图像到视频扩散模型（I2V-DMs），但在引入结束帧约束时存在设计缺陷。我们发现，简单地使用与起始帧相同的机制来引入结束帧约束，可能无法有效影响中间内容，从而导致生成帧的运动不一致或外观崩溃。Sci-Fi通过引入一种改进的机制和轻量级模块EF-Net，使结束帧约束的影响力与起始帧相当，从而实现更和谐的过渡效果。'}}}, {'id': 'https://huggingface.co/papers/2505.20650', 'title': 'FinTagging: An LLM-ready Benchmark for Extracting and Structuring\n  Financial Information', 'url': 'https://huggingface.co/papers/2505.20650', 'abstract': 'FinTagging evaluates LLMs for structured information extraction and semantic alignment in XBRL financial reporting, revealing challenges in fine-grained concept alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.', 'score': 3, 'issue_id': 4008, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '71aeb73f235b2c8a', 'authors': ['Yan Wang', 'Yang Ren', 'Lingfei Qian', 'Xueqing Peng', 'Keyi Wang', 'Yi Han', 'Dongji Feng', 'Xiao-Yang Liu', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['Columbia University', 'Gustavus Adolphus College', 'The Fin AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20650.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#alignment', '#dataset', '#reasoning', '#data'], 'emoji': '📊', 'ru': {'title': 'FinTagging: Новый бенчмарк для оценки LLM в финансовой отчетности XBRL', 'desc': 'Статья представляет FinTagging - первый полномасштабный бенчмарк для оценки возможностей больших языковых моделей (LLM) в извлечении структурированной информации и семантическом выравнивании в контексте финансовой отчетности на основе XBRL. FinTagging разбивает задачу тегирования XBRL на два подзадания: FinNI для извлечения финансовых сущностей и FinCL для концептуального выравнивания на основе таксономии. Исследование оценивает различные LLM в условиях нулевого обучения, анализируя их производительность на обеих подзадачах и общую точность тегирования. Результаты показывают, что хотя LLM демонстрируют сильную обобщающую способность в извлечении информации, они испытывают трудности с детальным концептуальным выравниванием, особенно при различении близких записей таксономии.'}, 'en': {'title': 'FinTagging: Enhancing LLMs for Accurate Financial Reporting', 'desc': 'FinTagging is a benchmark designed to evaluate how well large language models (LLMs) can extract structured information and align semantics in XBRL financial reports. It breaks down the tagging process into two tasks: extracting financial entities and aligning them with a detailed taxonomy. The benchmark assesses LLMs in zero-shot settings, focusing on their ability to handle both unstructured text and structured tables. Results show that while LLMs excel at extracting information, they face challenges in accurately aligning closely related concepts within the taxonomy, indicating a need for better semantic reasoning in financial reporting.'}, 'zh': {'title': 'FinTagging：提升财务报告信息提取与语义对齐的基准', 'desc': 'FinTagging是一个全新的基准，旨在评估大型语言模型（LLMs）在XBRL财务报告中的结构化信息提取和语义对齐能力。它将XBRL标记问题分解为两个子任务：FinNI用于提取财务实体，FinCL用于基于分类法的概念对齐。通过在无监督设置下评估多种LLMs，我们发现这些模型在信息提取方面表现良好，但在细粒度概念对齐上存在困难，尤其是在区分相似的分类法条目时。研究结果强调了现有LLMs在自动化XBRL标记方面的局限性，并指出需要改进语义推理和模式感知建模以满足准确财务披露的要求。'}}}, {'id': 'https://huggingface.co/papers/2505.19973', 'title': 'DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response', 'url': 'https://huggingface.co/papers/2505.19973', 'abstract': 'DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.', 'score': 3, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'c4a0e825312e2a47', 'authors': ['Bilel Cherif', 'Tamas Bisztray', 'Richard A. Dubniczky', 'Aaesha Aldahmani', 'Saeed Alshehhi', 'Norbert Tihanyi'], 'affiliations': ['Eötvös Loránd University, Budapest, Hungary', 'Technology Innovation Institute, Abu Dhabi, UAE', 'University of Oslo, Oslo, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2505.19973.jpg', 'data': {'categories': ['#science', '#hallucinations', '#benchmark', '#dataset', '#optimization', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Комплексная оценка языковых моделей для цифровой криминалистики', 'desc': 'DFIR-Metric - это комплексный инструмент для оценки больших языковых моделей (LLM) в области цифровой криминалистики. Он включает тесты на знания, реалистичные криминалистические задачи и практические случаи анализа. Benchmark оценивает 14 LLM по точности и согласованности результатов. Авторы также вводят новую метрику - Task Understanding Score (TUS), для оценки моделей в сценариях с почти нулевой точностью.'}, 'en': {'title': 'Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark', 'desc': 'The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics.'}, 'zh': {'title': 'DFIR-Metric：数字取证中的语言模型评估新标准', 'desc': 'DFIR-Metric 是一个评估大型语言模型在数字取证领域表现的基准工具。它包含三个主要部分：知识评估、现实取证挑战和实际分析案例，旨在全面测试模型的能力。通过对 14 个大型语言模型的评估，研究者分析了它们在准确性和一致性方面的表现。新引入的任务理解分数（TUS）可以更有效地评估模型在接近零准确率的场景中的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.18657', 'title': 'MLLMs are Deeply Affected by Modality Bias', 'url': 'https://huggingface.co/papers/2505.18657', 'abstract': 'MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence.', 'score': 3, 'issue_id': 3998, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '530ba33aeb74da52', 'authors': ['Xu Zheng', 'Chenfei Liao', 'Yuqian Fu', 'Kaiyu Lei', 'Yuanhuiyi Lyu', 'Lutao Jiang', 'Bin Ren', 'Jialei Chen', 'Jiawen Wang', 'Chengxin Li', 'Linfeng Zhang', 'Danda Pani Paudel', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Nicu Sebe', 'Dacheng Tao', 'Luc Van Gool', 'Xuming Hu'], 'affiliations': ['CSE, HKUST', 'China University of Mining & Technology, Beijing', 'College of Computing & Data Science, Nanyang Technological University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nagoya University', 'SPIC Energy Science and Technology Research Institute', 'Shanghai Jiao Tong University', 'Tongji University', 'University of Pisa, IT', 'University of Trento, IT', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18657.jpg', 'data': {'categories': ['#interpretability', '#training', '#agi', '#multimodal', '#architecture'], 'emoji': '🔬', 'ru': {'title': 'Преодоление языкового доминирования в мультимодальных ИИ-системах', 'desc': 'Статья исследует проблему модальной предвзятости в мультимодальных больших языковых моделях (MLLM), которые отдают предпочтение языковой информации над визуальной. Авторы выявляют ключевые факторы этой предвзятости: характеристики данных, несбалансированные возможности базовых моделей и цели обучения. Предлагается дорожная карта исследований для преодоления модальной предвзятости в MLLM. Статья призывает к междисциплинарным усилиям для разработки более сбалансированных и обобщаемых мультимодальных систем.'}, 'en': {'title': 'Balancing Modalities: Overcoming Bias in MLLMs', 'desc': 'This paper discusses the issue of modality bias in Multimodal Large Language Models (MLLMs), where these models tend to favor language inputs over visual ones. It highlights how this bias affects the integration of different modalities, leading to imbalanced learning and performance across tasks. The authors diagnose the current state of modality bias, propose a research roadmap to address it, and identify key factors contributing to this bias, such as data characteristics and training objectives. They emphasize the need for balanced training strategies and architectures to improve multimodal integration and advance towards more generalizable AI systems.'}, 'zh': {'title': '克服模态偏见，实现多模态平衡整合', 'desc': '多模态大型语言模型（MLLMs）在整合文本和图像等不同模态方面取得了显著进展，但它们存在模态偏见，通常更依赖语言而忽视视觉输入。这种偏见影响了多模态的平衡整合，导致学习动态的不平衡。本文诊断了模态偏见的现状，提出了系统的研究路线图，并识别了影响模态偏见的关键因素。我们建议未来的研究应关注平衡的训练策略和模型架构，以促进多模态的有效整合。'}}}, {'id': 'https://huggingface.co/papers/2505.21499', 'title': 'AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery', 'url': 'https://huggingface.co/papers/2505.21499', 'abstract': "AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.", 'score': 2, 'issue_id': 3997, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'd5affb6081e51a29', 'authors': ['Haowei Wang', 'Junjie Wang', 'Xiaojun Jia', 'Rupeng Zhang', 'Mingyang Li', 'Zhe Liu', 'Yang Liu', 'Qing Wang'], 'affiliations': ['Institute of Software, Chinese Academy of Sciences, Beijing, China', 'Nanyang Technological University, Singapore', 'State Key Laboratory of Intelligent Game, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21499.jpg', 'data': {'categories': ['#cv', '#agents', '#security'], 'emoji': '🕵️', 'ru': {'title': 'Реклама как оружие: новая угроза для ИИ-агентов в сети', 'desc': 'AdInject - это новый метод атаки на веб-агентов, основанных на мультимодальных языковых моделях, с использованием интернет-рекламы для внедрения вредоносного контента. Метод работает в условиях черного ящика, без доступа к параметрам модели агента или знания о намерениях пользователя. AdInject включает стратегии создания вредоносной рекламы и оптимизацию контента на основе мультимодальных языковых моделей для повышения эффективности атаки. Эксперименты показали высокую результативность метода, достигающую в некоторых случаях 100% успеха, что указывает на серьезную уязвимость в безопасности веб-агентов.'}, 'en': {'title': 'AdInject: Harnessing Ads for Real-World Attacks on Web Agents', 'desc': "AdInject is a new method that shows how internet advertising can be used to attack vision-language model-based web agents. This method highlights the security weaknesses of these agents when they operate in uncontrolled online environments. Unlike previous attacks that required detailed knowledge of the agent's workings, AdInject works in a black-box setting, making it more applicable to real-world scenarios. The research demonstrates that by cleverly designing malicious ads, attackers can significantly mislead web agents, achieving high success rates in their attacks."}, 'zh': {'title': '利用广告注入攻击网络代理的安全漏洞', 'desc': 'AdInject是一种新颖的黑箱攻击方法，利用互联网广告向基于视觉-语言模型的网络代理注入恶意内容，显示出网络代理安全性的重要漏洞。该方法在更现实的威胁模型下运行，不需要对用户意图的具体了解，且假设代理为黑箱。AdInject通过设计误导性的广告内容，诱使代理点击，并使用基于视觉-语言模型的广告内容优化技术，使广告内容与目标网站的上下文更相关，从而提高攻击效果。实验结果表明，AdInject在大多数场景中的攻击成功率超过60%，在某些情况下接近100%。'}}}, {'id': 'https://huggingface.co/papers/2505.20286', 'title': 'Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal\n  Predefinition and Maximal Self-Evolution', 'url': 'https://huggingface.co/papers/2505.20286', 'abstract': 'Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of "Simplicity is the ultimate sophistication," enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at https://github.com/CharlesQ9/Alita{https://github.com/CharlesQ9/Alita}.', 'score': 2, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'cbcb892a82c3101a', 'authors': ['Jiahao Qiu', 'Xuan Qi', 'Tongcheng Zhang', 'Xinzhe Juan', 'Jiacheng Guo', 'Yifu Lu', 'Yimin Wang', 'Zixin Yao', 'Qihan Ren', 'Xun Jiang', 'Xing Zhou', 'Dongrui Liu', 'Ling Yang', 'Yue Wu', 'Kaixuan Huang', 'Shilong Liu', 'Hongru Wang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'IIIS, Tsinghua University', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tianqiao and Chrissy Chen Institute', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.20286.jpg', 'data': {'categories': ['#open_source', '#agi', '#agents', '#benchmark', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Простота - ключ к универсальному ИИ-агенту', 'desc': 'Алита - это агент широкого профиля, достигающий высокой производительности на различных тестах благодаря минимальному предопределению и саморазвитию. Он использует протоколы контекста модели, связанные с задачей, для масштабируемого агентного рассуждения. Алита оснащена только одним компонентом для прямого решения проблем, что делает ее намного проще предыдущих подходов. Агент достигает высоких результатов на эталонных наборах данных GAIA, Mathvista и PathVQA, превосходя многие более сложные системы.'}, 'en': {'title': "Simplicity Fuels Alita's Generalist Intelligence", 'desc': 'Alita is a generalist agent that excels in various tasks by focusing on simplicity and self-evolution. It minimizes the need for predefined tools, allowing it to adapt and generalize better across different domains. By using task-related model context protocols, Alita can autonomously develop and refine its capabilities, enhancing its problem-solving skills. Its performance on benchmarks like GAIA demonstrates its effectiveness, achieving high accuracy rates compared to more complex systems.'}, 'zh': {'title': '简约设计，强大智能！', 'desc': 'Alita是一种以简约为驱动的通用智能体，通过最小的预定义和自我进化在多个基准测试中实现了高性能。与依赖手动预定义工具的现有框架不同，Alita仅配备一个直接解决问题的组件，使其设计更加简洁。它通过生成任务相关的模型上下文协议（MCPs）来增强自我进化能力，从而实现可扩展的智能推理。Alita在GAIA基准验证数据集上达到了75.15%的pass@1和87.27%的pass@3准确率，表现优于许多复杂的智能体系统。'}}}, {'id': 'https://huggingface.co/papers/2505.19650', 'title': 'Modality Curation: Building Universal Embeddings for Advanced Multimodal\n  Information Retrieval', 'url': 'https://huggingface.co/papers/2505.19650', 'abstract': 'UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.', 'score': 2, 'issue_id': 3997, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ef6a1b7516769657', 'authors': ['Fanheng Kong', 'Jingyuan Zhang', 'Yahui Liu', 'Hongzhi Zhang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yu Tian', 'Victoria W.', 'Fuzheng Zhang', 'Guorui Zhou'], 'affiliations': ['Kuaishou Technology', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19650.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#training', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'UNITE: Преодоление разрыва между модальностями в мультимодальном поиске', 'desc': 'Статья представляет UNITE - универсальную систему для мультимодального информационного поиска. Авторы решают проблемы гетерогенности данных и сложности кросс-модального выравнивания через курирование данных и модально-осведомленные конфигурации обучения. Предложенный метод Modal-Aware Masked Contrastive Learning (MAMCL) позволяет смягчить конкурентные отношения между экземплярами разных модальностей. UNITE достигает результатов на уровне современного состояния в нескольких эталонных тестах мультимодального поиска.'}, 'en': {'title': 'UNITE: Bridging Modalities for Superior Information Retrieval', 'desc': 'UNITE is a framework designed to improve multimodal information retrieval (MIR) by focusing on data curation and modality-aware training. It addresses the challenges of aligning different types of data, which often have varying characteristics. The framework introduces Modal-Aware Masked Contrastive Learning (MAMCL) to enhance the learning process by reducing conflicts between different modalities. By conducting extensive experiments, UNITE demonstrates significant improvements in retrieval performance across various benchmarks, setting a new standard in the field.'}, 'zh': {'title': 'UNITE：多模态信息检索的新突破', 'desc': 'UNITE是一个针对多模态信息检索（MIR）挑战的通用框架，主要通过数据整理和模态感知训练来解决问题。该研究首次系统分析了模态特定数据属性如何影响下游任务的表现，并提出了模态感知掩蔽对比学习（MAMCL），以减轻不同模态实例之间的竞争关系。通过在多个多模态检索基准上进行广泛实验，UNITE实现了最先进的结果，显著超越了现有方法。该工作为未来多模态系统的研究提供了基础蓝图。'}}}, {'id': 'https://huggingface.co/papers/2505.17908', 'title': 'ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback', 'url': 'https://huggingface.co/papers/2505.17908', 'abstract': 'ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind', 'score': 2, 'issue_id': 3995, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '002e547984f03ef8', 'authors': ['Litao Guo', 'Xinli Xu', 'Luozhou Wang', 'Jiantao Lin', 'Jinsong Zhou', 'Zixin Zhang', 'Bolan Su', 'Ying-Cong Chen'], 'affiliations': ['HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2505.17908.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#training', '#reasoning', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'ComfyMind: Улучшение генеративных рабочих процессов с помощью семантического интерфейса и адаптивного планирования', 'desc': 'ComfyMind - это система искусственного интеллекта, построенная на платформе ComfyUI. Она вводит Семантический Интерфейс Рабочего Процесса (SWI) для абстрагирования низкоуровневых графов узлов в вызываемые функциональные модули. Система также использует механизм Планирования Дерева Поиска для моделирования генерации как иерархического процесса принятия решений. ComfyMind превосходит существующие открытые системы в задачах генерации, редактирования и рассуждения.'}, 'en': {'title': 'Empowering Generative Workflows with ComfyMind', 'desc': 'ComfyMind is a collaborative AI system that enhances generative workflows by introducing a Semantic Workflow Interface and a Search Tree Planning mechanism. The Semantic Workflow Interface simplifies complex tasks by allowing users to describe workflows in natural language, which reduces errors and improves usability. The Search Tree Planning mechanism organizes the generation process into a hierarchical structure, enabling adaptive corrections and localized feedback during execution. Overall, ComfyMind demonstrates superior performance in generation, editing, and reasoning tasks compared to existing open-source systems, making it a significant advancement in general-purpose generative AI.'}, 'zh': {'title': 'ComfyMind：提升生成工作流程的协作AI系统', 'desc': 'ComfyMind是一个基于ComfyUI的协作AI系统，旨在增强生成工作流程。它引入了语义工作流接口和搜索树规划机制，使得生成、编辑和推理任务的性能超越现有的开源系统。通过将低级节点图抽象为自然语言描述的可调用功能模块，ComfyMind减少了结构错误并提高了高层次的组合能力。此外，搜索树规划机制允许在每个阶段进行自适应修正，从而提高了复杂生成工作流程的稳定性和灵活性。'}}}, {'id': 'https://huggingface.co/papers/2505.16673', 'title': 'R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO', 'url': 'https://huggingface.co/papers/2505.16673', 'abstract': 'Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.', 'score': 2, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'e90d190149bd97ed', 'authors': ['Huanjin Yao', 'Qixiang Yin', 'Jingyi Zhang', 'Min Yang', 'Yibo Wang', 'Wenhao Wu', 'Fei Su', 'Li Shen', 'Minghui Qiu', 'Dacheng Tao', 'Jiaxing Huang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'ByteDance', 'Nanyang Technological University', 'The University of Sydney', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16673.jpg', 'data': {'categories': ['#rl', '#training', '#multimodal', '#benchmark', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Усиление рассуждений MLLM через разделяемое обучение с подкреплением', 'desc': 'Share-GRPO - это новый подход в области обучения с подкреплением, который улучшает мультимодальные большие языковые модели (MLLM). Метод расширяет пространство вопросов, позволяя исследовать разнообразные траектории рассуждений. Share-GRPO использует иерархическое вычисление преимуществ для более точной оценки относительных выгод. Эксперименты на шести широко используемых тестах показали превосходную производительность этого метода.'}, 'en': {'title': 'Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning', 'desc': 'This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks.'}, 'zh': {'title': 'Share-GRPO：提升多模态语言模型推理能力的新方法', 'desc': '本文提出了一种新颖的强化学习方法Share-GRPO，旨在增强多模态大型语言模型（MLLM）的推理能力。通过扩展问题空间和共享多样的推理轨迹，Share-GRPO有效地解决了强化学习中的稀疏奖励和优势消失问题。该方法首先通过数据转换技术扩展给定问题的空间，然后鼓励MLLM在扩展的问题空间中探索多样的推理轨迹，并在强化学习过程中共享这些轨迹。此外，Share-GRPO在优势计算中共享奖励信息，从而提高了相对优势的估计准确性，增强了策略训练的稳定性。'}}}, {'id': 'https://huggingface.co/papers/2505.20162', 'title': 'Capability-Based Scaling Laws for LLM Red-Teaming', 'url': 'https://huggingface.co/papers/2505.20162', 'abstract': "Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.", 'score': 1, 'issue_id': 4003, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'f1f248fb099b6803', 'authors': ['Alexander Panfilov', 'Paul Kassianik', 'Maksym Andriushchenko', 'Jonas Geiping'], 'affiliations': ['Cisco Systems Inc.', 'ELLIS Institute Tübingen', 'EPFL', 'Foundation AI', 'Max Planck Institute for Intelligent Systems', 'Tübingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2505.20162.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#security', '#alignment'], 'emoji': '🛡️', 'ru': {'title': 'Превосходство в возможностях - ключ к безопасности ИИ', 'desc': 'Исследование показывает, что эффективность атак на большие языковые модели (LLM) резко снижается, когда возможности целевой модели превосходят возможности атакующего. Авторы провели эксперименты с более чем 500 парами атакующий-цель, используя атаки типа jailbreak, имитирующие действия человека. Выявлены три основные тенденции: более способные модели являются лучшими атакующими, успех атаки резко падает при превосходстве цели, и успех атаки коррелирует с высокой производительностью на социальных разделах бенчмарка MMLU-Pro. Результаты указывают на необходимость новых стратегий оценки и снижения рисков для будущих моделей искусственного интеллекта.'}, 'en': {'title': 'Bridging the Capability Gap: Rethinking Red-Teaming for Advanced AI', 'desc': "This paper explores the effectiveness of red-teaming, which is a method used to identify vulnerabilities in large language models (LLMs). It finds that as LLMs become more capable, traditional red-teaming strategies may fail, especially when the target model is stronger than the attacker. The study evaluates over 500 pairs of attackers and targets, revealing that attack success rates drop significantly when the target model's capabilities exceed those of the attacker. The authors propose a scaling law to predict attack success based on the capability gap, emphasizing the need for new strategies to ensure the safe deployment of advanced AI models."}, 'zh': {'title': '能力差距决定攻击成功率', 'desc': '这篇论文探讨了大型语言模型在红队测试中的应用，强调了攻击者与目标模型能力差距的重要性。研究发现，当目标模型的能力超过攻击者时，攻击成功率会显著下降。通过分析500多个攻击者与目标对，论文提出了一种新的监测和评估策略，以应对未来的风险。结果表明，能力更强的模型在攻击中表现更好，而固定能力的攻击者（如人类）可能在面对未来模型时变得无效。'}}}, {'id': 'https://huggingface.co/papers/2505.19377', 'title': 'Absolute Coordinates Make Motion Generation Easy', 'url': 'https://huggingface.co/papers/2505.19377', 'abstract': 'Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.', 'score': 1, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'd9415ccd47a86548', 'authors': ['Zichong Meng', 'Zeyu Han', 'Xiaogang Peng', 'Yiming Xie', 'Huaizu Jiang'], 'affiliations': ['Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19377.jpg', 'data': {'categories': ['#training', '#multimodal', '#games', '#optimization', '#cv'], 'emoji': '🤖', 'ru': {'title': 'Глобальные координаты для улучшения генерации движений из текста', 'desc': 'В статье предлагается новый подход к генерации движений на основе текста, использующий абсолютные координаты суставов в глобальном пространстве. Этот метод обеспечивает более высокую точность движений, улучшенное соответствие тексту и хорошую масштабируемость даже с простой архитектурой Transformer. Подход также поддерживает задачи управления движением на основе текста и редактирования без дополнительной доработки. Авторы демонстрируют возможность прямой генерации вершин меша SMPL-H в движении из текста.'}, 'en': {'title': 'Revolutionizing Text-to-Motion with Global Coordinates', 'desc': 'This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications.'}, 'zh': {'title': '全局绝对坐标提升文本到运动生成的效果', 'desc': '本文提出了一种新的文本到运动生成方法，使用全局空间中的绝对关节坐标来提高运动的真实感、文本对齐和可扩展性。传统的运动表示方法依赖于相对运动，虽然简化了训练过程，但对扩散模型的应用造成了限制。通过系统分析，我们证明了这种新的表示方法在运动真实感和文本对齐方面显著提升，且能够支持下游任务。最终，我们展示了该方法在从文本直接生成运动的SMPL-H网格顶点方面的良好泛化能力，为未来的研究和运动相关应用奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2505.19235', 'title': 'CoreMatching: A Co-adaptive Sparse Inference Framework with Token and\n  Neuron Pruning for Comprehensive Acceleration of Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.19235', 'abstract': 'A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.', 'score': 1, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '0d7373d1050c8170', 'authors': ['Qinsi Wang', 'Hancheng Ye', 'Ming-Yu Chung', 'Yudong Liu', 'Yueqian Lin', 'Martin Kuo', 'Mingyuan Ma', 'Jianyi Zhang', 'Yiran Chen'], 'affiliations': ['Department of Electrical and Computer Engineering, Duke University, North Carolina, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.19235.jpg', 'data': {'categories': ['#architecture', '#inference', '#cv', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'CoreMatching: синергия разреженности для эффективного вывода в мультимодальных моделях', 'desc': 'Статья представляет новую структуру под названием CoreMatching для повышения эффективности вывода в моделях компьютерного зрения и обработки естественного языка. Авторы исследуют взаимосвязь между разреженностью токенов и нейронов, обнаруживая их взаимное влияние. Предложенный метод превосходит современные базовые модели на десяти задачах понимания изображений и трех аппаратных устройствах. На NVIDIA Titan Xp достигнуто 5-кратное сокращение FLOPS и 10-кратное общее ускорение.'}, 'en': {'title': 'Unlocking Efficiency: Synergizing Token and Neuron Sparsity in VLMs', 'desc': 'This paper introduces a new framework called CoreMatching that improves the efficiency of vision-language models (VLMs) by combining two types of sparsity: token sparsity and neuron sparsity. Token sparsity helps reduce the number of tokens used, while neuron sparsity minimizes the computations needed for high-dimensional data. The authors explore the interaction between key neurons and tokens, revealing that they influence each other, which leads to better performance. By leveraging this synergy, CoreMatching significantly enhances inference efficiency, achieving remarkable speedups and reductions in computational load across various tasks and devices.'}, 'zh': {'title': '核心匹配：提升视觉语言模型推理效率的关键', 'desc': '本文提出了一种核心匹配框架，通过利用标记和神经元稀疏性之间的协同作用，提高了视觉语言模型的推理效率。传统的视觉语言模型在多种任务中表现优异，但在推理时消耗大量时间和内存。我们发现，关键的神经元和标记在推理过程中相互影响，增强了彼此的效果。基于这一发现，CoreMatching框架能够有效结合这两种稀疏性，从而在多个任务和设备上超越现有的基准。'}}}, {'id': 'https://huggingface.co/papers/2505.19094', 'title': 'SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.19094', 'abstract': 'SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.', 'score': 1, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'af44c678d01cadc9', 'authors': ['Chuming Shen', 'Wei Wei', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['School of Computer Science & Technology, Huazhong University of Science and Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19094.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#optimization', '#dataset', '#rl'], 'emoji': '🔍', 'ru': {'title': 'SATORI: Повышение точности VQA через декомпозицию и целенаправленное обучение', 'desc': 'SATORI - это новый подход к решению задач визуального вопросно-ответного анализа (VQA), использующий обучение с подкреплением. Метод разбивает VQA на три проверяемых этапа: общее описание изображения, локализацию регионов и предсказание ответа. SATORI решает проблемы рассеивания внимания и высокой дисперсии градиента политики, характерные для свободного рассуждения в мультимодальных задачах. Эксперименты показывают значительное улучшение точности до 15.7% по сравнению с базовой моделью на семи эталонных наборах данных VQA.'}, 'en': {'title': 'SATORI: Enhancing VQA with Structured Stages and Focused Rewards', 'desc': 'The paper introduces SATORI, a method that improves Visual Question Answering (VQA) by breaking it down into three clear stages: global image captioning, region localization, and answer prediction. This structured approach allows for explicit rewards at each stage, which helps the model focus on important areas of the image, thus enhancing accuracy. By addressing the issues of visual focus and policy-gradient variance, SATORI achieves significant performance gains, with improvements of up to 15.7% in accuracy over previous methods. Additionally, the authors provide a new dataset, VQA-Verify, to support the training of their model with annotated captions and bounding boxes.'}, 'zh': {'title': 'SATORI：提升视觉问答的关键区域关注与准确性', 'desc': '本文提出了一种名为SATORI的方法，将视觉问答（VQA）任务分解为三个可验证的阶段：全局图像描述、区域定位和答案预测。每个阶段都提供明确的奖励信号，以增强对关键区域的关注并减少策略梯度的方差。通过引入VQA-Verify数据集，本文为训练提供了带有答案对齐的描述和边界框的12k注释数据。实验结果表明，在七个VQA基准测试中，SATORI方法的准确率提高了最多15.7%。'}}}, {'id': 'https://huggingface.co/papers/2505.17855', 'title': 'Explaining Sources of Uncertainty in Automated Fact-Checking', 'url': 'https://huggingface.co/papers/2505.17855', 'abstract': 'CLUE generates natural language explanations for a language model\'s uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding sources of a model\'s uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes using numerical uncertainty or hedges ("I\'m not sure, but ..."), which do not explain uncertainty that arises from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of model uncertainty by (i) identifying relationships between spans of text that expose claim-evidence or inter-evidence conflicts and agreements that drive the model\'s predictive uncertainty in an unsupervised way, and (ii) generating explanations via prompting and attention steering that verbalize these critical interactions. Across three language models and two fact-checking datasets, we show that CLUE produces explanations that are more faithful to the model\'s uncertainty and more consistent with fact-checking decisions than prompting for uncertainty explanations without span-interaction guidance. Human evaluators judge our explanations to be more helpful, more informative, less redundant, and more logically consistent with the input than this baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and generalises readily to other tasks that require reasoning over complex information.', 'score': 1, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '331b47ce3c56f5bf', 'authors': ['Jingyi Sun', 'Greta Warren', 'Irina Shklovski', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17855.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#training', '#reasoning', '#data'], 'emoji': '🔍', 'ru': {'title': 'CLUE: Прозрачные объяснения неопределенности языковых моделей', 'desc': 'CLUE - это новый метод генерации объяснений неопределенности языковых моделей на естественном языке. Он выявляет конфликты и согласования между фрагментами текста, влияющие на неуверенность модели в предсказаниях. CLUE формирует объяснения с помощью промптов и управления вниманием, вербализуя критические взаимодействия между частями текста. Эксперименты показали, что объяснения CLUE более информативны и логически согласованы с входными данными по сравнению с базовыми методами.'}, 'en': {'title': 'CLUE: Clear Explanations for Model Uncertainty', 'desc': "CLUE is a framework that generates natural language explanations for a language model's uncertainty by analyzing conflicts and agreements in text spans. It identifies how different pieces of evidence relate to each other, revealing the reasons behind the model's uncertainty in its predictions. This approach enhances the clarity of explanations, making them more useful for tasks like fact-checking. CLUE operates without needing any modifications to the model, allowing it to be easily integrated into existing systems."}, 'zh': {'title': 'CLUE：揭示语言模型不确定性的智能解释工具', 'desc': 'CLUE是一个生成自然语言解释的框架，旨在揭示语言模型的不确定性。它通过识别文本片段之间的冲突和一致性，帮助用户理解模型的预测不确定性。与以往的数值不确定性方法不同，CLUE能够提供更清晰的解释，特别是在事实核查等任务中。该框架无需微调或架构更改，适用于任何白盒语言模型，能够有效支持复杂信息的推理。'}}}, {'id': 'https://huggingface.co/papers/2505.17190', 'title': 'Tropical Attention: Neural Algorithmic Reasoning for Combinatorial\n  Algorithms', 'url': 'https://huggingface.co/papers/2505.17190', 'abstract': 'Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.', 'score': 1, 'issue_id': 3997, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': 'e7c2b885aa2f16a0', 'authors': ['Baran Hashemi', 'Kurt Pasque', 'Chris Teska', 'Ruriko Yoshida'], 'affiliations': ['Naval Postgraduate School, Monterey, California', 'Origins Data Science Lab, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.17190.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#security', '#math', '#reasoning'], 'emoji': '🌴', 'ru': {'title': 'Тропическое внимание: острое масштабно-инвариантное рассуждение для нейроалгоритмических задач', 'desc': "В статье представлен новый метод внимания под названием 'Тропическое внимание', который работает в тропической геометрии. Авторы доказывают, что этот метод может аппроксимировать тропические схемы алгоритмов динамического программирования. Эксперименты показывают, что трансформеры с тропическим вниманием превосходят базовые модели с softmax по обобщающей способности вне распределения обучающей выборки. Метод также демонстрирует устойчивость к состязательным атакам."}, 'en': {'title': 'Tropical Attention: Enhancing Algorithmic Reasoning with Sharp Structures', 'desc': 'This paper introduces Tropical attention, a new attention mechanism designed for combinatorial optimization problems that traditionally use dynamic programming. Unlike existing models that use softmax-normalized attention, which can blur important structures in data, Tropical attention operates within the max-plus semiring, preserving the sharp characteristics of value functions. The authors demonstrate that Tropical attention can effectively approximate tropical circuits used in dynamic programming algorithms, leading to improved performance on out-of-distribution tasks. Additionally, they highlight the robustness of Tropical transformers against adversarial attacks, making them a strong alternative to softmax-based models in algorithmic reasoning.'}, 'zh': {'title': '热带注意力：提升算法推理的稳定性与性能', 'desc': '本文介绍了一种新的注意力机制，称为热带注意力，旨在解决组合优化问题中的动态规划算法。传统的神经算法推理模型使用softmax归一化的点积注意力，这会模糊掉重要的几何结构。热带注意力在热带几何的最大加法半环中原生操作，能够更好地近似动态规划类型的电路。实验结果表明，热带变换器在算法推理任务中在长度泛化和价值泛化方面的表现优于softmax基线，并且在对抗攻击下保持稳定。'}}}, {'id': 'https://huggingface.co/papers/2505.16340', 'title': 'Improving Chemical Understanding of LLMs via SMILES Parsing', 'url': 'https://huggingface.co/papers/2505.16340', 'abstract': 'Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.', 'score': 1, 'issue_id': 3993, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '22065ebe729018b8', 'authors': ['Yunhui Jang', 'Jaehyung Kim', 'Sungsoo Ahn'], 'affiliations': ['KAIST', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16340.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#science', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': '🧪', 'ru': {'title': 'CLEANMOL: Улучшение понимания молекул языковыми моделями', 'desc': 'Статья представляет CLEANMOL - новый фреймворк для улучшения понимания молекулярных структур большими языковыми моделями (LLM). CLEANMOL формулирует разбор SMILES-представлений молекул в виде набора детерминированных задач, специально разработанных для улучшения понимания графовой структуры молекул. Авторы создали датасет для предобучения с адаптивной оценкой сложности и предобучили открытые LLM на этих задачах. Результаты показывают, что CLEANMOL улучшает понимание структуры молекул и достигает лучших или сопоставимых результатов на бенчмарке Mol-Instructions.'}, 'en': {'title': 'CLEANMOL: Enhancing LLMs for Molecular Understanding', 'desc': 'This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks.'}, 'zh': {'title': 'CLEANMOL：提升分子结构理解的创新框架', 'desc': '大型语言模型（LLMs）在分子科学的科学发现中被越来越多地认可为强大的工具。为了使这些模型能够准确理解分子结构，我们提出了CLEANMOL框架，将SMILES解析转化为一系列清晰且确定的任务，以促进图级分子理解。这些任务包括子图匹配和全局图匹配，提供与分子结构特性相一致的结构化监督。我们的研究表明，CLEANMOL不仅增强了结构理解能力，还在Mol-Instructions基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.15561', 'title': 'Do RAG Systems Suffer From Positional Bias?', 'url': 'https://huggingface.co/papers/2505.15561', 'abstract': "Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.", 'score': 1, 'issue_id': 3996, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'ca5f3f3552697daa', 'authors': ['Florin Cuconasu', 'Simone Filice', 'Guy Horowitz', 'Yoelle Maarek', 'Fabrizio Silvestri'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15561.jpg', 'data': {'categories': ['#interpretability', '#rag', '#benchmark', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Позиционное смещение в RAG: не так страшно, как кажется', 'desc': 'Данная статья исследует влияние позиционного смещения на точность языковых моделей в контексте Retrieval Augmented Generation. Авторы обнаружили, что современные системы извлечения информации часто помещают отвлекающие отрывки текста на верхние позиции, что может негативно влиять на результаты. Эксперименты показали, что более 60% запросов содержат как минимум один сильно отвлекающий отрывок среди топ-10 извлеченных пассажей. В итоге, сложные стратегии переупорядочивания отрывков не показывают преимуществ перед случайным перемешиванием.'}, 'en': {'title': 'Navigating Positional Bias in Retrieval Augmented Generation', 'desc': "This paper explores how Retrieval Augmented Generation (RAG) can improve the accuracy of large language models (LLMs) by incorporating relevant passages from an external corpus. It specifically examines the effect of positional bias, which is how LLMs prioritize information based on its location in the input prompt. The study finds that while retrieval systems aim to provide relevant information, they often inadvertently highlight distracting passages, affecting the LLM's performance. Ultimately, the research shows that attempts to optimize passage arrangement based on positional bias do not yield better results than random ordering, indicating a need for improved retrieval strategies."}, 'zh': {'title': '位置偏差影响检索增强生成的效果', 'desc': '本论文研究了检索增强生成（RAG）如何通过将外部语料库中的段落添加到大语言模型（LLM）的提示中来提高其准确性。我们探讨了位置偏差，即LLM根据信息在提示中的位置不同而加权的倾向，如何影响其利用相关段落的能力以及对干扰段落的敏感性。通过在三个基准上进行广泛实验，我们发现尽管检索管道旨在获取相关段落，但却系统性地将高度干扰的段落排在前列，超过60%的查询在前10个检索段落中包含至少一个高度干扰的段落。我们的研究表明，试图根据LLM位置偏好重新排列段落的复杂策略并没有比随机打乱表现更好。'}}}, {'id': 'https://huggingface.co/papers/2505.21501', 'title': 'Vision Transformers with Self-Distilled Registers', 'url': 'https://huggingface.co/papers/2505.21501', 'abstract': 'Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher\'s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.', 'score': 0, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'e71640ad11c559f5', 'authors': ['Yinjie Chen', 'Zipeng Yan', 'Chong Zhou', 'Bo Dai', 'Andrew F. Luo'], 'affiliations': ['Nanyang Technological University', 'University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21501.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#training'], 'emoji': '🔬', 'ru': {'title': 'Самодистилляция для улучшения Vision Transformers без переобучения', 'desc': 'Исследователи предложили метод Post Hoc Registers (PH-Reg) для улучшения работы предобученных Vision Transformers (ViT). Этот метод интегрирует регистровые токены в существующие ViT с помощью эффективной самодистилляции, не требуя дополнительных размеченных данных и полного переобучения. PH-Reg инициализирует учительскую и ученическую сети из одного предобученного ViT, оставляя учителя неизменным, а ученика дополняя случайно инициализированными регистровыми токенами. Применение аугментации во время тестирования к входным данным учителя позволяет генерировать очищенные от артефактов плотные эмбеддинги, которые затем используются для оптимизации небольшого подмножества разблокированных весов ученика.'}, 'en': {'title': 'Enhancing Vision Transformers with Post Hoc Registers', 'desc': "This paper introduces Post Hoc Registers (PH-Reg), a self-distillation technique designed to enhance Vision Transformers (ViTs) by integrating register tokens. These register tokens help to mitigate the issue of artifact tokens that can negatively impact the model's performance in tasks requiring precise localization. The method allows for the incorporation of these tokens into pre-trained ViTs without the need for extensive retraining or additional labeled data. By leveraging a frozen teacher network and optimizing a small subset of the student network's weights, PH-Reg effectively improves segmentation and depth prediction capabilities."}, 'zh': {'title': '自蒸馏：提升视觉变换器性能的创新方法', 'desc': '本文提出了一种名为Post Hoc Registers（PH-Reg）的自蒸馏方法，旨在通过将注册令牌集成到预训练的视觉变换器（ViT）中，减少伪影令牌的影响，从而提升分割和深度预测的性能。ViT在视觉处理任务中表现出色，但伪影令牌会干扰局部语义，降低模型在细粒度定位和结构一致性任务中的表现。PH-Reg方法不需要从头开始重新训练模型，而是利用现有的预训练ViT，通过添加随机初始化的注册令牌来优化学生网络。通过这种方式，我们能够有效减少伪影令牌的数量，提升模型在零样本和线性探测下的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.20279', 'title': 'VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D\n  Reconstruction', 'url': 'https://huggingface.co/papers/2505.20279', 'abstract': 'VLM-3R, a framework for Vision-Language Models, incorporates 3D reconstructive instruction tuning to process monocular video frames and perform embodied reasoning with robust visual-spatial and temporal contextual understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, a unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing a geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our Spatial-Visual-View Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability.', 'score': 0, 'issue_id': 4008, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'fb0a1eeef2e6b7ac', 'authors': ['Zhiwen Fan', 'Jian Zhang', 'Renjie Li', 'Junge Zhang', 'Runjin Chen', 'Hezhen Hu', 'Kevin Wang', 'Huaizhi Qu', 'Dilin Wang', 'Zhicheng Yan', 'Hongyu Xu', 'Justin Theiss', 'Tianlong Chen', 'Jiachen Li', 'Zhengzhong Tu', 'Zhangyang Wang', 'Rakesh Ranjan'], 'affiliations': ['Meta', 'TAMU', 'UCR', 'UNC', 'UT Austin', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2505.20279.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#3d', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'VLM-3R: Пространственно-временной интеллект для видео на основе мультимодальных моделей', 'desc': 'VLM-3R - это фреймворк для обработки видеокадров и пространственно-временного рассуждения с использованием моделей компьютерного зрения и обработки естественного языка. Он использует энкодер геометрии для получения неявных 3D-токенов, представляющих пространственное понимание. Модель обучается на более чем 200 тысячах пар вопросов-ответов для 3D-реконструктивного обучения. VLM-3R демонстрирует надежное визуально-пространственное рассуждение и понимание изменений временного 3D-контекста.'}, 'en': {'title': 'Empowering Vision-Language Models with 3D Spatial Intelligence', 'desc': 'VLM-3R is a new framework designed for Vision-Language Models that enhances the understanding of 3D scenes from monocular video inputs. It uses 3D reconstructive instruction tuning to create implicit 3D tokens, which help the model grasp spatial relationships and context. By integrating over 200,000 curated question-answer pairs, VLM-3R aligns visual information with language instructions, enabling effective embodied reasoning. The framework also introduces a benchmark for evaluating temporal reasoning, demonstrating improved accuracy and scalability in understanding dynamic spatial contexts.'}, 'zh': {'title': 'VLM-3R：实现人类般的视觉空间智能', 'desc': 'VLM-3R是一个视觉-语言模型的框架，结合了3D重建指令调优，能够处理单目视频帧并进行具身推理。该模型通过几何编码器提取隐式3D标记，增强了对空间理解的能力。VLM-3R利用超过20万个精心策划的3D重建指令问答对，有效地将现实世界的空间上下文与语言指令对齐。实验结果表明，VLM-3R在视觉-空间推理和时间3D上下文变化理解方面表现出色，具有良好的准确性和可扩展性。'}}}, {'id': 'https://huggingface.co/papers/2505.20052', 'title': 'Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations', 'url': 'https://huggingface.co/papers/2505.20052', 'abstract': 'A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '74f830b28865c630', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.20052.jpg', 'data': {'categories': ['#training', '#optimization', '#healthcare', '#dataset', '#science'], 'emoji': '🧬', 'ru': {'title': 'Многозадачное обучение раскрывает потенциал белковых языковых моделей', 'desc': 'Исследователи разработали новую стратегию предварительного обучения белковых языковых моделей (PLM) с использованием нескольких задач. Модель Ankh3 была оптимизирована для маскированного языкового моделирования и завершения белковых последовательностей. Этот подход позволил PLM изучить более богатые и обобщаемые представления, используя только последовательности белков. Результаты показали улучшение производительности в задачах предсказания вторичной структуры, флуоресценции, фитнеса GB1 и контактов.'}, 'en': {'title': 'Unlocking Protein Insights with Multi-Task Learning', 'desc': 'This paper presents a multi-task pre-training strategy for protein language models (PLMs) to enhance their performance on various protein prediction tasks. The proposed model, Ankh3, is optimized on two objectives: masked language modeling with varying masking probabilities and protein sequence completion, both using only protein sequences. By leveraging multiple tasks during pre-training, the model learns richer and more generalizable representations of protein sequences. The results show significant improvements in downstream tasks, indicating that this approach provides a deeper understanding of protein properties, leading to better prediction accuracy.'}, 'zh': {'title': '多任务预训练，提升蛋白质预测能力', 'desc': '本研究提出了一种多任务预训练策略，用于提高蛋白质语言模型（PLMs）在下游蛋白质预测任务中的表现。通过同时优化多个目标，模型能够从蛋白质序列中学习更丰富的表示。我们开发的Ankh3模型结合了多种掩码语言建模和蛋白质序列补全任务，展示了PLMs在仅依赖蛋白质序列时的学习能力。实验结果表明，该方法在二级结构预测、荧光、GB1适应性和接触预测等任务中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2505.20036', 'title': 'Beyond Simple Concatenation: Fairly Assessing PLM Architectures for\n  Multi-Chain Protein-Protein Interactions Prediction', 'url': 'https://huggingface.co/papers/2505.20036', 'abstract': 'The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatenation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '778086e609851ecb', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Soudy', 'Sara Ossman', 'Abdallah Amr', 'Nehal Adel Abdelsalam', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.20036.jpg', 'data': {'categories': ['#training', '#optimization', '#leakage', '#architecture', '#dataset', '#science'], 'emoji': '🧬', 'ru': {'title': 'Улучшение предсказания аффинности белок-белковых взаимодействий с помощью продвинутых архитектур языковых моделей', 'desc': 'Исследование представляет курированный набор данных PPB-Affinity и оценивает четыре архитектурных подхода для адаптации языковых моделей белков к предсказанию аффинности связывания белок-белок. Авторы демонстрируют, что архитектуры с иерархическим пулингом и добавлением пулированного внимания превосходят методы конкатенации. Набор данных включает 8207 уникальных записей о взаимодействиях белок-белок с порогом идентичности последовательностей ≤30% для предотвращения утечки данных. Эксперименты проводились с использованием нескольких ведущих языковых моделей белков, включая ProtT5, ESM2, Ankh, Ankh2 и ESM3.'}, 'en': {'title': 'Unlocking Protein Interactions: Advanced Architectures for Better Predictions', 'desc': 'This study presents the PPB-Affinity dataset, which contains 8,207 unique entries of protein-protein interactions, refined to eliminate inconsistencies and duplicates. The authors evaluate four different architectural designs for adapting protein language models (PLMs) to predict binding affinity, focusing on hierarchical pooling and pooled attention addition methods. Their experiments show that these advanced architectures significantly outperform traditional concatenation methods, achieving up to a 12% improvement in Spearman correlation. This work emphasizes the importance of using sophisticated model designs to enhance the predictive power of PLMs in understanding protein interactions.'}, 'zh': {'title': '提升蛋白质相互作用预测的架构设计', 'desc': '本研究介绍了一个精心策划的PPB-Affinity数据集，并评估了四种架构设计，以适应蛋白质语言模型预测蛋白质-蛋白质相互作用结合亲和力。研究表明，层次池化和池化注意力加法架构的表现优于简单的连接方法。通过引入严格的数据集标准，确保了训练、验证和测试集的有效分割，减少了数据泄漏的风险。实验结果显示，复杂的架构设计对于充分利用蛋白质语言模型在PPI结合亲和力预测中的能力至关重要。'}}}, {'id': 'https://huggingface.co/papers/2505.19954', 'title': 'An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.19954', 'abstract': "A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions.", 'score': 0, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '3af67df3af862fe0', 'authors': ['Andrew Zamai', 'Nathanael Fijalkow', 'Boris Mansencal', 'Laurent Simon', 'Eloi Navet', 'Pierrick Coupe'], 'affiliations': ['Univ. Bordeaux, CNRS, Bordeaux INP, LaBRI, UMR 5800, F-33400 Talence, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.19954.jpg', 'data': {'categories': ['#3d', '#interpretability', '#cv', '#healthcare', '#multimodal', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Прозрачная нейродиагностика: ИИ с обоснованием', 'desc': 'Статья представляет фреймворк для улучшения диагностической ясности моделей глубокого обучения при нейродегенеративных деменциях. Он использует модульные пайплайны для преобразования 3D МРТ мозга в текстовые радиологические отчеты. Затем применяются большие языковые модели (LLM) для дифференциальной диагностики на основе этих отчетов. Обучение с подкреплением используется для стимулирования диагностических рассуждений в LLM, что позволяет получать структурированные обоснования, основанные на нейровизуализации.'}, 'en': {'title': 'Enhancing Diagnostic Clarity with Causally Grounded Explanations', 'desc': "This paper presents a new framework that improves the clarity of deep learning models used for diagnosing neurodegenerative dementias. It combines modular pipelines that convert brain MRIs into textual reports with reinforcement learning to enhance the reasoning capabilities of Large Language Models (LLMs). By generating explanations that are causally linked to neuroimaging data, the framework provides insights into the model's decision-making process. This approach not only maintains high diagnostic accuracy but also offers transparent rationales that assist clinicians in differentiating between various dementia types."}, 'zh': {'title': '提升神经退行性痴呆诊断透明度的智能框架', 'desc': '本研究提出了一种框架，结合模块化管道和强化学习，旨在提高深度学习模型在神经退行性痴呆诊断中的透明度。我们首先将3D T1加权脑MRI转换为文本放射学报告，然后利用大型语言模型帮助临床医生进行前额叶痴呆亚型、阿尔茨海默病和正常衰老的鉴别诊断。通过强化学习，我们鼓励语言模型进行诊断推理，从而生成基于神经影像学发现的结构化诊断理由。与传统的后验可解释性方法不同，我们的框架在推理过程中生成因果解释，既保持了深度学习模型的预测性能，又提供了支持诊断结论的合理性。'}}}, {'id': 'https://huggingface.co/papers/2505.17639', 'title': 'PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and\n  Retrieval', 'url': 'https://huggingface.co/papers/2505.17639', 'abstract': 'Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.', 'score': 0, 'issue_id': 4004, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '42ac16b897bccbaa', 'authors': ['Zehua Pei', 'Ying Zhang', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17639.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#open_source', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное развертывание гигантских языковых моделей в условиях ограниченной памяти', 'desc': 'Исследование представляет PreMoe - новую систему для эффективного развертывания крупных моделей смеси экспертов (MoE) в условиях ограниченной памяти. PreMoe включает вероятностную обрезку экспертов (PEP) и адаптивное извлечение экспертов под задачу (TAER). PEP использует новую метрику TCESS для определения важности экспертов, а TAER предварительно вычисляет и хранит компактные паттерны экспертов для различных задач. Эксперименты показывают, что PreMoe позволяет значительно сократить объем памяти при сохранении высокой точности на различных задачах.'}, 'en': {'title': 'Efficiently Scaling MoE Models with PreMoe', 'desc': 'This paper presents PreMoe, a framework designed to efficiently deploy large Mixture-of-Experts (MoE) models in environments with limited memory. It introduces two key components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER), which work together to optimize expert selection based on task-specific needs. PEP uses a new metric to determine the importance of experts for particular tasks, allowing for a significant reduction in the number of experts needed. TAER enhances inference speed by pre-computing expert patterns, ensuring that only the most relevant experts are loaded for each user query, thus minimizing memory usage while maintaining high accuracy.'}, 'zh': {'title': '高效部署大规模MoE模型的创新方案', 'desc': '混合专家（MoE）架构可以在不显著增加计算成本的情况下，扩展大型语言模型（LLM）的参数数量。然而，大型MoE模型的内存需求使其在各种计算环境中的部署受到限制。本文首先展示了MoE层中专家激活模式的任务特定专业化。基于此，我们提出了PreMoe框架，通过概率专家修剪（PEP）和任务自适应专家检索（TAER）来实现大规模MoE模型在内存受限环境中的高效部署。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (10)', '#agi (2)', '#alignment (6)', '#architecture (11)', '#audio (1)', '#benchmark (44)', '#cv (15)', '#data (8)', '#dataset (17)', '#diffusion (6)', '#ethics (1)', '#games (10)', '#graphs (1)', '#hallucinations (3)', '#healthcare (4)', '#inference (9)', '#interpretability (8)', '#leakage (1)', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (4)', '#multilingual (2)', '#multimodal (27)', '#open_source (13)', '#optimization (30)', '#plp', '#rag (4)', '#reasoning (37)', '#rl (17)', '#rlhf (4)', '#robotics', '#science (7)', '#security (5)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (26)', '#transfer_learning (1)', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-28 21:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-28 21:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-28 21:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    