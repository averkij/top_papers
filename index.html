
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 47 papers. May 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 Ğ¼Ğ°Ñ</span> | <span id="title-articles-count">47 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-28.html">â¬…ï¸ <span id="prev-date">28.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-30.html">â¡ï¸ <span id="next-date">30.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 Ğ¼Ğ°Ñ', 'en': 'May 29', 'zh': '5æœˆ29æ—¥'};
        let feedDateNext = {'ru': '30.05', 'en': '05/30', 'zh': '5æœˆ30æ—¥'};
        let feedDatePrev = {'ru': '28.05', 'en': '05/28', 'zh': '5æœˆ28æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.22617', 'title': 'The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models', 'url': 'https://huggingface.co/papers/2505.22617', 'abstract': 'This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.', 'score': 79, 'issue_id': 4014, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '7313a363374c80e6', 'authors': ['Ganqu Cui', 'Yuchen Zhang', 'Jiacheng Chen', 'Lifan Yuan', 'Zhi Wang', 'Yuxin Zuo', 'Haozhan Li', 'Yuchen Fan', 'Huayu Chen', 'Weize Chen', 'Zhiyuan Liu', 'Hao Peng', 'Lei Bai', 'Wanli Ouyang', 'Yu Cheng', 'Bowen Zhou', 'Ning Ding'], 'affiliations': ['CUHK', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'Tsinghua University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2505.22617.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ˜ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹, ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğµ Ğ·Ğ° ĞµĞµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ÑÑ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° - Clip-Cov Ğ¸ KL-Cov - Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ĞºĞ¾Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Managing Entropy for Enhanced Exploration in RL', 'desc': 'This paper addresses the issue of policy entropy collapse in reinforcement learning (RL) when applied to large language models (LLMs). It identifies that as training progresses, the policy entropy decreases significantly, leading to reduced exploration and stagnation in policy performance. The authors establish a relationship between entropy and performance, suggesting that managing entropy is crucial for effective exploration in RL. They propose two techniques, Clip-Cov and KL-Cov, to control high-covariance tokens, which help maintain policy entropy and improve overall performance.'}, 'zh': {'title': 'æ§åˆ¶ç†µä»¥ä¿ƒè¿›æ¢ç´¢ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ€§èƒ½', 'desc': 'æœ¬æ–‡æ—¨åœ¨å…‹æœåœ¨å¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ç›¸å…³çš„ä¸€ä¸ªä¸»è¦éšœç¢ï¼Œå³ç­–ç•¥ç†µçš„å´©æºƒã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ²¡æœ‰ç†µå¹²é¢„çš„æƒ…å†µä¸‹ï¼Œç­–ç•¥ç†µåœ¨è®­ç»ƒåˆæœŸæ€¥å‰§ä¸‹é™ï¼Œå¯¼è‡´æ¢ç´¢èƒ½åŠ›å‡å¼±ï¼Œå¹¶ä¼´éšç­–ç•¥æ€§èƒ½çš„é¥±å’Œã€‚æˆ‘ä»¬å»ºç«‹äº†ç†µä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´çš„å˜æ¢æ–¹ç¨‹ï¼Œè¡¨æ˜ç­–ç•¥æ€§èƒ½ä¸ç­–ç•¥ç†µä¹‹é—´å­˜åœ¨æƒè¡¡å…³ç³»ï¼Œå› æ­¤ç†µçš„è€—å°½æˆä¸ºç“¶é¢ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§ç®€å•æœ‰æ•ˆçš„æŠ€æœ¯ï¼ŒClip-Covå’ŒKL-Covï¼Œæ—¨åœ¨é€šè¿‡é™åˆ¶é«˜åæ–¹å·®æ ‡è®°çš„æ›´æ–°æ¥æ§åˆ¶ç†µï¼Œä»è€Œé¼“åŠ±æ¢ç´¢å¹¶æ”¹å–„ç­–ç•¥æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21600', 'title': 'R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large\n  Model Token Routing', 'url': 'https://huggingface.co/papers/2505.21600', 'abstract': "Roads to Rome (R2R) selectively utilizes large language models for critical reasoning tasks to enhance efficiency and performance in lightweight models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.", 'score': 53, 'issue_id': 4013, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '54850077437e9524', 'authors': ['Tianyu Fu', 'Yi Ge', 'Yichen You', 'Enshu Liu', 'Zhihang Yuan', 'Guohao Dai', 'Shengen Yan', 'Huazhong Yang', 'Yu Wang'], 'affiliations': ['Infinigence AI', 'Shanghai Jiao Tong University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21600.jpg', 'data': {'categories': ['#small_models', '#architecture', '#optimization', '#reasoning', '#benchmark', '#math', '#training'], 'emoji': 'ğŸ›£ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Roads to Rome (R2R), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. R2R Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ·Ğ°Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑ LLM Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ…, Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ°Ğ»Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (SLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğº Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ R2R Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Efficient Reasoning with Selective Token Routing', 'desc': 'The paper introduces Roads to Rome (R2R), a method that enhances the efficiency of lightweight models by selectively using large language models (LLMs) for critical reasoning tasks. It identifies that only a small number of tokens significantly affect the reasoning paths between LLMs and distilled small language models (SLMs). By focusing on these path-divergent tokens, R2R allows SLMs to handle the majority of token generation, improving overall performance without the heavy computational cost of LLMs. The results show that R2R achieves higher accuracy and faster processing times compared to existing models, pushing the boundaries of efficiency in machine learning applications.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†çš„æ–°è·¯å¾„ï¼šR2Ræ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºRoads to Rome (R2R)çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡é€‰æ‹©æ€§åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å¤„ç†å…³é”®æ¨ç†ä»»åŠ¡ï¼Œä»è€Œæé«˜è½»é‡çº§æ¨¡å‹çš„æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMså’Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä¹‹é—´çš„æ¨ç†è·¯å¾„ä»…åœ¨å°‘æ•°æ ‡è®°ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå¤§å¤šæ•°ç”Ÿæˆçš„æ ‡è®°è¦ä¹ˆç›¸åŒï¼Œè¦ä¹ˆå·®å¼‚å¾®å°ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼ŒR2Ræ–¹æ³•ä»…åœ¨å…³é”®çš„è·¯å¾„åˆ†æ­§æ ‡è®°ä¸Šä½¿ç”¨LLMsï¼Œè€Œå°†å¤§éƒ¨åˆ†æ ‡è®°ç”Ÿæˆç•™ç»™SLMsï¼Œä»è€Œå®ç°äº†é«˜æ•ˆçš„æ¨ç†ã€‚é€šè¿‡åœ¨å¤šä¸ªæ•°å­¦ã€ç¼–ç å’Œé—®ç­”åŸºå‡†ä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒR2Råœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—æé«˜äº†è®¡ç®—é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20411', 'title': 'SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents', 'url': 'https://huggingface.co/papers/2505.20411', 'abstract': 'A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.', 'score': 49, 'issue_id': 4020, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '223845bb2e40cbe5', 'authors': ['Ibragim Badertdinov', 'Alexander Golubev', 'Maksim Nekrashevich', 'Anton Shevtsov', 'Simon Karasik', 'Andrei Andriushchenko', 'Maria Trofimova', 'Daria Litvintseva', 'Boris Yangel'], 'affiliations': ['Nebius'], 'pdf_title_img': 'assets/pdf/title_img/2505.20411.jpg', 'data': {'categories': ['#benchmark', '#transfer_learning', '#data', '#open_source', '#games', '#rl', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞŸĞ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² GitHub. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑÑ‚Ğ°Ğ» Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SWE-rebench, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ĞµĞµ 21 000 Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ½Ğ° Python, Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ·Ğ°Ğ³Ñ€ÑĞ·Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞŸĞ.'}, 'en': {'title': 'Revolutionizing Software Engineering Evaluation with SWE-rebench', 'desc': 'This paper presents a new method for extracting real-world software engineering tasks from GitHub, creating a dataset called SWE-rebench. This dataset contains over 21,000 interactive Python tasks, which are essential for training reinforcement learning models in software engineering. The authors highlight the importance of having diverse and up-to-date tasks to evaluate the performance of these models accurately. By addressing the challenges of data scarcity and contamination in benchmarks, this work aims to enhance the development and assessment of AI agents in software engineering.'}, 'zh': {'title': 'æ„å»ºçœŸå®äº¤äº’ä»»åŠ¡ï¼Œæå‡è½¯ä»¶å·¥ç¨‹æ¨¡å‹è¯„ä¼°', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç®¡é“ï¼Œä»GitHubæå–çœŸå®ä¸–ç•Œçš„äº¤äº’å¼è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œä»¥åˆ›å»ºSWE-rebenchï¼Œä»è€Œæ”¹å–„å¼ºåŒ–å­¦ä¹ æ¨¡å‹åœ¨è½¯ä»¶å·¥ç¨‹ä¸­çš„è¯„ä¼°ã€‚å½“å‰ï¼Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ç¨€ç¼ºï¼Œå°¤å…¶æ˜¯èƒ½å¤Ÿåæ˜ çœŸå®å¼€å‘ç¯å¢ƒçš„äº¤äº’å¼ä»»åŠ¡ã€‚é€šè¿‡è‡ªåŠ¨åŒ–å’Œå¯æ‰©å±•çš„æ–¹å¼ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡21,000ä¸ªäº¤äº’å¼Pythonè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„å…¬å…±æ•°æ®é›†ï¼Œé€‚åˆå¤§è§„æ¨¡å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨SWE-rebenchæ–¹æ³•æ”¶é›†çš„æ–°ä»»åŠ¡ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ— æ±¡æŸ“çš„åŸºå‡†ï¼Œä»¥ä¾¿æ›´å‡†ç¡®åœ°è¯„ä¼°è½¯ä»¶å·¥ç¨‹ä»£ç†çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22651', 'title': 'Sherlock: Self-Correcting Reasoning in Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.22651', 'abstract': "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic beta for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data.", 'score': 43, 'issue_id': 4019, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '39e5a06798255d74', 'authors': ['Yi Ding', 'Ruqi Zhang'], 'affiliations': ['Department of Computer Science, Purdue University, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.22651.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#multimodal', '#reasoning'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ VLM: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²Ñ‹ÑˆĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Sherlock - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸ Ğ¸ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ (VLM). Sherlock Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½ÑƒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼ÑƒÑ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ±ĞµÑ‚Ğ°-Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºÑƒ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ²ÑĞµĞ³Ğ¾ 20 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ²Ğ¾ÑÑŒĞ¼Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Sherlock Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Sherlock: Self-Correction for Smarter Vision-Language Models', 'desc': 'The paper presents Sherlock, a framework designed to enhance reasoning in vision-language models (VLMs) through self-correction and self-improvement. It addresses the challenges of high sensitivity to reasoning errors and the need for extensive annotated data by introducing a trajectory-level self-correction objective and a method for constructing preference data. Sherlock allows the model to improve its reasoning capabilities using only a small amount of annotated data, achieving significant accuracy improvements across multiple benchmarks. The results show that Sherlock outperforms existing models while utilizing less than 20% of the required annotated data, demonstrating its efficiency and effectiveness in enhancing VLM performance.'}, 'zh': {'title': 'Sherlockï¼šæ¨ç†æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£ä¸æå‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†Sherlockï¼Œä¸€ä¸ªç”¨äºæ¨ç†è§†è§‰è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘ä¿®æ­£å’Œè‡ªæˆ‘æ”¹è¿›æ¡†æ¶ã€‚Sherlocké€šè¿‡å¼•å…¥è½¨è¿¹çº§è‡ªæˆ‘ä¿®æ­£ç›®æ ‡å’ŒåŸºäºè§†è§‰æ‰°åŠ¨çš„åå¥½æ•°æ®æ„å»ºæ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åœ¨ä»…ä½¿ç”¨2ä¸‡æ¡éšæœºé‡‡æ ·çš„æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå®ç°è‡ªæˆ‘æ”¹è¿›ï¼Œä¸”æ— éœ€å¤–éƒ¨ç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSherlockåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°64.1ï¼Œä¸”åœ¨è‡ªæˆ‘ä¿®æ­£åæå‡è‡³65.4ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22312', 'title': 'Skywork Open Reasoner 1 Technical Report', 'url': 'https://huggingface.co/papers/2505.22312', 'abstract': 'The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.', 'score': 43, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '712fb8daefb54030', 'authors': ['Jujie He', 'Jiacai Liu', 'Chris Yuhao Liu', 'Rui Yan', 'Chaojie Wang', 'Peng Cheng', 'Xiaoyu Zhang', 'Fuxiang Zhang', 'Jiacheng Xu', 'Wei Shen', 'Siyuan Li', 'Liang Zeng', 'Tianwen Wei', 'Cheng Cheng', 'Bo An', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.22312.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rl', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Skywork-OR1, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ° Ğ¸ Ñ„ĞµĞ½Ğ¾Ğ¼ĞµĞ½Ğ° ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿ÑĞ° ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºÑƒÑÑ‚ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ğ´ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°.'}, 'en': {'title': 'Boosting Reasoning in Language Models with Reinforcement Learning', 'desc': 'This paper introduces Skywork-OR1, a reinforcement learning (RL) framework designed to improve the reasoning abilities of large language models (LLMs) through long Chain-of-Thought (CoT) processes. By building on the previous DeepSeek-R1-Distill models, Skywork-OR1 achieves significant accuracy improvements on various benchmarks, with the 32B model increasing from 57.8% to 72.8%. The study also addresses the issue of entropy collapse, highlighting its impact on model performance and the importance of managing it during training. The authors provide open access to their model weights and training resources to foster further research in the community.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ æå‡è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„çªç ´', 'desc': 'DeepSeek-R1çš„æˆåŠŸè¡¨æ˜å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†Skywork-OR1ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¯æ‰©å±•çš„å¼ºåŒ–å­¦ä¹ å®ç°ï¼Œä¸“ä¸ºé•¿é“¾æ€ç»´æ¨¡å‹è®¾è®¡ã€‚é€šè¿‡å¯¹DeepSeek-R1-Distillæ¨¡å‹ç³»åˆ—çš„æ”¹è¿›ï¼Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜äº†å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼ŒéªŒè¯äº†è®­ç»ƒæµç¨‹ä¸­æ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ¢è®¨äº†ç†µå´©æºƒç°è±¡åŠå…¶å¯¹æµ‹è¯•æ€§èƒ½çš„å½±å“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22453', 'title': 'Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO', 'url': 'https://huggingface.co/papers/2505.22453', 'abstract': 'Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %rightarrow72.9 % on MathVista, 62.9 %rightarrow68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.', 'score': 34, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '96b8c222cef41b46', 'authors': ['Lai Wei', 'Yuting Li', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Weiran Huang', 'Lichao Sun'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22453.jpg', 'data': {'categories': ['#reasoning', '#rl', '#rlhf', '#multimodal', '#training', '#synthetic'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ MM-UPT Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ GRPO, Ğ·Ğ°Ğ¼ĞµĞ½ÑÑ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ¼ ÑĞ°Ğ¼Ğ¾Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-VL-7B Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. MM-UPT Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ½ĞµĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ Ğº Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ GRPO, Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ MLLM.'}, 'en': {'title': 'Autonomous Self-Improvement for MLLMs with MM-UPT', 'desc': 'This paper introduces MM-UPT, a novel framework for improving Multi-modal Large Language Models (MLLMs) without the need for expensive supervised fine-tuning. It leverages a stable online reinforcement learning algorithm called GRPO, which allows for continual self-improvement through a self-rewarding mechanism based on majority voting of responses. The experiments show that MM-UPT enhances the reasoning capabilities of the Qwen2.5-VL-7B model significantly, even outperforming previous unsupervised methods. Additionally, the incorporation of synthetic questions generated by the MLLM itself further boosts performance, suggesting a scalable approach for autonomous model enhancement.'}, 'zh': {'title': 'æ— ç›‘ç£è‡ªæˆ‘æ”¹è¿›çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ–°æ¡†æ¶', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶MM-UPTï¼Œç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ— ç›‘ç£åè®­ç»ƒã€‚ä¸ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ä¸åŒï¼ŒMM-UPTåˆ©ç”¨äº†ä¸€ç§åŸºäºå¤šæ•°æŠ•ç¥¨çš„è‡ªæˆ‘å¥–åŠ±æœºåˆ¶ï¼Œé¿å…äº†å¯¹æ˜‚è´µæ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å‘ç°MM-UPTæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ— ç›‘ç£çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜äºä¹‹å‰çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è¿˜å±•ç¤ºäº†é€šè¿‡ç”Ÿæˆåˆæˆé—®é¢˜æ¥è¿›ä¸€æ­¥æå‡æ€§èƒ½çš„æ½œåŠ›ï¼Œä¸ºMLLMsçš„æŒç»­è‡ªæˆ‘æ”¹è¿›æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21136', 'title': 'SageAttention2++: A More Efficient Implementation of SageAttention2', 'url': 'https://huggingface.co/papers/2505.21136', 'abstract': 'The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.', 'score': 29, 'issue_id': 4014, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'd26c2e844cce91c6', 'authors': ['Jintao Zhang', 'Xiaoming Xu', 'Jia Wei', 'Haofeng Huang', 'Pengle Zhang', 'Chendong Xiang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Department of Computer Science, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21136.jpg', 'data': {'categories': ['#architecture', '#inference'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'SageAttention2++ - ÑÑ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ SageAttention2, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğµ Ğ½Ğ¾Ğ²Ğ¾Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ FP8 Matmul Ñ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² FP16, Ñ‡Ñ‚Ğ¾ Ğ² Ğ´Ğ²Ğ° Ñ€Ğ°Ğ·Ğ° Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ñ‡ĞµĞ¼ FP8 Matmul Ğ² SageAttention2. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ SageAttention2++ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 3.9-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ FlashAttention, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ SageAttention2. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ°, Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸ Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ñ….'}, 'en': {'title': 'Accelerating Attention with SageAttention2++', 'desc': 'This paper introduces SageAttention2++, an improved version of the SageAttention2 model that enhances the efficiency of attention mechanisms in machine learning. It achieves this by implementing quantization techniques to speed up matrix multiplications, which are crucial for attention calculations. Additionally, it leverages a faster FP8 Matmul instruction that is twice as fast as the previous FP8 Matmul used in SageAttention2. The results demonstrate that SageAttention2++ provides a significant speedup while preserving the accuracy of attention, making it suitable for various applications in language, image, and video generation.'}, 'zh': {'title': 'åŠ é€Ÿæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶SageAttention2++ï¼Œæ—¨åœ¨æé«˜è®¡ç®—æ•ˆç‡ã€‚é€šè¿‡é‡åŒ–æŠ€æœ¯åŠ é€ŸçŸ©é˜µä¹˜æ³•ï¼ŒSageAttention2++åœ¨ä¿æŒç›¸åŒæ³¨æ„åŠ›å‡†ç¡®åº¦çš„åŒæ—¶ï¼Œå®ç°äº†3.9å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨FP8çŸ©é˜µä¹˜æ³•çš„æ›´å¿«æŒ‡ä»¤ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿäº†è®¡ç®—è¿‡ç¨‹ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºè¯­è¨€ã€å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰å¤šç§æ¨¡å‹ï¼Œä¸”å‡ ä¹æ²¡æœ‰æŸå¤±ç«¯åˆ°ç«¯çš„æ€§èƒ½æŒ‡æ ‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22457', 'title': 'Fostering Video Reasoning via Next-Event Prediction', 'url': 'https://huggingface.co/papers/2505.22457', 'abstract': 'Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.  \t\t\t\t\tAI-generated summary \t\t\t\t Next-token prediction serves as the foundational learning task enabling reasoning in LLMs. But what should the learning task be when aiming to equip MLLMs with temporal reasoning capabilities over video inputs? Existing tasks such as video question answering often rely on annotations from humans or much stronger MLLMs, while video captioning tends to entangle temporal reasoning with spatial information. To address this gap, we propose next-event prediction (NEP), a learning task that harnesses future video segments as a rich, self-supervised signal to foster temporal reasoning. We segment each video into past and future frames: the MLLM takes the past frames as input and predicts a summary of events derived from the future frames, thereby encouraging the model to reason temporally in order to complete the task. To support this task, we curate V1-33K, a dataset comprising 33,000 automatically extracted video segments spanning diverse real-world scenarios. We further explore a range of video instruction-tuning strategies to study their effects on temporal reasoning. To evaluate progress, we introduce FutureBench to assess coherence in predicting unseen future events. Experiments validate that NEP offers a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.', 'score': 25, 'issue_id': 4019, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '1ec4fe9e4580308e', 'authors': ['Haonan Wang', 'Hongfu Liu', 'Xiangyan Liu', 'Chao Du', 'Kenji Kawaguchi', 'Ye Wang', 'Tianyu Pang'], 'affiliations': ['National University of Singapore', 'SSea AI Lab, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.22457.jpg', 'data': {'categories': ['#benchmark', '#training', '#video', '#multimodal', '#dataset', '#reasoning'], 'emoji': 'ğŸ¬', 'ru': {'title': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ³Ğ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) - Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ (NEP) Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­Ñ‚Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ V1-33K Ğ¸Ğ· 33 000 Ğ²Ğ¸Ğ´ĞµĞ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ° Ğ±Ñ‹Ğ» Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº FutureBench, Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹.'}, 'en': {'title': 'Empowering MLLMs with Temporal Reasoning through Next-Event Prediction', 'desc': "This paper introduces Next-event prediction (NEP) as a new learning task designed to enhance the temporal reasoning abilities of Multi-Modal Language Models (MLLMs) when processing video data. NEP utilizes future video segments as a self-supervised signal, allowing the model to predict events based on past frames. The authors present a dataset called V1-33K, which contains 33,000 video segments to support this task, and they also propose FutureBench for evaluating the model's performance in predicting future events. The experiments demonstrate that NEP is an effective and scalable approach for improving temporal reasoning in MLLMs."}, 'zh': {'title': 'ä¸‹ä¸€äº‹ä»¶é¢„æµ‹ï¼šæå‡è§†é¢‘æ—¶é—´æ¨ç†èƒ½åŠ›çš„åˆ›æ–°ä»»åŠ¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸‹ä¸€äº‹ä»¶é¢„æµ‹ï¼ˆNEPï¼‰ä½œä¸ºä¸€ç§å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿå¯¹è§†é¢‘è¾“å…¥è¿›è¡Œæ—¶é—´æ¨ç†ã€‚é€šè¿‡åˆ©ç”¨æœªæ¥è§†é¢‘ç‰‡æ®µä½œä¸ºè‡ªç›‘ç£ä¿¡å·ï¼ŒNEPé¼“åŠ±æ¨¡å‹åœ¨å¤„ç†è¿‡å»å¸§æ—¶é¢„æµ‹æœªæ¥äº‹ä»¶çš„æ‘˜è¦ï¼Œä»è€Œå¢å¼ºæ—¶é—´æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†V1-33Kæ•°æ®é›†ï¼ŒåŒ…å«33,000ä¸ªè‡ªåŠ¨æå–çš„è§†é¢‘ç‰‡æ®µï¼Œæ¶µç›–å¤šç§çœŸå®åœºæ™¯ï¼Œä»¥æ”¯æŒè¿™ä¸€ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNEPä¸ºä¿ƒè¿›MLLMsçš„æ—¶é—´æ¨ç†æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22334', 'title': 'Advancing Multimodal Reasoning via Reinforcement Learning with Cold\n  Start', 'url': 'https://huggingface.co/papers/2505.22334', 'abstract': 'Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %rightarrow73.4 % on MathVista, 62.9 %rightarrow70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.', 'score': 25, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '129b6f3da36369f9', 'authors': ['Lai Wei', 'Yuting Li', 'Kaipeng Zheng', 'Chen Wang', 'Yue Wang', 'Linghe Kong', 'Lichao Sun', 'Weiran Huang'], 'affiliations': ['Lehigh University', 'School of Computer Science, Shanghai Jiao Tong University', 'Shanghai Innovation Institute', 'Zhongguancun Academy'], 'pdf_title_img': 'assets/pdf/title_img/2505.22334.jpg', 'data': {'categories': ['#reasoning', '#rl', '#multimodal', '#benchmark', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ²Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with a Two-Stage Approach', 'desc': "This paper explores the reasoning abilities of large language models (LLMs) and their improvement through reinforcement learning (RL). It reveals that 'aha moment' patterns, which indicate self-correction, are present in multimodal LLMs even before RL training, although they do not always lead to better reasoning. The authors propose a two-stage enhancement method: first, using supervised fine-tuning (SFT) to establish structured reasoning, and then applying RL to refine these skills. Their experiments show that this combined approach significantly outperforms traditional methods, achieving state-of-the-art results in multimodal reasoning tasks."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨ç†çš„åŒé˜¶æ®µæ–¹æ³•', 'desc': 'æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•æ˜¾ç¤ºå‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€ç»´é“¾æ¨ç†èƒ½åŠ›ï¼Œè€Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨è¿™ä¸€è¿›å±•ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚æˆ‘ä»¬é¦–æ¬¡è¯æ˜äº†â€œé¡¿æ‚Ÿæ—¶åˆ»â€æ¨¡å¼åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å­˜åœ¨ï¼Œè¿™äº›æ¨¡å¼åœ¨RLè®­ç»ƒä¹‹å‰å°±å·²å­˜åœ¨ï¼Œä½†ä¸ä¸€å®šä¸æ¨ç†æ€§èƒ½çš„æå‡ç›¸å…³ã€‚åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡ä¸¤é˜¶æ®µæ–¹æ³•å¢å¼ºå¤šæ¨¡æ€æ¨ç†çš„å…¨é¢ç ”ç©¶ï¼šé¦–å…ˆè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç„¶åé€šè¿‡GRPOè¿›è¡Œå¼ºåŒ–å­¦ä¹ ä»¥è¿›ä¸€æ­¥æå‡èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§ç»“åˆçš„æ–¹æ³•åœ¨å¤šæ¨¡æ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å§‹ç»ˆä¼˜äºä»…ä½¿ç”¨SFTæˆ–RLçš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21925', 'title': 'RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with\n  Global Illumination', 'url': 'https://huggingface.co/papers/2505.21925', 'abstract': 'We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.', 'score': 23, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'a6275b50c7372a2c', 'authors': ['Chong Zeng', 'Yue Dong', 'Pieter Peers', 'Hongzhi Wu', 'Xin Tong'], 'affiliations': ['College of William & Mary', 'Microsoft Research Asia', 'State Key Lab of CAD & CG, Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21925.jpg', 'data': {'categories': ['#architecture', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ğ±ĞµĞ· Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸: Ğ¾Ñ‚ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ğº Ğ¿Ğ¸ĞºÑĞµĞ»ÑĞ¼', 'desc': 'RenderFormer - ÑÑ‚Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ· Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ†ĞµĞ½Ñ‹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, Ğ³Ğ´Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¸ Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸ Ğ¾Ñ‚Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‚ÑÑ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğµ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. RenderFormer ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ²: Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ Ğ²Ğ¸Ğ´Ğ° ÑÑ‚Ğ°Ğ¿Ğ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑĞ²ĞµÑ‚Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ°Ğ¼Ğ¸, Ğ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚ Ğ²Ğ¸Ğ´Ğ° ÑÑ‚Ğ°Ğ¿Ğ°, Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿ÑƒÑ‡Ğ¾Ğº Ğ»ÑƒÑ‡ĞµĞ¹, Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. ĞĞ±Ğ° ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸.'}, 'en': {'title': 'Transforming Triangles to Pixels with RenderFormer', 'desc': 'RenderFormer is a novel neural rendering pipeline that generates images directly from a triangle-based scene representation, incorporating full global illumination effects without needing specific training for each scene. It approaches rendering as a sequence-to-sequence transformation, where input tokens representing triangles are converted into output tokens for pixel patches. The pipeline consists of two stages: a view-independent stage that models light transport between triangles, and a view-dependent stage that converts ray bundles into pixel values. Both stages utilize the transformer architecture and are designed to learn with minimal prior constraints, showcasing effectiveness across various scene complexities.'}, 'zh': {'title': 'RenderFormerï¼šæ— éœ€è®­ç»ƒçš„é«˜æ•ˆç¥ç»æ¸²æŸ“', 'desc': 'RenderFormeræ˜¯ä¸€ç§ç¥ç»æ¸²æŸ“ç®¡é“ï¼Œå¯ä»¥ç›´æ¥ä»åŸºäºä¸‰è§’å½¢çš„åœºæ™¯è¡¨ç¤ºä¸­æ¸²æŸ“å›¾åƒï¼Œå¹¶å®ç°å…¨å±€å…‰ç…§æ•ˆæœï¼Œè€Œæ— éœ€é’ˆå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒã€‚è¯¥æ–¹æ³•å°†æ¸²æŸ“è§†ä¸ºä¸€ç§åºåˆ—åˆ°åºåˆ—çš„è½¬æ¢ï¼Œå°†è¡¨ç¤ºä¸‰è§’å½¢åŠå…¶åå°„ç‰¹æ€§çš„ä»¤ç‰Œåºåˆ—è½¬æ¢ä¸ºè¡¨ç¤ºå°åƒç´ å—çš„è¾“å‡ºä»¤ç‰Œåºåˆ—ã€‚RenderFormeré‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼šç¬¬ä¸€é˜¶æ®µæ˜¯è§†å›¾æ— å…³çš„ä¸‰è§’å½¢å…‰ä¼ è¾“å»ºæ¨¡ï¼Œç¬¬äºŒé˜¶æ®µåˆ™æ˜¯å°†å…‰æŸçš„ä»¤ç‰Œè½¬æ¢ä¸ºç›¸åº”çš„åƒç´ å€¼ã€‚ä¸¤ä¸ªé˜¶æ®µéƒ½åŸºäºå˜æ¢å™¨æ¶æ„ï¼Œå¹¶åœ¨æœ€å°çš„å…ˆéªŒçº¦æŸä¸‹è¿›è¡Œå­¦ä¹ ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19253', 'title': 'DeepResearchGym: A Free, Transparent, and Reproducible Evaluation\n  Sandbox for Deep Research', 'url': 'https://huggingface.co/papers/2505.19253', 'abstract': "DeepResearchGym provides an open-source evaluation framework for deep research systems using a reproducible search API and LLM-as-a-judge assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai.", 'score': 20, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'fabfc1ec7f6826eb', 'authors': ['JoÃ£o Coelho', 'Jingjie Ning', 'Jingyuan He', 'Kangrui Mao', 'Abhijay Paladugu', 'Pranav Setlur', 'Jiahe Jin', 'Jamie Callan', 'JoÃ£o MagalhÃ£es', 'Bruno Martins', 'Chenyan Xiong'], 'affiliations': ['Carnegie Mellon University', 'IST and INESC-ID', 'NOVA LINCS'], 'pdf_title_img': 'assets/pdf/title_img/2505.19253.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#alignment', '#rag'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'DeepResearchGym - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ API Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹. ĞĞ½Ğ° Ğ¸Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ²ĞµĞ±-ĞºĞ¾Ñ€Ğ¿ÑƒÑÑ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€ĞµÑ‚Ñ€Ğ¸Ğ²ĞµÑ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞ¸Ñ… ÑĞ¾ÑĞµĞ´ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Researchy Questions Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ DeepResearchGym, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ñ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ API.'}, 'en': {'title': 'Revolutionizing Research Evaluation with DeepResearchGym', 'desc': "DeepResearchGym is an open-source framework designed to evaluate deep research systems, which are advanced methods for retrieving and generating detailed reports from complex queries. It addresses issues of reproducibility and transparency found in existing commercial search APIs by providing a stable and cost-free alternative. The framework utilizes a state-of-the-art dense retriever and approximate nearest neighbor search to index large public web corpora, ensuring lower latency and consistent document rankings. Additionally, it incorporates LLM-as-a-judge assessments to automatically evaluate the quality of outputs, aligning them with user needs and confirming the framework's effectiveness through both automatic and human evaluations."}, 'zh': {'title': 'æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„å¼€æºè¯„ä¼°æ¡†æ¶', 'desc': 'DeepResearchGymæ˜¯ä¸€ä¸ªå¼€æºè¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºæ·±åº¦ç ”ç©¶ç³»ç»Ÿæä¾›å¯é‡å¤çš„æœç´¢APIå’ŒLLMä½œä¸ºè¯„ä¼°è€…çš„è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰å•†ä¸šæœç´¢APIåœ¨å¯é‡å¤æ€§å’Œé€æ˜æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶é™ä½äº†æˆæœ¬ã€‚å®ƒä½¿ç”¨å…ˆè¿›çš„å¯†é›†æ£€ç´¢å™¨å’ŒDiskANNè¿›è¡Œè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ï¼Œèƒ½å¤Ÿåœ¨è¾ƒä½å»¶è¿Ÿä¸‹ç´¢å¼•å¤§è§„æ¨¡å…¬å…±ç½‘ç»œè¯­æ–™åº“ã€‚é€šè¿‡æ‰©å±•Researchy QuestionsåŸºå‡†ï¼ŒDeepResearchGymèƒ½å¤Ÿè‡ªåŠ¨è¯„ä¼°æ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„è¾“å‡ºï¼Œç¡®ä¿ä¸ç”¨æˆ·ä¿¡æ¯éœ€æ±‚çš„ä¸€è‡´æ€§å’ŒæŠ¥å‘Šè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18600', 'title': 'Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment', 'url': 'https://huggingface.co/papers/2505.18600', 'abstract': 'Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: https://bryanswkim.github.io/chain-of-zoom/ .', 'score': 19, 'issue_id': 4018, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': '4c38f564fd997a1c', 'authors': ['Bryan Sangwoo Kim', 'Jeongsol Kim', 'Jong Chul Ye'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18600.jpg', 'data': {'categories': ['#alignment', '#cv', '#optimization', '#diffusion', '#rlhf', '#rag'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¾Ñ‚ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Chain-of-Zoom (CoZ) - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. CoZ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ²ĞµÑ€Ñ…Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Zooming into High-Quality Super-Resolution with CoZ', 'desc': 'Chain-of-Zoom (CoZ) is a novel framework that improves single-image super-resolution (SISR) by breaking down the process into an autoregressive sequence of intermediate scale-states. This approach allows the model to achieve extreme magnifications while maintaining high image quality, even beyond the typical training scale. CoZ utilizes multi-scale-aware prompts generated by a vision-language model to enhance the visual cues at high magnifications. By employing Generalized Reward Policy Optimization (GRPO) for fine-tuning, CoZ aligns the prompts with human preferences, resulting in superior perceptual quality in the generated images.'}, 'zh': {'title': 'è¶…åˆ†è¾¨ç‡çš„æ–°çªç ´ï¼šChain-of-Zoom', 'desc': 'Chain-of-Zoom (CoZ) æ˜¯ä¸€ç§å¢å¼ºå•å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä½¿ç”¨è‡ªå›å½’çš„ä¸­é—´å°ºåº¦çŠ¶æ€é“¾å’Œå¤šå°ºåº¦æ„ŸçŸ¥æç¤ºï¼Œæ¥å®ç°é«˜è´¨é‡çš„æç«¯æ”¾å¤§ã€‚CoZ å°†è¶…åˆ†è¾¨ç‡ä»»åŠ¡åˆ†è§£ä¸ºå¯å¤„ç†çš„å­é—®é¢˜ï¼Œå…è®¸åœ¨ä¸é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹é‡å¤ä½¿ç”¨åŸºç¡€è¶…åˆ†è¾¨ç‡æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ CoZ çš„æ ‡å‡† 4 å€æ‰©æ•£è¶…åˆ†è¾¨ç‡æ¨¡å‹å¯ä»¥å®ç°è¶…è¿‡ 256 å€çš„æ”¾å¤§ï¼Œä¸”ä¿æŒé«˜æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22232', 'title': 'Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models', 'url': 'https://huggingface.co/papers/2505.22232', 'abstract': "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.  \t\t\t\t\tAI-generated summary \t\t\t\t High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.", 'score': 14, 'issue_id': 4020, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '77da366f396f2728', 'authors': ['Mehdi Ali', 'Manuel Brack', 'Max LÃ¼bbering', 'Elias Wendt', 'Abbas Goher Khan', 'Richard Rutmann', 'Alex Jude', 'Maurice Kraus', 'Alexander Arno Weber', 'Felix Stollenwerk', 'David KaczÃ©r', 'Florian Mai', 'Lucie Flek', 'Rafet Sifa', 'Nicolas Flores-Herr', 'Joachim KÃ¶hler', 'Patrick Schramowski', 'Michael Fromm', 'Kristian Kersting'], 'affiliations': ['AI Sweden', 'Computer Science Department, TU Darmstadt', 'DFKI SAINT', 'Fraunhofer IAIS', 'Hessian AI', 'Lamarr Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.22232.jpg', 'data': {'categories': ['#multilingual', '#data', '#transfer_learning', '#open_source', '#low_resource', '#dataset'], 'emoji': 'ğŸŒ', 'ru': {'title': 'JQL: ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'JQL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞŸĞ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, JQL Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ½Ğ° 35 ÑĞ·Ñ‹ĞºĞ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'JQL: Revolutionizing Multilingual Data Curation for Better AI Models', 'desc': 'This paper presents JQL, a novel method for curating high-quality multilingual training data using pretrained multilingual embeddings. JQL improves upon traditional heuristic methods by systematically selecting diverse datasets, which enhances the performance of downstream language models. The approach reduces computational costs while maintaining robust performance across various languages, including those not seen during training. Empirical evaluations show that JQL significantly outperforms existing methods, leading to better model training and higher data retention rates.'}, 'zh': {'title': 'JQLï¼šé«˜æ•ˆç­–åˆ’å¤šè¯­è¨€è®­ç»ƒæ•°æ®çš„ç³»ç»ŸåŒ–æ–¹æ³•', 'desc': 'JQLæ˜¯ä¸€ç§ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼Œç”¨äºé«˜æ•ˆåœ°ç­–åˆ’å¤šè¯­è¨€è®­ç»ƒæ•°æ®ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè¯­è¨€åµŒå…¥ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„å¯å‘å¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡ä¸‹å‡å°‘è®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶æé«˜ä¸‹æ¸¸æ¨¡å‹è®­ç»ƒçš„è´¨é‡ã€‚JQLé€šè¿‡è½»é‡çº§çš„æ³¨é‡Šå™¨æç‚¼å¤§å‹è¯­è¨€æ¨¡å‹çš„æ³¨é‡Šèƒ½åŠ›ï¼Œå±•ç°å‡ºå¼ºå¤§çš„å¤šè¯­è¨€å’Œè·¨è¯­è¨€æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¤šè¯­è¨€æ•°æ®ç­–åˆ’æä¾›äº†å®ç”¨çš„è§è§£å’Œå®è´µçš„èµ„æºï¼Œæå‡äº†å¤šè¯­è¨€æ•°æ®é›†å¼€å‘çš„æ ‡å‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21887', 'title': 'SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem', 'url': 'https://huggingface.co/papers/2505.21887', 'abstract': 'SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.  \t\t\t\t\tAI-generated summary \t\t\t\t Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.', 'score': 14, 'issue_id': 4016, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'ea6c5f0bb081e773', 'authors': ['Ahmed Heakl', 'Yahia Salaheldin Shaaban', 'Martin Takac', 'Salem Lahlou', 'Zangir Iklassov'], 'affiliations': ['MBZUAI, Abu Dhabi, UAE'], 'pdf_title_img': 'assets/pdf/title_img/2505.21887.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#rl', '#benchmark'], 'emoji': 'ğŸšš', 'ru': {'title': 'Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'SVRPBench Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ² Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´ÑĞºĞ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 500 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ĞµĞ² Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ¾ 1000, ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ Ğ°Ğ²Ğ°Ñ€Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ», Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº POMO Ğ¸ AM, ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 20% Ğ¿Ñ€Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ. SVRPBench Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµÑˆĞ°Ñ‚ĞµĞ»Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‚ÑÑ Ğ·Ğ° Ğ¿Ñ€ĞµĞ´ĞµĞ»Ñ‹ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğº Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Revolutionizing Vehicle Routing: Benchmarking Under Real-World Uncertainty', 'desc': 'SVRPBench is a new benchmark designed to evaluate vehicle routing algorithms under uncertain urban conditions. It simulates realistic scenarios with factors like traffic congestion, delays, and accidents, providing over 500 instances with up to 1000 customers. The study shows that advanced reinforcement learning (RL) solvers struggle with over 20% performance degradation when faced with real-world uncertainties, while traditional methods perform better. By releasing this dataset and evaluation suite, SVRPBench encourages researchers to develop more robust algorithms that can handle unpredictable environments.'}, 'zh': {'title': 'åº”å¯¹ä¸ç¡®å®šæ€§çš„è½¦è¾†è°ƒåº¦æ–°åŸºå‡†', 'desc': 'SVRPBenchæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œä¸“æ³¨äºä¸ç¡®å®šæ¡ä»¶ä¸‹çš„è½¦è¾†è°ƒåº¦ï¼Œæ¨¡æ‹Ÿç°å®åŸå¸‚ç¯å¢ƒï¼Œå¹¶çªæ˜¾å½“å‰æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨çš„å±€é™æ€§ã€‚è¯¥åŸºå‡†æ¶µç›–è¶…è¿‡500ä¸ªå®ä¾‹ï¼Œæœ€å¤šå¯å®¹çº³1000ä¸ªå®¢æˆ·ï¼Œæ¨¡æ‹Ÿäº†æ—¶é—´ä¾èµ–çš„æ‹¥å µã€å¯¹æ•°å»¶è¿Ÿã€æ¦‚ç‡æ€§äº‹æ•…ç­‰çœŸå®äº¤ä»˜æ¡ä»¶ã€‚é€šè¿‡åŸºå‡†æµ‹è¯•å‘ç°ï¼ŒåƒPOMOå’ŒAMè¿™æ ·çš„æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ æ±‚è§£å™¨åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹æ€§èƒ½ä¸‹é™è¶…è¿‡20%ï¼Œè€Œç»å…¸å’Œå…ƒå¯å‘å¼æ–¹æ³•åˆ™ä¿æŒç¨³å¥ã€‚SVRPBenchæ—¨åœ¨æ¨åŠ¨ç ”ç©¶ç¤¾åŒºè®¾è®¡èƒ½å¤Ÿè¶…è¶Šåˆæˆå‡è®¾å¹¶é€‚åº”ç°å®ä¸–ç•Œä¸ç¡®å®šæ€§çš„æ±‚è§£å™¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19075', 'title': 'Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs', 'url': 'https://huggingface.co/papers/2505.19075', 'abstract': 'UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have demonstrated remarkable general capabilities, but enhancing skills such as reasoning often demands substantial computational resources and may compromise their generalization. While Parameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious alternative, they typically requires retraining for each LLM backbone due to architectural dependencies. To address these challenges, here we propose Universal Reasoner (UniR) - a single, lightweight, composable, and plug-and-play reasoning module that can be used with any frozen LLM to endow it with specialized reasoning capabilities. Specifically, UniR decomposes the reward into a standalone reasoning module that is trained independently using predefined rewards, effectively translating trajectory-level signals into token-level guidance. Once trained, UniR can be combined with any frozen LLM at inference time by simply adding its output logits to those of the LLM backbone. This additive structure naturally enables modular composition: multiple UniR modules trained for different tasks can be jointly applied by summing their logits, enabling complex reasoning via composition. Experimental results on mathematical reasoning and machine translation tasks show that UniR significantly outperforms existing baseline fine-tuning methods using the Llama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong generalization: reasoning modules trained on smaller models effectively guide much larger LLMs. This makes UniR a cost-efficient, adaptable, and robust solution for enhancing reasoning in LLMs without compromising their core capabilities. Code is open-sourced at https://github.com/hangeol/UniR', 'score': 14, 'issue_id': 4018, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'afa75bef28fdbfdb', 'authors': ['Jaemin Kim', 'Hangeol Chang', 'Hyunmin Hwang', 'Choonghan Kim', 'Jong Chul Ye'], 'affiliations': ['Korea Advanced Institute of Science and Technology (KAIST)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19075.jpg', 'data': {'categories': ['#open_source', '#training', '#architecture', '#reasoning', '#rlhf'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UniR: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'UniR - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹. UniR Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ»ÑĞ±Ğ¾Ğ¹ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ LLM, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ ĞµĞ³Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ñ‹ Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‚Ğ°Ğ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ UniR Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing LLMs with Modular Reasoning: The Power of UniR', 'desc': 'The paper introduces UniR, a lightweight reasoning module designed to enhance Large Language Models (LLMs) with specialized reasoning skills while minimizing computational costs. Unlike traditional methods that require extensive retraining for each LLM, UniR operates as a plug-and-play module that can be added to any frozen LLM. It achieves this by independently training a reasoning module that translates high-level rewards into actionable guidance for the LLM. Experimental results demonstrate that UniR not only improves performance on tasks like mathematical reasoning and machine translation but also exhibits strong generalization capabilities across different model sizes.'}, 'zh': {'title': 'UniRï¼šè½»é‡çº§æ¨ç†æ¨¡å—ï¼Œæå‡LLMæ¨ç†èƒ½åŠ›', 'desc': 'UniRæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ¨ç†æ¨¡å—ï¼Œé€šè¿‡æ¨¡å—åŒ–ç»„åˆå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œæå‡æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚å®ƒå°†å¥–åŠ±åˆ†è§£ä¸ºç‹¬ç«‹çš„æ¨ç†æ¨¡å—ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„å¥–åŠ±è¿›è¡Œç‹¬ç«‹è®­ç»ƒï¼Œä»è€Œæœ‰æ•ˆåœ°å°†è½¨è¿¹çº§ä¿¡å·è½¬åŒ–ä¸ºæ ‡è®°çº§æŒ‡å¯¼ã€‚UniRå¯ä»¥ä¸ä»»ä½•å†»ç»“çš„LLMç»“åˆä½¿ç”¨ï¼Œåªéœ€åœ¨æ¨ç†æ—¶å°†å…¶è¾“å‡ºä¸LLMçš„è¾“å‡ºç›¸åŠ ï¼Œæ”¯æŒæ¨¡å—åŒ–ç»„åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUniRåœ¨æ•°å­¦æ¨ç†å’Œæœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„å¾®è°ƒæ–¹æ³•ï¼Œä¸”åœ¨å°æ¨¡å‹ä¸Šè®­ç»ƒçš„æ¨ç†æ¨¡å—èƒ½å¤Ÿæœ‰æ•ˆæŒ‡å¯¼æ›´å¤§çš„LLMã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22129', 'title': 'What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?', 'url': 'https://huggingface.co/papers/2505.22129', 'abstract': 'Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion, has stimulated research to adapt them to 360-degree panorama generation. Prior work has demonstrated the feasibility of using conventional low-rank adaptation techniques on pre-trained diffusion models to generate panoramic images. However, the substantial domain gap between perspective and panoramic images raises questions about the underlying mechanisms enabling this empirical success. We hypothesize and examine that the trainable counterparts exhibit distinct behaviors when fine-tuned on panoramic data, and such an adaptation conceals some intrinsic mechanism to leverage the prior knowledge within the pre-trained diffusion models. Our analysis reveals the following: 1) the query and key matrices in the attention modules are responsible for common information that can be shared between the panoramic and perspective domains, thus are less relevant to panorama generation; and 2) the value and output weight matrices specialize in adapting pre-trained knowledge to the panoramic domain, playing a more critical role during fine-tuning for panorama generation. We empirically verify these insights by introducing a simple framework called UniPano, with the objective of establishing an elegant baseline for future research. UniPano not only outperforms existing methods but also significantly reduces memory usage and training time compared to prior dual-branch approaches, making it scalable for end-to-end panorama generation with higher resolution. The code will be released.', 'score': 13, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '1148d530d0e60180', 'authors': ['Jinhong Ni', 'Chang-Bin Zhang', 'Qiang Zhang', 'Jing Zhang'], 'affiliations': ['Australian National University', 'Beijing Innovation Center of Humanoid Robotics', 'Hong Kong University of Science and Technology (Guangzhou)', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.22129.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#diffusion', '#cv'], 'emoji': 'ï¿½panorama', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ»Ğ¸ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ† Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ÑÑ‚ Ğ·Ğ° Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼ÑƒÑ ĞºĞ°Ğº Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğº Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ½Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğº Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ² Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº UniPano, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'UniPano: Efficient Panoramic Image Generation with Diffusion Models', 'desc': 'This paper investigates how to improve the generation of panoramic images using diffusion models, which are typically used for standard images. It identifies the specific roles of different components in the attention mechanism of these models, particularly how they adapt to the unique characteristics of panoramic data. The authors introduce a new framework called UniPano, which enhances efficiency by reducing memory usage and training time while achieving better performance than existing methods. This work aims to provide a solid foundation for future advancements in panoramic image generation.'}, 'zh': {'title': 'UniPanoï¼šé«˜æ•ˆå…¨æ™¯å›¾åƒç”Ÿæˆçš„æ–°åŸºçº¿', 'desc': 'æœ¬æ–‡åˆ†æäº†å¾®è°ƒæ‰©æ•£æ¨¡å‹åœ¨å…¨æ™¯å›¾åƒç”Ÿæˆä¸­çš„ä½œç”¨ï¼Œæ­ç¤ºäº†æ³¨æ„åŠ›æ¨¡å—çŸ©é˜µçš„ä¸åŒè§’è‰²ï¼Œå¹¶å¼•å…¥äº†UniPanoï¼Œä¸€ä¸ªå†…å­˜é«˜æ•ˆä¸”é€Ÿåº¦å¢å¼ºçš„åŸºçº¿æ¡†æ¶ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒæŸ¥è¯¢å’Œé”®çŸ©é˜µåœ¨å…¨æ™¯å’Œé€è§†é¢†åŸŸä¹‹é—´å…±äº«ä¿¡æ¯ï¼Œè€Œå€¼å’Œè¾“å‡ºæƒé‡çŸ©é˜µåˆ™ä¸“æ³¨äºå°†é¢„è®­ç»ƒçŸ¥è¯†é€‚åº”äºå…¨æ™¯é¢†åŸŸã€‚é€šè¿‡è¿™äº›å‘ç°ï¼ŒUniPanoåœ¨å…¨æ™¯å›¾åƒç”Ÿæˆä¸­è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶æ˜¾è‘—å‡å°‘äº†å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ—¶é—´ã€‚è¯¥æ¡†æ¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªä¼˜é›…çš„åŸºçº¿ï¼Œå¹¶å°†å‘å¸ƒç›¸å…³ä»£ç ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22648', 'title': 'WebDancer: Towards Autonomous Information Seeking Agency', 'url': 'https://huggingface.co/papers/2505.22648', 'abstract': 'Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.', 'score': 12, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '906f55ecff56c6ba', 'authors': ['Jialong Wu', 'Baixuan Li', 'Runnan Fang', 'Wenbiao Yin', 'Liwen Zhang', 'Zhengwei Tao', 'Dingchu Zhang', 'Zekun Xi', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.22648.jpg', 'data': {'categories': ['#agi', '#reasoning', '#agents', '#benchmark', '#training'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞµÑ‚Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰ÑƒÑ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ°, Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¾Ğ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ° WebDancer, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ReAct. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… GAIA Ğ¸ WebWalkerQA Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ WebDancer Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Agents for Autonomous Information Seeking', 'desc': "This paper introduces a new framework for creating intelligent agents that can autonomously seek information and perform multi-step reasoning. The proposed method involves four stages: constructing a dataset for browsing, sampling trajectories for learning, fine-tuning the model to improve initial performance, and applying reinforcement learning to enhance the agent's ability to generalize. The authors demonstrate their approach through a web agent called WebDancer, which shows strong performance on challenging benchmarks like GAIA and WebWalkerQA. The findings provide insights into training strategies that can lead to the development of more advanced agentic systems."}, 'zh': {'title': 'æ„å»ºæ™ºèƒ½ä¿¡æ¯æ£€ç´¢ä»£ç†çš„å…¨æ–°æ¡†æ¶', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¦‚ä½•æ„å»ºè‡ªä¸»çš„ä¿¡æ¯æ£€ç´¢ä»£ç†ç³»ç»Ÿï¼Œä»¥è§£å†³å¤æ‚çš„ç°å®é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ä»£ç†ä¿¡æ¯æ£€ç´¢æ¡†æ¶ï¼ŒåŒ…å«æ•°æ®æ„å»ºã€è½¨è¿¹é‡‡æ ·ã€ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ å››ä¸ªå…³é”®é˜¶æ®µã€‚é€šè¿‡åœ¨WebDancerä¸Šå®æ–½è¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨GAIAå’ŒWebWalkerQAç­‰ä¿¡æ¯æ£€ç´¢åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå¼€å‘æ›´å¼ºå¤§çš„ä»£ç†æ¨¡å‹æä¾›äº†ç³»ç»ŸåŒ–çš„è·¯å¾„å’Œæœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19187', 'title': 'LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling', 'url': 'https://huggingface.co/papers/2505.19187', 'abstract': 'A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to -41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.', 'score': 11, 'issue_id': 4013, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': '072196a147db8410', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Ruifeng Yuan', 'Chunpu Xu', 'Kaishuai Xu', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19187.jpg', 'data': {'categories': ['#optimization', '#inference', '#reasoning', '#benchmark', '#training'], 'emoji': 'âœ‚ï¸', 'ru': {'title': 'PIR: ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¼Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ', 'desc': 'PIR - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑˆĞ°Ğ³Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿ÑƒÑ‚ĞµĞ¼ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ğ»Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ PIR, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Reasoning for Efficient AI Performance', 'desc': 'The paper introduces a framework called PIR (Perplexity-based Importance Refinement) that enhances the reasoning capabilities of large language models (LLMs) by focusing on the importance of reasoning steps. It identifies and prunes low-importance functional elements from reasoning chains, which helps in reducing computational demands while maintaining essential progressive reasoning. By optimizing training data, models fine-tuned with PIR show improved accuracy and efficiency, achieving better performance on reasoning benchmarks. This approach allows for more concise reasoning outputs, making LLMs more practical for real-world applications where speed and resource usage are critical.'}, 'zh': {'title': 'ä¼˜åŒ–æ¨ç†é“¾ï¼Œæå‡æ¨¡å‹æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPIRï¼ˆåŸºäºå›°æƒ‘åº¦çš„é‡è¦æ€§ç²¾ç‚¼ï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†æ­¥éª¤ã€‚é€šè¿‡è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤å¯¹ç­”æ¡ˆé¢„æµ‹ä¿¡å¿ƒçš„å½±å“ï¼ŒPIRèƒ½å¤Ÿè¯†åˆ«å¹¶å‰ªé™¤ä½é‡è¦æ€§çš„åŠŸèƒ½æ€§å…ƒç´ ï¼Œä»è€Œç®€åŒ–æ¨ç†é“¾ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŒæ ¸å¿ƒè§£å†³è·¯å¾„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œå‡å°‘äº†å†—ä½™ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚ç»è¿‡PIRä¼˜åŒ–çš„æ•°æ®è®­ç»ƒçš„æ¨¡å‹åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ›´ä¼˜çš„æ€§èƒ½ï¼Œå‡å°‘äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17663', 'title': 'Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States', 'url': 'https://huggingface.co/papers/2505.17663', 'abstract': "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present DynToM, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.", 'score': 11, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '01d30f0f90863bd8', 'authors': ['Yang Xiao', 'Jiashuo Wang', 'Qiancheng Xu', 'Changhe Song', 'Chunpu Xu', 'Yi Cheng', 'Wenjie Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2505.17663.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ñ‚ÑÑ‚Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº DynToM Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1100 ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ 5500 ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼Ğ¸ Ğ¸ 78100 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 10 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¸Ñ… ÑÑ€ĞµĞ´Ğ½ÑÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 44.7% Ğ½Ğ¸Ğ¶Ğµ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ LLM Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¿ÑĞ¸Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹.'}, 'en': {'title': "Evaluating LLMs' Understanding of Dynamic Mental States", 'desc': "The DynToM benchmark assesses how well Large Language Models (LLMs) can understand and track changes in mental states over time, which is essential for effective human-AI interaction. Unlike previous benchmarks that only evaluate static mental states, DynToM focuses on the dynamic progression of these states in social contexts. The study involved creating a large dataset of scenarios and questions to rigorously test LLMs' Theory of Mind capabilities. Results showed that LLMs lag behind human performance by 44.7%, particularly struggling with the complexities of shifting mental states."}, 'zh': {'title': 'è¯„ä¼°LLMsçš„åŠ¨æ€å¿ƒç†çŠ¶æ€ç†è§£èƒ½åŠ›', 'desc': 'DynToMåŸºå‡†æµ‹è¯•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è·Ÿè¸ªå’Œç†è§£å¿ƒç†çŠ¶æ€çš„æ—¶é—´è¿›å±•æ–¹é¢çš„èƒ½åŠ›ï¼Œæ˜¾ç¤ºå‡ºä¸äººç±»è¡¨ç°ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚ç°æœ‰çš„åŸºå‡†ä¸»è¦å…³æ³¨é™æ€å¿ƒç†çŠ¶æ€çš„å¿«ç…§ï¼Œå¿½è§†äº†çœŸå®ç¤¾äº¤äº’åŠ¨ä¸­å¿ƒç†çŠ¶æ€çš„åŠ¨æ€æ¼”å˜ã€‚æˆ‘ä»¬æå‡ºäº†DynToMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°LLMsåœ¨ç›¸äº’å…³è”åœºæ™¯ä¸­ç†è§£å’Œè·Ÿè¸ªå¿ƒç†çŠ¶æ€æ—¶é—´è¿›å±•çš„èƒ½åŠ›ã€‚é€šè¿‡ç³»ç»Ÿçš„å››æ­¥æ¡†æ¶ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†1100ä¸ªç¤¾äº¤èƒŒæ™¯ï¼Œæ¶µç›–5500ä¸ªåœºæ™¯å’Œ78100ä¸ªé—®é¢˜ï¼ŒéªŒè¯äº†å…¶ç°å®æ€§å’Œè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22202', 'title': "Let's Predict Sentence by Sentence", 'url': 'https://huggingface.co/papers/2505.22202', 'abstract': 'Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.  \t\t\t\t\tAI-generated summary \t\t\t\t Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces.', 'score': 10, 'issue_id': 4017, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '2d6858513b9b0682', 'authors': ['Hyeonbin Hwang', 'Byeongguk Jeon', 'Seungone Kim', 'Jiyeon Kim', 'Hoyeon Chang', 'Sohee Yang', 'Seungpil Won', 'Dohaeng Lee', 'Youbin Ahn', 'Minjoon Seo'], 'affiliations': ['Carnegie Mellon University', 'KAIST', 'LG AI Research', 'University College London'], 'pdf_title_img': 'assets/pdf/title_img/2505.22202.jpg', 'data': {'categories': ['#architecture', '#inference', '#interpretability', '#data', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¯Ğ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ¼Ñ‹ÑĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Chain-of-Thought, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ²Ğ´Ğ²Ğ¾Ğµ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ² Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Empowering Language Models for Abstract Reasoning in Sentence Space', 'desc': 'This paper explores how pretrained language models (LMs) can be adapted to reason over higher-level abstractions like sentences instead of just raw tokens. The authors propose a framework that allows LMs to predict continuous embeddings of sentences, enhancing their reasoning capabilities. They investigate two types of embeddings: semantic embeddings that focus on meaning and contextual embeddings that capture anticipatory structures. The results show that contextual embeddings can achieve competitive performance with Chain-of-Thought reasoning while significantly reducing computational costs during inference.'}, 'zh': {'title': 'é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æŠ½è±¡æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å¦‚ä½•åœ¨å¥å­ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œè€Œä¸ä»…ä»…æ˜¯å¤„ç†åŸå§‹çš„æ ‡è®°åºåˆ—ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œä½¿å¾—é¢„è®­ç»ƒçš„æ ‡è®°çº§LMèƒ½å¤Ÿé€šè¿‡è‡ªå›å½’é¢„æµ‹ä¸‹ä¸€ä¸ªå¥å­çš„è¿ç»­åµŒå…¥æ¥é€‚åº”å¥å­ç©ºé—´ã€‚è®ºæ–‡ä¸­ä»‹ç»äº†ä¸¤ç§åµŒå…¥èŒƒå¼ï¼šè¯­ä¹‰åµŒå…¥å’Œä¸Šä¸‹æ–‡åµŒå…¥ï¼Œå¹¶åœ¨æ•°å­¦ã€é€»è¾‘ã€å¸¸è¯†å’Œè§„åˆ’ç­‰å››ä¸ªé¢†åŸŸè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨è¿ç»­æ¨ç†ä¸‹ï¼Œä¸Šä¸‹æ–‡åµŒå…¥çš„æ€§èƒ½ä¸é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç›¸å½“ï¼ŒåŒæ—¶æ¨ç†æ—¶é—´çš„FLOPså¹³å‡å‡å°‘äº†ä¸€åŠã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22019', 'title': 'VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.22019', 'abstract': "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at https://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.", 'score': 9, 'issue_id': 4017, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '913f87ab16364e21', 'authors': ['Qiuchen Wang', 'Ruixue Ding', 'Yu Zeng', 'Zehui Chen', 'Lin Chen', 'Shihang Wang', 'Pengjun Xie', 'Fei Huang', 'Feng Zhao'], 'affiliations': ['MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.22019.jpg', 'data': {'categories': ['#rl', '#rag', '#multimodal', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VRAG-RL: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² RAG Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'VRAG-RL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ… RAG. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ. VRAG-RL Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ RAG, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Enhancing Visual Reasoning in RAG with VRAG-RL', 'desc': "VRAG-RL is a reinforcement learning framework designed to improve the reasoning and visual information processing capabilities of Retrieval-Augmented Generation (RAG) methods. It addresses the limitations of traditional text-based approaches and current vision-based RAG methods by integrating visual perception tokens and creating specialized action spaces for better interaction with search engines. The framework allows Vision-Language Models (VLMs) to autonomously sample reasoning trajectories and optimize their performance through a tailored reward system that enhances query effectiveness. By focusing on visually rich inputs and refining the model's ability to articulate information needs, VRAG-RL aims to bridge the gap between user queries and relevant data retrieval."}, 'zh': {'title': 'æå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„VRAG-RLæ¡†æ¶', 'desc': 'VRAG-RLæ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æå‡RAGæ–¹æ³•åœ¨æ¨ç†å’Œè§†è§‰ä¿¡æ¯å¤„ç†æ–¹é¢çš„èƒ½åŠ›ã€‚å®ƒé€šè¿‡æ•´åˆè§†è§‰æ„ŸçŸ¥æ ‡è®°å’Œé‡‡ç”¨ä¸“é—¨çš„åŠ¨ä½œç©ºé—´ä¸å¥–åŠ±æœºåˆ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–‡æœ¬æ–¹æ³•æ— æ³•å¤„ç†è§†è§‰ä¿¡æ¯çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶å…è®¸è§†è§‰è¯­è¨€æ¨¡å‹ä¸æœç´¢å¼•æ“äº’åŠ¨ï¼Œè‡ªä¸»é‡‡æ ·æ¨ç†è½¨è¿¹ï¼Œå¹¶åŸºäºè¿™äº›æ ·æœ¬è¿›è¡ŒæŒç»­ä¼˜åŒ–ã€‚VRAG-RLé€šè¿‡å®šä¹‰é€‚åˆè§†è§‰ä¸°å¯Œè¾“å…¥çš„åŠ¨ä½œç©ºé—´å’Œæœ‰æ•ˆçš„å¥–åŠ±æœºåˆ¶ï¼Œä¼˜åŒ–äº†RAGä»»åŠ¡ä¸­çš„æ¨¡å‹è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20779', 'title': 'CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature', 'url': 'https://huggingface.co/papers/2505.20779', 'abstract': 'A large-scale knowledge base of recombination examples is built from scientific paper abstracts using an LLM-based extraction model to analyze and inspire new creative directions in AI.  \t\t\t\t\tAI-generated summary \t\t\t\t A hallmark of human innovation is the process of recombination -- creating original ideas by integrating elements of existing mechanisms and concepts. In this work, we automatically mine the scientific literature and build CHIMERA: a large-scale knowledge base (KB) of recombination examples. CHIMERA can be used to empirically explore at scale how scientists recombine concepts and take inspiration from different areas, or to train supervised machine learning models that learn to predict new creative cross-domain directions. To build this KB, we present a novel information extraction task of extracting recombination from scientific paper abstracts, collect a high-quality corpus of hundreds of manually annotated abstracts, and use it to train an LLM-based extraction model. The model is applied to a large corpus of papers in the AI domain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to explore the properties of recombination in different subareas of AI. Finally, we train a scientific hypothesis generation model using the KB, which predicts new recombination directions that real-world researchers find inspiring. Our data and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA', 'score': 8, 'issue_id': 4023, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'f73a43d6ad307bdc', 'authors': ['Noy Sternlicht', 'Tom Hope'], 'affiliations': ['School of Computer Science and Engineering, The Hebrew University of Jerusalem', 'The Allen Institute for AI (AI2)'], 'pdf_title_img': 'assets/pdf/title_img/2505.20779.jpg', 'data': {'categories': ['#science', '#multimodal', '#data', '#dataset', '#transfer_learning'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'CHIMERA: Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ CHIMERA - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ‘Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 28 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ°Ğ»Ğ¸Ğ· CHIMERA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ğ˜Ğ˜. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'CHIMERA: Unleashing Innovation through Recombination in AI', 'desc': 'This paper presents CHIMERA, a large-scale knowledge base that captures examples of recombination from scientific paper abstracts using a large language model (LLM) for information extraction. The process of recombination is essential for innovation, as it involves merging existing ideas to create new concepts. By analyzing a vast corpus of AI literature, the authors built a dataset of over 28,000 recombination examples, which can be used to understand how ideas are integrated across different domains. Additionally, they developed a model that generates scientific hypotheses based on this knowledge base, helping researchers identify novel directions for exploration.'}, 'zh': {'title': 'é‡ç»„åˆ›æ–°ï¼šä»ç§‘å­¦æ–‡çŒ®ä¸­æå–çµæ„Ÿ', 'desc': 'æœ¬ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåä¸ºCHIMERAçš„å¤§è§„æ¨¡çŸ¥è¯†åº“ï¼Œä¸“æ³¨äºä»ç§‘å­¦è®ºæ–‡æ‘˜è¦ä¸­æå–é‡ç»„ç¤ºä¾‹ã€‚é€šè¿‡ä½¿ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æå–æ¨¡å‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè‡ªåŠ¨æŒ–æ˜ç§‘å­¦æ–‡çŒ®ï¼Œåˆ†æç§‘å­¦å®¶å¦‚ä½•å°†ä¸åŒé¢†åŸŸçš„æ¦‚å¿µè¿›è¡Œé‡ç»„ã€‚è¯¥çŸ¥è¯†åº“åŒ…å«è¶…è¿‡28,000ä¸ªé‡ç»„ç¤ºä¾‹ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹æ–°çš„è·¨é¢†åŸŸåˆ›æ„æ–¹å‘ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨è¯¥çŸ¥è¯†åº“è®­ç»ƒäº†ä¸€ä¸ªç§‘å­¦å‡è®¾ç”Ÿæˆæ¨¡å‹ï¼Œä»¥æ¿€å‘ç ”ç©¶äººå‘˜çš„çµæ„Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22613', 'title': 'RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction', 'url': 'https://huggingface.co/papers/2505.22613', 'abstract': 'A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.  \t\t\t\t\tAI-generated summary \t\t\t\t Image recaptioning is widely used to generate training datasets with enhanced quality for various multimodal tasks. Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. To address these limitations, we propose RICO, a novel framework that refines captions through visual reconstruction. Specifically, we leverage a text-to-image model to reconstruct a caption into a reference image, and prompt an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. This process is performed iteratively, further progressively promoting the generation of more faithful and comprehensive descriptions. To mitigate the additional computational cost induced by the iterative process, we introduce RICO-Flash, which learns to generate captions like RICO using DPO. Extensive experiments demonstrate that our approach significantly improves caption accuracy and completeness, outperforms most baselines by approximately 10% on both CapsBench and CompreCap. Code released at https://github.com/wangyuchi369/RICO.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '0f01107a0f4b4350', 'authors': ['Yuchi Wang', 'Yishuo Cai', 'Shuhuai Ren', 'Sihan Yang', 'Linli Yao', 'Yuanxin Liu', 'Yuanxing Zhang', 'Pengfei Wan', 'Xu Sun'], 'affiliations': ['Central South University', 'Kuaishou Technology', 'National Key Laboratory for Multimedia Information Processing, Peking University', 'Xian JiaoTong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22613.jpg', 'data': {'categories': ['#optimization', '#dataset', '#training', '#open_source', '#hallucinations', '#rlhf', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ', 'desc': 'RICO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹. RICO-Flash Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DPO (Direct Preference Optimization). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ½Ğ° 10% Ğ² Ñ‚ĞµÑÑ‚Ğ°Ñ… CapsBench Ğ¸ CompreCap.'}, 'en': {'title': 'RICO: Refining Image Captions with Visual Reconstruction', 'desc': 'The paper introduces RICO, an innovative framework designed to enhance the accuracy of image captions by utilizing visual reconstruction techniques. It employs a text-to-image model to create a reference image from a caption, allowing a multimodal large language model (MLLM) to identify and correct discrepancies between the original and reconstructed images. This iterative process helps generate more accurate and detailed captions by progressively refining them. To improve efficiency, the authors present RICO-Flash, which uses DPO to streamline the caption generation process while maintaining high quality.'}, 'zh': {'title': 'RICOï¼šæå‡å›¾åƒæè¿°å‡†ç¡®æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¿­ä»£æ¡†æ¶RICOï¼Œé€šè¿‡è§†è§‰é‡å»ºå’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¥æé«˜å›¾åƒæè¿°çš„å‡†ç¡®æ€§ã€‚RICOåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹å°†æè¿°é‡å»ºä¸ºå‚è€ƒå›¾åƒï¼Œå¹¶é€šè¿‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰è¯†åˆ«åŸå§‹å›¾åƒä¸é‡å»ºå›¾åƒä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œé€æ­¥æ”¹è¿›æè¿°ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†RICO-Flashï¼Œåˆ©ç”¨DPOæŠ€æœ¯æ¥ç”Ÿæˆæè¿°ï¼Œå‡å°‘è®¡ç®—æˆæœ¬ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨CapsBenchå’ŒCompreCapæ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜äº†æè¿°çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œè¶…è¶Šäº†å¤§å¤šæ•°åŸºçº¿æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22525', 'title': 'Thinking with Generated Images', 'url': 'https://huggingface.co/papers/2505.22525', 'abstract': 'Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '0d852a3cf6a78a01', 'authors': ['Ethan Chern', 'Zhulin Hu', 'Steffi Chern', 'Siqi Kou', 'Jiadi Su', 'Yan Ma', 'Zhijie Deng', 'Pengfei Liu'], 'affiliations': ['Fudan University', 'Generative AI Research Lab (GAIR)', 'SII', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22525.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#cv', '#reasoning', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ 'ĞœÑ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸', ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ (LMM) Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑˆĞ°Ğ³Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸Ğ¼ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·Ñ‹ Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¸Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ¾Ñ‚ Ğ±Ğ¸Ğ¾Ñ…Ğ¸Ğ¼Ğ¸Ğ¸ Ğ´Ğ¾ ÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸."}, 'en': {'title': 'Empowering AI with Visual Imagination and Self-Critique', 'desc': "The paper introduces 'Thinking with Generated Images', a new approach that enhances large multimodal models (LMMs) by allowing them to generate and evaluate intermediate visual steps during reasoning. This method enables models to create visual representations spontaneously, rather than relying solely on fixed images or text-based reasoning. By breaking down complex visual tasks into smaller, manageable components and allowing for self-critique, the models can refine their outputs iteratively. The results show significant improvements in performance on visual reasoning tasks, demonstrating the potential for AI to mimic human-like visual thinking and creativity."}, 'zh': {'title': 'ç”Ÿæˆå›¾åƒæ€ç»´ï¼šæå‡è§†è§‰æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ€ç»´æ–¹å¼â€”â€”ç”Ÿæˆå›¾åƒæ€ç»´ï¼Œæ—¨åœ¨æå‡å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨è§†è§‰æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚é€šè¿‡ç”Ÿæˆä¸­é—´è§†è§‰æ­¥éª¤ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨æ–‡æœ¬å’Œè§†è§‰ä¹‹é—´è‡ªå‘åœ°è¿›è¡Œæ€è€ƒï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªæœºåˆ¶ï¼šä¸€æ˜¯é€šè¿‡ä¸­é—´è§†è§‰å­ç›®æ ‡åˆ†è§£å¤æ‚ä»»åŠ¡ï¼ŒäºŒæ˜¯é€šè¿‡è‡ªæˆ‘æ‰¹åˆ¤åˆ†æåˆæ­¥è§†è§‰å‡è®¾çš„ä¸è¶³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤„ç†å¤æ‚å¤šå¯¹è±¡åœºæ™¯æ—¶ï¼Œç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æå‡ï¼Œæœ€é«˜å¯è¾¾50%çš„ç›¸å¯¹æ”¹å–„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21876', 'title': 'EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance', 'url': 'https://huggingface.co/papers/2505.21876', 'abstract': 'EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent approaches on 3D camera control in video diffusion models (VDMs) often create anchor videos to guide diffusion models as a structured prior by rendering from estimated point clouds following annotated camera trajectories. However, errors inherent in point cloud estimation often lead to inaccurate anchor videos. Moreover, the requirement for extensive camera trajectory annotations further increases resource demands. To address these limitations, we introduce EPiC, an efficient and precise camera control learning framework that automatically constructs high-quality anchor videos without expensive camera trajectory annotations. Concretely, we create highly precise anchor videos for training by masking source videos based on first-frame visibility. This approach ensures high alignment, eliminates the need for camera trajectory annotations, and thus can be readily applied to any in-the-wild video to generate image-to-video (I2V) training pairs. Furthermore, we introduce Anchor-ControlNet, a lightweight conditioning module that integrates anchor video guidance in visible regions to pretrained VDMs, with less than 1% of backbone model parameters. By combining the proposed anchor video data and ControlNet module, EPiC achieves efficient training with substantially fewer parameters, training steps, and less data, without requiring modifications to the diffusion model backbone typically needed to mitigate rendering misalignments. Although being trained on masking-based anchor videos, our method generalizes robustly to anchor videos made with point clouds during inference, enabling precise 3D-informed camera control. EPiC achieves SOTA performance on RealEstate10K and MiraData for I2V camera control task, demonstrating precise and robust camera control ability both quantitatively and qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to video-to-video scenarios.', 'score': 6, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '2085924b7cd22768', 'authors': ['Zun Wang', 'Jaemin Cho', 'Jialu Li', 'Han Lin', 'Jaehong Yoon', 'Yue Zhang', 'Mohit Bansal'], 'affiliations': ['UNC Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2505.21876.jpg', 'data': {'categories': ['#optimization', '#video', '#training', '#transfer_learning', '#diffusion', '#3d', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ±ĞµĞ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹', 'desc': 'EPiC - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. ĞĞ½ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ° Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ ControlNet. EPiC Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (I2V) Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ°Ñ…. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒÑÑ Ğº Ğ»ÑĞ±Ğ¾Ğ¼Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ°Ñ€ I2V.'}, 'en': {'title': 'Efficient 3D Camera Control with EPiC', 'desc': 'EPiC is a novel framework designed to enhance 3D camera control in video diffusion models (VDMs) by creating high-quality anchor videos without the need for extensive camera trajectory annotations. It utilizes first-frame visibility masking to ensure that the generated anchor videos are accurately aligned with the source videos, thus improving the training process. The framework incorporates a lightweight ControlNet module that integrates these anchor videos into existing VDMs, achieving state-of-the-art performance on image-to-video (I2V) tasks while minimizing resource requirements. Additionally, EPiC demonstrates strong generalization capabilities, performing well even in zero-shot video-to-video scenarios, making it a versatile tool for efficient camera control.'}, 'zh': {'title': 'EPiCï¼šé«˜æ•ˆçš„3Dç›¸æœºæ§åˆ¶æ–°æ¡†æ¶', 'desc': 'EPiCæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„3Dç›¸æœºæ§åˆ¶æ¡†æ¶ï¼Œä¸“ä¸ºè§†é¢‘æ‰©æ•£æ¨¡å‹è®¾è®¡ã€‚å®ƒé€šè¿‡ç¬¬ä¸€å¸§å¯è§æ€§æ©è”½è‡ªåŠ¨æ„å»ºé«˜è´¨é‡çš„é”šè§†é¢‘ï¼Œé¿å…äº†æ˜‚è´µçš„ç›¸æœºè½¨è¿¹æ³¨é‡Šéœ€æ±‚ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è½»é‡çº§çš„ControlNetæ¨¡å—ï¼Œèƒ½å¤Ÿåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å®ç°å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ä»»åŠ¡çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚EPiCåœ¨RealEstate10Kå’ŒMiraDataæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç°äº†å…¶ç²¾ç¡®å’Œç¨³å¥çš„ç›¸æœºæ§åˆ¶èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22523', 'title': 'PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image\n  Generative Models', 'url': 'https://huggingface.co/papers/2505.22523', 'abstract': 'Generating high-quality, multi-layer transparent images from text prompts can unlock a new level of creative control, allowing users to edit each layer as effortlessly as editing text outputs from LLMs. However, the development of multi-layer generative models lags behind that of conventional text-to-image models due to the absence of a large, high-quality corpus of multi-layer transparent data. In this paper, we address this fundamental challenge by: (i) releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro) dataset of 200K (20K) multilayer transparent images with accurate alpha mattes, (ii) introducing a trainingfree synthesis pipeline that generates such data on demand using off-the-shelf diffusion models, and (iii) delivering a strong, open-source multi-layer generation model, ART+, which matches the aesthetics of modern text-to-image generation models. The key technical contributions include: LayerFLUX, which excels at generating high-quality single transparent layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple LayerFLUX outputs into complete images, guided by human-annotated semantic layout. To ensure higher quality, we apply a rigorous filtering stage to remove artifacts and semantic mismatches, followed by human selection. Fine-tuning the state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which outperforms the original ART in 60% of head-to-head user study comparisons and even matches the visual quality of images generated by the FLUX.1-[dev] model. We anticipate that our work will establish a solid dataset foundation for the multi-layer transparent image generation task, enabling research and applications that require precise, editable, and visually compelling layered imagery.', 'score': 5, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '2f373283a5bfede3', 'authors': ['Junwen Chen', 'Heyang Jiang', 'Yanbin Wang', 'Keming Wu', 'Ji Li', 'Chao Zhang', 'Keiji Yanai', 'Dong Chen', 'Yuhui Yuan'], 'affiliations': ['Microsoft Research Asia'], 'pdf_title_img': 'assets/pdf/title_img/2505.22523.jpg', 'data': {'categories': ['#data', '#cv', '#diffusion', '#dataset', '#open_source', '#synthetic'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PrismLayers Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 200 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ»ÑŒÑ„Ğ°-Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ‚Ğ°ĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ğ±Ñ‹Ğ»Ğ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ART+, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ°Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unlocking Creative Control with Multi-Layer Transparent Image Generation', 'desc': 'This paper presents a novel approach to generating high-quality, multi-layer transparent images from text prompts, enhancing creative control for users. The authors introduce the PrismLayersPro dataset, which contains 200,000 multilayer transparent images with accurate alpha mattes, addressing the lack of quality data in this area. They also propose a training-free synthesis pipeline that utilizes existing diffusion models to create these images on demand. Additionally, the ART+ model is developed, which outperforms previous models in user studies and provides a foundation for future research in editable layered imagery.'}, 'zh': {'title': 'å¼€å¯å¤šå±‚é€æ˜å›¾åƒç”Ÿæˆçš„æ–°ç¯‡ç« ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”Ÿæˆé«˜è´¨é‡å¤šå±‚é€æ˜å›¾åƒçš„æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·åƒç¼–è¾‘æ–‡æœ¬ä¸€æ ·è½»æ¾ç¼–è¾‘æ¯ä¸€å±‚ã€‚æˆ‘ä»¬å‘å¸ƒäº†é¦–ä¸ªå¼€æ”¾çš„è¶…é«˜ä¿çœŸPrismLayersæ•°æ®é›†ï¼ŒåŒ…å«20ä¸‡å¼ å¸¦æœ‰å‡†ç¡®alphaé€šé“çš„å¤šå±‚é€æ˜å›¾åƒã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ— è®­ç»ƒåˆæˆç®¡é“ï¼Œåˆ©ç”¨ç°æˆçš„æ‰©æ•£æ¨¡å‹æŒ‰éœ€ç”Ÿæˆæ•°æ®ï¼Œå¹¶æä¾›äº†å¼€æºçš„å¤šå±‚ç”Ÿæˆæ¨¡å‹ART+ï¼Œå…¶ç¾å­¦ä¸ç°ä»£æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ç›¸åŒ¹é…ã€‚é€šè¿‡ä¸¥æ ¼çš„è¿‡æ»¤å’Œäººå·¥é€‰æ‹©ï¼Œæˆ‘ä»¬ç¡®ä¿ç”Ÿæˆçš„å›¾åƒè´¨é‡æ›´é«˜ï¼ŒART+åœ¨ç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°ä¼˜äºåŸå§‹ARTæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22338', 'title': 'Text2Grad: Reinforcement Learning from Natural Language Feedback', 'url': 'https://huggingface.co/papers/2505.22338', 'abstract': "Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad", 'score': 5, 'issue_id': 4013, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': '6a628958ae1efdcc', 'authors': ['Hanyang Wang', 'Lu Wang', 'Chaoyun Zhang', 'Tianjun Mao', 'Si Qin', 'Qingwei Lin', 'Saravan Rajmohan', 'Dongmei Zhang'], 'affiliations': ['Fudan University', 'Microsoft', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2505.22338.jpg', 'data': {'categories': ['#optimization', '#training', '#interpretability', '#rlhf'], 'emoji': 'ğŸ”', 'ru': {'title': 'Text2Grad: Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Text2Grad. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Text2Grad ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ², Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Text2Grad Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹.'}, 'en': {'title': 'Transforming Textual Feedback into Targeted Learning Signals', 'desc': "This paper presents Text2Grad, a novel approach in reinforcement learning from human feedback (RLHF) that enhances language model training by using detailed textual critiques. Unlike traditional methods that rely on simple scalar rewards, Text2Grad translates free-form feedback into specific gradient updates for model parameters, allowing for more precise adjustments. The method involves a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer to ensure that critiques are effectively aligned with the relevant parts of the model's output. The results show that Text2Grad outperforms existing RLHF techniques in various tasks, providing better performance metrics and improved interpretability of the model's learning process."}, 'zh': {'title': 'Text2Gradï¼šå°†æ–‡æœ¬åé¦ˆè½¬åŒ–ä¸ºæ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä½¿ç”¨ç²—ç•¥çš„æ ‡é‡å¥–åŠ±æ¥ä¼˜åŒ–è¯­è¨€æ¨¡å‹ï¼Œè¿™ç§æ–¹æ³•æ©ç›–äº†æˆåŠŸæˆ–å¤±è´¥çš„ç»†å¾®åŸå› ï¼Œå¯¼è‡´å­¦ä¹ è¿‡ç¨‹ç¼“æ…¢ä¸”ä¸é€æ˜ã€‚æœ€è¿‘çš„ç ”ç©¶é€šè¿‡æç¤ºæˆ–åæ€å¢å¼ºäº†RLä¸æ–‡æœ¬æ‰¹è¯„çš„ç»“åˆï¼Œæé«˜äº†å¯è§£é‡Šæ€§ï¼Œä½†æœªèƒ½æ”¹å˜æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬æå‡ºäº†Text2Gradï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå°†è‡ªç”±å½¢å¼çš„æ–‡æœ¬åé¦ˆè½¬åŒ–ä¸ºè·¨åº¦çº§æ¢¯åº¦ã€‚Text2Gradé€šè¿‡å°†äººç±»æˆ–ç¨‹åºåŒ–çš„æ‰¹è¯„ä¸ç›¸å…³çš„æ ‡è®°è·¨åº¦å¯¹é½ï¼Œç”Ÿæˆå¯å¾®åˆ†çš„å¥–åŠ±ä¿¡å·ï¼Œä»è€Œå®ç°å¯¹æ¨¡å‹ç­–ç•¥çš„ç²¾ç¡®è°ƒæ•´ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18227', 'title': 'Token Reduction Should Go Beyond Efficiency in Generative Models -- From\n  Vision, Language to Multimodality', 'url': 'https://huggingface.co/papers/2505.18227', 'abstract': 'In Transformer architectures, tokens\\textemdash discrete units derived from raw data\\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input\'s essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.', 'score': 5, 'issue_id': 4024, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '404502beaef00992', 'authors': ['Zhenglun Kong', 'Yize Li', 'Fanhu Zeng', 'Lei Xin', 'Shvat Messica', 'Xue Lin', 'Pu Zhao', 'Manolis Kellis', 'Hao Tang', 'Marinka Zitnik'], 'affiliations': ['Chinese Academy of Sciences', 'Harvard University', 'Massachusetts Institute of Technology', 'Northeastern University', 'Peking University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18227.jpg', 'data': {'categories': ['#rl', '#optimization', '#multimodal', '#interpretability', '#long_context', '#architecture', '#hallucinations', '#alignment', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²: Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ñ€Ğ¾Ğ»ÑŒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñ Ğ·Ğ° Ñ€Ğ°Ğ¼ĞºĞ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ, ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ°Ğº Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğ¹ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ĞµĞµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ½Ğ°Ğ¼ĞµÑ‡Ğ°ĞµÑ‚ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Token Reduction: A Key to Enhanced Generative Modeling', 'desc': 'This paper discusses the role of token reduction in Transformer architectures, which are commonly used in machine learning for processing data. Traditionally, token reduction has been seen as a way to improve efficiency by reducing computational costs and memory usage. However, the authors argue that token reduction should be viewed as a fundamental principle that can enhance generative modeling across various domains, including vision and language. They propose that effective token reduction can lead to better model integration, reduce errors, and improve the overall stability and coherence of models.'}, 'zh': {'title': 'ä»¤ç‰Œå‡å°‘ï¼šç”Ÿæˆå»ºæ¨¡çš„æ–°åŸåˆ™', 'desc': 'åœ¨Transformeræ¶æ„ä¸­ï¼Œä»¤ç‰Œæ˜¯ä»åŸå§‹æ•°æ®ä¸­æå–çš„ç¦»æ•£å•å…ƒï¼Œé€šè¿‡å°†è¾“å…¥åˆ†å‰²æˆå›ºå®šé•¿åº¦çš„å—æ¥å½¢æˆã€‚æ¯ä¸ªä»¤ç‰Œè¢«æ˜ å°„åˆ°ä¸€ä¸ªåµŒå…¥ï¼Œä½¿å¾—åœ¨ä¿æŒè¾“å…¥å…³é”®ä¿¡æ¯çš„åŒæ—¶èƒ½å¤Ÿè¿›è¡Œå¹¶è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚å°½ç®¡ä»¤ç‰Œå‡å°‘ä¸»è¦è¢«è§†ä¸ºæé«˜æ•ˆç‡çš„ç­–ç•¥ï¼Œä½†æœ¬æ–‡è®¤ä¸ºåœ¨å¤§å‹ç”Ÿæˆæ¨¡å‹æ—¶ä»£ï¼Œä»¤ç‰Œå‡å°‘åº”è¶…è¶Šä¼ ç»Ÿçš„æ•ˆç‡å¯¼å‘è§’è‰²ï¼Œæˆä¸ºç”Ÿæˆå»ºæ¨¡çš„åŸºæœ¬åŸåˆ™ã€‚æˆ‘ä»¬æå‡ºï¼Œä»¤ç‰Œå‡å°‘å¯ä»¥ä¿ƒè¿›å¤šæ¨¡æ€æ•´åˆã€å‡è½»â€œè¿‡åº¦æ€è€ƒâ€å’Œå¹»è§‰ã€ä¿æŒé•¿è¾“å…¥çš„ä¸€è‡´æ€§ï¼Œå¹¶å¢å¼ºè®­ç»ƒç¨³å®šæ€§ç­‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22203', 'title': 'Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning', 'url': 'https://huggingface.co/papers/2505.22203', 'abstract': 'The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.', 'score': 4, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'bb47fbc50ee1bc3f', 'authors': ['Yuzhen Huang', 'Weihao Zeng', 'Xingshan Zeng', 'Qi Zhu', 'Junxian He'], 'affiliations': ['The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.22203.jpg', 'data': {'categories': ['#optimization', '#training', '#math', '#rl', '#security', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² RLVR: Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ (RLVR). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ» Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ÑÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ…, Ñ‡Ñ‚Ğ¾ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞ¾Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ²ĞµÑ€Ğ¶ĞµĞ½Ñ‹ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±Ğ¼Ğ°Ğ½ÑƒÑ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ¸ÑĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Enhancing Trust in Reinforcement Learning Verifiers', 'desc': 'This paper investigates the effectiveness of rule-based and model-based verifiers in reinforcement learning with verifiable rewards (RLVR), particularly in the context of mathematical reasoning tasks. It highlights that rule-based verifiers often fail to recognize equivalent answers in different formats, leading to false negatives that hinder the training of reinforcement learning models. The study also reveals that while model-based verifiers show higher accuracy in static evaluations, they are vulnerable to misclassifying incorrect patterns as correct, resulting in false positives that can inflate rewards during training. Overall, the research emphasizes the need for more reliable verification methods to enhance the robustness of reward systems in RLVR applications.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ä¸­çš„éªŒè¯å™¨é£é™©ä¸æŒ‘æˆ˜', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒåŸºäºè§„åˆ™å’ŒåŸºäºæ¨¡å‹çš„éªŒè¯å™¨çš„æœ‰æ•ˆæ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œå½“å‰çš„åŸºäºè§„åˆ™çš„éªŒè¯å™¨åœ¨è¯†åˆ«ä¸åŒæ ¼å¼çš„ç­‰æ•ˆç­”æ¡ˆæ—¶å­˜åœ¨æ˜¾è‘—çš„å‡é˜´æ€§ç‡ï¼Œè¿™å¯¹å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ€§èƒ½äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚è™½ç„¶åŸºäºæ¨¡å‹çš„éªŒè¯å™¨åœ¨é™æ€è¯„ä¼°ä¸­è¡¨ç°å‡ºæ›´é«˜çš„éªŒè¯å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­å®¹æ˜“å—åˆ°æ”»å‡»ï¼Œå¯¼è‡´å‡é˜³æ€§ç°è±¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†è¿™ä¸¤ç§éªŒè¯å™¨çš„ç‹¬ç‰¹é£é™©ï¼Œä¸ºå¼€å‘æ›´å¼ºå¤§çš„å¼ºåŒ–å­¦ä¹ å¥–åŠ±ç³»ç»Ÿæä¾›äº†é‡è¦è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18700', 'title': 'GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains', 'url': 'https://huggingface.co/papers/2505.18700', 'abstract': 'Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.', 'score': 4, 'issue_id': 4014, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': '6e364db427c53c05', 'authors': ['Chun Wang', 'Xiaoran Pan', 'Zihao Pan', 'Haofan Wang', 'Yiren Song'], 'affiliations': ['LibLib.ai', 'NUS', 'Sun Yat-sen University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18700.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#cv', '#interpretability', '#reasoning'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ: VLM Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Geo Reason Enhancement (GRE) Suite Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM). GRE Suite Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ GRE30K, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GRE Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº GREval-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ GRE Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ¾Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ³Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Geo-Localization with Structured Reasoning', 'desc': 'This paper introduces the Geo Reason Enhancement (GRE) Suite, a framework designed to improve visual language models (VLMs) for geo-localization tasks. It addresses the challenges of extracting detailed visual cues and integrating them with external knowledge for better reasoning. The GRE Suite includes a new dataset called GRE30K, a model that uses multi-stage reasoning to enhance location inference, and a benchmark for evaluating performance across various geographic contexts. Experimental results show that the GRE Suite significantly outperforms existing methods, highlighting the importance of structured reasoning in visual localization.'}, 'zh': {'title': 'å¢å¼ºæ¨ç†ï¼Œç²¾å‡†å®šä½ï¼', 'desc': 'æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œåœ°ç†å®šä½é¢ä¸´ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œéœ€è¦ä»å›¾åƒä¸­æå–å¤šå±‚æ¬¡çš„è§†è§‰çº¿ç´¢ï¼Œå¹¶å°†å…¶ä¸å¤–éƒ¨ä¸–ç•ŒçŸ¥è¯†ç»“åˆè¿›è¡Œç³»ç»Ÿæ¨ç†ã€‚ç›®å‰çš„åœ°ç†å®šä½æ–¹æ³•å¾€å¾€ç¼ºä¹ç¨³å¥çš„æ¨ç†æœºåˆ¶å’Œå¯è§£é‡Šæ€§ï¼Œé™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Geo Reason Enhancementï¼ˆGREï¼‰å¥—ä»¶ï¼Œè¿™æ˜¯ä¸€ç§æ–°æ¡†æ¶ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†é“¾å¢å¼ºVLMsï¼Œä»¥å®ç°å‡†ç¡®ä¸”å¯è§£é‡Šçš„å®šä½æ¨æ–­ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17870', 'title': 'Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods', 'url': 'https://huggingface.co/papers/2505.17870', 'abstract': 'A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to a weakened pathogen builds immunity, AI models should be fine tuned on small, quarantined sets of explicitly labeled falsehoods as a "vaccine" against misinformation. These curated false examples are periodically injected during finetuning, strengthening the model ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact checked falsehoods themselves as a supervised vaccine, rather than relying on input perturbations or generic human feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers a proactive paradigm for aligning AI systems with factuality.', 'score': 4, 'issue_id': 4017, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '434efd8595252228', 'authors': ['Shaina Raza', 'Rizwan Qureshi', 'Marcelo Lotif', 'Aman Chadha', 'Deval Pandya', 'Christos Emmanouilidis'], 'affiliations': ['Amazon Web Services', 'University of Central Florida, Orlando, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.17870.jpg', 'data': {'categories': ['#training', '#rlhf', '#alignment', '#ethics', '#hallucinations'], 'emoji': 'ğŸ’‰', 'ru': {'title': 'Ğ’Ğ°ĞºÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´ĞµĞ·Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ñ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ¼Ğ¼ÑƒĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ€Ğ³Ğ°Ñ‚ÑŒ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¸Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Immunizing AI Against Misinformation: A Proactive Approach', 'desc': "This paper presents a novel approach to combat misinformation in generative AI models by fine-tuning them with labeled falsehoods, similar to how vaccines work in biology. By exposing the model to controlled examples of misinformation, it learns to identify and reject misleading claims while maintaining its ability to generate accurate information. The study demonstrates that models trained with this 'immunization' technique produce significantly less misinformation compared to traditional methods. Additionally, the authors propose ethical guidelines to ensure the responsible use of false data in this training process."}, 'zh': {'title': 'ç”¨è™šå‡ä¿¡æ¯â€œå…ç–«â€AIæ¨¡å‹ï¼ŒæŠµå¾¡é”™è¯¯ä¿¡æ¯', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨æ ‡è®°çš„è™šå‡ä¿¡æ¯æ¥å‡å°‘é”™è¯¯ä¿¡æ¯çš„ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºç”Ÿç‰©å…ç–«ï¼Œé€šè¿‡æ§åˆ¶æš´éœ²äºå‡å¼±ç—…åŸä½“æ¥å»ºç«‹å…ç–«åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå®šæœŸæ³¨å…¥è¿™äº›ç»è¿‡ç­›é€‰çš„è™šå‡ç¤ºä¾‹å¯ä»¥å¢å¼ºæ¨¡å‹è¯†åˆ«å’Œæ‹’ç»è¯¯å¯¼æ€§å£°æ˜çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒå¯¹çœŸå®è¾“å…¥çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶é¦–æ¬¡å°†ç»è¿‡äº‹å®æ£€æŸ¥çš„è™šå‡ä¿¡æ¯è§†ä¸ºç›‘ç£ç–«è‹—ï¼Œä»¥å¢å¼ºæ¨¡å‹æŠµå¾¡æœªæ¥é”™è¯¯ä¿¡æ¯çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21960', 'title': 'One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models', 'url': 'https://huggingface.co/papers/2505.21960', 'abstract': 'Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency.', 'score': 3, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'fff7f01ccce8324f', 'authors': ['Senmao Li', 'Lei Wang', 'Kai Wang', 'Tao Liu', 'Jiehang Xie', 'Joost van de Weijer', 'Fahad Shahbaz Khan', 'Shiqi Yang', 'Yaxing Wang', 'Jian Yang'], 'affiliations': ['Computer Vision Center, Universitat Aut`onoma de Barcelona', 'Linkoping University', 'Mohamed bin Zayed University of AI', 'Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University', 'SB Intuitions, SoftBank', 'School of Big Data and Computer Science, Guizhou Normal University', 'VCIP, CS, Nankai University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21960.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#cv', '#inference'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Time-independent Unified Encoder (TiUE) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ. TiUE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ KL-Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TiUE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Accelerating Image Generation with Unified Encoding', 'desc': 'The paper introduces the Time-independent Unified Encoder (TiUE), a novel approach to enhance Text-to-Image (T2I) diffusion models. By sharing encoder features across different decoder time steps, TiUE reduces the inference time while improving the diversity and quality of generated images. The study highlights that decoders are better at capturing semantic information, allowing for a more efficient use of encoder resources. Experimental results show that TiUE surpasses existing models in generating high-fidelity images with greater diversity and lower computational costs.'}, 'zh': {'title': 'æ—¶é—´ç‹¬ç«‹ç»Ÿä¸€ç¼–ç å™¨TiUEï¼šæå‡æ¨ç†é€Ÿåº¦ä¸å›¾åƒè´¨é‡çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´ç‹¬ç«‹ç»Ÿä¸€ç¼–ç å™¨TiUEï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦å’Œå›¾åƒè´¨é‡ã€‚é€šè¿‡åœ¨è§£ç å™¨çš„å¤šä¸ªæ—¶é—´æ­¥éª¤ä¹‹é—´å…±äº«ç¼–ç å™¨ç‰¹å¾ï¼ŒTiUEæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´çš„å¤æ‚æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§£ç å™¨åœ¨æ•æ‰ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯æ–¹é¢æ›´ä¸ºæœ‰æ•ˆï¼Œè€Œç¼–ç å™¨å¯ä»¥åœ¨ä¸åŒæ—¶é—´æ­¥éª¤ä¹‹é—´æœ‰æ•ˆå…±äº«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTiUEåœ¨ç”Ÿæˆå¤šæ ·æ€§å’ŒçœŸå®æ„Ÿæ–¹é¢ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†è®¡ç®—æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20298', 'title': 'MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal\n  Manga Understanding', 'url': 'https://huggingface.co/papers/2505.20298', 'abstract': 'Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.', 'score': 3, 'issue_id': 4022, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '52a09929449c8817', 'authors': ['Jeonghun Baek', 'Kazuki Egashira', 'Shota Onohara', 'Atsuyuki Miyai', 'Yuki Imajuku', 'Hikaru Ikuta', 'Kiyoharu Aizawa'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2505.20298.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#multimodal', '#training', '#games'], 'emoji': 'ğŸ“š', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ°Ğ½Ğ³Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸: MangaOCR Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ MangaVQA Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MangaLMM Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen2.5-VL Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ĞµĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ MangaLMM Ñ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ²Ñ€Ğ¾Ğ´Ğµ GPT-4 Ğ¸ Gemini 2.5. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ„ĞµÑ€Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ€Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼Ğ°Ğ½Ğ³Ğ¸.'}, 'en': {'title': 'Enhancing Manga Understanding with Multimodal Models', 'desc': 'This paper presents a study on improving large multimodal models (LMMs) to understand manga, which combines images and text. It introduces two benchmarks: MangaOCR for recognizing text within manga pages and MangaVQA for assessing contextual understanding through visual question answering. The authors develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL, to effectively perform both tasks. Through experiments comparing it with advanced models like GPT-4o and Gemini 2.5, the paper aims to enhance the evaluation and capabilities of LMMs in the unique narrative style of manga.'}, 'zh': {'title': 'æå‡æ¼«ç”»ç†è§£çš„å¤šæ¨¡æ€æ¨¡å‹', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†æ¼«ç”»ï¼ˆæ—¥æœ¬æ¼«ç”»ï¼‰ä½œä¸ºä¸€ç§å¤šæ¨¡æ€å™äº‹å½¢å¼ï¼Œç»“åˆäº†å›¾åƒå’Œæ–‡æœ¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒMangaOCRç”¨äºè¯†åˆ«é¡µé¢å†…çš„æ–‡æœ¬ï¼ŒMangaVQAç”¨äºé€šè¿‡è§†è§‰é—®ç­”è¯„ä¼°ä¸Šä¸‹æ–‡ç†è§£ã€‚MangaVQAåŒ…å«526å¯¹é«˜è´¨é‡çš„æ‰‹åŠ¨æ„å»ºé—®ç­”å¯¹ï¼Œèƒ½å¤Ÿåœ¨å¤šæ ·çš„å™äº‹å’Œè§†è§‰åœºæ™¯ä¸­è¿›è¡Œå¯é è¯„ä¼°ã€‚åŸºäºè¿™äº›åŸºå‡†ï¼Œæˆ‘ä»¬å¼€å‘äº†MangaLMMï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹æ¼«ç”»çš„æ¨¡å‹ï¼Œç»è¿‡å¾®è°ƒä»¥åŒæ—¶å¤„ç†è¿™ä¸¤ä¸ªä»»åŠ¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17507', 'title': 'Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph', 'url': 'https://huggingface.co/papers/2505.17507', 'abstract': 'HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid growth of open source machine learning (ML) resources, such as models and datasets, has accelerated IR research. However, existing platforms like Hugging Face do not explicitly utilize structured representations, limiting advanced queries and analyses such as tracing model evolution and recommending relevant datasets. To fill the gap, we construct HuggingKG, the first large-scale knowledge graph built from the Hugging Face community for ML resource management. With 2.6 million nodes and 6.2 million edges, HuggingKG captures domain-specific relations and rich textual attributes. It enables us to further present HuggingBench, a multi-task benchmark with three novel test collections for IR tasks including resource recommendation, classification, and tracing. Our experiments reveal unique characteristics of HuggingKG and the derived tasks. Both resources are publicly available, expected to advance research in open source resource sharing and management.', 'score': 3, 'issue_id': 4013, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '5f55eec50d967abe', 'authors': ['Qiaosheng Chen', 'Kaijia Huang', 'Xiao Zhou', 'Weiqing Luo', 'Yuanning Cui', 'Gong Cheng'], 'affiliations': ['Nanjing University State Key Laboratory for Novel Software Technology Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17507.jpg', 'data': {'categories': ['#dataset', '#open_source', '#benchmark', '#graphs'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'HuggingKG: Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² ML Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°', 'desc': 'HuggingKG - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ° Hugging Face Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 2,6 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑƒĞ·Ğ»Ğ¾Ğ² Ğ¸ 6,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° ÑĞ²ÑĞ·ĞµĞ¹, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ML. HuggingKG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ HuggingBench - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­Ñ‚Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½Ñ‹ ÑĞ¿Ğ¾ÑĞ¾Ğ±ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ° Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµÑÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.'}, 'en': {'title': 'Empowering ML Resource Management with HuggingKG', 'desc': 'HuggingKG is a large-scale knowledge graph designed to improve the management of open source machine learning resources. It addresses the limitations of existing platforms by providing structured representations that allow for advanced queries and analyses. With millions of nodes and edges, HuggingKG captures important relationships and attributes within the ML community. Additionally, it introduces HuggingBench, a benchmark for evaluating tasks like resource recommendation and classification, facilitating better resource sharing and management in the field.'}, 'zh': {'title': 'HuggingKGï¼šå¼€æºæœºå™¨å­¦ä¹ èµ„æºç®¡ç†çš„æ–°è§†é‡', 'desc': 'HuggingKGæ˜¯ä¸€ä¸ªå¤§å‹çŸ¥è¯†å›¾è°±ï¼Œæ—¨åœ¨æ”¹å–„å¼€æºæœºå™¨å­¦ä¹ èµ„æºçš„ç®¡ç†ã€‚å®ƒé€šè¿‡ç»“æ„åŒ–è¡¨ç¤ºï¼Œæ”¯æŒé«˜çº§æŸ¥è¯¢å’Œåˆ†æï¼Œå¸®åŠ©è¿½è¸ªæ¨¡å‹æ¼”å˜å’Œæ¨èç›¸å…³æ•°æ®é›†ã€‚HuggingKGåŒ…å«260ä¸‡ä¸ªèŠ‚ç‚¹å’Œ620ä¸‡ä¸ªè¾¹ï¼Œæ•æ‰äº†é¢†åŸŸç‰¹å®šçš„å…³ç³»å’Œä¸°å¯Œçš„æ–‡æœ¬å±æ€§ã€‚è¯¥å›¾è°±ä¸HuggingBenchåŸºå‡†æµ‹è¯•ç»“åˆï¼Œæ¨åŠ¨äº†ä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬èµ„æºæ¨èå’Œåˆ†ç±»ç­‰ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.15813', 'title': 'Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex', 'url': 'https://huggingface.co/papers/2505.15813', 'abstract': 'BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity.', 'score': 3, 'issue_id': 4019, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ', 'en': 'May 21', 'zh': '5æœˆ21æ—¥'}, 'hash': '1dc1d35baba9a901', 'authors': ['Muquan Yu', 'Mu Nan', 'Hossein Adeli', 'Jacob S. Prince', 'John A. Pyles', 'Leila Wehbe', 'Margaret M. Henderson', 'Michael J. Tarr', 'Andrew F. Luo'], 'affiliations': ['Carnegie Mellon University', 'Chinese University of Hong Kong', 'Columbia University', 'Harvard University', 'University of Hong Kong', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.15813.jpg', 'data': {'categories': ['#architecture', '#interpretability', '#transfer_learning', '#training', '#multimodal', '#cv', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ‹: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ğ¼Ğ°Ğ»Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ BraInCoRL', 'desc': 'BraInCoRL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¾Ğ² Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ‹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¸ÑĞ¿Ñ‹Ñ‚ÑƒĞµĞ¼Ñ‹Ñ…, ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ°Ñ… Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸. BraInCoRL Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¾ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»Ğ¾Ğ² Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ‹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ² Ğ²Ñ‹ÑÑˆĞ¸Ñ… Ğ¾Ñ‚Ğ´ĞµĞ»Ğ°Ñ… Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ñ€Ñ‹.'}, 'en': {'title': 'Transforming Neural Insights with Few-Shot Learning', 'desc': 'BraInCoRL is a machine learning model that uses a transformer architecture to predict neural responses in the higher visual cortex from a few examples, without needing extensive retraining. It addresses the challenge of generalizing to new subjects and stimuli by employing in-context learning, which allows the model to adapt based on variable input data. The model is trained to optimize performance in low-data scenarios, demonstrating superior accuracy compared to traditional voxelwise encoders. Additionally, BraInCoRL enhances the interpretability of neural signals by linking them to relevant visual stimuli and natural language queries.'}, 'zh': {'title': 'å°‘æ ·æœ¬å­¦ä¹ ï¼Œè§£é”è§†è§‰çš®å±‚çš„å¥¥ç§˜', 'desc': 'BraInCoRLæ˜¯ä¸€ç§åŸºäºå˜æ¢å™¨çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡ç¤ºä¾‹å»ºæ¨¡é«˜è§†è§‰çš®å±‚çš„ç¥ç»ååº”ã€‚è¯¥æ–¹æ³•åœ¨æ–°å—è¯•è€…ã€åˆºæ¿€å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒBraInCoRLèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œç›´æ¥é¢„æµ‹ä½“ç´ çº§çš„ç¥ç»ååº”ã€‚è¯¥æ¨¡å‹è¿˜æé«˜äº†å¯¹é«˜è§†è§‰çš®å±‚ç¥ç»ä¿¡å·çš„å¯è§£é‡Šæ€§ï¼Œèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸ä½“ç´ é€‰æ‹©æ€§è¿›è¡Œå¯è§£é‡Šçš„æ˜ å°„ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.12667', 'title': 'Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking', 'url': 'https://huggingface.co/papers/2505.12667', 'abstract': 'Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.  \t\t\t\t\tAI-generated summary \t\t\t\t The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. We will release our code upon publication.', 'score': 3, 'issue_id': 4014, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 Ğ¼Ğ°Ñ', 'en': 'May 19', 'zh': '5æœˆ19æ—¥'}, 'hash': '975482063e194827', 'authors': ['Zihan Su', 'Xuerui Qiu', 'Hongbin Xu', 'Tangyu Jiang', 'Junhao Zhuang', 'Chun Yuan', 'Ming Li', 'Shengfeng He', 'Fei Richard Yu'], 'affiliations': ['Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)', 'Institute of Automation, Chinese Academy of Sciences', 'Singapore Management University', 'South China University of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.12667.jpg', 'data': {'categories': ['#video', '#3d', '#architecture', '#security'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ°Ğ² Ğ½Ğ° AI-Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Safe-Sora - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mamba Ñ 3D Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğº Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼. Safe-Sora Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ğ¾Ğ´ÑĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ.'}, 'en': {'title': 'Revolutionizing Copyright Protection in AI-Generated Videos with Safe-Sora', 'desc': 'Safe-Sora is a novel framework designed to embed invisible watermarks into AI-generated videos, addressing the need for copyright protection in the growing field of generative video models. It utilizes a hierarchical adaptive matching mechanism to ensure that the watermark is seamlessly integrated into the video by matching it with visually similar frames. The framework employs a 3D wavelet transform-enhanced Mamba architecture to effectively manage the spatial and temporal aspects of watermark embedding, allowing for robust watermark retrieval. Experimental results show that Safe-Sora achieves superior video quality and watermark fidelity compared to existing methods, marking a significant advancement in the field of generative watermarking.'}, 'zh': {'title': 'Safe-Soraï¼šä¿æŠ¤ AI è§†é¢‘ç‰ˆæƒçš„æ–°æ–¹æ³•', 'desc': 'Safe-Sora æ˜¯ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ AI ç”Ÿæˆçš„è§†é¢‘ä¸­åµŒå…¥ä¸å¯è§æ°´å°ï¼Œä»¥ä¿æŠ¤ç‰ˆæƒã€‚è¯¥æ–¹æ³•é‡‡ç”¨åˆ†å±‚è‡ªé€‚åº”åŒ¹é…æœºåˆ¶ï¼Œå°†æ°´å°å›¾åƒåˆ†å‰²æˆå¤šä¸ªå°å—ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°ä¸ä¹‹è§†è§‰ç›¸ä¼¼çš„è§†é¢‘å¸§ä¸­ã€‚é€šè¿‡å¢å¼ºçš„ 3D å°æ³¢å˜æ¢ Mamba æ¶æ„ï¼ŒSafe-Sora å®ç°äº†æ°´å°åœ¨æ—¶ç©ºä¸Šçš„èåˆï¼Œç¡®ä¿äº†æ°´å°çš„é²æ£’æ€§å’Œé«˜ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafe-Sora åœ¨è§†é¢‘è´¨é‡å’Œæ°´å°ä¿æŠ¤æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22645', 'title': 'Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese', 'url': 'https://huggingface.co/papers/2505.22645', 'abstract': 'Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.  \t\t\t\t\tAI-generated summary \t\t\t\t While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench).', 'score': 2, 'issue_id': 4015, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'fa422e70671c24d0', 'authors': ['Hanjia Lyu', 'Jiebo Luo', 'Jian Kang', 'Allison Koenecke'], 'affiliations': ['Cornell University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.22645.jpg', 'data': {'categories': ['#ethics', '#low_resource', '#multilingual', '#dataset', '#benchmark', '#open_source'], 'emoji': 'ğŸ‡¨ğŸ‡³', 'ru': {'title': 'Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ LLM Ğ² ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ: ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ vs Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ² Ğ¸ Ğ¸Ğ¼ĞµĞ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ LLM Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼Ñƒ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ², Ğ½Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼Ñƒ Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğµ Ğ¸Ğ¼ĞµĞ½. Ğ­Ñ‚Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¸ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¸ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Unveiling Biases: LLM Performance in Simplified vs. Traditional Chinese', 'desc': 'This research investigates how Large Language Models (LLMs) perform differently when using Simplified versus Traditional Chinese, focusing on regional term and name choice tasks. The study reveals that LLMs often show biases based on the language variant used, which can lead to unequal representation and potential harms in decision-making processes. By analyzing 11 different LLMs, the authors found that while Simplified Chinese was favored in naming items, Traditional Chinese names were preferred in hiring scenarios. The paper emphasizes the importance of understanding these biases and provides a benchmark dataset for future evaluations of LLM performance across Chinese language variants.'}, 'zh': {'title': 'å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸­æ–‡è¡¨ç°åå·®ç ”ç©¶', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç®€ä½“ä¸­æ–‡å’Œç¹ä½“ä¸­æ–‡ä¹‹é—´çš„è¡¨ç°åå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨åœ°åŒºæœ¯è¯­å’Œåç§°é€‰æ‹©ä»»åŠ¡ä¸­ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMçš„è¡¨ç°å·®å¼‚ä¸è®­ç»ƒæ•°æ®å’Œåˆ†è¯æ–¹å¼æœ‰å…³ï¼Œè¿™å¯èƒ½å¯¼è‡´å¯¹ä¸åŒæ–‡åŒ–èƒŒæ™¯çš„å¿½è§†ã€‚é€šè¿‡è®¾è®¡ä¸¤ä¸ªåŸºå‡†ä»»åŠ¡ï¼Œç ”ç©¶å®¡è®¡äº†11ä¸ªä¸»è¦çš„å•†ä¸šLLMæœåŠ¡å’Œå¼€æºæ¨¡å‹çš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒLLMåœ¨åœ°åŒºæœ¯è¯­é€‰æ‹©ä»»åŠ¡ä¸­åå‘ç®€ä½“ä¸­æ–‡ï¼Œè€Œåœ¨åç§°é€‰æ‹©ä»»åŠ¡ä¸­åˆ™æ„å¤–åå‘ç¹ä½“ä¸­æ–‡ï¼Œå¼ºè°ƒäº†å¯¹LLMåè§çš„è¿›ä¸€æ­¥åˆ†æçš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21191', 'title': "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities", 'url': 'https://huggingface.co/papers/2505.21191', 'abstract': 'The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.', 'score': 2, 'issue_id': 4016, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'd2c1aefe13ff525b', 'authors': ['Junyan Zhang', 'Yubo Gao', 'Yibo Yan', 'Jungang Li', 'Zhaorui Hou', 'Sicheng Tao', 'Shuliang Liu', 'Song Dai', 'Yonghua Hei', 'Junzhuo Li', 'Xuming Hu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (Guangzhou)'], 'pdf_title_img': 'assets/pdf/title_img/2505.21191.jpg', 'data': {'categories': ['#training', '#optimization', '#interpretability', '#dataset', '#architecture'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ñ‚Ğ°Ğ¹Ğ½Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: ĞºĞ°Ğº LLM ÑƒÑ‡Ğ°Ñ‚ÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚, ĞºĞ°Ğº Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ HexaInst - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸, Ğ¸ SPARCOM - Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² LLM. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ñ… ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ Ğ² Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° ÑƒĞ³Ğ»ÑƒĞ±Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº LLM ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'Unlocking the Secrets of Instruction-Following in LLMs', 'desc': "This paper explores how fine-tuning Large Language Models (LLMs) enhances their ability to follow instructions, focusing on the computational changes that occur during this process. The authors introduce HexaInst, a diverse dataset designed to analyze instruction-specific sparse components in LLMs, including neurons and experts in Mixture-of-Experts architectures. They present SPARCOM, a framework that identifies these components, evaluates their generality and uniqueness, and compares their changes during fine-tuning. The findings reveal the importance of these sparse components in executing instructions, offering valuable insights into the mechanisms behind LLMs' instruction-following capabilities."}, 'zh': {'title': 'æ­ç¤ºå¾®è°ƒä¸ç¨€ç–è®¡ç®—çš„å…³ç³»', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¾®è°ƒå¦‚ä½•å½±å“å…¶æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†HexaInstï¼Œä¸€ä¸ªæ¶µç›–å…­ä¸ªä¸åŒç±»åˆ«çš„å¹³è¡¡æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶æå‡ºäº†SPARCOMåˆ†ææ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬è¯†åˆ«ç¨€ç–ç»„ä»¶çš„æ–¹æ³•ã€è¯„ä¼°å…¶åŠŸèƒ½çš„é€šç”¨æ€§å’Œç‹¬ç‰¹æ€§ï¼Œä»¥åŠç³»ç»Ÿæ¯”è¾ƒå…¶å˜åŒ–ã€‚é€šè¿‡å®éªŒï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¿™äº›ç¨€ç–ç»„ä»¶åœ¨æŒ‡ä»¤æ‰§è¡Œä¸­çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†å¾®è°ƒä¸ç¨€ç–è®¡ç®—åŸºç¡€ä¹‹é—´çš„å…³ç³»ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21582', 'title': 'AITEE -- Agentic Tutor for Electrical Engineering', 'url': 'https://huggingface.co/papers/2505.21582', 'abstract': "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Intelligent tutoring systems combined with large language models offer a promising approach to address students' diverse needs and promote self-efficacious learning. While large language models possess good foundational knowledge of electrical engineering basics, they remain insufficiently capable of addressing specific questions about electrical circuits. In this paper, we present AITEE, an agent-based tutoring system for electrical engineering designed to accompany students throughout their learning process, offer individualized support, and promote self-directed learning. AITEE supports both hand-drawn and digital circuits through an adapted circuit reconstruction process, enabling natural interaction with students. Our novel graph-based similarity measure identifies relevant context from lecture materials through a retrieval augmented generation approach, while parallel Spice simulation further enhances accuracy in applying solution methodologies. The system implements a Socratic dialogue to foster learner autonomy through guided questioning. Experimental evaluations demonstrate that AITEE significantly outperforms baseline approaches in domain-specific knowledge application, with even medium-sized LLM models showing acceptable performance. Our results highlight the potential of agentic tutors to deliver scalable, personalized, and effective learning environments for electrical engineering education.", 'score': 2, 'issue_id': 4019, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '30e929164ff65b3c', 'authors': ['Christopher Knievel', 'Alexander Bernhardt', 'Christian Bernhardt'], 'affiliations': ['Department of Electrical Engineering and Information Technology, HTWG Hochschule Konstanz, University of Applied Sciences, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.21582.jpg', 'data': {'categories': ['#graphs', '#interpretability', '#healthcare', '#science', '#rag', '#games', '#agents', '#multimodal', '#reasoning'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ˜Ğ˜-Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€ Ğ¿Ğ¾ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ: Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ', 'desc': 'AITEE - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ‚Ğ¾Ñ€ÑÑ‚Ğ²Ğ° Ğ¿Ğ¾ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ ĞºĞ°Ğº Ñ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ğ¼Ğ¸, Ñ‚Ğ°Ğº Ğ¸ Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ ÑÑ…ĞµĞ¼Ğ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ¿ĞµĞ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Spice. AITEE Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ°Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹.'}, 'en': {'title': 'Empowering Electrical Engineering Learning with AITEE', 'desc': 'This paper introduces AITEE, an agent-based tutoring system specifically designed for electrical engineering education. AITEE enhances learning by allowing students to interact naturally with both hand-drawn and digital circuit designs, while also providing personalized support through guided questioning. The system utilizes a graph-based similarity measure to retrieve relevant context from lecture materials, improving the accuracy of responses through a retrieval augmented generation approach. Experimental results indicate that AITEE significantly outperforms traditional methods, showcasing its effectiveness in promoting self-directed learning and domain-specific knowledge application.'}, 'zh': {'title': 'æ™ºèƒ½è¾…å¯¼ï¼Œæå‡ç”µæ°”å·¥ç¨‹å­¦ä¹ æ•ˆæœ', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAITEEçš„åŸºäºä»£ç†çš„ç”µæ°”å·¥ç¨‹è¾…å¯¼ç³»ç»Ÿï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶ç”µè·¯äº¤äº’ã€ä¸Šä¸‹æ–‡æ£€ç´¢ç”Ÿæˆå’Œå¼•å¯¼æé—®æ¥å¢å¼ºå­¦ä¹ æ•ˆæœã€‚AITEEèƒ½å¤Ÿæ”¯æŒæ‰‹ç»˜å’Œæ•°å­—ç”µè·¯ï¼Œä¿ƒè¿›å­¦ç”Ÿçš„è‡ªä¸»å­¦ä¹ ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–çš„æ”¯æŒã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å›¾å½¢ç›¸ä¼¼åº¦æµ‹é‡å’ŒSpiceä»¿çœŸæŠ€æœ¯ï¼Œæé«˜äº†è§£å†³æ–¹æ¡ˆæ–¹æ³•çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAITEEåœ¨é¢†åŸŸç‰¹å®šçŸ¥è¯†åº”ç”¨æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†ä»£ç†è¾…å¯¼å‘˜åœ¨ç”µæ°”å·¥ç¨‹æ•™è‚²ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20715', 'title': 'MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding', 'url': 'https://huggingface.co/papers/2505.20715', 'abstract': 'MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video temporal understanding is crucial for multimodal large language models (MLLMs) to reason over events in videos. Despite recent advances in general video understanding, current MLLMs still struggle with fine-grained temporal reasoning. While reinforcement learning (RL) has been explored to address this issue recently, existing RL approaches remain limited in effectiveness. In this work, we propose MUSEG, a novel RL-based method that enhances temporal understanding by introducing timestamp-aware multi-segment grounding. MUSEG enables MLLMs to align queries with multiple relevant video segments, promoting more comprehensive temporal reasoning. To facilitate effective learning, we design a customized RL training recipe with phased rewards that progressively guides the model toward temporally grounded reasoning. Extensive experiments on temporal grounding and time-sensitive video QA tasks demonstrate that MUSEG significantly outperforms existing methods and generalizes well across diverse temporal understanding scenarios. View our project at https://github.com/THUNLP-MT/MUSEG.', 'score': 2, 'issue_id': 4017, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '7d2c1d63ea6e59e3', 'authors': ['Fuwen Luo', 'Shengfeng Lou', 'Chi Chen', 'Ziyue Wang', 'Chenliang Li', 'Weizhou Shen', 'Jiyue Guo', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China', 'Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China', 'School of Computer Science and Technology (School of Artificial Intelligence), Zhejiang Sci-Tech University, China', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.20715.jpg', 'data': {'categories': ['#training', '#rl', '#multimodal', '#alignment', '#video', '#reasoning'], 'emoji': 'â±ï¸', 'ru': {'title': 'MUSEG: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MUSEG - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğº Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğº, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. MUSEG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MUSEG Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ¸ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.'}, 'en': {'title': 'Enhancing Temporal Reasoning in MLLMs with MUSEG', 'desc': "MUSEG is a novel reinforcement learning (RL) method designed to improve the temporal understanding of large language models (MLLMs) when analyzing video content. It introduces timestamp-aware multi-segment grounding, which allows the model to better align its queries with relevant segments of a video, enhancing its ability to reason about events over time. The method employs a customized RL training strategy that uses phased rewards to progressively develop the model's temporal reasoning capabilities. Experimental results show that MUSEG outperforms existing approaches in tasks related to temporal grounding and time-sensitive video question answering, demonstrating its effectiveness across various scenarios."}, 'zh': {'title': 'MUSEGï¼šæå‡è§†é¢‘æ—¶é—´ç†è§£çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'MUSEGæ˜¯ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è§†é¢‘çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥æ—¶é—´æˆ³æ„ŸçŸ¥çš„å¤šæ®µè½å¯¹é½ï¼ŒMUSEGèƒ½å¤Ÿæ›´å¥½åœ°å°†æŸ¥è¯¢ä¸å¤šä¸ªç›¸å…³è§†é¢‘ç‰‡æ®µå¯¹é½ï¼Œä»è€Œä¿ƒè¿›æ›´å…¨é¢çš„æ—¶é—´æ¨ç†ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å®šåˆ¶çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ¡ˆï¼Œé‡‡ç”¨åˆ†é˜¶æ®µå¥–åŠ±ï¼Œé€æ­¥å¼•å¯¼æ¨¡å‹å®ç°æ—¶é—´åŸºç¡€çš„æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMUSEGåœ¨æ—¶é—´å¯¹é½å’Œæ—¶é—´æ•æ„Ÿçš„è§†é¢‘é—®ç­”ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å¤šç§æ—¶é—´ç†è§£åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20589', 'title': 'Prot2Token: A Unified Framework for Protein Modeling via Next-Token\n  Prediction', 'url': 'https://huggingface.co/papers/2505.20589', 'abstract': 'Prot2Token unifies protein prediction tasks using an autoregressive decoder with task tokens, improving efficiency and accuracy across different benchmarks compared to specialized models.  \t\t\t\t\tAI-generated summary \t\t\t\t The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .', 'score': 2, 'issue_id': 4027, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '57600073f21ca2c1', 'authors': ['Mahdi Pourmirzaei', 'Farzaneh Esmaili', 'Salhuldin Alqarghuli', 'Mohammadreza Pourmirzaei', 'Ye Han', 'Kai Chen', 'Mohsen Rezaei', 'Duolin Wang', 'Dong Xu'], 'affiliations': ['Politecnico di Milano, Milan, Italy', 'ProGene', 'University of Missouri'], 'pdf_title_img': 'assets/pdf/title_img/2505.20589.jpg', 'data': {'categories': ['#architecture', '#science', '#open_source', '#training', '#dataset', '#optimization'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ±ĞµĞ»ĞºĞ¾Ğ²', 'desc': 'Prot2Token - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ±ĞµĞ»ĞºĞ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµrÑ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€ Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ»ĞºĞ¾Ğ² Ğ² ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°. Prot2Token Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑƒÑĞºĞ¾Ñ€ÑÑ Ğ±Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ€ĞµĞ´ÑÑ‚Ğ².'}, 'en': {'title': 'Unified Protein Predictions with Prot2Token', 'desc': 'Prot2Token is a novel framework that simplifies protein prediction tasks by using an autoregressive decoder with task tokens. This approach allows the model to handle various protein-related predictions in a unified manner, improving both efficiency and accuracy. By leveraging pre-trained protein encoders and enabling multi-task learning, Prot2Token can perform multiple tasks simultaneously, often outperforming specialized models. The results show significant speed improvements and strong predictive capabilities, making it a promising tool for advancing protein modeling and biological research.'}, 'zh': {'title': 'Prot2Tokenï¼šé«˜æ•ˆç»Ÿä¸€çš„è›‹ç™½è´¨é¢„æµ‹æ¡†æ¶', 'desc': 'Prot2Token æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤šç§è›‹ç™½è´¨é¢„æµ‹ä»»åŠ¡ï¼Œé‡‡ç”¨è‡ªå›å½’è§£ç å™¨å’Œä»»åŠ¡æ ‡è®°ï¼Œæå‡äº†æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚è¯¥æ¨¡å‹å°†è›‹ç™½è´¨ç›¸å…³çš„é¢„æµ‹è½¬åŒ–ä¸ºæ ‡å‡†çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æ ¼å¼ï¼Œæ”¯æŒä»åºåˆ—ç‰¹æ€§åˆ°å¤æ‚çš„è›‹ç™½è´¨é—´ç›¸äº’ä½œç”¨çš„å¤šæ ·åŒ–é¢„æµ‹ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„è›‹ç™½è´¨ç¼–ç å™¨çš„åµŒå…¥å’Œå¯å­¦ä¹ çš„ä»»åŠ¡æ ‡è®°ï¼ŒProt2Token å®ç°äº†å¤šä»»åŠ¡å­¦ä¹ ï¼Œä½¿å¾—å•ä¸€æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåœ°æŒæ¡å¤šç§ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProt2Token åœ¨ä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œé¢„æµ‹èƒ½åŠ›å¼ºï¼Œé€Ÿåº¦æ˜¾è‘—æå‡ï¼Œæ¨åŠ¨äº†ç”Ÿç‰©å‘ç°å’Œæ–°ç–—æ³•çš„å¼€å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19051', 'title': 'Efficient Data Selection at Scale via Influence Distillation', 'url': 'https://huggingface.co/papers/2505.19051', 'abstract': 'Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample\'s influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a landmark-based approximation: influence is precisely computed for a small subset of "landmark" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to 3.5times faster selection.', 'score': 2, 'issue_id': 4021, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'cbb8b90722e6d09c', 'authors': ['Mahdi Nikdan', 'Vincent Cohen-Addad', 'Dan Alistarh', 'Vahab Mirrokni'], 'affiliations': ['Google Research', 'ISTA', 'Red Hat AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.19051.jpg', 'data': {'categories': ['#training', '#data', '#optimization'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Influence Distillation. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ° Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹Ğ²Ğ°ÑÑÑŒ Ğ½Ğ° Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğ¸ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Influence Distillation ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ´Ğ¾ 3,5 Ñ€Ğ°Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Optimizing Data Selection for Superior Model Performance', 'desc': "This paper presents Influence Distillation, a new method for selecting training data for Large Language Models (LLMs) that uses second-order information to determine the importance of each training sample. By calculating the influence of each sample on the model's performance, the method assigns specific weights to guide the selection of data for fine-tuning. The approach is designed to be efficient, using a landmark-based approximation to reduce computational costs while maintaining accuracy. Experiments demonstrate that this method not only matches but can also exceed the performance of existing state-of-the-art techniques, significantly speeding up the data selection process."}, 'zh': {'title': 'å½±å“è’¸é¦ï¼šä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®é€‰æ‹©çš„æ–°æ–¹æ³•', 'desc': 'æœ‰æ•ˆçš„æ•°æ®é€‰æ‹©å¯¹äºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é«˜æ•ˆè®­ç»ƒè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºå½±å“è’¸é¦çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ä½¿ç”¨äºŒé˜¶ä¿¡æ¯æ¥ä¼˜åŒ–è®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–æ¯ä¸ªæ ·æœ¬å¯¹ç›®æ ‡åˆ†å¸ƒçš„å½±å“ï¼Œåˆ†é…ç‰¹å®šäºæ¨¡å‹çš„æƒé‡ï¼Œä»¥é€‰æ‹©ç”¨äºLLMå¾®è°ƒçš„è®­ç»ƒæ•°æ®ï¼Œä»è€Œæé«˜åœ¨ç›®æ ‡é¢†åŸŸçš„è¡¨ç°ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ¨¡å‹ä¸ŠéªŒè¯äº†å½±å“è’¸é¦çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœè¡¨æ˜å…¶åœ¨æ€§èƒ½ä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ï¼ŒåŒæ—¶é€‰æ‹©é€Ÿåº¦æé«˜äº†3.5å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.22664', 'title': 'Zero-Shot Vision Encoder Grafting via LLM Surrogates', 'url': 'https://huggingface.co/papers/2505.22664', 'abstract': 'The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) typically pair a modestly sized vision encoder with a large language model (LLM), e.g., Llama-70B, making the decoder the primary computational burden during training. To reduce costs, a potential promising strategy is to first train the vision encoder using a small language model before transferring it to the large one. We construct small "surrogate models" that share the same embedding space and representation language as the large target LLM by directly inheriting its shallow layers. Vision encoders trained on the surrogate can then be directly transferred to the larger model, a process we call zero-shot grafting -- when plugged directly into the full-size target LLM, the grafted pair surpasses the encoder-surrogate pair and, on some benchmarks, even performs on par with full decoder training with the target LLM. Furthermore, our surrogate training approach reduces overall VLM training costs by ~45% when using Llama-70B as the decoder.', 'score': 1, 'issue_id': 4024, 'pub_date': '2025-05-28', 'pub_date_card': {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'}, 'hash': 'be8516f11be3656f', 'authors': ['Kaiyu Yue', 'Vasu Singla', 'Menglin Jia', 'John Kirchenbauer', 'Rifaa Qadri', 'Zikui Cai', 'Abhinav Bhatele', 'Furong Huang', 'Tom Goldstein'], 'affiliations': ['Meta'], 'pdf_title_img': 'assets/pdf/title_img/2505.22664.jpg', 'data': {'categories': ['#optimization', '#small_models', '#transfer_learning', '#training', '#cv'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ·Ñ‹Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-ÑÑƒÑ€Ñ€Ğ¾Ğ³Ğ°Ñ‚Ğ°, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ ĞµĞ³Ğ¾ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 'zero-shot grafting', Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ñ€ĞµÑÑƒÑ€ÑÑ‹, Ğ½Ğ¾ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ."}, 'en': {'title': 'Efficient Vision Encoder Training with Surrogate Models', 'desc': "This paper presents a method for training vision encoders using smaller surrogate models before integrating them with larger language models. By training the vision encoder on a compact model that shares the same embedding space as the larger model, the authors demonstrate a significant reduction in training costs and improved performance. The technique, termed 'zero-shot grafting', allows the trained vision encoder to be directly transferred to the larger language model, achieving results comparable to full decoder training. Overall, this approach can lower the training expenses of vision language models by approximately 45%."}, 'zh': {'title': 'å°æ¨¡å‹åŠ©åŠ›å¤§æ¨¡å‹ï¼Œé™ä½æˆæœ¬æå‡æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å…ˆç”¨å°å‹æ›¿ä»£æ¨¡å‹è®­ç»ƒè§†è§‰ç¼–ç å™¨ï¼Œå†å°†å…¶è½¬ç§»åˆ°å¤§å‹è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥é™ä½è®­ç»ƒæˆæœ¬å¹¶æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬æ„å»ºçš„å°å‹æ›¿ä»£æ¨¡å‹ä¸å¤§å‹ç›®æ ‡è¯­è¨€æ¨¡å‹å…±äº«ç›¸åŒçš„åµŒå…¥ç©ºé—´å’Œè¡¨ç¤ºè¯­è¨€ï¼Œä»è€Œå®ç°äº†é›¶-shotå«æ¥ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡è¿™ç§è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨åœ¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ç»“åˆæ—¶ï¼Œæ€§èƒ½è¶…è¿‡äº†ç›´æ¥è®­ç»ƒçš„ç¼–ç å™¨-æ›¿ä»£æ¨¡å‹ç»„åˆï¼Œå¹¶ä¸”åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸å®Œæ•´è§£ç å™¨è®­ç»ƒç›¸å½“ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21060', 'title': 'Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles', 'url': 'https://huggingface.co/papers/2505.21060', 'abstract': 'A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Stylizing 3D scenes instantly while maintaining multi-view consistency and faithfully resembling a style image remains a significant challenge. Current state-of-the-art 3D stylization methods typically involve computationally intensive test-time optimization to transfer artistic features into a pretrained 3D representation, often requiring dense posed input images. In contrast, leveraging recent advances in feed-forward reconstruction models, we demonstrate a novel approach to achieve direct 3D stylization in less than a second using unposed sparse-view scene images and an arbitrary style image. To address the inherent decoupling between reconstruction and stylization, we introduce a branched architecture that separates structure modeling and appearance shading, effectively preventing stylistic transfer from distorting the underlying 3D scene structure. Furthermore, we adapt an identity loss to facilitate pre-training our stylization model through the novel view synthesis task. This strategy also allows our model to retain its original reconstruction capabilities while being fine-tuned for stylization. Comprehensive evaluations, using both in-domain and out-of-domain datasets, demonstrate that our approach produces high-quality stylized 3D content that achieve a superior blend of style and scene appearance, while also outperforming existing methods in terms of multi-view consistency and efficiency.', 'score': 1, 'issue_id': 4020, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'cfa395c8cc947534', 'authors': ['Peng Wang', 'Xiang Liu', 'Peidong Liu'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21060.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#3d'], 'emoji': 'ğŸ¨', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ 3D-ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑÑ†ĞµĞ½Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ 3D-ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ½Ğ¾Ñ ÑÑ‚Ğ¸Ğ»Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞµ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Fast and Consistent 3D Stylization with Sparse Views', 'desc': "This paper presents a new feed-forward model for fast 3D stylization using sparse view images, which allows for quick artistic transformations of 3D scenes. Unlike traditional methods that require extensive computational resources and dense input images, this approach achieves stylization in under a second while ensuring multi-view consistency. The model features a branched architecture that separates the tasks of structure modeling and appearance shading, preventing style transfer from altering the 3D scene's structure. Additionally, an identity loss is utilized to pre-train the model, enabling it to maintain reconstruction accuracy while being fine-tuned for stylization tasks."}, 'zh': {'title': 'å¿«é€Ÿ3Dé£æ ¼åŒ–ï¼Œä¿æŒä¸€è‡´æ€§ä¸é«˜è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å‰é¦ˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå¿«é€Ÿå®ç°3Dé£æ ¼åŒ–ï¼Œä½¿ç”¨ç¨€ç–è§†å›¾å›¾åƒï¼ŒåŒæ—¶ä¿æŒå¤šè§†å›¾ä¸€è‡´æ€§å’Œé«˜è´¨é‡çš„é£æ ¼è½¬ç§»ã€‚ä¸ä¼ ç»Ÿçš„3Dé£æ ¼åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•é¿å…äº†è®¡ç®—å¯†é›†çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¸åˆ°ä¸€ç§’çš„æ—¶é—´å†…å®Œæˆé£æ ¼åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†æ”¯æ¶æ„ï¼Œå°†ç»“æ„å»ºæ¨¡å’Œå¤–è§‚ç€è‰²åˆ†å¼€ï¼Œæœ‰æ•ˆé˜²æ­¢é£æ ¼è½¬ç§»æ‰­æ›²3Dåœºæ™¯çš„ç»“æ„ã€‚é€šè¿‡é€‚åº”èº«ä»½æŸå¤±ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¿æŒé‡å»ºèƒ½åŠ›çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¿›è¡Œé£æ ¼åŒ–çš„å¾®è°ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20444', 'title': 'HoPE: Hybrid of Position Embedding for Length Generalization in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.20444', 'abstract': 'Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE.', 'score': 1, 'issue_id': 4024, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'b461185334c2de42', 'authors': ['Haoran Li', 'Yingjie Qin', 'Baoyuan Ou', 'Lai Xu', 'Ruiwen Xu'], 'affiliations': ['Carnegie Mellon University', 'Xiaohongshu Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.20444.jpg', 'data': {'categories': ['#video', '#benchmark', '#multimodal', '#long_context', '#architecture'], 'emoji': 'ğŸ¬', 'ru': {'title': 'HoPE: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ HoPE (Hybrid of Position Embedding). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. HoPE Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ HoPE Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Enhancing Long-Context Understanding in VLMs with HoPE', 'desc': 'This paper addresses the limitations of Vision-Language Models (VLMs) in handling long-context scenarios, especially with long videos. It critiques existing Rotary Position Embedding (RoPE) methods for their inadequate ability to capture complex spatial-temporal dependencies. The authors introduce HoPE, a new Hybrid of Position Embedding that employs a novel frequency allocation strategy and a dynamic temporal scaling mechanism. Through extensive experiments, HoPE shows significant improvements in long video understanding and retrieval tasks compared to current methods.'}, 'zh': {'title': 'æå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡èƒ½åŠ›', 'desc': 'è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨é•¿è§†é¢‘ç­‰é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸­çš„è¡¨ç°å¾€å¾€ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®åµŒå…¥æ–¹æ³•HoPEï¼Œæ—¨åœ¨æ”¹å–„VLMsåœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„èƒ½åŠ›ã€‚HoPEé‡‡ç”¨æ··åˆé¢‘ç‡åˆ†é…ç­–ç•¥ï¼Œä»¥å¯é åœ°å»ºæ¨¡è¯­ä¹‰ï¼Œå¹¶å¼•å…¥åŠ¨æ€æ—¶é—´ç¼©æ”¾æœºåˆ¶ï¼Œä»¥æ”¯æŒä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„çµæ´»æ¨ç†ã€‚é€šè¿‡åœ¨å››ä¸ªè§†é¢‘åŸºå‡†ä¸Šçš„å¹¿æ³›å®éªŒï¼ŒHoPEçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18366', 'title': 'Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems', 'url': 'https://huggingface.co/papers/2505.18366', 'abstract': "Enterprise search systems often struggle to retrieve accurate, domain-specific information due to semantic mismatches and overlapping terminologies. These issues can degrade the performance of downstream applications such as knowledge management, customer support, and retrieval-augmented generation agents. To address this challenge, we propose a scalable hard-negative mining framework tailored specifically for domain-specific enterprise data. Our approach dynamically selects semantically challenging but contextually irrelevant documents to enhance deployed re-ranking models.   Our method integrates diverse embedding models, performs dimensionality reduction, and uniquely selects hard negatives, ensuring computational efficiency and semantic precision. Evaluation on our proprietary enterprise corpus (cloud services domain) demonstrates substantial improvements of 15\\% in MRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other negative sampling techniques. Further validation on public domain-specific datasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability and readiness for real-world applications.", 'score': 0, 'issue_id': 4027, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': 'b9fbad227b517357', 'authors': ['Hansa Meghwani', 'Amit Agarwal', 'Priyaranjan Pattnayak', 'Hitesh Laxmichand Patel', 'Srikant Panda'], 'affiliations': ['Oracle AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18366.jpg', 'data': {'categories': ['#rag', '#optimization', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° hard-negative mining. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ, Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾ Ğ½ĞµÑ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº MRR@3 Ğ¸ MRR@10 Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Enhancing Enterprise Search with Hard-Negative Mining', 'desc': "This paper addresses the challenges faced by enterprise search systems in retrieving accurate information due to semantic mismatches and overlapping terminologies. It introduces a scalable hard-negative mining framework that focuses on domain-specific data to improve the performance of re-ranking models. The method involves selecting semantically difficult but contextually irrelevant documents to enhance the model's ability to distinguish relevant information. Evaluation results show significant improvements in mean reciprocal rank (MRR) metrics, indicating the effectiveness and generalizability of the proposed approach across various datasets."}, 'zh': {'title': 'æå‡ä¼ä¸šæœç´¢çš„è¯­ä¹‰ç²¾åº¦', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¼ä¸šç‰¹å®šæ•°æ®çš„å¯æ‰©å±•ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ¡†æ¶ï¼Œä»¥è§£å†³ä¼ä¸šæœç´¢ç³»ç»Ÿåœ¨æ£€ç´¢å‡†ç¡®çš„é¢†åŸŸç‰¹å®šä¿¡æ¯æ—¶é¢ä¸´çš„è¯­ä¹‰ä¸åŒ¹é…å’Œæœ¯è¯­é‡å é—®é¢˜ã€‚è¯¥æ–¹æ³•åŠ¨æ€é€‰æ‹©è¯­ä¹‰ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§ä½†ä¸Šä¸‹æ–‡æ— å…³çš„æ–‡æ¡£ï¼Œä»¥å¢å¼ºå·²éƒ¨ç½²çš„é‡æ’åºæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¤šç§åµŒå…¥æ¨¡å‹ï¼Œè¿›è¡Œé™ç»´å¤„ç†ï¼Œå¹¶ç‹¬ç‰¹åœ°é€‰æ‹©ç¡¬è´Ÿæ ·æœ¬ï¼Œç¡®ä¿è®¡ç®—æ•ˆç‡å’Œè¯­ä¹‰ç²¾åº¦ã€‚é€šè¿‡åœ¨ä¸“æœ‰ä¼ä¸šè¯­æ–™åº“ä¸Šçš„è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨MRR@3å’ŒMRR@10ä¸Šåˆ†åˆ«æ¯”ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æé«˜äº†15%å’Œ19%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18149', 'title': 'First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.18149', 'abstract': "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or require a large number of samples to be generated, increasing the token usage and inference latency. We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones. Motivated by this, we introduce First Finish Search (FFS), a training-free parallel decoding strategy that launches n independent samples and returns as soon as any one completes. We evaluate FFS alongside simple decoding, beam search, majority voting, and budget forcing on four reasoning models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With DeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15% improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's o4-mini performance. Our theoretical analysis explains why stopping at the shortest trace is likely to yield a correct answer and identifies the conditions under which early stopping may be suboptimal. The elegance and simplicity of FFS demonstrate that straightforward TTS strategies can perform remarkably well, revealing the untapped potential of simple approaches at inference time.", 'score': 0, 'issue_id': 4019, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': 'd05921967a27b012', 'authors': ['Aradhye Agarwal', 'Ayan Sengupta', 'Tanmoy Chakraborty'], 'affiliations': ['Indian Institute of Technology Delhi'], 'pdf_title_img': 'assets/pdf/title_img/2505.18149.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': 'ğŸ', 'ru': {'title': 'ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ğ¸Ğ½Ğ¸ÑˆĞ¸Ñ€ÑƒĞµÑ‚ - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ±ĞµĞ¶Ğ´Ğ°ĞµÑ‚: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ First Finish Search (FFS) ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğµ. FFS Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²ĞµĞ½, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸. FFS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 82.23% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AIME, Ñ‡Ñ‚Ğ¾ Ğ½Ğ° 15% Ğ»ÑƒÑ‡ÑˆĞµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ DeepSeek-R1.'}, 'en': {'title': 'First Finish Search: Quick Answers, Better Accuracy!', 'desc': 'First Finish Search (FFS) is a novel decoding strategy that enhances the accuracy of large language models by terminating inference as soon as the first complete sample is produced. This method contrasts with traditional approaches that often generate multiple samples or rely on lengthy decoding paths, which can lead to increased latency and token usage. FFS has been shown to significantly improve performance on reasoning tasks, achieving a notable accuracy boost compared to existing methods. The findings suggest that shorter inference traces are more likely to yield correct answers, highlighting the effectiveness of simpler, more efficient strategies in machine learning inference.'}, 'zh': {'title': 'ç®€åŒ–æ¨ç†ï¼Œæå‡å‡†ç¡®æ€§ï¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œFirst Finish Searchâ€ï¼ˆFFSï¼‰çš„æ–°è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€‚FFSé€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šä¸ªç‹¬ç«‹æ ·æœ¬ï¼Œå¹¶åœ¨ç¬¬ä¸€ä¸ªæ ·æœ¬å®Œæˆæ—¶ç«‹å³è¿”å›ç»“æœï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´å’Œè®¡ç®—èµ„æºçš„ä½¿ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºæ¨ç†ä»»åŠ¡ï¼Œè¾ƒçŸ­çš„è§£ç è·¯å¾„æ¯”è¾ƒé•¿çš„è·¯å¾„æ›´å¯èƒ½äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆã€‚é€šè¿‡ä¸å…¶ä»–è§£ç æ–¹æ³•çš„æ¯”è¾ƒï¼ŒFFSåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºäº†ç®€å•è§£ç ç­–ç•¥åœ¨æ¨ç†æ—¶çš„å·¨å¤§æ½œåŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (2)', '#agi (1)', '#alignment (6)', '#architecture (13)', '#audio', '#benchmark (18)', '#cv (8)', '#data (6)', '#dataset (15)', '#diffusion (5)', '#ethics (2)', '#games (3)', '#graphs (2)', '#hallucinations (3)', '#healthcare (1)', '#inference (5)', '#interpretability (7)', '#leakage', '#long_context (2)', '#low_resource (2)', '#machine_translation', '#math (2)', '#multilingual (2)', '#multimodal (15)', '#open_source (14)', '#optimization (22)', '#plp', '#rag (5)', '#reasoning (19)', '#rl (10)', '#rlhf (6)', '#robotics', '#science (3)', '#security (2)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (2)', '#training (26)', '#transfer_learning (6)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-29 17:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-29 17:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-29 17:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    