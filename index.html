
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. June 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 июня</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-25.html">⬅️ <span id="prev-date">25.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-27.html">➡️ <span id="next-date">27.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 июня', 'en': 'June 26', 'zh': '6月26日'};
        let feedDateNext = {'ru': '27.06', 'en': '06/27', 'zh': '6月27日'};
        let feedDatePrev = {'ru': '25.06', 'en': '06/25', 'zh': '6月25日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.18095', 'title': 'ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image\n  Generation', 'url': 'https://huggingface.co/papers/2506.18095', 'abstract': "ShareGPT-4o-Image and Janus-4o enable open research in photorealistic, instruction-aligned image generation through a large dataset and multimodal model.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.", 'score': 51, 'issue_id': 4496, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': 'ea0d767800ce404b', 'authors': ['Junying Chen', 'Zhenyang Cai', 'Pengcheng Chen', 'Shunian Chen', 'Ke Ji', 'Xidong Wang', 'Yunjin Yang', 'Benyou Wang'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2506.18095.jpg', 'data': {'categories': ['#cv', '#dataset', '#open_source', '#multimodal', '#synthetic'], 'emoji': '🖼️', 'ru': {'title': 'Демократизация фотореалистичной генерации изображений с помощью открытых данных и моделей', 'desc': 'Статья представляет ShareGPT-4o-Image - первый набор данных, содержащий 45 тысяч примеров генерации изображений по тексту и 46 тысяч примеров генерации изображений по тексту и изображению, созданных с помощью GPT-4o. На основе этого датасета разработана мультимодальная языковая модель Janus-4o, способная генерировать изображения как по тексту, так и по тексту с изображением. Janus-4o значительно улучшает генерацию изображений по сравнению с предшественником Janus-Pro и достигает впечатляющих результатов в генерации изображений по тексту и изображению, используя всего 91 тысячу синтетических примеров. Авторы надеются, что публикация ShareGPT-4o-Image и Janus-4o будет способствовать открытым исследованиям в области фотореалистичной генерации изображений, согласованной с инструкциями.'}, 'en': {'title': 'Democratizing Photorealistic Image Generation with Open Datasets and Models', 'desc': 'This paper introduces ShareGPT-4o-Image, a comprehensive dataset designed to enhance photorealistic image generation aligned with user instructions. It includes 45,000 text-to-image and 46,000 text-and-image-to-image samples, all generated using the advanced capabilities of GPT-4o. The authors also present Janus-4o, a multimodal large language model that improves upon previous models by enabling both text-to-image and text-and-image-to-image generation. With only 91,000 synthetic samples and minimal training time, Janus-4o demonstrates significant advancements in generating high-quality images, promoting open research in this field.'}, 'zh': {'title': '开放研究，真实感图像生成的新纪元', 'desc': '本论文介绍了ShareGPT-4o-Image数据集和Janus-4o模型，旨在推动开放研究在真实感图像生成领域的发展。ShareGPT-4o-Image包含45K文本到图像和46K文本与图像到图像的数据，利用GPT-4o的图像生成能力进行合成。Janus-4o是一个多模态大语言模型，能够进行文本到图像和文本与图像到图像的生成，显著提升了生成效果。我们希望这些工具能够促进真实感、指令对齐的图像生成研究。'}}}, {'id': 'https://huggingface.co/papers/2506.19697', 'title': 'Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.19697', 'abstract': 'Outlier-Safe Pre-Training improves large language model quantization performance by preventing extreme activation outliers through innovative training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Extreme activation outliers in Large Language Models (LLMs) critically degrade quantization performance, hindering efficient on-device deployment. While channel-wise operations and adaptive gradient scaling are recognized causes, practical mitigation remains challenging. We introduce Outlier-Safe Pre-Training (OSP), a practical guideline that proactively prevents outlier formation rather than relying on post-hoc mitigation. OSP combines three key innovations: (1) the Muon optimizer, eliminating privileged bases while maintaining training efficiency; (2) Single-Scale RMSNorm, preventing channel-wise amplification; and (3) a learnable embedding projection, redistributing activation magnitudes originating from embedding matrices. We validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is the first production-scale LLM trained without such outliers. Under aggressive 4-bit quantization, our OSP model achieves a 35.7 average score across 10 benchmarks (compared to 26.5 for an Adam-trained model), with only a 2% training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis (0.04) compared to extreme values (1818.56) in standard models, fundamentally altering LLM quantization behavior. Our work demonstrates that outliers are not inherent to LLMs but are consequences of training strategies, paving the way for more efficient LLM deployment. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.', 'score': 34, 'issue_id': 4495, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '3a6d3af4578d26f6', 'authors': ['Jungwoo Park', 'Taewhoo Lee', 'Chanwoong Yoon', 'Hyeon Hwang', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2506.19697.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#open_source'], 'emoji': '🚀', 'ru': {'title': 'Безопасное предобучение LLM для эффективного квантования', 'desc': 'Статья представляет новый метод предварительного обучения больших языковых моделей (LLM), называемый Outlier-Safe Pre-Training (OSP). OSP предотвращает образование экстремальных выбросов активации, которые ухудшают производительность квантования моделей. Метод включает в себя три ключевые инновации: оптимизатор Muon, однопараметрическую нормализацию RMSNorm и обучаемую проекцию эмбеддингов. Эксперименты показывают, что OSP значительно улучшает производительность 4-битного квантования LLM, делая их более эффективными для развертывания на устройствах.'}, 'en': {'title': 'Preventing Outliers for Better LLM Performance', 'desc': 'This paper presents Outlier-Safe Pre-Training (OSP), a novel approach to enhance the quantization performance of large language models (LLMs) by preventing extreme activation outliers during training. The authors identify that these outliers significantly impair the efficiency of LLMs when deployed on devices, and propose a proactive strategy rather than relying on post-training fixes. OSP incorporates three innovations: the Muon optimizer for efficient training, Single-Scale RMSNorm to control channel-wise amplification, and a learnable embedding projection to manage activation magnitudes. The results show that OSP-trained models achieve superior performance in quantization benchmarks while maintaining low training overhead, indicating that outliers can be effectively managed through improved training techniques.'}, 'zh': {'title': '创新训练技术，提升模型量化性能', 'desc': '本论文提出了一种名为Outlier-Safe Pre-Training（OSP）的新方法，旨在改善大型语言模型（LLM）的量化性能。OSP通过创新的训练技术，主动防止极端激活异常值的形成，从而提高模型在设备上的高效部署。该方法结合了三项关键创新：Muon优化器、单尺度RMSNorm和可学习的嵌入投影，显著降低了训练过程中的异常值。实验结果表明，OSP模型在4位量化下的表现优于传统模型，展示了训练策略对异常值的影响。'}}}, {'id': 'https://huggingface.co/papers/2506.19103', 'title': 'Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency\n  Models', 'url': 'https://huggingface.co/papers/2506.19103', 'abstract': 'A new framework using consistency models enhances image inversion and editing efficiency, achieving top performance with fewer steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the generation process. However, these methods are computationally intensive because of their iterative nature. While distilled diffusion models enable faster inference, their editing capabilities remain limited, primarily because of poor inversion quality. High-fidelity inversion and reconstruction are essential for precise image editing, as they preserve the structural and semantic integrity of the source image. In this work, we propose a novel framework that enhances image inversion using consistency models, enabling high-quality editing in just four steps. Our method introduces a cycle-consistency optimization strategy that significantly improves reconstruction accuracy and enables a controllable trade-off between editability and content preservation. We achieve state-of-the-art performance across various image editing tasks and datasets, demonstrating that our method matches or surpasses full-step diffusion models while being substantially more efficient. The code of our method is available on GitHub at https://github.com/ControlGenAI/Inverse-and-Edit.', 'score': 30, 'issue_id': 4503, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': '2bce24e10f3bf399', 'authors': ['Ilia Beletskii', 'Andrey Kuznetsov', 'Aibek Alanov'], 'affiliations': ['AIRI', 'HSE University', 'Innopolis', 'Sber'], 'pdf_title_img': 'assets/pdf/title_img/2506.19103.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#open_source', '#training', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Эффективное редактирование изображений: быстро и качественно', 'desc': 'Авторы предлагают новую систему для улучшения инверсии и редактирования изображений с использованием моделей согласованности. Эта система позволяет достичь высококачественного редактирования всего за четыре шага. Метод вводит стратегию оптимизации цикличной согласованности, которая значительно улучшает точность реконструкции. Достигнуты передовые результаты в различных задачах редактирования изображений, превосходящие полношаговые диффузионные модели при существенно большей эффективности.'}, 'en': {'title': 'Efficient Image Editing with Consistency Models', 'desc': 'This paper presents a new framework that improves image inversion and editing by using consistency models. The proposed method allows for high-quality image editing in just four steps, making it much more efficient than traditional diffusion models. It employs a cycle-consistency optimization strategy to enhance reconstruction accuracy while balancing editability and content preservation. The results show that this approach achieves state-of-the-art performance in various image editing tasks, outperforming existing methods in both speed and quality.'}, 'zh': {'title': '高效图像编辑的新框架', 'desc': '本文提出了一种新的框架，利用一致性模型来增强图像反演和编辑的效率。该方法通过循环一致性优化策略，提高了重建的准确性，使得高质量的图像编辑只需四个步骤。与传统的扩散模型相比，我们的方法在多个图像编辑任务中表现出色，能够在保持内容完整性的同时，实现更高的可编辑性。我们的研究表明，这种方法在效率上显著优于全步骤的扩散模型。'}}}, {'id': 'https://huggingface.co/papers/2506.20512', 'title': 'OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling', 'url': 'https://huggingface.co/papers/2506.20512', 'abstract': 'Investigating mid-training strategies reveals that high-quality mathematical corpora and well-formatted chain-of-thought reasoning examples enhance reinforcement learning performance in language models, leading to the development of OctoThinker.  \t\t\t\t\tAI-generated summary \t\t\t\t Different base language model families, such as Llama and Qwen, exhibit divergent behaviors during post-training with reinforcement learning (RL), especially on reasoning-intensive tasks. What makes a base language model suitable for reinforcement learning? Gaining deeper insight into this question is essential for developing RL-scalable foundation models of the next generation. In this work, we investigate how mid-training strategies shape RL dynamics, focusing on two representative model families: Qwen and Llama. Our study reveals that (1) high-quality mathematical corpora, such as MegaMath-Web-Pro, significantly improve both base model and RL performance, while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further adding QA-style data, particularly long chain-of-thought (CoT) reasoning examples, enhances RL outcomes, and instruction data further unlocks this effect; (3) while long-CoT improves reasoning depth, it can also induce verbosity of model responses and unstability of RL training, underscoring the importance of data formatting; (4) scaling mid-training consistently leads to stronger downstream RL performance. Building on these insights, we introduce a two-stage mid-training strategy, Stable-then-Decay, in which base models are first trained on 200B tokens with a constant learning rate, followed by 20B tokens across three CoT-focused branches with learning rate decay. This yields OctoThinker, a family of models demonstrating strong RL compatibility and closing the performance gap with more RL-friendly model families, i.e., Qwen. We hope our work will help shape pre-training strategies for foundation models in the RL era. To support further research, we release our open-source models along with a curated math reasoning-intensive corpus of over 70 billion tokens (i.e., MegaMath-Web-Pro-Max).', 'score': 20, 'issue_id': 4500, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '67515964a17f77dc', 'authors': ['Zengzhi Wang', 'Fan Zhou', 'Xuefeng Li', 'Pengfei Liu'], 'affiliations': ['Shanghai Jiao Tong University, SII, GAIR Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.20512.jpg', 'data': {'categories': ['#rl', '#training', '#open_source', '#optimization', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Улучшение языковых моделей через оптимизацию промежуточного обучения', 'desc': 'Исследование стратегий промежуточного обучения показывает, что высококачественные математические корпусы и хорошо отформатированные примеры рассуждений по цепочке улучшают производительность обучения с подкреплением в языковых моделях. Различные семейства базовых языковых моделей, такие как Llama и Qwen, демонстрируют разное поведение при пост-обучении с помощью обучения с подкреплением, особенно на задачах, требующих интенсивных рассуждений. На основе этих выводов авторы представляют двухэтапную стратегию промежуточного обучения, названную Stable-then-Decay, которая привела к созданию семейства моделей OctoThinker. Эти модели демонстрируют сильную совместимость с обучением с подкреплением и сокращают разрыв в производительности с более дружественными к RL семействами моделей.'}, 'en': {'title': 'Enhancing RL Performance with Strategic Mid-Training', 'desc': "This paper explores how mid-training strategies can improve reinforcement learning (RL) performance in language models, specifically focusing on the Qwen and Llama families. It finds that using high-quality mathematical datasets and well-structured chain-of-thought reasoning examples significantly enhances the models' reasoning capabilities. The authors introduce a two-stage mid-training approach called Stable-then-Decay, which optimizes learning rates to improve RL outcomes. The resulting model, OctoThinker, shows improved compatibility with RL tasks, bridging the performance gap with other models designed for RL."}, 'zh': {'title': '提升语言模型的强化学习性能', 'desc': '本研究探讨了中期训练策略如何影响强化学习（RL）在语言模型中的表现，特别是针对推理密集型任务。我们发现高质量的数学语料库和格式良好的链式推理示例显著提升了模型的RL性能。通过引入稳定-衰减的两阶段中期训练策略，我们开发了OctoThinker模型系列，展示了与其他RL友好模型的竞争力。我们的工作旨在为基础模型的预训练策略提供指导，特别是在强化学习时代。'}}}, {'id': 'https://huggingface.co/papers/2506.16012', 'title': 'DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware\n  Planning', 'url': 'https://huggingface.co/papers/2506.16012', 'abstract': 'A simulator named DualTHOR for training dual-arm humanoid robots integrates real-world assets and physics to enhance the robustness and generalization of Vision Language Models.  \t\t\t\t\tAI-generated summary \t\t\t\t Developing embodied agents capable of performing complex interactive tasks in real-world scenarios remains a fundamental challenge in embodied AI. Although recent advances in simulation platforms have greatly enhanced task diversity to train embodied Vision Language Models (VLMs), most platforms rely on simplified robot morphologies and bypass the stochastic nature of low-level execution, which limits their transferability to real-world robots. To address these issues, we present a physics-based simulation platform DualTHOR for complex dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our simulator includes real-world robot assets, a task suite for dual-arm collaboration, and inverse kinematics solvers for humanoid robots. We also introduce a contingency mechanism that incorporates potential failures through physics-based low-level execution, bridging the gap to real-world scenarios. Our simulator enables a more comprehensive evaluation of the robustness and generalization of VLMs in household environments. Extensive evaluations reveal that current VLMs struggle with dual-arm coordination and exhibit limited robustness in realistic environments with contingencies, highlighting the importance of using our simulator to develop more capable VLMs for embodied tasks. The code is available at https://github.com/ds199895/DualTHOR.git.', 'score': 17, 'issue_id': 4499, 'pub_date': '2025-06-19', 'pub_date_card': {'ru': '19 июня', 'en': 'June 19', 'zh': '6月19日'}, 'hash': '6a45067f4e822638', 'authors': ['Boyu Li', 'Siyuan He', 'Hang Xu', 'Haoqi Yuan', 'Yu Zang', 'Liwei Hu', 'Junpeng Yue', 'Zhenxiong Jiang', 'Pengbo Hu', 'Börje F. Karlsson', 'Yehui Tang', 'Zongqing Lu'], 'affiliations': ['AgiBot', 'Beijing Academy of Artificial Intelligence', 'BeingBeyond', 'Institute of Automation, Chinese Academy of Sciences', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'School of Computer Science, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.16012.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#games', '#robotics', '#agents'], 'emoji': '🤖', 'ru': {'title': 'DualTHOR: Реалистичное обучение роботов в виртуальной среде', 'desc': 'DualTHOR - это симулятор для обучения двуруких гуманоидных роботов, интегрирующий реальные активы и физику для улучшения устойчивости и обобщения моделей компьютерного зрения и языка (VLM). Он включает реалистичные модели роботов, набор задач для сотрудничества двух рук и решатели обратной кинематики. Симулятор вводит механизм непредвиденных ситуаций через физическое выполнение низкого уровня, приближая модели к реальным сценариям. Оценки показывают, что современные VLM испытывают трудности с координацией двух рук и имеют ограниченную устойчивость в реалистичных средах.'}, 'en': {'title': 'Enhancing Dual-Arm Robots with DualTHOR Simulator', 'desc': 'The paper introduces DualTHOR, a physics-based simulator designed for training dual-arm humanoid robots. It enhances Vision Language Models (VLMs) by integrating real-world assets and accounting for the complexities of low-level execution. This simulator addresses the limitations of existing platforms by incorporating a task suite for dual-arm collaboration and a contingency mechanism for potential failures. The findings indicate that current VLMs face challenges in dual-arm coordination and robustness, emphasizing the need for advanced simulation tools like DualTHOR to improve performance in real-world tasks.'}, 'zh': {'title': 'DualTHOR：提升双臂机器人任务能力的仿真平台', 'desc': 'DualTHOR是一个用于训练双臂类人机器人的物理仿真平台，旨在提高视觉语言模型的鲁棒性和泛化能力。该平台结合了真实世界的机器人资产和任务套件，支持双臂协作，并引入了逆向运动学求解器。通过模拟低级执行中的潜在失败，DualTHOR能够更好地反映现实场景中的复杂性。研究表明，现有的视觉语言模型在双臂协调和应对现实环境中的突发情况时表现不佳，因此使用DualTHOR进行训练至关重要。'}}}, {'id': 'https://huggingface.co/papers/2506.18088', 'title': 'RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain\n  Randomization for Robust Bimanual Robotic Manipulation', 'url': 'https://huggingface.co/papers/2506.18088', 'abstract': 'RoboTwin 2.0 is a scalable simulation framework for bimanual robotic manipulation that uses expert data synthesis and structured domain randomization to generate diverse and realistic synthetic data, improving sim-to-real transfer and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical results show a 10.9% gain in code generation success and improved generalization to novel real-world scenarios. A VLA model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained solely on our synthetic data achieve a 228% relative gain, highlighting strong generalization without real-world supervision. We release the data generator, benchmark, dataset, and code to support scalable research in robust bimanual manipulation.', 'score': 10, 'issue_id': 4499, 'pub_date': '2025-06-22', 'pub_date_card': {'ru': '22 июня', 'en': 'June 22', 'zh': '6月22日'}, 'hash': '92b73795da81e648', 'authors': ['Tianxing Chen', 'Zanxin Chen', 'Baijun Chen', 'Zijian Cai', 'Yibin Liu', 'Qiwei Liang', 'Zixuan Li', 'Xianliang Lin', 'Yiheng Ge', 'Zhenyu Gu', 'Weiliang Deng', 'Yubin Guo', 'Tian Nian', 'Xuanbing Xie', 'Qiangyu Chen', 'Kailun Su', 'Tianling Xu', 'Guodong Liu', 'Mengkang Hu', 'Huan-ang Gao', 'Kaixuan Wang', 'Zhixuan Liang', 'Yusen Qin', 'Xiaokang Yang', 'Ping Luo', 'Yao Mu'], 'affiliations': ['CSU', 'D-Robotics', 'FDU', 'HKU MMLab', 'HKU-SH ICRC', 'Lumina EAI', 'NEU', 'NJU', 'SJTU ScaleLab', 'SUSTech', 'SYSU', 'SZU', 'Shanghai AI Lab', 'THU', 'TeleAI', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2506.18088.jpg', 'data': {'categories': ['#transfer_learning', '#synthetic', '#dataset', '#benchmark', '#optimization', '#data', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Виртуальный близнец для обучения роботов-манипуляторов', 'desc': 'RoboTwin 2.0 - это масштабируемая система симуляции для бимануальной роботизированной манипуляции. Она использует синтез экспертных данных и структурированную доменную рандомизацию для создания разнообразных и реалистичных синтетических данных. Система включает в себя большую библиотеку объектов RoboTwin-OD и конвейер синтеза данных на основе мультимодальных языковых моделей. RoboTwin 2.0 значительно улучшает перенос из симуляции в реальность и обобщение для задач бимануальной манипуляции.'}, 'en': {'title': 'Enhancing Bimanual Robot Manipulation with RoboTwin 2.0', 'desc': "RoboTwin 2.0 is a new simulation framework designed to improve how robots manipulate objects with both hands. It addresses the challenges of generating diverse and realistic synthetic data by using expert data synthesis and structured domain randomization. This framework creates a large-scale object library and employs advanced language models to automatically generate task execution code. The results show significant improvements in the robots' ability to perform tasks in real-world scenarios, demonstrating enhanced generalization and robustness."}, 'zh': {'title': 'RoboTwin 2.0：提升双手机器人操作的仿真框架', 'desc': 'RoboTwin 2.0 是一个可扩展的双手机器人操作仿真框架，旨在通过专家数据合成和结构化领域随机化生成多样且真实的合成数据，从而提高仿真到现实的转移和泛化能力。该框架解决了现有合成数据集在双手操作中面临的挑战，包括高效的数据生成方法和复杂的仿真环境。RoboTwin 2.0 结合了多模态大语言模型和仿真循环优化，自动生成任务执行代码，并通过五个维度的领域随机化增强数据的多样性和策略的鲁棒性。实验结果表明，该框架在代码生成成功率和对新场景的泛化能力上均有显著提升，支持双手操作的可扩展研究。'}}}, {'id': 'https://huggingface.co/papers/2506.18315', 'title': 'Use Property-Based Testing to Bridge LLM Code Generation and Validation', 'url': 'https://huggingface.co/papers/2506.18315', 'abstract': 'A novel framework using Property-Based Testing and collaborative LLM-based agents improves code generation correctness and generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the "cycle of self-deception" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.', 'score': 8, 'issue_id': 4499, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': 'b53df58acbb9720b', 'authors': ['Lehan He', 'Zeren Chen', 'Zhe Zhang', 'Jing Shao', 'Xiang Gao', 'Lu Sheng'], 'affiliations': ['School of Software, Beihang University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.18315.jpg', 'data': {'categories': ['#plp', '#benchmark', '#optimization', '#training', '#agents'], 'emoji': '🧪', 'ru': {'title': 'Улучшение генерации кода с помощью Property-Based Testing и LLM-агентов', 'desc': 'Эта статья представляет новый фреймворк под названием Property-Generated Solver для улучшения генерации кода с помощью больших языковых моделей (LLM). Фреймворк использует Property-Based Testing (PBT) для проверки высокоуровневых свойств программ вместо конкретных примеров ввода-вывода. Два агента на основе LLM - Generator и Tester - сотрудничают для создания и итеративного улучшения кода. Экспериментальные результаты показывают значительное улучшение корректности и обобщаемости сгенерированного кода по сравнению с традиционными методами разработки через тестирование (TDD).'}, 'en': {'title': 'Enhancing Code Generation with Property-Based Testing and LLM Collaboration', 'desc': 'This paper presents a new framework called Property-Generated Solver that enhances the correctness and generalization of code generated by Large Language Models (LLMs). It utilizes Property-Based Testing (PBT) to validate high-level program properties instead of relying solely on traditional test cases, which can be flawed or biased. The framework consists of two collaborative LLM agents: a Generator for creating and refining code, and a Tester that oversees the PBT process and provides feedback based on property violations. Experimental results show that this approach significantly improves the accuracy of code generation compared to conventional Test-Driven Development methods.'}, 'zh': {'title': '基于属性的测试提升代码生成的正确性', 'desc': '这篇论文提出了一种新的框架，利用基于属性的测试（PBT）和协作的基于大型语言模型（LLM）的代理来提高代码生成的正确性和泛化能力。传统的测试驱动开发（TDD）在处理LLM生成的代码时常常面临高质量测试用例稀缺的问题，而PBT通过验证高层程序属性来解决这一挑战。该框架包括两个协作的LLM代理：一个用于代码生成和迭代改进，另一个负责管理PBT生命周期并提供有意义的反馈。实验结果表明，该方法在多个代码生成基准上显著提高了通过率，优于传统的TDD方法。'}}}, {'id': 'https://huggingface.co/papers/2506.20544', 'title': 'When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs', 'url': 'https://huggingface.co/papers/2506.20544', 'abstract': 'The study examines and proposes new sampling and selection strategies to enhance inference-time compute for multilingual and multi-task large language models, demonstrating significant improvements in win-rates across various languages and tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in large language models (LLMs) have shifted focus toward scaling inference-time compute, improving performance without retraining the model. A common approach is to sample multiple outputs in parallel, and select one of these as the final output. However, work to date has focused on English and a handful of domains such as math and code. In contrast, we are most interested in techniques that generalize across open-ended tasks, formally verifiable tasks, and across languages. In this work, we study how to robustly scale inference-time compute for open-ended generative tasks in a multilingual, multi-task setting.   Our findings show that both sampling strategy based on temperature variation and selection strategy must be adapted to account for diverse domains and varied language settings. We evaluate existing selection methods, revealing that strategies effective in English often fail to generalize across languages. We propose novel sampling and selection strategies specifically adapted for multilingual and multi-task inference scenarios, and show they yield notable gains across languages and tasks. In particular, our combined sampling and selection methods lead to an average +6.8 jump in win-rates for our 8B models on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At larger scale, Command-A (111B model) equipped with our methods, shows +9.0 improvement in win-rates on the same benchmark with just five samples against single-sample decoding, a substantial increase at minimal cost. Our results underscore the need for language- and task-aware approaches to inference-time compute, aiming to democratize performance improvements in underrepresented languages.', 'score': 6, 'issue_id': 4497, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '8c4af7dcfe82a334', 'authors': ['Ammar Khairi', "Daniel D'souza", 'Ye Shen', 'Julia Kreutzer', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.20544.jpg', 'data': {'categories': ['#multilingual', '#inference', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'Повышение эффективности многоязычных LLM через оптимизацию вывода', 'desc': 'Исследование посвящено новым стратегиям выборки и отбора для улучшения вычислений во время вывода многоязычных и многозадачных больших языковых моделей (LLM). Авторы предлагают методы, адаптированные для различных языков и задач, которые показывают значительное улучшение результатов. Особое внимание уделяется обобщению на открытые генеративные задачи в многоязычном контексте. Результаты демонстрируют необходимость учета специфики языков и задач при оптимизации вычислений LLM на этапе вывода.'}, 'en': {'title': 'Enhancing Multilingual Performance in Large Language Models', 'desc': 'This paper explores new methods for sampling and selecting outputs from large language models (LLMs) during inference to improve their performance across multiple languages and tasks. The authors highlight that traditional strategies often focus on English and specific domains, which limits their effectiveness in diverse settings. They propose innovative sampling techniques that adjust for temperature variations and selection methods tailored to different languages and tasks. The results demonstrate significant improvements in win-rates, showcasing the importance of adapting inference strategies to enhance multilingual and multi-task capabilities of LLMs.'}, 'zh': {'title': '提升多语言模型推理效率的新策略', 'desc': '本研究探讨并提出了新的采样和选择策略，以增强多语言和多任务大语言模型的推理时间计算效率。我们发现，基于温度变化的采样策略和选择策略必须适应不同领域和语言环境。通过评估现有的选择方法，我们揭示了在英语中有效的策略往往无法跨语言推广。我们提出的创新方法在多语言和多任务推理场景中表现出显著的性能提升，特别是在m-ArenaHard-v2.0基准测试中，8B模型的胜率平均提高了6.8个百分点。'}}}, {'id': 'https://huggingface.co/papers/2506.20452', 'title': 'HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based\n  Diffusion Sampling', 'url': 'https://huggingface.co/papers/2506.20452', 'abstract': "HiWave enhances ultra-high-resolution image synthesis using pretrained diffusion models through a two-stage pipeline involving DDIM inversion and wavelet-based detail enhancement, improving visual fidelity and reducing artifacts.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have emerged as the leading approach for image synthesis, demonstrating exceptional photorealism and diversity. However, training diffusion models at high resolutions remains computationally prohibitive, and existing zero-shot generation techniques for synthesizing images beyond training resolutions often produce artifacts, including object duplication and spatial incoherence. In this paper, we introduce HiWave, a training-free, zero-shot approach that substantially enhances visual fidelity and structural coherence in ultra-high-resolution image synthesis using pretrained diffusion models. Our method employs a two-stage pipeline: generating a base image from the pretrained model followed by a patch-wise DDIM inversion step and a novel wavelet-based detail enhancer module. Specifically, we first utilize inversion methods to derive initial noise vectors that preserve global coherence from the base image. Subsequently, during sampling, our wavelet-domain detail enhancer retains low-frequency components from the base image to ensure structural consistency, while selectively guiding high-frequency components to enrich fine details and textures. Extensive evaluations using Stable Diffusion XL demonstrate that HiWave effectively mitigates common visual artifacts seen in prior methods, achieving superior perceptual quality. A user study confirmed HiWave's performance, where it was preferred over the state-of-the-art alternative in more than 80% of comparisons, highlighting its effectiveness for high-quality, ultra-high-resolution image synthesis without requiring retraining or architectural modifications.", 'score': 6, 'issue_id': 4499, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '470af96a76c911d8', 'authors': ['Tobias Vontobel', 'Seyedmorteza Sadat', 'Farnood Salehi', 'Romann M. Weber'], 'affiliations': ['Disney Research Studios, Switzerland', 'ETH Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2506.20452.jpg', 'data': {'categories': ['#dataset', '#diffusion', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'HiWave: Революция в синтезе изображений сверхвысокого разрешения', 'desc': 'HiWave - это метод улучшения синтеза изображений сверхвысокого разрешения с использованием предобученных диффузионных моделей. Он включает двухэтапный процесс: инверсию DDIM и улучшение деталей на основе вейвлетов. HiWave позволяет повысить визуальное качество и уменьшить артефакты при генерации изображений. Метод не требует дополнительного обучения модели и работает в режиме zero-shot.'}, 'en': {'title': 'HiWave: Elevating Ultra-High-Resolution Image Synthesis with Pretrained Diffusion Models', 'desc': 'HiWave is a novel approach for enhancing ultra-high-resolution image synthesis using pretrained diffusion models without the need for retraining. It employs a two-stage pipeline that first generates a base image and then applies DDIM inversion and a wavelet-based detail enhancement technique. This method improves visual fidelity by preserving global coherence and enriching fine details, effectively reducing common artifacts like object duplication. Evaluations show that HiWave outperforms existing methods, achieving superior perceptual quality in image synthesis.'}, 'zh': {'title': 'HiWave：超高分辨率图像合成的新突破', 'desc': 'HiWave是一种增强超高分辨率图像合成的技术，利用预训练的扩散模型，通过两阶段的流程来实现。首先，它从预训练模型生成基础图像，然后进行基于波形的小块DDIM反演步骤和细节增强。该方法有效提高了视觉保真度和结构一致性，减少了常见的视觉伪影。通过对Stable Diffusion XL的广泛评估，HiWave在用户研究中表现优异，超过80%的比较中优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.20495', 'title': 'ReCode: Updating Code API Knowledge with Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.20495', 'abstract': "ReCode, a rule-based reinforcement learning framework, enhances large language models' adaptation to API updates without compromising their general code generation capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit remarkable code generation capabilities but falter when adapting to frequent updates in external library APIs. This critical limitation, stemming from reliance on outdated API knowledge from their training data, even with access to current documentation, impedes reliable code generation in dynamic environments. To tackle this issue, we propose ReCode (rule-based Reinforcement learning for Code Update), a novel framework that mimics human programmer adaptation to API changes. Specifically, we construct a dataset of approximately 2,000 data entries to train the LLMs to perform version migration based on updated information. Then, we introduce a modified string similarity metric for code evaluation as the reward for reinforcement learning. Our experiments demonstrate that ReCode substantially boosts LLMs' code generation performance in dynamic API scenarios, especially on the unseen CodeUpdateArena task. Crucially, compared to supervised fine-tuning, ReCode has less impact on LLMs' general code generation abilities. We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and DAPO), all achieving consistent improvements. Notably, after training, Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned model and the reasoning model with the same architecture. Code is available at https://github.com/zjunlp/ReCode.", 'score': 5, 'issue_id': 4500, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '22cd91ad69753ea2', 'authors': ['Haoze Wu', 'Yunzhi Yao', 'Wenhao Yu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['Tencent AI, Seattle Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.20495.jpg', 'data': {'categories': ['#rl', '#training', '#rlhf', '#optimization', '#dataset', '#reasoning'], 'emoji': '🔄', 'ru': {'title': 'ReCode: Адаптация языковых моделей к изменениям API без потери универсальности', 'desc': 'Предложена новая система ReCode для улучшения адаптации больших языковых моделей (LLM) к обновлениям API без ущерба для общих способностей генерации кода. ReCode использует обучение с подкреплением на основе правил, имитируя адаптацию программистов к изменениям API. Система обучается на наборе данных из 2000 примеров миграции версий и использует модифицированную метрику сходства строк для оценки кода в качестве награды. Эксперименты показывают значительное улучшение производительности LLM при работе с динамическими API, особенно на новых задачах CodeUpdateArena.'}, 'en': {'title': 'Adapting Code Generation with ReCode: Reinforcement Learning for API Updates', 'desc': "ReCode is a rule-based reinforcement learning framework designed to improve large language models' (LLMs) ability to adapt to changes in external library APIs while maintaining their general code generation skills. The framework addresses the challenge that LLMs face due to outdated API knowledge, which hinders their performance in dynamic coding environments. By creating a dataset of around 2,000 entries for training and implementing a modified string similarity metric for evaluating code, ReCode effectively enhances the LLMs' adaptation capabilities. Experimental results show that ReCode significantly improves code generation performance in scenarios with frequent API updates, outperforming traditional supervised fine-tuning methods."}, 'zh': {'title': 'ReCode：提升代码生成与API适应能力的创新框架', 'desc': 'ReCode是一个基于规则的强化学习框架，旨在提高大型语言模型（LLMs）对API更新的适应能力，同时保持其代码生成的通用性。该框架通过构建一个包含约2000个数据条目的数据集，训练LLMs进行版本迁移，以应对动态环境中的API变化。我们还引入了一种修改过的字符串相似度度量作为强化学习的奖励，以评估代码的质量。实验结果表明，ReCode显著提升了LLMs在动态API场景下的代码生成性能，尤其是在未见过的CodeUpdateArena任务上。'}}}, {'id': 'https://huggingface.co/papers/2506.18674', 'title': 'Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?', 'url': 'https://huggingface.co/papers/2506.18674', 'abstract': 'Optimizing tokenizers for chatbot conversations reduces computational costs and energy usage with minimal impact on training corpus performance.  \t\t\t\t\tAI-generated summary \t\t\t\t The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.', 'score': 5, 'issue_id': 4497, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': '8daec89854af9338', 'authors': ['Raquel Ferrando', 'Javier Conde', 'Gonzalo Martínez', 'Pedro Reviriego'], 'affiliations': ['ETSI de Telecomunicación Universidad Politécnica de Madrid 28040 Madrid, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2506.18674.jpg', 'data': {'categories': ['#training', '#optimization', '#data'], 'emoji': '🤖', 'ru': {'title': 'Экономия энергии через оптимизацию токенизации для чат-ботов', 'desc': 'Исследование показывает, что оптимизация токенизаторов для чат-ботов может снизить вычислительные затраты и энергопотребление на 5-10%. Авторы перестроили словари токенизаторов, используя корпус диалогов с чат-ботами. Оптимизированные токенизаторы сократили количество токенов в диалогах, при этом минимально влияя на эффективность для оригинального обучающего корпуса. Это важно, так как токенизация - ключевой фактор эффективности больших языковых моделей (LLM).'}, 'en': {'title': 'Optimize Tokenizers, Save Energy!', 'desc': 'This paper explores the optimization of tokenizers specifically for chatbot conversations to enhance efficiency and reduce costs. It highlights that the computational expense of Large Language Models (LLMs) is closely tied to the number of tokens processed, making tokenizer performance crucial. By redesigning vocabularies based on a corpus of chatbot dialogues, the study demonstrates that conversation-optimized tokenizers can decrease token counts by 5% to 10%. Importantly, this optimization has little to no negative effect on the performance of the original training corpus, suggesting a dual benefit of energy savings and maintained efficiency.'}, 'zh': {'title': '优化分词器，节省能源与成本', 'desc': '本论文探讨了为聊天机器人对话优化分词器的潜在好处。随着大型语言模型（LLMs）的广泛应用，计算和能源成本急剧上升，而分词器在模型效率中扮演着重要角色。研究表明，针对聊天对话优化的分词器能够显著减少对话中的标记数量，从而节省5%到10%的能源消耗，同时对原始训练语料的标记化效率影响较小或略有正面效果。通过重新设计分词器的词汇，本文展示了在聊天机器人领域优化分词器的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.20331', 'title': 'Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining\n  and Extracting Rare and Hidden Content', 'url': 'https://huggingface.co/papers/2506.20331', 'abstract': 'A biomedical text dataset, constructed from PubMed, uses a two-stage annotation process involving large and small language models to fine-tune and extract subsets for clinical NLP, improving pretraining efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies.', 'score': 3, 'issue_id': 4501, 'pub_date': '2025-06-25', 'pub_date_card': {'ru': '25 июня', 'en': 'June 25', 'zh': '6月25日'}, 'hash': '3c4ae1c9ac9b2325', 'authors': ['Rian Touchent', 'Nathan Godey', 'Eric de la Clergerie'], 'affiliations': ['Sorbonne Université INRIA Paris'], 'pdf_title_img': 'assets/pdf/title_img/2506.20331.jpg', 'data': {'categories': ['#dataset', '#open_source', '#science', '#training', '#data', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'Biomed-Enriched: Улучшение предобучения языковых моделей для биомедицины', 'desc': 'Представлен новый биомедицинский текстовый датасет Biomed-Enriched, созданный на основе PubMed с использованием двухэтапного процесса аннотации. На первом этапе большая языковая модель (LLM) аннотирует 400 тысяч абзацев, присваивая им оценки по типу, домену и образовательному качеству. Затем эти аннотации используются для дообучения малой языковой модели (МЯМ), которая распространяет метки на весь корпус PMC-OA. Полученные метаданные позволяют извлекать уточненные подмножества, включая 2 миллиона абзацев с клиническими случаями, что делает датасет ценным ресурсом для биомедицинской и клинической обработки естественного языка.'}, 'en': {'title': 'Enhancing Clinical NLP with Biomed-Enriched Dataset', 'desc': 'The paper presents Biomed-Enriched, a biomedical text dataset derived from PubMed using a two-stage annotation process. Initially, a large language model annotates 400,000 paragraphs, categorizing them by type, domain, and educational quality. This annotated data is then used to fine-tune a smaller language model, which helps in labeling the entire PMC-OA corpus. The resulting dataset, which includes high-quality clinical case paragraphs, enhances the efficiency of pretraining and improves performance in clinical NLP tasks.'}, 'zh': {'title': '生物医学文本数据集的高效构建与应用', 'desc': '本文介绍了一个名为Biomed-Enriched的生物医学文本数据集，该数据集来源于PubMed，并采用了两阶段的注释过程。第一阶段使用大型语言模型对40万段PubMed科学文章进行注释，评估其类型、领域和教育质量。通过对小型语言模型的微调，进一步传播这些标签，从而提取出高质量的临床案例段落。该数据集为生物医学和临床自然语言处理提供了一个开放的资源，能够提高预训练的效率和性能。'}}}, {'id': 'https://huggingface.co/papers/2506.19502', 'title': 'MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility\n  Applications', 'url': 'https://huggingface.co/papers/2506.19502', 'abstract': "MATE, a multimodal accessibility multi-agent system, converts data into understandable formats based on user needs, supporting various disabilities and integrating with institutional technologies.  \t\t\t\t\tAI-generated summary \t\t\t\t Accessibility remains a critical concern in today's society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user's needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at https://github.com/AlgazinovAleksandr/Multi-Agent-MATE.", 'score': 2, 'issue_id': 4494, 'pub_date': '2025-06-24', 'pub_date_card': {'ru': '24 июня', 'en': 'June 24', 'zh': '6月24日'}, 'hash': '85d844ff6061cc93', 'authors': ['Aleksandr Algazinov', 'Matt Laing', 'Paul Laban'], 'affiliations': ['Dept. of Comp. Sci. & Tech. Tsinghua University Beijing, China', 'Dept. of Psych. & Cog. Sci. Tsinghua University Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.19502.jpg', 'data': {'categories': ['#healthcare', '#agents', '#multimodal', '#ethics', '#open_source'], 'emoji': '♿', 'ru': {'title': 'MATE: Интеллектуальная система для преодоления барьеров доступности', 'desc': 'MATE - это мультимодальная система мультиагентов для обеспечения доступности, которая преобразует данные в понятные форматы в зависимости от потребностей пользователя. Система поддерживает различные виды инвалидности и может интегрироваться с институциональными технологиями. MATE использует широкий спектр моделей, от вызовов API больших языковых моделей до пользовательских классификаторов машинного обучения. Система включает в себя ModCon-Task-Identifier - модель, способную точно определять задачу преобразования модальности из пользовательского ввода.'}, 'en': {'title': 'Empowering Accessibility Through Intelligent Data Conversion', 'desc': 'MATE is a multimodal accessibility multi-agent system designed to convert data into formats that are understandable for users with disabilities. It addresses the limitations of existing multi-agent systems by providing customizable solutions that adapt to individual user needs. The system can perform tasks like converting images to audio descriptions, making digital content more accessible. Additionally, MATE integrates with institutional technologies and ensures user privacy by running locally, while its ModCon-Task-Identifier model excels in identifying specific modality conversion tasks.'}, 'zh': {'title': 'MATE：为每个人提供无障碍的智能助手', 'desc': 'MATE是一个多模态无障碍多代理系统，旨在根据用户需求将数据转换为可理解的格式，以支持各种残疾人士。该系统通过执行模态转换，帮助用户更好地与数字环境互动，例如将图像转换为音频描述，以满足视觉障碍者的需求。MATE具有灵活性，支持多种模型和硬件，确保能够适应不同的用户需求，并保护敏感信息的隐私和安全。通过与机构技术的有效集成，MATE能够提供实时的用户支持，提升无障碍服务的质量。'}}}, {'id': 'https://huggingface.co/papers/2506.18403', 'title': 'The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs', 'url': 'https://huggingface.co/papers/2506.18403', 'abstract': 'The Debugging Decay Index (DDI) quantifies and optimizes the effectiveness of iterative AI debugging by predicting intervention points to revive and enhance debugging capability.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.', 'score': 2, 'issue_id': 4494, 'pub_date': '2025-06-23', 'pub_date_card': {'ru': '23 июня', 'en': 'June 23', 'zh': '6月23日'}, 'hash': '30aa788d94afe45d', 'authors': ['Muntasir Adnan', 'Carlos C. N. Kuhn'], 'affiliations': ['Open Source Institute, University of Canberra, Bruce, Canberra, Australia'], 'pdf_title_img': 'assets/pdf/title_img/2506.18403.jpg', 'data': {'categories': ['#optimization', '#training', '#math'], 'emoji': '🔍', 'ru': {'title': 'Оптимизация отладки ИИ: измерение и преодоление затухания эффективности', 'desc': 'Статья представляет Индекс Затухания Отладки (DDI), который количественно оценивает эффективность итеративной отладки ИИ. Авторы обнаружили, что способность моделей к отладке быстро снижается, следуя экспоненциальному паттерну затухания. DDI позволяет предсказать оптимальные точки вмешательства для восстановления эффективности отладки. Предложенный подход стратегического свежего старта демонстрирует, что своевременные вмешательства могут значительно улучшить процесс отладки кода.'}, 'en': {'title': 'Reviving AI Debugging with Strategic Interventions', 'desc': 'The Debugging Decay Index (DDI) is a new method that measures how quickly AI debugging abilities decline over time. It shows that most AI models lose a significant portion of their debugging skills after just a few attempts, which is a major issue for systems that generate code. By using DDI, developers can identify the best moments to intervene and improve the debugging process, shifting from refining existing solutions to exploring new ones. This approach not only highlights a key limitation in current AI debugging methods but also offers a way to enhance the effectiveness of iterative code generation.'}, 'zh': {'title': '优化AI调试的关键：调试衰减指数', 'desc': '调试衰减指数（DDI）量化并优化了迭代AI调试的有效性，通过预测干预点来恢复和增强调试能力。研究表明，AI调试的有效性遵循可预测的指数衰减模式，大多数模型在仅仅2-3次尝试后就会失去60-80%的调试能力。我们提出的DDI框架可以量化调试失效的时机，并预测干预点，从而在调试过程中实现从利用到探索的战略转变。DDI揭示了当前AI调试的基本局限性，并提供了优化迭代代码生成策略的首个定量框架。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (3)', '#agi', '#alignment', '#architecture', '#audio', '#benchmark (2)', '#cv (4)', '#data (3)', '#dataset (6)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (2)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (1)', '#multilingual (1)', '#multimodal (2)', '#open_source (6)', '#optimization (8)', '#plp (1)', '#rag', '#reasoning (2)', '#rl (2)', '#rlhf (1)', '#robotics (2)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (2)', '#training (8)', '#transfer_learning (2)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-26 16:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-26 16:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-26 16:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    