
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. July 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 июля</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-24.html">⬅️ <span id="prev-date">24.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-28.html">➡️ <span id="next-date">28.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 июля', 'en': 'July 25', 'zh': '7月25日'};
        let feedDateNext = {'ru': '28.07', 'en': '07/28', 'zh': '7月28日'};
        let feedDatePrev = {'ru': '24.07', 'en': '07/24', 'zh': '7月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.13546', 'title': 'nablaNABLA: Neighborhood Adaptive Block-Level Attention', 'url': 'https://huggingface.co/papers/2507.13546', 'abstract': "NABLA, a dynamic block-level attention mechanism, improves video diffusion transformers by enhancing computational efficiency without sacrificing generative quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in transformer-based architectures has demonstrated remarkable success in video generation tasks. However, the quadratic complexity of full attention mechanisms remains a critical bottleneck, particularly for high-resolution and long-duration video sequences. In this paper, we propose NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that dynamically adapts to sparsity patterns in video diffusion transformers (DiTs). By leveraging block-wise attention with adaptive sparsity-driven threshold, NABLA reduces computational overhead while preserving generative quality. Our method does not require custom low-level operator design and can be seamlessly integrated with PyTorch's Flex Attention operator. Experiments demonstrate that NABLA achieves up to 2.7x faster training and inference compared to baseline almost without compromising quantitative metrics (CLIP score, VBench score, human evaluation score) and visual quality drop. The code and model weights are available here: https://github.com/gen-ai-team/Wan2.1-NABLA", 'score': 57, 'issue_id': 5019, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 июля', 'en': 'July 17', 'zh': '7月17日'}, 'hash': '4a4705d5ba2d25e6', 'authors': ['Dmitrii Mikhailov', 'Aleksey Letunovskiy', 'Maria Kovaleva', 'Vladimir Arkhipkin', 'Vladimir Korviakov', 'Vladimir Polovnikov', 'Viacheslav Vasilev', 'Evelina Sidorova', 'Denis Dimitrov'], 'affiliations': ['Artificial Intelligence Research Institute (AIRI), Moscow, Russia', 'Lomonosov Moscow State University (MSU), Moscow, Russia', 'Moscow Institute of Physics and Technology (MIPT), Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.13546.jpg', 'data': {'categories': ['#open_source', '#training', '#video', '#optimization', '#diffusion', '#architecture'], 'emoji': '🎬', 'ru': {'title': 'NABLA: Умное внимание для быстрой генерации видео', 'desc': 'NABLA - это новый механизм внимания для видео-диффузионных трансформеров, который адаптивно работает на уровне блоков. Он позволяет значительно ускорить обучение и инференс моделей без существенной потери качества генерации. NABLA использует пороговое значение для определения важных связей между блоками, что снижает вычислительную сложность. Метод легко интегрируется в существующие архитектуры и показывает ускорение до 2.7 раз по сравнению с базовыми моделями.'}, 'en': {'title': 'NABLA: Speeding Up Video Generation with Smart Attention', 'desc': 'This paper introduces NABLA, a new attention mechanism designed to enhance video diffusion transformers by improving their computational efficiency. NABLA utilizes a Neighborhood Adaptive Block-Level Attention approach that adjusts to the sparsity patterns found in video data, allowing for faster processing without losing quality. The method significantly reduces the computational burden associated with traditional full attention mechanisms, especially for high-resolution and lengthy video sequences. Experimental results show that NABLA can accelerate training and inference times by up to 2.7 times while maintaining high generative quality as measured by various evaluation metrics.'}, 'zh': {'title': 'NABLA：提升视频生成效率的创新机制', 'desc': 'NABLA是一种动态块级注意力机制，旨在提高视频扩散变换器的计算效率，同时保持生成质量。传统的全注意力机制在处理高分辨率和长时序视频时，计算复杂度过高，成为瓶颈。NABLA通过邻域自适应块级注意力，动态适应视频中的稀疏模式，从而减少计算开销。实验结果表明，NABLA在训练和推理速度上比基线快2.7倍，同时几乎不影响生成质量和评估指标。'}}}, {'id': 'https://huggingface.co/papers/2507.18071', 'title': 'Group Sequence Policy Optimization', 'url': 'https://huggingface.co/papers/2507.18071', 'abstract': 'This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.', 'score': 42, 'issue_id': 5014, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '0eb25e9d4ec9527e', 'authors': ['Chujie Zheng', 'Shixuan Liu', 'Mingze Li', 'Xiong-Hui Chen', 'Bowen Yu', 'Chang Gao', 'Kai Dang', 'Yuqiong Liu', 'Rui Men', 'An Yang', 'Jingren Zhou', 'Junyang Lin'], 'affiliations': ['Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2507.18071.jpg', 'data': {'categories': ['#rl', '#training', '#optimization', '#games'], 'emoji': '🚀', 'ru': {'title': 'GSPO: Эффективное обучение с подкреплением для языковых моделей на уровне последовательностей', 'desc': 'Статья представляет новый алгоритм обучения с подкреплением для больших языковых моделей - Group Sequence Policy Optimization (GSPO). В отличие от предыдущих подходов, GSPO использует коэффициент важности на уровне последовательностей, а не отдельных токенов. Авторы демонстрируют, что GSPO превосходит алгоритм GRPO по эффективности обучения и производительности. Кроме того, GSPO стабилизирует обучение с подкреплением для моделей Mixture-of-Experts и упрощает инфраструктуру для RL.'}, 'en': {'title': 'Revolutionizing RL Training with GSPO for Language Models', 'desc': 'This paper presents Group Sequence Policy Optimization (GSPO), a new reinforcement learning algorithm designed for training large language models. GSPO improves upon previous methods by using sequence likelihood to define importance ratios, rather than focusing on individual tokens. It also incorporates sequence-level clipping and optimization, which enhances training stability and efficiency. The results show that GSPO outperforms the GRPO algorithm and significantly benefits the training of Mixture-of-Experts models, leading to advancements in the Qwen3 models.'}, 'zh': {'title': '群序列策略优化：提升强化学习效率的创新算法', 'desc': '本文介绍了一种新的强化学习算法，称为群序列策略优化（GSPO），旨在高效且稳定地训练大型语言模型。与以往基于令牌级重要性比率的算法不同，GSPO基于序列的似然性定义重要性比率，并进行序列级的裁剪、奖励和优化。研究表明，GSPO在训练效率和性能上优于GRPO算法，特别是在混合专家（MoE）强化学习训练中表现出色，并有潜力简化强化学习基础设施的设计。GSPO的这些优点为最新的Qwen3模型带来了显著的改进。'}}}, {'id': 'https://huggingface.co/papers/2507.14958', 'title': 'MUR: Momentum Uncertainty guided Reasoning for Large Language Models', 'url': 'https://huggingface.co/papers/2507.14958', 'abstract': 'Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have achieved impressive performance on reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it often leads to overthinking, wasting tokens on redundant computations. This work investigates how to efficiently and adaptively guide LLM test-time scaling without additional training. Inspired by the concept of momentum in physics, we propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically allocates thinking budgets to critical reasoning steps by tracking and aggregating stepwise uncertainty over time. To support flexible inference-time control, we introduce gamma-control, a simple mechanism that tunes the reasoning budget via a single hyperparameter. We provide in-depth theoretical proof to support the superiority of MUR in terms of stability and biases. MUR is comprehensively evaluated against various TTS methods across four challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate that MUR reduces computation by over 50% on average while improving accuracy by 0.62-3.37%.', 'score': 27, 'issue_id': 5011, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': 'bf11cf5df484123c', 'authors': ['Hang Yan', 'Fangzhi Xu', 'Rongman Xu', 'Yifei Li', 'Jian Zhang', 'Haoran Luo', 'Xiaobao Wu', 'Luu Anh Tuan', 'Haiteng Zhao', 'Qika Lin', 'Jun Liu'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Peking University', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.14958.jpg', 'data': {'categories': ['#inference', '#reasoning', '#benchmark', '#architecture', '#training', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективные рассуждения в LLM: меньше вычислений, выше точность', 'desc': 'Статья представляет метод Momentum Uncertainty-guided Reasoning (MUR) для оптимизации рассуждений в больших языковых моделях (LLM). MUR динамически распределяет вычислительные ресурсы на критические этапы рассуждений, отслеживая неопределенность на каждом шаге. Метод включает механизм гамма-контроля для настройки бюджета рассуждений через один гиперпараметр. Эксперименты показали, что MUR снижает вычислительные затраты более чем на 50% при одновременном повышении точности на 0.62-3.37%.'}, 'en': {'title': 'Optimize Reasoning Budgets with MUR!', 'desc': 'Momentum Uncertainty-guided Reasoning (MUR) is a novel approach that enhances the efficiency of Large Language Models (LLMs) during inference by optimizing their reasoning budgets. It addresses the challenge of overthinking in LLMs, which can lead to unnecessary computations and wasted tokens. By utilizing a concept from physics, MUR tracks and aggregates uncertainty over time to dynamically allocate reasoning resources to the most critical steps. The method is validated through extensive testing on various benchmarks, showing significant reductions in computation and improvements in accuracy compared to existing methods.'}, 'zh': {'title': '动量不确定性引导推理：优化推理效率的创新方法', 'desc': '本文提出了一种名为动量不确定性引导推理（MUR）的方法，旨在动态优化大型语言模型（LLM）在推理过程中的计算预算。MUR通过跟踪和聚合逐步的不确定性，灵活地分配思考预算，从而提高推理效率并减少冗余计算。该方法引入了伽马控制机制，通过一个超参数调节推理预算，支持灵活的推理时间控制。实验结果表明，MUR在多个基准测试中显著降低了计算量，同时提高了推理准确性。'}}}, {'id': 'https://huggingface.co/papers/2507.15758', 'title': 'LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization', 'url': 'https://huggingface.co/papers/2507.15758', 'abstract': "Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\% while improving accuracy by 2.3\\%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.", 'score': 23, 'issue_id': 5006, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'ce0e5d782e94f5ee', 'authors': ['Xingyu Wu', 'Yuchen Yan', 'Shangke Lyu', 'Linjuan Wu', 'Yiwen Qiu', 'Yongliang Shen', 'Weiming Lu', 'Jian Shao', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15758.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#math', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Эффективные рассуждения: меньше слов, больше смысла', 'desc': 'Статья представляет новый подход к оптимизации длины рассуждений в больших языковых моделях под названием LAPO (Length-Adaptive Policy Optimization). Этот метод использует двухэтапный процесс обучения с подкреплением, чтобы модели могли самостоятельно определять оптимальную глубину рассуждений. LAPO позволяет сократить использование токенов на 40.9%, одновременно повышая точность на 2.3% на задачах математических рассуждений. Исследование показывает, что модели, обученные с помощью LAPO, развивают способность эффективно распределять вычислительные ресурсы в зависимости от сложности задачи.'}, 'en': {'title': 'Smart Reasoning: Less is More with LAPO', 'desc': 'This paper introduces Length-Adaptive Policy Optimization (LAPO), a new method for improving reasoning models in machine learning. LAPO allows models to learn how much reasoning is needed for different problems, rather than setting strict limits on reasoning length. It uses a two-stage reinforcement learning process where models first learn successful reasoning patterns and then apply this knowledge during inference. The results show that LAPO can significantly reduce the number of tokens used while also increasing the accuracy of the models.'}, 'zh': {'title': '智能推理，灵活控制！', 'desc': '本文提出了一种新的框架，称为长度自适应策略优化（LAPO），旨在改善推理模型的效率。LAPO通过强化学习的两阶段过程，使模型能够内在理解适当的推理深度，从而减少不必要的计算。第一阶段，模型学习成功解决方案长度的统计分布；第二阶段，模型将这些模式作为元认知指导，嵌入推理上下文中。实验结果表明，LAPO在数学推理基准测试中减少了多达40.9%的标记使用，同时提高了2.3%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2507.18634', 'title': 'Captain Cinema: Towards Short Movie Generation', 'url': 'https://huggingface.co/papers/2507.18634', 'abstract': 'Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Captain Cinema, a generation framework for short movie generation. Given a detailed textual description of a movie storyline, our approach firstly generates a sequence of keyframes that outline the entire narrative, which ensures long-range coherence in both the storyline and visual appearance (e.g., scenes and characters). We refer to this step as top-down keyframe planning. These keyframes then serve as conditioning signals for a video synthesis model, which supports long context learning, to produce the spatio-temporal dynamics between them. This step is referred to as bottom-up video synthesis. To support stable and efficient generation of multi-scene long narrative cinematic works, we introduce an interleaved training strategy for Multimodal Diffusion Transformers (MM-DiT), specifically adapted for long-context video data. Our model is trained on a specially curated cinematic dataset consisting of interleaved data pairs. Our experiments demonstrate that Captain Cinema performs favorably in the automated creation of visually coherent and narrative consistent short movies in high quality and efficiency. Project page: https://thecinema.ai', 'score': 16, 'issue_id': 5006, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': 'a52dc2cc9f063733', 'authors': ['Junfei Xiao', 'Ceyuan Yang', 'Lvmin Zhang', 'Shengqu Cai', 'Yang Zhao', 'Yuwei Guo', 'Gordon Wetzstein', 'Maneesh Agrawala', 'Alan Yuille', 'Lu Jiang'], 'affiliations': ['ByteDance Seed', 'CUHK Project Lead', 'Johns Hopkins University', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.18634.jpg', 'data': {'categories': ['#diffusion', '#video', '#long_context', '#story_generation', '#multimodal', '#dataset'], 'emoji': '🎬', 'ru': {'title': 'От текста к кино: ИИ-режиссер Captain Cinema', 'desc': 'Captain Cinema - это система генерации коротких фильмов на основе текстовых описаний. Она использует двухэтапный подход: сначала планирование ключевых кадров сверху вниз, а затем синтез видео снизу вверх. Для обучения применяются мультимодальные диффузионные трансформеры с чередующейся стратегией обучения. Система демонстрирует хорошие результаты в создании визуально согласованных и нарративно последовательных коротких фильмов высокого качества.'}, 'en': {'title': 'Transforming Text to Cinematic Reality with Captain Cinema', 'desc': "Captain Cinema is a framework designed to create short movies from text descriptions. It uses a two-step process: first, it generates keyframes that outline the movie's storyline, ensuring that the visuals and narrative are coherent. Next, it synthesizes video by connecting these keyframes, allowing for smooth transitions and dynamic scenes. The model employs Multimodal Diffusion Transformers with a unique training strategy to enhance the quality and efficiency of the movie generation process."}, 'zh': {'title': '用文本描述生成高质量短片的创新框架', 'desc': 'Captain Cinema 是一个短片生成框架，可以根据文本描述生成高质量的短电影。首先，它通过自上而下的关键帧规划生成一系列关键帧，以确保故事情节和视觉外观的一致性。接着，这些关键帧作为条件信号，供视频合成模型使用，从而生成场景之间的时空动态。我们还引入了一种交错训练策略，专门为长上下文视频数据设计的多模态扩散变换器，以支持多场景长叙事电影的稳定高效生成。'}}}, {'id': 'https://huggingface.co/papers/2507.18537', 'title': 'TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation', 'url': 'https://huggingface.co/papers/2507.18537', 'abstract': "TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VAR's hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at https://github.com/ali-vilab/TTS-VAR.", 'score': 10, 'issue_id': 5005, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': 'acf3b32f27e7342b', 'authors': ['Zhekai Chen', 'Ruihang Chu', 'Yukang Chen', 'Shiwei Zhang', 'Yujie Wei', 'Yingya Zhang', 'Xihui Liu'], 'affiliations': ['CUHK', 'HKU MMLab', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.18537.jpg', 'data': {'categories': ['#cv', '#training', '#optimization'], 'emoji': '🖼️', 'ru': {'title': 'Умное масштабирование для лучшей генерации изображений', 'desc': 'TTS-VAR - это новая система масштабирования визуальных авторегрессионных моделей во время тестирования. Она улучшает качество генерации изображений, динамически регулируя размеры батчей и применяя методы кластеризации и ресэмплинга. TTS-VAR моделирует процесс генерации как задачу поиска пути, балансируя вычислительную эффективность и способность к исследованию. Эксперименты показали значительное улучшение оценки GenEval на 8.7% для модели Infinity.'}, 'en': {'title': 'Dynamic Scaling for Enhanced Visual Generation Quality', 'desc': 'TTS-VAR is a novel framework designed to enhance the quality of visual auto-regressive (VAR) models during the generation process. It introduces a dynamic batch size adjustment strategy that balances computational efficiency with the ability to explore diverse outputs. The framework employs clustering techniques at coarse scales to maintain structural diversity and resampling methods at fine scales to prioritize high-potential candidates based on their generation history. Experiments demonstrate a significant improvement in generation quality, highlighting the importance of early-stage features in the overall output.'}, 'zh': {'title': '动态调整，提升生成质量的创新框架', 'desc': 'TTS-VAR是一个用于视觉自回归模型的测试时间缩放框架，通过动态调整批量大小和使用聚类与重采样技术来提高生成质量。该框架将生成过程建模为路径搜索问题，旨在平衡计算效率与探索能力。它在粗尺度上采用基于聚类的多样性搜索，以保留结构多样性，并在细尺度上通过重采样优先选择潜在的优质样本。实验结果表明，TTS-VAR在强大的VAR模型Infinity上实现了8.7%的GenEval分数提升，显示出早期结构特征对最终质量的有效影响。'}}}, {'id': 'https://huggingface.co/papers/2507.15844', 'title': 'Hierarchical Budget Policy Optimization for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2507.15844', 'abstract': 'Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.', 'score': 9, 'issue_id': 5006, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': 'c5c45ecb7b33f3cb', 'authors': ['Shangke Lyu', 'Linjuan Wu', 'Yuchen Yan', 'Xingyu Wu', 'Hao Li', 'Yongliang Shen', 'Peisheng Jiang', 'Weiming Lu', 'Jun Xiao', 'Yueting Zhuang'], 'affiliations': ['SF Technology', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.15844.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'Адаптивные рассуждения: эффективность без потери возможностей', 'desc': 'Статья представляет метод Hierarchical Budget Policy Optimization (HBPO) для оптимизации глубины рассуждений в больших языковых моделях. HBPO использует иерархическое исследование бюджета токенов и дифференцированные механизмы вознаграждения, чтобы модели могли адаптировать глубину рассуждений к сложности задачи. Эксперименты показывают, что HBPO снижает использование токенов на 60.6% при одновременном повышении точности на 3.14% на четырех эталонных тестах. Метод позволяет оптимизировать как эффективность, так и способности моделей машинного обучения.'}, 'en': {'title': 'Optimizing Reasoning Efficiency with Adaptive Depth', 'desc': 'This paper introduces Hierarchical Budget Policy Optimization (HBPO), a new reinforcement learning framework designed to improve the efficiency of reasoning models. HBPO allows models to adapt their reasoning depth based on the complexity of the problem, rather than using a one-size-fits-all approach. By implementing hierarchical budget exploration and differentiated reward mechanisms, the framework encourages models to allocate resources effectively while maintaining their performance. The results show that HBPO can significantly reduce token usage and enhance accuracy, demonstrating that efficiency and capability can be optimized together.'}, 'zh': {'title': '高效推理，能力与效率并存', 'desc': '本文提出了一种名为层次预算策略优化（HBPO）的强化学习框架，旨在提高推理模型的计算效率。HBPO通过分层预算探索，将样本分成多个具有不同令牌预算的子组，从而实现高效的资源分配。该方法引入了差异化奖励机制，使模型能够根据问题复杂性调整推理深度，避免了长输出长度的惩罚对模型的负面影响。实验结果表明，HBPO在四个推理基准上平均减少了60.6%的令牌使用，同时提高了3.14%的准确率，证明了推理效率与能力可以同时优化。'}}}, {'id': 'https://huggingface.co/papers/2507.16535', 'title': 'EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion', 'url': 'https://huggingface.co/papers/2507.16535', 'abstract': "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/", 'score': 8, 'issue_id': 5005, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '7ee137161bbe047e', 'authors': ['Shang Liu', 'Chenjie Cao', 'Chaohui Yu', 'Wen Qian', 'Jing Wang', 'Fan Wang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Fudan University', 'Hupan Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.16535.jpg', 'data': {'categories': ['#diffusion', '#architecture', '#synthetic', '#3d', '#dataset'], 'emoji': '🌎', 'ru': {'title': 'EarthCrafter: революция в широкомасштабном 3D-моделировании Земли', 'desc': 'Статья представляет Aerial-Earth3D - крупнейший набор данных 3D аэросъемки, охватывающий 50 тысяч сцен по всей территории США. На основе этих данных разработан фреймворк EarthCrafter для широкомасштабной генерации 3D-моделей земной поверхности с использованием разреженной латентной диффузии. Архитектура EarthCrafter разделяет генерацию структуры и текстуры, применяя сжатие геометрии и текстур в компактные латентные пространства. Модель демонстрирует превосходные результаты в генерации крупномасштабных 3D-сцен и поддерживает различные приложения, сохраняя географическую достоверность.'}, 'en': {'title': 'Revolutionizing Large-Scale 3D Earth Generation', 'desc': "This paper presents a solution to the challenge of generating large-scale 3D models of Earth's surface by introducing a new dataset and a novel model architecture. The Aerial-Earth3D dataset is the largest of its kind, containing 50,000 scenes with detailed annotations that support diverse terrain representation. The EarthCrafter framework utilizes a dual approach with sparse-decoupled latent diffusion, allowing for efficient generation of 3D structures and textures while managing computational costs. The results show significant improvements in generating realistic and plausible large-scale 3D environments, with applications in urban planning and terrain synthesis."}, 'zh': {'title': '大规模3D地球生成的新突破', 'desc': '尽管最近的3D生成技术取得了显著进展，但将这些方法扩展到大规模地理范围仍然是一个挑战。我们通过数据基础设施和模型架构的双重创新来解决这个问题。我们介绍了Aerial-Earth3D，这是迄今为止最大的3D航空数据集，包含50,000个场景，支持大规模的3D地球生成。基于此，我们提出了EarthCrafter框架，通过稀疏解耦的潜在扩散技术，实现了高效的地形生成。'}}}, {'id': 'https://huggingface.co/papers/2507.18464', 'title': 'DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts', 'url': 'https://huggingface.co/papers/2507.18464', 'abstract': "DriftMoE, an online Mixture-of-Experts architecture with a compact neural router, achieves competitive results in adapting to concept drift in data streams through a symbiotic learning loop.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using a multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with a clear training signal while accelerating expert specialization. We evaluate DriftMoE's performance across nine state-of-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-of-the-art stream learning adaptive ensembles, offering a principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.", 'score': 5, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '8de2367db0f263c3', 'authors': ['Miguel Aspis', 'Sebastián A. Cajas Ordónez', 'Andrés L. Suárez-Cetrulo', 'Ricardo Simón Carbajo'], 'affiliations': ['Irelands Centre for Applied AI (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9, Ireland', 'Irelands National Centre for Artificial Intelligence (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9'], 'pdf_title_img': 'assets/pdf/title_img/2507.18464.jpg', 'data': {'categories': ['#open_source', '#architecture', '#data', '#optimization', '#benchmark'], 'emoji': '🌊', 'ru': {'title': 'Адаптивное онлайн-обучение с DriftMoE: укрощение потока данных', 'desc': 'DriftMoE - это новая архитектура онлайн-обучения на основе смеси экспертов для адаптации к концептуальному дрейфу в потоковых данных. Она использует компактный нейронный маршрутизатор, который обучается совместно с пулом инкрементальных деревьев Хефдинга. Ключевая инновация заключается в симбиотическом цикле обучения, который позволяет экспертам специализироваться, а маршрутизатору - улучшать выбор экспертов. DriftMoE показывает конкурентоспособные результаты на девяти современных бенчмарках потокового обучения.'}, 'en': {'title': "Adapting to Change: DriftMoE's Smart Learning Loop", 'desc': 'DriftMoE is an innovative online Mixture-of-Experts architecture designed to effectively handle concept drift in data streams. It utilizes a compact neural router that works in tandem with incremental Hoeffding tree experts, allowing for real-time adaptation to changing data patterns. The architecture features a symbiotic learning loop where the router selects the best expert for predictions, and experts update their knowledge based on true labels, enhancing their specialization. This approach not only improves prediction accuracy but also maintains resource efficiency, making DriftMoE a competitive solution in the realm of adaptive ensemble methods.'}, 'zh': {'title': 'DriftMoE：高效应对概念漂移的在线专家架构', 'desc': 'DriftMoE是一种在线混合专家架构，能够有效应对数据流中的概念漂移。它通过一种新颖的共训练框架，结合紧凑的神经路由器和增量Hoeffding树专家，实现了专家的专业化选择。该架构的关键创新在于其共生学习循环，使得路由器能够选择最合适的专家进行预测，并通过真实标签的增量更新来提升专家的性能。实验结果表明，DriftMoE在多个数据流学习基准上表现出色，提供了一种高效的概念漂移适应方法。'}}}, {'id': 'https://huggingface.co/papers/2507.18013', 'title': 'Technical Report of TeleChat2, TeleChat2.5 and T1', 'url': 'https://huggingface.co/papers/2507.18013', 'abstract': "The TeleChat2, TeleChat2.5, and T1 models enhance language capabilities through advanced training strategies, including Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning, achieving superior performance in reasoning and speed compared to previous models.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce the latest series of TeleChat models: TeleChat2, TeleChat2.5, and T1, offering a significant upgrade over their predecessor, TeleChat. Despite minimal changes to the model architecture, the new series achieves substantial performance gains through enhanced training strategies in both pre-training and post-training stages. The series begins with TeleChat2, which undergoes pretraining on 10 trillion high-quality and diverse tokens. This is followed by Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to further enhance its capabilities. TeleChat2.5 and T1 expand the pipeline by incorporating a continual pretraining phase with domain-specific datasets, combined with reinforcement learning (RL) to improve performance in code generation and mathematical reasoning tasks. The T1 variant is designed for complex reasoning, supporting long Chain-of-Thought (CoT) reasoning and demonstrating substantial improvements in mathematics and coding. In contrast, TeleChat2.5 prioritizes speed, delivering rapid inference. Both flagship models of T1 and TeleChat2.5 are dense Transformer-based architectures with 115B parameters, showcasing significant advancements in reasoning and general task performance compared to the original TeleChat. Notably, T1-115B outperform proprietary models such as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2, TeleChat2.5 and T1, including post-trained versions with 35B and 115B parameters, to empower developers and researchers with state-of-the-art language models tailored for diverse applications.", 'score': 4, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '5ea71fd57b80ec78', 'authors': ['Zihan Wang', 'Xinzhang Liu', 'Yitong Yao', 'Chao Wang', 'Yu Zhao', 'Zhihao Yang', 'Wenmin Deng', 'Kaipeng Jia', 'Jiaxin Peng', 'Yuyao Huang', 'Sishi Xiong', 'Zhuo Jiang', 'Kaidong Yu', 'Xiaohui Hu', 'Fubei Yao', 'Ruiyu Fang', 'Zhuoru Jiang', 'Ruiting Song', 'Qiyi Xie', 'Rui Xue', 'Xuewei He', 'Yanlei Xue', 'Zhu Yuan', 'Zhaoxi Zhang', 'Zilu Huang', 'Shiquan Wang', 'Xin Wang', 'Hanming Wu', 'Mingyuan Wang', 'Xufeng Zhan', 'Yuhan Sun', 'Zhaohu Xing', 'Yuhao Jiang', 'Bingkai Yang', 'Shuangyong Song', 'Yongxiang Li', 'Zhongjiang He', 'Xuelong Li'], 'affiliations': ['China Telecom'], 'pdf_title_img': 'assets/pdf/title_img/2507.18013.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#open_source', '#architecture', '#training', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'TeleChat: новое поколение языковых моделей с улучшенными способностями рассуждения', 'desc': 'Новая серия моделей TeleChat (TeleChat2, TeleChat2.5 и T1) демонстрирует значительное улучшение производительности по сравнению с предшественниками. Улучшения достигнуты благодаря усовершенствованным стратегиям обучения, включая предварительное обучение на 10 триллионах токенов, Supervised Fine-Tuning и Direct Preference Optimization. Модели TeleChat2.5 и T1 дополнительно используют непрерывное предварительное обучение на специализированных датасетах и обучение с подкреплением для улучшения генерации кода и математических рассуждений. Флагманские модели T1 и TeleChat2.5 с 115 миллиардами параметров превосходят проприетарные модели, такие как OpenAI o1-mini и GPT-4o, в задачах рассуждения и общей производительности.'}, 'en': {'title': 'Revolutionizing Language Models with TeleChat Series', 'desc': "The TeleChat2, TeleChat2.5, and T1 models represent a significant advancement in language processing capabilities through innovative training techniques. These models utilize Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning to enhance their performance in reasoning and speed. TeleChat2 is pretrained on a vast dataset of 10 trillion tokens, while TeleChat2.5 and T1 incorporate continual pretraining with domain-specific data to excel in tasks like code generation and mathematical reasoning. The T1 model is particularly designed for complex reasoning tasks, demonstrating notable improvements over previous models, including proprietary ones like OpenAI's GPT-4o."}, 'zh': {'title': '提升语言能力的先进模型系列', 'desc': 'TeleChat2、TeleChat2.5和T1模型通过先进的训练策略提升了语言能力，包括监督微调、直接偏好优化和强化学习。这些模型在推理和速度方面相较于之前的模型表现更为出色。TeleChat2在10万亿高质量多样化的标记上进行预训练，随后通过监督微调和直接偏好优化进一步增强能力。T1模型专注于复杂推理，支持长链思维推理，并在数学和编码任务上显示出显著的改进。'}}}, {'id': 'https://huggingface.co/papers/2507.14988', 'title': 'DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis', 'url': 'https://huggingface.co/papers/2507.14988', 'abstract': 'DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at https://dmospeech2.github.io/.', 'score': 4, 'issue_id': 5005, 'pub_date': '2025-07-20', 'pub_date_card': {'ru': '20 июля', 'en': 'July 20', 'zh': '7月20日'}, 'hash': '27df664b6cc093b4', 'authors': ['Yinghao Aaron Li', 'Xilin Jiang', 'Fei Tao', 'Cheng Niu', 'Kaifeng Xu', 'Juntong Song', 'Nima Mesgarani'], 'affiliations': ['Columbia University', 'NewsBreak'], 'pdf_title_img': 'assets/pdf/title_img/2507.14988.jpg', 'data': {'categories': ['#diffusion', '#training', '#rl', '#audio', '#optimization'], 'emoji': '🎙️', 'ru': {'title': 'Оптимизация синтеза речи на всех уровнях', 'desc': 'DMOSpeech 2 - это улучшенная система синтеза речи, оптимизирующая предсказание длительности звуков с помощью обучения с подкреплением. Система использует новый подход к политике длительности, основанный на групповой оптимизации относительных предпочтений (GRPO) с использованием схожести голоса диктора и уровня ошибок распознавания слов в качестве сигналов вознаграждения. DMOSpeech 2 также вводит метод выборки с учительским руководством, что улучшает разнообразие выходных данных при сохранении эффективности. Комплексные оценки показывают превосходную производительность по всем метрикам по сравнению с предыдущими системами.'}, 'en': {'title': 'Optimizing Duration for Diverse Speech Synthesis', 'desc': 'DMOSpeech 2 enhances speech synthesis by optimizing duration prediction, which was previously unaddressed. It employs a reinforcement learning strategy with group relative preference optimization (GRPO) to improve the duration predictor using metrics like speaker similarity and word error rate. Additionally, the paper introduces teacher-guided sampling, which combines a teacher model for initial processing with a student model for efficiency, leading to greater output diversity. Overall, DMOSpeech 2 achieves superior performance in speech synthesis while reducing the number of sampling steps needed, marking a significant advancement in metric-optimized TTS systems.'}, 'zh': {'title': '优化语音合成，提升多样性与效率', 'desc': 'DMOSpeech 2 是一种优化语音合成中持续时间预测的新方法，采用了强化学习的策略来提升合成效果。该系统引入了基于组相对偏好的优化框架，利用说话者相似性和词错误率作为奖励信号，从而优化了之前未优化的持续时间预测组件。除此之外，DMOSpeech 2 还采用了教师引导采样的方法，通过教师模型进行初步去噪，再转向学生模型，从而显著提高了输出的多样性。综合评估结果显示，该系统在各项指标上均优于之前的系统，同时将采样步骤减少了一半，且没有降低质量。'}}}, {'id': 'https://huggingface.co/papers/2507.18103', 'title': 'A New Pair of GloVes', 'url': 'https://huggingface.co/papers/2507.18103', 'abstract': 'New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t This report documents, describes, and evaluates new 2024 English GloVe (Global Vectors for Word Representation) models. While the original GloVe models built in 2014 have been widely used and found useful, languages and the world continue to evolve and we thought that current usage could benefit from updated models. Moreover, the 2014 models were not carefully documented as to the exact data versions and preprocessing that were used, and we rectify this by documenting these new models. We trained two sets of word embeddings using Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary comparison, direct testing, and NER tasks shows that the 2024 vectors incorporate new culturally and linguistically relevant words, perform comparably on structural tasks like analogy and similarity, and demonstrate improved performance on recent, temporally dependent NER datasets such as non-Western newswire data.', 'score': 3, 'issue_id': 5010, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '6dc651e0c2dd6898', 'authors': ['Riley Carlson', 'John Bauer', 'Christopher D. Manning'], 'affiliations': ['Stanford NLP Group, Stanford University, 353 Jane Stanford Way, Stanford CA 94305-9035, U.S.A.'], 'pdf_title_img': 'assets/pdf/title_img/2507.18103.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#transfer_learning', '#benchmark'], 'emoji': '🔄', 'ru': {'title': 'Обновленные модели GloVe: шаг вперед в понимании современного языка', 'desc': 'Новые модели GloVe 2024 года улучшают версии 2014 года, включая обновленные наборы данных. Они демонстрируют повышенную производительность в задачах распознавания именованных сущностей, актуальных с культурной и временной точки зрения. Модели обучены на данных из Wikipedia, Gigaword и подмножества Dolma. Оценка показывает, что векторы 2024 года включают новые культурно и лингвистически релевантные слова и лучше справляются с задачами NER на современных данных.'}, 'en': {'title': 'Evolving GloVe: Enhanced Word Vectors for Modern Language Understanding', 'desc': 'The 2024 GloVe models enhance the original 2014 versions by utilizing updated datasets that reflect current language use. These models are specifically designed to improve Named Entity Recognition (NER) tasks, particularly in culturally and temporally relevant contexts. The paper details the training process of the new word embeddings using sources like Wikipedia and Gigaword, ensuring thorough documentation of the data and preprocessing methods. Evaluation results indicate that the new models not only maintain performance on traditional tasks but also excel in recognizing contemporary and diverse entities in language.'}, 'zh': {'title': '更新GloVe模型，提升语言理解能力', 'desc': '2024年的GloVe模型在2014年版本的基础上进行了改进，采用了更新的数据集，并在与文化和时间相关的命名实体识别任务中表现出更好的性能。这些新模型通过使用维基百科、Gigaword和Dolma的子集进行训练，解决了2014年模型在数据版本和预处理方面文档不全的问题。评估结果显示，2024年的词向量在词汇比较、直接测试和命名实体识别任务中表现出色，能够更好地反映当前的语言和文化变化。总的来说，这些改进使得GloVe模型在处理现代语言任务时更加有效。'}}}, {'id': 'https://huggingface.co/papers/2507.18546', 'title': 'GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface', 'url': 'https://huggingface.co/papers/2507.18546', 'abstract': 'GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.', 'score': 2, 'issue_id': 5011, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '1145b7615152d229', 'authors': ['Urchade Zaratiana', 'Gil Pasternak', 'Oliver Boyd', 'George Hurn-Maloney', 'Ash Lewis'], 'affiliations': ['Fastino AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.18546.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#open_source', '#data', '#architecture', '#training'], 'emoji': '🤖', 'ru': {'title': 'Единая модель для многозадачной обработки текста', 'desc': 'GLiNER2 - это унифицированная система для решения нескольких задач обработки естественного языка с помощью одной эффективной трансформерной модели. Она поддерживает распознавание именованных сущностей, классификацию текста и извлечение иерархических структурированных данных. GLiNER2 основана на предобученной архитектуре трансформерного энкодера и обеспечивает эффективность работы на CPU при компактном размере. Эксперименты показывают конкурентоспособную производительность по сравнению с альтернативами на основе больших языковых моделей.'}, 'en': {'title': 'GLiNER2: One Model, Many NLP Tasks!', 'desc': 'GLiNER2 is a versatile framework designed for various natural language processing (NLP) tasks, allowing users to perform named entity recognition, text classification, and data extraction with a single transformer model. This approach reduces the need for multiple specialized models and minimizes the computational demands typically associated with large language models. By utilizing a pretrained transformer encoder, GLiNER2 achieves efficiency in both CPU usage and model size while offering a user-friendly schema-based interface for multi-task operations. The framework has shown strong performance in experiments, making it a practical choice for developers looking for accessible NLP solutions.'}, 'zh': {'title': 'GLiNER2：高效统一的自然语言处理框架', 'desc': 'GLiNER2是一个统一的框架，支持多种自然语言处理任务，使用单一高效的变换器模型，提升了大语言模型的部署可及性。信息提取是许多自然语言处理应用的基础，但现有解决方案通常需要为不同任务设计专门模型，或者依赖计算成本高的大语言模型。GLiNER2增强了原有的GLiNER架构，支持命名实体识别、文本分类和层次结构数据提取，且在一个高效模型中实现多任务组合。我们的实验表明，GLiNER2在提取和分类任务上表现出竞争力，并在部署可及性上相比基于大语言模型的替代方案有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2507.18192', 'title': 'TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance', 'url': 'https://huggingface.co/papers/2507.18192', 'abstract': "TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-image synthesis largely benefit from sophisticated sampling strategies and classifier-free guidance (CFG) to ensure high-quality generation. However, CFG's reliance on two forward passes, especially when combined with intricate sampling algorithms, results in prohibitively high inference costs. To address this, we introduce TeEFusion (Text Embeddings Fusion), a novel and efficient distillation method that directly incorporates the guidance magnitude into the text embeddings and distills the teacher model's complex sampling strategy. By simply fusing conditional and unconditional text embeddings using linear operations, TeEFusion reconstructs the desired guidance without adding extra parameters, simultaneously enabling the student model to learn from the teacher's output produced via its sophisticated sampling approach. Extensive experiments on state-of-the-art models such as SD3 demonstrate that our method allows the student to closely mimic the teacher's performance with a far simpler and more efficient sampling strategy. Consequently, the student model achieves inference speeds up to 6times faster than the teacher model, while maintaining image quality at levels comparable to those obtained through the teacher's complex sampling approach. The code is publicly available at https://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.", 'score': 2, 'issue_id': 5006, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': 'c5028e9aad7b0964', 'authors': ['Minghao Fu', 'Guo-Hua Wang', 'Xiaohao Chen', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['Alibaba International Digital Commerce Group', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'School of Artificial Intelligence, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2507.18192.jpg', 'data': {'categories': ['#optimization', '#training', '#open_source', '#inference', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'TeEFusion: быстрый синтез изображений без потери качества', 'desc': 'TeEFusion - это новый метод дистилляции для синтеза изображений по тексту. Он объединяет условные и безусловные текстовые эмбеддинги, позволяя эффективно применять classifier-free guidance без дополнительных параметров. Метод позволяет ученической модели имитировать сложную стратегию сэмплирования учительской модели, значительно ускоряя инференс. Эксперименты на современных моделях вроде SD3 показывают ускорение до 6 раз при сохранении качества изображений.'}, 'en': {'title': 'TeEFusion: Fast and Efficient Text-to-Image Synthesis', 'desc': 'TeEFusion is a new method that improves text-to-image synthesis by integrating classifier-free guidance directly into text embeddings. This approach reduces the need for multiple forward passes, which are costly in terms of computation, while still producing high-quality images. By fusing conditional and unconditional text embeddings through simple linear operations, TeEFusion allows a student model to learn from a more complex teacher model without increasing the number of parameters. As a result, the student model can generate images up to six times faster than the teacher model, while maintaining similar image quality.'}, 'zh': {'title': 'TeEFusion：高效的文本到图像合成方法', 'desc': 'TeEFusion是一种新颖的文本嵌入融合方法，旨在提高文本到图像合成的效率。它通过将无分类器引导直接融入文本嵌入中，减少了推理成本，同时保持了图像质量。该方法通过线性操作简单地融合条件和无条件文本嵌入，避免了额外参数的增加。实验表明，TeEFusion使得学生模型在推理速度上比教师模型快6倍，同时图像质量与教师模型相当。'}}}, {'id': 'https://huggingface.co/papers/2507.15595', 'title': 'SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging', 'url': 'https://huggingface.co/papers/2507.15595', 'abstract': 'SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Medical image segmentation is crucial for many healthcare tasks, including disease diagnosis and treatment planning. One key area is the segmentation of skin lesions, which is vital for diagnosing skin cancer and monitoring patients. In this context, this paper introduces SegDT, a new segmentation model based on diffusion transformer (DiT). SegDT is designed to work on low-cost hardware and incorporates Rectified Flow, which improves the generation quality at reduced inference steps and maintains the flexibility of standard diffusion models. Our method is evaluated on three benchmarking datasets and compared against several existing works, achieving state-of-the-art results while maintaining fast inference speeds. This makes the proposed model appealing for real-world medical applications. This work advances the performance and capabilities of deep learning models in medical image analysis, enabling faster, more accurate diagnostic tools for healthcare professionals. The code is made publicly available at https://github.com/Bekhouche/SegDT{GitHub}.', 'score': 2, 'issue_id': 5011, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '8ea1ce4e3edb7f0b', 'authors': ['Salah Eddine Bekhouche', 'Gaby Maroun', 'Fadi Dornaika', 'Abdenour Hadid'], 'affiliations': ['IKERBASQUE, Basque Foundation for Science, Bilbao, Spain', 'Sorbonne University Abu Dhabi, Abu Dhabi, UAE', 'University of the Basque Country UPV/EHU, San Sebastian, Spain'], 'pdf_title_img': 'assets/pdf/title_img/2507.15595.jpg', 'data': {'categories': ['#inference', '#cv', '#diffusion', '#healthcare', '#open_source', '#benchmark'], 'emoji': '🔬', 'ru': {'title': 'SegDT: Прорыв в сегментации поражений кожи с помощью диффузионных трансформеров', 'desc': 'Статья представляет SegDT - новую модель сегментации на основе диффузионного трансформера для анализа поражений кожи. SegDT достигает передовых результатов на трех эталонных наборах данных, превосходя существующие методы. Модель использует технику Rectified Flow для улучшения качества генерации при меньшем количестве шагов вывода. SegDT обеспечивает высокую точность и быстродействие, что делает ее перспективной для реальных медицинских приложений.'}, 'en': {'title': 'SegDT: Fast and Accurate Skin Lesion Segmentation for Real-World Healthcare', 'desc': 'SegDT is a novel segmentation model that utilizes a diffusion transformer architecture to enhance the segmentation of skin lesions, which is essential for skin cancer diagnosis. The model incorporates Rectified Flow to improve the quality of generated segmentations while reducing the number of inference steps required. Evaluated on three benchmark datasets, SegDT outperforms existing models, achieving state-of-the-art results with rapid inference speeds. This advancement in deep learning for medical image analysis makes SegDT a practical tool for healthcare professionals in real-world applications.'}, 'zh': {'title': 'SegDT：快速高效的皮肤病变分割模型', 'desc': 'SegDT是一种基于扩散变换器的分割模型，专注于皮肤病变的分割任务。该模型在低成本硬件上运行，具有快速推理速度，适合实际医疗应用。通过引入修正流（Rectified Flow），SegDT在减少推理步骤的同时提高了生成质量。经过在三个基准数据集上的评估，SegDT在多个现有方法中取得了最先进的结果，推动了医疗图像分析中深度学习模型的性能和能力。'}}}, {'id': 'https://huggingface.co/papers/2507.18565', 'title': 'Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age\n  Estimation and Gender Classification for Targeted Advertisement', 'url': 'https://huggingface.co/papers/2507.18565', 'abstract': 'A custom CNN architecture simultaneously classifies age and gender from facial images, improving performance by learning shared representations and achieving high accuracy and low mean absolute error.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a novel deep learning-based approach for simultaneous age and gender classification from facial images, designed to enhance the effectiveness of targeted advertising campaigns. We propose a custom Convolutional Neural Network (CNN) architecture, optimized for both tasks, which leverages the inherent correlation between age and gender information present in facial features. Unlike existing methods that often treat these tasks independently, our model learns shared representations, leading to improved performance. The network is trained on a large, diverse dataset of facial images, carefully pre-processed to ensure robustness against variations in lighting, pose, and image quality. Our experimental results demonstrate a significant improvement in gender classification accuracy, achieving 95%, and a competitive mean absolute error of 5.77 years for age estimation. Critically, we analyze the performance across different age groups, identifying specific challenges in accurately estimating the age of younger individuals. This analysis reveals the need for targeted data augmentation and model refinement to address these biases. Furthermore, we explore the impact of different CNN architectures and hyperparameter settings on the overall performance, providing valuable insights for future research.', 'score': 1, 'issue_id': 5012, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '7ecd81e416dd179d', 'authors': ['Muhammad Imran Zaman', 'Nisar Ahmed'], 'affiliations': ['Department of Computer Science (New Campus), University of Engineering and Technology Lahore, Pakistan', 'Department of Computer Science, COMSATS University Islamabad Lahore Campus, Lahore, Pakistan'], 'pdf_title_img': 'assets/pdf/title_img/2507.18565.jpg', 'data': {'categories': ['#cv', '#dataset', '#architecture', '#ethics', '#optimization', '#training'], 'emoji': '👤', 'ru': {'title': 'Единая CNN для точного определения возраста и пола по лицу', 'desc': 'Статья представляет новый подход к одновременной классификации возраста и пола по изображениям лиц с использованием глубокого обучения. Авторы предлагают специальную архитектуру сверточной нейронной сети (CNN), оптимизированную для обеих задач и использующую корреляцию между возрастными и гендерными признаками. Модель обучается на большом разнообразном наборе данных и демонстрирует точность 95% для классификации пола и среднюю абсолютную ошибку 5,77 лет для оценки возраста. Исследование также выявляет проблемы в точности оценки возраста молодых людей и предлагает пути улучшения модели.'}, 'en': {'title': 'Simultaneous Age and Gender Classification with Custom CNN', 'desc': "This paper introduces a custom Convolutional Neural Network (CNN) designed to classify both age and gender from facial images simultaneously. By learning shared representations, the model improves performance compared to traditional methods that treat these tasks separately. The network is trained on a large dataset, ensuring it can handle variations in lighting and image quality effectively. Results show high accuracy in gender classification and a low mean absolute error for age estimation, highlighting the model's potential for applications like targeted advertising."}, 'zh': {'title': '面部图像的年龄与性别双重分类新方法', 'desc': '本文提出了一种新颖的深度学习方法，通过面部图像同时分类年龄和性别，旨在提高针对性广告活动的效果。我们设计了一种定制的卷积神经网络（CNN）架构，优化了这两个任务，利用面部特征中年龄和性别信息的内在关联。与现有方法通常独立处理这些任务不同，我们的模型学习共享表示，从而提高了性能。实验结果显示，性别分类准确率达到95%，年龄估计的平均绝对误差为5.77岁，表明该方法在不同年龄组的表现具有显著改进。'}}}, {'id': 'https://huggingface.co/papers/2507.16802', 'title': 'Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning', 'url': 'https://huggingface.co/papers/2507.16802', 'abstract': 'The Agentar-Fin-R1 series of financial large language models enhances reasoning, reliability, and domain specialization through a trustworthiness assurance framework and achieves state-of-the-art performance on financial and general reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.', 'score': 1, 'issue_id': 5014, 'pub_date': '2025-07-22', 'pub_date_card': {'ru': '22 июля', 'en': 'July 22', 'zh': '7月22日'}, 'hash': '227d9ddcf1258003', 'authors': ['Yanjun Zheng', 'Xiyang Du', 'Longfei Liao', 'Xiaoke Zhao', 'Zhaowen Zhou', 'Jingze Song', 'Bo Zhang', 'Jiawei Liu', 'Xiang Qi', 'Zhe Li', 'Zhiqiang Zhang', 'Wei Wang', 'Peng Zhang'], 'affiliations': ['@antgroup.com'], 'pdf_title_img': 'assets/pdf/title_img/2507.16802.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#multimodal', '#training', '#optimization', '#science', '#agents', '#reasoning'], 'emoji': '💹', 'ru': {'title': 'Надежные языковые модели для финансовых задач', 'desc': 'Исследователи представили серию финансовых языковых моделей Agentar-Fin-R1, разработанных на основе модели Qwen3. Эти модели оптимизированы для улучшения рассуждений, надежности и специализации в финансовой сфере с помощью многоуровневой системы обеспечения достоверности. Agentar-Fin-R1 достигает передовых результатов как на финансовых, так и на общих тестах рассуждений. Авторы также предложили новый бенчмарк Finova для оценки финансовых рассуждений и проверки соответствия нормативным требованиям.'}, 'en': {'title': 'Empowering Financial Intelligence with Trustworthy LLMs', 'desc': 'The Agentar-Fin-R1 series of financial large language models (LLMs) is designed to improve reasoning, reliability, and specialization in financial contexts. It utilizes a trustworthiness assurance framework that includes high-quality knowledge engineering and rigorous data validation. By employing a systematic labeling system and a two-stage training pipeline, these models achieve enhanced training efficiency and performance. The models have been evaluated on various financial and general reasoning benchmarks, demonstrating state-of-the-art results and strong capabilities for real-world financial applications.'}, 'zh': {'title': '金融领域的信任与推理新标准', 'desc': 'Agentar-Fin-R1系列金融大型语言模型通过信任保障框架增强了推理能力、可靠性和领域专业化，达到了金融和一般推理任务的最先进性能。这些模型基于Qwen3基础模型设计，旨在解决现有模型在复杂推理和信任标准方面的局限性。我们采用高质量的金融任务标签系统和多层次的信任保障框架，优化了模型的训练效率。实验结果表明，Agentar-Fin-R1在金融任务上表现出色，同时在一般推理能力上也表现优异，证明了其在高风险金融应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2507.16038', 'title': 'Discovering and using Spelke segments', 'url': 'https://huggingface.co/papers/2507.16038', 'abstract': 'A visual world model called SpelkeNet outperforms existing methods in identifying Spelke objects in images, improving performance in tasks like physical object manipulation.  \t\t\t\t\tAI-generated summary \t\t\t\t Segments in computer vision are often defined by semantic considerations and are highly dependent on category-specific conventions. In contrast, developmental psychology suggests that humans perceive the world in terms of Spelke objects--groupings of physical things that reliably move together when acted on by physical forces. Spelke objects thus operate on category-agnostic causal motion relationships which potentially better support tasks like manipulation and planning. In this paper, we first benchmark the Spelke object concept, introducing the SpelkeBench dataset that contains a wide variety of well-defined Spelke segments in natural images. Next, to extract Spelke segments from images algorithmically, we build SpelkeNet, a class of visual world models trained to predict distributions over future motions. SpelkeNet supports estimation of two key concepts for Spelke object discovery: (1) the motion affordance map, identifying regions likely to move under a poke, and (2) the expected-displacement map, capturing how the rest of the scene will move. These concepts are used for "statistical counterfactual probing", where diverse "virtual pokes" are applied on regions of high motion-affordance, and the resultant expected displacement maps are used define Spelke segments as statistical aggregates of correlated motion statistics. We find that SpelkeNet outperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench. Finally, we show that the Spelke concept is practically useful for downstream applications, yielding superior performance on the 3DEditBench benchmark for physical object manipulation when used in a variety of off-the-shelf object manipulation models.', 'score': 1, 'issue_id': 5012, 'pub_date': '2025-07-21', 'pub_date_card': {'ru': '21 июля', 'en': 'July 21', 'zh': '7月21日'}, 'hash': '23df9dd6fca8cb24', 'authors': ['Rahul Venkatesh', 'Klemen Kotar', 'Lilian Naing Chen', 'Seungwoo Kim', 'Luca Thomas Wheeler', 'Jared Watrous', 'Ashley Xu', 'Gia Ancone', 'Wanhee Lee', 'Honglin Chen', 'Daniel Bear', 'Stefan Stojanov', 'Daniel Yamins'], 'affiliations': ['Noetik Inc.', 'OpenAI', 'Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2507.16038.jpg', 'data': {'categories': ['#multimodal', '#games', '#cv', '#dataset', '#3d', '#optimization', '#benchmark'], 'emoji': '🧠', 'ru': {'title': 'SpelkeNet: Революция в восприятии объектов искусственным интеллектом', 'desc': "SpelkeNet - это визуальная модель мира, которая превосходит существующие методы в идентификации объектов Спелке на изображениях. Модель обучена предсказывать распределения будущих движений и использует концепции карты возможностей движения и карты ожидаемых смещений. SpelkeNet применяет метод 'статистического контрфактического зондирования' для определения сегментов Спелке. Эта модель показывает лучшие результаты на наборе данных SpelkeBench и в задачах физического манипулирования объектами."}, 'en': {'title': 'Revolutionizing Object Recognition with SpelkeNet', 'desc': 'This paper introduces SpelkeNet, a visual world model that excels in identifying Spelke objects in images, which are groupings of physical entities that move together due to physical forces. Unlike traditional methods that rely on specific categories, SpelkeNet leverages category-agnostic causal relationships to enhance tasks such as object manipulation and planning. The authors present the SpelkeBench dataset to benchmark the Spelke object concept and demonstrate that SpelkeNet outperforms existing models like SegmentAnything. Additionally, the Spelke concept proves beneficial for practical applications, improving performance in physical object manipulation tasks.'}, 'zh': {'title': 'SpelkeNet：超越传统的视觉对象识别', 'desc': '本文介绍了一种名为SpelkeNet的视觉世界模型，它在识别图像中的Spelke对象方面优于现有方法，特别是在物理对象操作任务中表现出色。Spelke对象是基于物理运动关系的分组，能够更好地支持操作和规划任务。我们构建了SpelkeBench数据集，以基准测试Spelke对象的概念，并通过SpelkeNet算法提取图像中的Spelke段。研究表明，SpelkeNet在多个下游应用中表现优异，尤其是在物理对象操作的3DEditBench基准测试中。'}}}, {'id': 'https://huggingface.co/papers/2507.18405', 'title': 'Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows', 'url': 'https://huggingface.co/papers/2507.18405', 'abstract': "Iwin Transformer, a hierarchical vision transformer without position embeddings, combines interleaved window attention and depthwise separable convolution for efficient global information exchange, achieving competitive performance in image classification, semantic segmentation, and video action recognition.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.", 'score': 0, 'issue_id': 5019, 'pub_date': '2025-07-24', 'pub_date_card': {'ru': '24 июля', 'en': 'July 24', 'zh': '7月24日'}, 'hash': '093938764e40d430', 'authors': ['Simin Huo', 'Ning Li'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.18405.jpg', 'data': {'categories': ['#open_source', '#video', '#optimization', '#architecture', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Иерархический трансформер без позиционных эмбеддингов для эффективной обработки визуальных данных', 'desc': 'Представлен Iwin Transformer - новая иерархическая архитектура трансформера для компьютерного зрения, не использующая позиционные эмбеддинги. Модель сочетает чередующееся оконное внимание и глубинно-разделимую свертку для эффективного глобального обмена информацией. Iwin Transformer показывает конкурентоспособные результаты в задачах классификации изображений, семантической сегментации и распознавания действий в видео. Архитектура позволяет напрямую дообучать модель с низкого до высокого разрешения.'}, 'en': {'title': 'Iwin Transformer: Efficient Global Information Exchange in Vision Tasks', 'desc': 'The Iwin Transformer is a new type of vision transformer that does not use position embeddings, allowing it to efficiently process images and videos. It combines interleaved window attention, which connects distant parts of the image, with depthwise separable convolution, which focuses on nearby areas. This design enables the model to exchange global information effectively within a single module, improving upon previous models like the Swin Transformer. The Iwin Transformer shows strong performance in various tasks, including image classification and semantic segmentation, and can also enhance class-conditional image generation.'}, 'zh': {'title': 'Iwin Transformer：高效的视觉变换器新选择', 'desc': 'Iwin Transformer是一种新型的分层视觉变换器，不使用位置嵌入。它通过交错窗口注意力和深度可分离卷积的结合，实现了高效的全局信息交换。该模型在图像分类、语义分割和视频动作识别等任务中表现出色，尤其在ImageNet-1K上达到了87.4的顶级准确率。Iwin Transformer的核心组件可以作为独立模块，替代自注意力模块，具有广泛的研究潜力。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (1)', '#agi', '#alignment', '#architecture (8)', '#audio (1)', '#benchmark (8)', '#cv (6)', '#data (3)', '#dataset (7)', '#diffusion (5)', '#ethics (1)', '#games (2)', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (3)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (4)', '#open_source (8)', '#optimization (14)', '#plp', '#rag', '#reasoning (5)', '#rl (5)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation (1)', '#survey', '#synthetic (1)', '#training (12)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-25 16:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-25 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-25 16:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    