
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 11 papers. June 6.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">6 июня</span> | <span id="title-articles-count">11 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-05.html">⬅️ <span id="prev-date">05.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-09.html">➡️ <span id="next-date">09.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '6 июня', 'en': 'June 6', 'zh': '6月6日'};
        let feedDateNext = {'ru': '09.06', 'en': '06/09', 'zh': '6月9日'};
        let feedDatePrev = {'ru': '05.06', 'en': '06/05', 'zh': '6月5日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.04308', 'title': 'RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language\n  Models for Robotics', 'url': 'https://huggingface.co/papers/2506.04308', 'abstract': 'Spatial referring is a fundamental capability of embodied robots to interact with the 3D physical world. However, even with the powerful pretrained vision language models (VLMs), recent approaches are still not qualified to accurately understand the complex 3D scenes and dynamically reason about the instruction-indicated locations for interaction. To this end, we propose RoboRefer, a 3D-aware VLM that can first achieve precise spatial understanding by integrating a disentangled but dedicated depth encoder via supervised fine-tuning (SFT). Moreover, RoboRefer advances generalized multi-step spatial reasoning via reinforcement fine-tuning (RFT), with metric-sensitive process reward functions tailored for spatial referring tasks. To support SFT and RFT training, we introduce RefSpatial, a large-scale dataset of 20M QA pairs (2x prior), covering 31 spatial relations (vs. 15 prior) and supporting complex reasoning processes (up to 5 steps). In addition, we introduce RefSpatial-Bench, a challenging benchmark filling the gap in evaluating spatial referring with multi-step reasoning. Experiments show that SFT-trained RoboRefer achieves state-of-the-art spatial understanding, with an average success rate of 89.6%. RFT-trained RoboRefer further outperforms all other baselines by a large margin, even surpassing Gemini-2.5-Pro by 17.4% in average accuracy on RefSpatial-Bench. Notably, RoboRefer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (e,g., UR5, G1 humanoid) in cluttered real-world scenes.', 'score': 7, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ef5abd087929ed17', 'authors': ['Enshen Zhou', 'Jingkun An', 'Cheng Chi', 'Yi Han', 'Shanyu Rong', 'Chi Zhang', 'Pengwei Wang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'Shanghang Zhang'], 'affiliations': ['Beihang University', 'Beijing Academy of Artificial Intelligence', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04308.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#robotics', '#reasoning', '#dataset', '#3d', '#training', '#rl'], 'emoji': '🤖', 'ru': {'title': 'RoboRefer: Пространственный интеллект для роботов нового поколения', 'desc': 'RoboRefer - это модель пространственного понимания для роботов, основанная на зрении и языке. Она использует специальный энкодер глубины и обучение с подкреплением для точного понимания 3D-сцен и рассуждений о пространственных отношениях. Авторы также представили большой набор данных RefSpatial и бенчмарк RefSpatial-Bench для обучения и оценки модели. RoboRefer превзошла современные методы, включая Gemini-2.5-Pro, и может применяться для управления различными роботами в реальных условиях.'}, 'en': {'title': 'RoboRefer: Advancing Spatial Understanding for Robots in 3D Environments', 'desc': 'The paper introduces RoboRefer, a novel 3D-aware vision language model (VLM) designed to enhance spatial referring capabilities in robots. It achieves improved spatial understanding through a depth encoder and supervised fine-tuning (SFT), allowing for accurate interpretation of complex 3D environments. Additionally, RoboRefer employs reinforcement fine-tuning (RFT) with specialized reward functions to facilitate multi-step spatial reasoning. The authors also present RefSpatial, a comprehensive dataset and benchmark that supports the training and evaluation of RoboRefer, demonstrating its superior performance in real-world robotic tasks.'}, 'zh': {'title': 'RoboRefer：提升机器人空间理解与推理能力的创新模型', 'desc': '空间指向是具身机器人与三维物理世界互动的基本能力。尽管现有的预训练视觉语言模型（VLMs）很强大，但它们在理解复杂的三维场景和动态推理指示位置方面仍然存在不足。为此，我们提出了RoboRefer，这是一种具有三维感知能力的VLM，通过监督微调（SFT）集成了专门的深度编码器，实现了精确的空间理解。此外，RoboRefer通过强化微调（RFT）推进了多步骤空间推理，采用针对空间指向任务的度量敏感过程奖励函数。'}}}, {'id': 'https://huggingface.co/papers/2506.04209', 'title': 'Language-Image Alignment with Fixed Text Encoders', 'url': 'https://huggingface.co/papers/2506.04209', 'abstract': 'Learning Language-Image alignment with a Fixed Text encoder (LIFT) using pre-trained large language models effectively guides visual representation learning, outperforming joint training methods like CLIP in compositional understanding and long captions.  \t\t\t\t\tAI-generated summary \t\t\t\t Currently, the most dominant approach to establishing language-image alignment is to pre-train text and image encoders jointly through contrastive learning, such as CLIP and its variants. In this work, we question whether such a costly joint training is necessary. In particular, we investigate if a pre-trained fixed large language model (LLM) offers a good enough text encoder to guide visual representation learning. That is, we propose to learn Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by training only the image encoder. Somewhat surprisingly, through comprehensive benchmarking and ablation studies, we find that this much simplified framework LIFT is highly effective and it outperforms CLIP in most scenarios that involve compositional understanding and long captions, while achieving considerable gains in computational efficiency. Our work takes a first step towards systematically exploring how text embeddings from LLMs can guide visual learning and suggests an alternative design choice for learning language-aligned visual representations.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '921137445b3be92e', 'authors': ['Jingfeng Yang', 'Ziyang Wu', 'Yue Zhao', 'Yi Ma'], 'affiliations': ['The University of Hong Kong', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2506.04209.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#long_context', '#alignment', '#cv'], 'emoji': '🔍', 'ru': {'title': 'Фиксированный языковой энкодер улучшает визуальное обучение', 'desc': 'Исследование LIFT предлагает новый подход к обучению визуальных представлений с использованием предобученных языковых моделей (LLM) в качестве фиксированного текстового энкодера. Этот метод превосходит совместное обучение текстовых и визуальных энкодеров, как в CLIP, особенно в задачах композиционного понимания и работы с длинными подписями. LIFT демонстрирует высокую эффективность и вычислительную эффективность, обучая только визуальный энкодер. Результаты исследования открывают новые перспективы использования текстовых эмбеддингов из LLM для улучшения визуального обучения.'}, 'en': {'title': 'LIFT: Efficient Language-Image Alignment with Fixed Text Encoders', 'desc': 'This paper introduces a method called LIFT, which stands for Learning Language-Image alignment with a Fixed Text encoder. Instead of training both text and image encoders together, LIFT uses a pre-trained large language model (LLM) as a fixed text encoder to improve visual representation learning. The authors demonstrate that this approach outperforms traditional joint training methods like CLIP, especially in tasks requiring compositional understanding and handling long captions. Additionally, LIFT is more computationally efficient, suggesting a new way to leverage LLMs for better language-image alignment.'}, 'zh': {'title': '简化训练，提升视觉理解的LIFT方法', 'desc': '本文提出了一种新的方法，称为LIFT（使用固定文本编码器的语言-图像对齐），旨在通过预训练的大型语言模型来指导视觉表示学习。与传统的联合训练方法（如CLIP）相比，LIFT只训练图像编码器，而使用固定的文本编码器，从而简化了训练过程。研究表明，LIFT在处理组合理解和长文本描述时，表现优于CLIP，并且在计算效率上也有显著提升。该研究为如何利用大型语言模型的文本嵌入来指导视觉学习提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2505.23656', 'title': 'VideoREPA: Learning Physics for Video Generation through Relational\n  Alignment with Foundation Models', 'url': 'https://huggingface.co/papers/2505.23656', 'abstract': 'Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.', 'score': 4, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'aa7bbc7378df2b3e', 'authors': ['Xiangdong Zhang', 'Jiaqi Liao', 'Shaofeng Zhang', 'Fanqing Meng', 'Xiangpeng Wan', 'Junchi Yan', 'Yu Cheng'], 'affiliations': ['Dept. of CSE & School of AI & MoE Key Lab of Al, Shanghai Jiao Tong University', 'NetMind.AI', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.23656.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#training', '#video', '#diffusion'], 'emoji': '🎥', 'ru': {'title': 'Физически достоверное видео из текста: VideoREPA улучшает понимание физики в моделях T2V', 'desc': 'В статье представлен новый метод VideoREPA для улучшения физической достоверности видео, генерируемых моделями text-to-video (T2V). Авторы предлагают дистиллировать понимание физики из моделей видеопонимания в модели T2V путем выравнивания отношений на уровне токенов. Ключевым элементом метода является функция потерь Token Relation Distillation (TRD), которая обеспечивает мягкое руководство для дообучения предобученных моделей T2V. Эмпирические оценки показывают, что VideoREPA значительно улучшает физический здравый смысл базового метода CogVideoX.'}, 'en': {'title': 'Bridging the Physics Gap in Text-to-Video Generation', 'desc': 'This paper introduces VideoREPA, a new framework that improves text-to-video (T2V) models by enhancing their understanding of physics. Current T2V models often produce unrealistic videos due to their limited grasp of physical principles. VideoREPA addresses this issue by distilling knowledge from advanced video understanding models, using a novel Token Relation Distillation (TRD) loss to align token-level relationships. The results show that VideoREPA significantly boosts the physics commonsense of T2V models, leading to more realistic video generation.'}, 'zh': {'title': 'VideoREPA：提升文本到视频模型的物理理解能力', 'desc': '最近，文本到视频（T2V）扩散模型的进展使得高保真和真实的视频合成成为可能。然而，当前的T2V模型在生成物理上合理的内容方面常常面临挑战，因为它们对物理的理解能力有限。我们提出了一种新框架，称为VideoREPA，通过对齐令牌级关系，将视频理解基础模型中的物理理解能力提炼到T2V模型中，从而缩小物理理解的差距。我们的实验表明，VideoREPA显著增强了基线方法CogVideoX的物理常识，能够生成与直观物理一致的视频。'}}}, {'id': 'https://huggingface.co/papers/2506.05344', 'title': 'SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs', 'url': 'https://huggingface.co/papers/2506.05344', 'abstract': 'Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.', 'score': 3, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '0175b3788ebacf29', 'authors': ['Jiahui Wang', 'Zuyan Liu', 'Yongming Rao', 'Jiwen Lu'], 'affiliations': ['Tencent Hunyuan Research', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05344.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#architecture', '#open_source', '#inference'], 'emoji': '🔍', 'ru': {'title': 'Эффективное зрение: оптимизация визуального восприятия в мультимодальных ИИ', 'desc': 'Исследование показывает, что мультимодальные большие языковые модели (MLLM) используют лишь небольшую часть (менее 5%) механизмов внимания для обработки визуальной информации. Авторы разработали метод SparseMM для оптимизации KV-кэша, который распределяет вычислительные ресурсы асимметрично, основываясь на визуальной релевантности головок внимания. Этот подход позволяет ускорить вывод MLLM в 1,38 раза и сократить использование памяти на 52% при сохранении производительности. Метод SparseMM показывает лучшее соотношение точности и эффективности по сравнению с существующими методами оптимизации KV-кэша.'}, 'en': {'title': 'Optimizing Visual Understanding in MLLMs with Sparse Attention', 'desc': 'This paper explores how Multimodal Large Language Models (MLLMs) handle visual information by examining their attention mechanisms. It uncovers that only a small fraction of attention heads, known as visual heads, are crucial for understanding visual inputs. To efficiently identify these heads, the authors propose a training-free method that assesses head-level visual relevance. They also introduce SparseMM, a KV-Cache optimization technique that improves inference speed and reduces memory usage by focusing computational resources on the most relevant visual heads, achieving significant performance improvements on multimodal tasks.'}, 'zh': {'title': '利用稀疏视觉头加速多模态模型推理', 'desc': '多模态大型语言模型（MLLMs）通过扩展预训练的大型语言模型（LLMs）来增加视觉能力。我们研究了MLLMs如何处理视觉输入，分析了它们的注意力机制。我们发现了一个惊人的稀疏现象：在LLMs中，只有少量（大约5%以下）的注意力头积极参与视觉理解，这些被称为视觉头。基于这一发现，我们提出了SparseMM，一种KV-Cache优化策略，根据视觉得分为LLMs中的头分配不对称的计算预算，从而加速MLLMs的推理。'}}}, {'id': 'https://huggingface.co/papers/2506.05328', 'title': 'AV-Reasoner: Improving and Benchmarking Clue-Grounded Audio-Visual\n  Counting for MLLMs', 'url': 'https://huggingface.co/papers/2506.05328', 'abstract': "Despite progress in video understanding, current MLLMs struggle with counting tasks. Existing benchmarks are limited by short videos, close-set queries, lack of clue annotations, and weak multimodal coverage. In this paper, we introduce CG-AV-Counting, a manually-annotated clue-grounded counting benchmark with 1,027 multimodal questions and 5,845 annotated clues over 497 long videos. It supports both black-box and white-box evaluation, serving as a comprehensive testbed for both end-to-end and reasoning-based counting. To explore ways to improve model's counting capability, we propose AV-Reasoner, a model trained with GRPO and curriculum learning to generalize counting ability from related tasks. AV-Reasoner achieves state-of-the-art results across multiple benchmarks, demonstrating the effectiveness of reinforcement learning. However, experiments show that on out-of-domain benchmarks, reasoning in the language space fails to bring performance gains. The code and benchmark have been realeased on https://av-reasoner.github.io.", 'score': 3, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '774ccf3fd01aa4ef', 'authors': ['Lidong Lu', 'Guo Chen', 'Zhiqi Li', 'Yicheng Liu', 'Tong Lu'], 'affiliations': ['Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05328.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#multimodal', '#reasoning', '#dataset', '#long_context', '#training', '#video', '#rl'], 'emoji': '🧮', 'ru': {'title': 'Продвинутый подсчет объектов в видео с помощью мультимодального ИИ', 'desc': 'Статья представляет новый бенчмарк CG-AV-Counting для задач подсчета объектов в длинных видео с использованием мультимодальных вопросов и аннотированных подсказок. Авторы предлагают модель AV-Reasoner, обученную с помощью обучения с подкреплением и куррикулярного обучения для улучшения способности подсчета. Модель достигает лучших результатов на нескольких бенчмарках, демонстрируя эффективность обучения с подкреплением. Однако эксперименты показывают, что рассуждения в языковом пространстве не приносят улучшений на бенчмарках вне домена обучения.'}, 'en': {'title': 'Enhancing Video Counting with CG-AV-Counting and AV-Reasoner', 'desc': 'This paper addresses the limitations of current machine learning language models (MLLMs) in performing counting tasks in videos. It introduces CG-AV-Counting, a new benchmark that includes a large set of multimodal questions and clues, designed to evaluate counting capabilities in long videos. The authors propose a model called AV-Reasoner, which utilizes gradient-based reinforcement learning and curriculum learning to enhance counting performance. Despite achieving state-of-the-art results on various benchmarks, the model struggles with out-of-domain tasks, indicating challenges in generalizing reasoning across different contexts.'}, 'zh': {'title': '提升视频计数能力的新基准与模型', 'desc': '尽管视频理解取得了进展，但当前的多模态学习模型在计数任务上仍然存在困难。现有的基准测试受限于短视频、封闭式查询、缺乏线索注释和多模态覆盖不足。本文介绍了CG-AV-Counting，这是一个手动注释的线索基础计数基准，包含1,027个多模态问题和5,845个注释线索，覆盖497个长视频。我们提出的AV-Reasoner模型通过GRPO和课程学习进行训练，能够从相关任务中推广计数能力，并在多个基准测试中取得了最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2506.05176', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through\n  Foundation Models', 'url': 'https://huggingface.co/papers/2506.05176', 'abstract': "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.", 'score': 3, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '90ebf52dd91334c2', 'authors': ['Yanzhao Zhang', 'Mingxin Li', 'Dingkun Long', 'Xin Zhang', 'Huan Lin', 'Baosong Yang', 'Pengjun Xie', 'An Yang', 'Dayiheng Liu', 'Junyang Lin', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Alibaba Group', 'Tongyi Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.05176.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#multilingual', '#open_source', '#training', '#small_models', '#low_resource'], 'emoji': '🔍', 'ru': {'title': 'Qwen3 Embedding: Новый стандарт многоязычных текстовых эмбеддингов', 'desc': 'В работе представлена серия моделей Qwen3 Embedding, улучшающая возможности текстовых эмбеддингов и ранжирования по сравнению с предшественником GTE-Qwen. Модели основаны на фундаментальных моделях Qwen3 и используют многоэтапный процесс обучения, включающий масштабную предварительную подготовку и тонкую настройку на качественных датасетах. Qwen3 Embedding предлагает модели различных размеров (0.6B, 4B, 8B) для задач эмбеддинга и ранжирования, что позволяет оптимизировать их под разные сценарии применения. Эмпирические оценки показывают, что серия Qwen3 Embedding достигает передовых результатов в различных бенчмарках, особенно в многоязычной оценке MTEB и задачах поиска.'}, 'en': {'title': 'Empowering Multilingual Text Understanding with Qwen3 Embeddings', 'desc': 'The Qwen3 Embedding series represents a major improvement in text embedding and reranking, building on the capabilities of the Qwen3 foundation models. It utilizes a multi-stage training approach that combines unsupervised pre-training with supervised fine-tuning, enhancing its performance across various languages and domains. The series includes multiple model sizes, allowing users to choose between efficiency and effectiveness based on their needs. Empirical results show that the Qwen3 Embedding series achieves top performance on benchmarks, particularly in multilingual tasks, and is available for public use to encourage further research.'}, 'zh': {'title': 'Qwen3嵌入系列：多语言文本处理的新突破', 'desc': '本文介绍了Qwen3嵌入系列，这是在文本嵌入和重排序能力上相较于GTE-Qwen系列的重大进展。该系列基于Qwen3基础模型，利用其强大的多语言文本理解和生成能力，采用多阶段训练流程，结合大规模无监督预训练和高质量数据集的监督微调。通过有效的模型合并策略，Qwen3嵌入系列确保了模型的鲁棒性和适应性，提供了多种模型规模以满足不同的部署场景。实证评估表明，Qwen3嵌入系列在多项基准测试中取得了最先进的结果，尤其在多语言评估基准MTEB上表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.05240', 'title': 'Aligning Latent Spaces with Flow Priors', 'url': 'https://huggingface.co/papers/2506.05240', 'abstract': 'This paper presents a novel framework for aligning learnable latent spaces to arbitrary target distributions by leveraging flow-based generative models as priors. Our method first pretrains a flow model on the target features to capture the underlying distribution. This fixed flow model subsequently regularizes the latent space via an alignment loss, which reformulates the flow matching objective to treat the latents as optimization targets. We formally prove that minimizing this alignment loss establishes a computationally tractable surrogate objective for maximizing a variational lower bound on the log-likelihood of latents under the target distribution. Notably, the proposed method eliminates computationally expensive likelihood evaluations and avoids ODE solving during optimization. As a proof of concept, we demonstrate in a controlled setting that the alignment loss landscape closely approximates the negative log-likelihood of the target distribution. We further validate the effectiveness of our approach through large-scale image generation experiments on ImageNet with diverse target distributions, accompanied by detailed discussions and ablation studies. With both theoretical and empirical validation, our framework paves a new way for latent space alignment.', 'score': 2, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'c776325b1f9f6966', 'authors': ['Yizhuo Li', 'Yuying Ge', 'Yixiao Ge', 'Ying Shan', 'Ping Luo'], 'affiliations': ['ARC Lab, Tencent', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.05240.jpg', 'data': {'categories': ['#optimization', '#alignment', '#training', '#cv', '#math', '#diffusion'], 'emoji': '🔄', 'ru': {'title': 'Выравнивание латентных пространств с помощью потоковых моделей', 'desc': 'Статья представляет новый подход к выравниванию обучаемых латентных пространств с произвольными целевыми распределениями, используя генеративные модели на основе потоков в качестве априорных. Метод сначала предобучает модель потока на целевых признаках, а затем использует ее для регуляризации латентного пространства через функцию потерь выравнивания. Авторы доказывают, что минимизация этой функции потерь является вычислительно эффективным суррогатом для максимизации вариационной нижней границы правдоподобия латентов. Эффективность подхода подтверждается экспериментами по генерации изображений на ImageNet с различными целевыми распределениями.'}, 'en': {'title': 'Aligning Latent Spaces with Flow-Based Models', 'desc': "This paper introduces a new method for aligning latent spaces in machine learning to match specific target distributions using flow-based generative models. The approach involves pretraining a flow model to understand the target distribution, which then helps to regularize the latent space through an alignment loss. This alignment loss is designed to optimize the latent variables effectively without the need for complex likelihood calculations or solving ordinary differential equations. The authors demonstrate the method's effectiveness through experiments on image generation, showing that it can accurately approximate the target distribution's characteristics."}, 'zh': {'title': '潜在空间对齐的新方法', 'desc': '本文提出了一种新颖的框架，通过利用基于流的生成模型作为先验，将可学习的潜在空间对齐到任意目标分布。我们的方法首先在目标特征上预训练流模型，以捕捉潜在分布。然后，这个固定的流模型通过对齐损失来规范化潜在空间，重新构造流匹配目标，将潜在变量视为优化目标。我们正式证明，最小化这个对齐损失建立了一个计算上可处理的替代目标，用于最大化潜在变量在目标分布下的变分下界的对数似然。'}}}, {'id': 'https://huggingface.co/papers/2506.04405', 'title': 'MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at\n  Scale', 'url': 'https://huggingface.co/papers/2506.04405', 'abstract': 'We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.', 'score': 2, 'issue_id': 4155, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '2cf822e179634776', 'authors': ['Ran Xu', 'Yuchen Zhuang', 'Yishan Zhong', 'Yue Yu', 'Xiangru Tang', 'Hang Wu', 'May D. Wang', 'Peifeng Ruan', 'Donghan Yang', 'Tao Wang', 'Guanghua Xiao', 'Carl Yang', 'Yang Xie', 'Wenqi Shi'], 'affiliations': ['Emory University', 'Georgia Tech', 'UT Southwestern Medical Center', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04405.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#dataset', '#open_source', '#training', '#rl'], 'emoji': '🩺', 'ru': {'title': 'MedAgentGYM: Революция в обучении ИИ для медицинского кодирования', 'desc': 'MedAgentGYM - это новая среда обучения для улучшения навыков медицинского рассуждения у агентов на основе больших языковых моделей (LLM). Она включает более 72 тысяч задач из 129 категорий, основанных на реальных биомедицинских сценариях. Задачи представлены в виде исполняемых кодовых сред с подробными описаниями, интерактивной обратной связью и верифицируемыми аннотациями. Используя MedAgentGYM, модель Med-Copilot-7B достигла значительного улучшения производительности через тонкую настройку и обучение с подкреплением.'}, 'en': {'title': 'Empowering Medical Reasoning in LLMs with MedAgentGYM', 'desc': 'MedAgentGYM is a new training environment aimed at improving the coding abilities of large language models (LLMs) in medical reasoning. It includes over 72,000 tasks from real-world biomedical situations, allowing LLMs to learn through interactive coding environments. The platform provides detailed task descriptions, feedback, and verified annotations to support effective training. Benchmarking shows that models like Med-Copilot-7B can significantly improve their performance through fine-tuning and reinforcement learning, making it a strong alternative to more expensive models like gpt-4o.'}, 'zh': {'title': 'MedAgentGYM：提升医学推理能力的创新平台', 'desc': '我们介绍了MedAgentGYM，这是第一个公开可用的训练环境，旨在增强大型语言模型（LLM）代理的基于编码的医学推理能力。MedAgentGYM包含72,413个任务实例，涵盖129个类别，来源于真实的生物医学场景。每个任务都在可执行的编码环境中封装，提供详细的任务描述、互动反馈机制、可验证的真实注释和可扩展的训练轨迹生成。通过对30多种LLM的广泛基准测试，发现商业API模型与开源模型之间存在显著的性能差异。'}}}, {'id': 'https://huggingface.co/papers/2506.04734', 'title': 'Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning\n  Capabilities Through Evaluation Design', 'url': 'https://huggingface.co/papers/2506.04734', 'abstract': 'Reasoning models represented by the Deepseek-R1-Distill series have been widely adopted by the open-source community due to their strong performance in mathematics, science, programming, and other domains. However, our study reveals that their benchmark evaluation results are subject to significant fluctuations caused by various factors. Subtle differences in evaluation conditions can lead to substantial variations in results. Similar phenomena are observed in other open-source inference models fine-tuned based on the Deepseek-R1-Distill series, as well as in the QwQ-32B model, making their claimed performance improvements difficult to reproduce reliably. Therefore, we advocate for the establishment of a more rigorous paradigm for model performance evaluation and present our empirical assessments of the Deepseek-R1-Distill series models.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'ce810e0cc38b26e4', 'authors': ['Lin Sun', 'Weihong Lin', 'Jinzhu Wu', 'Yongfu Zhu', 'Xiaoqi Jian', 'Guangxiang Zhao', 'Change Jia', 'Linglin Zhang', 'Sai-er Hu', 'Yuhan Wu', 'Xiangzheng Zhang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.04734.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#open_source', '#training', '#math'], 'emoji': '🎢', 'ru': {'title': 'Нестабильность оценок: вызов для бенчмаркинга языковых моделей', 'desc': 'Исследование показывает, что результаты оценки моделей серии Deepseek-R1-Distill подвержены значительным колебаниям из-за различных факторов. Небольшие изменения в условиях оценки могут привести к существенным различиям в результатах. Аналогичные явления наблюдаются и в других моделях, основанных на Deepseek-R1-Distill, а также в модели QwQ-32B. Авторы призывают к созданию более строгой парадигмы оценки производительности моделей машинного обучения.'}, 'en': {'title': 'Ensuring Reliable Evaluations for Deep Learning Models', 'desc': 'The Deepseek-R1-Distill series of reasoning models are popular in the open-source community for their strong capabilities in various fields like mathematics and programming. However, our research shows that their performance evaluations can vary greatly due to different testing conditions. These inconsistencies also appear in other models that are fine-tuned from the Deepseek-R1-Distill series, making it hard to trust their reported improvements. We propose a stricter framework for evaluating model performance to ensure more reliable and reproducible results.'}, 'zh': {'title': '建立更严格的模型评估标准', 'desc': 'Deepseek-R1-Distill系列模型在数学、科学和编程等领域表现出色，受到开源社区的广泛采用。然而，我们的研究发现，这些模型的基准评估结果受到多种因素的显著波动影响。评估条件的细微差异可能导致结果的重大变化。类似现象也出现在基于Deepseek-R1-Distill系列微调的其他开源推理模型中，因此我们呼吁建立更严格的模型性能评估范式。'}}}, {'id': 'https://huggingface.co/papers/2506.03238', 'title': 'Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric\n  Approach', 'url': 'https://huggingface.co/papers/2506.03238', 'abstract': 'OminiAbnorm-CT, a model for automated interpretation of CT images, outperforms existing methods in localizing and describing abnormalities across different body regions using text queries and visual prompts.  \t\t\t\t\tAI-generated summary \t\t\t\t Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'f234199601bef528', 'authors': ['Ziheng Zhao', 'Lisong Dai', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Department of Radiology, Renmin Hospital of Wuhan University', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'Shanghai Artificial Intelligence Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2506.03238.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#healthcare', '#cv'], 'emoji': '🔬', 'ru': {'title': 'ИИ-революция в интерпретации КТ-снимков', 'desc': 'Модель OminiAbnorm-CT предназначена для автоматизированной интерпретации КТ-изображений. Она превосходит существующие методы в локализации и описании аномалий в различных областях тела с использованием текстовых запросов и визуальных подсказок. Модель основана на всеобъемлющей иерархической системе классификации, разработанной совместно с опытными радиологами. OminiAbnorm-CT обучена на большом наборе данных КТ-изображений с тщательно размеченными аномалиями.'}, 'en': {'title': 'Revolutionizing CT Image Analysis with OminiAbnorm-CT', 'desc': 'OminiAbnorm-CT is a novel model designed to enhance the automated interpretation of CT images by accurately localizing and describing abnormalities. It introduces a comprehensive hierarchical classification system developed in collaboration with radiologists, covering 404 abnormal findings across various body regions. The model is trained on a large dataset of over 14.5K CT images, with detailed annotations for more than 19K abnormalities, ensuring robust performance. Through rigorous evaluation, OminiAbnorm-CT demonstrates superior accuracy compared to existing methods, making it a significant advancement in clinical radiology.'}, 'zh': {'title': 'OminiAbnorm-CT：CT图像异常自动解读的新突破', 'desc': 'OminiAbnorm-CT是一种用于自动解读CT图像的模型，能够在不同身体部位中定位和描述异常情况。该研究通过与资深放射科医生合作，提出了一个包含404种异常发现的层次分类系统。我们还贡献了一个包含超过14.5K CT图像的数据集，并为超过19K异常提供了详细的注释。通过大量实验，OminiAbnorm-CT在所有任务和指标上显著优于现有方法。'}}}, {'id': 'https://huggingface.co/papers/2506.04245', 'title': 'Contextual Integrity in LLMs via Reasoning and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04245', 'abstract': 'As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only sim700 examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.', 'score': 1, 'issue_id': 4155, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '8716d3ca53b1d58f', 'authors': ['Guangchen Lan', 'Huseyin A. Inan', 'Sahar Abdelnabi', 'Janardhan Kulkarni', 'Lukas Wutschitz', 'Reza Shokri', 'Christopher G. Brinton', 'Robert Sim'], 'affiliations': ['Microsoft', 'National University of Singapore', 'Purdue University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04245.jpg', 'data': {'categories': ['#synthetic', '#agents', '#benchmark', '#reasoning', '#dataset', '#transfer_learning', '#rl', '#leakage'], 'emoji': '🔐', 'ru': {'title': 'Разумное раскрытие информации: обучение ИИ-агентов контекстной целостности', 'desc': 'Статья посвящена проблеме контекстной целостности (CI) в эпоху автономных агентов, принимающих решения за пользователей. Авторы предлагают метод, использующий языковые модели (LLM) и обучение с подкреплением (RL) для обучения агентов рассуждать о контексте и принимать решения о раскрытии информации. Эксперименты на синтетическом наборе данных показали значительное снижение неуместного раскрытия информации при сохранении производительности задач. Улучшения переносятся на реальные бенчмарки CI, такие как PrivacyLens.'}, 'en': {'title': 'Enhancing Contextual Integrity in Autonomous Agents', 'desc': 'This paper addresses the challenge of contextual integrity (CI) in autonomous agents, focusing on how these agents decide what information to share during tasks. The authors propose that effective CI requires agents to reason about their operating context. They introduce a reinforcement learning (RL) framework that enhances this reasoning capability in language models (LLMs). Their experiments demonstrate that this approach significantly reduces inappropriate information disclosure while preserving task performance, and the improvements are validated against established benchmarks.'}, 'zh': {'title': '确保上下文完整性，提升自主代理决策能力', 'desc': '在自主代理为用户做决策的时代，确保上下文完整性（CI）成为一个重要问题。本文提出，CI需要代理在执行任务时对其操作的上下文进行推理。我们首先让大型语言模型（LLMs）明确推理CI，以决定披露哪些信息。接着，我们开发了一个强化学习（RL）框架，进一步增强模型进行CI所需的推理能力，实验结果表明，该方法显著减少了不当信息披露，同时保持了任务性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (2)', '#architecture (1)', '#audio', '#benchmark (10)', '#cv (3)', '#data (2)', '#dataset (6)', '#diffusion (2)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage (1)', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (3)', '#open_source (4)', '#optimization (5)', '#plp', '#rag', '#reasoning (5)', '#rl (4)', '#rlhf', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic (1)', '#training (7)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-06 02:40',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-06 02:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-06 02:40')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    