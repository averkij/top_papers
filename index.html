
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 22 papers. October 14.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">14 октября</span> | <span id="title-articles-count">22 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-13.html">⬅️ <span id="prev-date">13.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-15.html">➡️ <span id="next-date">15.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '14 октября', 'en': 'October 14', 'zh': '10月14日'};
        let feedDateNext = {'ru': '15.10', 'en': '10/15', 'zh': '10月15日'};
        let feedDatePrev = {'ru': '13.10', 'en': '10/13', 'zh': '10月13日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.11701', 'title': 'Demystifying Reinforcement Learning in Agentic Reasoning', 'url': 'https://huggingface.co/papers/2510.11701', 'abstract': "Agentic reinforcement learning enhances LLMs' reasoning ability through real datasets, exploration techniques, and a deliberative strategy, achieving strong performance with smaller models.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL", 'score': 15, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'd0b700b02dfd5da7', 'authors': ['Zhaochen Yu', 'Ling Yang', 'Jiaru Zou', 'Shuicheng Yan', 'Mengdi Wang'], 'affiliations': ['National University of Singapore', 'Princeton University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.11701.jpg', 'data': {'categories': ['#small_models', '#benchmark', '#reasoning', '#training', '#optimization', '#open_source', '#rl', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Эффективное обучение агентов с подкреплением для улучшения рассуждений LLM', 'desc': 'Исследователи провели систематический анализ применения обучения с подкреплением для улучшения способности LLM к агентным рассуждениям и использованию инструментов. Они обнаружили, что использование реальных траекторий взаимодействия с инструментами вместо синтетических данных, применение техник для стимулирования исследования среды и продуманная стратегия использования инструментов значительно повышают эффективность обучения. Предложенные методы позволяют даже небольшим моделям размером 4B параметров превосходить модели в 32B параметров на сложных бенчмарках, таких как AIME2024/2025 и GPQA-Diamond. Авторы также публикуют высококачественные датасеты для supervised fine-tuning и reinforcement learning, создавая практическую основу для будущих исследований агентного RL.'}, 'en': {'title': 'Boosting LLM Reasoning with Agentic RL Techniques', 'desc': 'This paper explores how agentic reinforcement learning (RL) can improve the reasoning abilities of large language models (LLMs) using real datasets and effective exploration techniques. The authors identify key practices such as using real tool-use trajectories instead of synthetic ones, which significantly enhances model performance. They also emphasize the importance of exploration-friendly techniques and a deliberative strategy that minimizes tool calls to boost training efficiency and accuracy. Overall, the findings provide a practical framework for enhancing agentic reasoning in smaller models, achieving results comparable to larger models on challenging benchmarks.'}, 'zh': {'title': '代理强化学习提升推理能力的有效策略', 'desc': '本论文探讨了代理强化学习（agentic RL）如何通过真实数据集、探索技术和深思熟虑的策略来增强大型语言模型（LLMs）的推理能力。研究表明，使用真实的工具使用轨迹替代合成轨迹可以显著提高模型的初始化效果，并且多样化的数据集能够支持探索，提升强化学习的表现。此外，采用友好的探索技术和减少工具调用的深思策略能够提高训练效率和最终准确性。通过这些简单的实践，研究展示了在多个具有挑战性的基准测试中，小型模型也能取得优异的代理推理表现。'}}}, {'id': 'https://huggingface.co/papers/2510.11696', 'title': 'QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs', 'url': 'https://huggingface.co/papers/2510.11696', 'abstract': "QeRL, a quantization-enhanced reinforcement learning framework, accelerates RL training for large language models by combining NVFP4 quantization with Low-Rank Adaptation and an Adaptive Quantization Noise mechanism, achieving significant speedups and improved performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.", 'score': 12, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'dd78eef9fba0abb4', 'authors': ['Wei Huang', 'Yi Ge', 'Shuai Yang', 'Yicheng Xiao', 'Huizi Mao', 'Yujun Lin', 'Hanrong Ye', 'Sifei Liu', 'Ka Chun Cheung', 'Hongxu Yin', 'Yao Lu', 'Xiaojuan Qi', 'Song Han', 'Yukang Chen'], 'affiliations': ['HKU', 'MIT', 'NVIDIA', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2510.11696.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#optimization', '#inference', '#rl'], 'emoji': '⚡', 'ru': {'title': 'Квантизация ускоряет RL-обучение LLM в полтора раза', 'desc': 'Представлен QeRL — фреймворк для ускорения обучения больших языковых моделей с помощью reinforcement learning. Ключевая идея заключается в использовании квантизации NVFP4 совместно с Low-Rank Adaptation (LoRA), что снижает потребление памяти и ускоряет процесс обучения. Квантизационный шум увеличивает энтропию политики, улучшая исследование пространства стратегий, а механизм Adaptive Quantization Noise динамически регулирует уровень шума. Фреймворк впервые позволяет обучать 32B модель на одной GPU H100 80GB и достигает точности 90.8% на GSM8K и 77.4% на MATH 500 для 7B модели.'}, 'en': {'title': 'Accelerating RL Training for LLMs with QeRL', 'desc': 'QeRL is a novel framework that enhances reinforcement learning (RL) for large language models (LLMs) by integrating NVFP4 quantization with Low-Rank Adaptation (LoRA) and an Adaptive Quantization Noise mechanism. This combination significantly accelerates the RL training process, reducing memory usage and rollout times while improving overall performance. The introduction of quantization noise helps increase policy entropy, which promotes better exploration of strategies during training. Experimental results show that QeRL achieves over 1.5 times speedup in the rollout phase and matches the performance of full-parameter fine-tuning on key mathematical benchmarks.'}, 'zh': {'title': 'QeRL：加速大型语言模型的强化学习训练', 'desc': 'QeRL是一个增强量化的强化学习框架，专为大型语言模型设计。它通过结合NVFP4量化和低秩适应（LoRA），加速了强化学习的训练过程，同时减少了内存开销。QeRL还引入了自适应量化噪声机制，动态调整训练中的噪声，从而提高策略的探索性，发现更好的策略。实验表明，QeRL在强化学习的回合阶段实现了超过1.5倍的加速，并在多个基准测试中表现出色。'}}}, {'id': 'https://huggingface.co/papers/2510.11690', 'title': 'Diffusion Transformers with Representation Autoencoders', 'url': 'https://huggingface.co/papers/2510.11690', 'abstract': 'Replacing VAEs with pretrained representation encoders in Diffusion Transformers enhances generative quality and convergence speed without auxiliary losses.  \t\t\t\t\tAI-generated summary \t\t\t\t Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.', 'score': 11, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '477819306d0110e0', 'authors': ['Boyang Zheng', 'Nanye Ma', 'Shengbang Tong', 'Saining Xie'], 'affiliations': ['New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11690.jpg', 'data': {'categories': ['#training', '#optimization', '#cv', '#diffusion', '#architecture'], 'emoji': '🎨', 'ru': {'title': 'RAE: новый стандарт для обучения диффузионных трансформеров', 'desc': 'В работе предлагается заменить традиционные VAE-энкодеры в Diffusion Transformers на предобученные энкодеры представлений (DINO, SigLIP, MAE) в паре с обученными декодерами, создавая Representation Autoencoders (RAE). Такой подход обеспечивает семантически богатое латентное пространство высокой размерности и улучшает качество генерации изображений. Авторы решают проблему работы диффузионных моделей в высокоразмерных пространствах через теоретически обоснованные методы. Результаты показывают ускоренную сходимость и отличное качество генерации на ImageNet с FID 1.51 без guidance и 1.13 с guidance.'}, 'en': {'title': 'Upgrade Diffusion Transformers with Representation Autoencoders!', 'desc': 'This paper discusses improving Diffusion Transformers (DiTs) by replacing the traditional Variational Autoencoder (VAE) with pretrained representation encoders, creating what are called Representation Autoencoders (RAEs). The authors highlight that using VAEs limits the generative quality due to outdated architectures and low-dimensional latent spaces. By employing advanced encoders like DINO and MAE, RAEs achieve better reconstructions and richer latent representations, which enhance the performance of DiTs. The study demonstrates that this approach leads to faster convergence and superior image generation results, suggesting that RAEs should become the standard for training diffusion transformers.'}, 'zh': {'title': '用RAE提升扩散变换器的生成能力', 'desc': '本论文提出了一种新的生成模型方法，通过用预训练的表示编码器替换传统的变分自编码器（VAE），来提升扩散变换器（Diffusion Transformers）的生成质量和收敛速度。我们引入了表示自编码器（RAE），这种模型结合了高质量的重建和丰富的语义潜在空间，克服了VAE的局限性。研究表明，RAE在高维潜在空间中有效运行，并且在没有辅助损失的情况下实现了更快的收敛。实验结果显示，RAE在图像生成任务中表现优异，成为扩散变换器训练的新标准。'}}}, {'id': 'https://huggingface.co/papers/2510.10670', 'title': 'AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning\n  in 4D Scenes', 'url': 'https://huggingface.co/papers/2510.10670', 'abstract': 'A two-stage paradigm adapts pre-trained Text-to-Video models for viewpoint prediction in 4D scenes by integrating an adaptive learning branch and a camera extrinsic diffusion branch.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.', 'score': 11, 'issue_id': 6398, 'pub_date': '2025-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': 'e9218d263c01e3bc', 'authors': ['Yu Li', 'Menghan Xia', 'Gongye Liu', 'Jianhong Bai', 'Xintao Wang', 'Conglang Zhang', 'Yuxuan Lin', 'Ruihang Chu', 'Pengfei Wan', 'Yujiu Yang'], 'affiliations': ['HKUST', 'HUST', 'Kling Team, Kuaishou Technology', 'Tsinghua University', 'Wuhan University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.10670.jpg', 'data': {'categories': ['#video', '#games', '#diffusion', '#multimodal'], 'emoji': '🎥', 'ru': {'title': 'Видео-генерация для планирования траектории камеры в 4D сценах', 'desc': 'Исследователи предлагают двухэтапный метод адаптации предобученных Text-to-Video моделей для предсказания точек обзора в 4D сценах. На первом этапе 4D сцена интегрируется в T2V модель через адаптивную ветку обучения, которая генерирует видео с визуально встроенными точками обзора. На втором этапе извлечение точек обзора формулируется как процесс диффузионного шумоподавления внешних параметров камеры с гибридным условием. Работа демонстрирует потенциал моделей видео-генерации как неявных world models для взаимодействия с реальным миром в 4D пространстве.'}, 'en': {'title': 'Harnessing Video Generation for 4D Viewpoint Prediction', 'desc': 'This paper presents a two-stage approach to adapt pre-trained Text-to-Video (T2V) models for predicting viewpoints in 4D scenes. The first stage involves integrating a 4D scene representation into the T2V model using an adaptive learning branch, allowing the model to generate videos that visually represent different viewpoints. The second stage formulates viewpoint extraction as a denoising process, utilizing a camera extrinsic diffusion branch that processes both the generated video and the 4D scene. The results demonstrate that this method outperforms existing techniques, highlighting the potential of T2V models for real-world 4D interactions.'}, 'zh': {'title': '利用视频生成模型进行4D视角预测的创新方法', 'desc': '本文提出了一种两阶段的范式，旨在通过适应性学习分支和相机外部扩散分支，将预训练的文本到视频模型（T2V）应用于4D场景的视角预测。首先，通过适应性学习分支将4D场景表示注入到预训练的T2V模型中，使得生成的视频能够自然地嵌入视角信息。接着，将视角提取过程视为一种混合条件引导的相机外部去噪过程，进一步引入相机外部扩散分支。实验结果表明，所提方法在性能上优于现有竞争者，验证了视频生成模型在现实世界4D交互中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.08886', 'title': 'FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark\n  for Evaluating LLMs', 'url': 'https://huggingface.co/papers/2510.08886', 'abstract': 'FinAuditing is a benchmark for evaluating LLMs on structured financial auditing tasks, revealing their limitations in handling taxonomy-driven, hierarchical financial documents.  \t\t\t\t\tAI-generated summary \t\t\t\t The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.', 'score': 11, 'issue_id': 6398, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'af8af45f11c7cfc5', 'authors': ['Yan Wang', 'Keyi Wang', 'Shanshan Yang', 'Jaisal Patel', 'Jeff Zhao', 'Fengran Mo', 'Xueqing Peng', 'Lingfei Qian', 'Jimin Huang', 'Guojun Xiong', 'Xiao-Yang Liu', 'Jian-Yun Nie'], 'affiliations': ['Columbia University USA', 'The Fin AI USA', 'University of Montreal Canada'], 'pdf_title_img': 'assets/pdf/title_img/2510.08886.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#survey', '#benchmark'], 'emoji': '📊', 'ru': {'title': 'LLM провалили экзамен по финансовому аудиту', 'desc': 'Исследователи создали бенчмарк FinAuditing для проверки способности больших языковых моделей работать со структурированными финансовыми документами в формате XBRL. Бенчмарк включает три задачи: проверку семантической, реляционной и численной согласованности данных в иерархических документах, составленных по стандартам US-GAAP. Тестирование 13 современных LLM показало катастрофические результаты — точность падала на 60-90% при работе с многодокументными иерархическими структурами. Результаты выявили системные ограничения AI в понимании таксономий и структурированного финансового анализа, что критично для создания надёжных систем автоматизации аудита.'}, 'en': {'title': 'FinAuditing: Bridging the Gap in Financial Reasoning for LLMs', 'desc': 'FinAuditing is a new benchmark designed to assess large language models (LLMs) on structured financial auditing tasks, particularly focusing on the challenges posed by hierarchical financial documents. It highlights the difficulties LLMs face in reasoning over complex, taxonomy-driven structures like those found in Generally Accepted Accounting Principles (GAAP) and eXtensible Business Reporting Language (XBRL) filings. The benchmark includes three specific subtasks: FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each addressing different aspects of financial auditing. Results from testing 13 advanced LLMs show significant performance drops, revealing their limitations in handling structured financial reasoning, thus paving the way for improved financial intelligence systems.'}, 'zh': {'title': 'FinAuditing：揭示LLMs在财务审计中的局限性', 'desc': 'FinAuditing是一个用于评估大型语言模型（LLMs）在结构化财务审计任务中的基准测试。该基准揭示了LLMs在处理基于分类法的层次财务文档时的局限性。通过定义三个互补的子任务，FinSM、FinRE和FinMR，FinAuditing专注于语义一致性、关系一致性和数值一致性。我们的实验表明，当前模型在处理层次多文档结构时，准确率下降高达60-90%，显示出现代LLMs在财务推理方面的系统性局限性。'}}}, {'id': 'https://huggingface.co/papers/2510.10395', 'title': 'AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration', 'url': 'https://huggingface.co/papers/2510.10395', 'abstract': 'AVoCaDO, an audiovisual video captioner, enhances temporal coherence and dialogue accuracy through a two-stage post-training pipeline, outperforming existing models across multiple benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.', 'score': 9, 'issue_id': 6399, 'pub_date': '2025-10-12', 'pub_date_card': {'ru': '12 октября', 'en': 'October 12', 'zh': '10月12日'}, 'hash': '11416ff379dbecb2', 'authors': ['Xinlong Chen', 'Yue Ding', 'Weihong Lin', 'Jingyun Hua', 'Linli Yao', 'Yang Shi', 'Bozhou Li', 'Yuanxing Zhang', 'Qiang Liu', 'Pengfei Wan', 'Liang Wang', 'Tieniu Tan'], 'affiliations': ['Kling Team, Kuaishou Technology', 'Nanjing University', 'New Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.10395.jpg', 'data': {'categories': ['#multimodal', '#video', '#benchmark', '#training', '#open_source', '#dataset', '#data'], 'emoji': '🎬', 'ru': {'title': 'Аудиовизуальные описания видео с точной временной синхронизацией', 'desc': 'AVoCaDO — это модель для генерации описаний видео, которая учитывает как визуальную, так и аудио информацию. Обучение проходит в два этапа: сначала файн-тюнинг на датасете из 107 тысяч высококачественных описаний с временной привязкой, затем оптимизация через GRPO с специальными функциями награды. Модель особенно хорошо справляется с синхронизацией аудио и визуальных событий во времени, а также с точным распознаванием диалогов. AVoCaDO превосходит существующие open-source модели на четырёх бенчмарках аудиовизуального описания видео и показывает конкурентные результаты даже в режиме работы только с визуальной информацией.'}, 'en': {'title': 'AVoCaDO: Enhancing Video Captions with Audio-Visual Harmony', 'desc': 'AVoCaDO is an advanced audiovisual video captioner that improves the quality of video descriptions by ensuring that the timing of audio and visual elements aligns well. It uses a two-stage post-training process, starting with fine-tuning on a large dataset of high-quality captions to enhance its understanding of audiovisual content. The second stage employs specialized reward functions to boost the accuracy of dialogue and maintain coherence in the captions while controlling their length. Overall, AVoCaDO shows superior performance compared to existing models in various benchmarks, making it a significant advancement in the field of video captioning.'}, 'zh': {'title': 'AVoCaDO：提升音视频字幕生成的准确性与一致性', 'desc': 'AVoCaDO是一种音视频字幕生成模型，旨在提高时间一致性和对话准确性。它通过一个两阶段的后训练流程来实现这一目标，首先对模型进行微调，然后利用定制的奖励函数进一步优化。实验结果表明，AVoCaDO在多个基准测试中显著超越了现有的开源模型。该模型不仅在音视频字幕生成方面表现出色，在仅使用视觉信息的情况下也能取得竞争力的成绩。'}}}, {'id': 'https://huggingface.co/papers/2510.04617', 'title': 'Making Mathematical Reasoning Adaptive', 'url': 'https://huggingface.co/papers/2510.04617', 'abstract': "AdaR framework enhances LLMs' robustness and generalization in mathematical reasoning by synthesizing logically equivalent queries and using RLVR to penalize spurious logic.  \t\t\t\t\tAI-generated summary \t\t\t\t Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR", 'score': 9, 'issue_id': 6398, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '82cf47c00d882ce9', 'authors': ['Zhejian Lai', 'Xiang Geng', 'Zhijun Wang', 'Yang Bai', 'Jiahuan Li', 'Rongxiang Weng', 'Jingang Wang', 'Xuezhi Cao', 'Xunliang Cai', 'Shujian Huang'], 'affiliations': ['Meituan Inc., China', 'Nanjing University, Nanjing, China'], 'pdf_title_img': 'assets/pdf/title_img/2510.04617.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#data', '#math'], 'emoji': '🔢', 'ru': {'title': 'Адаптивный reasoning вместо поверхностных решений в математике', 'desc': 'Статья представляет фреймворк AdaR для улучшения математического reasoning в LLM путём борьбы с ложной логикой (spurious reasoning), когда модель опирается на поверхностные признаки вместо настоящего решения. Метод синтезирует логически эквивалентные запросы с изменёнными значениями переменных и использует RLVR (reinforcement learning) для штрафования ложной логики и поощрения адаптивного reasoning. Для обеспечения качества данных авторы извлекают логику решения из исходной задачи, генерируют ответ через выполнение кода и применяют проверку корректности. Эксперименты показывают, что AdaR значительно улучшает робастность и способность к обобщению в математических задачах при высокой эффективности использования данных.'}, 'en': {'title': "Enhancing LLMs' Reasoning with AdaR Framework", 'desc': 'The AdaR framework aims to improve the robustness and generalization of large language models (LLMs) in mathematical reasoning tasks. It addresses the issue of spurious reasoning by synthesizing logically equivalent queries and employing Reinforcement Learning with Value Regularization (RLVR) to penalize incorrect logic. By extracting problem-solving logic and generating answers through code execution, AdaR enhances data quality and encourages models to rely on sound reasoning. Experimental results show that AdaR significantly boosts performance in mathematical reasoning while ensuring efficient use of data.'}, 'zh': {'title': 'AdaR框架：提升LLMs数学推理的鲁棒性与泛化能力', 'desc': '本论文提出了AdaR框架，以增强大型语言模型（LLMs）在数学推理中的鲁棒性和泛化能力。现有的LLMs常常因表面特征导致错误推理，缺乏深层次的逻辑思考。AdaR通过合成逻辑等价的查询并使用强化学习变体（RLVR）来惩罚虚假逻辑，从而促进模型的自适应推理。实验结果表明，AdaR显著提高了数学推理的表现，同时保持了高数据效率。'}}}, {'id': 'https://huggingface.co/papers/2510.09541', 'title': 'SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models', 'url': 'https://huggingface.co/papers/2510.09541', 'abstract': 'The Sandwiched Policy Gradient method improves reinforcement learning for diffusion large language models by using both upper and lower bounds of log-likelihood, outperforming ELBO-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.', 'score': 7, 'issue_id': 6399, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '637441c2a3e6a38a', 'authors': ['Chenyu Wang', 'Paria Rashidinejad', 'DiJia Su', 'Song Jiang', 'Sid Wang', 'Siyan Zhao', 'Cai Zhou', 'Shannon Zejiang Shen', 'Feiyu Chen', 'Tommi Jaakkola', 'Yuandong Tian', 'Bo Liu'], 'affiliations': ['MIT', 'Meta Superintelligence Labs', 'UCLA', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2510.09541.jpg', 'data': {'categories': ['#rlhf', '#training', '#diffusion', '#rl', '#reinforcement_learning'], 'emoji': '🥪', 'ru': {'title': 'Сэндвич из границ для обучения диффузионных языковых моделей', 'desc': 'Диффузионные языковые модели (dLLM) могут генерировать несколько токенов параллельно, но их сложно обучать с помощью reinforcement learning из-за невозможности точно вычислить log-likelihood. Существующие методы используют только нижнюю границу (ELBO), что вносит систематическую ошибку в градиенты политики. Предложенный метод Sandwiched Policy Gradient использует одновременно верхнюю и нижнюю границы log-likelihood для более точной оценки градиентов. Эксперименты показывают значительное улучшение точности: на 3.6% в GSM8K, 2.6% в MATH500, 18.4% в Countdown и 27.0% в Sudoku по сравнению с существующими подходами.'}, 'en': {'title': 'Reinforcing dLLMs with Balanced Policy Gradients', 'desc': 'The Sandwiched Policy Gradient (SPG) method enhances reinforcement learning for diffusion large language models (dLLMs) by utilizing both upper and lower bounds of log-likelihood. This approach addresses the limitations of traditional policy gradient methods, which struggle with the intractable log-likelihood of dLLMs. By avoiding the biases introduced by one-sided approximations like the evidence lower bound (ELBO), SPG provides a more accurate estimation of policy gradients. Experimental results demonstrate that SPG outperforms existing methods, achieving significant improvements in various benchmark tasks.'}, 'zh': {'title': '夹心策略梯度：提升扩散大语言模型的强化学习效果', 'desc': '本文提出了一种新的强化学习方法，称为夹心策略梯度（SPG），用于改进扩散大语言模型（dLLMs）。传统的策略梯度方法由于无法直接处理复杂的对数似然，难以与人类偏好或特定任务奖励对齐。SPG方法利用了真实对数似然的上下界，从而克服了以往方法中引入的偏差。实验结果表明，SPG在多个基准测试中显著优于基于证据下界（ELBO）的方法，提升了dLLMs的准确性。'}}}, {'id': 'https://huggingface.co/papers/2510.07841', 'title': 'Self-Improving LLM Agents at Test-Time', 'url': 'https://huggingface.co/papers/2510.07841', 'abstract': 'A test-time self-improvement method enhances language models by generating additional training examples from uncertain cases, leading to better performance with fewer samples.  \t\t\t\t\tAI-generated summary \t\t\t\t One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.', 'score': 5, 'issue_id': 6398, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '60bb755e99449195', 'authors': ['Emre Can Acikgoz', 'Cheng Qian', 'Heng Ji', 'Dilek Hakkani-Tür', 'Gokhan Tur'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2510.07841.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#transfer_learning'], 'emoji': '🔄', 'ru': {'title': 'Самообучение на лету: как модели учатся на своих ошибках во время тестирования', 'desc': 'Статья предлагает метод самосовершенствования языковых моделей во время тестирования (TT-SI). Модель сначала определяет примеры, с которыми она справляется плохо, затем генерирует похожие примеры для дообучения, и наконец использует их для улучшения своей работы прямо в момент тестирования. Этот подход показывает прирост точности в среднем на 5.48% при использовании в 68 раз меньшего количества обучающих примеров по сравнению со стандартными методами. Метод открывает новую парадигму создания AI-агентов, способных к самостоятельной эволюции без необходимости в больших датасетах.'}, 'en': {'title': 'Empowering Language Models Through Self-Improvement at Test-Time', 'desc': 'This paper introduces a novel method called Test-Time Self-Improvement (TT-SI) for enhancing language models by generating additional training examples from uncertain cases. The approach involves three key steps: identifying challenging samples, creating similar examples from these samples, and fine-tuning the model using the newly generated data. By focusing on self-awareness and self-data augmentation, TT-SI allows models to improve their performance significantly while using far fewer training samples. Empirical results show that this method leads to an average accuracy gain of 5.48% across various benchmarks, demonstrating its effectiveness compared to traditional learning techniques.'}, 'zh': {'title': '测试时自我改进：提升语言模型的新方法', 'desc': '本文提出了一种测试时自我改进的方法，通过从不确定的案例中生成额外的训练样本，增强语言模型的性能。该方法包括三个步骤：首先识别模型难以处理的样本，其次从这些不确定样本中生成相似的例子，最后在测试时进行微调。实验结果表明，该方法在多个基准测试中平均提高了5.48%的准确率，同时使用的训练样本减少了68倍。研究表明，测试时自我改进算法为构建更强大的智能体提供了新的思路。'}}}, {'id': 'https://huggingface.co/papers/2510.10023', 'title': 'Skill-Targeted Adaptive Training', 'url': 'https://huggingface.co/papers/2510.10023', 'abstract': 'A new fine-tuning strategy, STAT, uses a teacher model\'s metacognition to identify and address skill gaps in a student model, leading to improved performance on both in-distribution and out-of-distribution benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models often show little to no improvement (i.e., "saturation") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student\'s answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.', 'score': 4, 'issue_id': 6399, 'pub_date': '2025-10-11', 'pub_date_card': {'ru': '11 октября', 'en': 'October 11', 'zh': '10月11日'}, 'hash': '31df1a52ae903fc1', 'authors': ['Yinghui He', 'Abhishek Panigrahi', 'Yong Lin', 'Sanjeev Arora'], 'affiliations': ['Princeton Language and Intelligence, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2510.10023.jpg', 'data': {'categories': ['#transfer_learning', '#training', '#optimization', '#open_source', '#synthetic'], 'emoji': '🎯', 'ru': {'title': 'Целевое обучение через диагностику навыков учителем', 'desc': 'Статья представляет новый метод файнтюнинга STAT, который использует метакогнитивные способности сильной языковой модели-учителя для выявления пробелов в навыках модели-ученика. Учитель создаёт список необходимых навыков для задачи, отслеживает ошибки ученика и формирует профиль недостающих навыков. На основе этого профиля метод либо адаптивно перевзвешивает существующие примеры (STAT-Sel), либо синтезирует новые данные для проблемных навыков (STAT-Syn). STAT показывает улучшение до 7.5% на датасете MATH и 4.6% на out-of-distribution бенчмарках, причём метод хорошо комбинируется с reinforcement learning подходами.'}, 'en': {'title': 'Bridging Skill Gaps with STAT: A New Era in Fine-Tuning', 'desc': 'The paper introduces a new fine-tuning strategy called STAT, which leverages the metacognitive abilities of a teacher model to enhance the performance of a student model. By identifying skill gaps through a Missing-Skill-Profile, the teacher model can adaptively reweight existing training examples or synthesize new ones to address these gaps. This approach leads to significant improvements in performance on both in-distribution and out-of-distribution benchmarks, outperforming traditional supervised fine-tuning methods. The findings suggest that integrating skill-targeted adaptive training can enhance existing machine learning training pipelines.'}, 'zh': {'title': '利用元认知提升模型技能的微调策略', 'desc': '本文提出了一种新的微调策略STAT，利用教师模型的元认知能力来识别和解决学生模型的技能差距，从而提高模型在分布内和分布外基准测试上的表现。教师模型通过任务数据集创建所需技能列表，并根据学生的回答监控其技能应用情况，形成缺失技能档案。STAT策略包括两种方法：STAT-Sel通过调整现有训练样本的权重来适应缺失技能，而STAT-Syn则合成涉及缺失技能的额外示例。实验结果表明，STAT在MATH任务上提高了最多7.5%的性能，并在分布外基准上平均提升了4.6%。'}}}, {'id': 'https://huggingface.co/papers/2510.09905', 'title': 'The Personalization Trap: How User Memory Alters Emotional Reasoning in\n  LLMs', 'url': 'https://huggingface.co/papers/2510.09905', 'abstract': 'LLMs exhibit systematic biases in emotional interpretation and support based on user profiles, potentially reinforcing social hierarchies.  \t\t\t\t\tAI-generated summary \t\t\t\t When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities.', 'score': 4, 'issue_id': 6399, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': '8ab2b377c7e94c73', 'authors': ['Xi Fang', 'Weijie Xu', 'Yuchong Zhang', 'Stephanie Eckman', 'Scott Nickleach', 'Chandan K. Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2510.09905.jpg', 'data': {'categories': ['#alignment', '#ethics', '#multimodal', '#healthcare'], 'emoji': '⚖️', 'ru': {'title': 'Память LLM усиливает социальное неравенство в понимании эмоций', 'desc': 'Исследование показывает, что LLM демонстрируют систематические предвзятости при интерпретации эмоций пользователей в зависимости от их профилей. Тестирование 15 моделей выявило, что одинаковые эмоциональные ситуации интерпретируются по-разному для пользователей с разным социальным статусом. Модели проявляют более точное понимание эмоций и дают более качественные рекомендации пользователям с привилегированным статусом, что отражает социальное неравенство. Персонализация через долговременную память может непреднамеренно усиливать социальные иерархии в эмоциональном интеллекте AI-систем.'}, 'en': {'title': 'Bias in AI: Emotional Interpretation Reflects Social Hierarchies', 'desc': 'This paper explores how large language models (LLMs) interpret emotions based on user profiles, revealing systematic biases. It shows that LLMs provide different emotional responses to the same situation depending on whether the user is perceived as advantaged or disadvantaged. The study evaluates 15 models using human-validated emotional intelligence tests, finding that models often favor profiles with higher social status. This highlights a significant issue in AI personalization, where memory-enhanced systems may unintentionally perpetuate social hierarchies in emotional reasoning.'}, 'zh': {'title': '个性化AI可能加剧社会不平等', 'desc': '本研究探讨了大型语言模型（LLMs）在情感理解和支持方面的系统性偏见，尤其是如何受到用户档案的影响。我们评估了15个模型在经过人类验证的情感智力测试中的表现，发现相同情境下不同用户档案会导致情感解读的显著差异。研究表明，具有优势背景的用户更容易获得准确的情感解读，且在情感理解和支持建议任务中，LLMs在不同人口统计因素上存在显著差异。这些结果揭示了增强记忆的人工智能系统可能无意中加剧社会不平等的问题。'}}}, {'id': 'https://huggingface.co/papers/2510.11718', 'title': 'CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images', 'url': 'https://huggingface.co/papers/2510.11718', 'abstract': 'CodePlot-CoT, a code-driven Chain-of-Thought model, enhances multimodal mathematical reasoning by generating both text and executable plotting code to solve problems requiring visual assistance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for "thinking with images" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as "visual thought", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.', 'score': 3, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '3599d56e3f7eba18', 'authors': ['Chengqi Duan', 'Kaiyue Sun', 'Rongyao Fang', 'Manyuan Zhang', 'Yan Feng', 'Ying Luo', 'Yufang Liu', 'Ke Wang', 'Peng Pei', 'Xunliang Cai', 'Hongsheng Li', 'Yi Ma', 'Xihui Liu'], 'affiliations': ['CUHK', 'HKU', 'Meituan'], 'pdf_title_img': 'assets/pdf/title_img/2510.11718.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#math', '#reasoning', '#open_source', '#dataset'], 'emoji': '📊', 'ru': {'title': 'Код как визуальное мышление для математических задач', 'desc': 'Исследователи представили CodePlot-CoT — модель, которая решает сложные математические задачи, генерируя не только текстовые рассуждения, но и исполняемый код для построения графиков и визуализаций. Для обучения модели создан Math-VR — первый крупномасштабный двуязычный датасет из 178 тысяч математических задач, требующих визуального мышления. Ключевая инновация заключается в том, что VLM генерирует код для визуализации как часть Chain-of-Thought рассуждений, что обеспечивает точность и контролируемость в отличие от прямой генерации изображений. Модель показала улучшение до 21% по сравнению с базовой моделью, открывая новое направление в мультимодальном математическом reasoning.'}, 'en': {'title': 'Empowering Math with Code and Visual Thinking', 'desc': 'CodePlot-CoT is a novel model that enhances mathematical reasoning by integrating text and executable plotting code. It addresses the limitations of existing models that struggle with visual tasks, such as drawing or plotting, by generating both reasoning and visual outputs. The model is trained on a new dataset called Math-VR, which includes a large number of math problems requiring visual reasoning. Experimental results demonstrate that CodePlot-CoT significantly improves performance on these tasks, marking a breakthrough in multimodal mathematical reasoning.'}, 'zh': {'title': '代码驱动的多模态数学推理新方向', 'desc': 'CodePlot-CoT是一种基于代码的思维链模型，旨在增强多模态数学推理能力。它通过生成文本和可执行的绘图代码，帮助解决需要视觉辅助的数学问题。我们构建了Math-VR，这是第一个大规模的双语数学问题数据集，包含178K样本，以支持视觉推理。实验结果表明，CodePlot-CoT模型在新基准上比基础模型提高了21%的性能，验证了我们提出的基于代码的推理范式的有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.11391', 'title': 'DocReward: A Document Reward Model for Structuring and Stylizing', 'url': 'https://huggingface.co/papers/2510.11391', 'abstract': "DocReward, a document reward model, evaluates and enhances the structural and stylistic quality of generated documents, outperforming GPT-4o and GPT-5 in both accuracy and human-preferred document generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.", 'score': 3, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '3b81d49ce99c4b06', 'authors': ['Junpeng Liu', 'Yuzhong Zhao', 'Bowen Cao', 'Jiayu Ding', 'Yilin Jia', 'Tengchao Lv', 'Yupan Huang', 'Shaohan Huang', 'Nan Yang', 'Li Dong', 'Lei Cui', 'Tao Ge', 'Xun Wang', 'Huitian Jiao', 'Sun Mao', 'FNU Kartik', 'Si-Qing Chen', 'Wai Lam', 'Furu Wei'], 'affiliations': ['CUHK', 'Microsoft', 'UCAS', 'UMich', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2510.11391.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#dataset', '#data', '#agents', '#alignment'], 'emoji': '📄', 'ru': {'title': 'Научить AI отличать красиво оформленный документ от плохо оформленного', 'desc': 'DocReward — это reward model для оценки структуры и стиля документов, которая помогает AI-агентам генерировать профессионально выглядящие документы. Модель обучена на датасете DocPair из 117 тысяч парных документов с одинаковым содержанием, но разным уровнем профессионализма оформления. DocReward превосходит GPT-4o и GPT-5 в точности оценки на 30.6 и 19.4 процентных пункта соответственно. При использовании для генерации документов модель достигает 60.8% побед в сравнении с предпочтениями людей против 37.7% у GPT-5.'}, 'en': {'title': 'Enhancing Document Quality with DocReward', 'desc': 'DocReward is a novel document reward model designed to assess and improve the structural and stylistic quality of generated documents. Unlike previous models that focus solely on textual content, DocReward evaluates documents based on their visual structure and style, which are essential for enhancing readability and user engagement. It utilizes a large dataset called DocPair, consisting of 117,000 paired documents across various domains, to train its scoring system using the Bradley-Terry loss function. The results show that DocReward significantly outperforms existing models like GPT-4o and GPT-5 in both accuracy and user preference, making it a valuable tool for generating high-quality documents.'}, 'zh': {'title': '提升文档质量的智能评估工具', 'desc': 'DocReward是一种文档奖励模型，旨在评估和提升生成文档的结构和风格质量。它通过构建一个包含117K对文档的多领域数据集，全面评估文档的专业性。DocReward使用Bradley-Terry损失进行训练，能够有效地对文档进行评分，并在准确性上超越了GPT-4o和GPT-5。通过外部评估，DocReward在生成文档时显示出更高的胜率，证明了其在指导生成代理方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2510.10868', 'title': 'FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging\n  with Diffusion Decoding', 'url': 'https://huggingface.co/papers/2510.10868', 'abstract': 'Two merging strategies and a diffusion-based decoder improve 3D Human Mesh Recovery by reducing computational cost and slightly enhancing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline.', 'score': 3, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '4abc0308cd558402', 'authors': ['Soroush Mehraban', 'Andrea Iaboni', 'Babak Taati'], 'affiliations': ['KITE Research Institute, UHN', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.10868.jpg', 'data': {'categories': ['#benchmark', '#3d', '#optimization', '#diffusion', '#architecture'], 'emoji': '🏃', 'ru': {'title': 'Быстрое восстановление 3D-сетки человека через умное объединение слоёв и токенов', 'desc': 'Исследователи предложили два метода оптимизации transformer-моделей для восстановления 3D-сетки человека (Human Mesh Recovery). Первый метод объединяет слои трансформера с минимальным влиянием на ошибку позиции суставов, второй — удаляет фоновые токены, не влияющие на результат. Для компенсации возможного снижения качества авторы добавили diffusion-декодер, использующий временной контекст и prior-информацию о позах из больших датасетов захвата движений. Подход ускоряет работу модели в 2.3 раза при небольшом улучшении точности.'}, 'en': {'title': 'Efficient 3D Human Mesh Recovery with Smart Merging and Decoding', 'desc': 'This paper presents two innovative merging strategies to enhance 3D Human Mesh Recovery (HMR) while reducing computational costs. The Error-Constrained Layer Merging (ECLM) technique optimally merges transformer layers with minimal impact on accuracy, specifically the Mean Per Joint Position Error (MPJPE). Additionally, the Mask-guided Token Merging (Mask-ToMe) method targets the reduction of background tokens that do not significantly affect predictions. To maintain performance despite these reductions, a diffusion-based decoder is introduced, which utilizes temporal context and pose priors from extensive motion capture data, resulting in improved efficiency and performance.'}, 'zh': {'title': '提升3D人类网格恢复的速度与性能', 'desc': '本文提出了两种针对3D人类网格恢复的合并策略，分别是误差约束层合并（ECLM）和基于掩码的标记合并（Mask-ToMe），旨在降低计算成本并提高性能。ECLM选择性地合并对每个关节位置误差（MPJPE）影响最小的变换器层，而Mask-ToMe则专注于合并对最终预测贡献较小的背景标记。为了应对合并可能导致的性能下降，本文还提出了一种基于扩散的解码器，利用时间上下文和从大规模动作捕捉数据集中学习的姿态先验。实验结果表明，该方法在多个基准测试中实现了最高2.3倍的加速，同时在性能上略有提升。'}}}, {'id': 'https://huggingface.co/papers/2510.08026', 'title': 'PEAR: Phase Entropy Aware Reward for Efficient Reasoning', 'url': 'https://huggingface.co/papers/2510.08026', 'abstract': 'A reward mechanism called Phase Entropy Aware Reward (PEAR) controls the length of reasoning in large models by adjusting entropy at different stages, balancing conciseness and accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.', 'score': 3, 'issue_id': 6398, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': 'f4c7a863b396ac9a', 'authors': ['Chen Huang', 'Wei Lu', 'Wenxuan Zhang'], 'affiliations': ['Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2510.08026.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#reasoning', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Контроль длины рассуждений через энтропию на разных фазах', 'desc': 'Исследователи обнаружили, что энтропия модели коррелирует с длиной ответа на разных этапах рассуждения: высокая энтропия в фазе размышления приводит к длинным ответам, низкая в финальной фазе — к точным решениям. На основе этого наблюдения они разработали механизм наград PEAR, который штрафует избыточную энтропию в фазе размышления и допускает умеренную гибкость в финальной фазе. Такой подход позволяет Large Reasoning Models генерировать более короткие объяснения без потери точности, избегая избыточных шагов рассуждения. Эксперименты показали, что PEAR успешно сокращает длину ответов при сохранении конкурентной точности и демонстрирует хорошую устойчивость на данных вне обучающего распределения.'}, 'en': {'title': 'Balancing Conciseness and Accuracy with PEAR', 'desc': 'The paper introduces a new reward mechanism called Phase Entropy Aware Reward (PEAR) that helps large reasoning models (LRMs) generate concise yet accurate responses. It identifies a relationship between model entropy and response length, where higher entropy during the thinking phase leads to longer, more exploratory responses, while lower entropy in the final answer phase results in more deterministic outputs. PEAR adjusts the reward based on the entropy at different reasoning stages, penalizing excessive exploration in the thinking phase while allowing some flexibility in the final answer phase. This approach effectively reduces response length without compromising accuracy, demonstrating improved performance across various benchmarks and robustness to out-of-distribution scenarios.'}, 'zh': {'title': '控制推理长度，提升模型性能的PEAR机制', 'desc': '本文提出了一种名为阶段熵感知奖励（PEAR）的奖励机制，用于控制大型推理模型的推理长度。通过调整不同阶段的熵，PEAR在简洁性和准确性之间取得平衡。研究表明，模型的熵与响应长度之间存在正相关关系，思考阶段的熵较高，而最终答案阶段的熵较低。PEAR通过在思考阶段惩罚过高的熵，鼓励模型生成简洁的推理过程，同时保持足够的灵活性以正确解决任务。'}}}, {'id': 'https://huggingface.co/papers/2510.11712', 'title': 'DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training', 'url': 'https://huggingface.co/papers/2510.11712', 'abstract': 'DiT360 framework enhances panoramic image generation by hybrid training on perspective and panoramic data, incorporating cross-domain knowledge and hybrid supervision to improve boundary consistency and image fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.', 'score': 2, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '23c04d717e11fab7', 'authors': ['Haoran Feng', 'Dizhe Zhang', 'Xiangtai Li', 'Bo Du', 'Lu Qi'], 'affiliations': ['Insta360 Research', 'Nanyang Technological University', 'Tsinghua University', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11712.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#dataset', '#cv'], 'emoji': '🌐', 'ru': {'title': 'Гибридное обучение для реалистичных панорамных изображений', 'desc': 'DiT360 — это фреймворк на базе DiT для генерации панорамных изображений, использующий гибридное обучение на обычных перспективных и панорамных данных. Авторы решают проблему нехватки качественных панорамных датасетов через междоменную трансформацию и внутридоменную аугментацию на уровне изображений и токенов. На уровне изображений применяется guidance от перспективных изображений и панорамное уточнение, а на уровне токенов используются циклический padding, yaw loss для ротационной устойчивости и cube loss для учета искажений. Метод демонстрирует улучшенную консистентность границ и качество изображений в задачах text-to-panorama, inpainting и outpainting по одиннадцати метрикам.'}, 'en': {'title': 'Enhancing Panoramic Image Generation with DiT360', 'desc': 'The DiT360 framework improves the generation of panoramic images by using a combination of perspective and panoramic data for training. It addresses challenges in maintaining geometric accuracy and realistic image quality, which are often hindered by the scarcity of high-quality panoramic datasets. DiT360 employs key techniques such as inter-domain transformation and intra-domain augmentation to enhance image quality and consistency. Through extensive testing, the framework shows superior performance in boundary consistency and image fidelity across various tasks, including text-to-panorama and inpainting.'}, 'zh': {'title': 'DiT360：提升全景图像生成的创新框架', 'desc': 'DiT360框架通过在透视和全景数据上进行混合训练，增强了全景图像生成的能力。该方法解决了生成质量中的几何保真度和照片真实感问题，主要是由于缺乏大规模、高质量的真实全景数据。DiT360包含多个关键模块，用于跨域转换和域内增强，提升了感知质量并规范了多样性和真实感。通过在多个任务上的广泛实验，证明了该方法在边界一致性和图像保真度方面的优越性。'}}}, {'id': 'https://huggingface.co/papers/2510.11650', 'title': 'InfiniHuman: Infinite 3D Human Creation with Precise Control', 'url': 'https://huggingface.co/papers/2510.11650', 'abstract': 'InfiniHuman framework distills existing models to generate large-scale, richly annotated 3D human data using a diffusion-based generative pipeline, achieving high visual quality, speed, and controllability.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.', 'score': 1, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': '86292c83b31b7099', 'authors': ['Yuxuan Xue', 'Xianghui Xie', 'Margaret Kostyrko', 'Gerard Pons-Moll'], 'affiliations': ['University of Tübingen, Germany', 'University of Tübingen, Tübingen AI Center, Germany', 'University of Tübingen, Tübingen AI Center, MPI for Informatics, SIC, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2510.11650.jpg', 'data': {'categories': ['#multimodal', '#3d', '#open_source', '#diffusion', '#dataset', '#synthetic'], 'emoji': '👥', 'ru': {'title': 'Бесконечное разнообразие 3D-людей через дистилляцию AI-моделей', 'desc': 'InfiniHuman — это фреймворк для генерации реалистичных 3D-аватаров людей с использованием дистилляции существующих foundation-моделей. Система автоматически создает датасет InfiniHumanData из 111 тысяч уникальных персонажей с разнообразными характеристиками (возраст, этническая принадлежность, одежда) и мультимодальными аннотациями. На основе этого датасета обучена диффузионная модель InfiniHumanGen, которая генерирует аватары с точным контролем через текстовые описания, параметры тела SMPL и элементы одежды. Подход позволяет создавать высококачественные 3D-модели людей быстро и с минимальными затратами, превосходя существующие методы по качеству и управляемости.'}, 'en': {'title': 'Unbounded 3D Human Avatar Generation with InfiniHuman', 'desc': 'The InfiniHuman framework addresses the challenge of generating realistic 3D human avatars by distilling existing models to create large-scale, richly annotated datasets. It utilizes a diffusion-based generative pipeline to produce high-quality avatars that are controllable and diverse in attributes like ethnicity and clothing. The framework includes InfiniHumanData, a dataset with 111K unique identities, each annotated with detailed descriptions and images. This approach significantly enhances visual quality and generation speed compared to current methods, making it a practical solution for scalable avatar generation.'}, 'zh': {'title': '无限可能的3D人类数据生成', 'desc': 'InfiniHuman框架通过扩散生成管道提炼现有模型，生成大规模、丰富注释的3D人类数据，具有高视觉质量、快速性和可控性。生成逼真且可控的3D人类头像一直是一个挑战，尤其是在涵盖多样属性时。我们提出的InfiniHumanData是一个全自动管道，利用视觉-语言和图像生成模型创建大规模的多模态数据集。基于此数据集，InfiniHumanGen实现了快速、真实且可精确控制的头像生成，显著提升了视觉质量和生成速度。'}}}, {'id': 'https://huggingface.co/papers/2510.09189', 'title': 'LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning', 'url': 'https://huggingface.co/papers/2510.09189', 'abstract': 'A novel translation-enhanced recipe using layer-selective tuning on parallel data improves translation performance in both high- and low-resource languages while maintaining reasoning proficiency.  \t\t\t\t\tAI-generated summary \t\t\t\t General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available.', 'score': 1, 'issue_id': 6399, 'pub_date': '2025-10-10', 'pub_date_card': {'ru': '10 октября', 'en': 'October 10', 'zh': '10月10日'}, 'hash': 'ef9184902ab3f7b8', 'authors': ['Changjiang Gao', 'Zixian Huang', 'Jingyang Gong', 'Shujian Huang', 'Lei Li', 'Fei Yuan'], 'affiliations': ['Carnegie Mellon University', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'Shanghai Artificial Intelligent Laboratory', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.09189.jpg', 'data': {'categories': ['#low_resource', '#reasoning', '#training', '#translation', '#open_source', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Перевод без потери интеллекта: послойная настройка LLM', 'desc': 'Исследователи предложили новый метод улучшения качества перевода в больших языковых моделях без потери способности к рассуждению. Метод использует послойную выборочную донастройку только на параллельных данных, начиная с instruct-моделей. Результирующие модели Qwen3-XPlus показали значительное улучшение перевода как для высокоресурсных, так и низкоресурсных языков, включая суахили. При этом модели сохранили производительность в задачах reasoning на уровне базовой Qwen3, обучаясь только на небольших параллельных датасетах.'}, 'en': {'title': 'Enhancing Translation with Layer-Selective Tuning', 'desc': 'This paper presents a new method for improving translation in both high- and low-resource languages using a technique called layer-selective tuning on parallel data. The authors introduce the Qwen3-XPlus models, which show notable gains in translation quality, measured by metrics like spBLEU and xComet, especially in low-resource languages such as Swahili. By leveraging small parallel datasets, these models achieve better performance on multilingual tasks while retaining strong reasoning capabilities similar to the original Qwen3 instruct model. This approach simplifies the process of enhancing multilingual translation, making it more accessible for various languages.'}, 'zh': {'title': '翻译增强，推理能力双提升！', 'desc': '本文提出了一种新颖的翻译增强方法，通过对平行数据进行层选择性调优，提升了高资源和低资源语言的翻译性能，同时保持了推理能力。我们引入了Qwen3-XPlus模型，在低资源语言（如斯瓦希里语）中实现了显著的翻译性能提升，spBLEU和xComet指标均超过了15和40。即使仅使用小规模的平行数据集，Qwen3-XPlus在七个多语言任务上平均提高了1分，同时在15个流行的推理数据集上保持了与Qwen3指令模型相当的能力。该研究为多语言增强提供了一种有前景的方法，显著降低了复杂性，并提高了对更广泛语言的可及性。'}}}, {'id': 'https://huggingface.co/papers/2510.08744', 'title': 'Graph Diffusion Transformers are In-Context Molecular Designers', 'url': 'https://huggingface.co/papers/2510.08744', 'abstract': 'DemoDiff, a demonstration-conditioned diffusion model, uses molecule-score examples to guide a denoising Transformer for molecular design, outperforming larger language models and domain-specific approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5times fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000times larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.', 'score': 1, 'issue_id': 6399, 'pub_date': '2025-10-09', 'pub_date_card': {'ru': '9 октября', 'en': 'October 9', 'zh': '10月9日'}, 'hash': '4fd43962056ad19e', 'authors': ['Gang Liu', 'Jie Chen', 'Yihan Zhu', 'Michael Sun', 'Tengfei Luo', 'Nitesh V Chawla', 'Meng Jiang'], 'affiliations': ['MIT CSAIL', 'MIT-IBM Watson AI Lab, IBM Research', 'University of Notre Dame'], 'pdf_title_img': 'assets/pdf/title_img/2510.08744.jpg', 'data': {'categories': ['#training', '#optimization', '#open_source', '#diffusion', '#architecture', '#dataset', '#data'], 'emoji': '💊', 'ru': {'title': 'Молекулярный дизайн по примерам: DemoDiff учится создавать молекулы из нескольких демонстраций', 'desc': 'DemoDiff — это диффузионная модель, которая учится создавать молекулы с нужными свойствами на основе небольшого набора примеров молекул с их оценками, без текстовых описаний задачи. Модель использует новый токенизатор Node Pair Encoding, который представляет молекулы на уровне мотивов и требует в 5.5 раз меньше узлов для представления. Авторы предобучили модель на 0.7 миллиарда параметров на датасете из миллионов контекстных задач, охватывающих как лекарства, так и материалы. В результате DemoDiff превосходит языковые модели, которые в 100-1000 раз больше, и показывает лучшие результаты по сравнению с узкоспециализированными подходами в молекулярном дизайне.'}, 'en': {'title': 'Revolutionizing Molecular Design with DemoDiff', 'desc': 'DemoDiff is a novel diffusion model designed for molecular design that leverages demonstration-conditioned learning. It utilizes a small set of molecule-score examples to guide a denoising Transformer, allowing it to generate molecules that meet specific target properties. This approach overcomes the limitations of existing large language models and domain-specific methods by using a new molecular tokenizer that operates at the motif level, significantly reducing the complexity of the input data. The model has been pretrained on a vast dataset and has shown superior performance across multiple design tasks, establishing itself as a powerful tool for in-context molecular design.'}, 'zh': {'title': 'DemoDiff：分子设计的新基础模型', 'desc': 'DemoDiff是一种基于示例的扩散模型，用于分子设计。它通过少量的分子评分示例来指导去噪Transformer，从而生成符合目标属性的分子。与传统的文本描述方法相比，DemoDiff在多个设计任务中表现优异，超越了更大规模的语言模型和领域特定的方法。该模型的预训练使用了一种新的分子标记器，显著提高了效率，展示了其作为分子基础模型的潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.11647', 'title': 'IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment', 'url': 'https://huggingface.co/papers/2510.11647', 'abstract': 'IVEBench is a benchmark suite for instruction-guided video editing that addresses limitations in existing benchmarks through diverse video sources, comprehensive task coverage, and a multi-dimensional evaluation protocol.  \t\t\t\t\tAI-generated summary \t\t\t\t Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.', 'score': 0, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'd6b492d4f314d156', 'authors': ['Yinan Chen', 'Jiangning Zhang', 'Teng Hu', 'Yuxiang Zeng', 'Zhucun Xue', 'Qingdong He', 'Chengjie Wang', 'Yong Liu', 'Xiaobin Hu', 'Shuicheng Yan'], 'affiliations': ['National University of Singapore', 'Shanghai Jiao Tong University', 'Tencent Youtu Lab', 'University of Auckland', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.11647.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#video'], 'emoji': '🎬', 'ru': {'title': 'Комплексная оценка AI-редактирования видео по текстовым инструкциям', 'desc': 'IVEBench — это новый бенчмарк для оценки методов редактирования видео по текстовым инструкциям. Датасет включает 600 высококачественных видео длиной от 32 до 1024 кадров, охватывающих 8 категорий задач редактирования с 35 подкатегориями. Система оценки основана на трёх измерениях: качество видео, соответствие инструкциям и сохранение оригинального содержания, используя как традиционные метрики, так и мультимодальные LLM. Эксперименты показывают, что бенчмарк эффективно оценивает современные методы и даёт результаты, согласующиеся с человеческими оценками.'}, 'en': {'title': 'IVEBench: A Comprehensive Benchmark for Instruction-Guided Video Editing', 'desc': 'IVEBench is a new benchmark suite designed for evaluating instruction-guided video editing, addressing the shortcomings of existing benchmarks. It features a diverse collection of 600 high-quality videos and covers a wide range of editing tasks, ensuring comprehensive assessment. The evaluation protocol is multi-dimensional, focusing on video quality, adherence to instructions, and overall fidelity, using both traditional metrics and advanced assessments from large language models. Extensive testing shows that IVEBench effectively benchmarks the latest methods in instruction-guided video editing, providing results that align well with human judgment.'}, 'zh': {'title': 'IVEBench：指令引导视频编辑的全新评估标准', 'desc': 'IVEBench是一个针对指令引导视频编辑的基准测试套件，旨在解决现有基准的局限性。它通过多样化的视频来源、全面的任务覆盖和多维度的评估协议，提供了更系统的评估方法。IVEBench包含600个高质量源视频，涵盖七个语义维度，并设有8类编辑任务及35个子类别。该基准还建立了一个三维评估协议，结合了传统指标和多模态大语言模型的评估，能够有效评估最先进的指令引导视频编辑方法。'}}}, {'id': 'https://huggingface.co/papers/2510.11512', 'title': 'LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion\n  Models via Likelihood Preference', 'url': 'https://huggingface.co/papers/2510.11512', 'abstract': 'LikePhys evaluates intuitive physics in video diffusion models using a denoising objective-based metric, demonstrating better alignment with human preference than existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.', 'score': 0, 'issue_id': 6399, 'pub_date': '2025-10-13', 'pub_date_card': {'ru': '13 октября', 'en': 'October 13', 'zh': '10月13日'}, 'hash': 'd8a8f971250d7cbb', 'authors': ['Jianhao Yuan', 'Fabio Pizzati', 'Francesco Pinto', 'Lars Kunze', 'Ivan Laptev', 'Paul Newman', 'Philip Torr', 'Daniele De Martini'], 'affiliations': ['MBZUAI', 'UWE Bristol', 'University of Chicago', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2510.11512.jpg', 'data': {'categories': ['#video', '#benchmark', '#inference', '#diffusion', '#dataset', '#alignment'], 'emoji': '🎱', 'ru': {'title': 'Оценка физической интуиции видео-моделей через правдоподобие', 'desc': 'Исследователи представили LikePhys — метод оценки понимания интуитивной физики в диффузионных моделях для генерации видео. Метод работает без дополнительного обучения и различает физически корректные и невозможные видео, используя denoising objective как суррогат правдоподобия. Новая метрика Plausibility Preference Error лучше согласуется с человеческими предпочтениями, чем существующие подходы. Результаты показывают, что современные модели испытывают трудности со сложной динамикой, но их понимание физики улучшается с ростом размера модели и настроек инференса.'}, 'en': {'title': 'Evaluating Intuitive Physics in Video Models with LikePhys', 'desc': 'LikePhys is a novel method for evaluating how well video diffusion models understand intuitive physics. It uses a denoising objective to differentiate between physically valid and impossible video sequences, providing a more accurate assessment than previous methods. The evaluation metric, called Plausibility Preference Error (PPE), aligns closely with human preferences and outperforms existing evaluation baselines. The study also reveals that while current models face challenges with complex dynamics, they show improvement in physics understanding as their capacity and inference settings are enhanced.'}, 'zh': {'title': '评估视频模型的直观物理理解', 'desc': 'LikePhys 是一种评估视频扩散模型中直观物理理解的方法。它通过使用去噪目标作为评估指标，能够区分物理有效和不可能的视频。我们构建了一个包含十二种场景的基准测试，结果表明，LikePhys 的评估指标与人类偏好高度一致，优于现有的评估方法。研究还分析了模型设计和推理设置对直观物理理解的影响，显示出随着模型能力的提升，物理理解有明显改善。'}}}, {'id': 'https://huggingface.co/papers/2510.04587', 'title': 'Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole\n  Slide Image Diagnosis Behavior', 'url': 'https://huggingface.co/papers/2510.04587', 'abstract': 'A framework records and utilizes expert navigation behavior in whole-slide imaging to build an agentic system for pathology diagnosis, achieving high precision and recall in metastasis detection.  \t\t\t\t\tAI-generated summary \t\t\t\t Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired "where to look" and "why it matters" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.', 'score': 0, 'issue_id': 6398, 'pub_date': '2025-10-06', 'pub_date_card': {'ru': '6 октября', 'en': 'October 6', 'zh': '10月6日'}, 'hash': '89586b8e177d522a', 'authors': ['Sheng Wang', 'Ruiming Wu', 'Charles Herndon', 'Yihang Liu', 'Shunsuke Koga', 'Jeanne Shen', 'Zhi Huang'], 'affiliations': ['Department of Biostatistics, Epidemiology & Informatics, University of Pennsylvania', 'Department of Electrical and System Engineering, University of Pennsylvania', 'Department of Pathology and Laboratory Medicine, University of Pennsylvania', 'Department of Pathology, Stanford University', 'Department of Pathology, University of California at San Francisco'], 'pdf_title_img': 'assets/pdf/title_img/2510.04587.jpg', 'data': {'categories': ['#agents', '#agi', '#reasoning', '#healthcare', '#interpretability', '#science', '#dataset'], 'emoji': '🔬', 'ru': {'title': 'Агентная система патологии учится у экспертов через запись их навигации', 'desc': 'Исследователи создали AI Session Recorder — систему, которая незаметно записывает действия патологов при просмотре гистологических снимков и превращает эти данные в обучающий датасет. На основе записанного поведения экспертов построен агент Pathologist-o3, который сначала предлагает области интереса, а затем проводит анализ, имитируя логику специалиста. Система достигла 100% полноты и 84.5% точности в обнаружении метастазов в лимфоузлах, превзойдя OpenAI o3. Это один из первых агентных AI-систем в патологии, которая учится на реальном поведении врачей, а не только на текстовых описаниях.'}, 'en': {'title': 'Transforming Expert Navigation into Smart Pathology Diagnosis', 'desc': 'This paper presents a new framework that captures expert navigation behavior in whole-slide imaging to enhance pathology diagnosis. It introduces the AI Session Recorder, which records how pathologists interact with images and converts this data into actionable commands for an AI system. The resulting model, Pathologist-o3, uses this behavioral data to identify areas of interest and provide reasoning for its decisions, achieving impressive metrics in detecting metastasis. This approach not only improves diagnostic accuracy but also paves the way for more practical and explainable AI systems in clinical settings.'}, 'zh': {'title': '智能病理诊断的新路径', 'desc': '本研究提出了一种框架，通过记录和利用专家在全切片成像中的导航行为，构建了一种用于病理诊断的智能系统。该系统在转移性肿瘤检测中实现了高精度和高召回率。我们引入了AI会话记录器，能够无缝记录专家的常规导航，并将其转化为标准化的行为命令和边界框。最终，基于这些行为数据，我们构建了Pathologist-o3，一个能够提出感兴趣区域并进行行为引导推理的双阶段智能体。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi (1)', '#alignment (3)', '#architecture (3)', '#audio', '#benchmark (9)', '#cv (2)', '#data (4)', '#dataset (10)', '#diffusion (7)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations', '#healthcare (2)', '#inference (2)', '#interpretability (1)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (3)', '#multilingual (1)', '#multimodal (6)', '#open_source (7)', '#optimization (11)', '#plp', '#rag', '#reasoning (8)', '#rl (5)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (10)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-14 03:29',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-14 03:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-14 03:29')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    