
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 25 papers. June 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 июня</span> | <span id="title-articles-count">25 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-17.html">⬅️ <span id="prev-date">17.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-19.html">➡️ <span id="next-date">19.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 июня', 'en': 'June 18', 'zh': '6月18日'};
        let feedDateNext = {'ru': '19.06', 'en': '06/19', 'zh': '6月19日'};
        let feedDatePrev = {'ru': '17.06', 'en': '06/17', 'zh': '6月17日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.12928', 'title': 'Scaling Test-time Compute for LLM Agents', 'url': 'https://huggingface.co/papers/2506.12928', 'abstract': "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling test time compute has shown remarkable success in improving the reasoning abilities of large language models (LLMs). In this work, we conduct the first systematic exploration of applying test-time scaling methods to language agents and investigate the extent to which it improves their effectiveness. Specifically, we explore different test-time scaling strategies, including: (1) parallel sampling algorithms; (2) sequential revision strategies; (3) verifiers and merging methods; (4)strategies for diversifying rollouts.We carefully analyze and ablate the impact of different design strategies on applying test-time scaling on language agents, and have follow findings: 1. Scaling test time compute could improve the performance of agents. 2. Knowing when to reflect is important for agents. 3. Among different verification and result merging approaches, the list-wise method performs best. 4. Increasing diversified rollouts exerts a positive effect on the agent's task performance.", 'score': 29, 'issue_id': 4351, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '39c8f3e831e90d93', 'authors': ['King Zhu', 'Hanhao Li', 'Siwei Wu', 'Tianshun Xing', 'Dehua Ma', 'Xiangru Tang', 'Minghao Liu', 'Jian Yang', 'Jiaheng Liu', 'Yuchen Eleanor Jiang', 'Changwang Zhang', 'Chenghua Lin', 'Jun Wang', 'Ge Zhang', 'Wangchunshu Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2506.12928.jpg', 'data': {'categories': ['#training', '#reasoning', '#agents', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Масштабирование вычислений улучшает работу языковых агентов', 'desc': 'Исследование методов масштабирования во время тестирования для языковых агентов на основе больших языковых моделей (LLM) показало улучшение их эффективности. Были изучены различные стратегии, включая параллельную выборку, последовательную корректировку, верификацию и диверсификацию развертываний. Результаты демонстрируют, что увеличение вычислительных ресурсов на этапе тестирования повышает производительность агентов. Особенно эффективными оказались методы списочной верификации и увеличения разнообразия развертываний.'}, 'en': {'title': 'Boosting Language Agents with Test-Time Scaling', 'desc': 'This paper investigates how increasing computational resources at test time can enhance the performance of large language models (LLMs). It systematically examines various test-time scaling methods, such as parallel sampling, sequential revisions, and verification techniques. The findings indicate that scaling up computation not only boosts reasoning capabilities but also highlights the importance of strategic reflection and diverse rollouts. Notably, the study reveals that the list-wise verification method yields the best results among different merging approaches.'}, 'zh': {'title': '测试时间扩展提升语言代理性能', 'desc': '本文系统探讨了在大型语言模型（LLMs）中应用测试时间扩展方法的效果。研究表明，计算扩展能够显著提升语言代理的推理能力，尤其是通过并行采样、顺序修订、有效验证和增加多样化的回滚策略。我们分析了不同设计策略对语言代理性能的影响，并发现测试时间计算的扩展确实能提高代理的表现。特别是，采用列表式验证方法效果最佳，而多样化的回滚策略也对任务表现有积极影响。'}}}, {'id': 'https://huggingface.co/papers/2506.14429', 'title': 'LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs', 'url': 'https://huggingface.co/papers/2506.14429', 'abstract': 'This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \\textit{stable perplexity} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \\textit{local perception} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.', 'score': 27, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'd0032538675516d6', 'authors': ['Xiaoran Liu', 'Zhigeng Liu', 'Zengfeng Huang', 'Qipeng Guo', 'Ziwei He', 'Xipeng Qiu'], 'affiliations': ['School of Computer Science, Fudan University', 'Shanghai AI Lab', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14429.jpg', 'data': {'categories': ['#training', '#long_context', '#architecture', '#benchmark', '#diffusion', '#rl'], 'emoji': '🔬', 'ru': {'title': 'Диффузионные языковые модели: новые горизонты в обработке длинного контекста', 'desc': "Это исследование сравнивает производительность диффузионных и авторегрессивных языковых моделей при работе с длинным контекстом. Авторы обнаружили, что диффузионные модели демонстрируют стабильную перплексивность при расширении контекста и обладают феноменом 'локального восприятия'. На основе этих наблюдений был разработан метод LongLLaDA для расширения контекстного окна диффузионных моделей без дополнительного обучения. Исследование также выявило задачи, в которых диффузионные модели превосходят авторегрессивные при работе с длинным контекстом."}, 'en': {'title': 'Unlocking Long Contexts in Diffusion LLMs with LongLLaDA', 'desc': 'This paper explores how diffusion large language models (LLMs) perform with long contexts compared to traditional auto-regressive LLMs. It highlights that diffusion LLMs maintain stable perplexity when extending context, unlike their auto-regressive counterparts, which struggle with longer inputs. The authors introduce LongLLaDA, a method that allows for context window extension without additional training, leveraging insights from Rotary Position Embedding (RoPE) scaling. The findings reveal specific tasks where diffusion LLMs excel and others where they do not, paving the way for future research in long-context applications.'}, 'zh': {'title': '扩散模型的长上下文新方法：LongLLaDA', 'desc': '本研究探讨了扩散大语言模型（diffusion LLMs）与自回归大语言模型（auto-regressive LLMs）在长上下文性能方面的比较，识别了它们的独特特性，并提出了一种无训练的方法LongLLaDA来扩展上下文窗口。研究发现，扩散LLMs在直接上下文外推时保持了显著稳定的困惑度，而自回归模型在上下文超出预训练长度时则表现不佳。扩散LLMs展现出独特的局部感知现象，使其能够成功从最近的上下文片段中检索信息。通过旋转位置嵌入（RoPE）缩放理论，我们解释了这些现象，并验证了扩散LLMs的上下文外推方法的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.13363', 'title': 'Efficient Medical VIE via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.13363', 'abstract': 'An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Information Extraction (VIE) converts unstructured document images into structured formats like JSON, critical for medical applications such as report analysis and online consultations. Traditional methods rely on OCR and language models, while end-to-end multimodal models offer direct JSON generation. However, domain-specific schemas and high annotation costs limit their effectiveness in medical VIE. We base our approach on the Reinforcement Learning with Verifiable Rewards (RLVR) framework to address these challenges using only 100 annotated samples. Our approach ensures dataset diversity, a balanced precision-recall reward mechanism to reduce hallucinations and improve field coverage, and innovative sampling strategies to enhance reasoning capabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve state-of-the-art performance on medical VIE tasks, significantly improving F1, precision, and recall. While our models excel on tasks similar to medical datasets, performance drops on dissimilar tasks, highlighting the need for domain-specific optimization. Case studies further demonstrate the value of reasoning during training and inference for VIE.', 'score': 19, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '29de6bf10e7470ad', 'authors': ['Lijun Liu', 'Ruiyang Li', 'Zhaocheng Liu', 'Chenglin Zhu', 'Chong Li', 'Jiehan Cheng', 'Qiang Ju', 'Jian Xie'], 'affiliations': ['Baichuan Inc.', 'Peking University', 'Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13363.jpg', 'data': {'categories': ['#hallucinations', '#training', '#optimization', '#healthcare', '#reasoning', '#rl', '#multimodal'], 'emoji': '🏥', 'ru': {'title': 'RLVR: Прорыв в извлечении медицинской информации из изображений', 'desc': 'Представлена система RLVR на основе модели Qwen2.5-VL-7B для извлечения визуальной информации из медицинских документов. Система достигает наилучших результатов при ограниченном количестве размеченных примеров. RLVR улучшает способности к рассуждению и балансирует точность и полноту извлечения. Метод показывает высокую эффективность на медицинских данных, но требует оптимизации для других доменов.'}, 'en': {'title': 'Revolutionizing Medical VIE with Limited Data and Enhanced Reasoning', 'desc': "This paper presents a Reinforcement Learning with Verifiable Rewards (RLVR) framework that utilizes a fine-tuned Qwen2.5-VL-7B model to enhance Visual Information Extraction (VIE) in medical contexts. By leveraging only 100 annotated samples, the framework effectively balances precision and recall, addressing the challenges posed by limited annotated data and high annotation costs. The approach incorporates innovative sampling strategies and a balanced reward mechanism to improve reasoning capabilities and reduce hallucinations in the output. The results show significant improvements in F1 score, precision, and recall, although the model's performance varies with the similarity of the tasks to the training data."}, 'zh': {'title': '医疗视觉信息提取的创新突破', 'desc': '本文提出了一种基于强化学习可验证奖励（RLVR）框架的方法，利用微调的Qwen2.5-VL-7B模型，在医疗视觉信息提取（VIE）任务中实现了最先进的性能。该方法仅使用100个标注样本，解决了传统方法在医疗领域面临的高标注成本和领域特定模式的问题。通过确保数据集的多样性和平衡的精确率-召回率奖励机制，减少了模型的幻觉现象，并提高了领域覆盖率。案例研究进一步证明了在训练和推理过程中推理能力的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.13642', 'title': 'Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model', 'url': 'https://huggingface.co/papers/2506.13642', 'abstract': 'Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.  \t\t\t\t\tAI-generated summary \t\t\t\t The emergence of GPT-4o-like large multimodal models (LMMs) has raised the exploration of integrating text, vision, and speech modalities to support more flexible multimodal interaction. Existing LMMs typically concatenate representation of modalities along the sequence dimension and feed them into a large language model (LLM) backbone. While sequence-dimension concatenation is straightforward for modality integration, it often relies heavily on large-scale data to learn modality alignments. In this paper, we aim to model the relationships between modalities more purposefully, thereby achieving more efficient and flexible modality alignments. To this end, we propose Stream-Omni, a large language-vision-speech model with efficient modality alignments, which can simultaneously support interactions under various modality combinations. Stream-Omni employs LLM as the backbone and aligns the vision and speech to the text based on their relationships. For vision that is semantically complementary to text, Stream-Omni uses sequence-dimension concatenation to achieve vision-text alignment. For speech that is semantically consistent with text, Stream-Omni introduces a CTC-based layer-dimension mapping to achieve speech-text alignment. In this way, Stream-Omni can achieve modality alignments with less data (especially speech), enabling the transfer of text capabilities to other modalities. Experiments on various benchmarks demonstrate that Stream-Omni achieves strong performance on visual understanding, speech interaction, and vision-grounded speech interaction tasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously provide intermediate text outputs (such as ASR transcriptions and model responses) during speech interaction, offering users a comprehensive multimodal experience.', 'score': 18, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0d0624980a111254', 'authors': ['Shaolei Zhang', 'Shoutao Guo', 'Qingkai Fang', 'Yan Zhou', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13642.jpg', 'data': {'categories': ['#multimodal', '#audio', '#transfer_learning', '#cv', '#benchmark', '#agi'], 'emoji': '🔀', 'ru': {'title': 'Эффективная интеграция модальностей для мощных мультимодальных ИИ-моделей', 'desc': 'Stream-Omni - это крупная мультимодальная модель, объединяющая текст, изображения и речь. Она использует конкатенацию по измерению последовательности для визуальной информации и отображение по измерению слоев для речи, что позволяет эффективно выравнивать модальности. Модель достигает высокой производительности с меньшим количеством данных по сравнению с существующими подходами. Stream-Omni демонстрирует сильные результаты в задачах визуального понимания, речевого взаимодействия и речевого взаимодействия на основе изображений.'}, 'en': {'title': 'Stream-Omni: Efficient Multimodal Integration for Enhanced Interaction', 'desc': "Stream-Omni is a large multimodal model that effectively integrates text, vision, and speech by using innovative alignment techniques. It employs sequence-dimension concatenation for aligning vision with text and a layer-dimension mapping for aligning speech with text, which allows for more efficient learning of modality relationships. This approach reduces the reliance on large datasets, particularly for speech, while still achieving strong performance across various multimodal tasks. The model's design enables it to provide intermediate outputs during speech interactions, enhancing the overall user experience in multimodal applications."}, 'zh': {'title': 'Stream-Omni：高效的多模态整合模型', 'desc': 'Stream-Omni是一种大型多模态模型，能够有效整合文本、视觉和语音。它通过序列维度连接实现视觉与文本的对齐，并通过基于CTC的层维度映射实现语音与文本的对齐，从而在数据较少的情况下也能达到良好的性能。该模型支持多种模态组合的交互，能够在视觉理解、语音交互和视觉引导的语音交互任务中表现出色。Stream-Omni的设计使得用户在语音交互时可以同时获得中间文本输出，提供了全面的多模态体验。'}}}, {'id': 'https://huggingface.co/papers/2506.14758', 'title': 'Reasoning with Exploration: An Entropy Perspective', 'url': 'https://huggingface.co/papers/2506.14758', 'abstract': 'Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.', 'score': 17, 'issue_id': 4349, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '14595ff25bf8a37c', 'pdf_title_img': 'img/title_stub.png', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Энтропия как ключ к глубоким рассуждениям языковых моделей', 'desc': 'Статья представляет новый подход к улучшению рассуждений языковых моделей с помощью обучения с подкреплением. Авторы вводят энтропийный член в функцию преимущества, что способствует исследовательскому поведению модели. Этот метод приводит к значительному улучшению производительности на сложных задачах рассуждения, особенно по метрике Pass@K. Исследование показывает, что высокая энтропия коррелирует с ключевыми токенами, рефлексивными действиями и редкими поведениями модели.'}, 'en': {'title': 'Enhancing Language Model Reasoning through Entropy-Driven Exploration', 'desc': 'This paper introduces a new approach to enhance exploratory reasoning in language models (LMs) by modifying the advantage function in reinforcement learning (RL) with an entropy-based term. The authors highlight that traditional methods often focus on exploitation, leading to performance plateaus, and argue that incorporating entropy can promote better exploration. Their empirical analysis shows that high-entropy regions correlate with key reasoning actions, such as pivotal tokens and reflective behaviors. The proposed method not only encourages deeper reasoning chains but also significantly improves performance on the Pass@K metric, demonstrating its effectiveness in advancing LM reasoning capabilities.'}, 'zh': {'title': '增强语言模型推理的探索性', 'desc': '本文提出了一种基于熵的术语，应用于强化学习中的优势函数，以增强语言模型的探索性推理能力。这种方法通过引入熵信号，促进了探索与利用之间的平衡，解决了语言模型在复杂推理任务中性能停滞的问题。研究表明，高熵区域与三种探索性推理行为之间存在强正相关，包括关键标记、反思性行为和稀有行为。通过简单的代码修改，我们的方法显著提高了语言模型的推理能力，尤其在Pass@K指标上取得了显著进展。'}}}, {'id': 'https://huggingface.co/papers/2506.14234', 'title': 'Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team', 'url': 'https://huggingface.co/papers/2506.14234', 'abstract': "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.", 'score': 17, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '70ebdc96484832ea', 'authors': ['Md Tanzib Hosain', 'Salman Rahman', 'Md Kishor Morol', 'Md Rizwan Parvez'], 'affiliations': ['American International University-Bangladesh', 'Cornell University', 'Qatar Computing Research Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14234.jpg', 'data': {'categories': ['#training', '#agents', '#agi', '#open_source', '#reasoning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'Xolver: Опыт-ориентированные языковые агенты для экспертного рассуждения', 'desc': 'Xolver - это фреймворк мультиагентного рассуждения, который улучшает работу больших языковых моделей (LLM) за счет постоянной памяти и разнообразных модальностей опыта. Он позволяет моделям накапливать и интегрировать экспериентальные знания, подобно экспертам-решателям задач. Xolver включает в себя различные модальности опыта, такие как внешний и самостоятельный поиск, использование инструментов, совместные взаимодействия и итеративное улучшение. Даже с легковесными моделями Xolver превосходит специализированные агенты рассуждений и достигает новых лучших результатов на нескольких бенчмарках.'}, 'en': {'title': 'Empowering Language Models with Experience-Aware Reasoning', 'desc': 'Xolver is a multi-agent reasoning framework designed to enhance large language models (LLMs) by incorporating persistent memory and diverse experience modalities. Unlike traditional LLMs that treat each problem independently, Xolver allows agents to accumulate knowledge from past experiences, similar to expert problem solvers. This framework integrates various methods such as self-retrieval, tool usage, and collaborative interactions to refine reasoning and improve performance on complex tasks. As a result, Xolver consistently outperforms specialized reasoning agents, achieving state-of-the-art results on several benchmarks, demonstrating the importance of experience-aware learning in AI.'}, 'zh': {'title': 'Xolver：经验驱动的推理框架', 'desc': 'Xolver是一个多智能体推理框架，通过持久记忆和多样化的经验模式增强大型语言模型（LLM），从而提高复杂推理任务的表现。与传统的LLM孤立处理每个问题不同，Xolver能够整合和积累经验知识，模拟专家问题解决者的思维方式。它通过外部和自我检索、工具使用、协作互动等多种经验模式，避免从头生成解决方案。Xolver在多个基准测试中表现优异，展示了整体经验学习在实现通用智能体方面的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.14245', 'title': 'Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs', 'url': 'https://huggingface.co/papers/2506.14245', 'abstract': 'RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the Pass@K metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the Pass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT-Pass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT-Pass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.', 'score': 15, 'issue_id': 4348, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c78cc63a970ea4e9', 'authors': ['Xumeng Wen', 'Zihan Liu', 'Shun Zheng', 'Zhijian Xu', 'Shengyu Ye', 'Zhirong Wu', 'Xiao Liang', 'Yang Wang', 'Junjie Li', 'Ziming Miao', 'Jiang Bian', 'Mao Yang'], 'affiliations': ['Microsoft Research Asia', 'Peking University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2506.14245.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#training', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RLVR: путь к логически обоснованным рассуждениям ИИ', 'desc': 'Статья представляет новый подход к улучшению рассуждений моделей машинного обучения - Reinforcement Learning with Verifiable Rewards (RLVR). Авторы вводят более точную метрику оценки CoT-Pass@K, которая учитывает корректность как цепочки рассуждений, так и конечного ответа. Исследование показывает, что RLVR действительно способствует обобщению правильных рассуждений для всех значений K. Результаты подтверждают потенциал RLVR для существенного улучшения машинных рассуждений.'}, 'en': {'title': 'Enhancing Machine Reasoning with RLVR and CoT-Pass@K', 'desc': 'Reinforcement Learning with Verifiable Rewards (RLVR) enhances the reasoning abilities of Large Language Models (LLMs) by promoting logical thought processes. The study identifies a flaw in the existing evaluation metric, Pass@K, which inaccurately rewards correct answers that may stem from faulty reasoning paths. To improve this, the authors propose a new metric, CoT-Pass@K, that ensures both the reasoning chain and the final answer are accurate. The findings demonstrate that RLVR can effectively encourage correct reasoning from the early stages of training, leading to better generalization across various scenarios.'}, 'zh': {'title': 'RLVR：推动机器推理的新方法', 'desc': 'RLVR（可验证奖励的强化学习）通过激励正确和逻辑的思维链，推动了机器推理的发展。研究发现，传统的评估指标Pass@K存在缺陷，可能会错误地认可不完整的思维链所得到的正确答案。为了解决这个问题，本文提出了更精确的评估指标CoT-Pass@K，要求推理路径和最终答案都必须正确。我们的实验证明，RLVR能够有效激励正确推理的泛化，并且这种增强的推理能力在训练早期就能显现。'}}}, {'id': 'https://huggingface.co/papers/2506.12860', 'title': 'QFFT, Question-Free Fine-Tuning for Adaptive Reasoning', 'url': 'https://huggingface.co/papers/2506.12860', 'abstract': 'Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios.', 'score': 12, 'issue_id': 4348, 'pub_date': '2025-06-15', 'pub_date_card': {'ru': '15 июня', 'en': 'June 15', 'zh': '6月15日'}, 'hash': '4e8d6c1da3d2fdd1', 'authors': ['Wanlong Liu', 'Junxiao Xu', 'Fei Yu', 'Yukang Lin', 'Ke Ji', 'Wenyu Chen', 'Yan Xu', 'Yasheng Wang', 'Lifeng Shang', 'Benyou Wang'], 'affiliations': ['Huawei Noahs Ark Lab', 'The Chinese University of Hong Kong, Shenzhen', 'University of Electronic Science and Technology of China, Chengdu, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.12860.jpg', 'data': {'categories': ['#training', '#math', '#long_context', '#reasoning', '#low_resource'], 'emoji': '🧠', 'ru': {'title': 'QFFT: эффективное обучение ИИ гибкому мышлению', 'desc': 'Статья представляет новый метод обучения когнитивных моделей - Question-Free Fine-Tuning (QFFT). QFFT позволяет моделям эффективно использовать как короткие, так и длинные цепочки рассуждений. Этот подход сокращает среднюю длину ответов более чем на 50%, сохраняя при этом производительность на уровне стандартного обучения с учителем. QFFT также демонстрирует превосходные результаты в сценариях с шумом, вне домена и при ограниченных ресурсах.'}, 'en': {'title': 'Efficient Reasoning with Question-Free Fine-Tuning', 'desc': 'This paper introduces Question-Free Fine-Tuning (QFFT), a method that enhances cognitive models by combining short and long chain-of-thought reasoning patterns. QFFT addresses the issue of overthinking in Long Chain-of-Thought models, which can lead to unnecessary complexity in responses. By training models without input questions, QFFT allows them to learn from Long CoT responses while primarily using Short CoT patterns for efficiency. The results show that QFFT significantly reduces response length and performs well across various challenging scenarios, outperforming traditional Supervised Fine-Tuning methods in specific contexts.'}, 'zh': {'title': '无问微调：高效适应的推理新方法', 'desc': '这篇论文提出了一种新的微调方法，称为无问微调（QFFT），旨在提高认知模型的效率和适应性。通过结合短链和长链推理模式，QFFT能够在保持性能的同时减少响应长度。研究表明，短链推理在简单问题上表现出色，而长链推理在复杂任务中更具优势。实验结果显示，QFFT在多个数学数据集上平均响应长度减少超过50%，并在噪声、域外和低资源场景中表现优于传统的监督微调（SFT）。'}}}, {'id': 'https://huggingface.co/papers/2506.12278', 'title': 'Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure', 'url': 'https://huggingface.co/papers/2506.12278', 'abstract': 'TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs in test-case generation. TestCase-Eval includes 500 algorithm problems and 100,000 human-crafted solutions from the Codeforces platform. It focuses on two pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test sets probe diverse input scenarios and cover a wide range of potential failure modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored test input that reveals a specific incorrect code implementation. We provide a comprehensive assessment of 19 state-of-the-art open-source and proprietary LLMs on TestCase-Eval, offering insights into their strengths and limitations in generating effective test cases for algorithm problems.', 'score': 12, 'issue_id': 4348, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'c852db550c523453', 'authors': ['Zheyuan Yang', 'Zexi Kuang', 'Xue Xia', 'Yilun Zhao'], 'affiliations': ['HKUST', 'Northeastern University', 'Tongji University', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12278.jpg', 'data': {'categories': ['#open_source', '#optimization', '#benchmark'], 'emoji': '🧪', 'ru': {'title': 'TestCase-Eval: Новый стандарт оценки ЯМ в генерации тестов', 'desc': 'TestCase-Eval - это новый бенчмарк для оценки способности языковых моделей (ЯМ) генерировать тестовые случаи для алгоритмических задач. Он включает 500 задач и 100 000 решений с платформы Codeforces. Бенчмарк фокусируется на двух ключевых задачах: покрытие ошибок и выявление ошибок. Авторы провели оценку 19 современных ЯМ на TestCase-Eval, предоставив анализ их сильных и слабых сторон в генерации эффективных тестовых случаев.'}, 'en': {'title': 'Evaluating LLMs for Effective Test Case Generation', 'desc': 'TestCase-Eval is a benchmark designed to assess the performance of large language models (LLMs) in generating effective test cases for algorithmic problems. It consists of 500 algorithm problems paired with 100,000 human-created solutions sourced from the Codeforces platform. The evaluation focuses on two main aspects: Fault Coverage, which checks how well the generated test cases explore various input scenarios, and Fault Exposure, which determines the ability of LLMs to create specific test inputs that can uncover flaws in code implementations. The study evaluates 19 different LLMs, providing valuable insights into their capabilities and limitations in this area.'}, 'zh': {'title': '评估LLM生成测试用例的新基准', 'desc': 'TestCase-Eval是一个用于评估大型语言模型（LLMs）生成算法问题测试用例的新基准。它包含500个算法问题和来自Codeforces平台的100,000个人工解决方案。该基准关注两个关键任务：故障覆盖性，评估LLM生成的测试集是否能够探测多样的输入场景；故障暴露性，评估LLM是否能够生成特定的测试输入以揭示代码实现中的错误。我们对19个最先进的开源和专有LLM在TestCase-Eval上的表现进行了全面评估，提供了它们在生成有效测试用例方面的优缺点的见解。'}}}, {'id': 'https://huggingface.co/papers/2506.14606', 'title': 'Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees', 'url': 'https://huggingface.co/papers/2506.14606', 'abstract': 'A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.', 'score': 10, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c414a1f31e0417da', 'authors': ['Ahmed Heakl', 'Sarim Hashmi', 'Chaimaa Abi', 'Celine Lee', 'Abdulrahman Mahmoud'], 'affiliations': ['Cornell University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14606.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#benchmark', '#data', '#science'], 'emoji': '🔄', 'ru': {'title': 'ЯМБ и тестирование объединяются для эффективной транспиляции между архитектурами процессоров', 'desc': 'Статья представляет новый подход к транспиляции программ между различными архитектурами процессоров с использованием больших языковых моделей и методов тестирования программного обеспечения. Метод GG (Guaranteed Guess) генерирует варианты перевода кода с одной архитектуры на другую с помощью ЯМБ и проверяет их корректность через встроенную систему тестирования. Результаты показывают высокую точность перевода (99% для HumanEval и 49% для BringupBench) и превосходство над существующими решениями по производительности, энергоэффективности и использованию памяти. Авторы планируют открыть исходный код, данные и модели для дальнейших исследований в этой области.'}, 'en': {'title': 'Efficient ISA Translation with Guaranteed Guess', 'desc': 'This paper presents a new transpilation pipeline called GG (Guaranteed Guess) that focuses on translating programs between complex (CISC) and reduced (RISC) instruction set architectures (ISAs). By leveraging large language models (LLMs) for generating translation candidates, the pipeline integrates software testing to ensure high correctness and efficiency. The authors demonstrate that their approach achieves over 99% functional correctness on specific benchmarks and outperforms the existing Rosetta 2 framework in terms of runtime speed, energy efficiency, and memory usage. The research aims to enhance the portability of code across different hardware architectures and will provide open-source resources for further exploration in ISA-level code translation.'}, 'zh': {'title': '高效准确的ISA转译新方法', 'desc': '本文提出了一种新的ISA中心的转译管道，利用大型语言模型（LLMs）和软件测试技术，实现了在复杂和简化硬件架构之间的高效且正确的代码转换。该方法通过LLM生成候选翻译，并将其嵌入软件测试框架中，以提高翻译的可靠性。我们在两个不同的数据集上评估了该方法，达到了99%的功能和语义正确率，并且在性能上优于现有的Rosetta 2框架。最终，我们将开源代码、数据、模型和基准，以推动ISA级代码翻译研究的发展。'}}}, {'id': 'https://huggingface.co/papers/2506.14603', 'title': 'Align Your Flow: Scaling Continuous-Time Flow Map Distillation', 'url': 'https://huggingface.co/papers/2506.14603', 'abstract': 'Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.', 'score': 9, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': 'c235653dff87ea28', 'authors': ['Amirmojtaba Sabour', 'Sanja Fidler', 'Karsten Kreis'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.14603.jpg', 'data': {'categories': ['#training', '#dataset', '#cv', '#benchmark', '#optimization', '#diffusion', '#small_models'], 'emoji': '🌊', 'ru': {'title': 'Flow maps: революция в эффективной генерации изображений', 'desc': "Статья представляет новый подход к генеративному моделированию под названием 'flow maps'. Эта техника позволяет эффективно соединять любые два уровня шума за один шаг, сохраняя высокую производительность при различном количестве шагов. Авторы вводят новые непрерывные по времени целевые функции и техники обучения для flow maps. Модели, обученные с помощью этого метода, достигают наилучших результатов в генерации изображений и преобразовании текста в изображения за небольшое количество шагов."}, 'en': {'title': 'Flow Maps: Efficient Few-Step Generative Modeling', 'desc': 'This paper introduces flow maps, a new approach in generative modeling that connects different noise levels in a single step, allowing for efficient image and text-to-image generation. Unlike traditional diffusion and consistency models, which require many sampling steps and degrade in performance with increased steps, flow maps maintain effectiveness across all step counts. The authors propose two continuous-time training objectives and novel techniques that enhance the training of flow maps, including autoguidance and adversarial finetuning. The results demonstrate that their method, called Align Your Flow, achieves state-of-the-art performance in few-step generation tasks on various benchmarks, outperforming existing models in both image and text-conditioned synthesis.'}, 'zh': {'title': '流图模型：高效的少步骤生成新方法', 'desc': '本文介绍了一种新的流图模型，旨在提高图像和文本到图像生成的效率。流图通过在单一步骤中连接任意两个噪声水平，克服了传统扩散和流模型在多步骤采样中的性能下降问题。我们提出了两种新的连续时间目标和训练技术，进一步优化了流图的训练过程。实验结果表明，流图模型在图像生成基准测试中表现出色，尤其是在少步骤生成任务中，达到了最先进的性能。'}}}, {'id': 'https://huggingface.co/papers/2506.13977', 'title': 'CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios', 'url': 'https://huggingface.co/papers/2506.13977', 'abstract': 'A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.  \t\t\t\t\tAI-generated summary \t\t\t\t The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at https://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.', 'score': 7, 'issue_id': 4351, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': 'd72fbcfec0e28e92', 'authors': ['Shiting Huang', 'Zhen Fang', 'Zehui Chen', 'Siyu Yuan', 'Junjie Ye', 'Yu Zeng', 'Lin Chen', 'Qi Mao', 'Feng Zhao'], 'affiliations': ['Communication University of China', 'Fudan University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13977.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#optimization'], 'emoji': '🛠️', 'ru': {'title': 'Повышение надежности языковых моделей при работе с инструментами', 'desc': 'CRITICTOOL - это комплексный бенчмарк для оценки и повышения устойчивости больших языковых моделей (LLM) при обработке ошибок во время использования инструментов. Он анализирует типы ошибок, возникающих в процессе вызова функций, и использует эволюционную стратегию для создания набора данных с разнообразными ошибками различной сложности. CRITICTOOL проводит обширные эксперименты для оценки способности LLM к рефлексии при использовании инструментов. Этот бенчмарк предлагает новый взгляд на область обучения инструментам в контексте больших языковых моделей.'}, 'en': {'title': 'Enhancing LLM Robustness with CRITICTOOL', 'desc': 'This paper introduces CRITICTOOL, a benchmark designed to assess and improve the robustness of large language models (LLMs) when using external tools. It identifies and categorizes various errors that can occur during the function-calling process, especially as tasks become more complex. The benchmark employs an innovative evolutionary strategy for dataset construction, ensuring a wide range of tool-use errors that mimic real-world challenges. Through extensive experiments, the authors demonstrate the effectiveness of CRITICTOOL in enhancing the error-handling capabilities of LLMs and provide insights into their tool reflection abilities.'}, 'zh': {'title': '提升大型语言模型工具使用的鲁棒性', 'desc': '本文介绍了CRITICTOOL，一个全面的基准测试工具，用于评估和增强大型语言模型在使用工具时处理错误的能力。随着任务的复杂性增加，工具使用过程中可能会出现各种意外错误，因此有效处理这些错误成为了一个重要的研究方向。我们分析了在多个竞争性工具评估基准中遇到的错误类型，并基于此构建了CRITICTOOL，专注于工具学习的批判性评估。通过广泛的实验，我们验证了该基准策略的有效性，并提供了对不同大型语言模型工具反应能力的深入分析。'}}}, {'id': 'https://huggingface.co/papers/2506.13651', 'title': 'xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations', 'url': 'https://huggingface.co/papers/2506.13651', 'abstract': "We introduce xbench, a dynamic, profession-aligned evaluation suite designed to bridge the gap between AI agent capabilities and real-world productivity. While existing benchmarks often focus on isolated technical skills, they may not accurately reflect the economic value agents deliver in professional settings. To address this, xbench targets commercially significant domains with evaluation tasks defined by industry professionals. Our framework creates metrics that strongly correlate with productivity value, enables prediction of Technology-Market Fit (TMF), and facilitates tracking of product capabilities over time. As our initial implementations, we present two benchmarks: Recruitment and Marketing. For Recruitment, we collect 50 tasks from real-world headhunting business scenarios to evaluate agents' abilities in company mapping, information retrieval, and talent sourcing. For Marketing, we assess agents' ability to match influencers with advertiser needs, evaluating their performance across 50 advertiser requirements using a curated pool of 836 candidate influencers. We present initial evaluation results for leading contemporary agents, establishing a baseline for these professional domains. Our continuously updated evalsets and evaluations are available at https://xbench.org.", 'score': 5, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'ce4f94d367671b84', 'authors': ['Kaiyuan Chen', 'Yixin Ren', 'Yang Liu', 'Xiaobo Hu', 'Haotong Tian', 'Tianbao Xie', 'Fangfu Liu', 'Haoye Zhang', 'Hongzhang Liu', 'Yuan Gong', 'Chen Sun', 'Han Hou', 'Hui Yang', 'James Pan', 'Jianan Lou', 'Jiayi Mao', 'Jizheng Liu', 'Jinpeng Li', 'Kangyi Liu', 'Kenkun Liu', 'Rui Wang', 'Run Li', 'Tong Niu', 'Wenlong Zhang', 'Wenqi Yan', 'Xuanzheng Wang', 'Yuchen Zhang', 'Yi-Hsin Hung', 'Yuan Jiang', 'Zexuan Liu', 'Zihan Yin', 'Zijian Ma', 'Zhiwen Mo'], 'affiliations': ['Carnegie Mellon University', 'Fudan University', 'Imperial College London', 'Massachusetts Institute of Technology', 'National University of Singapore', 'Peking University', 'Shanghai Jiao Tong University', 'Stanford University', 'The Chinese University of Hong Kong (Shenzhen)', 'The Ohio State University', 'Tsinghua University', 'University of Chinese Academy of Sciences', 'University of Oxford', 'University of Pennsylvania', 'University of Science and Technology of China', 'University of Sydney', 'University of Toronto', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2506.13651.jpg', 'data': {'categories': ['#benchmark', '#agents'], 'emoji': '📊', 'ru': {'title': 'xbench: оценка ИИ-агентов в реальных профессиональных задачах', 'desc': 'Представлен xbench - динамический набор оценок для ИИ-агентов, ориентированный на реальные профессиональные задачи. В отличие от существующих бенчмарков, xbench фокусируется на коммерчески значимых областях с задачами, определенными профессионалами индустрии. Фреймворк создает метрики, коррелирующие с продуктивностью, позволяет прогнозировать соответствие технологий рынку и отслеживать возможности продуктов. Представлены два начальных бенчмарка: для рекрутинга и маркетинга, оценивающие способности агентов в реальных бизнес-сценариях.'}, 'en': {'title': 'Bridging AI Performance with Real-World Productivity', 'desc': "The paper introduces xbench, a new evaluation suite aimed at assessing AI agents in real-world professional contexts. Unlike traditional benchmarks that focus on isolated skills, xbench emphasizes the economic impact of AI agents in industries like recruitment and marketing. It includes tasks defined by industry experts to ensure relevance and creates metrics that correlate with productivity value. The initial benchmarks evaluate agents' performance in real-world scenarios, providing a baseline for future assessments and tracking improvements over time."}, 'zh': {'title': 'xbench：连接AI能力与真实生产力的桥梁', 'desc': '我们介绍了xbench，这是一个动态的、与职业相关的评估套件，旨在弥合人工智能代理能力与现实世界生产力之间的差距。现有的基准测试通常关注孤立的技术技能，但可能无法准确反映代理在专业环境中所带来的经济价值。为了解决这个问题，xbench针对商业上重要的领域，评估任务由行业专业人士定义。我们的框架创建了与生产力价值高度相关的指标，能够预测技术市场契合度（TMF），并便于跟踪产品能力的变化。'}}}, {'id': 'https://huggingface.co/papers/2506.14002', 'title': 'Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders', 'url': 'https://huggingface.co/papers/2506.14002', 'abstract': "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.  \t\t\t\t\tAI-generated summary \t\t\t\t We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.", 'score': 4, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': 'f27daadffcce7200', 'authors': ['Siyu Chen', 'Heejune Sheen', 'Xuyuan Xiong', 'Tianhao Wang', 'Zhuoran Yang'], 'affiliations': ['Antai College of Economics and Management, Shanghai Jiao Tong University', 'Department of Statistics and Data Science, Yale University', 'Toyota Technological Institute at Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2506.14002.jpg', 'data': {'categories': ['#training', '#architecture', '#interpretability', '#math', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в интерпретации языковых моделей: теоретически обоснованное извлечение признаков', 'desc': 'Исследователи представили новый статистический фреймворк и алгоритм обучения под названием Group Bias Adaptation для улучшения разреженных автоэнкодеров (Sparse Autoencoders). Этот метод позволяет извлекать моносемантические признаки из больших языковых моделей (Large Language Models) с теоретическими гарантиями. Авторы предложили новое понятие идентифицируемости признаков, моделируя полисемантические признаки как разреженные смеси моносемантических концепций. Эксперименты показали превосходную производительность метода на языковых моделях с до 1,5 миллиардов параметров по сравнению с эталонными методами.'}, 'en': {'title': 'Enhancing Feature Recovery in Language Models with Group Bias Adaptation', 'desc': 'This paper introduces a new method called Group Bias Adaptation (GBA) to improve Sparse Autoencoders (SAEs) for extracting clear features from Large Language Models (LLMs). The authors address the limitations of existing SAE training methods, which often lack solid mathematical backing and can be unstable. They propose a statistical framework that models complex features as combinations of simpler, clear concepts, ensuring better feature recovery. The new training algorithm not only provides theoretical guarantees for recovering these features but also shows better performance compared to traditional methods when tested on large models.'}, 'zh': {'title': '群体偏差适应：提升稀疏自编码器的单义特征恢复能力', 'desc': '本文提出了一种新的统计框架和训练算法，称为群体偏差适应（Group Bias Adaptation），旨在增强稀疏自编码器（Sparse Autoencoders）在大型语言模型中的单义特征恢复能力。现有的稀疏自编码器训练算法缺乏严格的数学保证，并且在超参数敏感性和不稳定性方面存在实际限制。我们通过引入特征可识别性的新概念，解决了特征恢复问题，并提出了一种基于偏差适应的新训练算法。理论证明该算法能够在特定统计模型下正确恢复所有单义特征，从而为稀疏自编码器的训练提供了理论支持。'}}}, {'id': 'https://huggingface.co/papers/2506.10100', 'title': 'EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models', 'url': 'https://huggingface.co/papers/2506.10100', 'abstract': 'EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.', 'score': 4, 'issue_id': 4348, 'pub_date': '2025-06-11', 'pub_date_card': {'ru': '11 июня', 'en': 'June 11', 'zh': '6月11日'}, 'hash': '6a877c4c5f1d1f72', 'authors': ['Yantai Yang', 'Yuhao Wang', 'Zichen Wen', 'Luo Zhongwei', 'Chang Zou', 'Zhipeng Zhang', 'Chuan Wen', 'Linfeng Zhang'], 'affiliations': ['Harbin Institute of Technology', 'School of Artificial Intelligence, Shanghai Jiao Tong University', 'University of Electronic Science and Technology of China', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.10100.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#architecture', '#inference', '#multimodal'], 'emoji': '🚀', 'ru': {'title': 'EfficientVLA: ускорение моделей VLA без потери качества', 'desc': 'EfficientVLA - это фреймворк для ускорения вывода моделей Vision-Language-Action (VLA). Он использует три основные стратегии: обрезание избыточных слоев в языковом модуле, оптимизацию отбора визуальных токенов и кэширование промежуточных признаков в диффузионной голове действий. Применение EfficientVLA к модели CogACT позволило достичь ускорения вывода в 1,93 раза и сокращения FLOP на 71,1% при минимальном снижении точности. Этот подход позволяет эффективно устранить вычислительные и память барьеры в моделях VLA.'}, 'en': {'title': 'Accelerating VLA Models with EfficientVLA', 'desc': 'EfficientVLA is a framework designed to speed up Vision-Language-Action (VLA) models by addressing their high computational and memory requirements. It achieves this by pruning unnecessary language layers, optimizing the selection of visual tokens, and caching important features during the action generation process. This approach not only reduces the overall processing time but also minimizes the number of floating-point operations (FLOPs) needed for inference. As a result, EfficientVLA significantly enhances the efficiency of VLA models while maintaining a high level of performance.'}, 'zh': {'title': '高效加速视觉-语言-动作模型的解决方案', 'desc': 'EfficientVLA是一种加速视觉-语言-动作（VLA）模型的框架，通过修剪语言层、优化视觉标记选择和缓存中间特征来提高效率。该方法系统性地消除了计算和内存瓶颈，解决了现有加速方法无法全面应对的问题。通过分析层间冗余，EfficientVLA去除了功能不重要的语言模块层，并采用任务感知策略优化视觉处理路径。实验结果表明，应用EfficientVLA后，标准VLA模型CogACT的推理速度提高了1.93倍，FLOPs减少至28.9%，成功率仅下降0.6%。'}}}, {'id': 'https://huggingface.co/papers/2506.10038', 'title': 'Ambient Diffusion Omni: Training Good Models with Bad Data', 'url': 'https://huggingface.co/papers/2506.10038', 'abstract': 'Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.  \t\t\t\t\tAI-generated summary \t\t\t\t We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times.', 'score': 3, 'issue_id': 4349, 'pub_date': '2025-06-10', 'pub_date_card': {'ru': '10 июня', 'en': 'June 10', 'zh': '6月10日'}, 'hash': 'f746e9dd50fb7b78', 'authors': ['Giannis Daras', 'Adrian Rodriguez-Munoz', 'Adam Klivans', 'Antonio Torralba', 'Constantinos Daskalakis'], 'affiliations': ['Massachusetts Institute of Technology', 'The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2506.10038.jpg', 'data': {'categories': ['#training', '#cv', '#dataset', '#synthetic', '#diffusion', '#data'], 'emoji': '🖼️', 'ru': {'title': 'Извлечение пользы из шума: улучшение диффузионных моделей с помощью низкокачественных изображений', 'desc': 'Статья представляет фреймворк Ambient Diffusion Omni, который использует низкокачественные изображения для улучшения диффузионных моделей. Авторы показывают, что даже отбракованные изображения могут быть полезны при обучении, используя свойства естественных изображений, такие как спектральный степенной закон затухания и локальность. Фреймворк успешно применяется для улучшения FID на ImageNet и качества генерации изображений по тексту. Теоретическое обоснование подхода анализирует компромисс между обучением на смещенных данных и ограниченных несмещенных данных.'}, 'en': {'title': 'Unlocking Potential: Enhancing Diffusion Models with Low-Quality Images', 'desc': 'The Ambient Diffusion Omni framework enhances diffusion models by effectively utilizing low-quality images, which are often overlooked. It demonstrates that these lower-quality images can significantly improve model performance on tasks like text-to-image generation. The framework leverages natural image properties, such as spectral power law decay and locality, to extract valuable signals during training. By validating its approach with various synthetic corruptions, the framework achieves state-of-the-art results in ImageNet FID, showcasing improved image quality and diversity.'}, 'zh': {'title': '利用低质量图像提升扩散模型的质量', 'desc': '本论文提出了一种名为Ambient Diffusion Omni的框架，利用低质量图像来提升扩散模型的性能。研究表明，通常被丢弃的低质量图像实际上具有很大的价值，可以改善模型的训练效果。该框架利用自然图像的两个特性——谱功率法则衰减和局部性，成功地从合成模糊、JPEG压缩和运动模糊的图像中提取信号。最终，我们在ImageNet FID上取得了最先进的结果，并显著提高了文本到图像生成模型的图像质量和多样性。'}}}, {'id': 'https://huggingface.co/papers/2506.05336', 'title': 'VideoMolmo: Spatio-Temporal Grounding Meets Pointing', 'url': 'https://huggingface.co/papers/2506.05336', 'abstract': 'VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatio-temporal localization is vital for precise interactions across diverse domains, from biological research to autonomous navigation and interactive interfaces. Current video-based approaches, while proficient in tracking, lack the sophisticated reasoning capabilities of large language models, limiting their contextual understanding and generalization. We introduce VideoMolmo, a large multimodal model tailored for fine-grained spatio-temporal pointing conditioned on textual descriptions. Building upon the Molmo architecture, VideoMolmo incorporates a temporal module utilizing an attention mechanism to condition each frame on preceding frames, ensuring temporal consistency. Additionally, our novel temporal mask fusion pipeline employs SAM2 for bidirectional point propagation, significantly enhancing coherence across video sequences. This two-step decomposition, i.e., first using the LLM to generate precise pointing coordinates, then relying on a sequential mask-fusion module to produce coherent segmentation, not only simplifies the task for the language model but also enhances interpretability. Due to the lack of suitable datasets, we curate a comprehensive dataset comprising 72k video-caption pairs annotated with 100k object points. To evaluate the generalization of VideoMolmo, we introduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five real-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving, Video-GUI Interaction, and Robotics. We also evaluate our model on Referring Video Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to existing models, VideoMolmo substantially improves spatio-temporal pointing accuracy and reasoning capability. Our code and models are publicly available at https://github.com/mbzuai-oryx/VideoMolmo.', 'score': 3, 'issue_id': 4348, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': 'def5ee56ea3b6157', 'authors': ['Ghazi Shazan Ahmad', 'Ahmed Heakl', 'Hanan Gani', 'Abdelrahman Shaker', 'Zhiqiang Shen', 'Ranjay Krishna', 'Fahad Shahbaz Khan', 'Salman Khan'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Australian National University', 'Linköping University', 'Mohamed Bin Zayed University of Artificial Intelligence', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2506.05336.jpg', 'data': {'categories': ['#dataset', '#interpretability', '#benchmark', '#open_source', '#reasoning', '#video', '#multimodal'], 'emoji': '🎯', 'ru': {'title': 'Точная локализация объектов в видео с помощью ИИ', 'desc': 'VideoMolmo - это мультимодальная модель для точной пространственно-временной локализации объектов в видео на основе текстовых описаний. Она использует временной модуль с механизмом внимания для обеспечения временной согласованности между кадрами. Модель также применяет SAM2 для двунаправленного распространения точек, что значительно улучшает согласованность масок объектов в видеопоследовательностях. VideoMolmo превосходит существующие модели по точности указания объектов и способности к рассуждениям в различных сценариях реального мира.'}, 'en': {'title': 'Enhancing Spatio-Temporal Reasoning with VideoMolmo', 'desc': 'VideoMolmo is a multimodal model designed to improve spatio-temporal pointing accuracy and reasoning in various real-world applications. It combines a temporal attention mechanism with a novel mask fusion technique called SAM2, which enhances the coherence of video sequences. By generating precise pointing coordinates through a large language model and then refining them with a mask-fusion module, VideoMolmo simplifies the task and improves interpretability. The model is evaluated on a newly curated dataset and a challenging benchmark, demonstrating significant advancements over existing video-based approaches.'}, 'zh': {'title': 'VideoMolmo：提升时空指向与推理能力的多模态模型', 'desc': 'VideoMolmo是一种多模态模型，结合了时间注意机制和SAM2进行掩膜融合，显著提高了时空指向的准确性和推理能力。该模型专为基于文本描述的细粒度时空指向而设计，能够在不同的真实场景中进行精确交互。通过引入时间模块和双向点传播的掩膜融合管道，VideoMolmo确保了视频序列的时间一致性和连贯性。我们还创建了一个包含72,000个视频-字幕对的数据集，以评估模型在多种真实场景中的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.14755', 'title': 'Optimizing Length Compression in Large Reasoning Models', 'url': 'https://huggingface.co/papers/2506.14755', 'abstract': 'LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.', 'score': 1, 'issue_id': 4347, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '837a56d067dd6e74', 'authors': ['Zhengxiang Cheng', 'Dongping Chen', 'Mingyang Fu', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.14755.jpg', 'data': {'categories': ['#reasoning', '#training', '#architecture', '#benchmark', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'LC-R1: Оптимизация рассуждений ИИ без потери качества', 'desc': 'LC-R1 - это метод пост-обучения для больших моделей рассуждений (LRM), основанный на принципах краткости и достаточности. Он нацелен на устранение избыточных рассуждений в выводах моделей без существенной потери точности. LC-R1 использует комбинацию наград за длину и сжатие в рамках оптимизации групповой относительной политики (GRPO). Эксперименты показывают, что метод сокращает длину последовательностей примерно на 50% при падении точности всего на 2%.'}, 'en': {'title': 'Streamlining Reasoning: LC-R1 for Efficient Large Models', 'desc': "The paper introduces LC-R1, a post-training method aimed at improving Large Reasoning Models (LRMs) by reducing unnecessary reasoning while maintaining accuracy. It identifies 'invalid thinking' as a key issue where models redundantly verify correct answers, leading to verbosity. To combat this, the authors propose two principles: Brevity, which focuses on cutting out redundant reasoning, and Sufficiency, which ensures essential reasoning steps are retained. Through experiments, LC-R1 demonstrates a significant reduction in reasoning sequence length by about 50% with only a slight accuracy drop of around 2%, showcasing an effective balance between compression and performance."}, 'zh': {'title': '简化推理，提升效率！', 'desc': 'LC-R1是一种后训练方法，旨在通过简洁性和充分性原则来减少大型推理模型中的不必要推理，同时保持较小的准确性损失。该方法识别出模型在推理过程中存在的“无效思维”问题，即模型在得出正确答案后仍然反复检查。为了解决这一低效问题，LC-R1提出了两个新原则：简洁性，强调消除冗余；充分性，确保关键推理步骤得以保留。通过对多个推理基准的广泛实验，LC-R1实现了序列长度的显著减少（约50%），而准确性仅下降约2%，在高压缩率和准确性之间达成了良好的平衡。'}}}, {'id': 'https://huggingface.co/papers/2506.14731', 'title': 'Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs', 'url': 'https://huggingface.co/papers/2506.14731', 'abstract': 'Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.', 'score': 1, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '7c1c5a66d6e8f898', 'authors': ['Ring Team', 'Bin Hu', 'Cai Chen', 'Deng Zhao', 'Ding Liu', 'Dingnan Jin', 'Feng Zhu', 'Hao Dai', 'Hongzhi Luan', 'Jia Guo', 'Jiaming Liu', 'Jiewei Wu', 'Jun Mei', 'Jun Zhou', 'Junbo Zhao', 'Junwu Xiong', 'Kaihong Zhang', 'Kuan Xu', 'Lei Liang', 'Liang Jiang', 'Liangcheng Fu', 'Longfei Zheng', 'Qiang Gao', 'Qing Cui', 'Quan Wan', 'Shaomian Zheng', 'Shuaicheng Li', 'Tongkai Yang', 'Wang Ren', 'Xiaodong Yan', 'Xiaopei Wan', 'Xiaoyun Feng', 'Xin Zhao', 'Xinxing Yang', 'Xinyu Kong', 'Xuemin Yang', 'Yang Li', 'Yingting Wu', 'Yongkang Liu', 'Zhankai Xu', 'Zhenduo Zhang', 'Zhenglei Zhou', 'Zhenyu Huang', 'Zhiqiang Zhang', 'Zihao Wang', 'Zujie Wen'], 'affiliations': ['Ring Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.14731.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#reasoning', '#open_source', '#rl', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с меньшими вычислительными затратами', 'desc': 'Ring-lite - это модель на основе архитектуры Mixture-of-Experts (MoE), оптимизированная с помощью обучения с подкреплением для эффективного рассуждения. Модель достигает производительности современных моделей рассуждения, активируя лишь треть параметров. Авторы представляют новый метод C3PO для повышения стабильности обучения и вычислительной эффективности. Они также предлагают двухэтапную парадигму обучения для интеграции данных из разных доменов.'}, 'en': {'title': 'Efficient Reasoning with Fewer Parameters: Introducing Ring-lite', 'desc': 'Ring-lite is a large language model that uses a Mixture-of-Experts (MoE) architecture combined with reinforcement learning (RL) to enhance reasoning capabilities while minimizing parameter activation. It builds on the Ling-lite model, achieving state-of-the-art performance on various reasoning benchmarks with only a fraction of the parameters activated compared to similar models. The paper introduces a novel training method called Constrained Contextual Computation Policy Optimization (C3PO) to improve stability during RL training and optimize computational efficiency. Additionally, it highlights the importance of selecting distillation checkpoints based on entropy loss for better performance in RL training and proposes a two-stage training approach to manage domain conflicts in mixed datasets.'}, 'zh': {'title': '高效推理，激活更少参数的Ring-lite', 'desc': 'Ring-lite是一种基于专家混合（MoE）架构的大型语言模型，通过强化学习（RL）进行优化，以实现高效且稳健的推理能力。该模型在Ling-lite的基础上构建，具有168亿个参数，但仅激活2.75亿个参数，能够在多个具有挑战性的基准测试中与小规模的最先进推理模型相匹配。我们提出了一种联合训练流程，将蒸馏与强化学习结合，解决了MoE强化学习训练中的一些未记录的挑战。通过引入受限上下文计算策略优化（C3PO），我们提高了训练的稳定性，并通过算法与系统的共同设计方法改善了计算吞吐量。'}}}, {'id': 'https://huggingface.co/papers/2506.14702', 'title': 'Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers', 'url': 'https://huggingface.co/papers/2506.14702', 'abstract': 'A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.  \t\t\t\t\tAI-generated summary \t\t\t\t One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.', 'score': 1, 'issue_id': 4350, 'pub_date': '2025-06-17', 'pub_date_card': {'ru': '17 июня', 'en': 'June 17', 'zh': '6月17日'}, 'hash': '2fbff0f4b562f92e', 'authors': ["Daniel D'souza", 'Julia Kreutzer', 'Adrien Morisot', 'Ahmet Üstün', 'Sara Hooker'], 'affiliations': ['Cohere', 'Cohere Labs'], 'pdf_title_img': 'assets/pdf/title_img/2506.14702.jpg', 'data': {'categories': ['#long_context', '#optimization', '#training'], 'emoji': '🎯', 'ru': {'title': 'Точная настройка моделей для редких случаев', 'desc': 'Статья представляет новый подход к дообучению моделей машинного обучения для улучшения их производительности и управляемости на редких и недопредставленных случаях использования. Авторы разработали таксономию характеристик данных и происхождения задач для явного контроля атрибутов генерации. Модель обучается автоматически определять эти маркеры, что делает их опциональными при выводе. Такой подход показывает значительные улучшения производительности, особенно на примерах из длинного хвоста распределения обучающих данных.'}, 'en': {'title': 'Optimizing Model Performance for Rare Use Cases', 'desc': "This paper addresses the challenge of improving machine learning model performance on rare and underrepresented use cases, often referred to as the long tail. It proposes a method for fine-tuning models that enhances both controllability and performance by automatically inferring generation attributes during inference. The authors introduce a taxonomy of data characteristics to help guide the model's output, allowing for better adaptation to specific tasks without relying heavily on prompt engineering. Their approach demonstrates significant performance improvements, particularly in underrepresented domains, achieving notable gains in generation quality and task-specific evaluations."}, 'zh': {'title': '优化模型以提升稀有用例的性能与可控性', 'desc': '本文提出了一种系统的方法，通过自动推断生成属性来微调模型，以提高在稀有和未充分代表的用例上的性能和可控性。现代机器学习面临的一个主要挑战是如何在长尾特征上表现良好，尤其是在训练数据中较少出现的特征。我们重新审视训练和推理技术之间的差距，以改善长尾性能，并为用户提供一组可控的生成属性。通过对基础模型进行微调，我们实现了在推理时自动推断这些标记，从而在未充分代表的领域中显著提高了模型的表现。'}}}, {'id': 'https://huggingface.co/papers/2506.13901', 'title': 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations', 'url': 'https://huggingface.co/papers/2506.13901', 'abstract': "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.  \t\t\t\t\tAI-generated summary \t\t\t\t Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.   To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.   Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.", 'score': 1, 'issue_id': 4351, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '5492fc2feb0ae2f3', 'authors': ['Abhilekh Borah', 'Chhavi Sharma', 'Danush Khanna', 'Utkarsh Bhatt', 'Gurpreet Singh', 'Hasnat Md Abdullah', 'Raghav Kaushik Ravi', 'Vinija Jain', 'Jyoti Patel', 'Shubham Singh', 'Vasu Sharma', 'Arpita Vats', 'Rahul Raja', 'Aman Chadha', 'Amitava Das'], 'affiliations': ['Amazon AI', 'BITS Goa, India', 'Evalueserve', 'IIIT Guwahati, India', 'IIT Kharagpur, India', 'LinkedIn', 'Manipal University Jaipur, India', 'Meta AI', 'New York University, USA', 'Texas A&M University, USA', 'Vellore Institute of Technology, Chennai, India'], 'pdf_title_img': 'assets/pdf/title_img/2506.13901.jpg', 'data': {'categories': ['#security', '#rlhf', '#alignment', '#benchmark', '#dataset', '#open_source'], 'emoji': '🎯', 'ru': {'title': 'AQI: Геометрический подход к оценке выравнивания языковых моделей', 'desc': 'Предложена новая метрика оценки выравнивания (alignment) больших языковых моделей - Индекс Качества Выравнивания (AQI). AQI анализирует активации в скрытом пространстве модели, оценивая качество кластеризации для выявления скрытых несоответствий и фальшивого выравнивания. Метрика дополняет существующие поведенческие прокси-метрики, такие как частота отказов и токсичность. AQI также может служить ранним индикатором попыток обхода ограничений модели, предоставляя надежный инструмент для аудита безопасности.'}, 'en': {'title': 'Ensuring True Alignment in Language Models with AQI', 'desc': "The paper introduces a new evaluation metric called the Alignment Quality Index (AQI) to assess the alignment of large language models (LLMs). AQI analyzes latent space activations to measure clustering quality, helping to identify misalignments and instances of alignment faking that traditional behavioral proxies may overlook. By utilizing established clustering metrics like the Davies-Bouldin Score and Dunn Index, AQI provides a more reliable assessment of model safety and alignment in high-stakes applications. The authors also present the LITMUS dataset to support rigorous evaluation, demonstrating AQI's effectiveness in revealing vulnerabilities that other metrics fail to detect."}, 'zh': {'title': '对齐质量指数：确保大型语言模型的安全与可靠', 'desc': '本文提出了一种新的评估指标，称为对齐质量指数（AQI），用于评估大型语言模型的对齐情况。AQI通过分析潜在空间中的激活分离，捕捉聚类质量，以检测模型的错误对齐和伪对齐现象。与现有的行为代理相比，AQI能够更有效地识别模型的安全性和潜在风险。我们还提出了LITMUS数据集，以支持在复杂条件下的稳健评估，并展示了AQI与外部评审者的相关性。'}}}, {'id': 'https://huggingface.co/papers/2506.13599', 'title': 'CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation', 'url': 'https://huggingface.co/papers/2506.13599', 'abstract': 'CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose CityGPT-Powered Agentic framework for Mobility Simulation (CAMS), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. CAMS comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that CAMS achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, CAMS generates more realistic and plausible trajectories. In general, CAMS establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.', 'score': 1, 'issue_id': 4348, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '0b0a6282d1310e1b', 'authors': ['Yuwei Du', 'Jie Feng', 'Jian Yuan', 'Yong Li'], 'affiliations': ['Department of Electronic Engineering, BRNist, Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.13599.jpg', 'data': {'categories': ['#agents', '#synthetic', '#reasoning', '#multimodal'], 'emoji': '🏙️', 'ru': {'title': 'Умное моделирование городской мобильности с помощью ИИ', 'desc': 'CAMS (CityGPT-Powered Agentic framework for Mobility Simulation) - это новый подход к моделированию человеческой мобильности в городском пространстве. Он использует языковые модели с глубоким пониманием городской среды для более реалистичного моделирования индивидуальных и коллективных паттернов передвижения. CAMS включает три ключевых модуля: MobExtractor для извлечения шаблонов мобильности, GeoGenerator для генерации опорных точек, и TrajEnhancer для создания траекторий. Эксперименты показывают, что CAMS превосходит традиционные методы и создает более правдоподобные траектории движения.'}, 'en': {'title': 'Revolutionizing Urban Mobility Simulation with CAMS', 'desc': 'CAMS introduces a novel framework that combines agent-based modeling with large language models to enhance the simulation of human mobility in urban environments. It addresses the limitations of traditional methods by integrating individual and collective mobility patterns, allowing for more realistic trajectory generation. The framework consists of three main components: MobExtractor for mobility pattern extraction, GeoGenerator for urban geospatial knowledge generation, and TrajEnhancer for trajectory refinement. Experiments demonstrate that CAMS outperforms existing approaches by generating plausible mobility trajectories without needing external geospatial data.'}, 'zh': {'title': '城市移动模拟的新范式', 'desc': 'CAMS是一个结合了代理框架和城市知识的大型语言模型，用于更真实地模拟人类的移动行为。它通过三个核心模块来实现这一目标：MobExtractor提取和合成用户的移动模式，GeoGenerator生成考虑集体知识的城市地理信息，TrajEnhancer根据移动模式生成符合真实偏好的轨迹。与传统方法相比，CAMS在不依赖外部地理信息的情况下，能够更好地建模个体和集体的移动模式。实验结果表明，CAMS生成的轨迹更加真实可信，开创了人类移动模拟的新范式。'}}}, {'id': 'https://huggingface.co/papers/2506.05426', 'title': 'Mixture-of-Experts Meets In-Context Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.05426', 'abstract': 'T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.  \t\t\t\t\tAI-generated summary \t\t\t\t In-context reinforcement learning (ICRL) has emerged as a promising paradigm for adapting RL agents to downstream tasks through prompt conditioning. However, two notable challenges remain in fully harnessing in-context learning within RL domains: the intrinsic multi-modality of the state-action-reward data and the diverse, heterogeneous nature of decision tasks. To tackle these challenges, we propose T2MIR (Token- and Task-wise MoE for In-context RL), an innovative framework that introduces architectural advances of mixture-of-experts (MoE) into transformer-based decision models. T2MIR substitutes the feedforward layer with two parallel layers: a token-wise MoE that captures distinct semantics of input tokens across multiple modalities, and a task-wise MoE that routes diverse tasks to specialized experts for managing a broad task distribution with alleviated gradient conflicts. To enhance task-wise routing, we introduce a contrastive learning method that maximizes the mutual information between the task and its router representation, enabling more precise capture of task-relevant information. The outputs of two MoE components are concatenated and fed into the next layer. Comprehensive experiments show that T2MIR significantly facilitates in-context learning capacity and outperforms various types of baselines. We bring the potential and promise of MoE to ICRL, offering a simple and scalable architectural enhancement to advance ICRL one step closer toward achievements in language and vision communities. Our code is available at https://github.com/NJU-RL/T2MIR.', 'score': 1, 'issue_id': 4356, 'pub_date': '2025-06-05', 'pub_date_card': {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'}, 'hash': '4e79eb4ebca225c7', 'authors': ['Wenhao Wu', 'Fuhong Liu', 'Haoru Li', 'Zican Hu', 'Daoyi Dong', 'Chunlin Chen', 'Zhi Wang'], 'affiliations': ['Australian Artificial Intelligence Institute, University of Technology Sydney', 'Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2506.05426.jpg', 'data': {'categories': ['#optimization', '#rl', '#games', '#multimodal', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'T2MIR: Смесь экспертов для улучшения обучения с подкреплением в контексте', 'desc': 'T2MIR - это новая архитектура для обучения с подкреплением в контексте (ICRL), использующая смесь экспертов (MoE) в трансформерных моделях принятия решений. Она решает проблемы мультимодальности входных данных и разнообразия задач с помощью токен-ориентированного и задаче-ориентированного MoE. T2MIR также применяет контрастивное обучение для улучшения маршрутизации задач. Эксперименты показывают значительное улучшение способности к обучению в контексте по сравнению с базовыми моделями.'}, 'en': {'title': 'Enhancing In-Context Learning with T2MIR: A Mixture-of-Experts Approach', 'desc': 'The paper introduces T2MIR, a novel framework that enhances in-context reinforcement learning (ICRL) by integrating mixture-of-experts (MoE) into transformer-based decision models. It addresses the challenges of multi-modality in state-action-reward data and the diversity of decision tasks by implementing token-wise and task-wise MoE layers. The token-wise MoE captures different meanings of input tokens, while the task-wise MoE directs tasks to specialized experts, reducing conflicts during training. Experimental results demonstrate that T2MIR improves the learning capacity of ICRL and outperforms existing methods, showcasing its potential for advancements in both language and vision tasks.'}, 'zh': {'title': 'T2MIR：提升上下文强化学习的专家混合框架', 'desc': 'T2MIR是一个框架，利用基于Transformer的决策模型中的逐token和逐任务的专家混合（MoE）方法，增强了上下文强化学习（ICRL）。该框架解决了状态-动作-奖励数据的多模态性和决策任务的多样性问题。T2MIR通过引入两个并行层，分别是逐token的MoE和逐任务的MoE，来捕捉输入token的不同语义，并将多样化的任务分配给专门的专家。实验结果表明，T2MIR显著提高了上下文学习能力，并在多种基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2506.13387', 'title': 'TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast', 'url': 'https://huggingface.co/papers/2506.13387', 'abstract': "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.  \t\t\t\t\tAI-generated summary \t\t\t\t This work presents a generalizable framework to transfer relative depth to metric depth. Current monocular depth estimation methods are mainly divided into metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs estimate depth in metric scale but are often limited to a specific domain. MRDEs generalize well across different domains, but with uncertain scales which hinders downstream applications. To this end, we aim to build up a framework to solve scale uncertainty and transfer relative depth to metric depth. Previous methods used language as input and estimated two factors for conducting rescaling. Our approach, TR2M, utilizes both text description and image as inputs and estimates two rescale maps to transfer relative depth to metric depth at pixel level. Features from two modalities are fused with a cross-modality attention module to better capture scale information. A strategy is designed to construct and filter confident pseudo metric depth for more comprehensive supervision. We also develop scale-oriented contrastive learning to utilize depth distribution as guidance to enforce the model learning about intrinsic knowledge aligning with the scale distribution. TR2M only exploits a small number of trainable parameters to train on datasets in various domains and experiments not only demonstrate TR2M's great performance in seen datasets but also reveal superior zero-shot capabilities on five unseen datasets. We show the huge potential in pixel-wise transferring relative depth to metric depth with language assistance. (Code is available at: https://github.com/BeileiCui/TR2M)", 'score': 0, 'issue_id': 4347, 'pub_date': '2025-06-16', 'pub_date_card': {'ru': '16 июня', 'en': 'June 16', 'zh': '6月16日'}, 'hash': '6d0fc497ae4dcfd0', 'authors': ['Beilei Cui', 'Yiming Huang', 'Long Bai', 'Hongliang Ren'], 'affiliations': ['The Chinese University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.13387.jpg', 'data': {'categories': ['#transfer_learning', '#cv', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Универсальное преобразование глубины с помощью мультимодального обучения', 'desc': 'TR2M - это фреймворк для преобразования относительной глубины в метрическую с использованием мультимодальных входных данных. Он применяет кросс-модальное внимание и контрастное обучение для улучшения производительности на различных наборах данных. TR2M использует как текстовое описание, так и изображение в качестве входных данных и оценивает две карты масштабирования для преобразования относительной глубины в метрическую на уровне пикселей. Модель демонстрирует отличные результаты как на знакомых, так и на новых наборах данных, показывая большой потенциал в попиксельном преобразовании глубины с помощью языковой информации.'}, 'en': {'title': 'Transforming Relative Depth to Metric Depth with TR2M', 'desc': 'The paper introduces TR2M, a framework that effectively converts relative depth information into metric depth using multimodal inputs, specifically images and text. It addresses the limitations of existing monocular depth estimation methods by combining the strengths of metric and relative depth estimation. TR2M employs cross-modality attention to enhance feature fusion and utilizes contrastive learning to improve scale alignment. The framework demonstrates strong performance across various datasets, including impressive zero-shot capabilities on unseen data, showcasing its versatility and effectiveness in depth estimation tasks.'}, 'zh': {'title': 'TR2M：相对深度到度量深度的智能转换', 'desc': '本文提出了一种名为TR2M的框架，旨在将相对深度转换为度量深度，利用多模态输入提升在不同数据集上的表现。当前的单目深度估计方法主要分为度量深度估计和相对深度估计，前者在特定领域表现良好，但局限性较大，而后者在不同领域具有良好的泛化能力，但存在尺度不确定性的问题。TR2M通过融合文本描述和图像输入，利用交叉模态注意力模块和对比学习策略，构建了两个重标定图以在像素级别上进行深度转换。实验结果表明，TR2M在已知数据集上表现优异，并在五个未知数据集上展现出卓越的零样本能力，显示出在像素级别上利用语言辅助进行深度转换的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.12015', 'title': 'EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction', 'url': 'https://huggingface.co/papers/2506.12015', 'abstract': 'EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.  \t\t\t\t\tAI-generated summary \t\t\t\t Open-source foundation models have seen rapid adoption and development, enabling powerful general-purpose capabilities across diverse domains. However, fine-tuning large foundation models for domain-specific or personalized tasks remains prohibitively expensive for most users due to the significant memory overhead beyond that of inference. We introduce EMLoC, an Emulator-based Memory-efficient fine-tuning framework with LoRA Correction, which enables model fine-tuning within the same memory budget required for inference. EMLoC constructs a task-specific light-weight emulator using activation-aware singular value decomposition (SVD) on a small downstream calibration set. Fine-tuning then is performed on this lightweight emulator via LoRA. To tackle the misalignment between the original model and the compressed emulator, we propose a novel compensation algorithm to correct the fine-tuned LoRA module, which thus can be merged into the original model for inference. EMLoC supports flexible compression ratios and standard training pipelines, making it adaptable to a wide range of applications. Extensive experiments demonstrate that EMLoC outperforms other baselines across multiple datasets and modalities. Moreover, without quantization, EMLoC enables fine-tuning of a 38B model on a single 24GB consumer GPU-bringing efficient and practical model adaptation to individual users.', 'score': 0, 'issue_id': 4356, 'pub_date': '2025-06-13', 'pub_date_card': {'ru': '13 июня', 'en': 'June 13', 'zh': '6月13日'}, 'hash': 'd559dcf057099acf', 'authors': ['Hsi-Che Lin', 'Yu-Chu Yu', 'Kai-Po Chang', 'Yu-Chiang Frank Wang'], 'affiliations': ['NVIDIA', 'National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.12015.jpg', 'data': {'categories': ['#optimization', '#training', '#dataset', '#open_source', '#inference'], 'emoji': '🧠', 'ru': {'title': 'Тонкая настройка гигантских моделей на обычном ПК', 'desc': 'EMLoC - это фреймворк для эффективной по памяти тонкой настройки больших языковых моделей. Он использует активационно-ориентированное SVD и LoRA для создания легковесного эмулятора модели. Это позволяет выполнять тонкую настройку в рамках тех же ограничений по памяти, что и при обычном выводе. EMLoC превосходит другие подходы на различных наборах данных и модальностях, делая адаптацию моделей доступной для индивидуальных пользователей.'}, 'en': {'title': 'Efficient Fine-Tuning for Everyone with EMLoC!', 'desc': 'EMLoC is a memory-efficient framework designed for fine-tuning large foundation models while adhering to inference memory limits. It utilizes activation-aware singular value decomposition (SVD) to create a lightweight emulator from a small calibration dataset, allowing for effective model adaptation. Fine-tuning is achieved through Low-Rank Adaptation (LoRA), which is then corrected with a novel compensation algorithm to ensure alignment with the original model. This approach enables users to fine-tune large models on standard consumer hardware, making advanced machine learning accessible and practical for diverse applications.'}, 'zh': {'title': 'EMLoC：高效内存微调的新选择', 'desc': 'EMLoC是一种内存高效的微调框架，利用激活感知的奇异值分解（SVD）和LoRA技术，使得在推理内存限制下进行模型适应成为可能。该框架通过在小型下游校准集上构建任务特定的轻量级模拟器，来实现微调。为了纠正原始模型与压缩模拟器之间的错位，EMLoC提出了一种新颖的补偿算法，使得微调后的LoRA模块可以合并回原始模型中进行推理。实验结果表明，EMLoC在多个数据集和模态上优于其他基线，且无需量化即可在单个24GB的消费级GPU上微调38B模型，极大地提高了模型适应的效率和实用性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (4)', '#agi (2)', '#alignment (1)', '#architecture (7)', '#audio (1)', '#benchmark (11)', '#cv (4)', '#data (2)', '#dataset (8)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (2)', '#interpretability (3)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (8)', '#open_source (7)', '#optimization (13)', '#plp', '#rag', '#reasoning (10)', '#rl (6)', '#rlhf (2)', '#robotics', '#science (1)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (2)', '#training (14)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-18 11:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-18 11:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-18 11:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    