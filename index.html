
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. December 2.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            flex: 1 0 auto;
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 0px;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
                margin: 0 -20px;
            }
            footer {
                margin-top: -20px;
            }
            article {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item" id="nav-prev"><a href="/d/2024-11-29.html">â¬…ï¸ <span id="prev-date">29.11</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-03.html">â¡ï¸ <span id="next-date">03.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'};
        let feedDateNext = {'ru': '03.12', 'en': '12/03', 'zh': '12æœˆ3æ—¥'};
        let feedDatePrev = {'ru': '29.11', 'en': '11/29', 'zh': '11æœˆ29æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2411.19930', 'title': 'On Domain-Specific Post-Training for Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2411.19930', 'abstract': 'Recent years have witnessed the rapid development of general multimodal large language models (MLLMs). However, adapting general MLLMs to specific domains, such as scientific fields and industrial applications, remains less explored. This paper systematically investigates domain adaptation of MLLMs through post-training, focusing on data synthesis, training pipelines, and task evaluation. (1) Data Synthesis: Using open-source models, we develop a visual instruction synthesizer that effectively generates diverse visual instruction tasks from domain-specific image-caption pairs. Our synthetic tasks surpass those generated by manual rules, GPT-4, and GPT-4V in enhancing the domain-specific performance of MLLMs. (2) Training Pipeline: While the two-stage training--initially on image-caption pairs followed by visual instruction tasks--is commonly adopted for developing general MLLMs, we apply a single-stage training pipeline to enhance task diversity for domain-specific post-training. (3) Task Evaluation: We conduct experiments in two domains, biomedicine and food, by post-training MLLMs of different sources and scales (e.g., Qwen2-VL-2B, LLaVA-v1.6-8B, Llama-3.2-11B), and then evaluating MLLM performance on various domain-specific tasks. To support further research in MLLM domain adaptation, we will open-source our implementations.', 'score': 11, 'issue_id': 885, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '5d14749b38f15e60', 'authors': ['Daixuan Cheng', 'Shaohan Huang', 'Ziyu Zhu', 'Xintong Zhang', 'Wayne Xin Zhao', 'Zhongzhi Luan', 'Bo Dai', 'Zhenliang Zhang'], 'affiliations': ['Beihang University', 'Beijing Institute of Technology', 'Renmin University of China', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19930.jpg', 'data': {'categories': ['#transfer_learning', '#dataset', '#training', '#open_source', '#multimodal', '#synthetic'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜ Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ°Ñ‚Ğ¾Ñ€ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ-Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ñ€Ğ¸ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ğ² Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¸ Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… MLLM, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing MLLMs for Specific Domains through Innovative Adaptation Techniques', 'desc': 'This paper explores how to adapt general multimodal large language models (MLLMs) for specific fields like biomedicine and food. It introduces a method for data synthesis that creates diverse visual instruction tasks from image-caption pairs, outperforming previous methods. The authors propose a single-stage training pipeline to improve task diversity during post-training, rather than the usual two-stage approach. Finally, they evaluate the performance of various MLLMs on domain-specific tasks and plan to share their findings to aid future research in this area.'}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸé€‚åº”æ€§', 'desc': 'è¿‘å¹´æ¥ï¼Œé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œå°†é€šç”¨MLLMsé€‚åº”äºç‰¹å®šé¢†åŸŸï¼Œå¦‚ç§‘å­¦å’Œå·¥ä¸šåº”ç”¨ï¼Œä»ç„¶è¾ƒå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†MLLMsçš„é¢†åŸŸé€‚åº”æ€§ï¼Œé‡ç‚¹åœ¨äºæ•°æ®åˆæˆã€è®­ç»ƒæµç¨‹å’Œä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§è§†è§‰æŒ‡ä»¤åˆæˆå™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆç”Ÿæˆå¤šæ ·åŒ–çš„è§†è§‰æŒ‡ä»¤ä»»åŠ¡ï¼Œä»è€Œæå‡MLLMsåœ¨ç‰¹å®šé¢†åŸŸçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19108', 'title': "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model", 'url': 'https://huggingface.co/papers/2411.19108', 'abstract': 'As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.', 'score': 5, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a6c2edf156e9d3', 'authors': ['Feng Liu', 'Shiwei Zhang', 'Xiaofeng Wang', 'Yujie Wei', 'Haonan Qiu', 'Yuzhong Zhao', 'Yingya Zhang', 'Qixiang Ye', 'Fang Wan'], 'affiliations': ['Alibaba Group', 'Fudan University', 'Institute of Automation, Chinese Academy of Sciences', 'Nanyang Technological University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2411.19108.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': 'â±ï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TeaCache. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ÑˆĞ°Ğ³Ğ¾Ğ². TeaCache Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ´Ğ»Ñ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ TeaCache Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4,41 Ñ€Ğ°Ğ·Ğ° Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Open-Sora-Plan Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Accelerating Video Generation with Smart Caching', 'desc': 'This paper presents TeaCache, a novel caching method designed to enhance the efficiency of diffusion models in video generation. Traditional approaches cache model outputs at fixed timesteps, which can lead to suboptimal performance due to the uneven differences in outputs across timesteps. TeaCache addresses this by focusing on modulating noisy inputs with timestep embeddings, allowing for a more accurate estimation of output differences without the heavy computational cost. The results demonstrate that TeaCache significantly accelerates inference speed while maintaining high visual quality, achieving a notable performance improvement over existing methods.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆé€Ÿåº¦çš„æ–°æ–¹æ³•ï¼šTeaCache', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¼“å­˜æ–¹æ³•ï¼Œç§°ä¸ºæ—¶é—´æ­¥åµŒå…¥æ„ŸçŸ¥ç¼“å­˜ï¼ˆTeaCacheï¼‰ï¼Œæ—¨åœ¨æé«˜è§†é¢‘ç”Ÿæˆä¸­çš„æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡åœ¨å‡åŒ€é€‰æ‹©çš„æ—¶é—´æ­¥ç¼“å­˜æ¨¡å‹è¾“å‡ºï¼Œä½†å¿½ç•¥äº†ä¸åŒæ—¶é—´æ­¥ä¹‹é—´è¾“å‡ºå·®å¼‚çš„ä¸å‡åŒ€æ€§ã€‚TeaCacheé€šè¿‡è°ƒèŠ‚å™ªå£°è¾“å…¥ï¼Œåˆ©ç”¨æ—¶é—´æ­¥åµŒå…¥æ¥æ›´å¥½åœ°è¿‘ä¼¼æ¨¡å‹è¾“å‡ºçš„å·®å¼‚ï¼Œä»è€Œä¼˜åŒ–ç¼“å­˜é€‰æ‹©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTeaCacheåœ¨ä¿æŒè§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œæ¨ç†é€Ÿåº¦æé«˜äº†4.41å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19460', 'title': 'Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing', 'url': 'https://huggingface.co/papers/2411.19460', 'abstract': 'With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma^2mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma^2mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks.', 'score': 4, 'issue_id': 886, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'b96751a3db484750', 'authors': ['Hosu Lee', 'Junho Kim', 'Hyunjun Kim', 'Yong Man Ro'], 'affiliations': ['Integrated Vision and Language Lab, KAIST, South Korea'], 'pdf_title_img': 'assets/pdf/title_img/2411.19460.jpg', 'data': {'categories': ['#architecture', '#long_context', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Video-Ma^2mba', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Video-Ma^2mba - Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LMM) Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¾ÑĞµĞ²Ğ¾Ğ³Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° (MA-GC) Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Video-Ma^2mba Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¼ GPU, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾.'}, 'en': {'title': 'Revolutionizing Long Video Processing with Linear Scalability', 'desc': 'The paper presents Video-Ma^2mba, a new architecture designed to efficiently process long video sequences by integrating State Space Models (SSMs) into the Mamba-2 framework. This innovative approach replaces traditional attention mechanisms, allowing the model to scale linearly in memory and computational requirements, which is crucial for handling extensive video data. Additionally, the introduction of Multi-Axis Gradient Checkpointing (MA-GC) optimizes memory usage by retaining only necessary activations, significantly reducing the memory footprint. Empirical results indicate that Video-Ma^2mba can effectively manage video sequences equivalent to millions of tokens, enhancing the accuracy of long video understanding tasks compared to existing models.'}, 'zh': {'title': 'é«˜æ•ˆå¤„ç†é•¿è§†é¢‘åºåˆ—çš„æ–°æ–¹æ³•', 'desc': 'éšç€è§†é¢‘æ•°æ®è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢åŠ ï¼Œå¤„ç†é•¿è§†é¢‘åºåˆ—é¢ä¸´ç€æ˜¾è‘—çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ¶æ„Video-Ma^2mbaï¼Œå®ƒåœ¨Mamba-2æ¡†æ¶ä¸­å¼•å…¥äº†çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œä½¿å¾—å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨æ—¶é—´å’Œå†…å­˜éœ€æ±‚ä¸Šå®ç°çº¿æ€§æ‰©å±•ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šè½´æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆMA-GCï¼‰æ–¹æ³•ï¼Œä¼˜åŒ–å†…å­˜ç®¡ç†ï¼Œä»…ä¿ç•™å¿…è¦çš„æ¿€æ´»ä¿¡æ¯ï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVideo-Ma^2mbaèƒ½å¤Ÿåœ¨å•ä¸ªGPUä¸Šå¤„ç†ç›¸å½“äºæ•°ç™¾ä¸‡ä¸ªæ ‡è®°æˆ–è¶…è¿‡ä¸¤å°æ—¶çš„è¿ç»­è§†é¢‘åºåˆ—ï¼Œæå‡äº†é•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19950', 'title': 'AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos', 'url': 'https://huggingface.co/papers/2411.19950', 'abstract': 'We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline for 3D planar reconstruction from monocular videos. Starting with 2D superpixels and geometric cues from pre-trained models, we initialize 3D planes as AlphaTablets and optimize them via differentiable rendering. An effective merging scheme is introduced to facilitate the growth and refinement of AlphaTablets. Through iterative optimization and merging, we reconstruct complete and accurate 3D planes with solid surfaces and clear boundaries. Extensive experiments on the ScanNet dataset demonstrate state-of-the-art performance in 3D planar reconstruction, underscoring the great potential of AlphaTablets as a generic 3D plane representation for various applications. Project page is available at: https://hyzcluster.github.io/alphatablets', 'score': 3, 'issue_id': 888, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '9f7d2daec9cb311d', 'authors': ['Yuze He', 'Wang Zhao', 'Shaohui Liu', 'Yubin Hu', 'Yushi Bai', 'Yu-Hui Wen', 'Yong-Jin Liu'], 'affiliations': ['Beijing Jiaotong University', 'ETH Zurich', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19950.jpg', 'data': {'categories': ['#3d'], 'emoji': 'ğŸ“', 'ru': {'title': 'AlphaTablets: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AlphaTablets - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ñ€ÑĞ¼Ğ¾ÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ¾Ğ² Ñ Ğ°Ğ»ÑŒÑ„Ğ°-ĞºĞ°Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… 2D Ğ¸ 3D Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ñ€Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ 3D Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ScanNet Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾ÑĞºĞ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'AlphaTablets: Revolutionizing 3D Plane Representation', 'desc': 'AlphaTablets is a new way to represent 3D planes that combines the benefits of both 2D and 3D models. It uses rectangles with alpha channels to create smooth surfaces and clear edges for accurate modeling. The paper introduces a method for rendering these planes efficiently and a pipeline for reconstructing 3D planes from single videos. By optimizing the representation through merging and iterative processes, AlphaTablets achieves high-quality 3D reconstructions, as shown in tests on the ScanNet dataset.'}, 'zh': {'title': 'AlphaTabletsï¼š3Då¹³é¢é‡å»ºçš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†AlphaTabletsï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–ä¸”é€šç”¨çš„3Då¹³é¢è¡¨ç¤ºæ–¹æ³•ï¼Œå…·æœ‰è¿ç»­çš„3Dè¡¨é¢å’Œç²¾ç¡®çš„è¾¹ç•Œåˆ’åˆ†ã€‚é€šè¿‡å°†3Då¹³é¢è¡¨ç¤ºä¸ºå¸¦æœ‰alphaé€šé“çš„çŸ©å½¢ï¼ŒAlphaTabletsç»“åˆäº†å½“å‰2Då’Œ3Då¹³é¢è¡¨ç¤ºçš„ä¼˜ç‚¹ï¼Œå®ç°äº†3Då¹³é¢çš„å‡†ç¡®ã€ä¸€è‡´å’Œçµæ´»å»ºæ¨¡ã€‚æˆ‘ä»¬åœ¨AlphaTabletsçš„åŸºç¡€ä¸Šæ¨å¯¼å‡ºå¯å¾®åˆ†å…‰æ …åŒ–æŠ€æœ¯ï¼Œä»¥é«˜æ•ˆåœ°å°†3Då¹³é¢æ¸²æŸ“ä¸ºå›¾åƒï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªä¸‹è€Œä¸Šçš„å•ç›®è§†é¢‘3Då¹³é¢é‡å»ºç®¡é“ã€‚é€šè¿‡è¿­ä»£ä¼˜åŒ–å’Œåˆå¹¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé‡å»ºå‡ºå®Œæ•´ä¸”å‡†ç¡®çš„3Då¹³é¢ï¼Œå…·æœ‰åšå®çš„è¡¨é¢å’Œæ¸…æ™°çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19146', 'title': 'Puzzle: Distillation-Based NAS for Inference-Optimized LLMs', 'url': 'https://huggingface.co/papers/2411.19146', 'abstract': "Large language models (LLMs) have demonstrated remarkable capabilities, but their adoption is limited by high computational costs during inference. While increasing parameter counts enhances accuracy, it also widens the gap between state-of-the-art capabilities and practical deployability. We present Puzzle, a framework to accelerate LLM inference on specific hardware while preserving their capabilities. Through an innovative application of neural architecture search (NAS) at an unprecedented scale, Puzzle systematically optimizes models with tens of billions of parameters under hardware constraints. Our approach utilizes blockwise local knowledge distillation (BLD) for parallel architecture exploration and employs mixed-integer programming for precise constraint optimization.   We demonstrate the real-world impact of our framework through Llama-3.1-Nemotron-51B-Instruct (Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct. Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while preserving 98.4% of the original model's capabilities. Nemotron-51B currently stands as the most accurate language model capable of inference on a single GPU with large batch sizes. Remarkably, this transformation required just 45B training tokens, compared to over 15T tokens used for the 70B model it was derived from. This establishes a new paradigm where powerful models can be optimized for efficient deployment with only negligible compromise of their capabilities, demonstrating that inference performance, not parameter count alone, should guide model selection. With the release of Nemotron-51B and the presentation of the Puzzle framework, we provide practitioners immediate access to state-of-the-art language modeling capabilities at significantly reduced computational costs.", 'score': 3, 'issue_id': 886, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': 'b33bb17742a81e99', 'authors': ['Akhiad Bercovich', 'Tomer Ronen', 'Talor Abramovich', 'Nir Ailon', 'Nave Assaf', 'Mohammad Dabbah', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Netanel Haber', 'Ehud Karpas', 'Itay Levy', 'Shahar Mor', 'Zach Moshe', 'Najeeb Nabwani', 'Omri Puny', 'Ran Rubin', 'Itamar Schen', 'Ido Shahaf', 'Oren Tropp', 'Omer Ullman Argov', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2411.19146.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#inference'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Puzzle Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº (NAS) Ğ¸ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ (BLD), Puzzle Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ´ĞµÑÑÑ‚ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Nemotron-51B, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ· Llama-3.1-70B-Instruct, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ 2.17-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 98.4% Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°, Ğ° Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Optimizing Large Language Models for Efficient Inference', 'desc': "This paper introduces Puzzle, a framework designed to enhance the inference speed of large language models (LLMs) while maintaining their performance. It employs neural architecture search (NAS) to optimize models with billions of parameters specifically for certain hardware, addressing the challenge of high computational costs. The framework utilizes blockwise local knowledge distillation (BLD) and mixed-integer programming to efficiently explore and optimize model architectures. The resulting model, Nemotron-51B, achieves a significant speedup in inference on a single GPU while retaining most of the original model's capabilities, showcasing a new approach to deploying powerful LLMs more efficiently."}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œå¼ºå¤§æ¨¡å‹çš„æ–°èŒƒå¼', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†é«˜è®¡ç®—æˆæœ¬é™åˆ¶äº†å®ƒä»¬çš„åº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†Puzzleæ¡†æ¶ï¼Œé€šè¿‡ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰åœ¨ç‰¹å®šç¡¬ä»¶ä¸ŠåŠ é€ŸLLMæ¨ç†ï¼ŒåŒæ—¶ä¿æŒå…¶èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å—çŠ¶å±€éƒ¨çŸ¥è¯†è’¸é¦ï¼ˆBLDï¼‰è¿›è¡Œå¹¶è¡Œæ¶æ„æ¢ç´¢ï¼Œå¹¶é‡‡ç”¨æ··åˆæ•´æ•°è§„åˆ’è¿›è¡Œç²¾ç¡®çº¦æŸä¼˜åŒ–ã€‚é€šè¿‡Nemotron-51Bæ¨¡å‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨å•ä¸ªNVIDIA H100 GPUä¸Šå®ç°2.17å€æ¨ç†ååé‡æå‡çš„å®é™…æ•ˆæœï¼ŒåŒæ—¶ä¿ç•™äº†98.4%çš„åŸå§‹æ¨¡å‹èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19324', 'title': 'Trajectory Attention for Fine-grained Video Motion Control', 'url': 'https://huggingface.co/papers/2411.19324', 'abstract': 'Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both precise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.', 'score': 3, 'issue_id': 885, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '02a266f597ae69e7', 'authors': ['Zeqi Xiao', 'Wenqi Ouyang', 'Yifan Zhou', 'Shuai Yang', 'Lei Yang', 'Jianlou Si', 'Xingang Pan'], 'affiliations': ['S-Lab, Nanyang Technological University', 'Sensetime Research', 'Wangxuan Institute of Computer Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2411.19324.jpg', 'data': {'categories': ['#diffusion', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ trajectory attention', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'trajectory attention' Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ´Ğ¾Ğ»ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. Trajectory attention Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞºĞ°Ğº Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²ĞµÑ‚Ğ²ÑŒ Ğ½Ğ°Ñ€ÑĞ´Ñƒ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ ĞºĞ°Ğº Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸ÑÑ… Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸."}, 'en': {'title': 'Enhancing Video Generation with Trajectory Attention', 'desc': 'This paper presents a new method called trajectory attention for improving video generation, particularly in controlling camera motion. It enhances the traditional temporal attention mechanism by incorporating pixel trajectories, allowing for more precise and consistent video outputs. The method effectively combines trajectory information with temporal attention, addressing challenges in generating view-customized content. Experiments show that this approach not only improves motion control but also maintains high-quality video generation across various tasks.'}, 'zh': {'title': 'è½¨è¿¹æ³¨æ„åŠ›ï¼šç²¾ç¡®æ§åˆ¶è§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„è½¨è¿¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œç”¨äºè§†é¢‘ç”Ÿæˆä¸­çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæ›´ç²¾ç¡®åœ°å¤„ç†è¿åŠ¨æ§åˆ¶ï¼Œå¹¶æœ‰æ•ˆåœ°ç»“åˆäº†è½¨è¿¹ä¿¡æ¯ã€‚é€šè¿‡å°†è½¨è¿¹æ³¨æ„åŠ›ä½œä¸ºè¾…åŠ©åˆ†æ”¯ä¸ä¼ ç»Ÿæ—¶é—´æ³¨æ„åŠ›ç»“åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆæ–°å†…å®¹çš„åŒæ—¶ï¼Œç¡®ä¿äº†è¿åŠ¨æ§åˆ¶çš„ç²¾ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘çš„ç›¸æœºè¿åŠ¨æ§åˆ¶ä¸­æ˜¾è‘—æé«˜äº†ç²¾åº¦å’Œé•¿è·ç¦»ä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.18673', 'title': 'AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers', 'url': 'https://huggingface.co/papers/2411.18673', 'abstract': 'Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to 4x reduction of training parameters, improved training speed and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse dynamic videos with stationary cameras. This helps the model disambiguate the difference between camera and scene motion, and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.', 'score': 2, 'issue_id': 886, 'pub_date': '2024-11-27', 'pub_date_card': {'ru': '27 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 27', 'zh': '11æœˆ27æ—¥'}, 'hash': '1ea35d3552a278a3', 'authors': ['Sherwin Bahmani', 'Ivan Skorokhodov', 'Guocheng Qian', 'Aliaksandr Siarohin', 'Willi Menapace', 'Andrea Tagliasacchi', 'David B. Lindell', 'Sergey Tulyakov'], 'affiliations': ['SFU', 'Snap Inc.', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2411.18673.jpg', 'data': {'categories': ['#dataset', '#architecture', '#diffusion', '#games', '#3d', '#training', '#optimization', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞŸÑ€ĞµÑ†Ğ¸Ğ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ 3D-ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ñ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Advanced 3D Camera Control (AC3D), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ AC3D Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğ¼ĞµÑ€Ğ¾Ğ¹.'}, 'en': {'title': 'Precision in 3D Camera Control for Enhanced Video Generation', 'desc': 'This paper presents a novel approach to improve 3D camera control in text-to-video models, addressing issues of imprecision and video quality. By analyzing camera motion, the authors discover that it is primarily low-frequency, which leads to adjustments in training and testing schedules that enhance convergence and visual fidelity. They also find that only certain layers of a video diffusion transformer contain relevant camera information, allowing for a more efficient architecture that reduces training parameters while boosting quality. Finally, the introduction of a curated dataset of dynamic videos aids the model in distinguishing between camera and scene motion, culminating in the development of the Advanced 3D Camera Control (AC3D) model, which sets a new standard in generative video modeling.'}, 'zh': {'title': 'å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶ï¼Œæå‡è§†é¢‘ç”Ÿæˆè´¨é‡', 'desc': 'æœ¬ç ”ç©¶åˆ†æäº†3Dç›¸æœºæ§åˆ¶åœ¨æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå‘ç°ç›¸æœºè¿åŠ¨å¯¹è§†é¢‘ç”Ÿæˆè´¨é‡æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒå’Œæµ‹è¯•å§¿æ€è°ƒèŠ‚ç­–ç•¥ï¼Œä»¥æé«˜è®­ç»ƒæ”¶æ•›é€Ÿåº¦å’Œè§†é¢‘çš„è§†è§‰è´¨é‡ã€‚é€šè¿‡å¯¹æ— æ¡ä»¶è§†é¢‘æ‰©æ•£å˜æ¢å™¨çš„è¡¨ç¤ºè¿›è¡Œæ¢æµ‹ï¼Œæˆ‘ä»¬å‘ç°ç›¸æœºå§¿æ€ä¼°è®¡åœ¨æ¨¡å‹å†…éƒ¨éšå¼æ‰§è¡Œï¼Œå› æ­¤æˆ‘ä»¬é™åˆ¶äº†ç›¸æœºæ¡ä»¶çš„æ³¨å…¥ï¼Œä»¥å‡å°‘å¯¹å…¶ä»–è§†é¢‘ç‰¹å¾çš„å¹²æ‰°ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†å…ˆè¿›çš„3Dç›¸æœºæ§åˆ¶æ¶æ„ï¼ˆAC3Dï¼‰ï¼Œæˆä¸ºå…·æœ‰ç›¸æœºæ§åˆ¶çš„ç”Ÿæˆè§†é¢‘å»ºæ¨¡çš„æ–°ä¸€ä»£æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19189', 'title': 'Video Depth without Video Models', 'url': 'https://huggingface.co/papers/2411.19189', 'abstract': 'Video depth estimation lifts monocular video clips to 3D by inferring dense depth at every frame. Recent advances in single-image depth estimation, brought about by the rise of large foundation models and the use of synthetic training data, have fueled a renewed interest in video depth. However, naively applying a single-image depth estimator to every frame of a video disregards temporal continuity, which not only leads to flickering but may also break when camera motion causes sudden changes in depth range. An obvious and principled solution would be to build on top of video foundation models, but these come with their own limitations; including expensive training and inference, imperfect 3D consistency, and stitching routines for the fixed-length (short) outputs. We take a step back and demonstrate how to turn a single-image latent diffusion model (LDM) into a state-of-the-art video depth estimator. Our model, which we call RollingDepth, has two main ingredients: (i) a multi-frame depth estimator that is derived from a single-image LDM and maps very short video snippets (typically frame triplets) to depth snippets. (ii) a robust, optimization-based registration algorithm that optimally assembles depth snippets sampled at various different frame rates back into a consistent video. RollingDepth is able to efficiently handle long videos with hundreds of frames and delivers more accurate depth videos than both dedicated video depth estimators and high-performing single-frame models. Project page: rollingdepth.github.io.', 'score': 0, 'issue_id': 889, 'pub_date': '2024-11-28', 'pub_date_card': {'ru': '28 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 28', 'zh': '11æœˆ28æ—¥'}, 'hash': '1fc611a9a44595a1', 'authors': ['Bingxin Ke', 'Dominik Narnhofer', 'Shengyu Huang', 'Lei Ke', 'Torben Peters', 'Katerina Fragkiadaki', 'Anton Obukhov', 'Konrad Schindler'], 'affiliations': ['Carnegie Mellon University', 'ETH Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2411.19189.jpg', 'data': {'categories': ['#3d', '#diffusion', '#video', '#optimization'], 'emoji': 'ğŸ¥', 'ru': {'title': 'RollingDepth: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LDM', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ RollingDepth. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LDM) Ğ´Ğ»Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. RollingDepth ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½Ñ‰Ğ¸ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Transforming Monocular Videos into Accurate 3D Depth with RollingDepth', 'desc': 'This paper presents RollingDepth, a novel approach for estimating depth in videos by leveraging a single-image latent diffusion model (LDM). The method addresses the challenge of temporal continuity in video depth estimation, which often leads to flickering and inconsistencies when using traditional single-image models. RollingDepth utilizes a multi-frame depth estimator to analyze short video snippets and a robust registration algorithm to combine these depth estimates into a coherent video output. The results show that RollingDepth outperforms existing video depth estimators and single-frame models, providing accurate depth information for long video sequences.'}, 'zh': {'title': 'å°†å•å›¾åƒæ·±åº¦ä¼°è®¡æå‡ä¸ºè§†é¢‘æ·±åº¦ä¼°è®¡çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'è§†é¢‘æ·±åº¦ä¼°è®¡é€šè¿‡æ¨æ–­æ¯å¸§çš„å¯†é›†æ·±åº¦ï¼Œå°†å•ç›®è§†é¢‘ç‰‡æ®µæå‡ä¸º3Dã€‚æœ€è¿‘ï¼Œå•å›¾åƒæ·±åº¦ä¼°è®¡çš„è¿›å±•æ¿€å‘äº†å¯¹è§†é¢‘æ·±åº¦çš„å…³æ³¨ï¼Œä½†ç®€å•åœ°å°†å•å›¾åƒæ·±åº¦ä¼°è®¡å™¨åº”ç”¨äºæ¯å¸§ä¼šå¿½ç•¥æ—¶é—´è¿ç»­æ€§ï¼Œå¯¼è‡´é—ªçƒå’Œæ·±åº¦èŒƒå›´çš„çªç„¶å˜åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºRollingDepthçš„æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤šå¸§æ·±åº¦ä¼°è®¡å’Œä¼˜åŒ–çš„æ³¨å†Œç®—æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿è§†é¢‘å¹¶æä¾›æ›´å‡†ç¡®çš„æ·±åº¦è§†é¢‘ã€‚è¯¥æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¸“ç”¨è§†é¢‘æ·±åº¦ä¼°è®¡å™¨å’Œé«˜æ€§èƒ½å•å¸§æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19638', 'title': 'LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification', 'url': 'https://huggingface.co/papers/2411.19638', 'abstract': "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema.", 'score': 0, 'issue_id': 889, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': '98bf5f113194343b', 'authors': ['Taja Kuzman', 'Nikola LjubeÅ¡iÄ‡'], 'affiliations': ['Department of Knowledge Technologies, JoÅ¾ef Stefan Institute, 1000 Ljubljana, Slovenia', 'JoÅ¾ef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia', 'University of Ljubljana, 1000 Ljubljana, Slovenia'], 'pdf_title_img': 'assets/pdf/title_img/2411.19638.jpg', 'data': {'categories': ['#translation', '#training', '#low_resource', '#multilingual', '#dataset'], 'emoji': 'ğŸ“°', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ 'ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒ-ÑƒÑ‡ĞµĞ½Ğ¸Ğº'. Ğ‘Ğ¾Ğ»ÑŒÑˆĞ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ GPT Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ½Ğ° Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸-ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ BERT Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸ ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹."}, 'en': {'title': 'Empowering Multilingual News Classification with Teacher-Student LLMs', 'desc': 'This paper presents a teacher-student framework utilizing large language models (LLMs) for multilingual news classification without manual data annotation. The teacher model, a Generative Pretrained Transformer (GPT), automatically annotates news articles in multiple languages, achieving high zero-shot performance and agreement with human annotators. Smaller BERT-like student models are then fine-tuned on this annotated dataset, demonstrating comparable performance to the teacher model while being computationally efficient. The study also highlights the effectiveness of the student models in multilingual and zero-shot cross-lingual tasks, ultimately providing a robust news topic classifier for diverse languages.'}, 'zh': {'title': 'å¤šè¯­è¨€æ–°é—»åˆ†ç±»çš„æ–°æ–¹æ³•', 'desc': 'éšç€åœ¨çº¿æ–°é—»æ•°é‡çš„ä¸æ–­å¢åŠ ï¼ŒæŒ‰ä¸»é¢˜å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶ï¼Œç”¨äºå¼€å‘å¤šè¯­è¨€æ–°é—»åˆ†ç±»æ¨¡å‹ï¼Œä¸”æ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡æ³¨ã€‚æ•™å¸ˆæ¨¡å‹ä½¿ç”¨ç”Ÿæˆé¢„è®­ç»ƒå˜æ¢å™¨ï¼ˆGPTï¼‰è‡ªåŠ¨æ ‡æ³¨æ–°é—»æ–‡ç« ï¼Œå±•ç¤ºå‡ºåœ¨å¤šç§è¯­è¨€ä¸Šçš„é«˜é›¶æ ·æœ¬æ€§èƒ½ã€‚é€šè¿‡å¾®è°ƒè¾ƒå°çš„BERTç±»å­¦ç”Ÿæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨ç›¸å¯¹è¾ƒå°‘çš„è®­ç»ƒå®ä¾‹ä¸‹ä¹Ÿèƒ½è¾¾åˆ°ä¸æ•™å¸ˆæ¨¡å‹ç›¸å½“çš„é«˜æ€§èƒ½ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents', '#agi', '#alignment', '#architecture (3)', '#audio', '#benchmark', '#cv', '#data', '#dataset (3)', '#diffusion (4)', '#ethics', '#games (1)', '#graphs', '#hallucinations', '#healthcare', '#inference (2)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (1)', '#open_source (1)', '#optimization (5)', '#plp', '#rag', '#reasoning', '#rl', '#rlhf', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey', '#synthetic (1)', '#training (4)', '#transfer_learning (1)', '#video (5)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-02 08:15',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-02 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-02 08:15')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    