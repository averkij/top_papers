
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF (15 —Å—Ç–∞—Ç–µ–π)</title>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #0989eacf;
            --secondary-color: #03dac6;
            --background-color: #f5f5f5;
            --text-color: #333333;
            --header-color: #0989eacf;
            --body-color: #f5f5f5;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            padding: 1.6em 0;
            text-align: center;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 2em;
            padding: 10px 0 20px 0;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.tags {
            color: #555;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        body.dark-theme>div>main>article {
            background-color: #444;
        }
        body.light-theme>div>main>article {
            background-color: #fff;
        }
        body.dark-theme>div>main>article:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article:hover {
            background-color: #fafafa;
        }
        article {
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
            transition: background-color 0.2s ease;
            display: flex;
            flex-direction: column;
            position: relative;
        }
        .article-content {
            padding: 1.5em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
        }
        .pub-date {
            font-size: 0.9em;
            margin-bottom: 0.8em;
            font-weight: 300;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 1em;
            position: absolute;
            bottom: 10px;
            font-weight: 300;
            font-family: 'Roboto Slab';
        }
        .background-digit {
            position: absolute;
            bottom: -20px;
            right: -10px;
            font-size: 12em;
            font-weight: bold;
            color: rgba(0, 0, 0, 0.03);
            z-index: 0;
            line-height: 1;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 80px;
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        a:hover {
            color: var(--secondary-color);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1em 0;
            margin-top: 2em;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
        }
        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
        }
        .update-info-container {
            flex: 1;
        }
        .sort-container {
            flex: 2;
        }
        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .category-toggle {
            display: none;
            margin-bottom: 10px;
            margin-top: 15px;
            cursor: pointer;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }

        .svg-container span {
            position: relative;
            z-index: 1;
        }

        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .sort-container {
                margin-top: 0px;
                text-align: left;
                width: 100%;
            .sort-dropdown {
                float: right;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiffRu(dateString) {
        const timeUnits = {
            minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
            hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
            day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"]
        };

        function getRussianPlural(number, words) {
            if (number % 10 === 1 && number % 100 !== 11) {
                return words[0];
            } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                return words[1];
            } else {
                return words[2];
            }
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);

        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes == 0) {
            return '—Ç–æ–ª—å–∫–æ —á—Ç–æ';
        }
        else if (minutes < 60) {
            return `${minutes} ${getRussianPlural(minutes, timeUnits.minute)} –Ω–∞–∑–∞–¥`;
        } else if (hours < 24) {
            return `${hours} ${getRussianPlural(hours, timeUnits.hour)} –Ω–∞–∑–∞–¥`;
        } else {
            return `${days} ${getRussianPlural(days, timeUnits.day)} –Ω–∞–∑–∞–¥`;
        }
    }
    function formatArticlesTitle(number) {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;

        let word;

        if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
            word = "—Å—Ç–∞—Ç–µ–π";
        } else if (lastDigit === 1) {
            word = "—Å—Ç–∞—Ç—å—è";
        } else if (lastDigit >= 2 && lastDigit <= 4) {
            word = "—Å—Ç–∞—Ç—å–∏";
        } else {
            word = "—Å—Ç–∞—Ç–µ–π";
        }

        return `${number} ${word}`;
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">
            <h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">—Ö—Ñ –¥—ç–π–ª–∏</h1>
            <p>15 –æ–∫—Ç—è–±—Ä—è | 15 —Å—Ç–∞—Ç–µ–π</p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="category-toggle">
            <div class="svg-container">
                <span id="category-toggle">üîç –§–∏–ª—å—Ç—Ä</span>
                <svg height="3" width="200">
                    <line x1="0" y1="0" x2="200" y2="0" 
                        stroke="black" 
                        stroke-width="2" 
                        stroke-dasharray="3, 3" />
                </svg>
            </div>
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">–≥—Ä–∞–¥–∏–µ–Ω—Ç –æ–±—Ä–µ—á–µ–Ω–Ω—ã–π</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>    
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "—Ö—Ñ –Ω–∞–π—Ç–ª–∏";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "—Ö—Ñ –¥—ç–π–ª–∏";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2410.10139', 'title': 'MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models', 'url': 'https://huggingface.co/papers/2410.10139', 'abstract': 'Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/.', 'score': 38, 'issue_id': 107, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'MMIE - —ç—Ç–æ –Ω–æ–≤—ã–π –º–∞—Å—à—Ç–∞–±–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å —á–µ—Ä–µ–¥—É—é—â–∏–º—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π (LVLMs). –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç 20 000 —Ç—â–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ 12 –æ–±–ª–∞—Å—Ç–µ–π –∑–Ω–∞–Ω–∏–π. MMIE –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —á–µ—Ä–µ–¥—É—é—â–∏–µ—Å—è –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤–æ–ø—Ä–æ—Å–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –æ—Ü–µ–Ω–∫–∏, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ –º–æ–¥–µ–ª–∏, –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —á–µ–ª–æ–≤–µ–∫–æ–º –¥–∞–Ω–Ω—ã—Ö.', 'tags': ['#LVLMs', '#–∏–Ω—Ç–µ—Ä–ª–∏–≤–∏–Ω–≥–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å', '#–æ—Ü–µ–Ω–∫–∞–ú–æ–¥–µ–ª–µ–π'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'üîÑ', 'title': 'MMIE: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä–µ–¥—É—é—â–∏—Ö—Å—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π'}}, {'id': 'https://huggingface.co/papers/2410.09732', 'title': 'LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models', 'url': 'https://huggingface.co/papers/2410.09732', 'abstract': 'With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found at https://opendatalab.github.io/LOKI/', 'score': 31, 'issue_id': 107, 'pub_date': '2024-10-13', 'pub_date_ru': '13 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'LOKI - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (LMM) –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞—Ç—å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è 18 —Ç—ã—Å—è—á –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –≤–∏–¥–µ–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, 3D, —Ç–µ–∫—Å—Ç—É –∏ –∞—É–¥–∏–æ, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 26 –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å —á–µ—Ç–∫–∏–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. LOKI –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ LMM —á–µ—Ä–µ–∑ –∑–∞–¥–∞—á–∏ –≥—Ä—É–±–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≤—ã–±–æ—Ä–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤—ã—è–≤–ª–µ–Ω–∏—è –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π. –ê–≤—Ç–æ—Ä—ã –æ—Ü–µ–Ω–∏–ª–∏ 28 –º–æ–¥–µ–ª–µ–π LMM –Ω–∞ —ç—Ç–æ–º –±–µ–Ω—á–º–∞—Ä–∫–µ, –≤—ã—è–≤–∏–≤ –∏—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.', 'tags': ['#syntheticDataDetection', '#multimodalBenchmark', '#LMMevaluation'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'üïµÔ∏è', 'title': 'LOKI: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–∏–≤ –¥–ª—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞'}}, {'id': 'https://huggingface.co/papers/2410.09584', 'title': 'Toward General Instruction-Following Alignment for Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2410.09584', 'abstract': 'Following natural instructions is crucial for the effective application of Retrieval-Augmented Generation (RAG) systems. Despite recent advancements in Large Language Models (LLMs), research on assessing and improving instruction-following (IF) alignment within the RAG domain remains limited. To address this issue, we propose VIF-RAG, the first automated, scalable, and verifiable synthetic pipeline for instruction-following alignment in RAG systems. We start by manually crafting a minimal set of atomic instructions (<100) and developing combination rules to synthesize and verify complex instructions for a seed set. We then use supervised models for instruction rewriting while simultaneously generating code to automate the verification of instruction quality via a Python executor. Finally, we integrate these instructions with extensive RAG and general data samples, scaling up to a high-quality VIF-RAG-QA dataset (>100k) through automated processes. To further bridge the gap in instruction-following auto-evaluation for RAG systems, we introduce FollowRAG Benchmark, which includes approximately 3K test samples, covering 22 categories of general instruction constraints and four knowledge-intensive QA datasets. Due to its robust pipeline design, FollowRAG can seamlessly integrate with different RAG benchmarks. Using FollowRAG and eight widely-used IF and foundational abilities benchmarks for LLMs, we demonstrate that VIF-RAG markedly enhances LLM performance across a broad range of general instruction constraints while effectively leveraging its capabilities in RAG scenarios. Further analysis offers practical insights for achieving IF alignment in RAG systems. Our code and datasets are released at https://FollowRAG.github.io.', 'score': 28, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'VIF-RAG - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Å–∏—Å—Ç–µ–º–∞—Ö RAG. –ê–≤—Ç–æ—Ä—ã —Å–æ–∑–¥–∞–ª–∏ –Ω–∞–±–æ—Ä –∞—Ç–æ–º–∞—Ä–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø—Ä–∞–≤–∏–ª–∞ –∏—Ö –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∞ —Ç–∞–∫–∂–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–≥–æ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç VIF-RAG-QA –∏ —Å–æ–∑–¥–∞–Ω –±–µ–Ω—á–º–∞—Ä–∫ FollowRAG –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAG-—Å–∏—Å—Ç–µ–º. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ VIF-RAG –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö.', 'tags': ['#instruction-following', '#synthetic-data-generation', '#rag-evaluation'], 'categories': ['#nlp', '#rag', '#benchmark', '#dataset'], 'emoji': 'üß†', 'title': 'VIF-RAG: –ù–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é RAG-—Å–∏—Å—Ç–µ–º —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º'}}, {'id': 'https://huggingface.co/papers/2410.10563', 'title': 'MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks', 'url': 'https://huggingface.co/papers/2410.10563', 'abstract': 'We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MMBench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. Unlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.', 'score': 19, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'MEGA-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—é—â–∏–π –±–æ–ª–µ–µ 500 —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á. –û–Ω –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤—ã–≤–æ–¥–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª–µ–µ 40 –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏. MEGA-Bench –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –º–æ–¥–µ–ª–µ–π –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∏–∑–º–µ—Ä–µ–Ω–∏—è–º. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ –æ—Ü–µ–Ω–∫—É —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —ç—Ç–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.', 'tags': ['#–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è–û—Ü–µ–Ω–∫–∞', '#—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ–ó–∞–¥–∞—á–∏', '#–º–Ω–æ–≥–æ—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π–í—ã–≤–æ–¥'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'üß†', 'title': 'MEGA-Bench: –ú–∞—Å—à—Ç–∞–±–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö'}}, {'id': 'https://huggingface.co/papers/2410.10783', 'title': 'LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content', 'url': 'https://huggingface.co/papers/2410.10783', 'abstract': 'The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.', 'score': 15, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'LiveXiv - —ç—Ç–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π—Å—è –±–µ–Ω—á–º–∞—Ä–∫, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö ArXiv. –û–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –ø–æ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É (VQA) –∏–∑ —Ä—É–∫–æ–ø–∏—Å–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è –≥—Ä–∞—Ñ–∏–∫–∏, –¥–∏–∞–≥—Ä–∞–º–º—ã –∏ —Ç–∞–±–ª–∏—Ü—ã. –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ—Ü–µ–Ω–∫–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π, —Å–Ω–∏–∂–∞—è –æ–±—â–∏–µ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –æ—Ü–µ–Ω–∫—É. LiveXiv –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏–∑–±–µ–≥–∞—è –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.', 'tags': ['#LiveXiv', '#VQA', '#ArXiv'], 'categories': ['#multimodal', '#benchmark', '#dataset'], 'emoji': 'üìä', 'title': 'LiveXiv: –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π'}}, {'id': 'https://huggingface.co/papers/2410.10306', 'title': 'Animate-X: Universal Character Image Animation with Enhanced Motion Representation', 'url': 'https://huggingface.co/papers/2410.10306', 'abstract': 'Character image animation, which generates high-quality videos from a reference image and target pose sequence, has seen significant progress in recent years. However, most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters commonly used in industries like gaming and entertainment. Our in-depth analysis suggests to attribute this limitation to their insufficient modeling of motion, which is unable to comprehend the movement pattern of the driving video, thus imposing a pose sequence rigidly onto the target character. To this end, this paper proposes Animate-X, a universal animation framework based on LDM for various character types (collectively named X), including anthropomorphic characters. To enhance motion representation, we introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner. The former leverages CLIP visual features of a driving video to extract its gist of motion, like the overall movement pattern and temporal relations among motions, while the latter strengthens the generalization of LDM by simulating possible inputs in advance that may arise during inference. Moreover, we introduce a new Animated Anthropomorphic Benchmark (A^2Bench) to evaluate the performance of Animate-X on universal and widely applicable animation images. Extensive experiments demonstrate the superiority and effectiveness of Animate-X compared to state-of-the-art methods.', 'score': 15, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Animate-X - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–Ω–∏–º–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ LDM, –≤–∫–ª—é—á–∞—è –∞–Ω—Ç—Ä–æ–ø–æ–º–æ—Ä—Ñ–Ω—ã—Ö. –ê–≤—Ç–æ—Ä—ã –≤–≤–æ–¥—è—Ç Pose Indicator –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É—è –∫–∞–∫ –Ω–µ—è–≤–Ω—ã–µ, —Ç–∞–∫ –∏ —è–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ CLIP –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è –∏–∑ —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ –≤–∏–¥–µ–æ. –¢–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ A^2Bench –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö.', 'tags': ['#characterAnimation', '#poseIndicator', '#anthropomorphicCharacters'], 'categories': ['#cv', '#video', '#benchmark', '#multimodal'], 'emoji': 'üé≠', 'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –∞–Ω–∏–º–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –¥–≤–∏–∂–µ–Ω–∏–π'}}, {'id': 'https://huggingface.co/papers/2410.07985', 'title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models', 'url': 'https://huggingface.co/papers/2410.07985', 'abstract': "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.", 'score': 15, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —ç—Ç–∞–ª–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Ä–æ–≤–Ω–µ –æ–ª–∏–º–ø–∏–∞–¥. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç 4428 –∑–∞–¥–∞—á –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å —Ç—â–∞—Ç–µ–ª—å–Ω–æ–π —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –Ω–∞ 33 –ø–æ–¥–¥–æ–º–µ–Ω–∞ –∏ 10 —É—Ä–æ–≤–Ω–µ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏, —á—Ç–æ –¥–∞–∂–µ —Å–∞–º—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ OpenAI o1-mini –∏ OpenAI o1-preview, –∏—Å–ø—ã—Ç—ã–≤–∞—é—Ç —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ —Å —Ä–µ—à–µ–Ω–∏–µ–º —Å–ª–æ–∂–Ω—ã—Ö –æ–ª–∏–º–ø–∏–∞–¥–Ω—ã—Ö –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è —Ç–æ—á–Ω–æ—Å—Ç–∏ 60.54% –∏ 52.55% —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ. –≠—Ç–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–ª—è–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –Ω–∞ –æ–ª–∏–º–ø–∏–∞–¥–Ω–æ–º —É—Ä–æ–≤–Ω–µ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.', 'tags': ['#olympiad-math', '#benchmark-dataset', '#llm-evaluation'], 'categories': ['#benchmark', '#dataset', '#nlp'], 'emoji': 'üßÆ', 'title': '–ù–æ–≤—ã–π –≤—ã–∑–æ–≤ –¥–ª—è –ò–ò: –æ–ª–∏–º–ø–∏–∞–¥–Ω–∞—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∫–∞–∫ —Ç–µ—Å—Ç –Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç'}}, {'id': 'https://huggingface.co/papers/2410.10774', 'title': 'Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention', 'url': 'https://huggingface.co/papers/2410.10774', 'abstract': 'In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/', 'score': 14, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'Cavia - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –∫–∞–º–µ—Ä–æ–π –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ä–∞–∫—É—Ä—Å–æ–≤. –û–Ω–∞ —Ä–∞—Å—à–∏—Ä—è–µ—Ç –º–æ–¥—É–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —É–ª—É—á—à–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ä–∞–∫—É—Ä—Å–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å. Cavia –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Ç–æ—á–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã, –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–ª—É—á–∞—è –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ Cavia –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∫–∞—á–µ—Å—Ç–≤—É.', 'tags': ['#image2video', '#cameraControl', '#multiViewGeneration'], 'categories': ['#cv', '#video', '#multimodal'], 'emoji': 'üé•', 'title': 'Cavia: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞–º–µ—Ä—ã'}}, {'id': 'https://huggingface.co/papers/2410.10818', 'title': 'TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models', 'url': 'https://huggingface.co/papers/2410.10818', 'abstract': "Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.", 'score': 10, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': 'TemporalBench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –≤–∏–¥–µ–æ. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø—Ä–∏–º–µ—Ä–Ω–æ 10 —Ç—ã—Å—è—á –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –ø–æ –≤–∏–¥–µ–æ, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –≤–∏–¥–µ–æ–∫–ª–∏–ø–∞—Ö. –ë–µ–Ω—á–º–∞—Ä–∫ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫–∏–µ –∫–∞–∫ —á–∞—Å—Ç–æ—Ç–∞ –¥–µ–π—Å—Ç–≤–∏–π, –≤–µ–ª–∏—á–∏–Ω–∞ –¥–≤–∏–∂–µ–Ω–∏—è, –ø–æ—Ä—è–¥–æ–∫ —Å–æ–±—ã—Ç–∏–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º –∏ –º–æ–¥–µ–ª—è–º. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ª—é–¥—å–º–∏ –∏ –ò–ò –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤–∏–¥–µ–æ.', 'tags': ['#TemporalBench', '#VideoQA', '#TemporalReasoning'], 'categories': ['#benchmark', '#video', '#multimodal', '#dataset'], 'emoji': '‚è≥', 'title': 'TemporalBench: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –≤–∏–¥–µ–æ'}}, {'id': 'https://huggingface.co/papers/2410.10792', 'title': 'Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations', 'url': 'https://huggingface.co/papers/2410.10792', 'abstract': 'Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.', 'score': 5, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –∏–Ω–≤–µ—Ä—Å–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞ (Rectified Flow). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∏–Ω–≤–µ—Ä—Å–∏—é RF —Å –ø–æ–º–æ—â—å—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —á–µ—Ä–µ–∑ –ª–∏–Ω–µ–π–Ω—ã–π –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π —Ä–µ–≥—É–ª—è—Ç–æ—Ä. –ú–µ—Ç–æ–¥ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –∑–∞–¥–∞—á–∞—Ö –∏–Ω–≤–µ—Ä—Å–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –º–µ—Ç–æ–¥–∞ –ø–µ—Ä–µ–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏.', 'tags': ['#rectified_flow', '#image_inversion', '#image_editing'], 'categories': ['#cv', '#generative_models'], 'emoji': 'üñºÔ∏è', 'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∏–Ω–≤–µ—Ä—Å–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é –≤—ã–ø—Ä—è–º–ª–µ–Ω–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤'}}, {'id': 'https://huggingface.co/papers/2410.09335', 'title': 'Rethinking Data Selection at Scale: Random Selection is Almost All You Need', 'url': 'https://huggingface.co/papers/2410.09335', 'abstract': 'Supervised fine-tuning (SFT) is crucial for aligning Large Language Models (LLMs) with human instructions. The primary goal during SFT is to select a small yet representative subset of training data from the larger pool, such that fine-tuning with this subset achieves results comparable to or even exceeding those obtained using the entire dataset. However, most existing data selection techniques are designed for small-scale data pools, which fail to meet the demands of real-world SFT scenarios. In this paper, we replicated several self-scoring methods those that do not rely on external model assistance on two million scale datasets, and found that nearly all methods struggled to significantly outperform random selection when dealing with such large-scale data pools. Moreover, our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high quality data. We also analyzed the limitations of several current approaches, explaining why they perform poorly on large-scale datasets and why they are unsuitable for such contexts. Finally, we found that filtering data by token length offers a stable and efficient method for improving results. This approach, particularly when training on long text data, proves highly beneficial for relatively weaker base models, such as Llama3.', 'score': 5, 'issue_id': 107, 'pub_date': '2024-10-12', 'pub_date_ru': '12 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–º–æ—â—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π –¥–æ–≤–æ–¥–∫–∏ (SFT). –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤ —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∏ –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–≤—É—Ö –º–∏–ª–ª–∏–æ–Ω–æ–≤ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ –Ω–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—Ç —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ, —á—Ç–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–∏ SFT. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –¥–ª–∏–Ω–µ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –±–æ–ª–µ–µ —Å–ª–∞–±—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.', 'tags': ['#dataSelection', '#SFT', '#largeScaleDatasets'], 'categories': ['#nlp', '#dataset', '#benchmark'], 'emoji': 'üîç', 'title': '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π'}}, {'id': 'https://huggingface.co/papers/2410.06634', 'title': 'Tree of Problems: Improving structured problem solving with compositionality', 'url': 'https://huggingface.co/papers/2410.06634', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.', 'score': 4, 'issue_id': 109, 'pub_date': '2024-10-09', 'pub_date_ru': '9 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Tree of Problems (ToP) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. ToP —è–≤–ª—è–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π –º–µ—Ç–æ–¥–∞ Tree of Thoughts (ToT) –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–¥–∞—á, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –ø–æ–¥–∑–∞–¥–∞—á–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–æ–≤–µ–ª–∏ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –ø–æ–∫–∞–∑–∞–≤—à–∏–µ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—Å—Ç–≤–æ ToP –Ω–∞–¥ –º–µ—Ç–æ–¥–∞–º–∏ ToT –∏ Graph of Thoughts (GoT). –ö—Ä–æ–º–µ —Ç–æ–≥–æ, ToP –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º Chain-of-Thought (CoT) –ø—Ä–æ–º–ø—Ç–∏–Ω–≥ –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.', 'tags': ['#TreeOfProblems', '#ComplexReasoning', '#PromptEngineering'], 'categories': ['#nlp', '#rlhf', '#code'], 'emoji': 'üå≥', 'title': '–î—Ä–µ–≤–æ –ø—Ä–æ–±–ª–µ–º: –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á —Å –ø–æ–º–æ—â—å—é –ò–ò'}}, {'id': 'https://huggingface.co/papers/2410.07752', 'title': 'TVBench: Redesigning Video-Language Evaluation', 'url': 'https://huggingface.co/papers/2410.07752', 'abstract': 'Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.', 'score': 4, 'issue_id': 107, 'pub_date': '2024-10-10', 'pub_date_ru': '10 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–í —Å—Ç–∞—Ç—å–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø—Ä–æ–±–ª–µ–º—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –≤—ã—è–≤–∏–ª–∏, —á—Ç–æ –º–Ω–æ–≥–∏–µ –∑–∞–¥–∞—á–∏ –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ–æ. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ TVBench, —Ç—Ä–µ–±—É—é—â–∏–π –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–µ —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å TVBench, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è.', 'tags': ['#video-language-models', '#temporal-reasoning', '#benchmark-evaluation'], 'categories': ['#multimodal', '#benchmark', '#video'], 'emoji': 'üé•', 'title': '–ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ –æ—Ü–µ–Ω–∫—É –≤–∏–¥–µ–æ-—è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π: –≤–∞–∂–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞'}}, {'id': 'https://huggingface.co/papers/2410.10813', 'title': 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory', 'url': 'https://huggingface.co/papers/2410.10813', 'abstract': 'Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.', 'score': 2, 'issue_id': 109, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç LongMemEval - –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM). –ê–≤—Ç–æ—Ä—ã –≤—ã–¥–µ–ª—è—é—Ç –ø—è—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –∏ —Å–æ–∑–¥–∞—é—Ç –Ω–∞–±–æ—Ä –∏–∑ 500 –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∏—Ö —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–∏—Å—Ç–µ–º –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏. –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏, –≤–∫–ª—é—á–∞—é—â–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —á—Ç–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.', 'tags': ['#LongMemEval', '#–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è–ü–∞–º—è—Ç—å', '#—á–∞—Ç–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã'], 'categories': ['#nlp', '#benchmark', '#dataset'], 'emoji': 'üß†', 'title': 'LongMemEval: –ù–æ–≤—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ —á–∞—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤'}}, {'id': 'https://huggingface.co/papers/2410.10803', 'title': 'Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies', 'url': 'https://huggingface.co/papers/2410.10803', 'abstract': 'Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io', 'score': 2, 'issue_id': 108, 'pub_date': '2024-10-14', 'pub_date_ru': '14 –æ–∫—Ç—è–±—Ä—è', 'data': {'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ - Improved 3D Diffusion Policy (iDP3). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≥–æ—Ü–µ–Ω—Ç—Ä–∏—á–µ—Å–∫–∏–µ 3D –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –∫–∞–º–µ—Ä—ã –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –æ–±–ª–∞–∫–∞ —Ç–æ—á–µ–∫. iDP3 –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–≥–æ –≥—É–º–∞–Ω–æ–∏–¥–Ω–æ–≥–æ —Ä–æ–±–æ—Ç–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ, —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –≤ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤–ø–µ—Ä–µ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤ –≤ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö.', 'tags': ['#humanoidRobots', '#3DVisuomotorPolicy', '#autonomousManipulation'], 'categories': ['#rlhf', '#cv', '#robotics'], 'emoji': 'ü§ñ', 'title': '–†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –≥—É–º–∞–Ω–æ–∏–¥–Ω—ã—Ö —Ä–æ–±–æ—Ç–æ–≤'}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'default';        

        function loadSettings() {
            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            const themeToggle = document.getElementById('theme-toggle');
            let settingSortBy = localStorage.getItem('sort_by');
            const sortDropdown = document.getElementById('sort-dropdown');
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "—Ö—Ñ –Ω–∞–π—Ç–ª–∏";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'default';
            }
            
            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            return Array.from(categories);
        }
        
        function createCategoryButtons() {
            const categories = getUniqueCategories(articlesData);
            categories.forEach(category => {
                const button = document.createElement('span');
                button.textContent = category;
                button.className = 'category-button';
                button.onclick = () => toggleCategory(category, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if (selectedArticles.length === articlesData.length) {
                categoryToggle.textContent = 'üè∑Ô∏è –§–∏–ª—å—Ç—Ä';
            } else {
                categoryToggle.textContent = `üè∑Ô∏è –§–∏–ª—å—Ç—Ä (${formatArticlesTitle(selectedArticles.length)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const savedCategories = localStorage.getItem('selectedCategories');
            if (savedCategories) {
                if (savedCategories != '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);
                    updateCategoryButtonStates();
                }
            }
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles = selectedCategories.length === 0
                ? articlesData
                : articlesData.filter(article => 
                    article.data && article.data.categories && 
                    article.data.categories.some(cat => selectedCategories.includes(cat))
                );

            console.log('filteredArticles', filteredArticles)

            if (filteredArticles.length === 0) {
                selectedArticles = articlesData;
                selectedCategories = [];
                cleanCategorySelection();
            } else {
                selectedArticles = filteredArticles;
            }

            console.log('selectedArticles', selectedArticles)

            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
        }

        function renderArticles(articles) {
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                const explanation = item["data"]["desc"];
                const cats = item["data"]["categories"].join(" ");
                const articleHTML = `
                    <article>
                        <div class="background-digit">${index + 1}</div>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <h2>${item['data']['emoji']} ${item['title']}</h2>
                            <p class="meta"><svg class="text-sm peer-checked:text-gray-500 group-hover:text-gray-500" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 12 12"><path transform="translate(0, 2)" fill="currentColor" d="M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z"></path></svg> ${item['score']}. ${item['data']['title']}</p>
                            <p class="pub-date">üìÖ –°—Ç–∞—Ç—å—è –æ—Ç ${item['pub_date_ru']}</p>
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>
                            <div class="links">
                                <a href="${item['url']}" target="_blank">–°—Ç–∞—Ç—å—è</a>
                            </div>
                            <p class="tags">${cats}</p>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            }
            if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
        });

        clearCategoriesButton.addEventListener('click', clearAllCategories);
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiffRu('2024-10-15 10:14');
        } 

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();  
    </script>
</body>
</html>
    