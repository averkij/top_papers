
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 21 papers. March 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 марта</span> | <span id="title-articles-count">21 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-06.html">⬅️ <span id="prev-date">06.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-10.html">➡️ <span id="next-date">10.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'};
        let feedDateNext = {'ru': '10.03', 'en': '03/10', 'zh': '3月10日'};
        let feedDatePrev = {'ru': '06.03', 'en': '03/06', 'zh': '3月6日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.00865', 'title': 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers', 'url': 'https://huggingface.co/papers/2503.00865', 'abstract': "Large language models (LLMs) have revolutionized natural language processing (NLP), yet open-source multilingual LLMs remain scarce, with existing models often limited in language coverage. Such models typically prioritize well-resourced languages, while widely spoken but under-resourced languages are often overlooked. To address this disparity, we introduce Babel, an open multilingual LLM that covers the top 25 languages by number of speakers, supports over 90% of the global population, and includes many languages neglected by other open multilingual LLMs. Unlike traditional continue pretraining approaches, Babel expands its parameter count through a layer extension technique that elevates Babel's performance ceiling. We introduce two variants: Babel-9B, designed for efficient inference and fine-tuning, and Babel-83B, which sets a new standard for open multilingual LLMs. Extensive evaluations on multilingual tasks demonstrate its superior performance compared to open LLMs of comparable size. In addition, using open-source supervised fine-tuning datasets, Babel achieves remarkable performance, with Babel-9B-Chat leading among 10B-sized LLMs and Babel-83B-Chat setting a new standard for multilingual tasks, reaching the same level of commercial models.", 'score': 42, 'issue_id': 2555, 'pub_date': '2025-03-02', 'pub_date_card': {'ru': '2 марта', 'en': 'March 2', 'zh': '3月2日'}, 'hash': 'bc2424e709a2dd78', 'authors': ['Yiran Zhao', 'Chaoqun Liu', 'Yue Deng', 'Jiahao Ying', 'Mahani Aljunied', 'Zhaodonghui Li', 'Lidong Bing', 'Hou Pong Chan', 'Yu Rong', 'Deli Zhao', 'Wenxuan Zhang'], 'affiliations': ['DAMO Academy, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.00865.jpg', 'data': {'categories': ['#low_resource', '#architecture', '#open_source', '#training', '#multilingual'], 'emoji': '🌍', 'ru': {'title': 'Babel: революция в многоязычном машинном обучении', 'desc': 'Представлена новая многоязычная языковая модель Babel, охватывающая 25 самых распространенных языков мира. Модель использует технику расширения слоев для улучшения производительности. Предложены две версии: Babel-9B для эффективного вывода и дообучения, и Babel-83B, устанавливающая новый стандарт для открытых многоязычных моделей. Обе версии показывают превосходные результаты в многоязычных задачах по сравнению с аналогичными открытыми моделями.'}, 'en': {'title': 'Babel: Bridging the Language Gap with Open Multilingual LLMs', 'desc': "This paper presents Babel, an innovative open-source multilingual large language model (LLM) that addresses the lack of coverage for under-resourced languages in existing models. Babel supports the top 25 languages spoken globally, reaching over 90% of the world's population, and includes many languages that are often neglected. The model employs a unique layer extension technique to increase its parameter count, enhancing its performance beyond traditional pretraining methods. With two variants, Babel-9B and Babel-83B, the model demonstrates superior performance on multilingual tasks, outperforming other open LLMs of similar size and achieving results comparable to commercial models."}, 'zh': {'title': 'Babel：打破语言壁垒的多语言模型', 'desc': '大型语言模型（LLMs）在自然语言处理（NLP）领域带来了革命性的变化，但开源的多语言LLMs仍然稀缺，现有模型通常在语言覆盖上有限。许多模型优先考虑资源丰富的语言，而广泛使用但资源不足的语言常常被忽视。为了解决这一差距，我们推出了Babel，一个开放的多语言LLM，覆盖全球前25种语言，支持超过90%的人口，并包括许多其他开源多语言LLMs忽视的语言。Babel通过层扩展技术增加参数数量，提升了性能，并推出了两个变体：Babel-9B和Babel-83B，后者在多语言任务中设定了新的标准。'}}}, {'id': 'https://huggingface.co/papers/2503.02003', 'title': 'HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs', 'url': 'https://huggingface.co/papers/2503.02003', 'abstract': 'An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.', 'score': 26, 'issue_id': 2575, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'f2f75d12b377e06a', 'authors': ['Tin Nguyen', 'Logan Bolton', 'Mohammad Reza Taesiri', 'Anh Totti Nguyen'], 'affiliations': ['Auburn University, USA', 'University of Alberta, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2503.02003.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#hallucinations', '#multimodal'], 'emoji': '🔍', 'ru': {'title': 'Выделение фактов для борьбы с галлюцинациями в языковых моделях', 'desc': 'Статья представляет новый метод под названием Highlighted Chain-of-Thought Prompting (HoT) для борьбы с галлюцинациями в больших языковых моделях (LLM). HoT использует XML-теги для выделения фактов из входных данных в ответах модели. В экспериментах HoT превзошел обычный метод chain-of-thought prompting на 17 различных задачах. Хотя HoT помогает людям быстрее и точнее определять правильные ответы LLM, он также может ввести в заблуждение при неверных ответах.'}, 'en': {'title': 'Enhancing LLM Accuracy with Highlighted Prompts', 'desc': "This paper addresses the issue of Large Language Models (LLMs) generating incorrect or misleading information, known as hallucinations. The authors introduce a new technique called Highlighted Chain-of-Thought Prompting (HoT), which uses XML tags to emphasize factual information in the model's responses. By formatting the input question with these highlights, LLMs can produce more accurate answers, especially in few-shot learning scenarios across various tasks. However, while HoT improves verification of correct answers, it can also lead users to mistakenly trust incorrect responses due to the highlighted format."}, 'zh': {'title': '高亮链式思维提示：提升语言模型的准确性', 'desc': '大型语言模型（LLMs）存在一个弱点，即它们容易产生虚假的陈述。混合真实和虚假信息的回答使得人类难以验证并准确做出决策。为了解决这个问题，我们提出了高亮链式思维提示（HoT），这是一种通过XML标签来增强事实的提示技术。HoT在少量示例的情况下，在算术、阅读理解和逻辑推理等17个任务上表现优于传统的链式思维提示（CoT）。'}}}, {'id': 'https://huggingface.co/papers/2503.03746', 'title': 'Process-based Self-Rewarding Language Models', 'url': 'https://huggingface.co/papers/2503.03746', 'abstract': "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities.", 'score': 25, 'issue_id': 2564, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '808bee960390ec29', 'authors': ['Shimao Zhang', 'Xiao Liu', 'Xin Zhang', 'Junxiao Liu', 'Zheheng Luo', 'Shujian Huang', 'Yeyun Gong'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University', 'The University of Manchester'], 'pdf_title_img': 'assets/pdf/title_img/2503.03746.jpg', 'data': {'categories': ['#math', '#alignment', '#rlhf', '#reasoning', '#training'], 'emoji': '🧮', 'ru': {'title': 'Самообучение ИИ математике: шаг за шагом к сверхчеловеческим способностям', 'desc': 'Статья представляет новый метод самообучения языковых моделей для задач математических рассуждений. Авторы предлагают подход Process-based Self-Rewarding, который включает пошаговое рассуждение и оценку промежуточных результатов самой моделью. Этот метод позволяет преодолеть ограничения существующих подходов к самообучению в математических задачах. Эксперименты показывают, что новый метод значительно улучшает способности языковых моделей к математическим рассуждениям на различных тестовых наборах.'}, 'en': {'title': 'Empowering LLMs with Process-based Self-Rewarding for Superior Reasoning', 'desc': 'This paper discusses the limitations of current self-rewarding methods used to train Large Language Models (LLMs), particularly in mathematical reasoning tasks. The authors introduce a new approach called Process-based Self-Rewarding, which incorporates long-thought reasoning and a step-wise evaluation process. By allowing LLMs to act as judges of their own outputs, this method optimizes the training process iteratively. The results show significant improvements in LLM performance on mathematical reasoning benchmarks, suggesting that self-rewarding can enhance reasoning capabilities beyond human levels.'}, 'zh': {'title': '基于过程的自我奖励：超越人类的推理能力', 'desc': '大型语言模型在各种下游任务中表现出色，并广泛应用于多个场景。为了进一步提高其性能，研究者使用人类标注的偏好数据进行训练，但这受到人类表现上限的限制。因此，提出了自我奖励的方法，让语言模型通过奖励自己的输出生成训练数据。然而，现有的自我奖励方法在数学推理场景中效果不佳，甚至可能导致性能下降。本文提出了一种基于过程的自我奖励管道，通过引入长时间思考推理、逐步的语言模型评判和逐步的偏好优化，成功提升了语言模型在多个数学推理基准上的表现，展示了自我奖励在超越人类能力的推理中的巨大潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.00329', 'title': 'ABC: Achieving Better Control of Multimodal Embeddings using VLMs', 'url': 'https://huggingface.co/papers/2503.00329', 'abstract': 'Visual embedding models excel at zero-shot tasks like visual retrieval and classification. However, these models cannot be used for tasks that contain ambiguity or require user instruction. These tasks necessitate a multimodal embedding model, which outputs embeddings that combine visual and natural language input. Existing CLIP-based approaches embed images and text independently, and fuse the result. We find that this results in weak interactions between modalities, and poor user control over the representation. We introduce ABC, an open-source multimodal embedding model that uses a vision-language model backbone to deeply integrate image features with natural language instructions. ABC achieves bestfor-size performance on MSCOCO image-to-text retrieval and is the top performing model on classification and VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly unified vision-language representation, ABC can use natural language to solve subtle and potentially ambiguous visual retrieval problems. To evaluate this capability, we design CtrlBench, a benchmark that requires interleaving textual instructions with image content for correct retrieval. ABC advances the state of multimodal embeddings by offering high-quality representations and flexible natural language control. Our model and datasets are available at our project page.', 'score': 15, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '0483c542c8885777', 'authors': ['Benjamin Schneider', 'Florian Kerschbaum', 'Wenhu Chen'], 'affiliations': ['Cheriton School of Computer Science, University of Waterloo', 'Vector Institute, Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2503.00329.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#open_source', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'ABC: Мультимодальные встраивания с гибким языковым контролем', 'desc': 'Статья представляет новую мультимодальную модель встраивания под названием ABC, которая объединяет визуальные и текстовые данные. В отличие от существующих подходов, ABC использует глубокую интеграцию изображений и естественного языка. Модель демонстрирует высокую производительность в задачах поиска изображений по тексту и классификации. ABC также позволяет использовать естественный язык для решения сложных задач визуального поиска с неоднозначностями.'}, 'en': {'title': 'ABC: Unifying Vision and Language for Enhanced Multimodal Understanding', 'desc': "This paper presents ABC, a new multimodal embedding model that integrates visual and natural language inputs more effectively than existing CLIP-based methods. Unlike previous models that treat images and text separately, ABC combines these modalities deeply, allowing for better interaction and user control. The model excels in zero-shot tasks, particularly in image-to-text retrieval and classification, outperforming others in the Massive Multimodal Embedding Benchmark. To assess its capabilities, the authors introduce CtrlBench, a benchmark designed to evaluate the model's performance in handling complex visual retrieval tasks with natural language instructions."}, 'zh': {'title': 'ABC：多模态嵌入的新突破', 'desc': '这篇论文介绍了一种名为ABC的多模态嵌入模型，旨在解决视觉检索和分类中的模糊性问题。与现有的CLIP方法不同，ABC通过深度整合图像特征和自然语言指令，提供更强的模态交互。ABC在MSCOCO图像到文本检索任务中表现出色，并在分类和视觉问答任务中取得了最佳性能。通过设计CtrlBench基准，评估了ABC在处理复杂视觉检索问题时的能力，展示了其高质量的表示和灵活的自然语言控制。'}}}, {'id': 'https://huggingface.co/papers/2503.03751', 'title': 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control', 'url': 'https://huggingface.co/papers/2503.03751', 'abstract': 'We present GEN3C, a generative video model with precise Camera Control and temporal 3D Consistency. Prior video models already generate realistic videos, but they tend to leverage little 3D information, leading to inconsistencies, such as objects popping in and out of existence. Camera control, if implemented at all, is imprecise, because camera parameters are mere inputs to the neural network which must then infer how the video depends on the camera. In contrast, GEN3C is guided by a 3D cache: point clouds obtained by predicting the pixel-wise depth of seed images or previously generated frames. When generating the next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with the new camera trajectory provided by the user. Crucially, this means that GEN3C neither has to remember what it previously generated nor does it have to infer the image structure from the camera pose. The model, instead, can focus all its generative power on previously unobserved regions, as well as advancing the scene state to the next frame. Our results demonstrate more precise camera control than prior work, as well as state-of-the-art results in sparse-view novel view synthesis, even in challenging settings such as driving scenes and monocular dynamic video. Results are best viewed in videos. Check out our webpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/', 'score': 14, 'issue_id': 2555, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '8f5f2ad910a260c0', 'authors': ['Xuanchi Ren', 'Tianchang Shen', 'Jiahui Huang', 'Huan Ling', 'Yifan Lu', 'Merlin Nimier-David', 'Thomas Müller', 'Alexander Keller', 'Sanja Fidler', 'Jun Gao'], 'affiliations': ['NVIDIA', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.03751.jpg', 'data': {'categories': ['#3d', '#video'], 'emoji': '🎥', 'ru': {'title': 'Точный контроль камеры и 3D-согласованность в генерации видео', 'desc': 'GEN3C - это генеративная модель видео с точным контролем камеры и временной 3D-согласованностью. Она использует 3D-кэш в виде облаков точек, полученных из глубинных карт исходных изображений или ранее сгенерированных кадров. При генерации следующих кадров GEN3C опирается на 2D-рендеринг 3D-кэша с новой траекторией камеры, заданной пользователем. Это позволяет модели сфокусироваться на ранее ненаблюдаемых областях и продвижении состояния сцены, не тратя ресурсы на запоминание предыдущих результатов или вывод структуры изображения из положения камеры.'}, 'en': {'title': 'GEN3C: Mastering Video Generation with 3D Precision and Camera Control', 'desc': 'GEN3C is a generative video model that enhances video generation by incorporating precise camera control and maintaining temporal 3D consistency. Unlike previous models that often lack 3D information, GEN3C utilizes a 3D cache of point clouds derived from depth predictions, allowing for more coherent object presence in videos. The model is conditioned on 2D renderings from this cache, enabling it to generate new frames without needing to remember past outputs or infer scene structure from camera angles. This approach results in superior camera control and state-of-the-art performance in generating novel views, particularly in complex scenarios like driving scenes.'}, 'zh': {'title': 'GEN3C：精确相机控制与时间一致性的视频生成模型', 'desc': '我们提出了GEN3C，这是一种具有精确相机控制和时间一致性的生成视频模型。以往的视频模型虽然能够生成逼真的视频，但往往缺乏3D信息，导致物体出现和消失的不一致性。GEN3C通过3D缓存来指导生成过程，利用从种子图像或先前生成帧中预测的像素深度获得的点云。这样，GEN3C能够在用户提供的新相机轨迹下，专注于生成未观察到的区域，并有效推进场景状态到下一个帧。'}}}, {'id': 'https://huggingface.co/papers/2503.02951', 'title': 'KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding', 'url': 'https://huggingface.co/papers/2503.02951', 'abstract': 'We introduce KodCode, a synthetic dataset that addresses the persistent challenge of acquiring high-quality, verifiable training data across diverse difficulties and domains for training Large Language Models for coding. Existing code-focused resources typically fail to ensure either the breadth of coverage (e.g., spanning simple coding tasks to advanced algorithmic problems) or verifiable correctness (e.g., unit tests). In contrast, KodCode comprises question-solution-test triplets that are systematically validated via a self-verification procedure. Our pipeline begins by synthesizing a broad range of coding questions, then generates solutions and test cases with additional attempts allocated to challenging problems. Finally, post-training data synthesis is done by rewriting questions into diverse formats and generating responses under a test-based reject sampling procedure from a reasoning model (DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding dataset. KodCode is suitable for supervised fine-tuning and the paired unit tests also provide great potential for RL tuning. Fine-tuning experiments on coding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench) demonstrate that KodCode-tuned models achieve state-of-the-art performance, surpassing models like Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Llama-70B.', 'score': 13, 'issue_id': 2555, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '6c344ba0bf71ac84', 'authors': ['Zhangchen Xu', 'Yang Liu', 'Yueqin Yin', 'Mingyuan Zhou', 'Radha Poovendran'], 'affiliations': ['Microsoft', 'The University of Texas at Austin', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2503.02951.jpg', 'data': {'categories': ['#dataset', '#rl', '#optimization', '#synthetic', '#training'], 'emoji': '🧑\u200d💻', 'ru': {'title': 'KodCode: Синтетические данные для обучения ИИ программированию', 'desc': 'KodCode - это синтетический набор данных для обучения больших языковых моделей программированию. Он состоит из триплетов вопрос-решение-тест, которые проходят процедуру самопроверки. Процесс создания KodCode включает синтез вопросов по программированию, генерацию решений и тестовых случаев, а также постобработку данных. Эксперименты показывают, что модели, обученные на KodCode, достигают наилучших результатов на различных бенчмарках по программированию.'}, 'en': {'title': 'KodCode: Elevating Coding Models with Verified Data', 'desc': 'KodCode is a synthetic dataset designed to improve the training of Large Language Models (LLMs) for coding tasks by providing high-quality, verifiable data. It includes question-solution-test triplets that are validated through a self-verification process, ensuring both correctness and a wide range of coding difficulties. The dataset is generated using a systematic pipeline that synthesizes coding questions, creates solutions, and develops test cases, particularly focusing on challenging problems. Fine-tuning experiments show that models trained on KodCode outperform existing models on various coding benchmarks, demonstrating its effectiveness in enhancing LLM performance.'}, 'zh': {'title': 'KodCode：高质量编码数据集的解决方案', 'desc': '我们介绍了KodCode，这是一个合成数据集，旨在解决获取高质量、可验证的训练数据的挑战，以训练大型语言模型进行编码。现有的代码资源通常无法确保覆盖范围广泛或正确性可验证。KodCode由问题-解决方案-测试三元组组成，通过自我验证程序系统地验证。我们的流程包括合成各种编码问题，生成解决方案和测试用例，并在后期通过重写问题和生成响应来进行数据合成，最终生成一个大规模、强大且多样化的编码数据集。'}}}, {'id': 'https://huggingface.co/papers/2503.03278', 'title': 'Enhancing Abnormality Grounding for Vision Language Models with Knowledge Descriptions', 'url': 'https://huggingface.co/papers/2503.03278', 'abstract': 'Visual Language Models (VLMs) have demonstrated impressive capabilities in visual grounding tasks. However, their effectiveness in the medical domain, particularly for abnormality detection and localization within medical images, remains underexplored. A major challenge is the complex and abstract nature of medical terminology, which makes it difficult to directly associate pathological anomaly terms with their corresponding visual features. In this work, we introduce a novel approach to enhance VLM performance in medical abnormality detection and localization by leveraging decomposed medical knowledge. Instead of directly prompting models to recognize specific abnormalities, we focus on breaking down medical concepts into fundamental attributes and common visual patterns. This strategy promotes a stronger alignment between textual descriptions and visual features, improving both the recognition and localization of abnormalities in medical images.We evaluate our method on the 0.23B Florence-2 base model and demonstrate that it achieves comparable performance in abnormality grounding to significantly larger 7B LLaVA-based medical VLMs, despite being trained on only 1.5% of the data used for such models. Experimental results also demonstrate the effectiveness of our approach in both known and previously unseen abnormalities, suggesting its strong generalization capabilities.', 'score': 11, 'issue_id': 2560, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '6103dbe5d60b5f3f', 'authors': ['Jun Li', 'Che Liu', 'Wenjia Bai', 'Rossella Arcucci', 'Cosmin I. Bercea', 'Julia A. Schnabel'], 'affiliations': ['Helmholtz AI and Helmholtz Munich, Germany', 'Imperial College London, UK', 'Kings College London, UK', 'Munich Center for Machine Learning, Germany', 'Technical University of Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.03278.jpg', 'data': {'categories': ['#cv', '#multimodal', '#healthcare', '#alignment', '#transfer_learning'], 'emoji': '🔬', 'ru': {'title': 'Декомпозиция медицинских знаний для повышения эффективности VLM в анализе медицинских изображений', 'desc': 'Эта статья представляет новый подход к улучшению работы визуальных языковых моделей (VLM) в обнаружении и локализации аномалий на медицинских изображениях. Вместо прямого распознавания конкретных патологий, метод фокусируется на разложении медицинских концепций на базовые атрибуты и общие визуальные паттерны. Это улучшает связь между текстовыми описаниями и визуальными характеристиками, повышая точность распознавания и локализации аномалий. Метод был протестирован на модели Florence-2 и показал результаты, сравнимые с гораздо более крупными медицинскими VLM, несмотря на использование значительно меньшего объема данных для обучения.'}, 'en': {'title': 'Enhancing Medical VLMs through Decomposed Knowledge', 'desc': 'This paper presents a new method to improve Visual Language Models (VLMs) for detecting and locating abnormalities in medical images. The authors address the challenge of complex medical terminology by breaking down medical concepts into simpler attributes and common visual patterns. This approach enhances the alignment between text descriptions and visual features, leading to better performance in recognizing and localizing abnormalities. The proposed method shows competitive results with larger models while using significantly less training data, indicating its efficiency and strong generalization capabilities.'}, 'zh': {'title': '分解医学知识，提升视觉语言模型的异常检测能力', 'desc': '视觉语言模型（VLMs）在视觉定位任务中表现出色，但在医学领域，尤其是医学图像中的异常检测和定位方面，仍然缺乏研究。医学术语的复杂性使得将病理异常术语与相应的视觉特征直接关联变得困难。我们提出了一种新方法，通过分解医学知识来增强VLM在医学异常检测和定位中的性能。该方法通过将医学概念分解为基本属性和常见视觉模式，促进了文本描述与视觉特征之间的更强对齐，从而提高了医学图像中异常的识别和定位能力。'}}}, {'id': 'https://huggingface.co/papers/2503.01836', 'title': 'CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom', 'url': 'https://huggingface.co/papers/2503.01836', 'abstract': "Distilling advanced Large Language Models' instruction-following capabilities into smaller models using a selected subset has become a mainstream approach in model training. While existing synthetic instruction data selection strategies rely mainly on single-dimensional signals (i.e., reward scores, model perplexity), they fail to capture the complexity of instruction-following across diverse fields. Therefore, we investigate more diverse signals to capture comprehensive instruction-response pair characteristics and propose three foundational metrics that leverage Multi-LLM wisdom, informed by (1) diverse LLM responses and (2) reward model assessment. Building upon base metrics, we propose CrowdSelect, an integrated metric incorporating a clustering-based approach to maintain response diversity. Our comprehensive experiments demonstrate that our foundation metrics consistently improve performance across 4 base models on MT-bench and Arena-Hard. CrowdSelect, efficiently incorporating all metrics, achieves state-of-the-art performance in both Full and LoRA fine-tuning, showing improvements of 4.81% on Arena-Hard and 11.1% on MT-bench with Llama-3.2-3b-instruct. We hope our findings will bring valuable insights for future research in this direction. Code are available at https://github.com/listentm/crowdselect.", 'score': 9, 'issue_id': 2560, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'd59d65fb3b60c043', 'authors': ['Yisen Li', 'Lingfeng Yang', 'Wenxuan Shen', 'Pan Zhou', 'Yao Wan', 'Weiwei Lin', 'Dongping Chen'], 'affiliations': ['Huazhong University of Science and Technology', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01836.jpg', 'data': {'categories': ['#small_models', '#training', '#synthetic', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'CrowdSelect: умный отбор инструкций для обучения языковых моделей', 'desc': 'Статья описывает новый метод отбора инструкций для обучения языковых моделей, названный CrowdSelect. Он использует три основных метрики, основанные на оценках различных большиx языковых моделей и моделей вознаграждения. CrowdSelect также включает кластеризацию для сохранения разнообразия ответов. Эксперименты показали, что этот метод превосходит существующие подходы на бенчмарках MT-bench и Arena-Hard. Авторы надеются, что их исследование внесет вклад в развитие этого направления.'}, 'en': {'title': 'Enhancing Model Training with Diverse Instruction Metrics', 'desc': 'This paper focuses on improving the training of smaller models by distilling the instruction-following abilities of larger language models (LLMs). It critiques existing methods that use simple metrics for selecting synthetic instruction data, which do not adequately reflect the complexity of instruction-following tasks. The authors propose new metrics that utilize diverse responses from multiple LLMs and a reward model to better assess instruction-response pairs. Their method, CrowdSelect, combines these metrics with a clustering approach to enhance response diversity, leading to significant performance improvements in various model evaluations.'}, 'zh': {'title': '提升小模型的指令跟随能力', 'desc': '本论文探讨了如何将大型语言模型的指令跟随能力提炼到更小的模型中。现有的合成指令数据选择策略主要依赖单一维度的信号，未能全面捕捉指令跟随的复杂性。我们提出了三种基础指标，利用多种大型语言模型的智慧，结合多样的响应和奖励模型评估。通过综合实验，我们的CrowdSelect指标在多个基模型上表现出色，显著提升了性能，展示了未来研究的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.01933', 'title': 'Fine-Tuning Small Language Models for Domain-Specific AI: An Edge AI Perspective', 'url': 'https://huggingface.co/papers/2503.01933', 'abstract': 'Deploying large scale language models on edge devices faces inherent challenges such as high computational demands, energy consumption, and potential data privacy risks. This paper introduces the Shakti Small Language Models (SLMs) Shakti-100M, Shakti-250M, and Shakti-500M which target these constraints headon. By combining efficient architectures, quantization techniques, and responsible AI principles, the Shakti series enables on-device intelligence for smartphones, smart appliances, IoT systems, and beyond. We provide comprehensive insights into their design philosophy, training pipelines, and benchmark performance on both general tasks (e.g., MMLU, Hellaswag) and specialized domains (healthcare, finance, and legal). Our findings illustrate that compact models, when carefully engineered and fine-tuned, can meet and often exceed expectations in real-world edge-AI scenarios.', 'score': 7, 'issue_id': 2563, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '6fa74210552cc49f', 'authors': ['Rakshit Aralimatti', 'Syed Abdul Gaffar Shakhadri', 'Kruthika KR', 'Kartik Basavaraj Angadi'], 'affiliations': ['SandLogic Technologies Pvt Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2503.01933.jpg', 'data': {'categories': ['#healthcare', '#benchmark', '#training', '#ethics', '#inference', '#small_models', '#optimization'], 'emoji': '📱', 'ru': {'title': 'Малые языковые модели для большого интеллекта на краю сети', 'desc': 'Статья представляет серию малых языковых моделей Shakti, разработанных для применения на периферийных устройствах. Модели Shakti-100M, Shakti-250M и Shakti-500M решают проблемы высоких вычислительных требований, энергопотребления и потенциальных рисков конфиденциальности данных. Используя эффективные архитектуры, методы квантования и принципы ответственного ИИ, серия Shakti обеспечивает локальный интеллект для смартфонов, умных устройств и IoT-систем. Исследование показывает, что компактные модели, при тщательной разработке и настройке, могут соответствовать и часто превосходить ожидания в реальных сценариях периферийного ИИ.'}, 'en': {'title': 'Empowering Edge Devices with Efficient Language Models', 'desc': 'This paper presents the Shakti Small Language Models (SLMs) designed to operate efficiently on edge devices while addressing challenges like high computational needs and energy consumption. The Shakti models, including Shakti-100M, Shakti-250M, and Shakti-500M, utilize advanced architectures and quantization techniques to optimize performance without compromising data privacy. The authors detail the design philosophy, training processes, and benchmark results across various tasks and specialized fields such as healthcare and finance. The results demonstrate that well-engineered compact models can perform effectively in real-world applications, showcasing the potential of on-device AI.'}, 'zh': {'title': '小型语言模型，智能边缘计算的未来', 'desc': '本论文介绍了Shakti小型语言模型（SLMs），包括Shakti-100M、Shakti-250M和Shakti-500M，旨在解决在边缘设备上部署大型语言模型时面临的高计算需求和能耗问题。通过结合高效架构、量化技术和负责任的人工智能原则，Shakti系列实现了智能手机、智能家电和物联网系统的本地智能。我们提供了关于其设计理念、训练流程和在一般任务（如MMLU、Hellaswag）及专业领域（医疗、金融和法律）上的基准性能的全面见解。研究结果表明，经过精心设计和微调的紧凑模型能够在实际边缘人工智能场景中满足并超越预期。'}}}, {'id': 'https://huggingface.co/papers/2502.20317', 'title': 'Mixture of Structural-and-Textual Retrieval over Text-rich Graph Knowledge Bases', 'url': 'https://huggingface.co/papers/2502.20317', 'abstract': 'Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for answering queries by providing textual and structural knowledge. However, current retrieval methods often retrieve these two types of knowledge in isolation without considering their mutual reinforcement and some hybrid methods even bypass structural retrieval entirely after neighboring aggregation. To fill in this gap, we propose a Mixture of Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR generates textual planning graphs delineating the logic for answering queries. Following planning graphs, in the Reasoning stage, MoR interweaves structural traversal and textual matching to obtain candidates from TG-KBs. In the Organizing stage, MoR further reranks fetched candidates based on their structural trajectory. Extensive experiments demonstrate the superiority of MoR in harmonizing structural and textual retrieval with insights, including uneven retrieving performance across different query logics and the benefits of integrating structural trajectories for candidate reranking. Our code is available at https://github.com/Yoega/MoR.', 'score': 6, 'issue_id': 2561, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 февраля', 'en': 'February 27', 'zh': '2月27日'}, 'hash': '686b2ff85600f281', 'authors': ['Yongjia Lei', 'Haoyu Han', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka', 'Mahantesh M Halappanavar', 'Jiliang Tang', 'Yu Wang'], 'affiliations': ['Adobe Research', 'Michigan State University', 'Pacific Northwest National Laboratory', 'University of Oregon'], 'pdf_title_img': 'assets/pdf/title_img/2502.20317.jpg', 'data': {'categories': ['#benchmark', '#data', '#graphs', '#dataset', '#multimodal', '#reasoning'], 'emoji': '🕸️', 'ru': {'title': 'Гармоничное слияние структурного и текстового поиска в графовых базах знаний', 'desc': 'Эта статья представляет новый метод под названием MoR (Mixture of Structural-and-Textual Retrieval) для работы с графовыми базами знаний, содержащими текстовую информацию. MoR использует трехэтапный подход: планирование, рассуждение и организация, чтобы эффективно объединить структурный и текстовый поиск. Метод генерирует текстовые графы планирования, затем переплетает структурный обход и текстовое сопоставление, и наконец, переранжирует кандидатов на основе их структурных траекторий. Эксперименты показывают превосходство MoR в гармонизации структурного и текстового поиска.'}, 'en': {'title': 'Harmonizing Text and Structure for Better Knowledge Retrieval', 'desc': 'This paper introduces a new method called Mixture of Structural-and-Textual Retrieval (MoR) for improving the retrieval of knowledge from Text-rich Graph Knowledge Bases (TG-KBs). Unlike traditional methods that treat textual and structural knowledge separately, MoR integrates both types through a three-stage framework: Planning, Reasoning, and Organizing. In the Planning stage, it creates graphs that outline the logic for query responses, while the Reasoning stage combines structural traversal with textual matching to gather relevant candidates. Finally, the Organizing stage enhances the selection process by reranking candidates based on their structural paths, leading to better retrieval performance across various query types.'}, 'zh': {'title': '结构与文本知识的完美结合', 'desc': '本文提出了一种新的混合检索方法，称为结构与文本检索的混合体（MoR），旨在同时利用文本和结构知识来回答查询。MoR通过规划-推理-组织的框架来实现这一目标，在规划阶段生成文本规划图，明确回答查询的逻辑。接着，在推理阶段，MoR结合结构遍历和文本匹配，从文本丰富的图知识库中获取候选答案。最后，在组织阶段，MoR根据结构轨迹对获取的候选答案进行重新排序，实验结果表明该方法在结构和文本检索的协调性上具有显著优势。'}}}, {'id': 'https://huggingface.co/papers/2503.03044', 'title': 'QE4PE: Word-level Quality Estimation for Human Post-Editing', 'url': 'https://huggingface.co/papers/2503.03044', 'abstract': "Word-level quality estimation (QE) detects erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. Our QE4PE study investigates the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.", 'score': 5, 'issue_id': 2560, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'e4d3d7db506b6e1c', 'authors': ['Gabriele Sarti', 'Vilém Zouhar', 'Grzegorz Chrupała', 'Ana Guerberof-Arenas', 'Malvina Nissim', 'Arianna Bisazza'], 'affiliations': ['CLCG, University of Groningen', 'CSAI, Tilburg University', 'ETH Zürich'], 'pdf_title_img': 'assets/pdf/title_img/2503.03044.jpg', 'data': {'categories': ['#machine_translation', '#data', '#multilingual', '#healthcare'], 'emoji': '🔍', 'ru': {'title': 'Оценка качества перевода: мост между точностью и практичностью', 'desc': 'Статья исследует влияние оценки качества перевода на уровне слов (word-level QE) на процесс постредактирования машинного перевода. В исследовании участвовали 42 профессиональных редактора, работавших с двумя направлениями перевода. Сравнивались четыре модальности подсветки ошибок, включая методы на основе обучения с учителем и неопределенности. Результаты показывают, что эффективность подсветки зависит от домена, языка и скорости работы редакторов, при этом разница между ручной и автоматической QE оказалась незначительной.'}, 'en': {'title': 'Enhancing Post-Editing Efficiency with Word-Level Quality Estimation', 'desc': 'This paper explores how word-level quality estimation (QE) can help improve the efficiency of human post-editing in machine translation (MT). It examines the effectiveness of different methods for highlighting potential translation errors, comparing supervised and uncertainty-based approaches. The study involves 42 professional post-editors and assesses their editing speed and quality improvements through detailed behavioral logs and human annotations. The findings reveal that factors like domain, language, and editor speed significantly influence the effectiveness of error highlights, indicating a need to bridge the gap between the accuracy of QE systems and their practical usability in real-world editing tasks.'}, 'zh': {'title': '提升机器翻译后编辑效率的关键', 'desc': '本文研究了词级质量评估（QE）在机器翻译后编辑中的影响。我们分析了42名专业后编辑在两种翻译方向下的表现，比较了四种错误范围高亮方式，包括监督和基于不确定性的词级QE方法。研究发现，领域、语言和编辑速度是影响高亮效果的关键因素。结果表明，人工和自动QE高亮之间存在适度差异，突显了专业工作流程中准确性与可用性之间的差距。'}}}, {'id': 'https://huggingface.co/papers/2503.00307', 'title': 'Remasking Discrete Diffusion Models with Inference-Time Scaling', 'url': 'https://huggingface.co/papers/2503.00307', 'abstract': 'Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io.', 'score': 4, 'issue_id': 2572, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': '7f31677f2cb675e1', 'authors': ['Guanghan Wang', 'Yair Schiff', 'Subham Sekhar Sahoo', 'Volodymyr Kuleshov'], 'affiliations': ['Department of Computer Science, Cornell Unversity'], 'pdf_title_img': 'assets/pdf/title_img/2503.00307.jpg', 'data': {'categories': ['#open_source', '#inference', '#diffusion', '#science', '#multimodal'], 'emoji': '🔄', 'ru': {'title': 'Перемаскировка для улучшения дискретных диффузионных моделей', 'desc': 'Статья представляет новый метод ReMDM (remasking diffusion model), который улучшает процесс генерации в дискретных диффузионных моделях. ReMDM позволяет итеративно уточнять сгенерированные токены, что ранее было невозможно в классических масочных диффузионных моделях. Этот подход повышает качество генерации естественного языка, приближая его к уровню авторегрессионных моделей, особенно при увеличении числа шагов сэмплирования. ReMDM также демонстрирует улучшения в генерации дискретизированных изображений и дизайне молекул.'}, 'en': {'title': 'Enhancing Masked Diffusion with Iterative Refinement', 'desc': 'This paper introduces the remasking diffusion model (ReMDM) sampler, which enhances the capabilities of masked discrete diffusion models by allowing iterative refinement during output generation. Unlike traditional methods where generated tokens cannot be updated, ReMDM enables the correction of errors by applying a remasking backward process. This approach not only improves the quality of natural language outputs to rival autoregressive models but also maintains high quality under limited computational resources. Additionally, ReMDM enhances the performance of masked diffusion models in generating discretized images and aids in molecule design, pushing the boundaries of controllability in scientific applications.'}, 'zh': {'title': '重掩蔽扩散模型：提升生成质量的新方法', 'desc': '扩散模型的成功部分源于其迭代精炼的能力，即在生成过程中不断修正输出。然而，现代的掩蔽离散扩散模型缺乏这种能力：一旦生成一个标记，就无法再次更新，即使它引入了错误。为了解决这个限制，我们提出了重掩蔽扩散模型（ReMDM）采样器，这是一种可以以原则性方式应用于预训练掩蔽扩散模型的方法。ReMDM通过增加采样步骤，生成的自然语言输出质量接近自回归模型，同时在计算预算有限时，ReMDM更好地保持质量。'}}}, {'id': 'https://huggingface.co/papers/2502.18860', 'title': 'Exploring Rewriting Approaches for Different Conversational Tasks', 'url': 'https://huggingface.co/papers/2502.18860', 'abstract': "Conversational assistants often require a question rewriting algorithm that leverages a subset of past interactions to provide a more meaningful (accurate) answer to the user's question or request. However, the exact rewriting approach may often depend on the use case and application-specific tasks supported by the conversational assistant, among other constraints. In this paper, we systematically investigate two different approaches, denoted as rewriting and fusion, on two fundamentally different generation tasks, including a text-to-text generation task and a multimodal generative task that takes as input text and generates a visualization or data table that answers the user's question. Our results indicate that the specific rewriting or fusion approach highly depends on the underlying use case and generative task. In particular, we find that for a conversational question-answering assistant, the query rewriting approach performs best, whereas for a data analysis assistant that generates visualizations and data tables based on the user's conversation with the assistant, the fusion approach works best. Notably, we explore two datasets for the data analysis assistant use case, for short and long conversations, and we find that query fusion always performs better, whereas for the conversational text-based question-answering, the query rewrite approach performs best.", 'score': 4, 'issue_id': 2561, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '1f018cc4f38149bc', 'authors': ['Md Mehrab Tanjim', 'Ryan A. Rossi', 'Mike Rimer', 'Xiang Chen', 'Sungchul Kim', 'Vaishnavi Muppala', 'Tong Yu', 'Zhengmian Hu', 'Ritwik Sinha', 'Wei Zhang', 'Iftikhar Ahamath Burhanuddin', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2502.18860.jpg', 'data': {'categories': ['#dataset', '#multimodal'], 'emoji': '🤖', 'ru': {'title': 'Переписывание запросов в разговорных ИИ: один метод не подходит для всех задач', 'desc': 'В статье исследуются два подхода к переписыванию запросов в разговорных ассистентах: переписывание и слияние. Эксперименты проводились на двух различных задачах генерации: текст-в-текст и мультимодальной генерации визуализаций. Результаты показывают, что эффективность подхода зависит от конкретного случая использования и задачи. Для текстового вопросно-ответного ассистента лучше работает переписывание запросов, а для ассистента по анализу данных - слияние запросов.'}, 'en': {'title': 'Tailoring Response Strategies for Conversational Assistants', 'desc': 'This paper explores how conversational assistants can improve their responses by using two methods: rewriting and fusion. The rewriting method modifies user questions to enhance accuracy, while the fusion method combines information from past interactions to generate answers. The study shows that the effectiveness of these methods varies based on the task; rewriting is better for text-based question answering, while fusion excels in generating visualizations from data analysis tasks. The findings highlight the importance of tailoring the approach to the specific use case for optimal performance.'}, 'zh': {'title': '对话助手中的问题重写与融合方法的最佳选择', 'desc': '本论文探讨了对话助手中问题重写算法的两种不同方法：重写和融合。这两种方法在文本生成和多模态生成任务中表现不同，具体取决于应用场景。研究发现，对于对话问答助手，查询重写方法效果最佳；而对于生成可视化和数据表的数据分析助手，查询融合方法更为有效。我们还分析了短对话和长对话的数据集，结果表明查询融合在数据分析任务中始终表现更好。'}}}, {'id': 'https://huggingface.co/papers/2503.01763', 'title': "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", 'url': 'https://huggingface.co/papers/2503.01763', 'abstract': 'Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'e6a23582f741dc5b', 'authors': ['Zhengliang Shi', 'Yuhan Wang', 'Lingyong Yan', 'Pengjie Ren', 'Shuaiqiang Wang', 'Dawei Yin', 'Zhaochun Ren'], 'affiliations': ['Baidu Inc., Beijing, China', 'Leiden University, Leiden, The Netherlands', 'Shandong University, Qingdao, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.01763.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#dataset', '#data'], 'emoji': '🔍', 'ru': {'title': 'ToolRet: Новый вызов для моделей поиска инструментов ИИ', 'desc': 'ToolRet - это новый эталонный тест для оценки поиска инструментов в контексте обучения инструментам для больших языковых моделей (LLM). Он включает 7,6 тысяч разнообразных задач поиска и корпус из 43 тысяч инструментов. Исследование показало, что даже модели с высокой производительностью в традиционных тестах информационного поиска плохо справляются с ToolRet. Авторы также предоставили обучающий набор данных из более чем 200 тысяч примеров для улучшения способностей моделей к поиску инструментов.'}, 'en': {'title': 'Enhancing Tool Retrieval for Language Models with ToolRet', 'desc': 'This paper introduces ToolRet, a benchmark designed to evaluate the effectiveness of information retrieval (IR) models in selecting tools for large language models (LLMs) in practical tasks. The authors highlight that existing benchmarks often rely on a limited set of pre-annotated tools, which does not reflect real-world complexities. Their findings reveal that even high-performing IR models struggle with tool retrieval in this new context, leading to lower task success rates for LLMs. To address this issue, they provide a large-scale training dataset that significantly enhances the tool retrieval capabilities of IR models.'}, 'zh': {'title': '工具检索：提升LLMs的实用能力', 'desc': '本文探讨了工具学习如何增强大型语言模型（LLMs）的能力，使其能够作为代理解决实际任务。由于工具使用的LLMs具有有限的上下文长度，因此采用信息检索（IR）模型从大量工具集中选择有用工具是关键的初步步骤。我们提出了ToolRet，一个包含7.6k多样化检索任务和43k工具的异构工具检索基准，旨在评估IR模型在工具检索任务中的表现。研究发现，即使在传统IR基准上表现良好的模型，在ToolRet上的表现却很差，这降低了工具使用LLMs的任务通过率。'}}}, {'id': 'https://huggingface.co/papers/2503.01729', 'title': 'FLAME: A Federated Learning Benchmark for Robotic Manipulation', 'url': 'https://huggingface.co/papers/2503.01729', 'abstract': 'Recent progress in robotic manipulation has been fueled by large-scale datasets collected across diverse environments. Training robotic manipulation policies on these datasets is traditionally performed in a centralized manner, raising concerns regarding scalability, adaptability, and data privacy. While federated learning enables decentralized, privacy-preserving training, its application to robotic manipulation remains largely unexplored. We introduce FLAME (Federated Learning Across Manipulation Environments), the first benchmark designed for federated learning in robotic manipulation. FLAME consists of: (i) a set of large-scale datasets of over 160,000 expert demonstrations of multiple manipulation tasks, collected across a wide range of simulated environments; (ii) a training and evaluation framework for robotic policy learning in a federated setting. We evaluate standard federated learning algorithms in FLAME, showing their potential for distributed policy learning and highlighting key challenges. Our benchmark establishes a foundation for scalable, adaptive, and privacy-aware robotic learning.', 'score': 4, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '893358a382c79250', 'authors': ['Santiago Bou Betran', 'Alberta Longhini', 'Miguel Vasco', 'Yuchong Zhang', 'Danica Kragic'], 'affiliations': ['KTH Royal Institute of Technology, Stockholm, Sweden'], 'pdf_title_img': 'assets/pdf/title_img/2503.01729.jpg', 'data': {'categories': ['#robotics', '#benchmark', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Федеративное обучение для масштабируемой и конфиденциальной робототехники', 'desc': 'Статья представляет FLAME - первый бенчмарк для федеративного обучения в робототехнической манипуляции. FLAME включает в себя большой набор данных с более чем 160 000 экспертных демонстраций различных задач манипуляции, собранных в симулированных средах. Бенчмарк также предоставляет фреймворк для обучения и оценки робототехнических политик в федеративной среде. Авторы оценивают стандартные алгоритмы федеративного обучения на FLAME, демонстрируя их потенциал для распределенного обучения политик и выявляя ключевые проблемы.'}, 'en': {'title': 'Empowering Robots with Federated Learning for Privacy and Scalability', 'desc': 'This paper presents FLAME, a benchmark for applying federated learning to robotic manipulation tasks. It addresses the limitations of centralized training by allowing robots to learn from diverse datasets while preserving data privacy. FLAME includes over 160,000 expert demonstrations from various simulated environments, facilitating decentralized training. The study evaluates existing federated learning algorithms, demonstrating their effectiveness and identifying challenges in distributed policy learning for robotics.'}, 'zh': {'title': '联邦学习助力机器人操控的未来', 'desc': '这篇论文介绍了FLAME（跨操控环境的联邦学习），这是一个为机器人操控设计的基准测试。FLAME包含超过160,000个专家演示的大规模数据集，涵盖多种操控任务，收集自多种模拟环境。通过在FLAME中评估标准的联邦学习算法，论文展示了分布式策略学习的潜力，并指出了关键挑战。该基准为可扩展、适应性强且注重隐私的机器人学习奠定了基础。'}}}, {'id': 'https://huggingface.co/papers/2503.01449', 'title': 'Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection', 'url': 'https://huggingface.co/papers/2503.01449', 'abstract': 'Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.', 'score': 3, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '1b4593bb9d78ec53', 'authors': ['Ting Zhang', 'Chengran Yang', 'Yindu Su', 'Martin Weyssow', 'Hung Nguyen', 'Tan Bui', 'Hong Jin Kang', 'Yikun Li', 'Eng Lieh Ouh', 'Lwin Khin Shar', 'David Lo'], 'affiliations': ['School of Computer Science, University of Sydney, Australia', 'School of Computing and Information Systems, Singapore Management University, Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.01449.jpg', 'data': {'categories': ['#open_source', '#plp', '#training', '#security', '#benchmark', '#dataset', '#data'], 'emoji': '🛡️', 'ru': {'title': 'LLM на страже безопасности кода: новые горизонты в обнаружении уязвимостей', 'desc': 'Статья представляет комплексное исследование возможностей больших языковых моделей (LLM) в обнаружении уязвимостей программного обеспечения (SVD). Авторы оценивают производительность пяти открытых LLM на наборах данных, включающих уязвимые функции на Python, Java и JavaScript, используя различные подходы, такие как инженерия промптов, настройка инструкций и тонкая настройка классификации последовательностей. Исследование также изучает способы улучшения производительности LLM в SVD, включая переобучение на сбалансированных наборах данных и использование ансамблевых методов обучения. Результаты показывают, что SVD остается сложной задачей для LLM, предоставляя ценные insights для будущих разработок в области применения генеративного ИИ для повышения безопасности программного обеспечения.'}, 'en': {'title': 'Unlocking LLMs for Software Vulnerability Detection', 'desc': "This paper investigates the effectiveness of large language models (LLMs) in detecting software vulnerabilities, an important area for software security. It highlights the lack of comprehensive studies on LLMs' capabilities across various programming languages, as most existing research focuses on C/C++ datasets. The authors present an empirical study using a dataset of over 44,000 vulnerable functions from Python, Java, and JavaScript, evaluating five open-source LLMs with different strategies like prompt engineering and instruction tuning. The findings reveal that while LLMs show promise, software vulnerability detection remains a challenging task, providing valuable insights for future improvements in this field."}, 'zh': {'title': '提升软件安全：大型语言模型在漏洞检测中的应用', 'desc': '最近生成性人工智能的进展使得大型语言模型（LLMs）在软件工程中得到了广泛应用，解决了许多长期存在的挑战。然而，目前缺乏对LLMs在软件漏洞检测（SVD）能力的全面研究，这对软件安全至关重要。现有研究主要集中在使用C/C++数据集评估LLMs，通常只探讨了提示工程、指令调优和序列分类微调中的一两种策略。因此，我们进行了一项全面的实证研究，评估LLMs在不同编程语言中检测漏洞的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01378', 'title': 'CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs', 'url': 'https://huggingface.co/papers/2503.01378', 'abstract': 'This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA) model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand advanced cognitive abilities. Trained on a dataset comprising over 8,000 simulated flight trajectories across three key categories-Human Recognition, Symbol Understanding, and Reasoning-the model generates real-time 4D action commands based on first-person visual inputs and textual instructions. To further enhance performance in intricate scenarios, we propose CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM) reasoning module to simplify task directives prior to high-frequency control. Experimental evaluations using our open-source benchmark, CognitiveDroneBench, reveal that while a racing-oriented model (RaceVLA) achieves an overall success rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate improvements of up to 30% in critical cognitive tasks, underscoring the effectiveness of incorporating advanced reasoning capabilities into UAV control systems. Our contributions include the development of a state-of-the-art VLA model for UAV control and the introduction of the first dedicated benchmark for assessing cognitive tasks in drone operations. The complete repository is available at cognitivedrone.github.io', 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '8a4aab69ce92453d', 'authors': ['Artem Lykov', 'Valerii Serpiva', 'Muhammad Haris Khan', 'Oleg Sautenkov', 'Artyom Myshlyaev', 'Grik Tadevosyan', 'Yasheerah Yaqoot', 'Dzmitry Tsetserukou'], 'affiliations': ['Intelligent Space Robotics Laboratory, Science Center for Digital Engineering, Technology. Skolkovo Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01378.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#benchmark', '#dataset', '#multimodal', '#cv'], 'emoji': '🚁', 'ru': {'title': 'Умные дроны: когнитивное управление БПЛА с помощью ИИ', 'desc': 'В статье представлена модель CognitiveDrone - новая модель Зрение-Язык-Действие (VLA) для сложных задач беспилотных летательных аппаратов (БПЛА). Модель обучена на наборе данных из более чем 8000 симулированных полетов и генерирует команды действий в реальном времени на основе визуальных входных данных и текстовых инструкций. Усовершенствованная версия CognitiveDrone-R1 включает дополнительный модуль рассуждений на основе Модели Зрения-Языка (VLM) для упрощения сложных задач. Экспериментальная оценка показывает, что CognitiveDrone-R1 достигает успешности выполнения задач в 77.2%, что на 30% лучше базовых моделей в критических когнитивных задачах.'}, 'en': {'title': 'CognitiveDrone: Elevating UAV Intelligence with Vision-Language-Action!', 'desc': 'This paper presents CognitiveDrone, a new Vision-Language-Action (VLA) model designed for complex tasks performed by Unmanned Aerial Vehicles (UAVs). It is trained on a dataset of over 8,000 simulated flight paths focusing on Human Recognition, Symbol Understanding, and Reasoning. The model can generate real-time 4D action commands from visual inputs and text instructions, with an enhanced version, CognitiveDrone-R1, that includes a Vision-Language Model (VLM) reasoning module for better task management. Experimental results show significant performance improvements, with CognitiveDrone-R1 achieving a 77.2% success rate, highlighting the importance of advanced reasoning in UAV operations.'}, 'zh': {'title': '智能无人机的认知飞行新纪元', 'desc': '本文介绍了一种名为CognitiveDrone的新型视觉-语言-行动（VLA）模型，专为复杂的无人机任务设计，具备高级认知能力。该模型在超过8000条模拟飞行轨迹的数据集上进行训练，涵盖人类识别、符号理解和推理三个关键类别。CognitiveDrone-R1通过集成额外的视觉-语言模型（VLM）推理模块，进一步提升在复杂场景中的表现。实验结果显示，CognitiveDrone模型的成功率达到59.6%，而CognitiveDrone-R1的成功率更是高达77.2%，证明了将高级推理能力融入无人机控制系统的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01372', 'title': 'SwiLTra-Bench: The Swiss Legal Translation Benchmark', 'url': 'https://huggingface.co/papers/2503.01372', 'abstract': "In Switzerland legal translation is uniquely important due to the country's four official languages and requirements for multilingual legal documentation. However, this process traditionally relies on professionals who must be both legal experts and skilled translators -- creating bottlenecks and impacting effective access to justice. To address this challenge, we introduce SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned Swiss legal translation pairs comprising laws, headnotes, and press releases across all Swiss languages along with English, designed to evaluate LLM-based translation systems. Our systematic evaluation reveals that frontier models achieve superior translation performance across all document types, while specialized translation systems excel specifically in laws but under-perform in headnotes. Through rigorous testing and human expert validation, we demonstrate that while fine-tuning open SLMs significantly improves their translation quality, they still lag behind the best zero-shot prompted frontier models such as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM evaluation system that aligns best with human expert assessments.", 'score': 2, 'issue_id': 2558, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '3de5be81537fa0fd', 'authors': ['Joel Niklaus', 'Jakob Merane', 'Luka Nenadic', 'Sina Ahmadi', 'Yingqiang Gao', 'Cyrill A. H. Chevalley', 'Claude Humbel', 'Christophe Gösken', 'Lorenzo Tanzi', 'Thomas Lüthi', 'Stefan Palombo', 'Spencer Poff', 'Boling Yang', 'Nan Wu', 'Matthew Guillod', 'Robin Mamié', 'Daniel Brunner', 'Julio Pereyra', 'Niko Grupen'], 'affiliations': ['Canton of Solothurn', 'ETH Zurich', 'Max Planck Institute for Research on Collective Goods', 'Swiss Federal Supreme Court', 'University of Basel', 'University of Geneva', 'University of Lausanne', 'University of Zurich'], 'pdf_title_img': 'assets/pdf/title_img/2503.01372.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#benchmark', '#dataset', '#machine_translation'], 'emoji': '⚖️', 'ru': {'title': 'Революция в юридическом переводе: ИИ покоряет многоязычную Швейцарию', 'desc': 'Статья представляет SwiLTra-Bench - многоязычный набор данных для оценки систем машинного перевода юридических текстов в Швейцарии. Авторы провели систематическую оценку различных моделей, включая крупные языковые модели и специализированные системы перевода. Результаты показывают, что передовые модели достигают лучших результатов во всех типах документов, а дообучение открытых моделей значительно улучшает качество перевода. Также представлена система SwiLTra-Judge для оценки качества перевода, которая хорошо коррелирует с оценками экспертов.'}, 'en': {'title': 'Enhancing Legal Translation with SwiLTra-Bench and LLMs', 'desc': 'This paper addresses the challenges of legal translation in Switzerland, where multiple languages complicate the process. It introduces SwiLTra-Bench, a benchmark dataset with over 180,000 aligned legal translation pairs to evaluate large language model (LLM) translation systems. The findings show that while advanced models perform well across various document types, specialized systems are better for translating laws but struggle with headnotes. The study also highlights the effectiveness of fine-tuning open-source language models, although they still do not match the performance of top zero-shot models like Claude-3.5-Sonnet.'}, 'zh': {'title': '瑞士法律翻译的智能解决方案', 'desc': '在瑞士，由于有四种官方语言，法律翻译显得尤为重要。传统上，这一过程依赖于既是法律专家又是翻译高手的专业人士，导致了瓶颈，影响了公正的有效获取。为了解决这个问题，我们推出了SwiLTra-Bench，这是一个包含超过18万对瑞士法律翻译的多语言基准数据集，旨在评估基于大语言模型的翻译系统。我们的评估显示，前沿模型在所有文档类型的翻译表现上优于其他系统，而专门的翻译系统在法律文本中表现出色，但在头注方面表现不佳。'}}}, {'id': 'https://huggingface.co/papers/2503.00502', 'title': 'Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions', 'url': 'https://huggingface.co/papers/2503.00502', 'abstract': "Autonomous Vehicles (AVs) have entered the commercialization stage, but their limited ability to interact and express intentions still poses challenges in interactions with Human-driven Vehicles (HVs). Recent advances in large language models (LLMs) enable bidirectional human-machine communication, but the conflict between slow inference speed and the need for real-time decision-making challenges practical deployment. To address these issues, this paper introduces a parallel Actor-Reasoner framework designed to enable explicit bidirectional AV-HV interactions across multiple scenarios. First, by facilitating interactions between the LLM-driven Reasoner and heterogeneous simulated HVs during training, an interaction memory database, referred to as the Actor, is established. Then, by introducing the memory partition module and the two-layer memory retrieval module, the Actor's ability to handle heterogeneous HVs is significantly enhanced. Ablation studies and comparisons with other decision-making methods demonstrate that the proposed Actor-Reasoner framework significantly improves safety and efficiency. Finally, with the combination of the external Human-Machine Interface (eHMI) information derived from Reasoner's reasoning and the feasible action solutions retrieved from the Actor, the effectiveness of the proposed Actor-Reasoner is confirmed in multi-scenario field interactions. Our code is available at https://github.com/FanGShiYuu/Actor-Reasoner.", 'score': 2, 'issue_id': 2555, 'pub_date': '2025-03-01', 'pub_date_card': {'ru': '1 марта', 'en': 'March 1', 'zh': '3月1日'}, 'hash': 'd184a5cae68093d5', 'authors': ['Shiyu Fang', 'Jiaqi Liu', 'Chengkai Xu', 'Chen Lv', 'Peng Hang', 'Jian Sun'], 'affiliations': ['College of Transportation, Tongji University, Shanghai 201804, China', 'Nanyang Technological University, 639798, Singapore', 'State Key Lab of Intelligent Transportation System, Beijing 100088, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.00502.jpg', 'data': {'categories': ['#rl', '#robotics', '#inference', '#optimization', '#agents', '#reasoning'], 'emoji': '🚗', 'ru': {'title': 'Интеллектуальное взаимодействие автономных и обычных автомобилей с помощью больших языковых моделей', 'desc': 'Эта статья представляет новую архитектуру Actor-Reasoner для улучшения взаимодействия между автономными и управляемыми человеком транспортными средствами. Авторы используют большие языковые модели для создания базы данных взаимодействий и двухуровневую систему извлечения памяти для работы с разнородными транспортными средствами. Предложенный подход значительно повышает безопасность и эффективность принятия решений по сравнению с другими методами. Эксперименты в реальных условиях подтверждают эффективность предложенной архитектуры Actor-Reasoner в различных сценариях.'}, 'en': {'title': 'Enhancing AV-HV Interactions with the Actor-Reasoner Framework', 'desc': "This paper presents a new framework called the Actor-Reasoner to improve interactions between Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). It leverages large language models (LLMs) to facilitate real-time communication and decision-making, addressing the challenge of slow inference speeds. The framework includes an interaction memory database, which enhances the AV's ability to understand and respond to various HV behaviors. Experimental results show that this approach significantly boosts both safety and efficiency in multi-scenario driving situations."}, 'zh': {'title': '提升自动驾驶与人类驾驶互动的智能框架', 'desc': '这篇论文介绍了一种新的并行演员-推理器框架，旨在改善自动驾驶汽车（AV）与人类驾驶汽车（HV）之间的互动。通过在训练过程中促进大语言模型（LLM）驱动的推理器与不同类型的模拟HV之间的互动，建立了一个互动记忆数据库。引入记忆分区模块和双层记忆检索模块后，演员的处理能力得到了显著提升。实验结果表明，该框架在多场景交互中显著提高了安全性和效率。'}}}, {'id': 'https://huggingface.co/papers/2503.02954', 'title': 'Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders', 'url': 'https://huggingface.co/papers/2503.02954', 'abstract': 'Multi-agent coordination is crucial for reliable multi-robot navigation in shared spaces such as automated warehouses. In regions of dense robot traffic, local coordination methods may fail to find a deadlock-free solution. In these scenarios, it is appropriate to let a central unit generate a global schedule that decides the passing order of robots. However, the runtime of such centralized coordination methods increases significantly with the problem scale. In this paper, we propose to leverage Graph Neural Network Variational Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale faster than through centralized optimization. We formulate the coordination problem as a graph problem and collect ground truth data using a Mixed-Integer Linear Program (MILP) solver. During training, our learning framework encodes good quality solutions of the graph problem into a latent space. At inference time, solution samples are decoded from the sampled latent variables, and the lowest-cost sample is selected for coordination. Finally, the feasible proposal with the highest performance index is selected for the deployment. By construction, our GNN-VAE framework returns solutions that always respect the constraints of the considered coordination problem. Numerical results show that our approach trained on small-scale problems can achieve high-quality solutions even for large-scale problems with 250 robots, being much faster than other baselines. Project page: https://mengyuest.github.io/gnn-vae-coord', 'score': 1, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': '1d0c072d834299e0', 'authors': ['Yue Meng', 'Nathalie Majcherczyk', 'Wenliang Liu', 'Scott Kiesel', 'Chuchu Fan', 'Federico Pecora'], 'affiliations': ['Amazon Robotics, North Reading, MA USA', 'Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02954.jpg', 'data': {'categories': ['#optimization', '#training', '#games', '#agents', '#inference', '#graphs'], 'emoji': '🤖', 'ru': {'title': 'GNN-VAE: Быстрая координация множества роботов', 'desc': 'Статья представляет новый подход к координации множества роботов в общих пространствах, таких как автоматизированные склады. Авторы предлагают использовать вариационные автоэнкодеры на основе графовых нейронных сетей (GNN-VAE) для решения проблемы координации в масштабе быстрее, чем с помощью централизованной оптимизации. Модель обучается на данных, сгенерированных решателем задач смешанного целочисленного линейного программирования (MILP). Результаты показывают, что подход может достигать высококачественных решений даже для крупномасштабных проблем с 250 роботами, работая значительно быстрее других базовых методов.'}, 'en': {'title': 'Efficient Multi-Robot Coordination with GNN-VAE', 'desc': 'This paper addresses the challenge of coordinating multiple robots in shared environments, particularly in high-traffic areas where traditional local methods may lead to deadlocks. The authors propose a novel approach using Graph Neural Network Variational Autoencoders (GNN-VAE) to efficiently generate coordination schedules at scale. By framing the coordination problem as a graph problem and utilizing a Mixed-Integer Linear Program (MILP) for data collection, the framework learns to encode effective solutions into a latent space. During inference, it decodes these solutions to select the most optimal coordination strategy, ensuring compliance with all operational constraints while significantly reducing computation time compared to centralized methods.'}, 'zh': {'title': '高效多智能体协调的新方法', 'desc': '多智能体协调在共享空间中的可靠多机器人导航中至关重要，尤其是在机器人交通密集的区域。传统的局部协调方法可能无法找到无死锁的解决方案，因此需要一个中央单元生成全局调度来决定机器人的通行顺序。本文提出利用图神经网络变分自编码器（GNN-VAE）来更快地解决大规模的多智能体协调问题，避免了集中优化方法的高运行时间。通过将协调问题形式化为图问题，并使用混合整数线性规划（MILP）求解器收集真实数据，我们的学习框架能够在潜在空间中编码高质量的解决方案。'}}}, {'id': 'https://huggingface.co/papers/2503.02924', 'title': 'Diverse Controllable Diffusion Policy with Signal Temporal Logic', 'url': 'https://huggingface.co/papers/2503.02924', 'abstract': 'Generating realistic simulations is critical for autonomous system applications such as self-driving and human-robot interactions. However, driving simulators nowadays still have difficulty in generating controllable, diverse, and rule-compliant behaviors for road participants: Rule-based models cannot produce diverse behaviors and require careful tuning, whereas learning-based methods imitate the policy from data but are not designed to follow the rules explicitly. Besides, the real-world datasets are by nature "single-outcome", making the learning method hard to generate diverse behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion Models to learn controllable, diverse, and rule-aware policy. We first calibrate the STL on the real-world data, then generate diverse synthetic data using trajectory optimization, and finally learn the rectified diffusion policy on the augmented dataset. We test on the NuScenes dataset and our approach can achieve the most diverse rule-compliant trajectories compared to other baselines, with a runtime 1/17X to the second-best approach. In the closed-loop testing, our approach reaches the highest diversity, rule satisfaction rate, and the least collision rate. Our method can generate varied characteristics conditional on different STL parameters in testing. A case study on human-robot encounter scenarios shows our approach can generate diverse and closed-to-oracle trajectories. The annotation tool, augmented dataset, and code are available at https://github.com/mengyuest/pSTL-diffusion-policy.', 'score': 0, 'issue_id': 2574, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'adc4dd2a16bb83a4', 'authors': ['Yue Meng', 'Chuchu fan'], 'affiliations': ['Department of Aeronautics and Astronautics, Massachusetts Institute of Technology, Cambridge, MA 02139 USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02924.jpg', 'data': {'categories': ['#dataset', '#training', '#data', '#rl', '#agents', '#diffusion', '#synthetic'], 'emoji': '🚗', 'ru': {'title': 'Реалистичные и разнообразные симуляции дорожного движения с помощью STL и диффузионных моделей', 'desc': 'Эта статья представляет новый подход к генерации реалистичных симуляций для автономных систем, таких как беспилотные автомобили и взаимодействие человека с роботом. Авторы используют комбинацию Сигнальной Темпоральной Логики (STL) и Диффузионных Моделей для создания контролируемой, разнообразной и соблюдающей правила политики поведения участников дорожного движения. Метод сначала калибрует STL на реальных данных, затем генерирует разнообразные синтетические данные с помощью оптимизации траекторий, и наконец обучает скорректированную диффузионную политику на расширенном наборе данных. Результаты показывают, что подход превосходит базовые методы по разнообразию, соблюдению правил и безопасности, а также позволяет генерировать различные характеристики в зависимости от параметров STL.'}, 'en': {'title': 'Diverse and Rule-Compliant Simulations for Autonomous Systems', 'desc': 'This paper addresses the challenge of generating realistic simulations for autonomous systems, particularly in driving scenarios. It combines Signal Temporal Logic (STL) with Diffusion Models to create a policy that is both controllable and diverse while adhering to traffic rules. By calibrating STL on real-world data and generating synthetic data through trajectory optimization, the authors enhance the learning process to produce varied and rule-compliant behaviors. The results demonstrate that their method outperforms existing approaches in terms of diversity and safety in simulated environments.'}, 'zh': {'title': '生成多样化且遵循规则的行为策略', 'desc': '本文提出了一种新方法，通过信号时序逻辑（STL）和扩散模型来生成可控、多样且遵循规则的行为策略，以解决当前驾驶模拟器在生成道路参与者行为时的局限性。我们首先在真实数据上校准STL，然后利用轨迹优化生成多样的合成数据，最后在增强数据集上学习修正的扩散策略。实验结果表明，我们的方法在NuScenes数据集上能够生成最具多样性且符合规则的轨迹，且运行时间显著优于其他基线方法。通过闭环测试，我们的方法在多样性、规则满足率和碰撞率方面均表现最佳，能够根据不同的STL参数生成多样化的特征。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi', '#alignment (2)', '#architecture (1)', '#audio', '#benchmark (8)', '#cv (2)', '#data (5)', '#dataset (10)', '#diffusion (2)', '#ethics (1)', '#games (1)', '#graphs (2)', '#hallucinations (1)', '#healthcare (3)', '#inference (4)', '#interpretability', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation (2)', '#math (1)', '#multilingual (3)', '#multimodal (7)', '#open_source (6)', '#optimization (6)', '#plp (1)', '#rag', '#reasoning (5)', '#rl (3)', '#rlhf (2)', '#robotics (2)', '#science (1)', '#security (1)', '#small_models (2)', '#story_generation', '#survey', '#synthetic (3)', '#training (9)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-07 02:17',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-07 02:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-07 02:17')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    