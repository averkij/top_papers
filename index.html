
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. May 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 мая</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-26.html">⬅️ <span id="prev-date">26.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-28.html">➡️ <span id="next-date">28.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'};
        let feedDateNext = {'ru': '28.05', 'en': '05/28', 'zh': '5月28日'};
        let feedDatePrev = {'ru': '26.05', 'en': '05/26', 'zh': '5月26日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.19147', 'title': 'Shifting AI Efficiency From Model-Centric to Data-Centric Compression', 'url': 'https://huggingface.co/papers/2505.19147', 'abstract': "The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.", 'score': 53, 'issue_id': 3968, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': '74f14f3f2d4f73b6', 'authors': ['Xuyang Liu', 'Zichen Wen', 'Shaobo Wang', 'Junjie Chen', 'Zhishan Tao', 'Yubo Wang', 'Xiangqi Jin', 'Chang Zou', 'Yiyu Wang', 'Chenfei Liao', 'Xu Zheng', 'Honggang Chen', 'Weijia Li', 'Xuming Hu', 'Conghui He', 'Linfeng Zhang'], 'affiliations': ['EPIC Lab, Shanghai Jiao Tong University', 'Hong Kong University of Science and Technology (Guangzhou)', 'Shanghai AI Laboratory', 'Sichuan University', 'Sun Yat-sen University', 'University of Electronic Science & Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19147.jpg', 'data': {'categories': ['#training', '#data', '#math', '#optimization', '#long_context', '#survey'], 'emoji': '🗜️', 'ru': {'title': 'Сжатие токенов: новый рубеж эффективности ИИ', 'desc': 'Статья рассматривает переход от масштабирования моделей к сжатию данных в контексте развития крупных языковых моделей (LLM) и мультимодальных LLM. Авторы утверждают, что сжатие токенов становится новым фронтиром в повышении эффективности искусственного интеллекта. В работе анализируются недавние разработки в области ИИ с длинным контекстом и предлагается единая математическая структура для существующих стратегий эффективности моделей. Статья также исследует преимущества сжатия токенов, текущие проблемы и перспективные направления исследований в этой области.'}, 'en': {'title': 'Token Compression: The Key to Efficient AI', 'desc': 'This paper discusses the shift in focus from increasing the size of language models to improving efficiency through data-centric methods, specifically token compression. As models grow larger, the computational cost of processing long sequences of tokens becomes a significant bottleneck. The authors propose that reducing the number of tokens used during training and inference can enhance AI performance without solely relying on larger models. They provide a comprehensive review of token compression techniques, their benefits, and the challenges that remain, aiming to inspire future research in this area.'}, 'zh': {'title': '令牌压缩：提升AI效率的新前沿', 'desc': '随着大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的快速发展，模型的规模不断扩大，参数数量从数百万增加到数百亿。然而，随着硬件限制的接近，计算瓶颈已转向自注意力机制在长序列上的二次成本，尤其是在处理超长文本、高分辨率图像和扩展视频时。本文提出，研究的重点应从以模型为中心的压缩转向以数据为中心的压缩，特别是令牌压缩被视为提高AI效率的新前沿。通过对令牌压缩的系统性回顾，我们分析了其基本优势和面临的挑战，并展望未来的发展方向。'}}}, {'id': 'https://huggingface.co/papers/2505.16348', 'title': 'Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance', 'url': 'https://huggingface.co/papers/2505.16348', 'abstract': "MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.  \t\t\t\t\tAI-generated summary \t\t\t\t Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO", 'score': 33, 'issue_id': 3969, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '2293dd9e12554028', 'authors': ['Taeyoon Kwon', 'Dongwook Choi', 'Sunghwan Kim', 'Hyojun Kim', 'Seungjun Moon', 'Beong-woo Kwak', 'Kuan-Hao Huang', 'Jinyoung Yeo'], 'affiliations': ['Texas A&M University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16348.jpg', 'data': {'categories': ['#agents', '#interpretability', '#multimodal', '#agi', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'Оценка памяти ИИ-ассистентов: путь к персонализации', 'desc': 'MEMENTO - это система оценки персонализированных воплощенных агентов, которая исследует их способность использовать память для оказания индивидуальной помощи. Фреймворк оценивает понимание агентами семантики объектов и пользовательских паттернов в задачах перестановки предметов. Эксперименты показали значительные ограничения в использовании памяти даже у передовых языковых моделей, таких как GPT-4. Результаты выявили 30.5% падение производительности при необходимости обращения к нескольким воспоминаниям, особенно в задачах, связанных с пользовательскими паттернами.'}, 'en': {'title': 'Enhancing Personalized Assistance in Embodied Agents through Memory Utilization', 'desc': 'MEMENTO is a framework that evaluates how well embodied agents use memory to provide personalized assistance. It highlights the challenges these agents face in understanding the unique meanings users assign to objects and their routines. The framework includes a two-stage evaluation process that measures how effectively agents can identify objects based on personal significance and infer user patterns. Experiments show that even advanced models like GPT-4o struggle with memory utilization, particularly when dealing with multiple memories, leading to a notable drop in performance.'}, 'zh': {'title': '个性化记忆利用的评估框架', 'desc': 'MEMENTO是一个评估个性化记忆利用的框架，专注于具身智能体在理解用户语义和日常习惯方面的局限性。尽管大型语言模型（LLMs）在家庭物品重排任务中表现出色，但这些任务主要是单轮交互，未能真实反映提供有意义帮助的挑战。为了提供个性化的帮助，具身智能体需要理解用户对物理世界的独特语义，并利用先前的交互历史来解释动态的现实指令。我们的实验表明，即使是最先进的模型，如GPT-4o，在需要参考多个记忆时，性能也会下降30.5%，这为未来开发更有效的个性化具身智能体提供了重要的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.20258', 'title': 'ARM: Adaptive Reasoning Model', 'url': 'https://huggingface.co/papers/2505.20258', 'abstract': 'Adaptive Reasoning Model (ARM) uses Ada-GRPO to reduce token usage and improve efficiency across different reasoning modes.  \t\t\t\t\tAI-generated summary \t\t\t\t While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the "overthinking" problem -- excessive and unnecessary reasoning -- which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones -- Direct Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a 2x speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens -- ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage.', 'score': 27, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'f56b53bb4f61d90d', 'authors': ['Siye Wu', 'Jian Xie', 'Yikai Zhang', 'Aili Chen', 'Kai Zhang', 'Yu Su', 'Yanghua Xiao'], 'affiliations': ['Fudan University', 'The Ohio State University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20258.jpg', 'data': {'categories': ['#reasoning', '#inference', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивные рассуждения для эффективного использования ресурсов ИИ', 'desc': 'Адаптивная модель рассуждений (ARM) использует Ada-GRPO для снижения использования токенов и повышения эффективности в различных режимах рассуждений. ARM способна адаптивно выбирать подходящие форматы рассуждений на основе поставленной задачи, включая три эффективных формата (прямой ответ, короткая цепочка мыслей и код) и более подробный формат длинной цепочки мыслей. Модель достигает высокой эффективности использования токенов, сокращая их в среднем на 30%, и до 70% в некоторых случаях, при сохранении производительности. ARM также поддерживает режимы с инструкциями и консенсусом для дополнительной гибкости.'}, 'en': {'title': 'Smart Reasoning: Less Tokens, More Efficiency!', 'desc': "The Adaptive Reasoning Model (ARM) introduces a novel approach to optimize reasoning in AI by dynamically selecting reasoning formats based on task complexity. It addresses the common issue of excessive token usage, known as the 'overthinking' problem, by employing Ada-GRPO, a refined version of Group Relative Policy Optimization. ARM can utilize various reasoning formats, including Direct Answer and Short CoT, to enhance efficiency while maintaining performance levels similar to more complex methods like Long CoT. This model not only reduces token consumption by up to 70% but also accelerates training speed by two times, making it a significant advancement in autonomous AI reasoning."}, 'zh': {'title': '自适应推理，提升效率与性能', 'desc': '自适应推理模型（ARM）通过使用Ada-GRPO来减少令牌使用量，提高不同推理模式下的效率。大型推理模型在复杂任务上表现出色，但缺乏根据任务难度调整推理令牌使用的能力，导致过度推理的问题。ARM能够根据具体任务自适应选择合适的推理格式，包括直接回答、简短链式推理和代码等高效格式，以及更复杂的长链式推理。通过Ada-GRPO，ARM实现了高达70%的令牌节省，同时保持与仅依赖长链式推理的模型相当的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.19457', 'title': 'BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs', 'url': 'https://huggingface.co/papers/2505.19457', 'abstract': 'BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models excel in general tasks, yet assessing their reliability in logic-heavy, precision-critical domains like finance, law, and healthcare remains challenging. To address this, we introduce BizFinBench, the first benchmark specifically designed to evaluate LLMs in real-world financial applications. BizFinBench consists of 6,781 well-annotated queries in Chinese, spanning five dimensions: numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering, grouped into nine fine-grained categories. The benchmark includes both objective and subjective metrics. We also introduce IteraJudge, a novel LLM evaluation method that reduces bias when LLMs serve as evaluators in objective metrics. We benchmark 25 models, including both proprietary and open-source systems. Extensive experiments show that no model dominates across all tasks. Our evaluation reveals distinct capability patterns: (1) In Numerical Calculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while smaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning, proprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with open-source models trailing by up to 19.49 points; (3) In Information Extraction, the performance spread is the largest, with DeepSeek-R1 scoring 71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition, performance variance is minimal, with top models scoring between 39.16 and 50.00. We find that while current LLMs handle routine finance queries competently, they struggle with complex scenarios requiring cross-concept reasoning. BizFinBench offers a rigorous, business-aligned benchmark for future research. The code and dataset are available at https://github.com/HiThink-Research/BizFinBench.', 'score': 20, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'f565f3ceb53b6631', 'authors': ['Guilong Lu', 'Xuntao Guo', 'Rongjunchen Zhang', 'Wenqiao Zhu', 'Ji Liu'], 'affiliations': ['Harbin Institute of Technology', 'HiThink Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.19457.jpg', 'data': {'categories': ['#benchmark', '#science', '#reasoning', '#dataset', '#open_source'], 'emoji': '💹', 'ru': {'title': 'BizFinBench: Новый стандарт оценки LLM в финансах', 'desc': 'BizFinBench - это новый эталонный тест для оценки больших языковых моделей (LLM) в финансовых приложениях. Он состоит из 6781 хорошо аннотированного запроса на китайском языке, охватывающих пять измерений: числовые вычисления, рассуждения, извлечение информации, распознавание прогнозов и ответы на вопросы на основе знаний. Исследователи протестировали 25 моделей, включая проприетарные и открытые системы, и обнаружили, что ни одна модель не доминирует во всех задачах. Результаты показывают, что современные LLM хорошо справляются с рутинными финансовыми запросами, но испытывают трудности со сложными сценариями, требующими рассуждений между концепциями.'}, 'en': {'title': 'BizFinBench: Evaluating LLMs for Financial Precision', 'desc': 'BizFinBench is a specialized benchmark designed to evaluate large language models (LLMs) in financial applications, highlighting their performance across various tasks. It includes 6,781 annotated queries in Chinese, covering areas such as numerical calculation, reasoning, information extraction, prediction recognition, and knowledge-based question answering. The benchmark employs both objective and subjective metrics, and introduces IteraJudge to minimize bias in evaluations. Results show that no single model excels in all tasks, with distinct performance patterns observed among different models, particularly in complex reasoning scenarios.'}, 'zh': {'title': 'BizFinBench：金融应用中的语言模型评估新基准', 'desc': 'BizFinBench是一个专门用于评估大型语言模型在金融应用中的基准测试。它包含6781个经过良好标注的查询，涵盖了数值计算、推理、信息提取、预测识别和基于知识的问题回答等五个维度。通过对25个模型的评估，发现没有一个模型在所有任务中表现优异，且不同模型在各个任务中的能力模式各异。该基准测试为未来的研究提供了严格且与商业相关的评估标准。'}}}, {'id': 'https://huggingface.co/papers/2505.20259', 'title': 'Lifelong Safety Alignment for Language Models', 'url': 'https://huggingface.co/papers/2505.20259', 'abstract': "A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have made impressive progress, but their growing capabilities also expose them to highly flexible jailbreaking attacks designed to bypass safety alignment. While many existing defenses focus on known types of attacks, it is more critical to prepare LLMs for unseen attacks that may arise during deployment. To address this, we propose a lifelong safety alignment framework that enables LLMs to continuously adapt to new and evolving jailbreaking strategies. Our framework introduces a competitive setup between two components: a Meta-Attacker, trained to actively discover novel jailbreaking strategies, and a Defender, trained to resist them. To effectively warm up the Meta-Attacker, we first leverage the GPT-4o API to extract key insights from a large collection of jailbreak-related research papers. Through iterative training, the first iteration Meta-Attacker achieves a 73% attack success rate (ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks. Meanwhile, the Defender progressively improves its robustness and ultimately reduces the Meta-Attacker's success rate to just 7%, enabling safer and more reliable deployment of LLMs in open-ended environments. The code is available at https://github.com/sail-sg/LifelongSafetyAlignment.", 'score': 19, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'af43851ea2378c4b', 'authors': ['Haoyu Wang', 'Zeyu Qin', 'Yifei Zhao', 'Chao Du', 'Min Lin', 'Xueqian Wang', 'Tianyu Pang'], 'affiliations': ['Sea AI Lab, Singapore', 'The Hong Kong University of Science and Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20259.jpg', 'data': {'categories': ['#inference', '#training', '#alignment', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Непрерывное обучение для защиты языковых моделей от новых атак', 'desc': 'Эта статья представляет систему непрерывного обучения для повышения безопасности языковых моделей (LLM) в условиях новых атак на их защиту. В основе системы лежит соревновательный процесс между Мета-Атакующим, который генерирует новые стратегии обхода защиты, и Защитником, который учится противостоять этим атакам. Исследователи использовали GPT-4 для анализа научных работ по взлому LLM и создания начальной версии Мета-Атакующего. В результате итеративного обучения Защитник смог снизить успешность атак с 73% до 7%, что значительно повышает надежность LLM при реальном применении.'}, 'en': {'title': 'Adapting LLMs for Unseen Threats: A Lifelong Safety Approach', 'desc': "This paper presents a lifecycle safety alignment framework designed to enhance the robustness of large language models (LLMs) against jailbreaking attacks. It introduces a competitive system involving a Meta-Attacker, which learns to identify new jailbreaking strategies, and a Defender, which evolves to counter these attacks. The framework emphasizes the importance of preparing LLMs for unforeseen threats rather than just known vulnerabilities. Through iterative training, the Defender significantly reduces the Meta-Attacker's success rate, ensuring safer deployment of LLMs in dynamic environments."}, 'zh': {'title': '提升LLMs安全性的生命周期对齐框架', 'desc': '本论文提出了一种生命周期安全对齐框架，旨在提高大型语言模型（LLMs）在面对新型越狱攻击时的鲁棒性。该框架通过引入一个元攻击者和一个防御者的竞争机制，使LLMs能够持续适应不断演变的越狱策略。元攻击者负责主动发现新的越狱策略，而防御者则致力于抵御这些攻击。通过迭代训练，防御者显著提高了对抗能力，将元攻击者的成功率降低到仅7%，从而实现了LLMs在开放环境中的更安全可靠部署。'}}}, {'id': 'https://huggingface.co/papers/2505.19914', 'title': 'Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles', 'url': 'https://huggingface.co/papers/2505.19914', 'abstract': "Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.", 'score': 18, 'issue_id': 3970, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '607e56f504f7f777', 'authors': ['Jiangjie Chen', 'Qianyu He', 'Siyu Yuan', 'Aili Chen', 'Zhicheng Cai', 'Weinan Dai', 'Hongli Yu', 'Qiying Yu', 'Xuefeng Li', 'Jiaze Chen', 'Hao Zhou', 'Mingxuan Wang'], 'affiliations': ['ByteDance Seed', 'Fudan University', 'Institute for AI Industry Research (AIR), Tsinghua University', 'Nanjing University', 'SIA-Lab of Tsinghua AIR and ByteDance Seed', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19914.jpg', 'data': {'categories': ['#reasoning', '#math', '#training', '#rl', '#optimization', '#benchmark', '#dataset'], 'emoji': '🧩', 'ru': {'title': 'Enigmata: прокачка логики языковых моделей через головоломки', 'desc': 'Enigmata - это комплексный набор инструментов для улучшения навыков решения головоломок у больших языковых моделей (LLM) с помощью масштабируемого многозадачного обучения с подкреплением. Он включает 36 задач в семи категориях, каждая с генератором примеров и верификатором для автоматической оценки. Модель Qwen2.5-32B-Enigmata, обученная с помощью Enigmata, превосходит аналоги на различных тестах по решению головоломок и математическим рассуждениям. Данный подход демонстрирует хорошую обобщаемость и предлагает унифицированную систему для улучшения логических рассуждений в LLM.'}, 'en': {'title': 'Enhancing Puzzle Reasoning in LLMs with Enigmata', 'desc': 'Enigmata is a novel framework designed to enhance Large Language Models (LLMs) in solving puzzles through scalable multi-task Reinforcement Learning (RL) training. It features a suite of 36 tasks across seven categories, each equipped with a generator for creating diverse examples and a rule-based verifier for automatic assessment. This setup allows for efficient training and evaluation, leading to improved performance on puzzle reasoning benchmarks and advanced math tasks. The results demonstrate that models trained with Enigmata, like Qwen2.5-32B-Enigmata, outperform existing models and show strong generalization capabilities in various reasoning challenges.'}, 'zh': {'title': '提升解谜推理能力的统一框架', 'desc': 'Enigmata是一个全面的工具套件，旨在通过可扩展的多任务强化学习训练来提高大型语言模型（LLMs）在解谜推理方面的能力。该套件包含36个任务，分为七个类别，每个任务都有一个生成器，可以生成可控难度的无限示例，以及一个基于规则的验证器，用于自动评估。通过这种生成器-验证器设计，Enigmata支持可扩展的多任务强化学习训练，并实现了细致的分析和无缝的可验证奖励集成。经过训练的模型Qwen2.5-32B-Enigmata在解谜推理基准测试中表现优异，超越了其他模型，展示了Enigmata在逻辑推理方面的统一和可控框架。'}}}, {'id': 'https://huggingface.co/papers/2505.19815', 'title': 'Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective', 'url': 'https://huggingface.co/papers/2505.19815', 'abstract': "LLM reasoning is understood through a meta-learning framework, treating reasoning as pseudo-gradient descent and questions as individual tasks, which enhances generalization and provides practical insights for improvement.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a novel framework for comprehending the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.", 'score': 16, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '145614af2451e525', 'authors': ['Junnan Liu', 'Hongwei Liu', 'Linchen Xiao', 'Shudong Liu', 'Taolin Zhang', 'Zihan Ma', 'Songyang Zhang', 'Kai Chen'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2505.19815.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Рассуждения LLM как мета-обучение: новый взгляд на искусственный интеллект', 'desc': 'Данная статья представляет новый подход к пониманию способностей рассуждения больших языковых моделей (LLM) через призму мета-обучения. Авторы рассматривают траектории рассуждений как псевдо-градиентный спуск для параметров модели, проводя параллели между рассуждениями LLM и различными парадигмами мета-обучения. Процесс обучения формализуется как мета-обучение, где каждый вопрос представляет собой отдельную задачу. Эмпирические исследования подтверждают сильную связь между рассуждениями LLM и мета-обучением, что открывает новые возможности для улучшения этих моделей.'}, 'en': {'title': 'Unlocking LLM Reasoning through Meta-Learning', 'desc': 'This paper introduces a new way to understand how large language models (LLMs) reason by using a meta-learning framework. It treats reasoning as a process similar to pseudo-gradient descent, where each question is seen as a separate task. By training the LLM on a variety of questions, it learns to adapt its parameters effectively, improving its reasoning skills. The findings show a strong link between LLM reasoning and meta-learning, offering valuable strategies for enhancing model performance.'}, 'zh': {'title': '通过元学习提升LLM推理能力', 'desc': '本文提出了一种新的框架，通过元学习的视角来理解大型语言模型（LLM）的推理能力。我们将推理过程视为对LLM参数的伪梯度下降更新，将每个问题视为独立任务，从而增强模型的泛化能力。通过在多样化问题集上进行训练，LLM能够发展出基本的推理能力，并能推广到未见过的问题。我们的研究不仅加深了对LLM推理的理解，还提供了通过已建立的元学习技术来改进这些模型的实用见解。'}}}, {'id': 'https://huggingface.co/papers/2505.19209', 'title': 'MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis\n  Discovery via Hierarchical Search', 'url': 'https://huggingface.co/papers/2505.19209', 'abstract': "A method is proposed to generate detailed scientific hypotheses using LLMs by defining and optimizing a latent reward landscape, outperforming baselines in benchmark evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.", 'score': 16, 'issue_id': 3968, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 мая', 'en': 'May 25', 'zh': '5月25日'}, 'hash': 'cb70528ba0407554', 'authors': ['Zonglin Yang', 'Wanhao Liu', 'Ben Gao', 'Yujie Liu', 'Wei Li', 'Tong Xie', 'Lidong Bing', 'Wanli Ouyang', 'Erik Cambria', 'Dongzhan Zhou'], 'affiliations': ['MiroMind', 'Nanyang Technological University', 'National University of Singapore', 'Shanghai Artificial Intelligence Laboratory', 'University of New South Wales', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19209.jpg', 'data': {'categories': ['#science', '#rlhf', '#benchmark', '#multimodal', '#optimization'], 'emoji': '🧪', 'ru': {'title': 'Искусственный интеллект на страже научного прогресса: от идеи к эксперименту', 'desc': 'Предложен метод генерации детальных научных гипотез с использованием больших языковых моделей (LLM) путем определения и оптимизации латентного ландшафта вознаграждений. Метод основан на иерархическом поиске, который постепенно предлагает и интегрирует детали в гипотезу, продвигаясь от общих концепций к конкретным экспериментальным конфигурациям. Этот иерархический процесс сглаживает ландшафт вознаграждений и позволяет проводить более эффективную оптимизацию. Эмпирические оценки на новом эталоне экспертно-аннотированных детализированных гипотез из недавней химической литературы показывают, что предложенный метод стабильно превосходит сильные базовые линии.'}, 'en': {'title': 'Unlocking Detailed Scientific Hypotheses with LLMs', 'desc': "This paper presents a new method for generating detailed scientific hypotheses using large language models (LLMs). It defines the task of fine-grained scientific hypothesis discovery, which focuses on creating actionable hypotheses from broader research ideas. The authors frame this task as a combinatorial optimization problem and explore how to optimize the hypothesis generation process by leveraging LLMs' internal scoring mechanisms. Their hierarchical search method improves the quality of generated hypotheses, as demonstrated by superior performance on benchmark evaluations compared to existing methods."}, 'zh': {'title': '利用LLMs生成细粒度科学假设的创新方法', 'desc': '本文提出了一种利用大型语言模型（LLMs）生成详细科学假设的方法，通过定义和优化潜在奖励景观，超越了基准评估中的其他方法。我们首次正式定义了细粒度科学假设发现的任务，旨在从粗略的研究方向生成可实验操作的详细假设。我们将此任务视为组合优化问题，并探讨LLMs在最大化利用时解决该问题的能力上限。通过分层搜索方法，我们逐步提出并整合假设细节，从一般概念到具体实验配置，实验证明该方法在新基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2505.18675', 'title': 'Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual\n  Reasoning from Transit Maps', 'url': 'https://huggingface.co/papers/2505.18675', 'abstract': 'Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.', 'score': 16, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '61fe1af51f08d30f', 'authors': ['Sicheng Feng', 'Song Wang', 'Shuyi Ouyang', 'Lingdong Kong', 'Zikai Song', 'Jianke Zhu', 'Huan Wang', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18675.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#cv', '#open_source'], 'emoji': '🗺️', 'ru': {'title': 'ReasonMap: новый взгляд на визуальное мышление языковых моделей', 'desc': 'ReasonMap - это новый бенчмарк для оценки способностей мультимодальных языковых моделей к пониманию визуальной информации и пространственному мышлению. Он включает в себя карты общественного транспорта из 30 городов и 1008 пар вопросов-ответов. Исследование показало, что среди открытых моделей базовые версии превосходят версии с рассуждениями, а у закрытых моделей наблюдается обратная тенденция. Результаты также указывают на то, что для эффективного решения задач визуального мышления моделям по-прежнему необходимо подлинное визуальное восприятие.'}, 'en': {'title': 'Evaluating Visual Reasoning in Multimodal Models with ReasonMap', 'desc': "This paper introduces ReasonMap, a benchmark aimed at evaluating the fine-grained visual understanding and spatial reasoning capabilities of multimodal large language models (MLLMs). The benchmark includes high-resolution transit maps and a set of question-answer pairs to rigorously test the models' reasoning abilities. The study finds that base models often outperform reasoning variants in open-source settings, while closed-source models show the opposite trend. Additionally, the results indicate that masking visual inputs generally leads to decreased performance, highlighting the importance of genuine visual perception in complex reasoning tasks."}, 'zh': {'title': '细粒度视觉推理的新基准', 'desc': '多模态大型语言模型（MLLMs）在视觉任务上取得了显著进展，但在细粒度视觉理解的推理任务上仍然不足。为了解决这个问题，我们提出了ReasonMap，一个基准测试，旨在评估MLLMs的细粒度视觉理解和空间推理能力。ReasonMap包含来自13个国家30个城市的高分辨率交通地图，并设计了两级评估流程来准确评估答案的正确性和质量。我们的研究揭示了一个反直觉的模式：在开源模型中，基础模型的表现优于推理模型，而在闭源模型中则相反。'}}}, {'id': 'https://huggingface.co/papers/2505.18545', 'title': 'B-score: Detecting biases in large language models using response\n  history', 'url': 'https://huggingface.co/papers/2505.18545', 'abstract': 'Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.', 'score': 16, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '60113080c6881b99', 'authors': ['An Vo', 'Mohammad Reza Taesiri', 'Daeyoung Kim', 'Anh Totti Nguyen'], 'affiliations': ['Auburn University, USA', 'KAIST, South Korea', 'University of Alberta, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2505.18545.jpg', 'data': {'categories': ['#data', '#ethics', '#hallucinations', '#benchmark', '#rlhf'], 'emoji': '🤖', 'ru': {'title': 'Самокоррекция предвзятости в языковых моделях через многоэтапный диалог', 'desc': 'Исследование посвящено изучению предвзятости в больших языковых моделях (LLM) и возможности ее уменьшения в многоэтапном диалоге. Авторы разработали набор вопросов по 9 темам, включая субъективные, случайные и объективные типы. Результаты показывают, что LLM способны снижать предвзятость при ответах на вопросы, требующие случайного, непредвзятого ответа. Предложен новый метрический показатель B-score для обнаружения предвзятости, который улучшает точность верификации ответов LLM на различных наборах данных.'}, 'en': {'title': 'De-biasing LLMs Through Multi-Turn Conversations', 'desc': "This paper explores how large language models (LLMs) can reduce their biases during multi-turn conversations by referencing their previous answers. The authors categorize questions into three types: Subjective, Random, and Objective, and find that LLMs can effectively 'de-bias' themselves when responding to Random questions. They introduce a new metric called B-score, which helps identify biases in LLM responses across various question types. The results show that using B-score enhances the accuracy of verifying LLM answers compared to traditional methods like confidence scores."}, 'zh': {'title': '多轮对话中的去偏见能力', 'desc': '大型语言模型（LLMs）常常表现出明显的偏见，例如对女性的偏见或对数字7的偏好。我们研究了在多轮对话中，LLMs是否能够在观察到自己之前的回答后，输出更少偏见的答案。我们提出了一组涵盖9个主题的测试问题，分为主观、随机和客观三种类型，以了解哪些问题更容易引发偏见。结果表明，LLMs在面对寻求随机、不偏见答案的问题时，能够自我“去偏见”，并且我们提出的B-score指标在检测偏见方面表现出色，显著提高了LLM答案的验证准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.18536', 'title': 'Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal\n  Large Language Models', 'url': 'https://huggingface.co/papers/2505.18536', 'abstract': 'Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.', 'score': 14, 'issue_id': 3967, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': '03c065b99c9707fe', 'authors': ['Haoyuan Sun', 'Jiaqi Wu', 'Bo Xia', 'Yifu Luo', 'Yifei Zhao', 'Kai Qin', 'Xufei Lv', 'Tiantian Zhang', 'Yongzhe Chang', 'Xueqian Wang'], 'affiliations': ['Tsinghua Shenzhen International Graduate School, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18536.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#benchmark', '#rlhf', '#agi', '#rl'], 'emoji': '🧠', 'ru': {'title': 'RFT: ключ к усилению рассуждений в мультимодальных ИИ-моделях', 'desc': 'Эта статья посвящена применению метода тонкой настройки с подкреплением (RFT) для улучшения способности мультимодальных больших языковых моделей (MLLM) к рассуждению. Авторы утверждают, что RFT значительно усиливает возможности MLLM в области рассуждений. В работе подробно рассматриваются пять ключевых аспектов улучшения MLLM с помощью RFT, включая разнообразие модальностей и задач, совершенствование алгоритмов обучения и создание новых бенчмарков. Статья также предлагает пять перспективных направлений для будущих исследований в этой области.'}, 'en': {'title': 'Reinforcement Fine-Tuning: Powering Reasoning in Multimodal AI', 'desc': 'This paper discusses the role of reinforcement fine-tuning (RFT) in improving the reasoning abilities of multimodal large language models (MLLMs). It highlights how RFT has contributed to the development of advanced AI models and emphasizes its importance in enhancing reasoning across various tasks and domains. The authors summarize five key improvements brought by RFT, including better training algorithms and diverse benchmarks. They also suggest future research directions to further explore the potential of RFT in the quest for Artificial General Intelligence (AGI).'}, 'zh': {'title': '强化微调：推动多模态模型推理能力的关键', 'desc': '在2025年，强化微调（RFT）在提升大型语言模型（LLMs）的推理能力方面展现出显著潜力，并推动了如OpenAI-o1和DeepSeek-R1等前沿AI模型的发展。RFT在多模态大型语言模型（MLLMs）中的高效应用引起了广泛关注。本文详细介绍了研究者应了解的基础知识，并总结了RFT在提升MLLM推理能力方面的五个关键改进点。最后，我们提出了五个未来研究的有前景方向，以期为AGI的进步提供有价值的见解。'}}}, {'id': 'https://huggingface.co/papers/2505.18601', 'title': 'Flex-Judge: Think Once, Judge Anywhere', 'url': 'https://huggingface.co/papers/2505.18601', 'abstract': 'Flex-Judge uses minimal textual reasoning data to generalize across multiple modalities and evaluation formats, outperforming state-of-the-art models in multimodal evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.', 'score': 13, 'issue_id': 3968, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'a0fd5b19cac44ab0', 'authors': ['Jongwoo Ko', 'Sungnyun Kim', 'Sungwoo Cho', 'Se-Young Yun'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18601.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#alignment', '#benchmark', '#multimodal', '#transfer_learning'], 'emoji': '🧠', 'ru': {'title': 'Рассуждения на основе текста - ключ к универсальной мультимодальной оценке', 'desc': 'Flex-Judge - это модель оценки мультимодальных данных, использующая минимальный объем текстовых данных для обучения. Она применяет структурированные текстовые объяснения для обобщения на различные модальности, включая изображения и видео. Flex-Judge превосходит современные коммерческие API и мультимодальные оценщики, обученные на больших объемах данных. Модель демонстрирует особую эффективность в областях с ограниченными ресурсами, таких как молекулярное моделирование.'}, 'en': {'title': 'Flex-Judge: Efficient Multimodal Evaluation with Minimal Data', 'desc': 'Flex-Judge is a novel multimodal evaluation model that utilizes minimal textual reasoning data to effectively generalize across various modalities and evaluation formats. It addresses the limitations of existing models that require extensive, modality-specific training data and often struggle with diverse tasks. By leveraging structured textual reasoning, Flex-Judge captures generalizable decision-making patterns, allowing it to perform well even with limited training resources. Empirical results show that it outperforms state-of-the-art models, making it a valuable tool in scenarios with scarce evaluation benchmarks.'}, 'zh': {'title': 'Flex-Judge：用最少数据实现多模态评估的突破', 'desc': 'Flex-Judge是一种多模态评估模型，它利用最少的文本推理数据来实现跨多种模态和评估格式的泛化。该模型通过结构化的文本推理解释，捕捉到可转移的决策模式，从而在图像或视频等多模态判断中表现出色。实验结果表明，尽管Flex-Judge的训练数据远少于其他模型，但其性能仍然与最先进的商业API相当，甚至更优。该框架展示了基于推理的文本监督作为一种强大且经济的替代方案，推动了可扩展的多模态模型评估。'}}}, {'id': 'https://huggingface.co/papers/2505.19752', 'title': 'Discrete Markov Bridge', 'url': 'https://huggingface.co/papers/2505.19752', 'abstract': 'A novel framework, Discrete Markov Bridge, is introduced for discrete data modeling with Matrix Learning and Score Learning components, demonstrating superior performance compared to existing methods on Text8 and CIFAR-10 datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Discrete diffusion has recently emerged as a promising paradigm in discrete data modeling. However, existing methods typically rely on a fixed rate transition matrix during training, which not only limits the expressiveness of latent representations, a fundamental strength of variational methods, but also constrains the overall design space. To address these limitations, we propose Discrete Markov Bridge, a novel framework specifically designed for discrete representation learning. Our approach is built upon two key components: Matrix Learning and Score Learning. We conduct a rigorous theoretical analysis, establishing formal performance guarantees for Matrix Learning and proving the convergence of the overall framework. Furthermore, we analyze the space complexity of our method, addressing practical constraints identified in prior studies. Extensive empirical evaluations validate the effectiveness of the proposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO) of 1.38 on the Text8 dataset, outperforming established baselines. Moreover, the proposed model demonstrates competitive performance on the CIFAR-10 dataset, achieving results comparable to those obtained by image-specific generation approaches.', 'score': 11, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ffd4e97918d0f33f', 'authors': ['Hengli Li', 'Yuxuan Wang', 'Song-Chun Zhu', 'Ying Nian Wu', 'Zilong Zheng'], 'affiliations': ['Department of Automation, Tsinghua University', 'Institute of Artificial Intelligence, Peking University', 'NLCo Lab, Beijing Institute for General Artificial Intelligence', 'State Key Laboratory of General Artificial Intelligence', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.19752.jpg', 'data': {'categories': ['#training', '#data', '#benchmark', '#diffusion', '#dataset', '#optimization', '#architecture'], 'emoji': '🌉', 'ru': {'title': 'Дискретный марковский мост: новый подход к моделированию дискретных данных', 'desc': 'Представлен новый фреймворк Discrete Markov Bridge для моделирования дискретных данных с компонентами Matrix Learning и Score Learning. Этот подход решает ограничения существующих методов дискретной диффузии, которые обычно используют фиксированную матрицу переходов. Проведен теоретический анализ, устанавливающий гарантии производительности для Matrix Learning и доказывающий сходимость всего фреймворка. Эмпирические оценки подтверждают эффективность предложенного метода, который превосходит базовые показатели на наборах данных Text8 и CIFAR-10.'}, 'en': {'title': 'Revolutionizing Discrete Data Modeling with Discrete Markov Bridge', 'desc': 'The paper introduces a new framework called Discrete Markov Bridge for modeling discrete data, which includes two main components: Matrix Learning and Score Learning. This framework overcomes limitations of existing methods that use a fixed transition matrix, allowing for more expressive latent representations. The authors provide theoretical guarantees for the performance of Matrix Learning and demonstrate the convergence of the entire framework. Empirical results show that Discrete Markov Bridge outperforms previous methods on the Text8 dataset and achieves competitive results on CIFAR-10, indicating its effectiveness in discrete representation learning.'}, 'zh': {'title': '离散马尔可夫桥：提升离散数据建模的新框架', 'desc': '本文提出了一种新的框架，称为离散马尔可夫桥，专门用于离散数据建模。该框架结合了矩阵学习和评分学习两个关键组件，克服了现有方法在训练中依赖固定转移矩阵的局限性。通过严格的理论分析，本文为矩阵学习建立了正式的性能保证，并证明了整体框架的收敛性。实证评估表明，离散马尔可夫桥在Text8和CIFAR-10数据集上表现优越，超越了现有基准。'}}}, {'id': 'https://huggingface.co/papers/2505.20256', 'title': 'Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration', 'url': 'https://huggingface.co/papers/2505.20256', 'abstract': "An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.  \t\t\t\t\tAI-generated summary \t\t\t\t Long-horizon video-audio reasoning and fine-grained pixel understanding impose conflicting requirements on omnimodal models: dense temporal coverage demands many low-resolution frames, whereas precise grounding calls for high-resolution inputs. We tackle this trade-off with a two-system architecture: a Global Reasoning System selects informative keyframes and rewrites the task at low spatial cost, while a Detail Understanding System performs pixel-level grounding on the selected high-resolution snippets. Because ``optimal'' keyframe selection and reformulation are ambiguous and hard to supervise, we formulate them as a reinforcement learning (RL) problem and present Omni-R1, an end-to-end RL framework built on Group Relative Policy Optimization. Omni-R1 trains the Global Reasoning System through hierarchical rewards obtained via online collaboration with the Detail Understanding System, requiring only one epoch of RL on small task splits.   Experiments on two challenging benchmarks, namely Referring Audio-Visual Segmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show that Omni-R1 not only surpasses strong supervised baselines but also outperforms specialized state-of-the-art models, while substantially improving out-of-domain generalization and mitigating multimodal hallucination. Our results demonstrate the first successful application of RL to large-scale omnimodal reasoning and highlight a scalable path toward universally foundation models.", 'score': 10, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '5a9a5050221ec342', 'authors': ['Hao Zhong', 'Muzhi Zhu', 'Zongze Du', 'Zheng Huang', 'Canyu Zhao', 'Mingyu Liu', 'Wen Wang', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20256.jpg', 'data': {'categories': ['#rl', '#reasoning', '#video', '#hallucinations', '#benchmark', '#multimodal', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Умное разделение труда: глобальное рассуждение и детальный анализ в одной модели', 'desc': 'Omni-R1 - это комплексная система обучения с подкреплением для задач рассуждения по видео и аудио, а также детального анализа пикселей. Она состоит из двух подсистем: Системы глобального рассуждения, выбирающей ключевые кадры, и Системы детального понимания, выполняющей анализ на уровне пикселей. Обучение происходит с помощью иерархических наград, полученных через онлайн-взаимодействие двух подсистем. Omni-R1 превосходит специализированные модели на сложных бенчмарках и улучшает обобщение на новые домены.'}, 'en': {'title': 'Omni-R1: Bridging Global Reasoning and Detail Understanding in Reinforcement Learning', 'desc': 'The paper presents Omni-R1, an end-to-end reinforcement learning framework designed for complex tasks involving both video and audio reasoning. It addresses the challenge of balancing the need for low-resolution frames for temporal coverage with the requirement for high-resolution inputs for detailed understanding. By employing a two-system architecture, it utilizes a Global Reasoning System to select keyframes and a Detail Understanding System for pixel-level analysis. The framework demonstrates superior performance on benchmarks, outperforming existing models and enhancing generalization across different tasks.'}, 'zh': {'title': 'Omni-R1：强化学习驱动的全模态推理新突破', 'desc': '本文提出了一种名为Omni-R1的端到端强化学习框架，旨在解决长时间视频音频推理和细粒度像素理解任务中的矛盾需求。该框架结合了全局推理系统和细节理解系统，通过选择关键信息帧来降低空间成本，同时在高分辨率片段上进行像素级的基础定位。我们将关键信息帧的选择和重构视为强化学习问题，利用层次奖励机制进行训练。实验结果表明，Omni-R1在多个基准测试中表现优异，超越了强大的监督基线和专业的最先进模型。'}}}, {'id': 'https://huggingface.co/papers/2505.19949', 'title': 'Which Data Attributes Stimulate Math and Code Reasoning? An\n  Investigation via Influence Functions', 'url': 'https://huggingface.co/papers/2505.19949', 'abstract': "Influence functions are used to attribute LLMs' reasoning in math and coding to individual training elements, revealing cross-domain effects and enabling a reweighting strategy that improves model accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. However, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. To address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning. Based on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts LiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax.", 'score': 10, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '1cb8e6d994dcd7db', 'authors': ['Siqi Kou', 'Qingyuan Tian', 'Hanwen Xu', 'Zihao Zeng', 'Zhijie Deng'], 'affiliations': ['Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19949.jpg', 'data': {'categories': ['#training', '#reasoning', '#data', '#dataset', '#optimization', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Функции влияния раскрывают секреты обучения LLM математике и программированию', 'desc': 'Исследователи использовали функции влияния для анализа вклада отдельных элементов обучающей выборки в способности больших языковых моделей (LLM) решать задачи по математике и программированию. Было обнаружено, что сложные математические примеры улучшают рассуждения как в математике, так и в программировании, в то время как простые задачи по программированию наиболее эффективны для улучшения навыков кодирования. На основе этих выводов была разработана стратегия перевзвешивания данных, которая значительно повысила точность модели на тестах AIME24 и LiveCodeBench. Анализ также выявил, что исследовательское поведение на уровне последовательностей улучшает производительность рассуждений в обеих областях.'}, 'en': {'title': 'Unlocking LLM Reasoning: Influence Functions for Enhanced Accuracy', 'desc': "This paper explores how influence functions can help us understand the reasoning abilities of large language models (LLMs) in math and coding tasks. By attributing the model's performance to specific training examples, the authors reveal important cross-domain effects, such as how challenging math problems can enhance coding skills. They propose a new dataset reweighting strategy that improves model accuracy significantly by adjusting the difficulty of tasks. Additionally, the study highlights distinct patterns in how LLMs utilize language for reasoning in math versus coding, providing insights for better training data curation."}, 'zh': {'title': '利用影响函数提升大语言模型推理能力', 'desc': '本文探讨了影响函数在大语言模型（LLMs）中的应用，旨在将模型在数学和编程推理中的表现归因于具体的训练样本。通过这种方法，研究揭示了跨领域的影响，表明高难度的数学示例可以同时提升数学和编程推理能力。基于这些发现，提出了一种简单有效的数据集重加权策略，通过调整任务难度显著提高了模型的准确性。最后，细致的归因分析显示，序列级的探索行为在数学和编程推理中均有助于提升表现，而在不同任务中，标记级的影响模式也存在显著差异。'}}}, {'id': 'https://huggingface.co/papers/2505.19788', 'title': 'Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition', 'url': 'https://huggingface.co/papers/2505.19788', 'abstract': 'Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) are criticized for the excessively lengthy Chain-of-Thought (CoT) to derive the final answer, suffering from high first-token and overall latency. Typically, the CoT of LRMs mixes multiple thinking units; each unit attempts to produce a candidate answer to the original query. Hence, a natural idea to improve efficiency is to reduce the unit number. Yet, the fact that the thinking units in vanilla CoT cannot be explicitly managed renders doing so challenging. This paper introduces Multi-Turn Decomposition (MinD) to decode conventional CoT into a sequence of explicit, structured, and turn-wise interactions to bridge the gap. In MinD, the model provides a multi-turn response to the query, where each turn embraces a thinking unit and yields a corresponding answer. The subsequent turns can reflect, verify, revise, or explore alternative approaches to both the thinking and answer parts of earlier ones. This not only makes the answer delivered more swiftly, but also enables explicit controls over the iterative reasoning process (i.e., users may halt or continue at any turn). We follow a supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We first rephrase the outputs of an LRM into multi-turn formats by prompting another LLM, and then tune the LRM with such data. Observing that the tuned model tends to consume even more tokens than the original one (probably due to that the multi-turn formats introduce additional answer tokens), we advocate leveraging RL algorithms like GRPO to prioritize correct outputs with fewer turns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up to ~70% reduction in both output token usage and time to first token (TTFT), while maintaining competitive performance on reasoning benchmarks such as MATH-500, AIME24, AMC23, and GPQA-Diamond.', 'score': 9, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '95cddffa071ceb3b', 'authors': ['Zihao Zeng', 'Xuyao Huang', 'Boxiu Li', 'Hao Zhang', 'Zhijie Deng'], 'affiliations': ['RealAI', 'Shanghai Jiao Tong University', 'University of California, San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2505.19788.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#benchmark', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение по шагам: MinD оптимизирует работу LRM', 'desc': 'Статья представляет метод Multi-Turn Decomposition (MinD) для повышения эффективности больших моделей рассуждений (LRM). MinD разбивает цепочку рассуждений на структурированные пошаговые взаимодействия, что позволяет сократить использование токенов и задержку. Метод использует комбинацию обучения с учителем и обучения с подкреплением для реализации. Результаты показывают значительное сокращение использования токенов и времени до первого токена при сохранении производительности на различных тестах рассуждений.'}, 'en': {'title': 'Streamlining Reasoning with Multi-Turn Decomposition', 'desc': "This paper presents Multi-Turn Decomposition (MinD), a method that enhances the efficiency of Large Reasoning Models (LRMs) by breaking down complex reasoning tasks into smaller, manageable turns. Each turn focuses on a specific thinking unit, allowing the model to generate responses iteratively, which reduces the overall token usage and latency. By structuring the reasoning process, users can control the flow of interactions, enabling them to verify or revise answers at each step. The approach combines supervised fine-tuning and reinforcement learning to optimize the model's performance while significantly decreasing the time to first token and output tokens used."}, 'zh': {'title': '多轮分解：提升推理模型效率的关键', 'desc': '本文提出了一种名为多轮分解（MinD）的方法，旨在提高大型推理模型的效率。通过将思维链（CoT）分解为可管理的多个回合，MinD减少了令牌的使用和延迟，同时保持了模型的性能。每个回合都包含一个思维单元，能够反思、验证或修正之前的思考和答案，从而实现更快速的响应。该方法通过监督微调和强化学习的结合，优化了推理过程，使用户能够在任意回合中控制推理的继续或停止。'}}}, {'id': 'https://huggingface.co/papers/2505.19439', 'title': 'Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers', 'url': 'https://huggingface.co/papers/2505.19439', 'abstract': 'Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers.Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams , in other words, to unlock the capabilities it already possesses.', 'score': 9, 'issue_id': 3971, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'fc8a2a184c9655b1', 'authors': ['Rihui Xin', 'Han Liu', 'Zecheng Wang', 'Yupeng Zhang', 'Dianbo Sui', 'Xiaolin Hu', 'Bingning Wang'], 'affiliations': ['Baichuan Inc.', 'Harbin Institute of Technology', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19439.jpg', 'data': {'categories': ['#reasoning', '#math', '#rl', '#optimization', '#training'], 'emoji': '🧮', 'ru': {'title': 'Обучение ИИ математике без правильных ответов', 'desc': 'Исследование посвящено использованию формата и длины ответов в качестве вспомогательных сигналов для обучения больших языковых моделей (LLM) решению математических задач без необходимости в традиционных эталонных ответах. Авторы показывают, что функция вознаграждения, основанная только на правильности формата, может дать улучшения производительности, сравнимые со стандартным алгоритмом GRPO на ранних этапах. Разработанный подход GRPO, использующий суррогатные сигналы формата и длины, не только соответствует, но и превосходит производительность стандартного алгоритма GRPO в некоторых сценариях. Исследование раскрывает суть успеха подхода без использования меток: базовая модель подобна отличному ученику, которому просто нужно развить хорошие привычки ответов для достижения выдающихся результатов на экзаменах.'}, 'en': {'title': 'Unlocking LLMs: Training with Format and Length Signals', 'desc': 'This paper explores how to train Large Language Models (LLMs) for solving mathematical problems without relying on traditional ground truth answers. It introduces the idea of using format and length as surrogate signals to guide the training process. The research demonstrates that a reward function focused on format correctness can improve performance significantly, especially in the early training phases. By incorporating length-based rewards later on, the proposed GRPO approach not only matches but exceeds the performance of standard methods that depend on ground truth data, achieving notable accuracy on mathematical problem sets.'}, 'zh': {'title': '利用格式与长度提升数学问题解决能力', 'desc': '本研究探讨了如何利用格式和长度作为替代信号，训练大型语言模型（LLMs）解决数学问题，而无需传统的真实答案。我们发现，仅依靠格式正确性的奖励函数，在早期阶段可以获得与标准GRPO算法相当的性能提升。随着训练的深入，格式奖励的局限性显现，因此我们引入了基于长度的奖励。最终的GRPO方法利用格式-长度替代信号，在某些情况下不仅匹配了标准GRPO算法的表现，还超越了其在AIME2024上的表现，达到了40.0%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2505.20152', 'title': 'Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models', 'url': 'https://huggingface.co/papers/2505.20152', 'abstract': 'A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our strong negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further study the impact of different negative sample construction methods and the number of negative samples on the geometric reasoning performance of LMM, yielding fruitful conclusions. The code and dataset are available at https://github.com/THU-KEG/MMGeoLM.', 'score': 8, 'issue_id': 3969, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ee370e43a48e58a9', 'authors': ['Kai Sun', 'Yushi Bai', 'Zhen Yang', 'Jiajie Zhang', 'Ji Qi', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20152.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#training', '#reasoning', '#dataset', '#open_source'], 'emoji': '📐', 'ru': {'title': 'Прорыв в геометрическом мышлении ИИ через усовершенствованное контрастное обучение', 'desc': 'Статья представляет новый метод контрастного обучения с использованием сложных отрицательных примеров для улучшения геометрического рассуждения в мультимодальных моделях. Авторы предлагают комбинацию контрастного обучения на основе изображений и текста, используя генеративные и правило-ориентированные отрицательные примеры. Разработанная модель MMGeoLM значительно превосходит существующие открытые модели на трех эталонных тестах по геометрическому рассуждению. Исследование также анализирует влияние различных методов построения отрицательных примеров на производительность модели.'}, 'en': {'title': 'Enhancing Geometric Reasoning with Hard Negative Contrastive Learning', 'desc': "This paper introduces a new framework for hard negative contrastive learning that enhances geometric reasoning in Large Multimodal Models (LMMs). By utilizing contrastively trained visual encoders and generating hard negatives through perturbations and rule-based modifications, the framework improves the model's ability to solve geometric problems. The proposed model, MMGeoLM, demonstrates superior performance on geometric reasoning tasks compared to existing models, even rivaling larger closed-source models. The study also explores how different methods of constructing negative samples affect the model's reasoning capabilities, providing valuable insights for future research."}, 'zh': {'title': '提升几何推理的新对比学习框架', 'desc': '本文提出了一种新颖的困难负样本对比学习框架，以提高大型多模态模型在几何推理方面的表现。通过对比学习训练的视觉编码器在自然场景图像上取得了显著的效果，但在细致推理方面存在局限。我们的方法结合了基于图像的对比学习和基于文本的对比学习，利用生成的困难负样本和修改的几何描述来增强模型的几何理解能力。实验结果表明，我们的模型在几何推理基准测试中显著优于其他开源模型，展示了其强大的推理能力。'}}}, {'id': 'https://huggingface.co/papers/2505.19602', 'title': 'Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV\n  Cache Compression', 'url': 'https://huggingface.co/papers/2505.19602', 'abstract': 'ScaleKV compresses the KV cache in Visual Autoregressive models by differentiating drafters and refiners across transformer layers, reducing memory consumption while maintaining high fidelity.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual Autoregressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction approach, which yields substantial improvements in efficiency, scalability, and zero-shot generalization. Nevertheless, the coarse-to-fine methodology inherent in VAR results in exponential growth of the KV cache during inference, causing considerable memory consumption and computational redundancy. To address these bottlenecks, we introduce ScaleKV, a novel KV cache compression framework tailored for VAR architectures. ScaleKV leverages two critical observations: varying cache demands across transformer layers and distinct attention patterns at different scales. Based on these insights, ScaleKV categorizes transformer layers into two functional groups: drafters and refiners. Drafters exhibit dispersed attention across multiple scales, thereby requiring greater cache capacity. Conversely, refiners focus attention on the current token map to process local details, consequently necessitating substantially reduced cache capacity. ScaleKV optimizes the multi-scale inference pipeline by identifying scale-specific drafters and refiners, facilitating differentiated cache management tailored to each scale. Evaluation on the state-of-the-art text-to-image VAR model family, Infinity, demonstrates that our approach effectively reduces the required KV cache memory to 10% while preserving pixel-level fidelity.', 'score': 7, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '26a6b9a07958f010', 'authors': ['Kunjun Li', 'Zigeng Chen', 'Cheng-Yen Yang', 'Jenq-Neng Hwang'], 'affiliations': ['National University of Singapore', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.19602.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture'], 'emoji': '🗜️', 'ru': {'title': 'ScaleKV: Эффективное сжатие кэша для визуальных ИИ-моделей', 'desc': 'ScaleKV - это новый метод сжатия KV-кэша для визуальных авторегрессионных моделей. Он разделяет слои трансформера на драфтеры и рефайнеры, что позволяет значительно уменьшить потребление памяти. Драфтеры имеют рассеянное внимание и требуют больше кэша, а рефайнеры фокусируются на текущей карте токенов и нуждаются в меньшем объеме. Тестирование на модели Infinity показало сокращение необходимой памяти KV-кэша до 10% при сохранении высокого качества изображений.'}, 'en': {'title': 'Efficient Memory Management in Visual Autoregressive Models with ScaleKV', 'desc': 'ScaleKV is a new framework designed to compress the key-value (KV) cache in Visual Autoregressive (VAR) models, which helps reduce memory usage during inference. It identifies two types of transformer layers: drafters, which need more cache due to their broad attention across multiple scales, and refiners, which focus on local details and require less cache. By optimizing cache management based on these layer types, ScaleKV significantly lowers memory consumption while maintaining high-quality outputs. Tests on the Infinity model show that ScaleKV can cut KV cache memory usage by 90% without sacrificing pixel-level fidelity.'}, 'zh': {'title': 'ScaleKV：高效压缩视觉自回归模型的KV缓存', 'desc': 'ScaleKV是一种新颖的KV缓存压缩框架，专为视觉自回归模型设计。它通过区分变压器层中的草图生成器和精细化处理器，显著降低了内存消耗，同时保持了高保真度。该方法利用了不同层对缓存的需求差异和不同尺度下的注意力模式，从而优化了多尺度推理流程。实验结果表明，ScaleKV能够将所需的KV缓存内存减少到10%，而像素级的保真度得以保持。'}}}, {'id': 'https://huggingface.co/papers/2505.19590', 'title': 'Learning to Reason without External Rewards', 'url': 'https://huggingface.co/papers/2505.19590', 'abstract': "Intuitor, a Reinforcement Learning from Internal Feedback method, uses self-certainty as a reward signal to enable unsupervised learning of large language models, achieving performance comparable to GRPO on benchmarks and superior generalization.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor", 'score': 7, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'd847f0743c20b2e9', 'authors': ['Xuandong Zhao', 'Zhewei Kang', 'Aosong Feng', 'Sergey Levine', 'Dawn Song'], 'affiliations': ['UC Berkeley', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19590.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#rlhf', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Самообучение ИИ: внутренняя уверенность как двигатель прогресса', 'desc': 'Статья представляет метод обучения с подкреплением на основе внутренней обратной связи под названием Intuitor. Этот метод использует самоуверенность модели в качестве сигнала награды для обучения больших языковых моделей без внешнего надзора. Intuitor достигает производительности, сравнимой с методом GRPO на контрольных задачах, и демонстрирует превосходную обобщающую способность. Исследование показывает, что внутренние сигналы модели могут эффективно управлять обучением в различных доменах, предлагая масштабируемую альтернативу обучению с подкреплением с верифицируемыми наградами.'}, 'en': {'title': "Learning from Confidence: Intuitor's Unsupervised Approach to AI", 'desc': "The paper introduces Intuitor, a method for Reinforcement Learning from Internal Feedback (RLIF) that allows large language models (LLMs) to learn without external supervision. Instead of relying on costly labeled data, Intuitor uses self-certainty, which is the model's own confidence in its predictions, as a reward signal. This approach enables unsupervised learning and matches the performance of existing methods like Group Relative Policy Optimization (GRPO) on benchmarks. Additionally, Intuitor demonstrates better generalization to tasks outside its training domain, such as code generation, highlighting the potential of intrinsic signals for effective learning in AI systems."}, 'zh': {'title': '自我确定性驱动的无监督学习', 'desc': 'Intuitor是一种基于内部反馈的强化学习方法，利用自我确定性作为奖励信号，使大型语言模型能够进行无监督学习。该方法在数学基准测试中表现出与GRPO相当的性能，并在代码生成等领域任务上展现出更好的泛化能力。通过使用模型自身的信心分数作为唯一的奖励信号，Intuitor实现了完全无监督的学习。研究结果表明，内部模型信号可以有效推动跨领域的学习，为缺乏可验证奖励的自主AI系统提供了一种可扩展的替代方案。'}}}, {'id': 'https://huggingface.co/papers/2505.16972', 'title': 'From Tens of Hours to Tens of Thousands: Scaling Back-Translation for\n  Speech Recognition', 'url': 'https://huggingface.co/papers/2505.16972', 'abstract': 'Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.', 'score': 7, 'issue_id': 3967, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '784a648ae844599f', 'authors': ['Tianduo Wang', 'Lu Xu', 'Wei Lu', 'Shanbo Cheng'], 'affiliations': ['ByteDance Seed', 'StatNLP Research Group, Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.16972.jpg', 'data': {'categories': ['#data', '#multilingual', '#low_resource', '#dataset', '#audio', '#synthetic'], 'emoji': '🗣️', 'ru': {'title': 'Синтетическая речь открывает новые горизонты для многоязычного ASR', 'desc': 'Эта статья представляет метод Speech Back-Translation для улучшения многоязычных систем автоматического распознавания речи (ASR). Метод использует текстовые корпусы и модели text-to-speech (TTS) для генерации синтетической речи в больших объемах. Авторы показывают, что даже небольшое количество реальной транскрибированной речи позволяет обучить TTS модели для генерации качественных синтетических данных. Применение этого метода для дообучения модели Whisper-large-v3 на 500 000 часах синтетической речи на 10 языках привело к снижению ошибок транскрипции в среднем на 30%.'}, 'en': {'title': 'Scaling ASR with Synthetic Speech: Speech Back-Translation', 'desc': 'This paper presents a method called Speech Back-Translation to enhance Automatic Speech Recognition (ASR) systems for multiple languages, especially those with limited resources. By using text-to-speech (TTS) models, the authors convert large text corpora into synthetic speech, significantly increasing the amount of training data available. They show that even a small amount of real speech can train TTS models to produce high-quality synthetic speech at a much larger scale. The results indicate that this approach can improve ASR performance, achieving over 30% reduction in transcription errors across ten languages.'}, 'zh': {'title': '语音反向翻译：提升多语言ASR的有效利器', 'desc': '这篇论文介绍了一种名为语音反向翻译（Speech Back-Translation）的新方法，旨在改善多语言自动语音识别（ASR）模型。该方法通过将大规模文本语料库转换为合成语音，利用现成的文本到语音（TTS）模型，解决了资源有限语言的覆盖问题。研究表明，仅需数十小时的真实转录语音，就能有效训练TTS模型生成数百倍的合成语音，同时保持高质量。通过这种方法，我们生成了超过50万小时的合成语音，并在多语言ASR系统中实现了超过30%的转录错误率降低。'}}}, {'id': 'https://huggingface.co/papers/2505.13426', 'title': 'G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.13426', 'abstract': 'VLM-Gym addresses the "knowing-doing" gap in Vision-Language Models by training them in a diverse RL environment, leading to enhanced perception and reasoning abilities that surpass existing models in interactive games.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing\'\' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.', 'score': 7, 'issue_id': 3967, 'pub_date': '2025-05-19', 'pub_date_card': {'ru': '19 мая', 'en': 'May 19', 'zh': '5月19日'}, 'hash': '8cfc8c8f1a7e5589', 'authors': ['Liang Chen', 'Hongcheng Gao', 'Tianyu Liu', 'Zhiqi Huang', 'Flood Sung', 'Xinyu Zhou', 'Yuxin Wu', 'Baobao Chang'], 'affiliations': ['Moonshot AI', 'Peking University', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2505.13426.jpg', 'data': {'categories': ['#multimodal', '#training', '#reasoning', '#open_source', '#agents', '#games', '#optimization', '#rl'], 'emoji': '🎮', 'ru': {'title': 'Преодоление разрыва между знанием и действием в Vision-Language Models с помощью RL', 'desc': 'Статья представляет VLM-Gym - среду обучения с подкреплением для Vision-Language Models, направленную на преодоление разрыва между знанием и действием. VLM-Gym включает разнообразные визуальные игры с унифицированными интерфейсами и настраиваемой сложностью. Используя эту среду, авторы обучили модели G0 и G1, которые демонстрируют улучшенные способности восприятия и рассуждения. Результаты показывают, что модели G1 превосходят ведущие проприетарные модели в интерактивных играх.'}, 'en': {'title': 'Bridging the Knowing-Doing Gap in Vision-Language Models', 'desc': 'VLM-Gym is a new training environment designed to improve Vision-Language Models (VLMs) by bridging the gap between their knowledge and practical application in interactive games. Traditional VLMs excel in tasks involving text and images but struggle with decision-making in dynamic environments. By using reinforcement learning (RL) in a diverse set of visual games, VLM-Gym enables models to develop better perception and reasoning skills. The G1 models trained in this environment outperform existing models, demonstrating that enhanced perception and reasoning can support each other during training.'}, 'zh': {'title': 'VLM-Gym：提升视觉语言模型的决策能力', 'desc': 'VLM-Gym 是一个针对视觉语言模型（VLM）的强化学习环境，旨在解决它们在互动游戏中的决策能力不足的问题。通过在多样化的游戏环境中训练，VLM-Gym 提升了模型的感知和推理能力，使其在简单游戏中表现优于现有模型。我们开发了 G0 和 G1 模型，其中 G1 模型在强化学习微调之前进行了感知增强，以应对游戏多样性带来的挑战。最终，G1 模型在所有游戏中均超越了其教师模型，并且在性能上超过了领先的专有模型。'}}}, {'id': 'https://huggingface.co/papers/2505.18759', 'title': 'The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation', 'url': 'https://huggingface.co/papers/2505.18759', 'abstract': 'Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.', 'score': 6, 'issue_id': 3971, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'eae631a1cc27fbf9', 'authors': ['Ruichen Zhang', 'Rana Muhammad Shahroz Khan', 'Zhen Tan', 'Dawei Li', 'Song Wang', 'Tianlong Chen'], 'affiliations': ['Arizona State University', 'University of North Carolina at Chapel Hill', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.18759.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#transfer_learning', '#dataset', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Оптимизация дистилляции языковых моделей через манипуляции с данными', 'desc': 'Эта статья представляет DC-CoT - первый дата-центричный бенчмарк для оценки методов дистилляции больших языковых моделей (LLM) с использованием цепочек рассуждений (Chain-of-Thought, CoT). Исследователи изучают влияние различных манипуляций с данными на производительность студенческих моделей, используя разные учительские модели и архитектуры. Особое внимание уделяется обобщению на распределенных и внераспределенных данных, а также переносу знаний между доменами. Результаты исследования направлены на оптимизацию процесса дистилляции CoT и создание более доступных моделей с сильными способностями к рассуждению.'}, 'en': {'title': 'Optimizing Reasoning in Smaller Models with Data-Centric Distillation', 'desc': 'This paper presents DC-CoT, a new benchmark for evaluating data-centric distillation methods in training smaller student Large Language Models (LLMs) while maintaining their reasoning capabilities. It explores various data manipulation techniques, such as augmentation, selection, and mixing, to understand their effects on model performance. The study uses different teacher and student model architectures to assess how these techniques influence both in-distribution and out-of-distribution generalization. The results aim to guide best practices for optimizing chain-of-thought distillation, making advanced reasoning models more efficient and accessible.'}, 'zh': {'title': '数据驱动的蒸馏优化之路', 'desc': '本文提出了一种名为DC-CoT的数据中心基准，旨在系统评估数据增强、选择和混合对链式推理（CoT）蒸馏的影响。通过使用多种教师模型和学生架构，我们对数据操作对学生模型性能的影响进行了严格评估，重点关注在分布内和分布外的泛化能力。研究结果为优化CoT蒸馏提供了可行的见解和最佳实践，促进了更高效的推理模型的开发。该数据集和代码可在指定链接中找到。'}}}, {'id': 'https://huggingface.co/papers/2505.19427', 'title': 'WINA: Weight Informed Neuron Activation for Accelerating Large Language\n  Model Inference', 'url': 'https://huggingface.co/papers/2505.19427', 'abstract': 'WINA, a training-free sparse activation framework for large language models, improves inference accuracy by considering hidden state magnitudes and weight matrix norms, outperforming existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise ell_2-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to 2.94% in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.', 'score': 5, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'b49e78670cfbb696', 'authors': ['Sihan Chen', 'Dan Zhao', 'Jongwoo Ko', 'Colby Banbury', 'Huiping Zhuang', 'Luming Liang', 'Tianyi Chen'], 'affiliations': ['Microsoft', 'New York University', 'Renmin University of China', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.19427.jpg', 'data': {'categories': ['#inference', '#training', '#open_source', '#optimization', '#architecture'], 'emoji': '🚀', 'ru': {'title': 'WINA: Эффективная разреженная активация для ускорения языковых моделей', 'desc': 'WINA - это новый метод разреженной активации для больших языковых моделей, не требующий дополнительного обучения. Он улучшает точность вывода, учитывая как величины скрытых состояний, так и нормы весовых матриц. WINA превосходит существующие методы, обеспечивая оптимальные границы ошибки аппроксимации с теоретическими гарантиями. Эмпирически WINA показывает улучшение производительности до 2.94% по сравнению с современными методами при том же уровне разреженности.'}, 'en': {'title': 'WINA: Efficient Inference with Training-Free Sparse Activation', 'desc': 'WINA is a new framework designed for sparse activation in large language models that does not require training. It improves inference accuracy by considering both the magnitudes of hidden states and the norms of weight matrices, which helps reduce errors in activation decisions. This approach allows for better resource efficiency and broader applicability compared to existing methods that rely only on hidden state magnitudes. Empirical results show that WINA outperforms current state-of-the-art techniques, making it a significant advancement in efficient inference for large language models.'}, 'zh': {'title': 'WINA：无训练的稀疏激活新突破', 'desc': 'WINA是一种无训练的稀疏激活框架，旨在提高大型语言模型的推理准确性。它通过同时考虑隐藏状态的大小和权重矩阵的列向ell_2范数，优化了激活策略。与现有方法相比，WINA在相同稀疏度下的平均性能提高了最多2.94%。这一创新方法为无训练的稀疏激活提供了新的性能基准，推动了高效推理的发展。'}}}, {'id': 'https://huggingface.co/papers/2505.20254', 'title': 'Position: Mechanistic Interpretability Should Prioritize Feature\n  Consistency in SAEs', 'url': 'https://huggingface.co/papers/2505.20254', 'abstract': 'Prioritizing feature consistency in sparse autoencoders improves mechanistic interpretability of neural networks by ensuring reliable and interpretable features.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI.', 'score': 4, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '8f3f187fdcf8d056', 'authors': ['Xiangchen Song', 'Aashiq Muhamed', 'Yujia Zheng', 'Lingjing Kong', 'Zeyu Tang', 'Mona T. Diab', 'Virginia Smith', 'Kun Zhang'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20254.jpg', 'data': {'categories': ['#interpretability', '#training', '#math', '#architecture'], 'emoji': '🔍', 'ru': {'title': 'Стабильность признаков - ключ к надежной интерпретации нейросетей', 'desc': 'Статья рассматривает важность стабильности признаков в разреженных автоэнкодерах для улучшения механистической интерпретируемости нейронных сетей. Авторы предлагают использовать метрику PW-MCC для измерения согласованности признаков между разными запусками обучения. Исследование показывает, что высокая согласованность признаков достижима при правильном выборе архитектуры и сильно коррелирует с семантическим сходством объяснений признаков. Авторы призывают сообщество систематически измерять стабильность признаков для обеспечения надежного прогресса в механистической интерпретируемости.'}, 'en': {'title': 'Enhancing Interpretability through Feature Consistency in Sparse Autoencoders', 'desc': 'This paper discusses the importance of feature consistency in Sparse Autoencoders (SAEs) for improving mechanistic interpretability (MI) of neural networks. It highlights that inconsistent features across different training runs can hinder the reliability of MI research. The authors propose the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a metric to measure feature consistency, showing that high consistency levels can be achieved with the right architectural choices. They advocate for a shift in the research community to prioritize and measure feature consistency to enhance the interpretability of neural networks.'}, 'zh': {'title': '优先考虑特征一致性，提升神经网络的可解释性', 'desc': '稀疏自编码器（SAE）在机制可解释性（MI）中是一个重要工具，可以将神经网络的激活分解为可解释的特征。然而，不同训练运行中学习到的SAE特征不一致，影响了MI研究的可靠性和效率。本文主张在SAE中优先考虑特征一致性，以确保在独立运行中收敛到等效特征集。我们提出使用成对字典均值相关系数（PW-MCC）作为衡量一致性的实用指标，并展示在适当的架构选择下可以实现高水平的一致性。'}}}, {'id': 'https://huggingface.co/papers/2505.17652', 'title': 'Rethinking the Sampling Criteria in Reinforcement Learning for LLM\n  Reasoning: A Competence-Difficulty Alignment Perspective', 'url': 'https://huggingface.co/papers/2505.17652', 'abstract': "CDAS addresses low sample efficiency in reinforcement learning by aligning model competence with problem difficulty, improving both accuracy and efficiency in mathematical benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces Competence-Difficulty Alignment Sampling (CDAS), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.", 'score': 4, 'issue_id': 3968, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '84801a2fd0a3c281', 'authors': ['Deyang Kong', 'Qi Guo', 'Xiangyu Xi', 'Wei Wang', 'Jingang Wang', 'Xunliang Cai', 'Shikun Zhang', 'Wei Ye'], 'affiliations': ['Meituan Group, Beijing, China', 'National Engineering Research Center for Software Engineering, Peking University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17652.jpg', 'data': {'categories': ['#rl', '#training', '#reasoning', '#math', '#optimization'], 'emoji': '🎯', 'ru': {'title': 'Точное соответствие: CDAS повышает эффективность обучения с подкреплением', 'desc': 'CDAS (Competence-Difficulty Alignment Sampling) - это новый метод в области обучения с подкреплением, направленный на повышение эффективности обучения моделей машинного обучения. Он решает проблему низкой эффективности выборки путем согласования компетенции модели со сложностью задачи. CDAS использует агрегирование исторических расхождений производительности для точной оценки сложности задач. Экспериментальные результаты на математических тестах показывают значительное улучшение точности и эффективности по сравнению с базовыми методами.'}, 'en': {'title': 'Aligning Model Skills with Task Challenges for Better Learning Efficiency', 'desc': "CDAS, or Competence-Difficulty Alignment Sampling, improves the efficiency of reinforcement learning by aligning the model's abilities with the difficulty of the tasks it faces. Traditional methods struggle with unstable estimates of problem difficulty, leading to poor training outcomes. CDAS addresses this by using historical performance data to accurately assess problem difficulties and adaptively select tasks that match the model's current competence level. Experimental results demonstrate that CDAS significantly enhances both accuracy and speed in solving complex mathematical problems compared to existing strategies."}, 'zh': {'title': '能力与难度的完美对齐', 'desc': 'CDAS（能力-难度对齐采样）旨在解决强化学习中的低样本效率问题。它通过对问题难度的历史表现差异进行聚合，提供准确且稳定的难度估计。该方法能够根据模型当前的能力，自适应选择与之对齐的难度问题，从而提高学习效率。实验结果表明，CDAS在多个数学基准测试中显著提高了准确性和效率。'}}}, {'id': 'https://huggingface.co/papers/2505.10887', 'title': 'InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer\n  Interaction', 'url': 'https://huggingface.co/papers/2505.10887', 'abstract': 'InfantAgent-Next is a multimodal agent that integrates tool-based and vision models in a modular architecture to solve various benchmarks, including OSWorld, GAIA, and SWE-Bench.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces InfantAgent-Next, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at https://github.com/bin123apple/InfantAgent.', 'score': 3, 'issue_id': 3968, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 мая', 'en': 'May 16', 'zh': '5月16日'}, 'hash': 'ed2c69dbcbbae339', 'authors': ['Bin Lei', 'Weitai Kang', 'Zijian Zhang', 'Winson Chen', 'Xi Xie', 'Shan Zuo', 'Mimi Xie', 'Ali Payani', 'Mingyi Hong', 'Yan Yan', 'Caiwen Ding'], 'affiliations': ['Cisco Research', 'The University of Texas at San Antonio', 'University of Connecticut', 'University of Illinois Chicago', 'University of Minnesota'], 'pdf_title_img': 'assets/pdf/title_img/2505.10887.jpg', 'data': {'categories': ['#agi', '#open_source', '#benchmark', '#agents', '#multimodal', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Универсальный мультимодальный агент для решения разнообразных задач', 'desc': 'InfantAgent-Next - это мультимодальный агент, интегрирующий инструментальные и визуальные модели в модульную архитектуру. Он способен взаимодействовать с компьютерами, обрабатывая текст, изображения, аудио и видео. Агент демонстрирует высокую обобщающую способность, решая как чисто визуальные задачи (OSWorld), так и более общие или инструментально-интенсивные бенчмарки (GAIA, SWE-Bench). На бенчмарке OSWorld InfantAgent-Next достигает точности 7.27%, превосходя Claude-Computer-Use.'}, 'en': {'title': 'Empowering Multimodal Interaction with InfantAgent-Next', 'desc': "InfantAgent-Next is a versatile multimodal agent designed to handle various tasks by integrating both tool-based and vision models. It features a modular architecture that allows different models to work together efficiently, solving tasks step-by-step. This approach contrasts with traditional methods that rely on a single large model or rigid workflows. The agent's effectiveness is showcased through its performance on multiple benchmarks, achieving notable accuracy in real-world scenarios."}, 'zh': {'title': '多模态智能体的未来：InfantAgent-Next', 'desc': '本文介绍了InfantAgent-Next，这是一种多模态智能体，能够通过文本、图像、音频和视频与计算机进行交互。与现有方法不同，InfantAgent-Next在高度模块化的架构中整合了基于工具和纯视觉的智能体，使不同模型能够以逐步的方式协作解决解耦任务。我们的智能体在多个基准测试中表现出色，包括纯视觉的OSWorld和更复杂的GAIA与SWE-Bench，展示了其通用性。具体而言，我们在OSWorld上达到了7.27%的准确率，超过了Claude-Computer-Use。'}}}, {'id': 'https://huggingface.co/papers/2505.20278', 'title': 'The Coverage Principle: A Framework for Understanding Compositional\n  Generalization', 'url': 'https://huggingface.co/papers/2505.20278', 'abstract': 'Large language models excel at pattern matching, yet often fall short in systematic compositional generalization. We propose the coverage principle: a data-centric framework showing that models relying primarily on pattern matching for compositional tasks cannot reliably generalize beyond substituting fragments that yield identical results when used in the same contexts. We demonstrate that this framework has a strong predictive power for the generalization capabilities of Transformers. First, we derive and empirically confirm that the training data required for two-hop generalization grows at least quadratically with the token set size, and the training data efficiency does not improve with 20x parameter scaling. Second, for compositional tasks with path ambiguity where one variable affects the output through multiple computational paths, we show that Transformers learn context-dependent state representations that undermine both performance and interoperability. Third, Chain-of-Thought supervision improves training data efficiency for multi-hop tasks but still struggles with path ambiguity. Finally, we outline a mechanism-based taxonomy that distinguishes three ways neural networks can generalize: structure-based (bounded by coverage), property-based (leveraging algebraic invariances), and shared-operator (through function reuse). This conceptual lens contextualizes our results and highlights where new architectural ideas are needed to achieve systematic compositionally. Overall, the coverage principle provides a unified lens for understanding compositional reasoning, and underscores the need for fundamental architectural or training innovations to achieve truly systematic compositionality.', 'score': 2, 'issue_id': 3968, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'afca6c0a8de30a05', 'authors': ['Hoyeon Chang', 'Jinho Park', 'Hanseul Cho', 'Sohee Yang', 'Miyoung Ko', 'Hyeonbin Hwang', 'Seungpil Won', 'Dohaeng Lee', 'Youbin Ahn', 'Minjoon Seo'], 'affiliations': ['KAIST', 'LG AI Research', 'UCL'], 'pdf_title_img': 'assets/pdf/title_img/2505.20278.jpg', 'data': {'categories': ['#interpretability', '#training', '#reasoning', '#data', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Принцип покрытия: новый взгляд на композиционное мышление в ИИ', 'desc': 'Статья исследует ограничения больших языковых моделей в области систематической композиционной генерализации. Авторы предлагают принцип покрытия, показывающий, что модели, основанные на сопоставлении шаблонов, не могут надежно обобщать за пределы замены фрагментов с идентичными результатами в одинаковых контекстах. Исследование демонстрирует, что для двухэтапного обобщения требуется квадратичный рост обучающих данных, а увеличение параметров модели в 20 раз не улучшает эффективность обучения. Авторы также предлагают таксономию механизмов обобщения нейронных сетей, включающую структурное, свойственное и операторное обобщение.'}, 'en': {'title': 'Unlocking Systematic Compositionality in Language Models', 'desc': 'This paper introduces the coverage principle, which highlights the limitations of large language models in systematic compositional generalization. It shows that models relying on pattern matching struggle to generalize effectively when faced with tasks that require substituting different fragments. The authors demonstrate that the amount of training data needed for effective two-hop generalization increases significantly with the size of the token set, and that simply increasing model parameters does not enhance training efficiency. Additionally, they propose a taxonomy for understanding different types of generalization in neural networks, emphasizing the need for new architectural innovations to improve compositional reasoning.'}, 'zh': {'title': '覆盖原则：理解组合推理的统一视角', 'desc': '大型语言模型在模式匹配方面表现出色，但在系统性组合泛化方面常常不足。我们提出了覆盖原则：这是一个以数据为中心的框架，表明主要依赖模式匹配的模型在组合任务中无法可靠地泛化。我们证明了该框架对变换器的泛化能力具有强大的预测能力，并指出训练数据的需求随着标记集大小的增加而至少呈平方增长。最后，我们提出了一种基于机制的分类法，区分了神经网络的三种泛化方式，强调了实现真正系统性组合所需的新架构创新。'}}}, {'id': 'https://huggingface.co/papers/2505.19706', 'title': 'Error Typing for Smarter Rewards: Improving Process Reward Models with\n  Error-Aware Hierarchical Supervision', 'url': 'https://huggingface.co/papers/2505.19706', 'abstract': 'PathFinder-PRM, a hierarchical and error-aware Process Reward Model, improves mathematical problem-solving by fine-grained error classification and step correctness estimation, achieving state-of-the-art PRMScore with reduced data usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.', 'score': 2, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'e60e6086053e62d0', 'authors': ['Tej Deep Pala', 'Panshul Sharma', 'Amir Zadeh', 'Chuan Li', 'Soujanya Poria'], 'affiliations': ['Lambda Labs', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2505.19706.jpg', 'data': {'categories': ['#reasoning', '#training', '#math', '#hallucinations', '#dataset', '#optimization'], 'emoji': '🧮', 'ru': {'title': 'Точная навигация в математических рассуждениях с PathFinder-PRM', 'desc': 'PathFinder-PRM - это новая иерархическая модель вознаграждения процесса, учитывающая ошибки, для улучшения решения математических задач. Она использует детальную классификацию ошибок и оценку правильности каждого шага. PathFinder-PRM достигает наилучших результатов по метрике PRMScore, используя при этом в 3 раза меньше данных для обучения. Модель эффективно направляет языковые модели к построению согласованных решений математических задач.'}, 'en': {'title': 'PathFinder-PRM: Enhancing Math Problem-Solving with Fine-Grained Error Detection', 'desc': 'PathFinder-PRM is a new model designed to enhance mathematical problem-solving by focusing on detailed error classification and assessing the correctness of each step in the solution process. Unlike traditional Outcome Reward Models that only evaluate final answers, this model uses a hierarchical and error-aware approach to score intermediate steps, which helps guide the generation of coherent solutions. It was trained on a large dataset that includes fine-grained labels for errors, allowing it to achieve a state-of-the-art PRMScore while using significantly less data. The results show that this model not only improves error detection but also enhances overall performance in reward-guided reasoning tasks.'}, 'zh': {'title': '提升数学推理的错误感知模型', 'desc': 'PathFinder-PRM是一种层次化且具备错误感知的过程奖励模型，旨在通过细致的错误分类和步骤正确性估计来提升数学问题解决能力。该模型通过对每个步骤的数学错误和一致性错误进行分类，结合这些细致的信号来评估步骤的正确性。通过构建一个包含40万样本的数据集，PathFinder-PRM在PRMBench上达到了67.7的最新状态，使用的数据量比之前减少了三倍。研究结果表明，解耦的错误检测和奖励估计不仅提升了细粒度错误检测的能力，还显著改善了基于奖励的数学推理效率。'}}}, {'id': 'https://huggingface.co/papers/2505.19640', 'title': 'Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.19640', 'abstract': "A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.  \t\t\t\t\tAI-generated summary \t\t\t\t Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. We introduce a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, our approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, our method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to reveal several valuable insights into conditional reward modeling.", 'score': 2, 'issue_id': 3970, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '6fff32b94b547c0e', 'authors': ['Roy Xie', 'David Qiu', 'Deepak Gopinath', 'Dong Lin', 'Yanchao Sun', 'Chong Wang', 'Saloni Potdar', 'Bhuwan Dhingra'], 'affiliations': ['Apple', 'Duke University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19640.jpg', 'data': {'categories': ['#reasoning', '#math', '#rlhf', '#training', '#rl', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение ИИ: мысль и ответ в одном потоке', 'desc': 'Статья представляет новый подход к обучению больших языковых моделей (LLM) для решения многоэтапных задач рассуждения. Авторы предлагают использовать обучение с подкреплением (RL) для того, чтобы научить модели чередовать процессы мышления и ответа. Этот метод позволяет значительно сократить время до первого токена (TTFT) и повысить точность ответов. Эксперименты на пяти различных наборах данных показали улучшение производительности и способность к обобщению на сложных задачах рассуждения.'}, 'en': {'title': 'Reinforcement Learning Boosts Reasoning Efficiency in Language Models', 'desc': 'This paper presents a new training method for large language models (LLMs) that improves their ability to answer multi-hop questions efficiently. By using reinforcement learning (RL), the model learns to alternate between thinking and answering, which enhances its reasoning capabilities. The authors introduce a reward system that encourages the model to take correct intermediate steps during reasoning, leading to better performance. Experiments show that this approach significantly reduces the time taken to generate answers and improves accuracy on various reasoning tasks without needing additional tools.'}, 'zh': {'title': '强化学习提升语言模型推理效率', 'desc': '本文提出了一种基于强化学习的训练范式，以提高大型语言模型在多跳问题上的推理效率和性能。通过交替思考和回答，模型能够更有效地进行推理，减少了首次生成令牌的时间。我们引入了一种简单有效的基于规则的奖励机制，鼓励模型在推理过程中采取正确的中间步骤。实验结果表明，该方法在多个数据集上表现出显著的改进，且无需外部工具。'}}}, {'id': 'https://huggingface.co/papers/2505.19630', 'title': 'DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning\n  System for Multi-Turn Clinical Dialogue', 'url': 'https://huggingface.co/papers/2505.19630', 'abstract': 'Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL', 'score': 2, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': 'ab312c010a92062f', 'authors': ['Yichun Feng', 'Jiawei Wang', 'Lu Zhou', 'Yixue Li'], 'affiliations': ['Department of EEIS, University of Science and Technology of China', 'Guangzhou National Laboratory', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Shanghai Institute of Nutrition and Health, Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.19630.jpg', 'data': {'categories': ['#training', '#reasoning', '#science', '#healthcare', '#dataset', '#games', '#optimization', '#rl'], 'emoji': '🩺', 'ru': {'title': 'Умный виртуальный доктор: ИИ учится вести диалог с пациентом', 'desc': 'Статья представляет DoctorAgent-RL - систему на основе обучения с подкреплением для моделирования медицинских консультаций. Она использует мультиагентный подход, где агент-врач оптимизирует стратегию опроса пациента через многоэтапное взаимодействие. Система преодолевает ограничения существующих методов, позволяя динамически адаптировать сбор информации. Эксперименты показывают превосходство DoctorAgent-RL над другими моделями в многоэтапных рассуждениях и точности диагностики.'}, 'en': {'title': 'Revolutionizing Clinical Consultations with Reinforcement Learning', 'desc': "This paper introduces DoctorAgent-RL, a novel framework that enhances biomedical question answering by using reinforcement learning (RL) to improve multi-turn medical consultations. Unlike traditional systems that rely on static data, DoctorAgent-RL allows a doctor agent to adaptively optimize its questioning strategy through dynamic interactions with a patient agent. The framework is designed to intelligently extract relevant clinical information, addressing the limitations of vague patient descriptions. Additionally, the authors present MTMedDialog, a new dataset for simulating patient interactions, which supports the framework's effectiveness in real-world clinical settings."}, 'zh': {'title': '智能医疗咨询的新突破', 'desc': '大型语言模型（LLMs）在生物医学问答领域表现出色，但在实际临床咨询中的应用仍面临核心挑战。现有系统依赖单向信息传递模式，患者必须在一次性描述症状，导致模糊投诉时的诊断建议不够具体。传统的基于监督学习的多轮对话方法受限于静态数据驱动的范式，缺乏泛化能力，难以智能提取关键临床信息。为了解决这些问题，我们提出了DoctorAgent-RL，一个基于强化学习的多智能体协作框架，将医疗咨询建模为不确定性下的动态决策过程。'}}}, {'id': 'https://huggingface.co/papers/2505.19443', 'title': 'Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications\n  of Agentic AI', 'url': 'https://huggingface.co/papers/2505.19443', 'abstract': 'A review contrasts vibe coding and agentic coding paradigms, highlighting their differences in interaction, autonomy, and application areas in AI-assisted software development.  \t\t\t\t\tAI-generated summary \t\t\t\t This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.', 'score': 2, 'issue_id': 3967, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '04b7019c8d659b76', 'authors': ['Ranjan Sapkota', 'Konstantinos I. Roumeliotis', 'Manoj Karkee'], 'affiliations': ['Cornell University, Department of Biological and Environmental Engineering, USA', 'University of the Peloponnese, Department of Informatics and Telecommunications, Tripoli, Greece'], 'pdf_title_img': 'assets/pdf/title_img/2505.19443.jpg', 'data': {'categories': ['#survey', '#agents', '#architecture', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Вайб vs Агент: Новые горизонты ИИ-ассистированной разработки', 'desc': 'Эта статья представляет сравнительный анализ двух парадигм в разработке программного обеспечения с помощью ИИ: вайб-кодинга и агентного кодинга. Вайб-кодинг фокусируется на интуитивном взаимодействии человека с ИИ через диалоговые интерфейсы, поддерживая творческий процесс и экспериментирование. Агентный кодинг, напротив, обеспечивает автономную разработку с минимальным вмешательством человека, используя целеориентированных агентов. Авторы предлагают таксономию, охватывающую концептуальные основы, модели выполнения, механизмы обратной связи и стратегии отладки для обеих парадигм.'}, 'en': {'title': 'Harmonizing Vibe and Agentic Coding for AI Development', 'desc': 'This paper reviews two coding paradigms in AI-assisted software development: vibe coding and agentic coding. Vibe coding focuses on human interaction and creativity, using conversational prompts to aid in ideation and experimentation. In contrast, agentic coding allows for more autonomous development, where AI agents can plan and execute tasks with little human input. The authors propose a taxonomy to compare these paradigms and suggest that the future of AI software engineering will benefit from integrating both approaches for a more effective development process.'}, 'zh': {'title': '融合情感编码与自主编码的未来软件开发', 'desc': '本文对两种新兴的人工智能辅助软件开发范式进行了全面分析：情感编码和自主编码。情感编码强调通过基于提示的对话工作流程实现直观的人机交互，适合于创意探索和实验。而自主编码则通过目标驱动的智能体实现自主软件开发，能够在最小人类干预下进行规划、执行和测试。研究表明，成功的人工智能软件工程将依赖于将这两种范式的优势结合在一个以人为中心的开发生命周期中。'}}}, {'id': 'https://huggingface.co/papers/2505.18384', 'title': 'Dynamic Risk Assessments for Offensive Cybersecurity Agents', 'url': 'https://huggingface.co/papers/2505.18384', 'abstract': "Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.  \t\t\t\t\tAI-generated summary \t\t\t\t Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.", 'score': 2, 'issue_id': 3971, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'ef99d86a3934644e', 'authors': ['Boyi Wei', 'Benedikt Stroebl', 'Jiacen Xu', 'Joie Zhang', 'Zhou Li', 'Peter Henderson'], 'affiliations': ['Princeton University', 'University of California, Irvine'], 'pdf_title_img': 'assets/pdf/title_img/2505.18384.jpg', 'data': {'categories': ['#cybersecurity', '#agents', '#security'], 'emoji': '🛡️', 'ru': {'title': 'Динамическая оценка угроз ИИ в кибербезопасности', 'desc': 'Исследование показывает, что злоумышленники могут значительно улучшить способности фундаментальных моделей в области наступательной кибербезопасности, используя ограниченные вычислительные ресурсы. Авторы утверждают, что оценки рисков должны учитывать расширенную модель угроз в контексте кибербезопасности. Эксперименты демонстрируют, что даже с небольшим вычислительным бюджетом можно улучшить возможности агента по кибербезопасности более чем на 40% по сравнению с базовым уровнем. Результаты подчеркивают необходимость динамической оценки рисков кибербезопасности агентов на основе искусственного интеллекта.'}, 'en': {'title': 'Enhancing Cybersecurity: Adversaries Boost AI with Limited Resources', 'desc': "This paper discusses how adversaries can improve the capabilities of foundation models in offensive cybersecurity, even with limited computational resources. It highlights the importance of dynamic threat model assessments that consider the various options available to adversaries in real-world scenarios. The authors demonstrate that adversaries can enhance an agent's performance significantly, achieving over 40% improvement in cybersecurity tasks with minimal compute resources. This underscores the necessity for more comprehensive evaluations of cybersecurity risks associated with AI agents."}, 'zh': {'title': '动态评估对抗者在网络安全中的威胁', 'desc': '本论文探讨了对抗者如何在有限的计算资源下显著提升基础模型在进攻性网络安全中的能力。研究表明，基础模型在自动化编程方面的进步可能使其能够自动化危险的网络攻击操作。当前的模型审计往往未能考虑现实世界中对抗者的自由度，导致对网络安全风险的评估不足。我们建议在网络安全评估中应考虑扩展的威胁模型，以动态方式评估对抗者在不同环境下的能力。'}}}, {'id': 'https://huggingface.co/papers/2505.16312', 'title': 'EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning', 'url': 'https://huggingface.co/papers/2505.16312', 'abstract': 'EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) excel at complex reasoning through search algorithms, yet current strategies often suffer from massive token consumption due to redundant exploration of semantically equivalent steps. Existing semantic similarity methods struggle to accurately identify such equivalence in domain-specific contexts like mathematical reasoning. To address this, we propose EquivPruner, a simple yet effective approach that identifies and prunes semantically equivalent actions during LLM reasoning search. We also introduce MathEquiv, the first dataset we created for mathematical statement equivalence, which enables the training of a lightweight equivalence detector. Extensive experiments across various models and tasks demonstrate that EquivPruner significantly reduces token consumption, improving searching efficiency and often bolstering reasoning accuracy. For instance, when applied to Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1\\% while also improving accuracy. Our code is available at https://github.com/Lolo1222/EquivPruner.', 'score': 1, 'issue_id': 3972, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '6ae91cff5462f129', 'authors': ['Jiawei Liu', 'Qisi Chen', 'Jianshu Zhang', 'Quan Liu', 'Defu Lian'], 'affiliations': ['University of Science and Technology of China', 'iFLYTEK Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.16312.jpg', 'data': {'categories': ['#reasoning', '#data', '#dataset', '#optimization', '#training'], 'emoji': '✂️', 'ru': {'title': 'EquivPruner: умное сокращение для эффективных LLM', 'desc': 'EquivPruner - это новый подход к оптимизации работы больших языковых моделей (LLM) при решении задач, требующих рассуждений. Он уменьшает потребление токенов и повышает точность, отсекая семантически эквивалентные действия в процессе поиска решения. Для обучения легковесного детектора эквивалентности был создан датасет MathEquiv, содержащий примеры математической эквивалентности. Эксперименты показали значительное сокращение потребления токенов и улучшение эффективности поиска при использовании EquivPruner.'}, 'en': {'title': 'Streamlining Reasoning with EquivPruner: Less Tokens, More Accuracy!', 'desc': 'EquivPruner is a novel method designed to enhance the efficiency of Large Language Models (LLMs) by reducing unnecessary token usage during reasoning tasks. It achieves this by identifying and eliminating semantically equivalent actions in the search process, which helps streamline the reasoning flow. The paper introduces a new dataset called MathEquiv, specifically created to train a lightweight equivalence detector for mathematical statements. Experimental results show that EquivPruner can significantly lower token consumption while improving the accuracy of reasoning tasks, demonstrating its effectiveness in optimizing LLM performance.'}, 'zh': {'title': '修剪等价动作，提升推理效率', 'desc': 'EquivPruner是一种新方法，通过在大型语言模型（LLM）搜索中修剪语义等价的动作，减少了令牌消耗并提高了推理准确性。该方法利用了我们创建的MathEquiv数据集，专注于数学等价性，从而有效识别和去除冗余的搜索步骤。现有的语义相似性方法在特定领域（如数学推理）中难以准确识别等价性，而EquivPruner则提供了一个简单有效的解决方案。实验结果表明，EquivPruner在多个模型和任务中显著降低了令牌消耗，同时提高了推理的效率和准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.15957', 'title': 'Towards Holistic Evaluation of Large Audio-Language Models: A\n  Comprehensive Survey', 'url': 'https://huggingface.co/papers/2505.15957', 'abstract': "A survey proposes a systematic taxonomy for evaluating large audio-language models across dimensions including auditory awareness, knowledge reasoning, dialogue ability, and fairness, to address fragmented benchmarks in the field.  \t\t\t\t\tAI-generated summary \t\t\t\t With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.", 'score': 1, 'issue_id': 3968, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': 'ed5ebb17c81ad1e0', 'authors': ['Chih-Kai Yang', 'Neo S. Ho', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University'], 'pdf_title_img': 'assets/pdf/title_img/2505.15957.jpg', 'data': {'categories': ['#ethics', '#reasoning', '#audio', '#benchmark', '#multimodal', '#survey'], 'emoji': '🎧', 'ru': {'title': 'Структурированный подход к оценке аудио-языковых ИИ-моделей', 'desc': 'Статья представляет систематическую таксономию для оценки больших аудио-языковых моделей (LALM). Авторы выделяют четыре основных измерения: общее слуховое восприятие, рассуждения на основе знаний, диалоговые способности и справедливость. Это первый обзор, фокусирующийся именно на оценке LALM, что предоставляет четкие ориентиры для исследовательского сообщества. Работа направлена на преодоление фрагментированности существующих методов оценки таких моделей.'}, 'en': {'title': 'A Unified Framework for Evaluating Large Audio-Language Models', 'desc': 'This paper presents a structured approach to evaluate large audio-language models (LALMs) by proposing a systematic taxonomy. It identifies four key dimensions for assessment: auditory awareness, knowledge reasoning, dialogue ability, and fairness. The authors highlight the current fragmentation in benchmarks and aim to provide clarity and direction for future evaluations. This survey is the first of its kind, offering comprehensive insights and guidelines for researchers in the field of LALMs.'}, 'zh': {'title': '系统评估大型音频语言模型的分类法', 'desc': '这篇论文提出了一种系统的分类法，用于评估大型音频语言模型（LALMs）。评估维度包括听觉意识、知识推理、对话能力和公平性，以解决该领域基准测试的碎片化问题。通过对现有文献的全面调查，论文为LALM的评估提供了清晰的指导，并指出了未来的研究方向。此研究是首次专注于LALM评估的调查，为相关社区提供了重要的参考。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (5)', '#agi (3)', '#alignment (2)', '#architecture (7)', '#audio (2)', '#benchmark (14)', '#cv (1)', '#data (7)', '#dataset (10)', '#diffusion (1)', '#ethics (2)', '#games (2)', '#graphs', '#hallucinations (3)', '#healthcare (1)', '#inference (4)', '#interpretability (5)', '#leakage', '#long_context (1)', '#low_resource (1)', '#machine_translation', '#math (8)', '#multilingual (1)', '#multimodal (10)', '#open_source (6)', '#optimization (20)', '#plp', '#rag', '#reasoning (23)', '#rl (10)', '#rlhf (6)', '#robotics', '#science (3)', '#security (2)', '#small_models', '#story_generation', '#survey (3)', '#synthetic (1)', '#training (22)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-27 07:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-27 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-27 07:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    