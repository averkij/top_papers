
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. September 9.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-08.html">â¬…ï¸ <span id="prev-date">08.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-10.html">â¡ï¸ <span id="next-date">10.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '9 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 9', 'zh': '9æœˆ9æ—¥'};
        let feedDateNext = {'ru': '10.09', 'en': '09/10', 'zh': '9æœˆ10æ—¥'};
        let feedDatePrev = {'ru': '08.09', 'en': '09/08', 'zh': '9æœˆ8æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.06160', 'title': 'Reverse-Engineered Reasoning for Open-Ended Generation', 'url': 'https://huggingface.co/papers/2509.06160', 'abstract': "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t While the ``deep reasoning'' paradigm has spurred significant advances in verifiable domains like mathematics, its application to open-ended, creative generation remains a critical challenge. The two dominant methods for instilling reasoning -- reinforcement learning (RL) and instruction distillation -- falter in this area; RL struggles with the absence of clear reward signals and high-quality reward models, while distillation is prohibitively expensive and capped by the teacher model's capabilities. To overcome these limitations, we introduce REverse-Engineered Reasoning (REER), a new paradigm that fundamentally shifts the approach. Instead of building a reasoning process ``forwards'' through trial-and-error or imitation, REER works ``backwards'' from known-good solutions to computationally discover the latent, step-by-step deep reasoning process that could have produced them. Using this scalable, gradient-free approach, we curate and open-source DeepWriting-20K, a large-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks. Our model, DeepWriter-8B, trained on this data, not only surpasses strong open-source baselines but also achieves performance competitive with, and at times superior to, leading proprietary models like GPT-4o and Claude 3.5.", 'score': 99, 'issue_id': 5784, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 7', 'zh': '9æœˆ7æ—¥'}, 'hash': '4df21d6c69df73d5', 'authors': ['Haozhe Wang', 'Haoran Que', 'Qixin Xu', 'Minghao Liu', 'Wangchunshu Zhou', 'Jiazhan Feng', 'Wanjun Zhong', 'Wei Ye', 'Tong Yang', 'Wenhao Huang', 'Ge Zhang', 'Fangzhen Lin'], 'affiliations': ['ByteDance Seed', 'Hong Kong University of Science and Technology', 'M-A-P', 'Peking University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06160.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#architecture', '#training', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'REER (Reverse-Engineered Reasoning) - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ REER Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… DeepWriting-20K Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ DeepWriter-8B, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ñ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Unlocking Creativity with Reverse Engineering in Deep Reasoning', 'desc': 'REER introduces a novel approach to deep reasoning by utilizing reverse engineering to uncover the step-by-step processes behind successful solutions. This method addresses the limitations of traditional reinforcement learning and instruction distillation, which struggle with open-ended tasks due to unclear rewards and high costs. By working backwards from known solutions, REER enables the discovery of effective reasoning pathways without the need for extensive trial-and-error. The resulting model, DeepWriter-8B, demonstrates competitive performance against both open-source and proprietary models, showcasing the potential of this new paradigm in creative generation tasks.'}, 'zh': {'title': 'é€†å‘æ¨ç†ï¼Œå¼€å¯æ·±åº¦æ¨ç†æ–°çºªå…ƒ', 'desc': 'REERæ˜¯ä¸€ç§æ–°çš„æ·±åº¦æ¨ç†èŒƒå¼ï¼Œé€šè¿‡é€†å‘å·¥ç¨‹å‘ç°é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¼ ç»Ÿçš„æ¨ç†æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ å’ŒæŒ‡ä»¤è’¸é¦ï¼Œåœ¨å¤„ç†å¼€æ”¾æ€§ç”Ÿæˆæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå‰è€…ç¼ºä¹æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œåè€…æˆæœ¬é«˜æ˜‚ä¸”å—é™äºæ•™å¸ˆæ¨¡å‹çš„èƒ½åŠ›ã€‚REERé€šè¿‡ä»å·²çŸ¥çš„ä¼˜ç§€è§£å†³æ–¹æ¡ˆå‘åæ¨å¯¼ï¼Œè®¡ç®—æ€§åœ°å‘ç°æ½œåœ¨çš„é€æ­¥æ·±åº¦æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå…‹æœäº†è¿™äº›é™åˆ¶ã€‚æˆ‘ä»¬è¿˜å¼€æºäº†DeepWriting-20Kæ•°æ®é›†ï¼ŒåŒ…å«20,000ä¸ªæ·±åº¦æ¨ç†è½¨è¿¹ï¼Œè®­ç»ƒçš„DeepWriter-8Bæ¨¡å‹åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­è¶…è¶Šäº†å¼ºå¤§çš„å¼€æºåŸºçº¿ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹ä¸é¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒClaude 3.5çš„è¡¨ç°ç›¸å½“æˆ–æ›´ä¼˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06501', 'title': 'WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents', 'url': 'https://huggingface.co/papers/2509.06501', 'abstract': 'WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.', 'score': 55, 'issue_id': 5787, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'a4011c0eeba062d1', 'authors': ['Junteng Liu', 'Yunji Li', 'Chi Zhang', 'Jingyang Li', 'Aili Chen', 'Ke Ji', 'Weiyu Cheng', 'Zijia Wu', 'Chengyu Du', 'Qidi Xu', 'Jiayuan Song', 'Zhengmao Zhu', 'Wenhu Chen', 'Pengyu Zhao', 'Junxian He'], 'affiliations': ['The Hong Kong University of Science and Technology', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2509.06501.jpg', 'data': {'categories': ['#agents', '#training', '#reasoning', '#agi', '#dataset', '#rl', '#long_context'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'WebExplorer: ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'desc': 'WebExplorer - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ WebExplorer-8B Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ 128K Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ 100 Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸. ĞĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²ÑĞµĞ³Ğ¾ 8 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ², WebExplorer-8B Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'WebExplorer: Pioneering Advanced Web Agents for Information Seeking', 'desc': "WebExplorer is a novel approach that enhances web agents' ability to seek information effectively by generating challenging data through model-based exploration and iterative query evolution. It addresses the limitations of existing web agents, which often struggle with complex tasks due to a lack of robust data. By employing reinforcement learning and supervised fine-tuning, WebExplorer-8B demonstrates superior performance in information-seeking benchmarks, outperforming larger models in accuracy. This work paves the way for advanced web agents capable of long-horizon problem solving and complex web navigation."}, 'zh': {'title': 'WebExplorerï¼šä¿¡æ¯æ£€ç´¢çš„å…ˆè¿›ç½‘ç»œä»£ç†', 'desc': 'WebExploreræ˜¯ä¸€ç§åŸºäºæ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œç”¨äºå¼€å‘å…ˆè¿›çš„ç½‘ç»œä»£ç†ï¼Œèƒ½å¤Ÿåœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿçš„æ•°æ®ç”Ÿæˆå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè§£å†³äº†ç°æœ‰å¼€æºç½‘ç»œä»£ç†åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„ä¿¡æ¯æ£€ç´¢èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚WebExploreré€šè¿‡æ¨¡å‹é©±åŠ¨çš„æ¢ç´¢å’Œè¿­ä»£çš„æŸ¥è¯¢æ¼”å˜ï¼Œç”Ÿæˆéœ€è¦å¤šæ­¥æ¨ç†å’Œå¤æ‚ç½‘é¡µå¯¼èˆªçš„æŒ‘æˆ˜æ€§æŸ¥è¯¢-ç­”æ¡ˆå¯¹ã€‚æœ€ç»ˆï¼ŒWebExplorer-8Bæ¨¡å‹åœ¨å¤šç§ä¿¡æ¯æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨é•¿æ—¶é—´é—®é¢˜è§£å†³ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06949', 'title': 'Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models', 'url': 'https://huggingface.co/papers/2509.06949', 'abstract': 'TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL', 'score': 36, 'issue_id': 5783, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'f6f75969a4d93684', 'authors': ['Yinjie Wang', 'Ling Yang', 'Bowen Li', 'Ye Tian', 'Ke Shen', 'Mengdi Wang'], 'affiliations': ['Princeton University', 'University of Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2509.06949.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#training', '#math', '#rl', '#open_source', '#diffusion'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'TraceRL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ğ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ ÑÑĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ¡ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ TraceRL Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ÑĞµÑ€Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ TraDo, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Language Models with Trajectory-Aware Reinforcement Learning', 'desc': 'TraceRL is a new framework that improves diffusion language models (DLMs) by using trajectory-aware reinforcement learning. This method helps the models perform better on complex reasoning tasks, such as math and coding, by incorporating preferred inference paths during training. The framework also allows for better sampling flexibility and can adapt smaller models to larger ones. With TraceRL, the resulting models, like TraDo, achieve significant accuracy improvements over existing models, demonstrating their effectiveness in challenging reasoning benchmarks.'}, 'zh': {'title': 'TraceRLï¼šæå‡æ¨ç†æ€§èƒ½çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ', 'desc': 'TraceRLæ˜¯ä¸€ç§é’ˆå¯¹æ‰©æ•£è¯­è¨€æ¨¡å‹çš„è½¨è¿¹æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨åæœŸè®­ç»ƒä¸­èå…¥ä¼˜å…ˆæ¨ç†è½¨è¿¹ï¼Œé€‚ç”¨äºä¸åŒçš„æ¨¡å‹æ¶æ„ã€‚è¯¥æ¡†æ¶é…å¤‡äº†åŸºäºæ‰©æ•£çš„ä»·å€¼æ¨¡å‹ï¼Œæå‡äº†è®­ç»ƒçš„ç¨³å®šæ€§ï¼Œå¹¶åœ¨å¤æ‚çš„æ•°å­¦å’Œç¼–ç¨‹ä»»åŠ¡ä¸Šå±•ç¤ºäº†æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒTraceRLè¿˜å¯ä»¥å°†ç‰¹å®šå—çš„æ¨¡å‹é€‚åº”åˆ°æ›´å¤§çš„å—ï¼Œä»è€Œæé«˜é‡‡æ ·çš„çµæ´»æ€§ã€‚é€šè¿‡ä½¿ç”¨TraceRLï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„æ‰©æ•£è¯­è¨€æ¨¡å‹TraDoï¼Œå°½ç®¡å…¶è§„æ¨¡å°äº7Bçš„è‡ªå›å½’æ¨¡å‹ï¼Œä½†åœ¨å¤æ‚çš„æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä»ç„¶è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06467', 'title': 'Does DINOv3 Set a New Medical Vision Standard?', 'url': 'https://huggingface.co/papers/2509.06467', 'abstract': "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.  \t\t\t\t\tAI-generated summary \t\t\t\t The advent of large-scale vision foundation models, pre-trained on diverse natural images, has marked a paradigm shift in computer vision. However, how the frontier vision foundation models' efficacies transfer to specialized domains remains such as medical imaging remains an open question. This report investigates whether DINOv3, a state-of-the-art self-supervised vision transformer (ViT) that features strong capability in dense prediction tasks, can directly serve as a powerful, unified encoder for medical vision tasks without domain-specific pre-training. To answer this, we benchmark DINOv3 across common medical vision tasks, including 2D/3D classification and segmentation on a wide range of medical imaging modalities. We systematically analyze its scalability by varying model sizes and input image resolutions. Our findings reveal that DINOv3 shows impressive performance and establishes a formidable new baseline. Remarkably, it can even outperform medical-specific foundation models like BiomedCLIP and CT-Net on several tasks, despite being trained solely on natural images. However, we identify clear limitations: The model's features degrade in scenarios requiring deep domain specialization, such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM), and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3 does not consistently obey scaling law in the medical domain; performance does not reliably increase with larger models or finer feature resolutions, showing diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3 as a strong baseline, whose powerful visual features can serve as a robust prior for multiple complex medical tasks. This opens promising future directions, such as leveraging its features to enforce multiview consistency in 3D reconstruction.", 'score': 27, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '05928cb3cc52644d', 'authors': ['Che Liu', 'Yinda Chen', 'Haoyuan Shi', 'Jinpeng Lu', 'Bailiang Jian', 'Jiazhen Pan', 'Linghan Cai', 'Jiayi Wang', 'Yundi Zhang', 'Jun Li', 'Cosmin I. Bercea', 'Cheng Ouyang', 'Chen Chen', 'Zhiwei Xiong', 'Benedikt Wiestler', 'Christian Wachinger', 'Daniel Rueckert', 'Wenjia Bai', 'Rossella Arcucci'], 'affiliations': ['Dresden University of Technology', 'Helmholtz AI and Helmholtz Munich', 'Imperial College London', 'Munich Center for Machine Learning', 'Technical University of Munich (TUM)', 'University of Erlangen-Nuremberg', 'University of Oxford', 'University of Science and Technology of China', 'University of Sheffield'], 'pdf_title_img': 'assets/pdf/title_img/2509.06467.jpg', 'data': {'categories': ['#transfer_learning', '#3d', '#cv', '#healthcare', '#benchmark', '#optimization', '#science'], 'emoji': 'ğŸ©º', 'ru': {'title': 'DINOv3: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'ĞœĞ¾Ğ´ĞµĞ»ÑŒ DINOv3, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Vision Transformer, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ DINOv3 Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾Ğ¹Ñ‚Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸Ñ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, DINOv3 Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ·Ğ°ĞºĞ¾Ğ½Ğ°Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸Ğ»Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'DINOv3: A Powerful Vision Transformer for Medical Imaging Tasks', 'desc': 'DINOv3 is a self-supervised vision transformer that excels in various medical imaging tasks without needing pre-training on medical data. It has been benchmarked against common tasks like classification and segmentation, showing strong performance and even surpassing some specialized medical models. However, it struggles in highly specialized areas, such as Whole-Slide Pathological Images and Electron Microscopy, where deep domain knowledge is crucial. Additionally, DINOv3 does not consistently follow scaling laws in the medical domain, indicating that larger models or higher resolutions do not always lead to better performance.'}, 'zh': {'title': 'DINOv3ï¼šåŒ»å­¦è§†è§‰ä»»åŠ¡çš„æ–°åŸºå‡†', 'desc': 'DINOv3æ˜¯ä¸€ç§è‡ªç›‘ç£çš„è§†è§‰å˜æ¢å™¨ï¼Œåœ¨å„ç§åŒ»å­¦è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ— éœ€ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒã€‚å°½ç®¡å®ƒåœ¨è®¸å¤šä»»åŠ¡ä¸­è¶…è¶Šäº†åŒ»å­¦ç‰¹å®šçš„åŸºç¡€æ¨¡å‹ï¼Œä½†åœ¨éœ€è¦æ·±åº¦ä¸“ä¸šåŒ–çš„é¢†åŸŸä¸­è¡¨ç°æœ‰é™ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒDINOv3çš„æ€§èƒ½åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œè¾“å…¥å›¾åƒåˆ†è¾¨ç‡ä¸‹å¹¶ä¸æ€»æ˜¯éµå¾ªæ‰©å±•è§„å¾‹ã€‚æ€»çš„æ¥è¯´ï¼ŒDINOv3ä¸ºå¤æ‚çš„åŒ»å­¦ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„è§†è§‰ç‰¹å¾åŸºç¡€ï¼Œå¼€å¯äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.01656', 'title': 'Reinforced Visual Perception with Tools', 'url': 'https://huggingface.co/papers/2509.01656', 'abstract': "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Visual reasoning, a cornerstone of human intelligence, encompasses complex perceptual and logical processes essential for solving diverse visual problems. While advances in computer vision have produced powerful models for various perceptual tasks, leveraging these for general visual reasoning remains challenging. Prior work demonstrates that augmenting LLMs with vision models via supervised finetuning improves performance, but faces key limitations such as expensive data generation, reliance on careful data filtering, and poor generalization. To address these issues, we propose ReVPT to enhance multi-modal LLMs' abilities to reason about and use visual tools through reinforcement learning. We introduce a novel RL algorithm based on GRPO, designed to train models to reason with a suite of four visual tools. Through extensive experiments, we show that our method achieves state-of-the-art performance on several perception-heavy benchmarks, including SAT, CV-Bench, BLINK and MMStar, significantly outperforming the supervised and text-based RL finetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the instruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the community new insights on RL-based visual tool-usage through extensive ablations. Our code is available at https://github.com/ls-kelvin/REVPT.", 'score': 24, 'issue_id': 5783, 'pub_date': '2025-09-01', 'pub_date_card': {'ru': '1 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 1', 'zh': '9æœˆ1æ—¥'}, 'hash': '08ec116a2ed1dc2c', 'authors': ['Zetong Zhou', 'Dongping Chen', 'Zixian Ma', 'Zhihan Hu', 'Mingyang Fu', 'Sinan Wang', 'Yao Wan', 'Zhou Zhao', 'Ranjay Krishna'], 'affiliations': ['ONE Lab, HUST', 'University of Maryland', 'University of Washington', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.01656.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#optimization', '#benchmark', '#rl', '#cv'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ReVPT, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ReVPT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ RL Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ GRPO Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°Ñ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ· Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ SAT, CV-Bench, BLINK Ğ¸ MMStar. ReVPT Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'ReVPT: Reinforcement Learning for Enhanced Visual Reasoning in Multi-Modal LLMs', 'desc': 'ReVPT is a novel approach that enhances the visual reasoning capabilities of multi-modal large language models (LLMs) using reinforcement learning (RL). It addresses the limitations of previous methods that relied on supervised finetuning, which often required expensive data generation and struggled with generalization. By introducing a new RL algorithm based on Generalized Relative Policy Optimization (GRPO), ReVPT trains models to effectively utilize a set of visual tools for reasoning tasks. The results demonstrate that ReVPT achieves state-of-the-art performance on various visual benchmarks, significantly outperforming existing methods.'}, 'zh': {'title': 'ReVPTï¼šæå‡å¤šæ¨¡æ€LLMçš„è§†è§‰æ¨ç†èƒ½åŠ›', 'desc': 'ReVPTæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§†è§‰æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼Œè¾¾åˆ°äº†è§†è§‰åŸºå‡†æµ‹è¯•çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚è§†è§‰æ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ¶‰åŠå¤æ‚çš„æ„ŸçŸ¥å’Œé€»è¾‘è¿‡ç¨‹ï¼Œä½†å°†è®¡ç®—æœºè§†è§‰æ¨¡å‹åº”ç”¨äºä¸€èˆ¬è§†è§‰æ¨ç†ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä»¥å¾€çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ç›‘ç£å¾®è°ƒå°†è§†è§‰æ¨¡å‹ä¸LLMç»“åˆå¯ä»¥æé«˜æ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®ç”Ÿæˆæˆæœ¬é«˜ã€æ•°æ®è¿‡æ»¤ä¾èµ–æ€§å¼ºå’Œæ³›åŒ–èƒ½åŠ›å·®ç­‰å…³é”®é™åˆ¶ã€‚ReVPTé€šè¿‡å¼•å…¥åŸºäºGRPOçš„æ–°å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œè®­ç»ƒæ¨¡å‹ä½¿ç”¨å››ç§è§†è§‰å·¥å…·è¿›è¡Œæ¨ç†ï¼Œä»è€Œæœ‰æ•ˆè§£å†³äº†è¿™äº›é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06733', 'title': 'Reinforcement Learning Foundations for Deep Research Systems: A Survey', 'url': 'https://huggingface.co/papers/2509.06733', 'abstract': 'Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.  \t\t\t\t\tAI-generated summary \t\t\t\t Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes work after DeepSeek-R1 along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.', 'score': 20, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '039bfe4e964d5aad', 'authors': ['Wenjun Li', 'Zhi Chen', 'Jingru Lin', 'Hannan Cao', 'Wei Han', 'Sheng Liang', 'Zhi Zhang', 'Kuicai Dong', 'Dexun Li', 'Chen Zhang', 'Yong Liu'], 'affiliations': ['Huawei Technologies Co., Ltd'], 'pdf_title_img': 'assets/pdf/title_img/2509.06733.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#long_context', '#benchmark', '#survey', '#optimization', '#multimodal'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸Ğº Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ°: ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ğ³Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'Empowering AI with Reinforcement Learning for Complex Task Mastery', 'desc': 'This paper discusses the use of reinforcement learning (RL) as a key method for training advanced AI systems that can perform complex tasks. It highlights the limitations of traditional supervised learning and preference alignment methods, which often rely on human-defined rules and can lead to biases. The authors propose that RL can improve the training of these systems by optimizing their interactions with tools and environments, allowing for better exploration and decision-making. The paper also provides a comprehensive overview of RL techniques and frameworks that can enhance the development of deep research agents.'}, 'zh': {'title': 'å¼ºåŒ–å­¦ä¹ ï¼šæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„åŸºç¡€', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ ä½œä¸ºè®­ç»ƒæ·±åº¦ç ”ç©¶ç³»ç»Ÿçš„åŸºç¡€æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç›‘ç£å­¦ä¹ å’Œåå¥½å¯¹é½æ–¹æ³•çš„å±€é™æ€§ã€‚é€šè¿‡ä¼˜åŒ–å·¥å…·äº¤äº’å’Œæ¢ç´¢çš„ç­–ç•¥ï¼Œå¼ºåŒ–å­¦ä¹ èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤ä»»åŠ¡ã€‚æˆ‘ä»¬ç³»ç»ŸåŒ–äº†ä¸æ·±åº¦ç ”ç©¶ç³»ç»Ÿç›¸å…³çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®åˆæˆã€æ ·æœ¬æ•ˆç‡å’Œå¤šç›®æ ‡ä¼˜åŒ–ç­‰æ–¹é¢ã€‚æœ€åï¼Œè®ºæ–‡æä¾›äº†å…³äºå¦‚ä½•è®­ç»ƒç¨³å¥ã€é€æ˜çš„æ·±åº¦ç ”ç©¶ä»£ç†çš„å®ç”¨æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06461', 'title': "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", 'url': 'https://huggingface.co/papers/2509.06461', 'abstract': "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) have demonstrated remarkable success across diverse visual tasks, yet their performance degrades in complex visual environments. While existing enhancement approaches require additional training, rely on external segmentation tools, or operate at coarse-grained levels, they overlook the innate ability within VLMs. To bridge this gap, we investigate VLMs' attention patterns and discover that: (1) visual complexity strongly correlates with attention entropy, negatively impacting reasoning performance; (2) attention progressively refines from global scanning in shallow layers to focused convergence in deeper layers, with convergence degree determined by visual complexity. (3) Theoretically, we prove that the contrast of attention maps between general queries and task-specific queries enables the decomposition of visual signal into semantic signals and visual noise components. Building on these insights, we propose Contrastive Attention Refinement for Visual Enhancement (CARVE), a training-free method that extracts task-relevant visual signals through attention contrasting at the pixel level. Extensive experiments demonstrate that CARVE consistently enhances performance, achieving up to 75% improvement on open-source models. Our work provides critical insights into the interplay between visual complexity and attention mechanisms, offering an efficient pathway for improving visual reasoning with contrasting attention.", 'score': 13, 'issue_id': 5785, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'f239789cf3c0a63d', 'authors': ['Yuyao Ge', 'Shenghua Liu', 'Yiwei Wang', 'Lingrui Mei', 'Baolong Bi', 'Xuanshan Zhou', 'Jiayu Yao', 'Jiafeng Guo', 'Xueqi Cheng'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'University of California, Merced'], 'pdf_title_img': 'assets/pdf/title_img/2509.06461.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#cv', '#training', '#multimodal', '#open_source'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ CARVE Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (VLM) Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. CARVE Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸ĞµĞ¹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, ĞºĞ°Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ÑÑ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑĞºĞ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ CARVE Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… VLM-Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Visual Reasoning with Contrastive Attention', 'desc': 'The paper introduces Contrastive Attention Refinement for Visual Enhancement (CARVE), a method designed to improve the performance of Vision-Language Models (VLMs) in complex visual environments. It identifies that visual complexity affects attention patterns, leading to decreased reasoning performance. By contrasting attention maps from general and task-specific queries, CARVE effectively separates useful visual signals from noise without requiring additional training or external tools. The results show significant performance improvements, highlighting the importance of attention mechanisms in visual reasoning tasks.'}, 'zh': {'title': 'å¯¹æ¯”æ³¨æ„åŠ›ç²¾ç‚¼ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ï¼', 'desc': 'å¯¹æ¯”æ³¨æ„åŠ›ç²¾ç‚¼ï¼ˆCARVEï¼‰æ˜¯ä¸€ç§æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ€§èƒ½çš„æ–¹æ³•ï¼Œé€šè¿‡å¯¹æ¯”æ³¨æ„åŠ›æœºåˆ¶æå–ä¸ä»»åŠ¡ç›¸å…³çš„è§†è§‰ä¿¡å·ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚è§†è§‰ç¯å¢ƒä¸­ï¼Œç°æœ‰å¢å¼ºæ–¹æ³•éœ€è¦é¢å¤–è®­ç»ƒæˆ–ä¾èµ–å¤–éƒ¨åˆ†å‰²å·¥å…·çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°ï¼Œè§†è§‰å¤æ‚æ€§ä¸æ³¨æ„åŠ›ç†µå¯†åˆ‡ç›¸å…³ï¼Œå½±å“æ¨ç†æ€§èƒ½ï¼Œè€Œæ³¨æ„åŠ›åœ¨ä¸åŒå±‚æ¬¡ä¸Šé€æ¸ä»å…¨å±€æ‰«æè½¬å‘æ·±å±‚çš„èšç„¦æ”¶æ•›ã€‚CARVEé€šè¿‡å¯¹æ¯”ä¸€èˆ¬æŸ¥è¯¢å’Œä»»åŠ¡ç‰¹å®šæŸ¥è¯¢çš„æ³¨æ„åŠ›å›¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†è§£è§†è§‰ä¿¡å·ï¼Œæå‡è§†è§‰æ¨ç†èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06155', 'title': 'UniVerse-1: Unified Audio-Video Generation via Stitching of Experts', 'url': 'https://huggingface.co/papers/2509.06155', 'abstract': 'UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce UniVerse-1, a unified, Veo-3-like model capable of simultaneously generating coordinated audio and video. To enhance training efficiency, we bypass training from scratch and instead employ a stitching of experts (SoE) technique. This approach deeply fuses the corresponding blocks of pre-trained video and music generation experts models, thereby fully leveraging their foundational capabilities. To ensure accurate annotations and temporal alignment for both ambient sounds and speech with video content, we developed an online annotation pipeline that processes the required training data and generates labels during training process. This strategy circumvents the performance degradation often caused by misalignment text-based annotations. Through the synergy of these techniques, our model, after being finetuned on approximately 7,600 hours of audio-video data, produces results with well-coordinated audio-visuals for ambient sounds generation and strong alignment for speech generation. To systematically evaluate our proposed method, we introduce Verse-Bench, a new benchmark dataset. In an effort to advance research in audio-video generation and to close the performance gap with state-of-the-art models such as Veo3, we make our model and code publicly available. We hope this contribution will benefit the broader research community. Project page: https://dorniwang.github.io/UniVerse-1/.', 'score': 13, 'issue_id': 5783, 'pub_date': '2025-09-07', 'pub_date_card': {'ru': '7 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 7', 'zh': '9æœˆ7æ—¥'}, 'hash': '9f7107d4dbe89f53', 'authors': ['Duomin Wang', 'Wei Zuo', 'Aojie Li', 'Ling-Hao Chen', 'Xinyao Liao', 'Deyu Zhou', 'Zixin Yin', 'Xili Dai', 'Daxin Jiang', 'Gang Yu'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology(GuangZhou)', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06155.jpg', 'data': {'categories': ['#multimodal', '#video', '#audio', '#benchmark', '#open_source'], 'emoji': 'ğŸ¥', 'ru': {'title': 'UniVerse-1: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ UniVerse-1, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ "stitching of experts". Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ¶Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¼ÑƒĞ·Ñ‹ĞºĞ¸, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ”Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ² Ğ¸ Ñ€ĞµÑ‡Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° 7600 Ñ‡Ğ°ÑĞ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'UniVerse-1: Harmonizing Audio and Video Generation', 'desc': 'UniVerse-1 is a cutting-edge model designed for generating synchronized audio and video content. It utilizes a stitching of experts (SoE) technique to merge pre-trained models for video and music, enhancing the quality of the generated outputs. The model incorporates an online annotation pipeline to ensure precise temporal alignment between audio and video, which is crucial for maintaining coherence in the generated media. After fine-tuning on a substantial dataset, UniVerse-1 demonstrates significant improvements in producing well-coordinated audio-visuals, making it a valuable tool for advancing research in the field.'}, 'zh': {'title': 'ç»Ÿä¸€éŸ³è§†é¢‘ç”Ÿæˆçš„æœªæ¥', 'desc': 'UniVerse-1 æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„éŸ³é¢‘è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œé‡‡ç”¨ä¸“å®¶æ‹¼æ¥æŠ€æœ¯ï¼Œå°†é¢„è®­ç»ƒçš„è§†é¢‘å’ŒéŸ³ä¹æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·ï¼Œç¡®ä¿æ—¶é—´ä¸Šçš„å‡†ç¡®å¯¹é½ï¼Œç”Ÿæˆé«˜è´¨é‡çš„éŸ³è§†é¢‘è¾“å‡ºã€‚è¯¥æ¨¡å‹é€šè¿‡æ·±åº¦èåˆé¢„è®­ç»ƒçš„ä¸“å®¶æ¨¡å‹ï¼Œé¿å…äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„ä½æ•ˆï¼Œå……åˆ†åˆ©ç”¨äº†å·²æœ‰çš„åŸºç¡€èƒ½åŠ›ã€‚ä¸ºäº†ç¡®ä¿éŸ³é¢‘å’Œè§†é¢‘å†…å®¹çš„å‡†ç¡®æ³¨é‡Šå’Œæ—¶é—´å¯¹é½ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåœ¨çº¿æ³¨é‡Šç®¡é“ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤„ç†æ‰€éœ€çš„æ•°æ®å¹¶ç”Ÿæˆæ ‡ç­¾ã€‚ç»è¿‡çº¦7600å°æ—¶çš„éŸ³è§†é¢‘æ•°æ®å¾®è°ƒåï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆåè°ƒè‰¯å¥½çš„éŸ³è§†é¢‘å†…å®¹ï¼Œå¹¶åœ¨è¯­éŸ³ç”Ÿæˆæ–¹é¢å®ç°å¼ºå¯¹é½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06917', 'title': 'Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06917', 'abstract': "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.", 'score': 10, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '2a482f9a59e68451', 'authors': ['Jiacheng Miao', 'Joe R. Davis', 'Jonathan K. Pritchard', 'James Zou'], 'affiliations': ['Department of Biology, Stanford University', 'Department of Biomedical Data Science, Stanford University', 'Department of Computer Science, Stanford University', 'Department of Genetics, Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06917.jpg', 'data': {'categories': ['#agents', '#science', '#multimodal'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Paper2Agent - ÑÑ‚Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ². ĞĞ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚ Ğ¸ ĞºĞ¾Ğ´ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Model Context Protocol (MCP) ÑĞµÑ€Ğ²ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğº Ñ‡Ğ°Ñ‚-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Paper2Agent Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ¾Ğ² Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ¼Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Transforming Research Papers into Interactive AI Agents', 'desc': 'Paper2Agent is a framework that transforms traditional research papers into interactive AI agents, making it easier for users to access and utilize scientific knowledge. By analyzing the paper and its associated code, it creates a Model Context Protocol (MCP) server that allows the AI agent to answer complex queries in natural language. This approach reduces the effort required for researchers to understand and apply the findings, thus enhancing the dissemination and reuse of research outputs. The effectiveness of Paper2Agent is demonstrated through case studies where it successfully reproduces original results and handles novel queries, paving the way for a new collaborative ecosystem in scientific research.'}, 'zh': {'title': 'å°†é™æ€è®ºæ–‡è½¬å˜ä¸ºåŠ¨æ€ AI ä»£ç†çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'Paper2Agent æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œå¯ä»¥å°†ç ”ç©¶è®ºæ–‡è½¬åŒ–ä¸ºäº’åŠ¨çš„ AI ä»£ç†ï¼Œä»¥ä¿ƒè¿›çŸ¥è¯†ä¼ æ’­å’Œå¤æ‚ç§‘å­¦æŸ¥è¯¢ã€‚å®ƒé€šè¿‡åˆ†æè®ºæ–‡åŠå…¶ç›¸å…³ä»£ç ï¼Œæ„å»ºæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰æœåŠ¡å™¨ï¼Œä½¿å¾—ç ”ç©¶æˆæœä»è¢«åŠ¨çš„æ–‡çŒ®å˜ä¸ºä¸»åŠ¨çš„ç³»ç»Ÿã€‚è¿™æ ·ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸ AI ä»£ç†è¿›è¡Œäº¤æµï¼Œè½»æ¾è·å–è®ºæ–‡ä¸­çš„ä¿¡æ¯å’Œå·¥å…·ã€‚Paper2Agent çš„æœ‰æ•ˆæ€§é€šè¿‡æ¡ˆä¾‹ç ”ç©¶å¾—åˆ°äº†éªŒè¯ï¼Œèƒ½å¤Ÿé‡ç°åŸè®ºæ–‡çš„ç»“æœå¹¶å¤„ç†æ–°çš„ç”¨æˆ·æŸ¥è¯¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.02108', 'title': 'DivMerge: A divergence-based model merging method for multi-tasking', 'url': 'https://huggingface.co/papers/2509.02108', 'abstract': 'Multi-task learning (MTL) is often achieved by merging datasets before fine-tuning, but the growing availability of fine-tuned models has led to new approaches such as model merging via task arithmetic. A major challenge in this setting is task interference, which worsens as the number of tasks increases. We propose a method that merges models trained on different tasks into a single model, maintaining strong performance across all tasks. Our approach leverages Jensen-Shannon divergence to guide the merging process without requiring additional labelled data, and automatically balances task importance. Unlike existing methods, our approach remains robust as the number of tasks grows and consistently outperforms prior work.', 'score': 10, 'issue_id': 5797, 'pub_date': '2025-09-02', 'pub_date_card': {'ru': '2 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 2', 'zh': '9æœˆ2æ—¥'}, 'hash': '615289bb7bc02e97', 'authors': ['Touayouch Brahim', 'Fosse LoÃ¯c', 'Damnati GÃ©raldine', 'LecorvÃ© GwÃ©nolÃ©'], 'affiliations': ['CNRS, LIS, Aix Marseille UniversitÃ©, France', 'Orange Research, Lannion, France', 'Ã‰cole polytechnique, Institut polytechnique de Paris, Palaiseau, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.02108.jpg', 'data': {'categories': ['#training', '#dataset', '#architecture', '#optimization', '#transfer_learning'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ² ĞµĞ´Ğ¸Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ¸Ğ²ĞµÑ€Ğ³ĞµĞ½Ñ†Ğ¸Ñ Ğ™ĞµĞ½ÑĞµĞ½Ğ°-Ğ¨ĞµĞ½Ğ½Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ¼ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ñ… ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Merging Models for Robust Multi-Task Learning', 'desc': 'This paper discusses a new method for multi-task learning (MTL) that combines models trained on different tasks into one model. The challenge of task interference, which can degrade performance as more tasks are added, is addressed by using Jensen-Shannon divergence to guide the merging process. This method does not need extra labeled data and automatically balances the importance of each task. The proposed approach shows improved robustness and performance compared to existing methods, even as the number of tasks increases.'}, 'zh': {'title': 'æ™ºèƒ½åˆå¹¶ï¼Œæå‡å¤šä»»åŠ¡å­¦ä¹ æ€§èƒ½', 'desc': 'å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰é€šå¸¸é€šè¿‡åœ¨å¾®è°ƒä¹‹å‰åˆå¹¶æ•°æ®é›†æ¥å®ç°ï¼Œä½†éšç€å¾®è°ƒæ¨¡å‹çš„æ—¥ç›Šå¢å¤šï¼Œå‡ºç°äº†é€šè¿‡ä»»åŠ¡ç®—æœ¯åˆå¹¶æ¨¡å‹çš„æ–°æ–¹æ³•ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯ä»»åŠ¡å¹²æ‰°ï¼Œéšç€ä»»åŠ¡æ•°é‡çš„å¢åŠ è€ŒåŠ å‰§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å°†ä¸åŒä»»åŠ¡è®­ç»ƒçš„æ¨¡å‹åˆå¹¶ä¸ºå•ä¸€æ¨¡å‹çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ä¿æŒå¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†è©¹æ£®-é¦™å†œæ•£åº¦æ¥æŒ‡å¯¼åˆå¹¶è¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–çš„æ ‡è®°æ•°æ®ï¼Œå¹¶è‡ªåŠ¨å¹³è¡¡ä»»åŠ¡çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03516', 'title': 'Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?', 'url': 'https://huggingface.co/papers/2509.03516', 'abstract': 'T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-to-image (T2I) generation aims to synthesize images from textual prompts, which jointly specify what must be shown and imply what can be inferred, thereby corresponding to two core capabilities: composition and reasoning. However, with the emerging advances of T2I models in reasoning beyond composition, existing benchmarks reveal clear limitations in providing comprehensive evaluations across and within these capabilities. Meanwhile, these advances also enable models to handle more complex prompts, whereas current benchmarks remain limited to low scene density and simplified one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a comprehensive and complex benchmark that evaluates both composition and reasoning capabilities of T2I models. To ensure comprehensiveness, we structure composition around scene graph elements (instance, attribute, and relation) and reasoning around the philosophical framework of inference (deductive, inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To increase complexity, driven by the inherent complexities of real-world scenarios, we curate each prompt with high compositional density for composition and multi-step inference for reasoning. We also pair each prompt with a checklist that specifies individual yes/no questions to assess each intended element independently to facilitate fine-grained and reliable evaluation. In statistics, our benchmark comprises 1,080 challenging prompts and around 13,500 checklist questions. Experiments across 27 current T2I models reveal that their composition capability still remains limited in complex high-density scenarios, while the reasoning capability lags even further behind as a critical bottleneck, with all models struggling to infer implicit elements from prompts. Our project page: https://t2i-corebench.github.io/.', 'score': 9, 'issue_id': 5784, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': '8f5ee8fae69242b1', 'authors': ['Ouxiang Li', 'Yuan Wang', 'Xinting Hu', 'Huijuan Huang', 'Rui Chen', 'Jiarong Ou', 'Xin Tao', 'Pengfei Wan', 'Fuli Feng'], 'affiliations': ['Kuaishou Technology', 'Nanyang Technological University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.03516.jpg', 'data': {'categories': ['#cv', '#benchmark', '#reasoning', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'T2I-CoReBench: ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ', 'desc': 'T2I-CoReBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 1080 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ Ğ¾ĞºĞ¾Ğ»Ğ¾ 13500 ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ 12-Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ° ÑÑ†ĞµĞ½Ñ‹ Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ„Ğ¸Ğ»Ğ¾ÑĞ¾Ñ„ÑĞºĞ¸Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ text-to-image Ğ²ÑĞµ ĞµÑ‰Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ñ‹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ, Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚.'}, 'en': {'title': 'Elevating Text-to-Image Models: A New Benchmark for Composition and Reasoning', 'desc': 'T2I-CoReBench is a new benchmark designed to assess the composition and reasoning abilities of text-to-image (T2I) models. It introduces a detailed evaluation framework that includes a 12-dimensional taxonomy focusing on scene graph elements and various types of reasoning. The benchmark features 1,080 complex prompts and approximately 13,500 checklist questions to ensure a thorough evaluation of model performance. Results indicate that while T2I models can handle basic tasks, they struggle significantly with complex scenarios and implicit reasoning, highlighting areas for improvement.'}, 'zh': {'title': 'è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç»„åˆä¸æ¨ç†èƒ½åŠ›', 'desc': 'T2I-CoReBenchæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç»„åˆå’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥åŸºå‡†é€šè¿‡å¤æ‚çš„æç¤ºå’Œæ£€æŸ¥é—®é¢˜ï¼Œå…¨é¢è€ƒå¯Ÿæ¨¡å‹åœ¨é«˜åœºæ™¯å¯†åº¦ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å¤æ‚åœºæ™¯ä¸­çš„ç»„åˆèƒ½åŠ›ä»ç„¶æœ‰é™ï¼Œè€Œæ¨ç†èƒ½åŠ›æ›´æ˜¯æˆä¸ºå…³é”®ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†åŒ…å«1080ä¸ªæŒ‘æˆ˜æ€§æç¤ºå’Œçº¦13500ä¸ªæ£€æŸ¥é—®é¢˜ï¼Œä»¥å®ç°ç»†è‡´å¯é çš„è¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06945', 'title': 'Interleaving Reasoning for Better Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2509.06945', 'abstract': 'Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .', 'score': 7, 'issue_id': 5786, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '0f828dd1d00b0a90', 'authors': ['Wenxuan Huang', 'Shuang Chen', 'Zheyong Xie', 'Shaosheng Cao', 'Shixiang Tang', 'Yufan Shen', 'Qingyu Yin', 'Wenbo Hu', 'Xiaoman Wang', 'Yuntian Tang', 'Junbo Qiao', 'Yue Guo', 'Yao Hu', 'Zhenfei Yin', 'Philip Torr', 'Yu Cheng', 'Wanli Ouyang', 'Shaohui Lin'], 'affiliations': ['East China Normal University', 'The Chinese University of Hong Kong', 'University of California, Los Angeles', 'University of Oxford', 'Xiaohongshu Inc.', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06945.jpg', 'data': {'categories': ['#training', '#multimodal', '#cv', '#optimization', '#reasoning', '#dataset', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ§ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Text-to-Image Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ - Interleaving Reasoning Generation (IRG). Ğ­Ñ‚Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºĞ° Ñ‡ĞµÑ€ĞµĞ´ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ IRGL Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ IRGL-300K Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ IRG Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹.'}, 'en': {'title': 'Enhancing Image Generation through Interleaved Reasoning', 'desc': 'The Interleaving Reasoning Generation (IRG) framework enhances Text-to-Image (T2I) generation by alternating between text-based reasoning and image synthesis. This approach allows the model to first generate an initial image based on textual input, then refine it by reflecting on the generated output to improve details and visual quality. The training process, called Interleaving Reasoning Generation Learning (IRGL), focuses on establishing a strong initial image and ensuring high-quality textual feedback for further refinements. The results demonstrate significant advancements in performance metrics and visual fidelity, showcasing the effectiveness of this interleaved reasoning approach.'}, 'zh': {'title': 'äº¤é”™æ¨ç†ç”Ÿæˆï¼šæå‡æ–‡æœ¬åˆ°å›¾åƒçš„è´¨é‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºäº¤é”™æ¨ç†ç”Ÿæˆï¼ˆIRGï¼‰çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„æ•ˆæœã€‚è¯¥æ¡†æ¶é€šè¿‡äº¤æ›¿è¿›è¡ŒåŸºäºæ–‡æœ¬çš„æ€è€ƒå’Œå›¾åƒåˆæˆï¼Œæ¥å¢å¼ºç”Ÿæˆå›¾åƒçš„è§†è§‰è´¨é‡å’Œç»†èŠ‚ä¿ç•™ã€‚IRGçš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆå»ºç«‹æ ¸å¿ƒå†…å®¹å’ŒåŸºç¡€è´¨é‡ï¼Œç„¶ååœ¨åç»­å›¾åƒä¸­å®ç°é«˜è´¨é‡çš„æ–‡æœ¬åæ€å’Œç»†è‡´çš„æ”¹è¿›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRGåœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06631', 'title': 'Guided Decoding and Its Critical Role in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2509.06631', 'abstract': 'Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.', 'score': 5, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '3deb208811cb9805', 'authors': ['Ã–zgÃ¼r UÄŸur', 'Musa YÄ±lmaz', 'Esra Åavirdi', 'Ã–zay Ezerceli', 'Mahmut El Huseyni', 'Selva TaÅŸ', 'Reyhan Bayraktar'], 'affiliations': ['Newmind AI Istanbul, TÃ¼rkiye'], 'pdf_title_img': 'assets/pdf/title_img/2509.06631.jpg', 'data': {'categories': ['#hallucinations', '#rag', '#alignment'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² RAG: ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ (RAG) Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° - Outlines, XGrammar Ğ¸ LM Format Enforcer - Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ…ĞµĞ¼Ğ°Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. ĞÑ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Structured Outputs in RAG Systems with Guided Decoding', 'desc': 'This paper investigates how guided decoding methods can improve structured output generation in Retrieval-Augmented Generation (RAG) systems. It compares three specific methods: Outlines, XGrammar, and LM Format Enforcer, under different prompting scenarios. The study measures success rates, hallucination rates, and overall output quality to understand how multi-turn interactions affect performance. The findings provide valuable insights for selecting appropriate methods in RAG systems, enhancing the reliability of AI-generated responses.'}, 'zh': {'title': 'ä¼˜åŒ–RAGç³»ç»Ÿä¸­çš„ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆ', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸­ï¼ŒæŒ‡å¯¼è§£ç æ–¹æ³•å¯¹ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆçš„å½±å“ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸‰ç§æ–¹æ³•ï¼šå¤§çº²ã€XGrammarå’Œè¯­è¨€æ¨¡å‹æ ¼å¼å¼ºåˆ¶å™¨ï¼Œåˆ†æäº†å®ƒä»¬åœ¨ä¸åŒå¤šè½®æç¤ºè®¾ç½®ä¸‹çš„è¡¨ç°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå¤šè½®äº¤äº’å¯¹æŒ‡å¯¼è§£ç çš„å½±å“ï¼Œä»¥åŠä¸åŒæ–¹æ³•åœ¨æˆåŠŸç‡ã€å¹»è§‰ç‡å’Œè¾“å‡ºè´¨é‡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚æ­¤é¡¹å·¥ä½œä¸ºRAGç³»ç»Ÿä¸­çš„ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæä¾›äº†ç†è®ºè§è§£å’Œå®é™…æŒ‡å¯¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06493', 'title': 'Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers', 'url': 'https://huggingface.co/papers/2509.06493', 'abstract': 'BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces BFS-Prover-V2, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.', 'score': 5, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'e9de5406241d634e', 'authors': ['Ran Xin', 'Zeyu Zheng', 'Yanchen Nie', 'Kun Yuan', 'Xia Xiao'], 'affiliations': ['ByteDance Seed', 'Carnegie Mellon University', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2509.06493.jpg', 'data': {'categories': ['#agents', '#reasoning', '#rl', '#training', '#benchmark', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğµ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'BFS-Prover-V2 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ñ€ĞµÑˆĞ°ÑÑ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ°ĞºÑ‚Ğ¸Ğº. BFS-Prover-V2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ.'}, 'en': {'title': 'Scaling Theorem Proving with Smart Collaboration', 'desc': "BFS-Prover-V2 is a system that enhances automated theorem proving by tackling the challenges of scaling in both training and inference. It introduces a multi-turn off-policy reinforcement learning (RL) framework that improves the performance of large language models (LLMs) during training, inspired by AlphaZero's expert iteration approach. Additionally, it features a planner-enhanced multi-agent search architecture that breaks down complex theorems into simpler subgoals, allowing multiple agents to work together efficiently. This innovative dual approach has achieved state-of-the-art results on formal mathematics benchmarks, demonstrating its potential for broader applications in complex reasoning tasks."}, 'zh': {'title': 'åŒé‡æ‰©å±•ï¼Œçªç ´å®šç†è¯æ˜çš„æé™', 'desc': 'BFS-Prover-V2 æ˜¯ä¸€ä¸ªé’ˆå¯¹è‡ªåŠ¨å®šç†è¯æ˜ä¸­çš„æ‰©å±•æŒ‘æˆ˜çš„ç³»ç»Ÿï¼Œç»“åˆäº†å¤šè½®ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œè§„åˆ’å¢å¼ºçš„å¤šæ™ºèƒ½ä½“æœç´¢æ¶æ„ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ›æ–°çš„å¤šè½®ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ŒæŒç»­æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è®­ç»ƒæ—¶çš„è¡¨ç°ï¼Œå¹¶å…‹æœäº†é•¿æœŸå¼ºåŒ–å­¦ä¹ ä¸­çš„æ€§èƒ½ç“¶é¢ˆã€‚å…¶æ¬¡ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ä¸ªé«˜å±‚æ¬¡çš„è§„åˆ’æ¨¡å‹ï¼Œå°†å¤æ‚å®šç†åˆ†è§£ä¸ºä¸€ç³»åˆ—ç®€å•çš„å­ç›®æ ‡ï¼Œä»è€Œåœ¨æ¨ç†æ—¶æœ‰æ•ˆç¼©å°æœç´¢ç©ºé—´ã€‚é€šè¿‡è¿™ç§åŒé‡æ‰©å±•æ–¹æ³•ï¼ŒBFS-Prover-V2 åœ¨æ­£å¼æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06861', 'title': 'Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet', 'url': 'https://huggingface.co/papers/2509.06861', 'abstract': 'Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge', 'score': 4, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '128069f43a6c18a2', 'authors': ['James Xu Zhao', 'Bryan Hooi', 'See-Kiong Ng'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2509.06861.jpg', 'data': {'categories': ['#inference', '#benchmark', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ¤”', 'ru': {'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ - Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° Ğ»ÑƒÑ‡ÑˆĞµ: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (test-time scaling) Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ»Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ 12 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ´Ğ²ÑƒÑ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¼Ñƒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸. Ğ¢ĞµĞ¼ Ğ½Ğµ Ğ¼ĞµĞ½ĞµĞµ, Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ¿Ğ° Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğ¼.'}, 'en': {'title': 'Test-Time Scaling: More Thinking, More Hallucinations?', 'desc': 'This paper investigates the effectiveness of test-time scaling in improving the performance of reasoning models on knowledge-intensive tasks. While test-time scaling allows models to generate longer reasoning chains, the authors find that it does not consistently enhance accuracy and can even increase the occurrence of hallucinations. The study evaluates 12 reasoning models across two benchmarks, revealing that longer reasoning often leads to overconfident errors rather than improved factual accuracy. The findings suggest that while extended reasoning can be beneficial, it may also induce confirmation bias, highlighting the need for careful application in knowledge-intensive scenarios.'}, 'zh': {'title': 'æµ‹è¯•æ—¶æ‰©å±•æ¨ç†çš„å±€é™æ€§ä¸æŒ‘æˆ˜', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶æ‰©å±•æ¨ç†å¯¹çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡çš„å½±å“ã€‚å°½ç®¡æµ‹è¯•æ—¶æ‰©å±•æ¨ç†å¯ä»¥å¢åŠ æ¨ç†é“¾çš„é•¿åº¦å¹¶æé«˜æ¨¡å‹çš„è¡¨ç°ï¼Œä½†åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å¹¶æœªæ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ›´å¤šçš„å¹»è§‰é”™è¯¯ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå»¶é•¿æ¨ç†æ—¶é—´å¹¶ä¸æ€»æ˜¯èƒ½æ”¹å–„æ¨¡å‹çš„äº‹å®å›å¿†ï¼Œåè€Œå¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨é¢å¯¹æœªå›ç­”çš„é—®é¢˜æ—¶äº§ç”Ÿè¿‡åº¦è‡ªä¿¡çš„å¹»è§‰ã€‚å°½ç®¡å­˜åœ¨è¿™äº›å±€é™æ€§ï¼Œå¯ç”¨æ€è€ƒä»ç„¶æ¯”ä¸æ€è€ƒæ›´æœ‰ç›Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06786', 'title': 'R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World', 'url': 'https://huggingface.co/papers/2509.06786', 'abstract': "A new framework, RÂ²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose safe-by-coevolution as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce R^2AI -- Resistant and Resilient AI -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. R^2AI integrates fast and slow safe models, adversarial simulation and verification through a safety wind tunnel, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI.", 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'bdf421cc1d868635', 'authors': ['Youbang Sun', 'Xiang Wang', 'Jie Fu', 'Chaochao Lu', 'Bowen Zhou'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.06786.jpg', 'data': {'categories': ['#agents', '#security', '#ethics', '#training', '#agi'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞšĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ RÂ²AI Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¼ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ°Ğ¼ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½ĞµĞ¿Ñ€ĞµĞ´Ğ²Ğ¸Ğ´ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ€Ğ¸ÑĞºĞ°Ğ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ¸ Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. RÂ²AI Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ adversarial simulation Ğ¸ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· 'safety wind tunnel', Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ñ†Ğ¸ĞºĞ»Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒÑ‚Ğ²ĞµÑ€Ğ¶Ğ´Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ° ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ¹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜ Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…."}, 'en': {'title': 'RÂ²AI: Evolving Safety for Advanced AI Systems', 'desc': 'The paper introduces RÂ²AI, a new framework aimed at improving AI safety by combining two key strategies: resistance to known threats and resilience to unexpected risks. It critiques existing safety approaches that either reactively apply safety measures or struggle with unforeseen challenges in complex environments. RÂ²AI proposes a coevolutionary approach to safety, inspired by biological immunity, where safety is treated as an ongoing learning process. This framework utilizes fast and slow safe models, adversarial simulations, and continuous feedback to ensure that AI systems can adapt and remain safe as they evolve.'}, 'zh': {'title': 'RÂ²AIï¼šå…±è¿›åŒ–æå‡äººå·¥æ™ºèƒ½å®‰å…¨æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶RÂ²AIï¼Œæ—¨åœ¨é€šè¿‡å…±è¿›åŒ–å¢å¼ºäººå·¥æ™ºèƒ½çš„å®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶ç»“åˆäº†å¯¹å·²çŸ¥å¨èƒçš„æŠµæŠ—åŠ›å’Œå¯¹æœªçŸ¥é£é™©çš„éŸ§æ€§ï¼Œä½¿ç”¨å¿«é€Ÿå’Œæ…¢é€Ÿå®‰å…¨æ¨¡å‹ä»¥åŠå¯¹æŠ—æ€§æ¨¡æ‹Ÿã€‚RÂ²AIçš„è®¾è®¡çµæ„Ÿæ¥è‡ªç”Ÿç‰©å…ç–«ï¼Œå¼ºè°ƒå®‰å…¨æ€§æ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€å¯¹æŠ—æ€§çš„æŒç»­å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡è¿™ä¸€æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­ä¿æŒæŒç»­çš„å®‰å…¨æ€§ï¼Œè§£å†³çŸ­æœŸè„†å¼±æ€§å’Œé•¿æœŸå­˜åœ¨é£é™©çš„é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.05668', 'title': 'Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian', 'url': 'https://huggingface.co/papers/2509.05668', 'abstract': 'Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.', 'score': 3, 'issue_id': 5784, 'pub_date': '2025-09-06', 'pub_date_card': {'ru': '6 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 6', 'zh': '9æœˆ6æ—¥'}, 'hash': '8fe2894e1bb529f0', 'authors': ['Michael Hoffmann', 'Jophin John', 'Stefan Schweter', 'Gokul Ramakrishnan', 'Hoi-Fong Mak', 'Alice Zhang', 'Dmitry Gaynullin', 'Nicolay J. Hammer'], 'affiliations': ['Cerebras Systems Sunnyvale, USA', 'Independent Researcher Holzkirchen, Germany', 'Leibniz Supercomputing Centre (LRZ) Garching, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2509.05668.jpg', 'data': {'categories': ['#dataset', '#machine_translation', '#architecture', '#training', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ½ĞµÑ€Ğ°Ğ²ĞµĞ½ÑÑ‚Ğ²Ğ° Ğ² Ğ˜Ğ˜: Ñ‚Ñ€ĞµÑ…ÑŠÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ°ĞºÑ†ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ½Ğ° Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¸', 'desc': 'Llama-GENBA-10B - ÑÑ‚Ğ¾ Ñ‚Ñ€ĞµÑ…ÑŠÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ğ½Ğ³Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾, Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² ĞºÑ€Ğ¾ÑÑ-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞÑĞ¾Ğ±Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑƒĞ´ĞµĞ»ĞµĞ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ğº Ğ¼Ğ°Ğ»Ğ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ñ Ğ±Ğ°Ğ²Ğ°Ñ€ÑĞºĞ¸Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ½ĞµĞ¼ĞµÑ†ĞºĞ¾Ğ¼.'}, 'en': {'title': 'Balancing Languages: Llama-GENBA-10B Redefines Multilingual AI', 'desc': 'Llama-GENBA-10B is a trilingual foundation model designed to reduce English-centric bias in language processing. It is trained on a balanced dataset of English, German, and Bavarian, ensuring that no single language dominates the training process. The model addresses challenges such as the scarcity of Bavarian data and the need for a unified tokenizer across languages. Evaluations indicate that Llama-GENBA-10B excels in cross-lingual tasks, particularly in Bavarian, setting new performance benchmarks and demonstrating effective multilingual pretraining techniques.'}, 'zh': {'title': 'æ‰“ç ´è¯­è¨€åè§ï¼Œä¿ƒè¿›å¤šè¯­è¨€å¹³è¡¡', 'desc': 'Llama-GENBA-10B æ˜¯ä¸€ä¸ªä¸‰è¯­åŸºç¡€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è‹±è¯­åè§é—®é¢˜ã€‚è¯¥æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹³è¡¡äº†è‹±è¯­ã€å¾·è¯­å’Œå·´ä¼åˆ©äºšè¯­çš„èµ„æºï¼Œä½¿ç”¨äº† 1640 äº¿ä¸ªæ ‡è®°ï¼Œç¡®ä¿äº†å„è¯­è¨€çš„å…¬å¹³æ€§ã€‚é€šè¿‡ä¼˜åŒ–æ¶æ„å’Œè¯­è¨€æ¯”ä¾‹è¶…å‚æ•°ï¼ŒLlama-GENBA-10B åœ¨è·¨è¯­è¨€è¿ç§»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨å·´ä¼åˆ©äºšè¯­çš„è¯„ä¼°ä¸­è®¾ç«‹äº†æ–°çš„åŸºå‡†ã€‚è¯¥æ¨¡å‹çš„å¼€å‘ä¸ºä½èµ„æºè¯­è¨€çš„æ•´åˆæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå±•ç¤ºäº†å¤šè¯­è¨€é¢„è®­ç»ƒçš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06771', 'title': 'D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning', 'url': 'https://huggingface.co/papers/2509.06771', 'abstract': "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.  \t\t\t\t\tAI-generated summary \t\t\t\t Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '48d37925f403181d', 'authors': ['Sai Kartheek Reddy Kasu', 'Mohammad Zia Ur Rehman', 'Shahid Shafi Dar', 'Rishi Bharat Junghare', 'Dhanvin Sanjay Namboodiri', 'Nagendra Kumar'], 'affiliations': ['Indian Institute of Information Technology Dharwad, India', 'Indian Institute of Technology Indore, India', 'Malaviya National Institute of Technology Jaipur, India'], 'pdf_title_img': 'assets/pdf/title_img/2509.06771.jpg', 'data': {'categories': ['#open_source', '#reasoning', '#dataset', '#games', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°ĞµÑ‚ Ñ‡ĞµÑ€Ğ½Ñ‹Ğ¹ ÑĞ¼Ğ¾Ñ€ Ğ² Ğ¼ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼ĞµĞ¼Ğ°Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM) Ğ¸ Ñ‚Ñ€ĞµÑ…Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (TCRNet). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 4,379 Ğ¼ĞµĞ¼Ğ¾Ğ² Reddit Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¿Ğ¾ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¼ĞµĞ¼Ğ°, Ğ° TCRNet Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼Ğ¾Ñ€Ğ°, Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµĞ½ÑĞ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unraveling Dark Humor in Memes with Advanced Reasoning Techniques', 'desc': "This paper presents a new framework for detecting dark humor in memes using a combination of a Large Vision-Language Model (VLM) and a Tri-stream Cross-Reasoning Network (TCRNet). The authors created a dataset of 4,379 Reddit memes, annotated for dark humor, target categories, and intensity levels. The framework generates structured explanations for memes and refines them through a self-loop mechanism, enhancing the model's understanding. By integrating textual and visual features with reasoning, the model achieves superior performance in identifying dark humor, targets, and intensity compared to existing methods."}, 'zh': {'title': 'å¢å¼ºæ¨ç†ï¼Œç²¾å‡†è¯†åˆ«é»‘è‰²å¹½é»˜', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¢å¼ºæ¨ç†çš„æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹å’Œä¸‰æµäº¤å‰æ¨ç†ç½‘ç»œï¼Œæ—¨åœ¨æé«˜å¯¹é»‘è‰²å¹½é»˜çš„æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«4379ä¸ªRedditè¡¨æƒ…åŒ…çš„æ–°æ•°æ®é›†ï¼Œæ ‡æ³¨äº†é»‘è‰²å¹½é»˜ã€ç›®æ ‡ç±»åˆ«å’Œå¼ºåº¦ç­‰çº§ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆç»“æ„åŒ–è§£é‡Šï¼Œç»“åˆæ–‡æœ¬ã€å›¾åƒå’Œæ¨ç†ç‰¹å¾ï¼Œè¿›è¡Œæœ‰æ•ˆçš„åˆ†ç±»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é»‘è‰²å¹½é»˜æ£€æµ‹ã€ç›®æ ‡è¯†åˆ«å’Œå¼ºåº¦é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06477', 'title': 'MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents', 'url': 'https://huggingface.co/papers/2509.06477', 'abstract': "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.  \t\t\t\t\tAI-generated summary \t\t\t\t To enhance the efficiency of GUI agents on various platforms like smartphones and computers, a hybrid paradigm that combines flexible GUI operations with efficient shortcuts (e.g., API, deep links) is emerging as a promising direction. However, a framework for systematically benchmarking these hybrid agents is still underexplored. To take the first step in bridging this gap, we introduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut hybrid agents with a specific focus on the mobile domain. Beyond merely using predefined shortcuts, MAS-Bench assesses an agent's capability to autonomously generate shortcuts by discovering and creating reusable, low-cost workflows. It features 139 complex tasks across 11 real-world applications, a knowledge base of 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation metrics. The tasks are designed to be solvable via GUI-only operations, but can be significantly accelerated by intelligently embedding shortcuts. Experiments show that hybrid agents achieve significantly higher success rates and efficiency than their GUI-only counterparts. This result also demonstrates the effectiveness of our method for evaluating an agent's shortcut generation capabilities. MAS-Bench fills a critical evaluation gap, providing a foundational platform for future advancements in creating more efficient and robust intelligent agents.", 'score': 2, 'issue_id': 5784, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': '4c5d5e5da6b90a95', 'authors': ['Pengxiang Zhao', 'Guangyi Liu', 'Yaozhen Liang', 'Weiqing He', 'Zhengxi Lu', 'Yuehao Huang', 'Yaxuan Guo', 'Kexin Zhang', 'Hao Wang', 'Liang Liu', 'Yong Liu'], 'affiliations': ['Huzhou Institute of Zhejiang University', 'Zhejiang University', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.06477.jpg', 'data': {'categories': ['#agents', '#benchmark', '#games', '#optimization'], 'emoji': 'ğŸ“±', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ - Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'MAS-Bench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ¸ ÑÑ€Ğ»Ñ‹ĞºĞ¸, Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ². ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 139 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² 11 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ¸ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· 88 Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… ÑÑ€Ğ»Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ñ… Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ, Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. MAS-Bench Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ» Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¾Ğº Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Efficiency: Evaluating Hybrid Agents with MAS-Bench', 'desc': 'MAS-Bench is a benchmarking framework designed to evaluate hybrid agents that combine graphical user interface (GUI) operations with shortcut methods on mobile devices. It addresses the need for a systematic way to assess these agents, which can autonomously generate shortcuts to improve task efficiency. The framework includes 139 complex tasks from real-world applications and evaluates agents based on their ability to utilize predefined shortcuts and create new, efficient workflows. Results indicate that hybrid agents outperform traditional GUI-only agents in both success rates and efficiency, highlighting the potential of this approach in enhancing intelligent agent performance.'}, 'zh': {'title': 'æ··åˆä»£ç†ï¼šæå‡ç§»åŠ¨è®¾å¤‡æ“ä½œæ•ˆç‡çš„æœªæ¥', 'desc': 'MAS-Benchæ˜¯ä¸€ä¸ªè¯„ä¼°ç§»åŠ¨è®¾å¤‡ä¸ŠGUI-å¿«æ·é”®æ··åˆä»£ç†çš„åŸºå‡†æ¡†æ¶ï¼Œå±•ç¤ºäº†å…¶åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šä¼˜äºä»…ä½¿ç”¨GUIçš„ä»£ç†ã€‚è¯¥æ¡†æ¶ä¸ä»…ä½¿ç”¨é¢„å®šä¹‰çš„å¿«æ·é”®ï¼Œè¿˜è¯„ä¼°ä»£ç†è‡ªä¸»ç”Ÿæˆå¿«æ·é”®çš„èƒ½åŠ›ï¼Œå‘ç°å’Œåˆ›å»ºå¯é‡ç”¨çš„ä½æˆæœ¬å·¥ä½œæµç¨‹ã€‚MAS-BenchåŒ…å«139ä¸ªå¤æ‚ä»»åŠ¡ï¼Œæ¶µç›–11ä¸ªçœŸå®åº”ç”¨ç¨‹åºï¼Œå¹¶æä¾›88ä¸ªé¢„å®šä¹‰å¿«æ·é”®çš„çŸ¥è¯†åº“å’Œ7ä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ··åˆä»£ç†çš„æˆåŠŸç‡å’Œæ•ˆç‡æ˜¾è‘—é«˜äºä»…ä½¿ç”¨GUIçš„ä»£ç†ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•åœ¨è¯„ä¼°ä»£ç†å¿«æ·é”®ç”Ÿæˆèƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06809', 'title': 'Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem', 'url': 'https://huggingface.co/papers/2509.06809', 'abstract': 'A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs\' mathematical reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover\'s saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1', 'score': 1, 'issue_id': 5789, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'e434898919e6911d', 'authors': ['Valentin Quesnel', 'Damien Sileo'], 'affiliations': ['Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France'], 'pdf_title_img': 'assets/pdf/title_img/2509.06809.jpg', 'data': {'categories': ['#data', '#open_source', '#dataset', '#math', '#reasoning', '#training'], 'emoji': 'ğŸ§®', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ˜Ğ˜', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. Ğ¦ĞµĞ»ÑŒ - ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ°ÑÑ‹Ñ‰ĞµĞ½Ğ¸Ñ E-prover Ğ½Ğ° Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞµ Ğ°ĞºÑĞ¸Ğ¾Ğ¼ TPTP Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ñ€Ğ¿ÑƒÑĞ° Ñ‚ĞµĞ¾Ñ€ĞµĞ¼. ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ² Ñ‚Ñ€Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ñ, Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»Ğ¾Ğº Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ².'}, 'en': {'title': 'Empowering LLMs with Valid Theorems for Better Reasoning', 'desc': "This paper presents a framework that generates a large set of valid mathematical theorems using automated theorem proving, which serves as symbolic training data for enhancing the mathematical reasoning capabilities of Large Language Models (LLMs). The authors address the issue of limited high-quality data by utilizing the E-prover's saturation abilities on the TPTP axiom library, ensuring that the generated theorems are logically sound. The framework operates by saturating axioms, filtering for interesting theorems, and creating specific tasks without involving LLMs, thus eliminating factual inaccuracies. The resulting symbolic data is used to create three types of challenges, revealing that current LLMs struggle with tasks that require deep structural reasoning, highlighting the need for better training resources."}, 'zh': {'title': 'è‡ªåŠ¨å®šç†è¯æ˜åŠ©åŠ›LLMsæ•°å­¦æ¨ç†æå‡', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡è‡ªåŠ¨å®šç†è¯æ˜ç”Ÿæˆå¤§é‡æœ‰æ•ˆçš„å®šç†ï¼Œä»¥åˆ›å»ºç¬¦å·è®­ç»ƒæ•°æ®ï¼Œä»è€Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨E-proverçš„é¥±å’Œèƒ½åŠ›ï¼Œç»“åˆTPTPå…¬ç†åº“ï¼Œç”Ÿæˆä¿è¯æœ‰æ•ˆçš„å®šç†è¯­æ–™åº“ï¼Œé¿å…äº†ä¾èµ–æ˜“å‡ºé”™çš„LLMsæˆ–å¤æ‚çš„è¯æ˜åŠ©æ‰‹è¯­æ³•ã€‚è¯¥æ¡†æ¶çš„æµç¨‹ç®€å•æ˜äº†ï¼šé¥±å’Œå…¬ç†ã€ç­›é€‰â€œæœ‰è¶£â€çš„å®šç†å¹¶ç”Ÿæˆä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå½“å‰æ¨¡å‹åœ¨éœ€è¦æ·±å±‚ç»“æ„æ¨ç†çš„ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œè€Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä½œä¸ºè¯Šæ–­å·¥å…·ï¼Œå¸®åŠ©å¡«è¡¥è¿™ä¸€å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.06285', 'title': 'DCReg: Decoupled Characterization for Efficient Degenerate LiDAR\n  Registration', 'url': 'https://huggingface.co/papers/2509.06285', 'abstract': 'DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.  \t\t\t\t\tAI-generated summary \t\t\t\t LiDAR point cloud registration is fundamental to robotic perception and navigation. However, in geometrically degenerate or narrow environments, registration problems become ill-conditioned, leading to unstable solutions and degraded accuracy. While existing approaches attempt to handle these issues, they fail to address the core challenge: accurately detection, interpret, and resolve this ill-conditioning, leading to missed detections or corrupted solutions. In this study, we introduce DCReg, a principled framework that systematically addresses the ill-conditioned registration problems through three integrated innovations. First, DCReg achieves reliable ill-conditioning detection by employing a Schur complement decomposition to the hessian matrix. This technique decouples the registration problem into clean rotational and translational subspaces, eliminating coupling effects that mask degeneracy patterns in conventional analyses. Second, within these cleanly subspaces, we develop quantitative characterization techniques that establish explicit mappings between mathematical eigenspaces and physical motion directions, providing actionable insights about which specific motions lack constraints. Finally, leveraging this clean subspace, we design a targeted mitigation strategy: a novel preconditioner that selectively stabilizes only the identified ill-conditioned directions while preserving all well-constrained information in observable space. This enables efficient and robust optimization via the Preconditioned Conjugate Gradient method with a single physical interpretable parameter. Extensive experiments demonstrate DCReg achieves at least 20% - 50% improvement in localization accuracy and 5-100 times speedup over state-of-the-art methods across diverse environments. Our implementation will be available at https://github.com/JokerJohn/DCReg.', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-08', 'pub_date_card': {'ru': '8 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 8', 'zh': '9æœˆ8æ—¥'}, 'hash': 'be54fd7fc77a2663', 'authors': ['Xiangcheng Hu', 'Xieyuanli Chen', 'Mingkai Jia', 'Jin Wu', 'Ping Tan', 'Steven L. Waslander'], 'affiliations': ['Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China', 'Department of Mechanical Engineering at National University of Defense Technology, Changsha, Hunan', 'School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing, China', 'University of Toronto Institute for Aerospace Studies and the University of Toronto Robotics Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.06285.jpg', 'data': {'categories': ['#3d', '#robotics'], 'emoji': 'ğŸš—', 'ru': {'title': 'DCReg: ĞĞ°Ğ´ĞµĞ¶Ğ½Ğ°Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº LiDAR Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…', 'desc': 'DCReg - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±Ğ»Ğ°ĞºĞ¾Ğ² Ñ‚Ğ¾Ñ‡ĞµĞº LiDAR Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¨ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. DCReg Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'DCReg: Revolutionizing LiDAR Point Cloud Registration with Smart Stability', 'desc': 'DCReg is a new framework designed to improve the registration of LiDAR point clouds, especially in challenging environments where traditional methods struggle. It uses Schur complement decomposition to break down the registration problem, allowing for better detection and understanding of ill-conditioning issues. By characterizing the relationship between mathematical eigenspaces and physical motion, DCReg identifies which movements are poorly constrained. Finally, it introduces a novel preconditioner that stabilizes only the problematic directions, leading to significant improvements in accuracy and speed during optimization.'}, 'zh': {'title': 'DCRegï¼šè§£å†³ç—…æ€é…å‡†é—®é¢˜çš„åˆ›æ–°æ¡†æ¶', 'desc': 'DCReg æ˜¯ä¸€ç§é’ˆå¯¹ LiDAR ç‚¹äº‘é…å‡†ä¸­ç—…æ€é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒé€šè¿‡æ–½å°”è¡¥åˆ†è§£å’Œæ–°å‹é¢„å¤„ç†å™¨æ¥æ£€æµ‹ã€è¡¨å¾å’Œç¼“è§£è¿™äº›ç—…æ€ç°è±¡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å°†é…å‡†é—®é¢˜åˆ†è§£ä¸ºå¹²å‡€çš„æ—‹è½¬å’Œä½ç§»å­ç©ºé—´ï¼Œä»è€Œæ¶ˆé™¤ä¼ ç»Ÿåˆ†æä¸­çš„è€¦åˆæ•ˆåº”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDCReg åœ¨å®šä½ç²¾åº¦ä¸Šæé«˜äº† 20% è‡³ 50%ï¼Œå¹¶åœ¨é€Ÿåº¦ä¸Šæ¯”ç°æœ‰æ–¹æ³•å¿« 5 åˆ° 100 å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.04582', 'title': 'Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping', 'url': 'https://huggingface.co/papers/2509.04582', 'abstract': 'Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Drag-based image editing has emerged as a powerful paradigm for intuitive image manipulation. However, existing approaches predominantly rely on manipulating the latent space of generative models, leading to limited precision, delayed feedback, and model-specific constraints. Accordingly, we present Inpaint4Drag, a novel framework that decomposes drag-based editing into pixel-space bidirectional warping and image inpainting. Inspired by elastic object deformation in the physical world, we treat image regions as deformable materials that maintain natural shape under user manipulation. Our method achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at 512x512 resolution, significantly improving the interaction experience compared to existing methods that require minutes per edit. By transforming drag inputs directly into standard inpainting formats, our approach serves as a universal adapter for any inpainting model without architecture modification, automatically inheriting all future improvements in inpainting technology. Extensive experiments demonstrate that our method achieves superior visual quality and precise control while maintaining real-time performance. Project page: https://visual-ai.github.io/inpaint4drag/', 'score': 1, 'issue_id': 5795, 'pub_date': '2025-09-04', 'pub_date_card': {'ru': '4 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 4', 'zh': '9æœˆ4æ—¥'}, 'hash': 'e61daa418b9bffa3', 'authors': ['Jingyi Lu', 'Kai Han'], 'affiliations': ['Visual AI Lab, The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2509.04582.jpg', 'data': {'categories': ['#cv', '#video'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³', 'desc': 'Inpaint4Drag - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ°ÑĞºĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ½Ğ° Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ»Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑƒÑ‡Ğ°ÑÑ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğº Ğ´ĞµÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹. Inpaint4Drag ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸Ğ½Ğ¿ĞµĞ¹Ğ½Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ² ÑÑ‚Ğ¾Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸.'}, 'en': {'title': 'Real-Time Image Editing with Natural Precision', 'desc': "Inpaint4Drag is a new framework that improves drag-based image editing by breaking it down into two main processes: pixel-space warping and inpainting. This method allows for real-time editing with quick feedback, achieving warping in just 0.01 seconds and inpainting in 0.3 seconds at a resolution of 512x512. By treating image regions like flexible materials, it ensures that the shapes remain natural during user manipulation. Additionally, Inpaint4Drag can work with any inpainting model without needing changes to the model's architecture, making it adaptable to future advancements in inpainting technology."}, 'zh': {'title': 'å®æ—¶å›¾åƒç¼–è¾‘çš„æ–°é©å‘½', 'desc': 'Inpaint4Drag æ˜¯ä¸€ç§å¢å¼ºæ‹–æ‹½å¼å›¾åƒç¼–è¾‘çš„æ–°æ¡†æ¶ï¼Œå®ƒå°†ç¼–è¾‘è¿‡ç¨‹åˆ†è§£ä¸ºåƒç´ ç©ºé—´çš„åŒå‘å˜å½¢å’Œå›¾åƒä¿®å¤ã€‚è¯¥æ–¹æ³•çµæ„Ÿæ¥æºäºç‰©ç†ä¸–ç•Œä¸­çš„å¼¹æ€§ç‰©ä½“å˜å½¢ï¼Œå°†å›¾åƒåŒºåŸŸè§†ä¸ºå¯å˜å½¢ææ–™ï¼Œèƒ½å¤Ÿåœ¨ç”¨æˆ·æ“ä½œä¸‹ä¿æŒè‡ªç„¶å½¢çŠ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒInpaint4Drag å®ç°äº†å®æ—¶çš„å˜å½¢é¢„è§ˆå’Œé«˜æ•ˆçš„å›¾åƒä¿®å¤ï¼Œå¤§å¤§æå‡äº†ç”¨æˆ·çš„äº¤äº’ä½“éªŒã€‚è¯¥æ¡†æ¶å¯ä»¥ä½œä¸ºä»»ä½•å›¾åƒä¿®å¤æ¨¡å‹çš„é€šç”¨é€‚é…å™¨ï¼Œè‡ªåŠ¨ç»§æ‰¿æœªæ¥çš„ä¿®å¤æŠ€æœ¯æ”¹è¿›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.03740', 'title': 'Singular Value Few-shot Adaptation of Vision-Language Models', 'url': 'https://huggingface.co/papers/2509.03740', 'abstract': "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and few-shot learning capabilities across diverse applications. However, adapting these models to new fine-grained domains remains difficult due to reliance on prompt engineering and the high cost of full model fine-tuning. Existing adaptation approaches rely on augmented components, such as prompt tokens and adapter modules, which could limit adaptation quality, destabilize the model, and compromise the rich knowledge learned during pretraining. In this work, we present CLIP-SVD, a novel multi-modal and parameter-efficient adaptation technique that leverages Singular Value Decomposition (SVD) to modify the internal parameter space of CLIP without injecting additional modules. Specifically, we fine-tune only the singular values of the CLIP parameter matrices to rescale the basis vectors for domain adaptation while retaining the pretrained model. This design enables enhanced adaptation performance using only 0.04\\% of the model's total parameters and better preservation of its generalization ability. CLIP-SVD achieves state-of-the-art classification results on 11 natural and 10 biomedical datasets, outperforming previous methods in both accuracy and generalization under few-shot settings. Additionally, we leverage a natural language-based approach to analyze the effectiveness and dynamics of the CLIP adaptation to allow interpretability of CLIP-SVD. The code is publicly available at https://github.com/HealthX-Lab/CLIP-SVD.", 'score': 1, 'issue_id': 5797, 'pub_date': '2025-09-03', 'pub_date_card': {'ru': '3 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 3', 'zh': '9æœˆ3æ—¥'}, 'hash': '1f7bd75ad15dda94', 'authors': ['Taha Koleilat', 'Hassan Rivaz', 'Yiming Xiao'], 'affiliations': ['Department of Computer Science & Software Engineering, Concordia University, Montreal, Canada', 'Department of Electrical & Computer Engineering, Concordia University, Montreal, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2509.03740.jpg', 'data': {'categories': ['#training', '#interpretability', '#healthcare', '#open_source', '#multimodal', '#transfer_learning'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ CLIP Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLIP-SVD - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° ÑĞ¸Ğ½Ğ³ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¸ (SVD). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLIP Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ğ¼, Ğ¸Ğ·Ğ¼ĞµĞ½ÑÑ Ğ»Ğ¸ÑˆÑŒ 0.04% Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². CLIP-SVD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° 21 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸, Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ÑƒĞ»ĞµĞ¹ Ğ² Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ.'}, 'en': {'title': 'Efficient Domain Adaptation with CLIP-SVD', 'desc': "This paper introduces CLIP-SVD, a new method for adapting vision-language models like CLIP to specific domains without the need for extensive fine-tuning or additional components. By using Singular Value Decomposition (SVD), the approach modifies the model's internal parameters efficiently, only adjusting a small fraction of the total parameters. This technique not only improves adaptation performance but also maintains the model's ability to generalize well to new tasks. The results show that CLIP-SVD achieves superior classification accuracy on various datasets, demonstrating its effectiveness in few-shot learning scenarios."}, 'zh': {'title': 'CLIP-SVDï¼šé«˜æ•ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹é€‚åº”æŠ€æœ¯', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€é€‚åº”æŠ€æœ¯CLIP-SVDï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨ç»†ç²’åº¦é¢†åŸŸçš„é€‚åº”èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰æ¥è°ƒæ•´CLIPæ¨¡å‹çš„å†…éƒ¨å‚æ•°ç©ºé—´ï¼Œè€Œæ— éœ€æ·»åŠ é¢å¤–æ¨¡å—ã€‚é€šè¿‡ä»…å¾®è°ƒCLIPå‚æ•°çŸ©é˜µçš„å¥‡å¼‚å€¼ï¼ŒCLIP-SVDèƒ½å¤Ÿåœ¨ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„åŒæ—¶å®ç°æ›´å¥½çš„é¢†åŸŸé€‚åº”æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIP-SVDåœ¨11ä¸ªè‡ªç„¶æ•°æ®é›†å’Œ10ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„åˆ†ç±»æ•ˆæœï¼Œä¸”åœ¨å°‘æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.00328', 'title': 'Mechanistic interpretability for steering vision-language-action models', 'url': 'https://huggingface.co/papers/2509.00328', 'abstract': 'A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control - establishing a new paradigm for transparent and steerable foundation models in robotics.', 'score': 1, 'issue_id': 5798, 'pub_date': '2025-08-30', 'pub_date_card': {'ru': '30 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 30', 'zh': '8æœˆ30æ—¥'}, 'hash': 'be917881399e9b7f', 'authors': ['Bear HÃ¤on', 'Kaylene Stocking', 'Ian Chuang', 'Claire Tomlin'], 'affiliations': ['Department of Electrical Engineering and Computer Sciences, University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.00328.jpg', 'data': {'categories': ['#open_source', '#multimodal', '#agents', '#agi', '#inference', '#interpretability', '#robotics'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸ĞºÑƒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Vision-Language-Action (VLA) Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ñ… Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ¿Ñ€Ğ¾ĞµÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ğ² ÑĞ»Ğ¾ÑÑ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ğ±Ğ°Ğ·Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Pi0 Ğ¸ OpenVLA Ğ² ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğµ UR5.'}, 'en': {'title': 'Steering Robots with Insight: Real-Time Control of VLA Models', 'desc': "This paper presents a new framework for interpreting and controlling Vision-Language-Action (VLA) models, which are designed to help robots understand and perform tasks. The framework allows for real-time adjustments to the model's behavior without needing to retrain it or interact with the environment. By analyzing the internal representations of the VLA models, the authors identify key semantic directions that influence action choices, such as speed and direction. This approach enables effective steering of robot actions in both simulations and real-world applications, paving the way for more transparent and adaptable robotic systems."}, 'zh': {'title': 'å®æ—¶æ§åˆ¶è§†è§‰-è¯­è¨€-è¡ŒåŠ¨æ¨¡å‹çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç”¨äºé€šè¿‡å†…éƒ¨è¡¨ç¤ºè§£é‡Šå’Œå¼•å¯¼è§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹ï¼Œä»è€Œå®ç°å®æ—¶è¡Œä¸ºæ§åˆ¶ï¼Œè€Œæ— éœ€å¾®è°ƒæˆ–ä¸ç¯å¢ƒäº¤äº’ã€‚VLAæ¨¡å‹æœ‰æ½œåŠ›æˆä¸ºé€šç”¨çš„å…·èº«æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿå¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡å’Œç¯å¢ƒï¼Œä½†ç›®å‰çš„è§£é‡Šå’Œå¼•å¯¼æ–¹æ³•è¿œä¸å¦‚ä¼ ç»Ÿæœºå™¨äººç®¡é“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å˜æ¢å™¨å±‚ä¸­çš„å‰é¦ˆæ¿€æ´»è¿›è¡ŒæŠ•å½±ï¼Œè¯†åˆ«å‡ºä¸åŠ¨ä½œé€‰æ‹©å› æœç›¸å…³çš„ç¨€ç–è¯­ä¹‰æ–¹å‘ï¼Œå¦‚é€Ÿåº¦å’Œæ–¹å‘ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„æ¿€æ´»å¼•å¯¼æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å¾®è°ƒæˆ–å¥–åŠ±ä¿¡å·çš„æƒ…å†µä¸‹å®æ—¶è°ƒèŠ‚è¡Œä¸ºã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (7)', '#agi (3)', '#alignment (1)', '#architecture (4)', '#audio (1)', '#benchmark (8)', '#cv (6)', '#data (1)', '#dataset (7)', '#diffusion (1)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (2)', '#healthcare (2)', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (9)', '#open_source (9)', '#optimization (8)', '#plp', '#rag (1)', '#reasoning (12)', '#rl (6)', '#rlhf', '#robotics (2)', '#science (2)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (12)', '#transfer_learning (3)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-09-09 22:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-09 22:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-09 22:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    