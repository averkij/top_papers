
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 8 papers. May 6.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">6 мая</span> | <span id="title-articles-count">8 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-05.html">⬅️ <span id="prev-date">05.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-07.html">➡️ <span id="next-date">07.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '6 мая', 'en': 'May 6', 'zh': '5月6日'};
        let feedDateNext = {'ru': '07.05', 'en': '05/07', 'zh': '5月7日'};
        let feedDatePrev = {'ru': '05.05', 'en': '05/05', 'zh': '5月5日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.02835', 'title': 'R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2505.02835', 'abstract': "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", 'score': 8, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5134d59b0389ade9', 'authors': ['Yi-Fan Zhang', 'Xingyu Lu', 'Xiao Hu', 'Chaoyou Fu', 'Bin Wen', 'Tianke Zhang', 'Changyi Liu', 'Kaiyu Jiang', 'Kaibing Chen', 'Kaiyu Tang', 'Haojie Ding', 'Jiankang Chen', 'Fan Yang', 'Zhang Zhang', 'Tingting Gao', 'Liang Wang'], 'affiliations': ['CASIA', 'KuaiShou', 'NJU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2505.02835.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#training', '#multimodal', '#rl', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'Стабильное обучение с подкреплением для улучшения мультимодальных моделей вознаграждения', 'desc': 'Эта статья представляет новый подход к улучшению мультимодальных моделей вознаграждения (MRM) с использованием обучения с подкреплением (RL). Авторы предлагают алгоритм StableReinforce, который решает проблемы нестабильности, возникающие при применении существующих RL-алгоритмов к моделированию вознаграждений. Они собрали набор данных из 200 тысяч примеров предпочтений для обучения MRM. Их модель R1-Reward, обученная с помощью StableReinforce, значительно превосходит предыдущие модели на эталонных тестах мультимодального моделирования вознаграждений.'}, 'en': {'title': 'Stable Reinforcement for Enhanced Multimodal Reward Modeling', 'desc': 'This paper investigates the use of Reinforcement Learning (RL) to enhance Multimodal Reward Models (MRMs) for Multimodal Large Language Models (MLLMs). It identifies challenges in applying traditional RL algorithms to reward modeling, which can lead to instability during training. To overcome these issues, the authors introduce the StableReinforce algorithm, which improves training dynamics through better loss refinement and reward design. The results show that their reward model, R1-Reward, outperforms existing state-of-the-art models on multimodal benchmarks, demonstrating the effectiveness of their approach.'}, 'zh': {'title': '稳定强化学习提升多模态奖励模型性能', 'desc': '多模态奖励模型（MRMs）在提升多模态大型语言模型（MLLMs）的性能中起着重要作用。本文探讨了如何利用强化学习（RL）来改善奖励建模，提出将奖励建模问题重新表述为基于规则的RL任务。我们提出的StableReinforce算法通过优化训练损失、优势估计策略和奖励设计，解决了现有RL算法在奖励建模中导致的不稳定性问题。经过StableReinforce算法训练的奖励模型R1-Reward在多模态奖励建模基准上显著提高了性能，展示了RL算法在优化MRMs中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.02735', 'title': 'FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language\n  Models', 'url': 'https://huggingface.co/papers/2505.02735', 'abstract': 'Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.', 'score': 7, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '1ffa5567eb2f4acc', 'authors': ['Zhouliang Yu', 'Ruotian Peng', 'Keyi Ding', 'Yizhe Li', 'Zhongyuan Peng', 'Minghao Liu', 'Yifan Zhang', 'Zheng Yuan', 'Huajian Xin', 'Wenhao Huang', 'Yandong Wen', 'Ge Zhang', 'Weiyang Liu'], 'affiliations': ['M-A-P', 'Max Planck Institute for Intelligent Systems, Tübingen', 'Numina', 'The Chinese University of Hong Kong', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2505.02735.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#math', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'FormalMATH: Новый рубеж в формальном математическом рассуждении для ИИ', 'desc': 'FormalMATH - это масштабный бенчмарк на Lean4, содержащий 5560 формально верифицированных задач от олимпиадного до университетского уровня. Авторы представляют новый полуавтоматический конвейер для формализации, использующий специализированные языковые модели (LLM) и стратегии фильтрации. Оценка современных LLM-доказателей теорем показала их ограниченность, с максимальным успехом в 16.46% при практических ограничениях. Бенчмарк выявил обратную зависимость между наличием решения на естественном языке и успехом доказательства в сценариях рассуждений по цепочке мыслей.'}, 'en': {'title': 'FormalMATH: Advancing AI in Formal Mathematical Reasoning', 'desc': 'The paper introduces FormalMATH, a comprehensive benchmark designed to enhance formal mathematical reasoning in AI. It consists of 5,560 verified problems that cover a wide range of mathematical topics, from high school to undergraduate level. The authors propose a human-in-the-loop autoformalization pipeline that utilizes large language models (LLMs) for automating the formalization process, significantly reducing the need for expert input. The study also highlights the limitations of current LLM-based theorem provers, revealing their low success rates and biases across different mathematical domains.'}, 'zh': {'title': 'FormalMATH：推动正式数学推理的基准', 'desc': '本文提出了FormalMATH，这是一个大规模的Lean4基准，包含5560个经过正式验证的数学问题，涵盖从高中奥林匹克挑战到本科定理的多个领域。为了解决手动形式化的低效问题，我们引入了一种新的人机协作自动形式化流程，结合了专门的大型语言模型（LLMs）进行语句自动形式化、多LLM语义验证和基于否定的反驳过滤策略。我们的评估显示，当前最先进的LLM定理证明器在实际采样预算下的成功率仅为16.46%，并且在不同领域表现出明显的偏差。我们认为，FormalMATH为正式数学推理提供了一个强有力的基准。'}}}, {'id': 'https://huggingface.co/papers/2505.02156', 'title': 'Think on your Feet: Adaptive Thinking via Reinforcement Learning for\n  Social Agents', 'url': 'https://huggingface.co/papers/2505.02156', 'abstract': "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction rightarrow deep contemplation) based on real-time context. Our framework's core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach", 'score': 6, 'issue_id': 3602, 'pub_date': '2025-05-04', 'pub_date_card': {'ru': '4 мая', 'en': 'May 4', 'zh': '5月4日'}, 'hash': '13e5cf390c136aeb', 'authors': ['Minzheng Wang', 'Yongbin Li', 'Haobo Wang', 'Xinghua Zhang', 'Nan Xu', 'Bingli Wu', 'Fei Huang', 'Haiyang Yu', 'Wenji Mao'], 'affiliations': ['MAIS, Institute of Automation, Chinese Academy of Sciences', 'Peking University', 'School of Artificial Intelligence, University of Chinese Academy of Sciences', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.02156.jpg', 'data': {'categories': ['#reasoning', '#agents', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Адаптивное обучение режимам мышления для улучшения социального интеллекта ИИ', 'desc': 'Статья представляет новый метод машинного обучения под названием Adaptive Mode Learning (AML) для улучшения социального интеллекта языковых агентов. AML использует алгоритм Adaptive Mode Policy Optimization (AMPO), который динамически выбирает один из четырех режимов мышления в зависимости от контекста. Этот подход позволяет агентам адаптивно регулировать глубину рассуждений, что приводит к более эффективному использованию токенов и улучшенной производительности на задачах социального интеллекта. Эксперименты показывают, что AML превосходит современные методы, достигая на 15.6% более высокой производительности при использовании на 32.8% более коротких цепочек рассуждений.'}, 'en': {'title': 'Dynamic Reasoning for Smarter Social Agents', 'desc': 'This paper introduces Adaptive Mode Learning (AML), a new approach for simulating social intelligence in language agents. Unlike existing methods that either lack dynamic reasoning or use a one-size-fits-all strategy, AML allows agents to switch between four different thinking modes based on the context of the interaction. The core of this framework is the Adaptive Mode Policy Optimization (AMPO) algorithm, which enhances reasoning efficiency by adapting the depth of thought to the situation. Experimental results show that AML significantly improves task performance and reduces the length of reasoning chains compared to current state-of-the-art methods.'}, 'zh': {'title': '自适应模式学习：提升社交智能的推理能力', 'desc': '本论文提出了一种新的方法，称为自适应模式学习（AML），旨在提高社交智能模拟的有效性。现有方法在推理深度上缺乏灵活性，导致在不同场景下的推理不够高效。AML通过实时上下文动态选择四种思维模式，从直觉反应到深度思考，优化了推理过程。实验结果表明，AML在社交智能任务中比现有最先进的方法提高了15.6%的性能，同时推理链长度减少了32.8%。'}}}, {'id': 'https://huggingface.co/papers/2505.02370', 'title': 'SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based\n  Image Editing', 'url': 'https://huggingface.co/papers/2505.02370', 'abstract': 'Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.', 'score': 4, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '5833f2e06b661a76', 'authors': ['Ming Li', 'Xin Gu', 'Fan Chen', 'Xiaoying Xing', 'Longyin Wen', 'Chen Chen', 'Sijie Zhu'], 'affiliations': ['ByteDance Intelligent Creation (USA)', 'Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2505.02370.jpg', 'data': {'categories': ['#data', '#benchmark', '#dataset', '#training', '#cv'], 'emoji': '✏️', 'ru': {'title': 'Улучшение редактирования изображений через оптимизацию инструкций', 'desc': 'Статья представляет новый подход к улучшению моделей редактирования изображений на основе инструкций. Авторы предлагают метод создания более эффективных инструкций для редактирования, включая их исправление для лучшего соответствия парам оригинальных и отредактированных изображений. Они также вводят контрастные инструкции редактирования для повышения эффективности обучения. Результаты показывают значительное улучшение производительности по сравнению с существующими методами, достигая 9.19% улучшения на бенчмарке Real-Edit при использовании в 30 раз меньше данных для обучения и в 13 раз меньшем размере модели.'}, 'en': {'title': 'Enhancing Image Editing with Accurate Instructions and Contrastive Supervision', 'desc': 'This paper addresses the problem of noisy supervision in image editing tasks caused by mismatched editing instructions and image pairs. The authors propose a novel approach that involves creating more accurate editing instructions that align better with the original and edited images. They introduce contrastive editing instructions to enhance the effectiveness of these instructions and utilize a unified guide for vision-language models to rectify them. Their method improves supervision signals without relying on pre-training or VLM modules, achieving significant performance gains on benchmarks with less training data and smaller model sizes.'}, 'zh': {'title': '提升图像编辑效果的新方法', 'desc': '本论文提出了一种新方法，旨在改善图像编辑模型的监督信号。我们通过构建更有效的编辑指令，使其与原始和编辑后的图像对更好地对齐，并引入对比编辑指令来增强效果。研究发现，编辑模型在不同推理步骤中表现出特定的生成属性，这些属性与文本无关。我们的方法不依赖于之前的视觉语言模型或预训练任务，提供了一种更直接和高效的监督信号，显著提升了图像编辑的效果。'}}}, {'id': 'https://huggingface.co/papers/2505.01043', 'title': 'Low-Precision Training of Large Language Models: Methods, Challenges,\n  and Opportunities', 'url': 'https://huggingface.co/papers/2505.01043', 'abstract': 'Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.', 'score': 4, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'b67e3ec75756e896', 'authors': ['Zhiwei Hao', 'Jianyuan Guo', 'Li Shen', 'Yong Luo', 'Han Hu', 'Guoxia Wang', 'Dianhai Yu', 'Yonggang Wen', 'Dacheng Tao'], 'affiliations': ['Baidu Inc., Beijing 100000, China', 'College of Computing and Data Science, Nanyang Technological University, 639798, Singapore', 'Computer Science, City University of Hong Kong, Hong Kong 999077, China', 'School of Computer Science, Wuhan University, Wuhan 430072, China', 'School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China', 'School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.01043.jpg', 'data': {'categories': ['#optimization', '#inference', '#survey', '#training'], 'emoji': '🔬', 'ru': {'title': 'Систематизация методов обучения нейросетей с низкой точностью для повышения эффективности', 'desc': 'Статья представляет собой обзор методов обучения нейронных сетей с низкой точностью вычислений. Авторы систематизируют существующие подходы, разделяя их на три основные категории: методы на основе фиксированной точки и целых чисел, методы на основе чисел с плавающей запятой и методы с использованием специализированных форматов. В работе также обсуждаются подходы к обучению с учетом квантования и намечаются перспективные направления исследований в этой области. Обзор направлен на предоставление исследователям единого представления о ландшафте методов обучения с низкой точностью.'}, 'en': {'title': 'Optimizing Large Language Models with Low-Precision Training', 'desc': 'This paper reviews low-precision training techniques for large language models (LLMs), which help improve training efficiency while reducing hardware resource demands. It categorizes these methods into three main groups based on numerical formats: fixed-point and integer-based, floating-point-based, and customized format-based methods. The survey also addresses quantization-aware training, which is closely related to low-precision training during model inference. By organizing the existing research, the paper aims to provide clarity and direction for future advancements in low-precision training.'}, 'zh': {'title': '低精度训练：提升效率的关键', 'desc': '大型语言模型（LLMs）在多个领域取得了显著的表现，但其训练所需的硬件资源极大，成为效率和可扩展性的障碍。为了应对这一挑战，低精度训练技术被广泛采用，显著提高了训练效率。尽管如此，低精度训练涉及多个组件，如权重、激活和梯度，每个组件可以用不同的数值格式表示，导致研究领域的碎片化。本文综述了现有的低精度训练方法，并根据数值格式将其系统地分类为三大类，以便于研究者理解和参考。'}}}, {'id': 'https://huggingface.co/papers/2505.02471', 'title': 'Ming-Lite-Uni: Advancements in Unified Architecture for Natural\n  Multimodal Interaction', 'url': 'https://huggingface.co/papers/2505.02471', 'abstract': 'We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.', 'score': 2, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': 'fa1cacd261a625e3', 'authors': ['Biao Gong', 'Cheng Zou', 'Dandan Zheng', 'Hu Yu', 'Jingdong Chen', 'Jianxin Sun', 'Junbo Zhao', 'Jun Zhou', 'Kaixiang Ji', 'Lixiang Ru', 'Libin Wang', 'Qingpei Guo', 'Rui Liu', 'Weilong Chai', 'Xinyu Xiao', 'Ziyuan Huang'], 'affiliations': ['Ant Group', 'Ming-Lite-Uni'], 'pdf_title_img': 'assets/pdf/title_img/2505.02471.jpg', 'data': {'categories': ['#agi', '#multimodal', '#open_source', '#cv'], 'emoji': '🤖', 'ru': {'title': 'Объединение зрения и языка в единой мультимодальной системе', 'desc': 'Ming-Lite-Uni - это открытая мультимодальная система, объединяющая генерацию изображений и обработку естественного языка. Она использует фиксированную мультимодальную языковую модель (MLLM) и обучаемую диффузионную модель для выполнения задач генерации изображений по тексту и редактирования изображений на основе инструкций. Система вводит новые многомасштабные обучаемые токены и стратегию выравнивания многомасштабных представлений. Экспериментальные результаты демонстрируют высокую производительность Ming-Lite-Uni и впечатляющую гибкость интерактивного процесса.'}, 'en': {'title': 'Unifying Vision and Language for Advanced AI', 'desc': "Ming-Lite-Uni is an innovative open-source framework designed to integrate vision and language through a unified visual generator and a multimodal autoregressive model. It introduces advanced techniques like multi-scale learnable tokens and representation alignment to enhance the interaction between text and images. By utilizing a fixed MLLM alongside a learnable diffusion model, it supports both text-to-image generation and image editing based on instructions. The framework's strong performance and interactive capabilities highlight its potential impact on the development of advanced AI systems, contributing to the journey towards artificial general intelligence (AGI)."}, 'zh': {'title': 'Ming-Lite-Uni：统一视觉与语言的多模态框架', 'desc': 'Ming-Lite-Uni是一个开源的多模态框架，旨在统一视觉和语言。它引入了新的视觉生成器和多模态自回归模型，支持文本到图像生成和图像编辑任务。该框架采用了多尺度可学习标记和多尺度表示对齐策略，提升了模型的表现。所有代码和模型权重都是开源的，鼓励社区进一步探索。'}}}, {'id': 'https://huggingface.co/papers/2505.01583', 'title': 'TEMPURA: Temporal Event Masked Prediction and Understanding for\n  Reasoning in Action', 'url': 'https://huggingface.co/papers/2505.01583', 'abstract': 'Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.', 'score': 2, 'issue_id': 3602, 'pub_date': '2025-05-02', 'pub_date_card': {'ru': '2 мая', 'en': 'May 2', 'zh': '5月2日'}, 'hash': 'd29565337415d11f', 'authors': ['Jen-Hao Cheng', 'Vivian Wang', 'Huayu Wang', 'Huapeng Zhou', 'Yi-Hao Peng', 'Hou-I Liu', 'Hsiang-Wei Huang', 'Kuang-Ming Chen', 'Cheng-Yen Yang', 'Wenhao Chai', 'Yi-Ling Chen', 'Vibhav Vineet', 'Qin Cai', 'Jenq-Neng Hwang'], 'affiliations': ['Carnegie Mellon University', 'Microsoft', 'National Yang Ming Chiao Tung University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.01583.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#video', '#benchmark'], 'emoji': '🎬', 'ru': {'title': 'TEMPURA: Улучшение понимания причинно-следственных связей в видео с помощью маскированного предсказания событий', 'desc': 'TEMPURA - это двухэтапная система обучения для улучшения понимания временных событий в видео. На первом этапе она использует маскированное предсказание событий для реконструкции пропущенных событий и генерации пошаговых причинно-следственных объяснений. Затем TEMPURA учится сегментировать видео и создавать подробные описания для каждого сегмента. Система обучается на большом наборе данных VER, содержащем 1 млн обучающих примеров и 500 тыс. видео с временными метками событий. Эксперименты показывают, что TEMPURA превосходит базовые модели в задачах временной локализации и обнаружения ключевых моментов.'}, 'en': {'title': 'TEMPURA: Enhancing Video Understanding through Causal Reasoning and Temporal Segmentation', 'desc': 'This paper introduces TEMPURA, a novel framework designed to improve how vision-language models understand the timing and relationships of events in videos. It addresses the limitations of existing methods that either lose important details by compressing video data or treat videos as continuous streams without clear boundaries. TEMPURA employs a two-stage training process that first predicts missing events to create causal explanations and then segments videos into distinct events with precise descriptions. The framework is trained on a large dataset, VER, and shows significant improvements in tasks like temporal grounding and highlight detection compared to existing models.'}, 'zh': {'title': 'TEMPURA：提升视频理解的因果推理与时间分割结合', 'desc': '本论文提出了一种名为TEMPURA的框架，用于提高视频的时间理解能力。TEMPURA通过掩蔽事件预测推理，重建缺失事件并生成逐步的因果解释，从而更好地理解视频中的事件关系。接着，它学习视频分割和密集标注，将视频分解为不重叠的事件，并提供详细的时间戳对齐描述。实验结果表明，TEMPURA在时间定位和高亮检测基准测试中优于现有的强基线模型，验证了因果推理与细粒度时间分割的结合能够提升视频理解。'}}}, {'id': 'https://huggingface.co/papers/2505.02625', 'title': 'LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive\n  Streaming Speech Synthesis', 'url': 'https://huggingface.co/papers/2505.02625', 'abstract': 'Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.', 'score': 1, 'issue_id': 3602, 'pub_date': '2025-05-05', 'pub_date_card': {'ru': '5 мая', 'en': 'May 5', 'zh': '5月5日'}, 'hash': '2f8233e0d3083036', 'authors': ['Qingkai Fang', 'Yan Zhou', 'Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'], 'affiliations': ['Key Laboratory of AI Safety, Chinese Academy of Sciences', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.02625.jpg', 'data': {'categories': ['#audio', '#small_models', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'Прорыв в разговорном ИИ: компактная модель превосходит гигантов', 'desc': 'В статье представлена модель LLaMA-Omni 2 - серия речевых языковых моделей для естественного голосового взаимодействия. Модель основана на архитектуре Qwen2.5 и дополнена речевым энкодером и декодером. Несмотря на небольшой объем обучающих данных, LLaMA-Omni 2 показывает высокие результаты в задачах голосового вопросно-ответного взаимодействия. Модель превосходит предыдущие аналоги, обученные на гораздо большем объеме речевых данных.'}, 'en': {'title': 'Revolutionizing Speech Interaction with LLaMA-Omni 2', 'desc': 'This paper presents LLaMA-Omni 2, a new series of speech language models designed for real-time speech interaction. These models, ranging from 0.5B to 14B parameters, utilize a speech encoder and an autoregressive streaming speech decoder to facilitate intelligent spoken chatbots. Remarkably, LLaMA-Omni 2 achieves high performance despite being trained on only 200K multi-turn speech dialogue samples. It outperforms previous models like GLM-4-Voice, which were trained on significantly larger datasets, showcasing the efficiency of the new architecture.'}, 'zh': {'title': '实时智能语音交互的新突破', 'desc': '本论文介绍了LLaMA-Omni 2，这是一个系列的语音语言模型，参数范围从0.5亿到14亿，能够实现高质量的实时语音交互。该模型基于Qwen2.5系列，结合了语音编码器和自回归流式语音解码器。尽管仅在20万多轮语音对话样本上进行训练，LLaMA-Omni 2在多个语音问答和语音指令跟随基准测试中表现出色，超越了之前的最先进模型GLM-4-Voice。这表明，使用较少的数据也能训练出高效的智能语音聊天机器人。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment', '#architecture', '#audio (1)', '#benchmark (5)', '#cv (2)', '#data (1)', '#dataset (3)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability', '#leakage', '#long_context', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (2)', '#open_source (1)', '#optimization (3)', '#plp', '#rag', '#reasoning (4)', '#rl (1)', '#rlhf', '#robotics', '#science', '#security', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic', '#training (4)', '#transfer_learning', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-06 03:35',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-06 03:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-06 03:35')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    