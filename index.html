
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 35 papers. September 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ</span> | <span id="title-articles-count">35 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-09-26.html">â¬…ï¸ <span id="prev-date">26.09</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-09-30.html">â¡ï¸ <span id="next-date">30.09</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-09.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 29', 'zh': '9æœˆ29æ—¥'};
        let feedDateNext = {'ru': '30.09', 'en': '09/30', 'zh': '9æœˆ30æ—¥'};
        let feedDatePrev = {'ru': '26.09', 'en': '09/26', 'zh': '9æœˆ26æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2509.19803', 'title': 'VCRL: Variance-based Curriculum Reinforcement Learning for Large\n  Language Models', 'url': 'https://huggingface.co/papers/2509.19803', 'abstract': "A curriculum reinforcement learning framework dynamically adjusts training sample difficulty based on reward variance, improving LLM performance on mathematical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.", 'score': 111, 'issue_id': 6099, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '28f78fd1381a5d22', 'authors': ['Guochao Jiang', 'Wenfeng Feng', 'Guofeng Quan', 'Chuzhan Hao', 'Yuewei Zhang', 'Guohua Liu', 'Hao Wang'], 'affiliations': ['Alibaba Cloud Computing'], 'pdf_title_img': 'assets/pdf/title_img/2509.19803.jpg', 'data': {'categories': ['#math', '#training', '#rl', '#optimization', '#reasoning'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ: Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ VCRL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ curriculum reinforcement learning. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ³Ğ¾ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¼Ñƒ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ»ĞµĞ³ĞºĞ¸Ğµ Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½Ğ¸Ğ·ĞºÑƒÑ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ, Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ - Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¿ÑÑ‚Ğ¸ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… benchmarks Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° VCRL Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ reinforcement learning Ğ´Ğ»Ñ LLM.'}, 'en': {'title': 'Dynamic Difficulty for Smarter Learning in LLMs', 'desc': 'This paper introduces a new framework called VCRL, which stands for Variance-Controlled Reinforcement Learning, aimed at enhancing the performance of large language models (LLMs) in mathematical reasoning tasks. The framework adjusts the difficulty of training samples based on the variance of rewards received during training, aligning with how humans typically learn from easier to harder problems. By focusing on samples with moderate difficulty, which show higher reward variance, VCRL helps LLMs learn more effectively. Experiments demonstrate that VCRL outperforms existing reinforcement learning methods in improving LLM capabilities on various mathematical benchmarks.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´æ ·æœ¬éš¾åº¦ï¼Œæå‡æ•°å­¦æ¨ç†èƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç§°ä¸ºVCRLï¼Œæ—¨åœ¨æ ¹æ®å¥–åŠ±æ–¹å·®åŠ¨æ€è°ƒæ•´è®­ç»ƒæ ·æœ¬çš„éš¾åº¦ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç°æœ‰çš„åŸºäºå›åˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘LLMå¯¹ä¸åŒéš¾åº¦æ ·æœ¬çš„å­¦ä¹ èƒ½åŠ›ï¼Œè¿™ä¸äººç±»åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶ä»æ˜“åˆ°éš¾çš„è®¤çŸ¥è¿‡ç¨‹ç›¸æ‚–ã€‚ç ”ç©¶å‘ç°ï¼Œå›åˆç»„å¥–åŠ±çš„æ–¹å·®å¯ä»¥éƒ¨åˆ†åæ˜ å½“å‰æ ·æœ¬çš„éš¾åº¦ï¼Œé€‚ä¸­éš¾åº¦çš„æ ·æœ¬å…·æœ‰è¾ƒé«˜çš„æ–¹å·®ï¼Œè€Œè¿‡äºç®€å•æˆ–å›°éš¾çš„æ ·æœ¬åˆ™æ–¹å·®è¾ƒä½ã€‚é€šè¿‡åœ¨äº”ä¸ªæ•°å­¦åŸºå‡†å’Œä¸¤ä¸ªæ¨¡å‹ä¸Šçš„å®éªŒï¼ŒVCRLæ˜¾ç¤ºå‡ºç›¸è¾ƒäºç°æœ‰LLMå¼ºåŒ–å­¦ä¹ åŸºçº¿çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21268', 'title': 'MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources', 'url': 'https://huggingface.co/papers/2509.21268', 'abstract': 'Variance-Aware Sampling and large-scale CoT data improve multimodal reasoning models by stabilizing RL fine-tuning and enhancing performance on benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large multimodal reasoning models have achieved rapid progress, but their advancement is constrained by two major limitations: the absence of open, large-scale, high-quality long chain-of-thought (CoT) data, and the instability of reinforcement learning (RL) algorithms in post-training. Group Relative Policy Optimization (GRPO), the standard framework for RL fine-tuning, is prone to gradient vanishing when reward variance is low, which weakens optimization signals and impairs convergence. This work makes three contributions: (1) We propose Variance-Aware Sampling (VAS), a data selection strategy guided by Variance Promotion Score (VPS) that combines outcome variance and trajectory diversity to promote reward variance and stabilize policy optimization. (2) We release large-scale, carefully curated resources containing ~1.6M long CoT cold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty, and diversity, along with a fully reproducible end-to-end training codebase. (3) We open-source a family of multimodal reasoning models in multiple scales, establishing standardized baselines for the community. Experiments across mathematical reasoning benchmarks demonstrate the effectiveness of both the curated data and the proposed VAS. Comprehensive ablation studies and analyses provide further insight into the contributions of each component. In addition, we theoretically establish that reward variance lower-bounds the expected policy gradient magnitude, with VAS serving as a practical mechanism to realize this guarantee. Our code, data, and checkpoints are available at https://github.com/LengSicong/MMR1.', 'score': 90, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '7153bc23f1974ebe', 'authors': ['Sicong Leng', 'Jing Wang', 'Jiaxi Li', 'Hao Zhang', 'Zhiqiang Hu', 'Boqiang Zhang', 'Yuming Jiang', 'Hang Zhang', 'Xin Li', 'Lidong Bing', 'Deli Zhao', 'Wei Lu', 'Yu Rong', 'Aixin Sun', 'Shijian Lu'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Nanyang Technological University', 'Singapore University of Technology and Design'], 'pdf_title_img': 'assets/pdf/title_img/2509.21268.jpg', 'data': {'categories': ['#dataset', '#training', '#architecture', '#reasoning', '#benchmark', '#rl', '#optimization', '#multimodal', '#data', '#open_source'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ RL-Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Variance-Aware Sampling (VAS) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² RL Ğ¸Ğ·-Ğ·Ğ° Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¸ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². VAS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Variance Promotion Score Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ñ 1.6M Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Multimodal Reasoning with Variance-Aware Sampling and Quality Data', 'desc': 'This paper addresses the challenges faced by large multimodal reasoning models, particularly the lack of high-quality long chain-of-thought (CoT) data and the instability of reinforcement learning (RL) during fine-tuning. It introduces Variance-Aware Sampling (VAS), a method that enhances reward variance and stabilizes policy optimization by selecting data based on outcome variance and trajectory diversity. The authors also provide a substantial dataset of approximately 1.6 million CoT examples and 15,000 RL question-answer pairs, ensuring diversity and quality for training. Additionally, they release a set of multimodal reasoning models and establish standardized benchmarks for future research in the field.'}, 'zh': {'title': 'æ–¹å·®æ„ŸçŸ¥é‡‡æ ·æå‡å¤šæ¨¡æ€æ¨ç†æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é€‰æ‹©ç­–ç•¥ï¼Œç§°ä¸ºæ–¹å·®æ„ŸçŸ¥é‡‡æ ·ï¼ˆVASï¼‰ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€æ¨ç†æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡ç»“åˆç»“æœæ–¹å·®å’Œè½¨è¿¹å¤šæ ·æ€§ï¼ŒVASå¯ä»¥ä¿ƒè¿›å¥–åŠ±æ–¹å·®ï¼Œä»è€Œç¨³å®šå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†å¤§è§„æ¨¡çš„é«˜è´¨é‡é•¿é“¾æ€ç»´ï¼ˆCoTï¼‰æ•°æ®é›†ï¼ŒåŒ…å«çº¦160ä¸‡æ¡å†·å¯åŠ¨æ•°æ®å’Œçº¦15000ä¸ªRLé—®ç­”å¯¹ï¼Œä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVASå’Œæ–°æ•°æ®é›†æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨æ•°å­¦æ¨ç†åŸºå‡†ä¸Šçš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21320', 'title': 'SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines', 'url': 'https://huggingface.co/papers/2509.21320', 'abstract': 'A scientific reasoning foundation model pre-trained on diverse scientific data supports multiple tasks and enhances cross-domain generalization and fidelity through specialized training techniques.  \t\t\t\t\tAI-generated summary \t\t\t\t We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.', 'score': 86, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'e33c7b540d84ad9a', 'authors': ['Yizhou Wang', 'Chen Tang', 'Han Deng', 'Jiabei Xiao', 'Jiaqi Liu', 'Jianyu Wu', 'Jun Yao', 'Pengze Li', 'Encheng Su', 'Lintao Wang', 'Guohang Zhuang', 'Yuchen Ren', 'Ben Fei', 'Ming Hu', 'Xin Chen', 'Dongzhan Zhou', 'Junjun He', 'Xiangyu Yue', 'Zhenfei Yin', 'Jiamin Wu', 'Qihao Zheng', 'Yuhao Zhou', 'Huihui Xu', 'Chenglong Ma', 'Yan Lu', 'Wenlong Zhang', 'Chunfeng Song', 'Philip Torr', 'Shixiang Tang', 'Xinzhu Ma', 'Wanli Ouyang', 'Lei Bai'], 'affiliations': ['Beihang University', 'Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'The University of Sydney', 'University of Oxford', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21320.jpg', 'data': {'categories': ['#dataset', '#training', '#reasoning', '#transfer_learning', '#multimodal', '#data', '#science', '#open_source'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ AI Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ foundation Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… - Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¸ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° ĞºĞ¾Ñ€Ğ¿ÑƒÑĞµ Ğ¸Ğ· 206 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ supervised fine-tuning Ğ½Ğ° 40 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ¸ reinforcement learning Ñ task-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ reward shaping. ĞĞ½Ğ° Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ², ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆÑƒÑ ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Scientific Reasoning with a Versatile Foundation Model', 'desc': 'This paper introduces a scientific reasoning foundation model that is trained on a vast dataset of scientific texts and sequences. It employs advanced techniques like supervised fine-tuning and reinforcement learning to improve its ability to perform various scientific tasks. The model can translate between different scientific formats, extract knowledge, predict properties, and generate sequences, making it versatile across multiple domains. By enhancing cross-domain generalization and fidelity, this model outperforms specialized systems in handling a wide range of scientific inquiries.'}, 'zh': {'title': 'ç§‘å­¦æ¨ç†æ¨¡å‹ï¼šè·¨é¢†åŸŸçš„æ™ºèƒ½åŠ©æ‰‹', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§ç§‘å­¦æ¨ç†åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨å¤šæ ·çš„ç§‘å­¦æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ”¯æŒå¤šç§ä»»åŠ¡å¹¶å¢å¼ºè·¨é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å‹ä½¿ç”¨äº†2060äº¿ä¸ªæ ‡è®°çš„è¯­æ–™åº“ï¼Œæ¶µç›–ç§‘å­¦æ–‡æœ¬ã€çº¯åºåˆ—å’Œåºåˆ—-æ–‡æœ¬å¯¹ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„è®­ç»ƒæŠ€æœ¯è¿›è¡Œå¯¹é½ã€‚å®ƒèƒ½å¤Ÿæ‰§è¡Œå¤šè¾¾103ä¸ªä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬ä¸ç§‘å­¦æ ¼å¼ä¹‹é—´çš„ç¿»è¯‘ã€çŸ¥è¯†æå–ã€å±æ€§é¢„æµ‹å’Œåˆ†ç±»ç­‰ã€‚ä¸ä¸“ä¸šç³»ç»Ÿç›¸æ¯”ï¼Œè¯¥æ¨¡å‹åœ¨æŒ‡ä»¤è¦†ç›–èŒƒå›´ã€è·¨é¢†åŸŸæ³›åŒ–å’Œå‡†ç¡®æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21240', 'title': 'Tree Search for LLM Agent Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.21240', 'abstract': 'Tree-based Group Relative Policy Optimization (Tree-GRPO) enhances reinforcement learning for large language models by using tree search to improve rollouts and estimate grouped relative advantages, outperforming chain-based methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) have significantly enhanced the agentic capabilities of large language models (LLMs). In long-term and multi-turn agent tasks, existing approaches driven solely by outcome rewards often suffer from the problem of sparse supervision. To address the challenge, we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped agent RL method based on tree search, where each tree node represents the complete agent interaction step. By sharing common prefixes, the tree search sampling increases the number of rollouts achievable within a fixed budget of tokens or tool calls. Moreover, we find that the tree-structured trajectory naturally allows the construction of step-wise process supervised signals even using only the outcome reward. Based on this, Tree-GRPO estimates the grouped relative advantages both on intra-tree and inter-tree levels. Through theoretical analysis, we demonstrate that the objective of intra-tree level group relative policy optimization is equivalent to that of step-level direct preference learning. Experiments across 11 datasets and 3 types of QA tasks demonstrate the superiority of the proposed tree-based RL over the chain-based RL method.', 'score': 71, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'bfd427434553568d', 'authors': ['Yuxiang Ji', 'Ziyu Ma', 'Yong Wang', 'Guanhua Chen', 'Xiangxiang Chu', 'Liaoni Wu'], 'affiliations': ['AMAP, Alibaba Group', 'Southern University of Science and Technology', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21240.jpg', 'data': {'categories': ['#reasoning', '#rlhf', '#rl', '#optimization'], 'emoji': 'ğŸŒ³', 'ru': {'title': 'Ğ”Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ ÑƒĞ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ AI-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Tree-GRPO - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ² Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚ÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ. Tree-GRPO Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ĞºĞ°Ğº Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ğ´ĞµÑ€ĞµĞ²Ğ°, Ñ‚Ğ°Ğº Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´ĞµÑ€ĞµĞ²ÑŒÑĞ¼Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞºĞ²Ğ¸Ğ²Ğ°Ğ»ĞµĞ½Ñ‚Ğ½Ğ¾ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 11 Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ñ€ĞµĞ²Ğ¾Ğ²Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ RL.'}, 'en': {'title': 'Optimizing Language Models with Tree-Based Reinforcement Learning', 'desc': "Tree-based Group Relative Policy Optimization (Tree-GRPO) is a novel approach in reinforcement learning that enhances the performance of large language models by utilizing tree search techniques. This method improves the efficiency of rollouts and allows for better estimation of grouped relative advantages, addressing the issue of sparse supervision in long-term tasks. By structuring the agent's interactions in a tree format, Tree-GRPO can generate more rollouts within a limited resource budget, leading to improved learning signals. Experimental results show that Tree-GRPO outperforms traditional chain-based methods across various datasets and question-answering tasks."}, 'zh': {'title': 'æ ‘åŸºä¼˜åŒ–ï¼Œæå‡å¼ºåŒ–å­¦ä¹ æ•ˆç‡', 'desc': 'æ ‘åŸºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆTree-GRPOï¼‰é€šè¿‡æ ‘æœç´¢æ¥å¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚è¯¥æ–¹æ³•é€šè¿‡å…±äº«å…¬å…±å‰ç¼€ï¼Œå¢åŠ äº†åœ¨å›ºå®šé¢„ç®—å†…å¯å®ç°çš„å›åˆæ•°ï¼Œä»è€Œæé«˜äº†å­¦ä¹ æ•ˆç‡ã€‚Tree-GRPOèƒ½å¤Ÿåœ¨æ ‘å†…å’Œæ ‘é—´å±‚é¢ä¼°è®¡åˆ†ç»„ç›¸å¯¹ä¼˜åŠ¿ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ç¨€ç–ç›‘ç£çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTree-GRPOåœ¨å¤šä¸ªæ•°æ®é›†å’Œé—®ç­”ä»»åŠ¡ä¸­ä¼˜äºåŸºäºé“¾çš„æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20427', 'title': 'Seedream 4.0: Toward Next-generation Multimodal Image Generation', 'url': 'https://huggingface.co/papers/2509.20427', 'abstract': 'Seedream 4.0 is a high-performance multimodal image generation system that integrates text-to-image synthesis, image editing, and multi-image composition using a diffusion transformer and VAE, achieving state-of-the-art results with efficient training and inference.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Seedream 4.0, an efficient and high-performance multimodal image generation system that unifies text-to-image (T2I) synthesis, image editing, and multi-image composition within a single framework. We develop a highly efficient diffusion transformer with a powerful VAE which also can reduce the number of image tokens considerably. This allows for efficient training of our model, and enables it to fast generate native high-resolution images (e.g., 1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning diverse taxonomies and knowledge-centric concepts. Comprehensive data collection across hundreds of vertical scenarios, coupled with optimized strategies, ensures stable and large-scale training, with strong generalization. By incorporating a carefully fine-tuned VLM model, we perform multi-modal post-training for training both T2I and image editing tasks jointly. For inference acceleration, we integrate adversarial distillation, distribution matching, and quantization, as well as speculative decoding. It achieves an inference time of up to 1.8 seconds for generating a 2K image (without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream 4.0 can achieve state-of-the-art results on both T2I and multimodal image editing. In particular, it demonstrates exceptional multimodal capabilities in complex tasks, including precise image editing and in-context reasoning, and also allows for multi-image reference, and can generate multiple output images. This extends traditional T2I systems into an more interactive and multidimensional creative tool, pushing the boundary of generative AI for both creativity and professional applications. Seedream 4.0 is now accessible on https://www.volcengine.com/experience/ark?launch=seedream.', 'score': 63, 'issue_id': 6098, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': 'fb2f872386c520ce', 'authors': ['Team Seedream', 'Yunpeng Chen', 'Yu Gao', 'Lixue Gong', 'Meng Guo', 'Qiushan Guo', 'Zhiyao Guo', 'Xiaoxia Hou', 'Weilin Huang', 'Yixuan Huang', 'Xiaowen Jian', 'Huafeng Kuang', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yanzuo Lu', 'Zhengxiong Luo', 'Tongtong Ou', 'Guang Shi', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Wenxu Wu', 'Yonghui Wu', 'Xin Xia', 'Xuefeng Xiao', 'Shuang Xu', 'Xin Yan', 'Ceyuan Yang', 'Jianchao Yang', 'Zhonghua Zhai', 'Chenlin Zhang', 'Heng Zhang', 'Qi Zhang', 'Xinyu Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Wenliang Zhao', 'Wenjia Zhu'], 'affiliations': ['ByteDance', 'Volcano Engine'], 'pdf_title_img': 'assets/pdf/title_img/2509.20427.jpg', 'data': {'categories': ['#inference', '#training', '#games', '#multimodal', '#cv', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Seedream 4.0 â€” ÑÑ‚Ğ¾ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ñ Ğ¼Ğ¾Ñ‰Ğ½Ñ‹Ğ¼ VAE, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ñ‹ÑÑ‚Ñ€ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 4K. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ñ… Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ¾Ğ¹ VLM Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ”Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ adversarial distillation, distribution matching, ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ speculative decoding, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ 2K Ğ·Ğ° 1.8 ÑĞµĞºÑƒĞ½Ğ´Ñ‹.'}, 'en': {'title': 'Revolutionizing Image Generation with Seedream 4.0', 'desc': 'Seedream 4.0 is a cutting-edge multimodal image generation system that combines text-to-image synthesis, image editing, and multi-image composition into one efficient framework. It utilizes a diffusion transformer and a variational autoencoder (VAE) to significantly reduce image token counts, enabling faster training and high-resolution image generation. The model is pretrained on a vast dataset of text-image pairs, ensuring strong generalization across various scenarios. With advanced techniques for inference acceleration, Seedream 4.0 achieves state-of-the-art performance in both T2I tasks and complex image editing, making it a powerful tool for creative and professional applications.'}, 'zh': {'title': 'Seedream 4.0ï¼šå¤šæ¨¡æ€å›¾åƒç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'Seedream 4.0 æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„å¤šæ¨¡æ€å›¾åƒç”Ÿæˆç³»ç»Ÿï¼Œç»“åˆäº†æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€å›¾åƒç¼–è¾‘å’Œå¤šå›¾åƒç»„åˆã€‚å®ƒé‡‡ç”¨äº†é«˜æ•ˆçš„æ‰©æ•£å˜æ¢å™¨å’Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥ç³»ç»Ÿç»è¿‡æ•°åäº¿å¯¹æ–‡æœ¬-å›¾åƒå¯¹çš„é¢„è®­ç»ƒï¼Œç¡®ä¿äº†å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œç¨³å®šæ€§ã€‚Seedream 4.0 ä¸ä»…èƒ½å¿«é€Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œè¿˜åœ¨å¤æ‚ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šçš„å¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ¨åŠ¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¾¹ç•Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21245', 'title': 'Hunyuan3D-Omni: A Unified Framework for Controllable Generation of 3D\n  Assets', 'url': 'https://huggingface.co/papers/2509.21245', 'abstract': 'Hunyuan3D-Omni is a unified 3D asset generation framework that accepts multiple conditioning signals, improving controllability and robustness in production workflows.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in 3D-native generative models have accelerated asset creation for games, film, and design. However, most methods still rely primarily on image or text conditioning and lack fine-grained, cross-modal controls, which limits controllability and practical adoption. To address this gap, we present Hunyuan3D-Omni, a unified framework for fine-grained, controllable 3D asset generation built on Hunyuan3D 2.1. In addition to images, Hunyuan3D-Omni accepts point clouds, voxels, bounding boxes, and skeletal pose priors as conditioning signals, enabling precise control over geometry, topology, and pose. Instead of separate heads for each modality, our model unifies all signals in a single cross-modal architecture. We train with a progressive, difficulty-aware sampling strategy that selects one control modality per example and biases sampling toward harder signals (e.g., skeletal pose) while downweighting easier ones (e.g., point clouds), encouraging robust multi-modal fusion and graceful handling of missing inputs. Experiments show that these additional controls improve generation accuracy, enable geometry-aware transformations, and increase robustness for production workflows.', 'score': 36, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '8c4661dd2c3016bb', 'authors': ['Team Hunyuan3D', ':', 'Bowen Zhang', 'Chunchao Guo', 'Haolin Liu', 'Hongyu Yan', 'Huiwen Shi', 'Jingwei Huang', 'Junlin Yu', 'Kunhong Li', 'Linus', 'Penghao Wang', 'Qingxiang Lin', 'Sicong Liu', 'Xianghui Yang', 'Yixuan Tang', 'Yunfei Zhao', 'Zeqiang Lai', 'Zhihao Liang', 'Zibo Zhao'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2509.21245.jpg', 'data': {'categories': ['#3d', '#training', '#architecture', '#games', '#synthetic', '#multimodal'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ 3D-Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Hunyuan3D-Omni â€” ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ğ½Ğ¾ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ²Ğ¾ĞºÑĞµĞ»Ñ‹, Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ¼ĞºĞ¸ Ğ¸ ÑĞºĞµĞ»ĞµÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ·Ñ‹ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ¾Ğ±Ğ°ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ğ¸ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğµ.'}, 'en': {'title': 'Unified Control for 3D Asset Generation', 'desc': 'Hunyuan3D-Omni is a comprehensive framework designed for generating 3D assets with enhanced control and reliability. It allows the use of various conditioning signals, such as point clouds and skeletal poses, in addition to traditional images, which improves the precision of the generated models. The framework employs a unified cross-modal architecture, eliminating the need for separate processing heads for each type of input. By using a progressive sampling strategy that prioritizes more complex controls, it ensures better integration of different modalities and improves the overall robustness of the asset generation process.'}, 'zh': {'title': 'ç»Ÿä¸€å¤šæ¨¡æ€çš„3Dèµ„äº§ç”Ÿæˆæ¡†æ¶', 'desc': 'Hunyuan3D-Omniæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„3Dèµ„äº§ç”Ÿæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ¥å—å¤šç§æ¡ä»¶ä¿¡å·ï¼Œä»è€Œæé«˜ç”Ÿäº§å·¥ä½œæµç¨‹ä¸­çš„å¯æ§æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒå›¾åƒï¼Œè¿˜å¯ä»¥å¤„ç†ç‚¹äº‘ã€ä½“ç´ ã€è¾¹ç•Œæ¡†å’Œéª¨éª¼å§¿æ€å…ˆéªŒç­‰å¤šç§è¾“å…¥ä¿¡å·ï¼Œå®ç°å¯¹å‡ ä½•å½¢çŠ¶ã€æ‹“æ‰‘ç»“æ„å’Œå§¿æ€çš„ç²¾ç¡®æ§åˆ¶ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒHunyuan3D-Omniå°†æ‰€æœ‰ä¿¡å·ç»Ÿä¸€åœ¨ä¸€ä¸ªè·¨æ¨¡æ€æ¶æ„ä¸­ï¼Œé¿å…äº†ä¸ºæ¯ç§æ¨¡æ€å•ç‹¬è®¾è®¡æ¨¡å‹çš„å¤æ‚æ€§ã€‚é€šè¿‡é€æ­¥çš„ã€å…³æ³¨éš¾åº¦çš„é‡‡æ ·ç­–ç•¥ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆèåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œå¹¶åœ¨å¤„ç†ç¼ºå¤±è¾“å…¥æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„é²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21138', 'title': 'AutoIntent: AutoML for Text Classification', 'url': 'https://huggingface.co/papers/2509.21138', 'abstract': 'AutoIntent is an automated machine learning tool for text classification that offers end-to-end automation, including embedding model selection, classifier optimization, and decision threshold tuning, and supports multi-label classification and out-of-scope detection.  \t\t\t\t\tAI-generated summary \t\t\t\t AutoIntent is an automated machine learning tool for text classification tasks. Unlike existing solutions, AutoIntent offers end-to-end automation with embedding model selection, classifier optimization, and decision threshold tuning, all within a modular, sklearn-like interface. The framework is designed to support multi-label classification and out-of-scope detection. AutoIntent demonstrates superior performance compared to existing AutoML tools on standard intent classification datasets and enables users to balance effectiveness and resource consumption.', 'score': 26, 'issue_id': 6104, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'aeb5276117d67ddf', 'authors': ['Ilya Alekseev', 'Roman Solomatin', 'Darina Rustamova', 'Denis Kuznetsov'], 'affiliations': ['ITMO University', 'Moscow Center for Advanced Studies', 'Moscow State University', 'dresscode.ai'], 'pdf_title_img': 'assets/pdf/title_img/2509.21138.jpg', 'data': {'categories': ['#dataset', '#optimization', '#training', '#data'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ğ¾Ñ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'AutoIntent Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°: Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ´Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ² Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²ÑƒÑ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ²Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. AutoIntent Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ AutoML Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ½Ğ° ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ… Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Automate Your Text Classification with AutoIntent!', 'desc': 'AutoIntent is a cutting-edge automated machine learning tool specifically designed for text classification tasks. It streamlines the process by providing end-to-end automation, which includes selecting embedding models, optimizing classifiers, and tuning decision thresholds. The tool is versatile, supporting both multi-label classification and out-of-scope detection, making it suitable for a variety of applications. In performance tests, AutoIntent outshines existing AutoML solutions, allowing users to achieve high accuracy while managing resource usage effectively.'}, 'zh': {'title': 'AutoIntentï¼šæ™ºèƒ½æ–‡æœ¬åˆ†ç±»çš„è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆ', 'desc': 'AutoIntent æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æœºå™¨å­¦ä¹ å·¥å…·ï¼Œä¸“æ³¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚å®ƒæä¾›äº†ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–åŠŸèƒ½ï¼ŒåŒ…æ‹¬åµŒå…¥æ¨¡å‹é€‰æ‹©ã€åˆ†ç±»å™¨ä¼˜åŒ–å’Œå†³ç­–é˜ˆå€¼è°ƒæ•´ã€‚è¯¥æ¡†æ¶æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»å’Œè¶…å‡ºèŒƒå›´æ£€æµ‹ï¼Œå…·æœ‰æ¨¡å—åŒ–çš„ sklearn é£æ ¼æ¥å£ã€‚ä¸ç°æœ‰çš„ AutoML å·¥å…·ç›¸æ¯”ï¼ŒAutoIntent åœ¨æ ‡å‡†æ„å›¾åˆ†ç±»æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œå¸®åŠ©ç”¨æˆ·åœ¨æ•ˆæœå’Œèµ„æºæ¶ˆè€—ä¹‹é—´å–å¾—å¹³è¡¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21117', 'title': 'TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them', 'url': 'https://huggingface.co/papers/2509.21117', 'abstract': "TrustJudge, a probabilistic framework, addresses inconsistencies in LLM-as-a-judge evaluation by using distribution-sensitive scoring and likelihood-aware aggregation, improving accuracy and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.", 'score': 22, 'issue_id': 6102, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'ccc68ce7d3aad944', 'authors': ['Yidong Wang', 'Yunze Song', 'Tingyuan Zhu', 'Xuanwang Zhang', 'Zhuohao Yu', 'Hao Chen', 'Chiyu Song', 'Qiufeng Wang', 'Cunxiang Wang', 'Zhen Wu', 'Xinyu Dai', 'Yue Zhang', 'Wei Ye', 'Shikun Zhang'], 'affiliations': ['Google DeepMind', 'Institute of Science Tokyo', 'Nanjing University', 'National University of Singapore', 'Peking University', 'Southeast University', 'Westlake University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21117.jpg', 'data': {'categories': ['#architecture', '#alignment', '#interpretability', '#data', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ”ĞµĞ»Ğ°ĞµĞ¼ LLM-ÑÑƒĞ´ĞµĞ¹ Ñ‡ĞµÑÑ‚Ğ½Ñ‹Ğ¼Ğ¸: Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸, Ğ³Ğ´Ğµ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹ - Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² ÑĞºĞ¾Ñ€Ğ¸Ğ½Ğ³Ğµ Ğ¸ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸ÑÑ…. ĞĞ½Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ TrustJudge - Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ¹. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ: Ğ½Ğ° 8.43% Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¸ Ğ½Ğ° 10.82% Ğ² Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ°Ñ€Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¾Ñ†ĞµĞ½Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ LLM-ÑÑƒĞ´ÑŒÑĞ¼Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ğº Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ñ‚Ğ°Ğº Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'}, 'en': {'title': 'TrustJudge: Enhancing Reliability in LLM Evaluations', 'desc': 'TrustJudge is a new framework designed to improve the evaluation of Large Language Models (LLMs) acting as judges. It tackles two main problems: inconsistencies in score comparisons and transitivity, which can lead to confusing results in evaluations. By using distribution-sensitive scoring, TrustJudge captures more information from ratings, and likelihood-aware aggregation helps resolve contradictions in preferences. This approach significantly reduces inconsistencies and enhances the accuracy of automated assessments without needing extra training or human input.'}, 'zh': {'title': 'TrustJudgeï¼šæå‡LLMè¯„ä¼°ä¸€è‡´æ€§çš„åˆ›æ–°æ¡†æ¶', 'desc': 'TrustJudgeæ˜¯ä¸€ä¸ªæ¦‚ç‡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°è€…æ—¶çš„è¯„ä¼°ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚å®ƒé€šè¿‡åˆ†å¸ƒæ•æ„Ÿè¯„åˆ†å’Œè€ƒè™‘ä¼¼ç„¶æ€§çš„èšåˆæ–¹æ³•ï¼Œæå‡äº†è¯„ä¼°çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å‰è¯„ä¼°æ¡†æ¶å­˜åœ¨è¯„åˆ†æ¯”è¾ƒä¸ä¸€è‡´å’Œæˆå¯¹ä¼ é€’ä¸ä¸€è‡´ç­‰é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æºäºç¦»æ•£è¯„åˆ†ç³»ç»Ÿçš„ä¿¡æ¯æŸå¤±ã€‚TrustJudgeé€šè¿‡è®¡ç®—è¿ç»­æœŸæœ›å’ŒåŒå‘åå¥½æ¦‚ç‡ï¼ŒæˆåŠŸå…‹æœäº†è¿™äº›é™åˆ¶ï¼Œæä¾›äº†æ›´ç²¾ç¡®çš„è¯„ä¼°ç»“æœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20712', 'title': 'CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning', 'url': 'https://huggingface.co/papers/2509.20712', 'abstract': 'A novel reinforcement learning algorithm, CE-GPPO, reintroduces gradients from clipped tokens to improve the exploration-exploitation balance in training large language models.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose Controlling Entropy via Gradient-Preserving Policy Optimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.', 'score': 16, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'ad1a5016131ff587', 'authors': ['Zhenpeng Su', 'Leiyu Pan', 'Minxuan Lv', 'Yuntao Li', 'Wenping Hu', 'Fuzheng Zhang', 'Kun Gai', 'Guorui Zhou'], 'affiliations': ['Independent', 'Klear Team, Kuaishou Technology'], 'pdf_title_img': 'assets/pdf/title_img/2509.20712.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ LLM', 'desc': 'Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ CE-GPPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ‚Ğ¸Ğ¿Ğ° PPO Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ñ‚Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ¾Ñ‚Ğ±Ñ€Ğ°ÑÑ‹Ğ²Ğ°ÑÑ‚ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸Ğ·-Ğ·Ğ° Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° ĞºĞ»Ğ¸Ğ¿Ğ¿Ğ¸Ğ½Ğ³Ğ°. CE-GPPO Ñ€ĞµÑˆĞ°ĞµÑ‚ ÑÑ‚Ñƒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Exploration-Exploitation Balance in Language Models with CE-GPPO', 'desc': 'The paper presents a new reinforcement learning algorithm called CE-GPPO, which enhances the training of large language models by reintroducing gradients from clipped tokens. This approach addresses the challenge of managing policy entropy, which is crucial for balancing exploration and exploitation during training. By carefully controlling the gradients from low-probability tokens, CE-GPPO improves the stability of entropy dynamics, which is often overlooked in existing methods. The authors provide both theoretical insights and empirical results demonstrating that CE-GPPO outperforms traditional algorithms like PPO on various reasoning tasks.'}, 'zh': {'title': 'é‡æ–°å¼•å…¥æ¢¯åº¦ï¼Œä¼˜åŒ–æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡', 'desc': 'CE-GPPOæ˜¯ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¤§è¯­è¨€æ¨¡å‹çš„æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡ã€‚è¯¥ç®—æ³•é€šè¿‡é‡æ–°å¼•å…¥è¢«å‰ªåˆ‡çš„æ ‡è®°çš„æ¢¯åº¦ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸¢å¤±æœ‰ä»·å€¼çš„æ¢¯åº¦ä¿¡å·çš„é—®é¢˜ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¿™äº›è¢«å‰ªåˆ‡çš„æ ‡è®°åœ¨è°ƒèŠ‚ç­–ç•¥ç†µçš„æ¼”å˜ä¸­èµ·ç€é‡è¦ä½œç”¨ã€‚CE-GPPOåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†å¼ºåŸºçº¿æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20186', 'title': 'Thinking Augmented Pre-training', 'url': 'https://huggingface.co/papers/2509.20186', 'abstract': 'Thinking augmented pre-training improves data efficiency and performance of large language models by augmenting text with automatically generated thinking trajectories.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to 100B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of 3. For a 3B parameter model, it improves the post-training performance by over 10% on several challenging reasoning benchmarks.', 'score': 16, 'issue_id': 6099, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': 'edd8ab635a2930b6', 'authors': ['Liang Wang', 'Nan Yang', 'Shaohan Huang', 'Li Dong', 'Furu Wei'], 'affiliations': ['Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.20186.jpg', 'data': {'categories': ['#data', '#training', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ”ÑƒĞ¼Ğ°ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: ĞºĞ°Ğº Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑÑÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ² Ñ‚Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Thinking augmented Pre-Training (TPT), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ»ÑƒÑ‡ÑˆĞµ ÑƒÑĞ²Ğ°Ğ¸Ğ²Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ TPT Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° 10% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ¼Ğ° Ğº Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ÑĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 100B Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Boosting Language Models with Thinking Trajectories', 'desc': 'This paper presents a novel approach called Thinking augmented Pre-Training (TPT) to enhance the data efficiency and performance of large language models (LLMs). By augmenting existing text data with automatically generated thinking trajectories, TPT increases the volume of training data and facilitates the learning of complex tokens through structured reasoning. The methodology addresses the challenge of limited high-quality data by making it easier for models to learn intricate concepts. Experimental results demonstrate that TPT significantly boosts LLM performance, achieving a threefold increase in data efficiency and over 10% improvement in reasoning tasks for a 3B parameter model.'}, 'zh': {'title': 'æ€ç»´å¢å¼ºé¢„è®­ç»ƒï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®æ•ˆç‡ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•ä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ€ç»´è½¨è¿¹æ¥å¢å¼ºç°æœ‰æ–‡æœ¬æ•°æ®ï¼Œä»è€Œæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒçš„æ•°æ®æ•ˆç‡ã€‚éšç€LLMé¢„è®­ç»ƒè®¡ç®—éœ€æ±‚çš„æ€¥å‰§å¢é•¿ï¼Œé«˜è´¨é‡æ•°æ®çš„å¯ç”¨æ€§å´ä¾ç„¶æœ‰é™ï¼Œå› æ­¤å¦‚ä½•æœ€å¤§åŒ–ç°æœ‰æ•°æ®çš„åˆ©ç”¨æˆä¸ºäº†ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„æ€ç»´å¢å¼ºé¢„è®­ç»ƒï¼ˆTPTï¼‰æ–¹æ³•ï¼Œé€šè¿‡é€æ­¥æ¨ç†å’Œåˆ†è§£ï¼Œä½¿å¾—é«˜è´¨é‡çš„æ ‡è®°æ›´æ˜“äºå­¦ä¹ ï¼Œä»è€Œæœ‰æ•ˆå¢åŠ è®­ç»ƒæ•°æ®çš„é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTPTåœ¨ä¸åŒæ¨¡å‹è§„æ¨¡å’Œç±»å‹ä¸Šæ˜¾è‘—æå‡äº†LLMçš„æ€§èƒ½ï¼Œæ•°æ®æ•ˆç‡æé«˜äº†ä¸‰å€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19301', 'title': 'Residual Off-Policy RL for Finetuning Behavior Cloning Policies', 'url': 'https://huggingface.co/papers/2509.19301', 'abstract': 'A residual learning framework combines behavior cloning and reinforcement learning to improve manipulation policies on high-degree-of-freedom systems using sparse binary rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io', 'score': 16, 'issue_id': 6103, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': '9db1658d8a8725c2', 'authors': ['Lars Ankile', 'Zhenyu Jiang', 'Rocky Duan', 'Guanya Shi', 'Pieter Abbeel', 'Anusha Nagabandi'], 'affiliations': ['Amazon FAR (Frontier AI & Robotics)', 'Carnegie Mellon University', 'Stanford University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2509.19301.jpg', 'data': {'categories': ['#games', '#optimization', '#rl', '#robotics', '#training'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ¾Ñ‚ Ğ¸Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğº Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ behavior cloning Ğ¸ reinforcement learning Ñ‡ĞµÑ€ĞµĞ· Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸ behavior cloning ĞºĞ°Ğº Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¸ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ Ğ¸Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ°Ğ¼ off-policy RL. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¾ ÑĞºÑƒĞ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ ÑÑ‚ĞµĞ¿ĞµĞ½ÑŒÑ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ñ‹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ RL Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ³ÑƒĞ¼Ğ°Ğ½Ğ¾Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ° Ñ Ğ»Ğ¾Ğ²ĞºĞ¸Ğ¼Ğ¸ Ñ€ÑƒĞºĞ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ.'}, 'en': {'title': 'Enhancing Robot Manipulation with Residual Learning: Merging Behavior Cloning and Reinforcement Learning', 'desc': 'This paper introduces a novel framework that merges behavior cloning (BC) and reinforcement learning (RL) to enhance manipulation policies for complex robotic systems. By using BC as a foundation, the method applies lightweight residual corrections through off-policy RL, allowing for efficient learning from sparse binary rewards. The approach addresses challenges such as sample inefficiency and safety concerns, particularly in high-degree-of-freedom environments. The authors showcase successful real-world applications, including training a humanoid robot with dexterous hands, achieving state-of-the-art performance in vision-based tasks.'}, 'zh': {'title': 'ç»“åˆè¡Œä¸ºå…‹éš†ä¸å¼ºåŒ–å­¦ä¹ çš„æ®‹å·®å­¦ä¹ æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ®‹å·®å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥æé«˜é«˜è‡ªç”±åº¦ç³»ç»Ÿçš„æ“ä½œç­–ç•¥ã€‚è¯¥æ–¹æ³•åˆ©ç”¨BCç­–ç•¥ä½œä¸ºåŸºç¡€ï¼Œé€šè¿‡æ ·æœ¬é«˜æ•ˆçš„ç¦»çº¿RLå­¦ä¹ è½»é‡çº§çš„é€æ­¥æ®‹å·®ä¿®æ­£ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»…éœ€ç¨€ç–çš„äºŒå…ƒå¥–åŠ±ä¿¡å·ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„é«˜è‡ªç”±åº¦ç³»ç»Ÿçš„æ“ä½œç­–ç•¥ï¼Œå¹¶åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œä¸­å‡å–å¾—äº†æˆåŠŸã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é¦–æ¬¡åœ¨å…·æœ‰çµå·§æ‰‹çš„ç±»äººæœºå™¨äººä¸ŠæˆåŠŸè¿›è¡Œäº†çœŸå®ä¸–ç•Œçš„RLè®­ç»ƒï¼Œå±•ç¤ºäº†åœ¨å„ç§åŸºäºè§†è§‰çš„ä»»åŠ¡ä¸­è¾¾åˆ°çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21114', 'title': 'CHARM: Control-point-based 3D Anime Hairstyle Auto-Regressive Modeling', 'url': 'https://huggingface.co/papers/2509.21114', 'abstract': 'CHARM uses a control-point-based parameterization and autoregressive transformer to generate high-fidelity anime hairstyles efficiently.  \t\t\t\t\tAI-generated summary \t\t\t\t We present CHARM, a novel parametric representation and generative framework for anime hairstyle modeling. While traditional hair modeling methods focus on realistic hair using strand-based or volumetric representations, anime hairstyle exhibits highly stylized, piecewise-structured geometry that challenges existing techniques. Existing works often rely on dense mesh modeling or hand-crafted spline curves, making them inefficient for editing and unsuitable for scalable learning. CHARM introduces a compact, invertible control-point-based parameterization, where a sequence of control points represents each hair card, and each point is encoded with only five geometric parameters. This efficient and accurate representation supports both artist-friendly design and learning-based generation. Built upon this representation, CHARM introduces an autoregressive generative framework that effectively generates anime hairstyles from input images or point clouds. By interpreting anime hairstyles as a sequential "hair language", our autoregressive transformer captures both local geometry and global hairstyle topology, resulting in high-fidelity anime hairstyle creation. To facilitate both training and evaluation of anime hairstyle generation, we construct AnimeHair, a large-scale dataset of 37K high-quality anime hairstyles with separated hair cards and processed mesh data. Extensive experiments demonstrate state-of-the-art performance of CHARM in both reconstruction accuracy and generation quality, offering an expressive and scalable solution for anime hairstyle modeling. Project page: https://hyzcluster.github.io/charm/', 'score': 15, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'd22edb1ca787e3d5', 'authors': ['Yuze He', 'Yanning Zhou', 'Wang Zhao', 'Jingwen Ye', 'Yushi Bai', 'Kaiwen Xiao', 'Yong-Jin Liu', 'Zhongqian Sun', 'Wei Yang'], 'affiliations': ['Tencent AIPD, China', 'Tsinghua University and Tencent AIPD, China', 'Tsinghua University, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21114.jpg', 'data': {'categories': ['#games', '#synthetic', '#training', '#cv', '#architecture', '#dataset'], 'emoji': 'ğŸ’‡', 'ru': {'title': 'AI ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ°Ğ½Ğ¸Ğ¼Ğµ-Ğ¿Ñ€Ğ¸Ñ‡Ñ‘ÑĞºĞ¸ Ñ‡ĞµÑ€ĞµĞ· "ÑĞ·Ñ‹Ğº Ğ²Ğ¾Ğ»Ğ¾Ñ"', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° CHARM - Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğµ-Ğ¿Ñ€Ğ¸Ñ‡Ñ‘ÑĞ¾Ğº Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ AI. Ğ’Ğ¼ĞµÑÑ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº, Ğ³Ğ´Ğµ ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¿Ñ€ÑĞ´ÑŒ Ğ²Ğ¾Ğ»Ğ¾Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ğ¾Ñ‡ĞµĞº Ñ Ğ¿ÑÑ‚ÑŒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ñ‘ÑĞºĞ¸ ĞºĞ°Ğº "ÑĞ·Ñ‹Ğº Ğ²Ğ¾Ğ»Ğ¾Ñ", Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ĞºĞ°Ğº Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ, Ñ‚Ğ°Ğº Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ AnimeHair Ñ 37 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ğ¼Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğµ-Ğ¿Ñ€Ğ¸Ñ‡Ñ‘ÑĞ¾Ğº.'}, 'en': {'title': 'Efficient Anime Hairstyle Generation with CHARM', 'desc': 'CHARM is a new method for creating anime hairstyles using a unique control-point-based system and an autoregressive transformer. Unlike traditional methods that use complex mesh or spline techniques, CHARM simplifies the process by representing hairstyles with a few control points, each defined by just five parameters. This makes it easier for artists to design and for machines to learn from, allowing for efficient generation of high-quality hairstyles. The framework is supported by a large dataset of 37,000 anime hairstyles, ensuring that the generated styles are both accurate and visually appealing.'}, 'zh': {'title': 'é«˜æ•ˆç”ŸæˆåŠ¨æ¼«å‘å‹çš„åˆ›æ–°æ¡†æ¶', 'desc': 'CHARMæ˜¯ä¸€ç§æ–°é¢–çš„å‚æ•°åŒ–è¡¨ç¤ºå’Œç”Ÿæˆæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºåŠ¨æ¼«å‘å‹å»ºæ¨¡ã€‚ä¸ä¼ ç»Ÿçš„å‘å‹å»ºæ¨¡æ–¹æ³•ä¸åŒï¼ŒCHARMé‡‡ç”¨åŸºäºæ§åˆ¶ç‚¹çš„ç´§å‡‘è¡¨ç¤ºï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°ç”Ÿæˆé«˜ä¿çœŸåº¦çš„åŠ¨æ¼«å‘å‹ã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªå›å½’å˜æ¢å™¨æ•æ‰å±€éƒ¨å‡ ä½•å’Œå…¨å±€å‘å‹æ‹“æ‰‘ï¼Œæ”¯æŒä»è¾“å…¥å›¾åƒæˆ–ç‚¹äº‘ç”Ÿæˆå‘å‹ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«37Ké«˜è´¨é‡åŠ¨æ¼«å‘å‹çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›å‘å‹ç”Ÿæˆçš„è®­ç»ƒå’Œè¯„ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21278', 'title': 'Does FLUX Already Know How to Perform Physically Plausible Image\n  Composition?', 'url': 'https://huggingface.co/papers/2509.21278', 'abstract': 'SHINE is a training-free framework that uses manifold-steered anchor loss and pretrained customization adapters to seamlessly insert objects into new scenes with high fidelity, addressing challenges like complex lighting and diverse inputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Image composition aims to seamlessly insert a user-specified object into a new scene, but existing models struggle with complex lighting (e.g., accurate shadows, water reflections) and diverse, high-resolution inputs. Modern text-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential physical and resolution priors, yet lack a framework to unleash them without resorting to latent inversion, which often locks object poses into contextually inappropriate orientations, or brittle attention surgery. We propose SHINE, a training-free framework for Seamless, High-fidelity Insertion with Neutralized Errors. SHINE introduces manifold-steered anchor loss, leveraging pretrained customization adapters (e.g., IP-Adapter) to guide latents for faithful subject representation while preserving background integrity. Degradation-suppression guidance and adaptive background blending are proposed to further eliminate low-quality outputs and visible seams. To address the lack of rigorous benchmarks, we introduce ComplexCompo, featuring diverse resolutions and challenging conditions such as low lighting, strong illumination, intricate shadows, and reflective surfaces. Experiments on ComplexCompo and DreamEditBench show state-of-the-art performance on standard metrics (e.g., DINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward). Code and benchmark will be publicly available upon publication.', 'score': 13, 'issue_id': 6101, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '3cadc9df7abcbbcc', 'authors': ['Shilin Lu', 'Zhuming Lian', 'Zihan Zhou', 'Shaocong Zhang', 'Chen Zhao', 'Adams Wai-Kin Kong'], 'affiliations': ['Nanjing University', 'Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21278.jpg', 'data': {'categories': ['#training', '#optimization', '#diffusion', '#benchmark', '#cv', '#open_source'], 'emoji': 'âœ¨', 'ru': {'title': 'Ğ‘ĞµĞ·ÑƒĞ¿Ñ€ĞµÑ‡Ğ½Ğ°Ñ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑÑ†ĞµĞ½Ñ‹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SHINE - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ±ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ±ĞµÑÑˆĞ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ²ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ†ĞµĞ½Ñ‹ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ manifold-steered anchor loss Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ñ‹ ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ ÑĞ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ„Ğ¾Ğ½Ğ° Ğ´Ğ»Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ñ‹Ñ… ÑˆĞ²Ğ¾Ğ² Ğ¸ Ğ½Ğ¸Ğ·ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ”Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ComplexCompo Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾ÑĞ²ĞµÑ‰ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Seamless Object Insertion with SHINE: High Fidelity, No Training Required!', 'desc': 'SHINE is a novel framework designed for seamlessly inserting objects into new scenes without the need for extensive training. It utilizes manifold-steered anchor loss and pretrained customization adapters to ensure high fidelity in object representation while maintaining the integrity of the background. The framework addresses common challenges in image composition, such as complex lighting and diverse input resolutions, by implementing degradation-suppression guidance and adaptive background blending. Additionally, SHINE introduces a new benchmark, ComplexCompo, to evaluate performance under various challenging conditions, demonstrating state-of-the-art results in both standard and human-aligned metrics.'}, 'zh': {'title': 'æ— ç¼é«˜ä¿çœŸæ’å…¥çš„åˆ›æ–°æ¡†æ¶', 'desc': 'SHINEæ˜¯ä¸€ä¸ªæ— è®­ç»ƒçš„æ¡†æ¶ï¼Œæ—¨åœ¨é«˜ä¿çœŸåœ°å°†å¯¹è±¡æ— ç¼æ’å…¥æ–°åœºæ™¯ä¸­ã€‚å®ƒé‡‡ç”¨äº†æµå½¢å¼•å¯¼é”šæŸå¤±å’Œé¢„è®­ç»ƒçš„å®šåˆ¶é€‚é…å™¨ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹å¤æ‚çš„å…‰ç…§å’Œå¤šæ ·åŒ–çš„è¾“å…¥ã€‚SHINEé€šè¿‡å¼•å…¥é™è§£æŠ‘åˆ¶æŒ‡å¯¼å’Œè‡ªé€‚åº”èƒŒæ™¯èåˆï¼Œè¿›ä¸€æ­¥æ¶ˆé™¤ä½è´¨é‡è¾“å‡ºå’Œå¯è§æ¥ç¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSHINEåœ¨æ ‡å‡†æŒ‡æ ‡å’Œäººç±»å¯¹é½è¯„åˆ†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨å›¾åƒåˆæˆé¢†åŸŸçš„å…ˆè¿›æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21072', 'title': 'Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web\n  Reconnaissance, Tool Generation, and Task Execution', 'url': 'https://huggingface.co/papers/2509.21072', 'abstract': 'Recon-Act, a self-evolving multi-agent framework, improves adaptability and performance on long-horizon web tasks by generating and utilizing generalized tools through reconnaissance and action teams.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent years, multimodal models have made remarkable strides and pave the way for intelligent browser use agents. However, when solving tasks on real world webpages in multi-turn, long-horizon trajectories, current agents still suffer from disordered action sequencing and excessive trial and error during execution. This paper introduces Recon-Act, a self-evolving multi-agent framework grounded in Reconnaissance-Action behavioral paradigm. The system comprises a Reconnaissance Team and an Action Team: the former conducts comparative analysis and tool generation, while the latter handles intent decomposition, tool orchestration, and execution. By contrasting the erroneous trajectories with successful ones, the Reconnaissance Team infers remedies, and abstracts them into a unified notion of generalized tools, either expressed as hints or as rule-based codes, and register to the tool archive in real time. The Action Team reinference the process empowered with these targeting tools, thus establishing a closed-loop training pipeline of data-tools-action-feedback. Following the 6 level implementation roadmap proposed in this work, we have currently reached Level 3 (with limited human-in-the-loop intervention). Leveraging generalized tools obtained through reconnaissance, Recon-Act substantially improves adaptability to unseen websites and solvability on long-horizon tasks, and achieves state-of-the-art performance on the challenging VisualWebArena dataset.', 'score': 13, 'issue_id': 6105, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '7a6532a574905646', 'authors': ['Kaiwen He', 'Zhiwei Wang', 'Chenyi Zhuang', 'Jinjie Gu'], 'affiliations': ['AWorld Team, Inclusion AI'], 'pdf_title_img': 'assets/pdf/title_img/2509.21072.jpg', 'data': {'categories': ['#dataset', '#agents', '#multimodal', '#optimization', '#agi'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ° Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµĞ±-Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Recon-Act - ÑĞ°Ğ¼Ğ¾ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸-Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ğ²ĞµĞ±-Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ñ€Ğ°Ğ·Ğ²ĞµĞ´ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹, Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ‚Ğ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ§ĞµÑ€ĞµĞ· ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ½ĞµÑƒĞ´Ğ°Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ» state-of-the-art Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ VisualWebArena Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ Ğ²ĞµĞ±-ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼.'}, 'en': {'title': 'Empowering AI Agents with Self-Evolving Tools for Web Mastery', 'desc': "Recon-Act is a multi-agent framework designed to enhance the performance of AI agents on complex web tasks. It operates through two main teams: the Reconnaissance Team, which analyzes past actions to generate useful tools, and the Action Team, which executes tasks using these tools. By learning from both successful and unsuccessful attempts, the framework creates generalized tools that help agents adapt to new situations more effectively. This self-evolving system establishes a feedback loop that continuously improves the agents' ability to navigate long-horizon tasks on various websites."}, 'zh': {'title': 'è‡ªæˆ‘è¿›åŒ–çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæå‡ç½‘é¡µä»»åŠ¡é€‚åº”æ€§', 'desc': 'Recon-Actæ˜¯ä¸€ä¸ªè‡ªæˆ‘è¿›åŒ–çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜åœ¨é•¿æ—¶é—´ç½‘é¡µä»»åŠ¡ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚è¯¥ç³»ç»Ÿç”±ä¾¦å¯Ÿå›¢é˜Ÿå’Œè¡ŒåŠ¨å›¢é˜Ÿç»„æˆï¼Œä¾¦å¯Ÿå›¢é˜Ÿè´Ÿè´£å·¥å…·ç”Ÿæˆå’Œæ¯”è¾ƒåˆ†æï¼Œè€Œè¡ŒåŠ¨å›¢é˜Ÿåˆ™å¤„ç†æ„å›¾åˆ†è§£å’Œå·¥å…·æ‰§è¡Œã€‚é€šè¿‡å¯¹æ¯”é”™è¯¯çš„æ‰§è¡Œè½¨è¿¹å’ŒæˆåŠŸçš„è½¨è¿¹ï¼Œä¾¦å¯Ÿå›¢é˜Ÿèƒ½å¤Ÿæ¨æ–­å‡ºè§£å†³æ–¹æ¡ˆï¼Œå¹¶å°†å…¶æŠ½è±¡ä¸ºé€šç”¨å·¥å…·ï¼Œå®æ—¶æ³¨å†Œåˆ°å·¥å…·åº“ä¸­ã€‚åˆ©ç”¨è¿™äº›é€šç”¨å·¥å…·ï¼ŒRecon-Actæ˜¾è‘—æé«˜äº†å¯¹æœªè§ç½‘ç«™çš„é€‚åº”èƒ½åŠ›ï¼Œå¹¶åœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.14662', 'title': "Understanding the Thinking Process of Reasoning Models: A Perspective\n  from Schoenfeld's Episode Theory", 'url': 'https://huggingface.co/papers/2509.14662', 'abstract': "A novel framework using Schoenfeld's Episode Theory is introduced to analyze the reasoning patterns of Large Reasoning Models in solving math problems, providing a benchmark for machine reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld's Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.", 'score': 11, 'issue_id': 6098, 'pub_date': '2025-09-18', 'pub_date_card': {'ru': '18 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 18', 'zh': '9æœˆ18æ—¥'}, 'hash': '1ed939a345480c28', 'authors': ['Ming Li', 'Nan Zhang', 'Chenrui Fan', 'Hong Jiao', 'Yanbin Fu', 'Sydney Peters', 'Qingshu Xu', 'Robert Lissitz', 'Tianyi Zhou'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2509.14662.jpg', 'data': {'categories': ['#math', '#reasoning', '#interpretability', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ°Ñ€Ñ‚Ğ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ AI Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¸Ğ·Ğ¼Ñƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ñ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ² Ğ¨Ñ‘Ğ½Ñ„ĞµĞ»ÑŒĞ´Ğ°, ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºÑƒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾Ğ¼, Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚Ğ¸Ğ² Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ÑŒÑ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚ĞºĞ°Ğ¼Ğ¸. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ¾Ğ³Ğ½Ğ¸Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸ÑĞ¼Ğ¸. Ğ­Ñ‚Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² AI Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¸ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Understanding Machine Reasoning with Cognitive Frameworks', 'desc': "This paper presents a new framework that uses Schoenfeld's Episode Theory to analyze how Large Reasoning Models (LRMs) approach math problems. By applying cognitive labels to thousands of sentences from model-generated solutions, the authors create a detailed benchmark for understanding machine reasoning. The study reveals specific patterns in the reasoning processes of LRMs, highlighting how they transition between different cognitive states. This framework not only aids in interpreting LRM cognition but also sets the stage for developing more transparent and controllable reasoning systems in the future."}, 'zh': {'title': 'æ­ç¤ºå¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ¨¡å¼', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œåˆ©ç”¨Schoenfeldçš„æƒ…èŠ‚ç†è®ºåˆ†æå¤§å‹æ¨ç†æ¨¡å‹åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„æ¨ç†æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å¯¹æ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆè¿›è¡Œæ ‡æ³¨ï¼Œä½¿ç”¨ä¸ƒç§è®¤çŸ¥æ ‡ç­¾ï¼ˆå¦‚è®¡åˆ’ã€å®æ–½ã€éªŒè¯ï¼‰æ¥åˆ†ææ¨ç†è½¨è¿¹ã€‚ç ”ç©¶ç»“æœæä¾›äº†ç¬¬ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„æœºå™¨æ¨ç†ç»†ç²’åº¦åˆ†æåŸºå‡†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¤§å‹æ ‡æ³¨è¯­æ–™åº“å’Œè¯¦ç»†çš„æ ‡æ³¨æŒ‡å—ã€‚åˆæ­¥åˆ†ææ˜¾ç¤ºäº†å¤§å‹æ¨ç†æ¨¡å‹æ¨ç†ä¸­çš„ç‹¬ç‰¹æ¨¡å¼ï¼Œå¦‚è®¤çŸ¥çŠ¶æ€ä¹‹é—´çš„è½¬å˜åŠ¨æ€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20136', 'title': 'V-GameGym: Visual Game Generation for Code Large Language Models', 'url': 'https://huggingface.co/papers/2509.20136', 'abstract': 'V-GameGym is a comprehensive benchmark for evaluating code generation in game development, focusing on multimodal evaluation including playability, visual aesthetics, and user engagement.  \t\t\t\t\tAI-generated summary \t\t\t\t Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problem-solving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, a comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting a novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce a multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation.', 'score': 9, 'issue_id': 6101, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '8ba29ccdb4749d95', 'authors': ['Wei Zhang', 'Jack Yang', 'Renshuai Tao', 'Lingzheng Chai', 'Shawn Guo', 'Jiajun Wu', 'Xiaoming Chen', 'Ganqu Cui', 'Ning Ding', 'Xander Xu', 'Hu Wei', 'Bowen Zhou'], 'affiliations': ['AIStrong', 'Alibaba Group', 'Beijing Jiaotong University', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2509.20136.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#games', '#multimodal', '#dataset'], 'emoji': 'ğŸ®', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ AI Ğ² Ğ³ĞµĞ¹Ğ¼Ğ´ĞµĞ²Ğµ: Ğ¾Ñ‚ ĞºĞ¾Ğ´Ğ° Ğº Ğ¸Ğ³Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'V-GameGym - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ¸Ğ³Ñ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¸Ğ³Ñ€Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑÑ‚ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ²Ğ¾Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 2219 Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² 100 Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ LLM Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° ĞºĞ¾Ğ´Ğ° Ğ² Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ğ¾Ğ¹ UI-ÑÑ€ĞµĞ´Ğµ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ³Ñ€, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Bridging Code Generation and Game Development with V-GameGym', 'desc': 'V-GameGym is a new benchmark designed to evaluate code generation specifically for game development. Unlike traditional benchmarks that focus only on syntax and execution, V-GameGym assesses important aspects like playability, visual appeal, and user engagement. It includes 2,219 diverse samples organized into 100 thematic clusters, ensuring a comprehensive evaluation of game development tasks. The benchmark also features a multimodal evaluation framework that uses automated pipelines for generating visual code, making it a valuable tool for improving AI in game design.'}, 'zh': {'title': 'V-GameGymï¼šæ¸¸æˆå¼€å‘çš„å¤šæ¨¡æ€è¯„ä¼°åŸºå‡†', 'desc': 'V-GameGymæ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¸¸æˆå¼€å‘ä¸­çš„ä»£ç ç”Ÿæˆï¼Œé‡ç‚¹å…³æ³¨å¤šæ¨¡æ€è¯„ä¼°ï¼ŒåŒ…æ‹¬å¯ç©æ€§ã€è§†è§‰ç¾å­¦å’Œç”¨æˆ·å‚ä¸åº¦ã€‚ç°æœ‰çš„ä»£ç ç›¸å…³åŸºå‡†ä¸»è¦å…³æ³¨è¯­æ³•æ­£ç¡®æ€§å’Œæ‰§è¡Œå‡†ç¡®æ€§ï¼Œè€Œå¿½è§†äº†æ¸¸æˆå¼€å‘ä¸­è‡³å…³é‡è¦çš„æŒ‡æ ‡ã€‚ä¸ºäº†è§£å†³å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç®—æ³•é—®é¢˜è§£å†³ä¸å®é™…æ¸¸æˆå¼€å‘éœ€æ±‚ä¹‹é—´çš„å·®è·ï¼ŒV-GameGymæä¾›äº†2219ä¸ªé«˜è´¨é‡æ ·æœ¬ï¼Œæ¶µç›–100ä¸ªä¸»é¢˜é›†ç¾¤ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªå¤šæ¨¡æ€è¯„ä¼°æ¡†æ¶ï¼Œåˆ©ç”¨è‡ªåŠ¨åŒ–çš„LLMé©±åŠ¨ç®¡é“è¿›è¡Œè§†è§‰ä»£ç åˆæˆï¼Œç¡®ä¿äº†ä»£ç ç”Ÿæˆçš„å‡†ç¡®æ€§ä¸å®é™…æ¸¸æˆå¼€å‘å·¥ä½œæµç¨‹ä¹‹é—´çš„æœ‰æ•ˆè¿æ¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19736', 'title': 'UserRL: Training Interactive User-Centric Agent via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2509.19736', 'abstract': 'UserRL framework enhances user-centric RL agents by optimizing reward assignment and user simulation, demonstrating the importance of these factors over model scale.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose UserRL, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal three key findings: (i) SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and establish UserRL as a practical pathway for developing robust user-centric agentic models. All codes and data are public for future research.', 'score': 9, 'issue_id': 6120, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': 'e6894061ac505189', 'authors': ['Cheng Qian', 'Zuxin Liu', 'Akshara Prabhakar', 'Jielin Qiu', 'Zhiwei Liu', 'Haolin Chen', 'Shirley Kokane', 'Heng Ji', 'Weiran Yao', 'Shelby Heinecke', 'Silvio Savarese', 'Caiming Xiong', 'Huan Wang'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2509.19736.jpg', 'data': {'categories': ['#training', '#open_source', '#reasoning', '#rl', '#rlhf', '#agents', '#optimization'], 'emoji': 'ğŸ¤', 'ru': {'title': 'ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ RL Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ UserRL - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ RL Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼Ğ¸. ĞĞ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğº Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾, Ğ¿Ñ€Ğ¾Ğ´ÑƒĞ¼Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€Ğ°Ğ¼Ğ¾Ñ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ¸ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ ÑÑ‚Ğ¾Ğ»ÑŒ Ğ¶Ğµ Ğ²Ğ°Ğ¶ĞµĞ½, ĞºĞ°Ğº Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'UserRL: Optimizing Rewards for User-Centric Reinforcement Learning', 'desc': "The UserRL framework focuses on improving reinforcement learning (RL) agents by refining how rewards are assigned and how user interactions are simulated. It emphasizes that the design of reward shaping and user simulation is just as important as the size of the model itself. Through experiments with various models, the study finds that initial training methods and thoughtful scoring of user interactions significantly enhance the agents' performance in multi-turn dialogues. The findings suggest that even less powerful simulators can effectively train agents, making the approach accessible for further research."}, 'zh': {'title': 'ç”¨æˆ·ä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ æ–°è·¯å¾„', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†UserRLæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ä¼˜åŒ–å¥–åŠ±åˆ†é…å’Œç”¨æˆ·æ¨¡æ‹Ÿæ¥å¢å¼ºä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»£ç†ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç”¨æˆ·äº¤äº’çš„å¤šæ ·æ€§å’ŒåŠ¨æ€æ€§å¯¹ä»£ç†çš„è®­ç»ƒæå‡ºäº†æŒ‘æˆ˜ï¼Œå› æ­¤éœ€è¦åœ¨æ ‡å‡†åŒ–çš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚é€šè¿‡ç³»ç»Ÿåœ°è°ƒæ•´å¥–åŠ±åˆ†é…å’Œè¯„åˆ†è®¡ç®—ï¼Œç ”ç©¶å‘ç°åˆå§‹äº¤äº’èƒ½åŠ›çš„è§£é”å’Œæœ‰æ•ˆçš„å¤šè½®äº¤äº’è®¾è®¡å¯¹å­¦ä¹ æ•ˆæœè‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ›´å¼ºå¤§çš„æ¨¡æ‹Ÿç”¨æˆ·æœ‰åŠ©äºè®­ç»ƒï¼Œä½†å¼€æºæ¨¡æ‹Ÿå™¨ä»ç„¶æ˜¯ä¸€ä¸ªç»æµæœ‰æ•ˆçš„é€‰æ‹©ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21318', 'title': 'SD3.5-Flash: Distribution-Guided Distillation of Generative Flows', 'url': 'https://huggingface.co/papers/2509.21318', 'abstract': 'SD3.5-Flash is an efficient few-step distillation framework that enhances image generation on consumer devices using rectified flow models with innovations like timestep sharing and split-timestep fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: "timestep sharing" to reduce gradient noise and "split-timestep fine-tuning" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.', 'score': 8, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '37d7c4e28964afd7', 'authors': ['Hmrishav Bandyopadhyay', 'Rahim Entezari', 'Jim Scott', 'Reshinth Adithyan', 'Yi-Zhe Song', 'Varun Jampani'], 'affiliations': ['Stability AI', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2509.21318.jpg', 'data': {'categories': ['#dataset', '#training', '#inference', '#optimization', '#data', '#cv', '#diffusion'], 'emoji': 'âš¡', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²', 'desc': 'SD3.5-Flash Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ·Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑˆĞ°Ğ³Ğ¾Ğ² Ğ½Ğ° Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ rectified flow Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº timestep sharing Ğ¸ split-timestep fine-tuning. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ text encoder Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ° Ğ¼Ğ°Ğ»Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ², Ğ´ĞµĞ»Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ AI Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Democratizing Image Generation with SD3.5-Flash', 'desc': 'SD3.5-Flash is a new framework designed to improve image generation on everyday devices by using a few-step distillation method. It focuses on simplifying complex rectified flow models to make them more efficient for consumer hardware. The framework introduces innovative techniques like timestep sharing to minimize noise during training and split-timestep fine-tuning to enhance the alignment with user prompts. Overall, SD3.5-Flash allows for faster and more memory-efficient image generation, making advanced AI technology available to a wider range of devices.'}, 'zh': {'title': 'è®©å…ˆè¿›ç”ŸæˆAIè§¦æ‰‹å¯åŠ', 'desc': 'SD3.5-Flashæ˜¯ä¸€ç§é«˜æ•ˆçš„å°‘æ­¥è’¸é¦æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ¶ˆè´¹è€…è®¾å¤‡ä¸Šçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡æ–°åˆ¶å®šçš„åˆ†å¸ƒåŒ¹é…ç›®æ ‡ï¼Œè’¸é¦è®¡ç®—ä¸Šæ˜‚è´µçš„ä¿®æ­£æµæ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹å°‘æ­¥ç”Ÿæˆè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼šæ—¶é—´æ­¥å…±äº«ä»¥å‡å°‘æ¢¯åº¦å™ªå£°ï¼Œä»¥åŠåˆ†æ­¥æ—¶é—´å¾®è°ƒä»¥æ”¹å–„æç¤ºå¯¹é½ã€‚é€šè¿‡å…¨é¢çš„ç®¡é“ä¼˜åŒ–ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿå®ç°äº†å¿«é€Ÿç”Ÿæˆå’Œå†…å­˜é«˜æ•ˆçš„éƒ¨ç½²ï¼Œä½¿å¾—ä»æ‰‹æœºåˆ°æ¡Œé¢ç”µè„‘çš„å„ç§è®¾å¤‡éƒ½èƒ½è½»æ¾è®¿é—®å…ˆè¿›çš„ç”ŸæˆAIã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21070', 'title': 'ScaleDiff: Scaling Difficult Problems for Advanced Mathematical\n  Reasoning', 'url': 'https://huggingface.co/papers/2509.21070', 'abstract': 'ScaleDiff uses an adaptive thinking model to identify and generate difficult mathematical problems, improving the performance of large reasoning models with cost-efficient training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME\'24, AIME\'25, HMMT-Feb\'25, BRUMO\'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.', 'score': 8, 'issue_id': 6099, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '9ea0c009dabeaeb0', 'authors': ['Qizhi Pei', 'Zhuoshi Pan', 'Honglin Lin', 'Xin Gao', 'Yu Li', 'Zinan Tang', 'Conghui He', 'Rui Yan', 'Lijun Wu'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China', 'OpenDataLab, Shanghai Artificial Intelligence Laboratory', 'School of Artificial Intelligence, Wuhan University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21070.jpg', 'data': {'categories': ['#math', '#training', '#optimization', '#transfer_learning', '#reasoning', '#dataset'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ', 'desc': 'ScaleDiff Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ñ†ĞµĞ»ÑŒÑ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°Ğ¼Ğ¸ Â«ThinkingÂ» Ğ¸ Â«NoThinkingÂ». Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ DiffGen-8B ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğµ Ğ±ĞµĞ· Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ñ… API Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ”Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-Math-7B-Instruct Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ ScaleDiff-Math Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 11.3% Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ 65.9% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Revolutionizing Problem Generation for Enhanced AI Reasoning', 'desc': 'ScaleDiff introduces an adaptive thinking model that identifies and generates challenging mathematical problems, enhancing the training of large reasoning models (LRMs) in a cost-effective manner. By efficiently filtering difficult problems from existing datasets with a single forward pass, it simplifies the problem generation process, avoiding complex prompting methods. The specialized generator, DiffGen-8B, produces new difficult problems at scale, leading to significant performance improvements in models fine-tuned on this data. This approach not only reduces computational costs but also demonstrates a clear scaling effect in model performance as the number of difficult problems increases.'}, 'zh': {'title': 'ScaleDiffï¼šé«˜æ•ˆç”Ÿæˆå›°éš¾æ•°å­¦é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ', 'desc': 'ScaleDiff æ˜¯ä¸€ç§è‡ªé€‚åº”æ€ç»´æ¨¡å‹ï¼Œæ—¨åœ¨è¯†åˆ«å’Œç”Ÿæˆå›°éš¾çš„æ•°å­¦é—®é¢˜ï¼Œä»è€Œæé«˜å¤§å‹æ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é™ä½è®­ç»ƒæˆæœ¬ã€‚è¯¥æ–¹æ³•é€šè¿‡å•æ¬¡å‰å‘ä¼ æ’­æœ‰æ•ˆè¯†åˆ«ç°æœ‰æ•°æ®é›†ä¸­å›°éš¾é—®é¢˜ï¼Œè‡ªåŠ¨åˆ‡æ¢æ€ç»´æ¨¡å¼ï¼Œç®€åŒ–äº†é—®é¢˜ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬è®­ç»ƒçš„ä¸“é—¨å›°éš¾é—®é¢˜ç”Ÿæˆå™¨ DiffGen-8B èƒ½å¤Ÿå¤§è§„æ¨¡ç”Ÿæˆæ–°é—®é¢˜ï¼Œé¿å…äº†å¤æ‚çš„é€å®ä¾‹æç¤ºå’Œé«˜æ˜‚çš„ API æˆæœ¬ã€‚é€šè¿‡åœ¨ ScaleDiff-Math æ•°æ®é›†ä¸Šå¾®è°ƒ Qwen2.5-Math-7B-Instructï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†åœ¨å›°éš¾åŸºå‡†ä¸Šéšç€é—®é¢˜æ•°é‡å¢åŠ è€Œæå‡çš„æ˜æ˜¾æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20414', 'title': 'SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and\n  Self-Reflective Agent', 'url': 'https://huggingface.co/papers/2509.20414', 'abstract': 'SceneWeaver, a reflective agentic framework, uses a language model-based planner to iteratively refine 3D scene synthesis, achieving high physical, visual, and semantic quality across diverse instructions.  \t\t\t\t\tAI-generated summary \t\t\t\t Indoor scene synthesis has become increasingly important with the rise of Embodied AI, which requires 3D environments that are not only visually realistic but also physically plausible and functionally diverse. While recent approaches have advanced visual fidelity, they often remain constrained to fixed scene categories, lack sufficient object-level detail and physical consistency, and struggle to align with complex user instructions. In this work, we present SceneWeaver, a reflective agentic framework that unifies diverse scene synthesis paradigms through tool-based iterative refinement. At its core, SceneWeaver employs a language model-based planner to select from a suite of extensible scene generation tools, ranging from data-driven generative models to visual- and LLM-based methods, guided by self-evaluation of physical plausibility, visual realism, and semantic alignment with user input. This closed-loop reason-act-reflect design enables the agent to identify semantic inconsistencies, invoke targeted tools, and update the environment over successive iterations. Extensive experiments on both common and open-vocabulary room types demonstrate that SceneWeaver not only outperforms prior methods on physical, visual, and semantic metrics, but also generalizes effectively to complex scenes with diverse instructions, marking a step toward general-purpose 3D environment generation. Project website: https://scene-weaver.github.io/.', 'score': 8, 'issue_id': 6098, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '4b1e989136f96d93', 'authors': ['Yandan Yang', 'Baoxiong Jia', 'Shujie Zhang', 'Siyuan Huang'], 'affiliations': ['State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20414.jpg', 'data': {'categories': ['#3d', '#reasoning', '#games', '#alignment', '#agents'], 'emoji': 'ğŸ ', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚Ğ¾Ñ€: AI-Ğ°Ğ³ĞµĞ½Ñ‚ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ 3D Ğ¸Ğ½Ñ‚ĞµÑ€ÑŒĞµÑ€Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ°Ğ¼Ğ¾Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ', 'desc': 'SceneWeaver - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° 3D ÑÑ†ĞµĞ½, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ-Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ†ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸ ÑÑ†ĞµĞ½, SceneWeaver ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'SceneWeaver: Crafting Realistic 3D Environments with Iterative Refinement', 'desc': 'SceneWeaver is a framework designed for creating 3D scenes that are not only visually appealing but also physically realistic and semantically accurate. It utilizes a language model-based planner to iteratively refine the scene generation process, allowing for adjustments based on user instructions. By integrating various scene generation tools and employing a closed-loop reasoning approach, SceneWeaver can identify and correct inconsistencies in the generated scenes. This innovative method significantly improves the quality of 3D environments, making it suitable for a wide range of applications in Embodied AI.'}, 'zh': {'title': 'SceneWeaverï¼šæ™ºèƒ½3Dåœºæ™¯åˆæˆçš„æ–°çªç ´', 'desc': 'SceneWeaveræ˜¯ä¸€ä¸ªåæ€æ€§ä»£ç†æ¡†æ¶ï¼Œåˆ©ç”¨åŸºäºè¯­è¨€æ¨¡å‹çš„è§„åˆ’å™¨æ¥è¿­ä»£ä¼˜åŒ–3Dåœºæ™¯åˆæˆã€‚å®ƒèƒ½å¤Ÿåœ¨å¤šæ ·åŒ–çš„æŒ‡ä»¤ä¸‹ï¼Œå®ç°é«˜æ°´å¹³çš„ç‰©ç†ã€è§†è§‰å’Œè¯­ä¹‰è´¨é‡ã€‚è¯¥æ¡†æ¶é€šè¿‡å·¥å…·é©±åŠ¨çš„è¿­ä»£ç²¾ç‚¼ï¼Œç»Ÿä¸€äº†ä¸åŒçš„åœºæ™¯åˆæˆèŒƒå¼ã€‚å®éªŒè¡¨æ˜ï¼ŒSceneWeaveråœ¨ç‰©ç†ã€è§†è§‰å’Œè¯­ä¹‰æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ä»¥å¾€çš„æ–¹æ³•ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”å¤æ‚åœºæ™¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21302', 'title': 'Quantized Visual Geometry Grounded Transformer', 'url': 'https://huggingface.co/papers/2509.21302', 'abstract': 'QuantVGGT, a quantization framework for Visual Geometry Grounded Transformers, achieves state-of-the-art results with memory reduction and acceleration while maintaining high reconstruction accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have made remarkable progress with the use of large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has become a common practice for compressing and accelerating models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. This paper proposes the first Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two technical contributions: First, we introduce Dual-Smoothed Fine-Grained Quantization, which integrates pre-global Hadamard rotation and post-local channel smoothing to mitigate heavy-tailed distributions and inter-channel variance robustly. Second, we design Noise-Filtered Diverse Sampling, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges. Comprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with a great margin. We highlight that our 4-bit QuantVGGT can deliver a 3.7times memory reduction and 2.5times acceleration in real-hardware inference, while maintaining reconstruction accuracy above 98\\% of its full-precision counterpart. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios. Our code is released in https://github.com/wlfeng0509/QuantVGGT.', 'score': 7, 'issue_id': 6104, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '8f544c805aaf2f14', 'authors': ['Weilun Feng', 'Haotong Qin', 'Mingqiang Wu', 'Chuanguang Yang', 'Yuqi Li', 'Xiangqi Li', 'Zhulin An', 'Libo Huang', 'Yulun Zhang', 'Michele Magno', 'Yongjun Xu'], 'affiliations': ['ETH Zurich', 'Institute of Computing Technology, Chinese Academy of Sciences', 'Shanghai Jiao Tong University', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2509.21302.jpg', 'data': {'categories': ['#benchmark', '#3d', '#optimization', '#inference'], 'emoji': 'ğŸ§Š', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ĞµĞ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ QuantVGGT - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Visual Geometry Grounded Transformers, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ 3D Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ½Ñ‹Ñ… VGGT Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹ Ñ Ñ‚ÑĞ¶ĞµĞ»Ñ‹Ğ¼Ğ¸ Ñ…Ğ²Ğ¾ÑÑ‚Ğ°Ğ¼Ğ¸, Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ²Ğ¸Ğ´Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ° 3D Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Dual-Smoothed Fine-Grained Quantization Ğ´Ğ»Ñ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¸ Noise-Filtered Diverse Sampling Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ°Ğ»Ğ¸Ğ±Ñ€Ğ¾Ğ²ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 3.7 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ² 2.5 Ñ€Ğ°Ğ·Ğ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ 98% Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Efficient Quantization', 'desc': 'QuantVGGT is a novel quantization framework designed specifically for Visual Geometry Grounded Transformers (VGGTs), which are advanced models for 3D reconstruction. The framework addresses the challenges of high memory usage and slow processing speeds that hinder the deployment of these models in real-world applications. It introduces two key techniques: Dual-Smoothed Fine-Grained Quantization to handle heavy-tailed activation distributions, and Noise-Filtered Diverse Sampling to stabilize calibration for quantization. As a result, QuantVGGT achieves significant improvements in memory efficiency and processing speed while maintaining high reconstruction accuracy, making it suitable for resource-limited environments.'}, 'zh': {'title': 'é‡åŒ–æ¡†æ¶QuantVGGTï¼šé«˜æ•ˆåŠ é€Ÿä¸ç²¾åº¦å…¼å¾—', 'desc': 'QuantVGGTæ˜¯ä¸€ä¸ªé’ˆå¯¹è§†è§‰å‡ ä½•åŸºç¡€å˜æ¢å™¨çš„é‡åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨ä¿æŒé«˜é‡å»ºç²¾åº¦çš„åŒæ—¶ï¼Œå®ç°å†…å­˜å‡å°‘å’ŒåŠ é€Ÿã€‚è¯¥æ¡†æ¶é€šè¿‡åŒå¹³æ»‘ç»†ç²’åº¦é‡åŒ–å’Œå™ªå£°è¿‡æ»¤å¤šæ ·åŒ–é‡‡æ ·ä¸¤é¡¹æŠ€æœ¯è´¡çŒ®ï¼Œè§£å†³äº†å¤§è§„æ¨¡VGGTåœ¨é‡åŒ–è¿‡ç¨‹ä¸­é¢ä¸´çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒQuantVGGTåœ¨ä¸åŒåŸºå‡†å’Œæ¯”ç‰¹å®½åº¦ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„é‡åŒ–æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œ4ä½çš„QuantVGGTåœ¨å®é™…ç¡¬ä»¶æ¨ç†ä¸­å®ç°äº†3.7å€çš„å†…å­˜å‡å°‘å’Œ2.5å€çš„åŠ é€Ÿï¼ŒåŒæ—¶ä¿æŒäº†è¶…è¿‡98%çš„é‡å»ºç²¾åº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21106', 'title': 'BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback', 'url': 'https://huggingface.co/papers/2509.21106', 'abstract': "BESPOKE is a benchmark for evaluating personalization in search-augmented LLMs using authentic user data and detailed feedback.  \t\t\t\t\tAI-generated summary \t\t\t\t Search-augmented large language models (LLMs) have advanced information-seeking tasks by integrating retrieval into generation, reducing users' cognitive burden compared to traditional search systems. Yet they remain insufficient for fully addressing diverse user needs, which requires recognizing how the same query can reflect different intents across users and delivering information in preferred forms. While recent systems such as ChatGPT and Gemini attempt personalization by leveraging user histories, systematic evaluation of such personalization is under-explored. To address this gap, we propose BESPOKE, the realistic benchmark for evaluating personalization in search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting authentic chat and search histories directly from humans, and diagnostic, by pairing responses with fine-grained preference scores and feedback. The benchmark is constructed through long-term, deeply engaged human annotation, where human annotators contributed their own histories, authored queries with detailed information needs, and evaluated responses with scores and diagnostic feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key requirements for effective personalization in information-seeking tasks, providing a foundation for fine-grained evaluation of personalized search-augmented LLMs. Our code and data are available at https://augustinlib.github.io/BESPOKE/.", 'score': 6, 'issue_id': 6107, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '73c2d8751f7343c3', 'authors': ['Hyunseo Kim', 'Sangam Lee', 'Kwangwook Seo', 'Dongha Lee'], 'affiliations': ['Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2509.21106.jpg', 'data': {'categories': ['#survey', '#multimodal', '#dataset', '#alignment', '#benchmark'], 'emoji': 'ğŸ¯', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… LLM Ğ¿Ğ¾Ğ´ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº BESPOKE Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ¡ÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºĞ°Ğº ChatGPT Ğ¸ Gemini Ğ¿Ñ‹Ñ‚Ğ°ÑÑ‚ÑÑ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, Ğ½Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ñ‹Ğ»Ğ° Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ»ÑĞ´ĞµĞ¹ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·ÑŒÑ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'BESPOKE: Personalization Benchmark for Search-Augmented LLMs', 'desc': 'BESPOKE is a new benchmark designed to evaluate how well search-augmented large language models (LLMs) personalize responses based on user data and feedback. It addresses the challenge of understanding different user intents behind the same query and aims to improve the relevance of information provided. By collecting real user chat and search histories, BESPOKE allows for a detailed analysis of how well these models meet individual preferences. This systematic evaluation helps identify essential factors for effective personalization in information-seeking tasks, paving the way for advancements in personalized AI systems.'}, 'zh': {'title': 'BESPOKEï¼šä¸ªæ€§åŒ–æœç´¢çš„è¯„ä¼°æ–°åŸºå‡†', 'desc': 'BESPOKEæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æœç´¢å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸ªæ€§åŒ–æ•ˆæœçš„åŸºå‡†ï¼Œä½¿ç”¨çœŸå®ç”¨æˆ·æ•°æ®å’Œè¯¦ç»†åé¦ˆã€‚è¯¥åŸºå‡†æ—¨åœ¨é€šè¿‡æ”¶é›†çœŸå®çš„èŠå¤©å’Œæœç´¢å†å²ï¼Œå¸®åŠ©è¯†åˆ«ç”¨æˆ·åœ¨ç›¸åŒæŸ¥è¯¢ä¸‹çš„ä¸åŒæ„å›¾ï¼Œå¹¶æä¾›ç”¨æˆ·åå¥½çš„ä¿¡æ¯å½¢å¼ã€‚å°½ç®¡ç°æœ‰ç³»ç»Ÿå¦‚ChatGPTå’ŒGeminiå°è¯•é€šè¿‡ç”¨æˆ·å†å²å®ç°ä¸ªæ€§åŒ–ï¼Œä½†å¯¹è¿™ç§ä¸ªæ€§åŒ–çš„ç³»ç»Ÿè¯„ä¼°ä»ç„¶ä¸è¶³ã€‚BESPOKEé€šè¿‡é•¿æœŸçš„äººå·¥æ³¨é‡Šï¼Œç»“åˆç”¨æˆ·çš„å†å²å’Œåé¦ˆï¼Œæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„ä¸ªæ€§åŒ–è¯„ä¼°åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21317', 'title': 'Interactive Recommendation Agent with Active User Commands', 'url': 'https://huggingface.co/papers/2509.21317', 'abstract': "IRF, a new recommendation system using natural language commands, improves user satisfaction and business outcomes through a dual-agent architecture and simulation-augmented knowledge distillation.  \t\t\t\t\tAI-generated summary \t\t\t\t Traditional recommender systems rely on passive feedback mechanisms that limit users to simple choices such as like and dislike. However, these coarse-grained signals fail to capture users' nuanced behavior motivations and intentions. In turn, current systems cannot also distinguish which specific item attributes drive user satisfaction or dissatisfaction, resulting in inaccurate preference modeling. These fundamental limitations create a persistent gap between user intentions and system interpretations, ultimately undermining user satisfaction and harming system effectiveness.   To address these limitations, we introduce the Interactive Recommendation Feed (IRF), a pioneering paradigm that enables natural language commands within mainstream recommendation feeds. Unlike traditional systems that confine users to passive implicit behavioral influence, IRF empowers active explicit control over recommendation policies through real-time linguistic commands. To support this paradigm, we develop RecBot, a dual-agent architecture where a Parser Agent transforms linguistic expressions into structured preferences and a Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly policy adjustment. To enable practical deployment, we employ simulation-augmented knowledge distillation to achieve efficient performance while maintaining strong reasoning capabilities. Through extensive offline and long-term online experiments, RecBot shows significant improvements in both user satisfaction and business outcomes.", 'score': 5, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'df6bcc9456addce3', 'authors': ['Jiakai Tang', 'Yujie Luo', 'Xunke Xi', 'Fei Sun', 'Xueyang Feng', 'Sunhao Dai', 'Chao Yi', 'Dian Chen', 'Zhujin Gao', 'Yang Li', 'Xu Chen', 'Wen Chen', 'Jian Wu', 'Yuning Jiang', 'Bo Zheng'], 'affiliations': ['Alibaba Group, Beijing, China', 'Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China', 'University of Chinese Academy of Sciences, China'], 'pdf_title_img': 'assets/pdf/title_img/2509.21317.jpg', 'data': {'categories': ['#training', '#architecture', '#reasoning', '#optimization', '#multimodal', '#agents'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ¼ - Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑˆÑŒ ÑƒĞ²Ğ¸Ğ´ĞµÑ‚ÑŒ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Interactive Recommendation Feed (IRF) - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° RecBot Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Parser Agent Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ, Ğ° Planner Agent Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ knowledge distillation Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸ Ğ¿Ğ°ÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ feedback.'}, 'en': {'title': 'Empowering Users with Natural Language in Recommendations', 'desc': 'The paper presents the Interactive Recommendation Feed (IRF), a novel recommendation system that utilizes natural language commands to enhance user engagement and satisfaction. Unlike traditional systems that rely on passive feedback, IRF allows users to actively express their preferences through real-time linguistic inputs. This is achieved using a dual-agent architecture, where a Parser Agent interprets user commands and a Planner Agent adjusts recommendation policies dynamically. The system employs simulation-augmented knowledge distillation to optimize performance while preserving robust reasoning capabilities, leading to improved user satisfaction and better business outcomes.'}, 'zh': {'title': 'è‡ªç„¶è¯­è¨€é©±åŠ¨çš„æ™ºèƒ½æ¨èç³»ç»Ÿ', 'desc': 'IRFæ˜¯ä¸€ç§æ–°å‹æ¨èç³»ç»Ÿï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è‡ªç„¶è¯­è¨€å‘½ä»¤è¿›è¡Œäº’åŠ¨ï¼Œä»è€Œæé«˜ç”¨æˆ·æ»¡æ„åº¦å’Œå•†ä¸šæˆæœã€‚ä¸ä¼ ç»Ÿæ¨èç³»ç»Ÿä¾èµ–è¢«åŠ¨åé¦ˆä¸åŒï¼ŒIRFé€šè¿‡å®æ—¶è¯­è¨€å‘½ä»¤èµ‹äºˆç”¨æˆ·ä¸»åŠ¨æ§åˆ¶æ¨èç­–ç•¥çš„èƒ½åŠ›ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨åŒä»£ç†æ¶æ„ï¼Œè§£æä»£ç†å°†è¯­è¨€è¡¨è¾¾è½¬åŒ–ä¸ºç»“æ„åŒ–åå¥½ï¼Œè§„åˆ’ä»£ç†åˆ™åŠ¨æ€è°ƒæ•´æ¨èç­–ç•¥ã€‚é€šè¿‡æ¨¡æ‹Ÿå¢å¼ºçŸ¥è¯†è’¸é¦ï¼ŒIRFåœ¨ä¿æŒå¼ºå¤§æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20293', 'title': 'When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity', 'url': 'https://huggingface.co/papers/2509.20293', 'abstract': "LLM-judged benchmarks can produce unreliable rankings due to schema incoherence and factor collapse, which are diagnosed using schematic adherence and psychometric validity.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We release our code at https://anonymous.4open.science/r/judgment-to-noise-947D/README.md", 'score': 5, 'issue_id': 6107, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '330d430b8bdd426b', 'authors': ['Benjamin Feuer', 'Chiung-Yi Tseng', 'Astitwa Sarthak Lathe', 'Oussama Elachqar', 'John P Dickerson'], 'affiliations': ['Google DeepMind', 'University of California, Berkeley', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2509.20293.jpg', 'data': {'categories': ['#hallucinations', '#interpretability', '#benchmark'], 'emoji': 'âš–ï¸', 'ru': {'title': 'LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ´Ğ°ÑÑ‚ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ·-Ğ·Ğ° Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ²', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸ ÑĞµÑ€ÑŒĞµĞ·Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ³Ğ´Ğµ LLM Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ÑÑ‚ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ÑÑƒĞ´ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ°: ÑÑ…ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ (ĞºĞ¾Ğ³Ğ´Ğ° ÑÑƒĞ´ÑŒĞ¸ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑÑÑ‚ÑÑ Ğ¾Ñ‚ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸) Ğ¸ ĞºĞ¾Ğ»Ğ»Ğ°Ğ¿Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² (ĞºĞ¾Ğ³Ğ´Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ Ğ½ĞµÑ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ğ¼Ñ‹Ğ¼Ğ¸). Ğ”Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ñ‹ Ğ´Ğ²Ğ° Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°: Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ ÑÑ…ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Arena-Hard Auto Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ LLM-ÑÑƒĞ´ÑŒĞ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºÑ€Ğ°Ğ¹Ğ½Ğµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ½ĞµĞ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼ÑƒÑ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ (ÑĞ²Ñ‹ÑˆĞµ 90% Ğ´Ğ»Ñ DeepSeek-R1-32B) Ğ¸ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ (Ğ²Ñ‹ÑˆĞµ 0.93), Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼Ğ¸.'}, 'en': {'title': 'Improving Reliability in LLM-Judged Benchmarks', 'desc': 'This paper discusses the problems with benchmarks that use large language models (LLMs) for evaluation, highlighting issues like schema incoherence and factor collapse. It introduces two diagnostic tools: schematic adherence, which measures how well judges follow their own evaluation criteria, and psychometric validity, which assesses the reliability of the benchmark results. The authors demonstrate that many popular judges exhibit high levels of unexplained variance and strong correlations between criteria, indicating unreliable rankings. They propose guidelines for creating more reliable benchmarks that can better reflect model performance without the noise introduced by current methods.'}, 'zh': {'title': 'æå‡LLMè¯„ä¼°åŸºå‡†çš„å¯é æ€§', 'desc': 'æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°åŸºå‡†æ—¶å¯èƒ½å‡ºç°çš„ä¸å¯é æ’åé—®é¢˜ï¼Œä¸»è¦ç”±äºè¯„ä¼°æ¡†æ¶ä¸ä¸€è‡´å’Œå› ç´ å´©æºƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æœºåˆ¶æ¥è¯Šæ–­è¿™äº›é—®é¢˜ï¼šä¸€æ˜¯é€šè¿‡è¯„ä¼°æ¡†æ¶çš„ä¸€è‡´æ€§æ¥é‡åŒ–è¯„å®¡ç»“æœçš„è§£é‡Šç¨‹åº¦ï¼ŒäºŒæ˜¯é€šè¿‡å¿ƒç†æµ‹é‡æœ‰æ•ˆæ€§æ¥è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæµè¡Œè¯„å®¡è€…åœ¨è¯„ä¼°æ—¶å­˜åœ¨ä¸¥é‡çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´é«˜è¾¾90%çš„æœªè§£é‡Šæ–¹å·®ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†è®¾è®¡ç¼ºé™·ï¼Œå¹¶æä¾›äº†æ„å»ºæ›´å¯é çš„LLMè¯„ä¼°åŸºå‡†çš„å¯è¡ŒåŸåˆ™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21113', 'title': 'MOSS-ChatV: Reinforcement Learning with Process Reasoning Reward for\n  Video Temporal Reasoning', 'url': 'https://huggingface.co/papers/2509.21113', 'abstract': 'MOSS-ChatV, a reinforcement learning framework with a DTW-based reward, improves video reasoning consistency and performance across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Video reasoning has emerged as a critical capability for multimodal large language models (MLLMs), requiring models to move beyond static perception toward coherent understanding of temporal dynamics in complex scenes. Yet existing MLLMs often exhibit process inconsistency, where intermediate reasoning drifts from video dynamics even when the final answer is correct, undermining interpretability and robustness. To address this issue, we introduce MOSS-ChatV, a reinforcement learning framework with a Dynamic Time Warping (DTW)-based process reward. This rule-based reward aligns reasoning traces with temporally grounded references, enabling efficient process supervision without auxiliary reward models. We further identify dynamic state prediction as a key measure of video reasoning and construct MOSS-Video, a benchmark with annotated reasoning traces, where the training split is used to fine-tune MOSS-ChatV and the held-out split is reserved for evaluation. MOSS-ChatV achieves 87.2\\% on MOSS-Video (test) and improves performance on general video benchmarks such as MVBench and MMVU. The framework consistently yields gains across different architectures, including Qwen2.5-VL and Phi-2, confirming its broad applicability. Evaluations with GPT-4o-as-judge further show that MOSS-ChatV produces more consistent and stable reasoning traces.', 'score': 4, 'issue_id': 6100, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '8f3917f7caee4e55', 'authors': ['Sicheng Tao', 'Jungang Li', 'Yibo Yan', 'Junyan Zhang', 'Yubo Gao', 'Hanqian Li', 'ShuHang Xun', 'Yuxuan Fan', 'Hong Chen', 'Jianxiang He', 'Xuming Hu'], 'affiliations': ['HIT', 'HKUST', 'HKUST (GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2509.21113.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#video', '#rl', '#interpretability', '#multimodal'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¡Ğ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° MOSS-ChatV â€” Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Dynamic Time Warping (DTW) Ğ´Ğ»Ñ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»ĞµĞ´Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ°Ğ¼Ğ¸. Ğ‘Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MOSS-Video Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ»ĞµĞ´Ğ°Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MVBench Ğ¸ MMVU.'}, 'en': {'title': 'Enhancing Video Reasoning Consistency with MOSS-ChatV', 'desc': "MOSS-ChatV is a reinforcement learning framework designed to enhance video reasoning in multimodal large language models (MLLMs). It utilizes a Dynamic Time Warping (DTW)-based reward system to ensure that the reasoning process aligns closely with the actual dynamics of the video content. This approach addresses the issue of process inconsistency, where the model's intermediate reasoning may not accurately reflect the video, even if the final answer is correct. By introducing a benchmark called MOSS-Video, MOSS-ChatV demonstrates significant improvements in reasoning consistency and performance across various video benchmarks."}, 'zh': {'title': 'MOSS-ChatVï¼šæå‡è§†é¢‘æ¨ç†ä¸€è‡´æ€§çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'MOSS-ChatVæ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¡†æ¶ï¼Œé‡‡ç”¨åŠ¨æ€æ—¶é—´è§„æ•´ï¼ˆDTWï¼‰ä½œä¸ºå¥–åŠ±æœºåˆ¶ï¼Œæ—¨åœ¨æé«˜è§†é¢‘æ¨ç†çš„ä¸€è‡´æ€§å’Œæ€§èƒ½ã€‚ç°æœ‰çš„å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†è§†é¢‘æ—¶å¸¸å¸¸å‡ºç°æ¨ç†è¿‡ç¨‹ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå³ä½¿æœ€ç»ˆç­”æ¡ˆæ­£ç¡®ï¼Œæ¨ç†è¿‡ç¨‹ä¹Ÿå¯èƒ½åç¦»è§†é¢‘åŠ¨æ€ã€‚MOSS-ChatVé€šè¿‡å°†æ¨ç†è½¨è¿¹ä¸æ—¶é—´ä¸Šå¯¹é½çš„å‚è€ƒè¿›è¡Œå¯¹æ¯”ï¼Œæä¾›äº†é«˜æ•ˆçš„è¿‡ç¨‹ç›‘ç£ï¼Œé¿å…äº†ä½¿ç”¨è¾…åŠ©å¥–åŠ±æ¨¡å‹ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¯æ˜äº†å…¶åœ¨ä¸åŒæ¶æ„ä¸­çš„å¹¿æ³›é€‚ç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.21042', 'title': 'Behind RoPE: How Does Causal Mask Encode Positional Information?', 'url': 'https://huggingface.co/papers/2509.21042', 'abstract': "The causal mask in Transformer decoders induces position-dependent attention patterns, which can interact with explicit positional encodings like RoPE, affecting their relative attention score patterns.  \t\t\t\t\tAI-generated summary \t\t\t\t While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.", 'score': 4, 'issue_id': 6101, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': '4228468a7ee42c33', 'authors': ['Junu Kim', 'Xiao Liu', 'Zhenghao Lin', 'Lei Ji', 'Yeyun Gong', 'Edward Choi'], 'affiliations': ['KAIST', 'Microsoft Research'], 'pdf_title_img': 'assets/pdf/title_img/2509.21042.jpg', 'data': {'categories': ['#math', '#optimization', '#architecture', '#interpretability'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞšĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° ĞºĞ°Ğº ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°ÑĞºĞ° Ğ² Ğ´ĞµĞºĞ¾Ğ´ĞµÑ€Ğ°Ñ… Transformer ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ·Ğ°Ğ²Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ°Ğ¶Ğµ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¸Ğ»Ğ¸ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ», Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ğ´ÑƒÑ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğ¾Ñ‚Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ-ĞºĞ»ÑÑ‡, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº. Ğ’Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ Ğ¸ RoPE Ğ¸ÑĞºĞ°Ğ¶Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ RoPE, Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‰Ğ°Ñ Ğ¸Ñ… Ğ² Ğ½ĞµĞ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ ÑÑ„Ñ„ĞµĞºÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµÑ‚ÑÑ Ğ² ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡Ñ‘Ñ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ğ¸Ñ ĞºĞ°ÑƒĞ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°ÑĞºĞ¸ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Causal Mask: A Hidden Source of Positional Information in Transformers', 'desc': 'This paper explores how the causal mask in Transformer decoders influences attention patterns based on position. It shows that the causal mask can create position-dependent attention scores, even without additional parameters. The study reveals that this effect tends to prioritize nearby query-key pairs, similar to traditional positional encodings like RoPE. Furthermore, the interaction between the causal mask and RoPE can distort the expected relative attention patterns, highlighting the need to consider both sources of positional information in model design.'}, 'zh': {'title': 'å› æœæ©ç ä¸ä½ç½®ç¼–ç çš„ç›¸äº’ä½œç”¨', 'desc': 'åœ¨Transformerè§£ç å™¨ä¸­ï¼Œå› æœæ©ç ä¼šå¼•å…¥ä¾èµ–äºä½ç½®çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™ä¸æ˜¾å¼ä½ç½®ç¼–ç ï¼ˆå¦‚RoPEï¼‰ç›¸äº’ä½œç”¨ï¼Œå½±å“ç›¸å¯¹æ³¨æ„åŠ›å¾—åˆ†æ¨¡å¼ã€‚æœ¬æ–‡è¯æ˜äº†å› æœæ©ç èƒ½å¤Ÿåœ¨æ²¡æœ‰å‚æ•°æˆ–è¾“å…¥å› æœä¾èµ–çš„æƒ…å†µä¸‹ï¼Œè¯±å¯¼å‡ºä½ç½®ä¾èµ–çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§è¯±å¯¼çš„æ³¨æ„åŠ›æ¨¡å¼å€¾å‘äºåå‘äºç›¸é‚»çš„æŸ¥è¯¢-é”®å¯¹ï¼Œç±»ä¼¼äºå¸¸è§ä½ç½®ç¼–ç çš„è¡Œä¸ºã€‚å®è¯åˆ†æç¡®è®¤ï¼Œè®­ç»ƒåçš„æ¨¡å‹è¡¨ç°å‡ºç›¸åŒçš„è¡Œä¸ºï¼Œå­¦ä¹ çš„å‚æ•°è¿›ä¸€æ­¥æ”¾å¤§äº†è¿™äº›æ¨¡å¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19228', 'title': 'CompLLM: Compression for Long Context Q&A', 'url': 'https://huggingface.co/papers/2509.19228', 'abstract': 'CompLLM, a soft compression technique for LLMs, divides contexts into segments for efficient, scalable, and reusable compression, enhancing performance and reducing computational costs.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.', 'score': 4, 'issue_id': 6119, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': 'e9dc06e52d011d7e', 'authors': ['Gabriele Berton', 'Jayakrishnan Unnikrishnan', 'Son Tran', 'Mubarak Shah'], 'affiliations': ['Amazon', 'Center For Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2509.19228.jpg', 'data': {'categories': ['#inference', '#training', '#optimization', '#long_context'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ LLM', 'desc': 'CompLLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ self-attention Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ². Ğ’Ğ¼ĞµÑÑ‚Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ²ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° ĞºĞ°Ğº ĞµĞ´Ğ¸Ğ½Ğ¾Ğ³Ğ¾ Ñ†ĞµĞ»Ğ¾Ğ³Ğ¾, CompLLM Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ ĞµĞ³Ğ¾ Ğ½Ğ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ ÑĞ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾. Ğ­Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ 100k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸ Ğ´Ğ²ÑƒĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ²Ñ€ĞµĞ¼Ñ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ° Ğ² 4 Ñ€Ğ°Ğ·Ğ° Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ KV cache Ğ½Ğ° 50%, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Efficient Context Compression for Scalable LLMs', 'desc': 'CompLLM is a novel soft compression technique designed to improve the efficiency of Large Language Models (LLMs) when handling long contexts. By segmenting the input context and compressing each segment independently, it reduces the computational complexity from quadratic to linear, making it more scalable. This method allows for the reuse of compressed segments across different queries, significantly lowering the computational costs and speeding up processing times. Experiments show that CompLLM can achieve a 2x compression rate while maintaining performance comparable to uncompressed contexts, especially on longer sequences.'}, 'zh': {'title': 'CompLLMï¼šé«˜æ•ˆçš„ä¸Šä¸‹æ–‡å‹ç¼©æŠ€æœ¯', 'desc': 'CompLLMæ˜¯ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è½¯å‹ç¼©æŠ€æœ¯ï¼Œå®ƒé€šè¿‡å°†ä¸Šä¸‹æ–‡åˆ’åˆ†ä¸ºå¤šä¸ªæ®µè½æ¥å®ç°é«˜æ•ˆã€å¯æ‰©å±•å’Œå¯é‡ç”¨çš„å‹ç¼©ã€‚è¿™ç§æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿå‹ç¼©æŠ€æœ¯åœ¨å¤„ç†é•¿ä¸Šä¸‹æ–‡æ—¶çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ã€‚CompLLMçš„è®¾è®¡ä½¿å¾—å‹ç¼©è¿‡ç¨‹ä¸ä¸Šä¸‹æ–‡é•¿åº¦å‘ˆçº¿æ€§å…³ç³»ï¼Œä»è€Œæé«˜äº†æ•ˆç‡ï¼Œå¹¶å…è®¸åœ¨ä¸åŒæŸ¥è¯¢ä¸­é‡ç”¨å‹ç¼©æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCompLLMåœ¨é«˜ä¸Šä¸‹æ–‡é•¿åº¦ä¸‹èƒ½å¤Ÿæ˜¾è‘—åŠ å¿«é¦–æ¬¡ç”Ÿæˆæ—¶é—´ï¼Œå¹¶åœ¨æ€§èƒ½ä¸Šä¸æœªå‹ç¼©ä¸Šä¸‹æ–‡ç›¸å½“ï¼Œç”šè‡³åœ¨éå¸¸é•¿çš„åºåˆ—ä¸­è¡¨ç°æ›´ä½³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20868', 'title': 'StyleBench: Evaluating thinking styles in Large Language Models', 'url': 'https://huggingface.co/papers/2509.20868', 'abstract': 'StyleBench evaluates various reasoning styles across tasks and models, revealing that strategy efficacy depends on model scale and task type.  \t\t\t\t\tAI-generated summary \t\t\t\t The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.', 'score': 3, 'issue_id': 6098, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'fa050993ad9cfcf9', 'authors': ['Junyu Guo', 'Shangding Gu', 'Ming Jin', 'Costas Spanos', 'Javad Lavaei'], 'affiliations': ['University of California, Berkeley', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2509.20868.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#benchmark', '#multimodal'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€ĞµÑˆĞ°ĞµÑ‚, ĞºĞ°ĞºĞ¾Ğ¹ ÑÑ‚Ğ¸Ğ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ StyleBench - Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¿ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¸Ğ½Ğ³Ğ° (Chain of Thought, Tree of Thought Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ) Ğ½Ğ° 15 Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¾Ñ‚ 270M Ğ´Ğ¾ 120B Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸: Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ»ÑƒÑ‡ÑˆĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ° Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ½Ğ° Ñ‡ĞµÑ‚ĞºĞ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼ Ğ¸ ÑĞºĞ»Ğ¾Ğ½Ğ½Ñ‹ Ğº ÑƒĞ³Ğ°Ğ´Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ¾Ğ³Ğ´Ğ° ĞºĞ°Ğº ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking Reasoning Styles for Optimal Model Performance', 'desc': 'StyleBench is a new benchmark designed to evaluate different reasoning styles used in prompts for Large Language Models (LLMs). It examines how the effectiveness of these reasoning strategies varies depending on the model size and the type of task being performed. The study analyzes five reasoning styles across various tasks using 15 different models, revealing that no single style works best for all scenarios. The results indicate that larger models perform better with complex reasoning styles, while simpler styles are more efficient for straightforward tasks.'}, 'zh': {'title': 'æ¨ç†é£æ ¼ä¸æ¨¡å‹è§„æ¨¡çš„æœ€ä½³é€‰æ‹©', 'desc': 'StyleBench æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºç³»ç»Ÿè¯„ä¼°ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­çš„æ¨ç†é£æ ¼ã€‚æˆ‘ä»¬ç ”ç©¶äº†äº”ç§ä»£è¡¨æ€§çš„æ¨ç†é£æ ¼ï¼ŒåŒ…æ‹¬æ€ç»´é“¾ï¼ˆCoTï¼‰ã€æ€ç»´æ ‘ï¼ˆToTï¼‰ã€æ€ç»´ç®—æ³•ï¼ˆAoTï¼‰ã€æ€ç»´è‰å›¾ï¼ˆSoTï¼‰å’Œè‰ç¨¿é“¾ï¼ˆCoDï¼‰ï¼Œå¹¶åœ¨äº”ä¸ªæ¨ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œæ²¡æœ‰ä¸€ç§æ¨ç†é£æ ¼åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ˜¯æœ€ä½³çš„ï¼Œå…¶æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºæ¨¡å‹è§„æ¨¡å’Œä»»åŠ¡ç±»å‹ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒåŸºäºæœç´¢çš„æ–¹æ³•åœ¨å¼€æ”¾æ€§é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€è¦å¤§è§„æ¨¡æ¨¡å‹ï¼Œè€Œç®€æ´çš„é£æ ¼åœ¨å®šä¹‰æ˜ç¡®çš„ä»»åŠ¡ä¸­åˆ™èƒ½æ˜¾è‘—æé«˜æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20109', 'title': 'Discrete Diffusion for Reflective Vision-Language-Action Models in\n  Autonomous Driving', 'url': 'https://huggingface.co/papers/2509.20109', 'abstract': 'ReflectDrive uses a reflection mechanism with discrete diffusion and pre-trained Diffusion Language Models to generate safe trajectories for autonomous driving systems.  \t\t\t\t\tAI-generated summary \t\t\t\t End-to-End (E2E) solutions have emerged as a mainstream approach for autonomous driving systems, with Vision-Language-Action (VLA) models representing a new paradigm that leverages pre-trained multimodal knowledge from Vision-Language Models (VLMs) to interpret and interact with complex real-world environments. However, these methods remain constrained by the limitations of imitation learning, which struggles to inherently encode physical rules during training. Existing approaches often rely on complex rule-based post-refinement, employ reinforcement learning that remains largely limited to simulation, or utilize diffusion guidance that requires computationally expensive gradient calculations. To address these challenges, we introduce ReflectDrive, a novel learning-based framework that integrates a reflection mechanism for safe trajectory generation via discrete diffusion. We first discretize the two-dimensional driving space to construct an action codebook, enabling the use of pre-trained Diffusion Language Models for planning tasks through fine-tuning. Central to our approach is a safety-aware reflection mechanism that performs iterative self-correction without gradient computation. Our method begins with goal-conditioned trajectory generation to model multi-modal driving behaviors. Based on this, we apply local search methods to identify unsafe tokens and determine feasible solutions, which then serve as safe anchors for inpainting-based regeneration. Evaluated on the NAVSIM benchmark, ReflectDrive demonstrates significant advantages in safety-critical trajectory generation, offering a scalable and reliable solution for autonomous driving systems.', 'score': 3, 'issue_id': 6101, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '7a95d9753228508a', 'authors': ['Pengxiang Li', 'Yinan Zheng', 'Yue Wang', 'Huimin Wang', 'Hang Zhao', 'Jingjing Liu', 'Xianyuan Zhan', 'Kun Zhan', 'Xianpeng Lang'], 'affiliations': ['LiAuto', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20109.jpg', 'data': {'categories': ['#optimization', '#agents', '#diffusion', '#benchmark', '#multimodal', '#rl'], 'emoji': 'ğŸš—', 'ru': {'title': 'Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ñ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ñ', 'desc': 'ReflectDrive Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸ Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Diffusion Language Models. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ²ÑƒĞ¼ĞµÑ€Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ ĞºĞ½Ğ¸Ğ³Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ±ĞµĞ· Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ². ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ñ€ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ NAVSIM.'}, 'en': {'title': 'ReflectDrive: Safe Trajectories for Autonomous Driving', 'desc': 'ReflectDrive is a novel framework designed to enhance the safety of trajectory generation for autonomous driving systems. It utilizes a reflection mechanism combined with discrete diffusion and pre-trained Diffusion Language Models to create safe driving paths. Unlike traditional methods that rely heavily on imitation learning or complex rule-based systems, ReflectDrive incorporates a safety-aware approach that allows for iterative self-correction without the need for gradient calculations. This innovative method has shown significant improvements in safety-critical scenarios, making it a promising solution for the future of autonomous driving.'}, 'zh': {'title': 'ReflectDriveï¼šå®‰å…¨çš„è‡ªåŠ¨é©¾é©¶è½¨è¿¹ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'ReflectDrive æ˜¯ä¸€ç§æ–°é¢–çš„å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨åå°„æœºåˆ¶å’Œç¦»æ•£æ‰©æ•£ç”Ÿæˆå®‰å…¨çš„è‡ªåŠ¨é©¾é©¶è½¨è¿¹ã€‚è¯¥æ–¹æ³•é€šè¿‡ç¦»æ•£åŒ–äºŒç»´é©¾é©¶ç©ºé—´ï¼Œæ„å»ºåŠ¨ä½œä»£ç æœ¬ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£è¯­è¨€æ¨¡å‹è¿›è¡Œè§„åˆ’ä»»åŠ¡ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ä¸ªå®‰å…¨æ„ŸçŸ¥çš„åå°„æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸è®¡ç®—æ¢¯åº¦çš„æƒ…å†µä¸‹è¿›è¡Œè¿­ä»£è‡ªæˆ‘ä¿®æ­£ã€‚ç»è¿‡åœ¨ NAVSIM åŸºå‡†ä¸Šçš„è¯„ä¼°ï¼ŒReflectDrive åœ¨å®‰å…¨å…³é”®çš„è½¨è¿¹ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæä¾›äº†å¯æ‰©å±•å’Œå¯é çš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19676', 'title': 'Thinking While Listening: Simple Test Time Scaling For Audio\n  Classification', 'url': 'https://huggingface.co/papers/2509.19676', 'abstract': 'A framework incorporating reasoning into audio classification improves performance through test-time scaling and lightweight retraining of embedding matrices.  \t\t\t\t\tAI-generated summary \t\t\t\t We propose a framework that enables neural models to "think while listening" to everyday sounds, thereby enhancing audio classification performance. Motivated by recent advances in the reasoning capabilities of large language models, we address two central questions: (i) how can thinking be incorporated into existing audio classification pipelines to enable reasoning in the category space and improve performance, and (ii) can a new architecture be designed from the ground up to support both thinking and test-time scaling? We demonstrate that in both settings, our models exhibit improved classification accuracy. Leveraging test-time scaling, we observe consistent gains as the number of sampled traces increases. Furthermore, we evaluate two open-source reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are capable of zero-shot reasoning, a lightweight approach--retraining only the embedding matrix of a frozen, smaller model like GPT-2--can surpass the performance of billion-parameter text-based reasoning models.', 'score': 3, 'issue_id': 6108, 'pub_date': '2025-09-24', 'pub_date_card': {'ru': '24 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 24', 'zh': '9æœˆ24æ—¥'}, 'hash': '9423c6d2db011a9e', 'authors': ['Prateek Verma', 'Mert Pilanci'], 'affiliations': ['Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.19676.jpg', 'data': {'categories': ['#audio', '#reasoning', '#multimodal', '#training', '#open_source', '#architecture'], 'emoji': 'ğŸ§', 'ru': {'title': 'ĞĞ°ÑƒÑ‡Ğ¸Ñ‚ÑŒ AI Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ·Ğ²ÑƒĞºĞ¾Ğ²', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ 'Ğ´ÑƒĞ¼Ğ°Ñ‚ÑŒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¿Ñ€Ğ¾ÑĞ»ÑƒÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ' Ğ·Ğ²ÑƒĞºĞ¾Ğ², Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ÑƒÑĞ¿ĞµÑ…Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ test-time scaling. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞµĞ¼Ğ¿Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ°ÑÑ. Ğ›ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ GPT-2 Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞµĞ» Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²."}, 'en': {'title': 'Enhancing Audio Classification with Reasoning and Lightweight Retraining', 'desc': 'This paper presents a new framework that enhances audio classification by integrating reasoning capabilities into neural models. It explores how to incorporate reasoning into existing audio classification systems to improve their performance and proposes a novel architecture that supports reasoning and test-time scaling. The authors demonstrate that their models achieve better classification accuracy, especially when using test-time scaling with increased sampled traces. Additionally, they show that a lightweight retraining approach can outperform larger reasoning models, highlighting the effectiveness of optimizing smaller models for audio tasks.'}, 'zh': {'title': 'æ¨ç†æå‡éŸ³é¢‘åˆ†ç±»æ€§èƒ½çš„æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§å°†æ¨ç†èƒ½åŠ›èå…¥éŸ³é¢‘åˆ†ç±»çš„æ¡†æ¶ï¼Œä»è€Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬æ¢è®¨äº†å¦‚ä½•åœ¨ç°æœ‰éŸ³é¢‘åˆ†ç±»æµç¨‹ä¸­åŠ å…¥æ¨ç†ï¼Œä»¥æ”¹å–„åˆ†ç±»æ•ˆæœï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°æ¶æ„æ¥æ”¯æŒæ¨ç†å’Œæµ‹è¯•æ—¶çš„æ‰©å±•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç°æœ‰æ¨¡å‹ä¸Šè¿›è¡Œæ¨ç†ï¼Œè¿˜æ˜¯åœ¨æ–°æ¶æ„ä¸­ï¼Œåˆ†ç±»å‡†ç¡®ç‡éƒ½æœ‰æ˜¾è‘—æå‡ã€‚é€šè¿‡æµ‹è¯•æ—¶æ‰©å±•ï¼Œæˆ‘ä»¬å‘ç°éšç€é‡‡æ ·è½¨è¿¹æ•°é‡çš„å¢åŠ ï¼Œæ€§èƒ½æŒç»­æå‡ï¼Œä¸”è½»é‡çº§çš„é‡è®­ç»ƒæ–¹æ³•åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†å¤§å‹æ–‡æœ¬æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.19282', 'title': 'OverLayBench: A Benchmark for Layout-to-Image Generation with Dense\n  Overlaps', 'url': 'https://huggingface.co/papers/2509.19282', 'abstract': 'A new benchmark and metric are introduced to evaluate layout-to-image generation models on complex overlapping bounding boxes, along with a fine-tuned model to improve performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite steady progress in layout-to-image generation, current methods still struggle with layouts containing significant overlap between bounding boxes. We identify two primary challenges: (1) large overlapping regions and (2) overlapping instances with minimal semantic distinction. Through both qualitative examples and quantitative analysis, we demonstrate how these factors degrade generation quality. To systematically assess this issue, we introduce OverLayScore, a novel metric that quantifies the complexity of overlapping bounding boxes. Our analysis reveals that existing benchmarks are biased toward simpler cases with low OverLayScore values, limiting their effectiveness in evaluating model performance under more challenging conditions. To bridge this gap, we present OverLayBench, a new benchmark featuring high-quality annotations and a balanced distribution across different levels of OverLayScore. As an initial step toward improving performance on complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a curated amodal mask dataset. Together, our contributions lay the groundwork for more robust layout-to-image generation under realistic and challenging scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.', 'score': 3, 'issue_id': 6120, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': '2e4c637aaccbbb5f', 'authors': ['Bingnan Li', 'Chen-Yu Wang', 'Haiyang Xu', 'Xiang Zhang', 'Ethan Armand', 'Divyansh Srivastava', 'Xiaojun Shan', 'Zeyuan Chen', 'Jianwen Xie', 'Zhuowen Tu'], 'affiliations': ['Lambda, Inc', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2509.19282.jpg', 'data': {'categories': ['#cv', '#training', '#benchmark'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¼Ğ°ĞºĞµÑ‚Ñƒ - Ğ¾Ğ½Ğ¸ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ bounding box'Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ OverLayScore Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ñ‹ Ğ² ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ĞµĞ². ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº OverLayBench Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CreatiLayout-AM, Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞºÑ€Ñ‹Ñ‚Ğ¸ÑĞ¼Ğ¸."}, 'en': {'title': 'Enhancing Layout-to-Image Generation with OverLayScore and OverLayBench', 'desc': 'This paper addresses the challenges faced by layout-to-image generation models when dealing with complex overlapping bounding boxes. It introduces a new metric called OverLayScore, which measures the complexity of these overlaps, highlighting the limitations of existing benchmarks that favor simpler cases. The authors also present OverLayBench, a benchmark with high-quality annotations that provides a more balanced evaluation of model performance across varying levels of overlap complexity. Additionally, they propose CreatiLayout-AM, a fine-tuned model aimed at improving generation quality in scenarios with significant bounding box overlap.'}, 'zh': {'title': 'æå‡å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆçš„è¯„ä¼°æ ‡å‡†', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºå‡†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°åœ¨å¤æ‚é‡å è¾¹ç•Œæ¡†ä¸Šçš„å¸ƒå±€åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚æˆ‘ä»¬è¯†åˆ«å‡ºä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šå¤§é¢ç§¯é‡å åŒºåŸŸå’Œè¯­ä¹‰ä¸Šå‡ ä¹æ²¡æœ‰åŒºåˆ«çš„é‡å å®ä¾‹ã€‚é€šè¿‡å®šæ€§ç¤ºä¾‹å’Œå®šé‡åˆ†æï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¿™äº›å› ç´ å¦‚ä½•é™ä½ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OverLayScoreï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–é‡å è¾¹ç•Œæ¡†çš„å¤æ‚æ€§ï¼Œå¹¶æå‡ºäº†OverLayBenchåŸºå‡†ï¼Œä»¥ä¾¿åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ¡ä»¶ä¸‹è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20878', 'title': 'The Unanticipated Asymmetry Between Perceptual Optimization and\n  Assessment', 'url': 'https://huggingface.co/papers/2509.20878', 'abstract': 'The study reveals an asymmetry between perceptual optimization and image quality assessment, showing that effective IQA metrics are not always suitable for perceptual optimization, especially under adversarial training, and highlights the importance of discriminator design in optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization.', 'score': 2, 'issue_id': 6100, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'e54e3db347b3c50f', 'authors': ['Jiabei Zhang', 'Qi Wang', 'Siyu Wu', 'Du Chen', 'Tianhe Wu'], 'affiliations': ['Beihang University', 'City University of Hong Kong', 'Institute of Microelectronics of the Chinese Academy of Sciences', 'The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2509.20878.jpg', 'data': {'categories': ['#training', '#cv', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ IQA, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ adversarial Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. Ğ”Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ñ‹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ…Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑÑÑ‚ÑÑ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸Ğ³Ñ€Ğ°ĞµÑ‚ Ñ€ĞµÑˆĞ°ÑÑ‰ÑƒÑ Ñ€Ğ¾Ğ»ÑŒ - patch-level Ğ¸ ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹, Ñ‡ĞµĞ¼ vanilla Ğ¸Ğ»Ğ¸ Transformer-based Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹.'}, 'en': {'title': 'Bridging the Gap: Optimizing Perception Beyond Quality Assessment', 'desc': 'This study investigates the differences between perceptual optimization and image quality assessment (IQA) in machine learning. It finds that metrics that work well for assessing image quality do not always perform effectively when optimizing images, particularly in adversarial training scenarios. The research emphasizes the critical role of discriminator design in the optimization process, showing that certain architectures yield better results in detail reconstruction. Overall, the paper highlights the need for a deeper understanding of how loss functions relate to IQA metrics to improve perceptual optimization techniques.'}, 'zh': {'title': 'æ„ŸçŸ¥ä¼˜åŒ–ä¸å›¾åƒè´¨é‡è¯„ä¼°çš„éå¯¹ç§°æ€§', 'desc': 'æœ¬ç ”ç©¶æ­ç¤ºäº†æ„ŸçŸ¥ä¼˜åŒ–ä¸å›¾åƒè´¨é‡è¯„ä¼°ä¹‹é—´çš„éå¯¹ç§°æ€§ï¼Œè¡¨æ˜æœ‰æ•ˆçš„å›¾åƒè´¨é‡è¯„ä¼°æŒ‡æ ‡å¹¶ä¸æ€»æ˜¯é€‚åˆç”¨äºæ„ŸçŸ¥ä¼˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—è®­ç»ƒä¸‹ã€‚æ„ŸçŸ¥ä¼˜åŒ–ä¸»è¦ç”±ä¿çœŸåº¦ç›®æ ‡é©±åŠ¨ï¼Œå¼ºè°ƒè¯­ä¹‰ä¸€è‡´æ€§å’Œæ•´ä½“è§†è§‰çœŸå®æ„Ÿï¼Œè€Œå¯¹æŠ—ç›®æ ‡åˆ™é€šè¿‡å¢å¼ºæ„ŸçŸ¥æ¸…æ™°åº¦å’Œç»†èŠ‚æä¾›è¡¥å……ä¼˜åŒ–ã€‚æˆ‘ä»¬ç³»ç»Ÿåˆ†æäº†è¿™ä¸€ç°è±¡ï¼Œå‘ç°ä¿çœŸåº¦æŒ‡æ ‡åœ¨å›¾åƒè´¨é‡è¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ„ŸçŸ¥ä¼˜åŒ–ä¸­å´æœªå¿…æœ‰æ•ˆï¼Œå°¤å…¶åœ¨å¯¹æŠ—è®­ç»ƒä¸­è¿™ç§ä¸ä¸€è‡´æ€§æ›´åŠ æ˜æ˜¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜ï¼Œé‰´åˆ«å™¨çš„è®¾è®¡åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­èµ·ç€å†³å®šæ€§ä½œç”¨ï¼Œè¡¥ä¸çº§å’Œå·ç§¯æ¶æ„åœ¨ç»†èŠ‚é‡å»ºæ–¹é¢ä¼˜äºä¼ ç»Ÿæˆ–åŸºäºå˜æ¢å™¨çš„æ›¿ä»£æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20706', 'title': 'MI-Fuse: Label Fusion for Unsupervised Domain Adaptation with\n  Closed-Source Large-Audio Language Model', 'url': 'https://huggingface.co/papers/2509.20706', 'abstract': 'MI-Fuse, a denoised label fusion framework, enhances speech emotion recognition in target domains using an API-only LALM and a source-domain SER classifier, achieving better performance than the LALM and other baselines.  \t\t\t\t\tAI-generated summary \t\t\t\t Large audio-language models (LALMs) show strong zero-shot ability on speech tasks, suggesting promise for speech emotion recognition (SER). However, SER in real-world deployments often fails under domain mismatch, where source data are unavailable and powerful LALMs are accessible only through an API. We ask: given only unlabeled target-domain audio and an API-only LALM, can a student model be adapted to outperform the LALM in the target domain? To this end, we propose MI-Fuse, a denoised label fusion framework that supplements the LALM with a source-domain trained SER classifier as an auxiliary teacher. The framework draws multiple stochastic predictions from both teachers, weights their mean distributions by mutual-information-based uncertainty, and stabilizes training with an exponential moving average teacher. Experiments across three public emotion datasets and six cross-domain transfers show consistent gains, with the student surpassing the LALM and outperforming the strongest baseline by 3.9%. This approach strengthens emotion-aware speech systems without sharing source data, enabling realistic adaptation.', 'score': 2, 'issue_id': 6102, 'pub_date': '2025-09-25', 'pub_date_card': {'ru': '25 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 25', 'zh': '9æœˆ25æ—¥'}, 'hash': 'ae753d2855aba13d', 'authors': ['Hsiao-Ying Huang', 'Yi-Cheng Lin', 'Hung-yi Lee'], 'affiliations': ['National Taiwan University, Taiwan'], 'pdf_title_img': 'assets/pdf/title_img/2509.20706.jpg', 'data': {'categories': ['#optimization', '#audio', '#multimodal', '#transfer_learning', '#training'], 'emoji': 'ğŸ­', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº MI-Fuse Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹ Ğ² Ñ€ĞµÑ‡Ğ¸ Ğº Ğ½Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LALM), Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡ĞµÑ€ĞµĞ· API, Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ° Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑĞ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¸ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¼ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LALM Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ° 3.9% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºÑ€Ğ¾ÑÑ-Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ°.'}, 'en': {'title': 'Enhancing Speech Emotion Recognition with MI-Fuse', 'desc': 'The paper introduces MI-Fuse, a framework designed to improve speech emotion recognition (SER) in situations where there is a mismatch between the source and target domains. It utilizes an API-only large audio-language model (LALM) alongside a source-domain SER classifier to enhance performance. By employing a denoised label fusion technique, MI-Fuse combines predictions from both models, using mutual information to weigh their contributions effectively. The results demonstrate that this method allows a student model to outperform the LALM and other baseline models, achieving significant improvements in emotion recognition tasks.'}, 'zh': {'title': 'MI-Fuseï¼šæå‡è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«çš„å»å™ªæ ‡ç­¾èåˆæ¡†æ¶', 'desc': 'MI-Fuseæ˜¯ä¸€ç§å»å™ªæ ‡ç­¾èåˆæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜ç›®æ ‡é¢†åŸŸçš„è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰æ€§èƒ½ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ä»…é€šè¿‡APIè®¿é—®çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å’Œæºé¢†åŸŸè®­ç»ƒçš„SERåˆ†ç±»å™¨ï¼Œä½œä¸ºè¾…åŠ©æ•™å¸ˆã€‚é€šè¿‡ä»ä¸¤ä¸ªæ•™å¸ˆæ¨¡å‹ä¸­è·å–å¤šä¸ªéšæœºé¢„æµ‹ï¼Œå¹¶æ ¹æ®äº’ä¿¡æ¯çš„ä¸ç¡®å®šæ€§åŠ æƒå…¶å‡å€¼åˆ†å¸ƒï¼ŒMI-Fuseèƒ½å¤Ÿç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæƒ…æ„Ÿæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½è¶…è¿‡äº†LALMï¼Œæå‡å¹…åº¦è¾¾åˆ°3.9%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.20394', 'title': 'Blueprints of Trust: AI System Cards for End to End Transparency and\n  Governance', 'url': 'https://huggingface.co/papers/2509.20394', 'abstract': "The Hazard-Aware System Card (HASC) enhances AI system safety and accountability by integrating security and safety identifiers into a standardized framework.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon existing model card and system card concepts by integrating a comprehensive, dynamic record of an AI system's security and safety posture. The framework proposes a standardized system of identifiers, including a novel AI Safety Hazard (ASH) ID, to complement existing security identifiers like CVEs, allowing for clear and consistent communication of fixed flaws. By providing a single, accessible source of truth, the HASC empowers developers and stakeholders to make more informed decisions about AI system safety throughout its lifecycle. Ultimately, we also compare our proposed AI system cards with the ISO/IEC 42001:2023 standard and discuss how they can be used to complement each other, providing greater transparency and accountability for AI systems.", 'score': 2, 'issue_id': 6102, 'pub_date': '2025-09-23', 'pub_date_card': {'ru': '23 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 23', 'zh': '9æœˆ23æ—¥'}, 'hash': '0e773214c050c323', 'authors': ['Huzaifa Sidhpurwala', 'Emily Fox', 'Garth Mollett', 'Florencio Cano Gabarda', 'Roman Zhukov'], 'affiliations': ['Red Hat'], 'pdf_title_img': 'assets/pdf/title_img/2509.20394.jpg', 'data': {'categories': ['#dataset', '#architecture', '#ethics', '#data', '#security', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI Ñ‡ĞµÑ€ĞµĞ· ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸ ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ ÑƒĞ³Ñ€Ğ¾Ğ·', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Hazard-Aware System Card (HASC), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ model card Ğ¸ system card Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ AI Safety Hazard (ASH) ID, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° CVE. HASC ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ AI ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ñ‚ÑĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ¶Ğ¸Ğ·Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ†Ğ¸ĞºĞ»Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ¾Ğ¼ ISO/IEC 42001:2023 Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ°.'}, 'en': {'title': 'Enhancing AI Safety with the Hazard-Aware System Card', 'desc': 'The Hazard-Aware System Card (HASC) is a new framework aimed at improving the safety and accountability of AI systems. It combines security and safety identifiers into a standardized format, enhancing transparency in AI development and deployment. The HASC introduces a unique AI Safety Hazard (ASH) ID alongside existing security identifiers, facilitating better communication about vulnerabilities. By serving as a centralized resource, the HASC helps developers and stakeholders make informed decisions regarding AI system safety throughout its lifecycle.'}, 'zh': {'title': 'æå‡AIç³»ç»Ÿå®‰å…¨ä¸é€æ˜åº¦çš„å…³é”®', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶â€”â€”å±é™©æ„è¯†ç³»ç»Ÿå¡ï¼ˆHASCï¼‰ï¼Œæ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é€æ˜åº¦å’Œé—®è´£åˆ¶ã€‚HASCåœ¨ç°æœ‰çš„æ¨¡å‹å¡å’Œç³»ç»Ÿå¡æ¦‚å¿µåŸºç¡€ä¸Šï¼Œæ•´åˆäº†AIç³»ç»Ÿå®‰å…¨å’Œå®‰å…¨çŠ¶æ€çš„åŠ¨æ€è®°å½•ã€‚è¯¥æ¡†æ¶æå‡ºäº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ ‡è¯†ç¬¦ï¼ŒåŒ…æ‹¬æ–°é¢–çš„AIå®‰å…¨å±é™©ï¼ˆASHï¼‰IDï¼Œä»¥è¡¥å……ç°æœ‰çš„å®‰å…¨æ ‡è¯†ç¬¦ï¼Œå¦‚CVEï¼Œä»è€Œå®ç°ç¼ºé™·ä¿®å¤çš„æ¸…æ™°å’Œä¸€è‡´çš„æ²Ÿé€šã€‚é€šè¿‡æä¾›ä¸€ä¸ªå•ä¸€ã€å¯è®¿é—®çš„çœŸå®ä¿¡æ¯æ¥æºï¼ŒHASCä½¿å¼€å‘è€…å’Œåˆ©ç›Šç›¸å…³è€…èƒ½å¤Ÿåœ¨AIç³»ç»Ÿçš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸä¸­åšå‡ºæ›´æ˜æ™ºçš„å®‰å…¨å†³ç­–ã€‚'}}}, {'id': 'https://huggingface.co/papers/2509.18293', 'title': 'Evaluating Large Language Models for Detecting Antisemitism', 'url': 'https://huggingface.co/papers/2509.18293', 'abstract': "Evaluation of open-source LLMs for antisemitic content detection using in-context definition and a new Guided-CoT prompt shows improved performance and highlights differences in model utility, explainability, and reliability.  \t\t\t\t\tAI-generated summary \t\t\t\t Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition as a policy guideline. We explore various prompting techniques and design a new CoT-like prompt, Guided-CoT. Guided-CoT handles the in-context policy well, increasing performance across all evaluated models, regardless of decoding configuration, model sizes, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability.", 'score': 1, 'issue_id': 6118, 'pub_date': '2025-09-22', 'pub_date_card': {'ru': '22 ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 'en': 'September 22', 'zh': '9æœˆ22æ—¥'}, 'hash': 'c28b331ea06b1a84', 'authors': ['Jay Patel', 'Hrudayangam Mehta', 'Jeremy Blackburn'], 'affiliations': ['Binghamton University, NY, USA'], 'pdf_title_img': 'assets/pdf/title_img/2509.18293.jpg', 'data': {'categories': ['#alignment', '#training', '#open_source', '#interpretability', '#multimodal', '#dataset', '#ethics', '#benchmark'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Guided-CoT Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ°Ğ½Ñ‚Ğ¸ÑĞµĞ¼Ğ¸Ñ‚ÑĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ² open-source LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ»Ğ¸ Ğ²Ğ¾ÑĞµĞ¼ÑŒ open-source LLM Ğ½Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ°Ğ½Ñ‚Ğ¸ÑĞµĞ¼Ğ¸Ñ‚ÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ°Ğº Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Guided-CoT, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ» Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµÑ… Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. Llama 3.1 70B Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ GPT-3.5, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… open-source Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ LLM Ğ² Ğ¿Ğ»Ğ°Ğ½Ğµ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚Ğ¸, Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ²Ñ€Ğ°Ğ¶Ğ´ĞµĞ±Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°.'}, 'en': {'title': 'Enhancing Antisemitic Content Detection with Guided-CoT Prompts', 'desc': "This paper evaluates the effectiveness of eight open-source large language models (LLMs) in detecting antisemitic content on social media. It introduces a new prompting technique called Guided-CoT, which enhances the models' performance by effectively utilizing in-context definitions as guidelines. The study finds that Llama 3.1 70B outperforms fine-tuned GPT-3.5 in this task, demonstrating the importance of model selection. Additionally, the authors analyze the errors made by the LLMs and propose new metrics to measure the differences in their reasoning and explainability."}, 'zh': {'title': 'æå‡åçŠ¹å¤ªå†…å®¹æ£€æµ‹çš„æ¨¡å‹æ€§èƒ½', 'desc': 'æœ¬ç ”ç©¶è¯„ä¼°äº†å…«ç§å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ£€æµ‹åçŠ¹å¤ªå†…å®¹æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸Šä¸‹æ–‡å®šä¹‰ä½œä¸ºæ”¿ç­–æŒ‡å¯¼ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ–°çš„æç¤ºæŠ€æœ¯ï¼Œç§°ä¸ºGuided-CoTï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLlama 3.1 70Båœ¨æ€§èƒ½ä¸Šä¼˜äºå¾®è°ƒåçš„GPT-3.5ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†æ¨¡å‹çš„é”™è¯¯ï¼Œå¹¶å¼•å…¥äº†é‡åŒ–è¯­ä¹‰åå·®çš„æ–°æŒ‡æ ‡ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚å’ŒçŸ›ç›¾è¡Œä¸ºã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (5)', '#agi (1)', '#alignment (4)', '#architecture (8)', '#audio (2)', '#benchmark (14)', '#cv (6)', '#data (7)', '#dataset (11)', '#diffusion (4)', '#ethics (2)', '#games (6)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (6)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (4)', '#multilingual', '#multimodal (14)', '#open_source (7)', '#optimization (20)', '#plp', '#rag', '#reasoning (14)', '#rl (8)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security (1)', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (21)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-09-29 00:52',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-09-29 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-09-29 00:52')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    