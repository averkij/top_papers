
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 9 papers. March 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 марта</span> | <span id="title-articles-count">9 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-07.html">⬅️ <span id="prev-date">07.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-11.html">➡️ <span id="next-date">11.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'};
        let feedDateNext = {'ru': '11.03', 'en': '03/11', 'zh': '3月11日'};
        let feedDatePrev = {'ru': '07.03', 'en': '03/07', 'zh': '3月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 50, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Единая модель вознаграждения для улучшения мультимодальных AI-систем', 'desc': 'Статья представляет UnifiedReward - первую унифицированную модель вознаграждения для оценки мультимодального понимания и генерации. Модель обучается на большом наборе данных о человеческих предпочтениях, включающем задачи по генерации и пониманию изображений и видео. UnifiedReward используется для автоматического создания высококачественных пар предпочтений на основе моделей компьютерного зрения. Затем эти данные применяются для настройки предпочтений моделей с помощью метода Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': '统一奖励模型，提升多模态理解与生成', 'desc': '最近在人类偏好对齐方面的进展显著提升了多模态生成和理解的能力。关键方法是训练奖励模型来指导偏好优化。然而，现有模型通常是特定于任务的，限制了它们在不同视觉应用中的适应性。本文提出了UnifiedReward，这是第一个用于多模态理解和生成评估的统一奖励模型，能够同时进行成对排名和逐点评分，从而实现视觉模型的偏好对齐。'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 22, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с минимальным использованием токенов', 'desc': 'Статья представляет новый метод промптинга под названием Sketch-of-Thought (SoT), который сочетает когнитивно-вдохновленные парадигмы рассуждений с лингвистическими ограничениями. SoT направлен на минимизацию использования токенов при сохранении точности рассуждений в больших языковых моделях. Метод включает три парадигмы: Conceptual Chaining, Chunked Symbolism и Expert Lexicons, каждая из которых адаптирована для различных задач рассуждения. Эксперименты на 15 наборах данных показали, что SoT сокращает использование токенов на 76% без значительного влияния на точность, а в некоторых областях даже улучшает ее.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': '思维草图：高效推理的新方法', 'desc': '本文介绍了一种新的提示框架，称为思维草图（Sketch-of-Thought，SoT），旨在提高大型语言模型的推理能力，同时减少中间输出的冗长性。SoT结合了认知科学的推理范式和语言约束，以最小化令牌使用量，同时保持推理的准确性。该框架灵活，可以根据认知科学的不同推理范式进行定制，并通过轻量级路由模型动态选择。通过在15个推理数据集上的全面评估，SoT实现了76%的令牌减少，且对准确性影响微乎其微，甚至在某些领域提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 4, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Forgetting Transformer: Улучшение обработки длинных последовательностей в трансформерах', 'desc': "Исследователи представили новую модель под названием Forgetting Transformer (FoX), которая включает механизм 'забывающего внимания' в архитектуру трансформера. FoX превосходит стандартный трансформер в задачах моделирования языка с длинным контекстом и экстраполяции длины, сохраняя при этом высокую производительность на задачах с коротким контекстом. Модель совместима с алгоритмом FlashAttention и не требует позиционных эмбеддингов. Анализ показывает, что FoX сохраняет преимущества трансформера в обработке длинного контекста по сравнению с рекуррентными моделями."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': '遗忘变换器：提升长上下文建模的利器', 'desc': '本文提出了一种新的注意力机制，称为遗忘注意力（Forgetting Attention），可以有效地将遗忘门集成到Transformer模型中。通过以数据为依赖的方式降低未归一化注意力分数，遗忘注意力使得Transformer在长上下文语言建模和长度外推任务中表现优于传统的Transformer。我们还设计了一个“Pro”模块，结合了递归序列模型中的一些常见架构组件，显著提升了FoX和Transformer的性能。此外，FoX在长上下文任务中保持了Transformer的优势，超越了其他递归序列模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 3, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление поисковых способностей ИИ через обучение с подкреплением', 'desc': 'R1-Searcher - это новый двухэтапный подход к обучению с подкреплением, улучшающий способности больших языковых моделей к поиску информации. Он позволяет моделям автономно обращаться к внешним поисковым системам во время рассуждений. Метод основан исключительно на обучении с подкреплением, без необходимости в процессных наградах или дистилляции. Эксперименты показывают, что R1-Searcher превосходит предыдущие методы RAG, даже в сравнении с закрытой моделью GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': '增强推理能力，R1-Searcher助力LLMs', 'desc': '现有的大型推理模型（LRMs）展示了强化学习（RL）在增强大型语言模型（LLMs）复杂推理能力方面的潜力。尽管它们在数学和编程等挑战性任务上表现出色，但在处理时间敏感或知识密集的问题时，往往依赖内部知识，导致不准确和幻觉现象。为了解决这个问题，我们提出了R1-Searcher，这是一种新颖的基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。该方法允许LLMs在推理过程中自主调用外部搜索系统，以获取额外知识，从而显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 2, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RLVR: Революция в мультимодальном распознавании эмоций', 'desc': 'В статье представлено первое применение обучения с подкреплением с проверяемым вознаграждением (RLVR) к мультимодальной большой языковой модели для распознавания эмоций. RLVR используется для оптимизации Omni-модели, значительно улучшая её способности к рассуждению, точность распознавания эмоций и способность к обобщению. Модель демонстрирует повышенную производительность на исходных данных и устойчивость на новых наборах данных. Улучшенная способность к рассуждениям позволяет анализировать вклад различных модальностей в процесс распознавания эмоций.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': '情感识别中的全模态强化学习新突破', 'desc': '本研究首次将可验证奖励的强化学习（RLVR）应用于情感识别的全模态大型语言模型中。在这个任务中，视觉和音频模态起着至关重要的作用。通过使用RLVR，我们显著提升了模型在推理能力、情感识别准确性和泛化能力等三个关键方面的表现。此外，RLVR的引入不仅提高了模型在同分布数据上的整体性能，还在异分布数据集上展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 2, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'Комплексная система для обучения роботов домашним задачам', 'desc': 'Статья представляет BEHAVIOR Robot Suite (BRS) - комплексную систему для манипуляции роботов в домашних условиях. BRS основан на двуруком колесном роботе с 4-осевым торсом и включает интерфейс телеуправления для сбора данных и новый алгоритм обучения визуомоторным политикам. Система оценивается на пяти сложных бытовых задачах, требующих бимануальной координации, точной навигации и широкой досягаемости манипуляторов. BRS представляет значительный шаг вперед в решении задач роботизированной манипуляции в реальных домашних условиях.'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': '实现家庭任务的全身操控机器人', 'desc': '本论文介绍了BEHAVIOR机器人套件（BRS），旨在解决移动操作机器人在家庭任务中面临的挑战。研究表明，成功完成任务依赖于三项关键的全身控制能力：双手协调、稳定精确的导航和广泛的末端执行器可达性。BRS框架结合了一个双手轮式机器人和4自由度的躯干，提供了一种经济高效的全身遥操作接口用于数据收集，并提出了一种新算法用于学习全身视觉运动策略。通过在五个复杂的家庭任务上评估BRS，展示了其在长距离导航、与可动和可变形物体的交互以及在狭小空间中的操作能力。'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 1, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты мультимодального обучения', 'desc': 'Исследователи из DeepSeek R1 показали, как обучение с подкреплением может помочь LLM развивать сложные навыки рассуждения, включая моменты "эврики". Однако при попытках применить эти методы к мультимодальному обучению часто не удавалось достичь таких же результатов. В этом исследовании удалось впервые воспроизвести эти характеристики на мультимодальной модели без использования SFT, достигнув высокой точности. Авторы также делятся неудачными попытками и выводами, чтобы лучше понять возникающие трудности.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': '强化学习助力多模态推理的突破', 'desc': '最近，DeepSeek R1展示了如何通过简单的基于规则的激励来实现强化学习，使大型语言模型能够自主发展复杂的推理能力。这种能力在训练过程中表现为“恍然大悟”的时刻，模型会自我反思并增加响应长度。然而，尝试将这种成功扩展到多模态推理时，往往无法重现这些关键特征。在本报告中，我们首次成功复制了这些特征，并在非SFT的2B模型上实现了多模态推理的进展。'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 1, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Ветвление и слияние: новый путь к компактным и мощным языковым моделям', 'desc': 'Статья представляет новый подход к сжатию больших языковых моделей (LLM) под названием Branch-Merge distillation. Метод состоит из двух фаз: Branch, где знания из большой модели-учителя дистиллируются в специализированные модели-ученики, и Merge, где эти модели объединяются для улучшения обобщения. Эксперименты показали, что полученная модель TinyR1-32B-Preview превосходит аналоги по нескольким бенчмаркам. Этот подход предлагает масштабируемое решение для создания меньших, но эффективных LLM с пониженными вычислительными затратами.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': '分支合并蒸馏：高效压缩大型语言模型的创新方法', 'desc': '本文提出了一种新的模型蒸馏方法，称为分支合并蒸馏，旨在在保持性能的同时减少大型语言模型的体积。该方法分为两个阶段：分支阶段通过领域特定的监督微调将知识从大型教师模型选择性地蒸馏到专门的学生模型中；合并阶段则将这些学生模型合并，以实现跨领域知识转移并提高模型的泛化能力。实验结果表明，合并后的模型TinyR1-32B-Preview在多个基准测试中表现优于其对应的学生模型DeepSeek-R1-Distill-Qwen-32B。该方法为创建更小且高性能的语言模型提供了一种可扩展的解决方案，降低了计算成本和时间。'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 1, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': '🔁', 'ru': {'title': 'Многопопыточное обучение: путь к более эффективным языковым моделям', 'desc': 'Это исследование расширяет подход обучения с подкреплением для больших языковых моделей, внедряя многопопыточную задачу вместо стандартной однопопыточной. Модель получает несколько попыток ответить на вопрос, получая обратную связь после неверных ответов, что способствует улучшению рассуждений и эффективности поиска. Эксперименты показывают, что даже небольшая языковая модель, обученная на многопопыточной задаче, достигает значительно более высокой точности при оценке с большим количеством попыток. Результаты демонстрируют, что модель, обученная на многопопыточной задаче, не только показывает лучшие результаты на математических тестах, но и эффективнее улучшает свои ответы на основе обратной связи пользователя.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': '多次尝试，提升推理能力！', 'desc': '本研究探讨了在大型语言模型（LLM）中应用强化学习（RL）的新方法，特别是通过多次尝试的任务设置来提升模型的推理能力。与传统的单次回答不同，模型在每个问题上可以进行多次尝试，并在错误回答后获得反馈。这种多次尝试的任务设置促使模型改进之前的回答，从而提高搜索效率。实验结果表明，即使是小型LLM，在多次尝试的任务训练下，其准确率显著提高，显示出多次尝试对模型学习和反馈的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents', '#agi', '#alignment (1)', '#architecture (1)', '#audio (1)', '#benchmark', '#cv (2)', '#data', '#dataset (3)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations (1)', '#healthcare', '#inference', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual (1)', '#multimodal (4)', '#open_source (2)', '#optimization (7)', '#plp', '#rag (2)', '#reasoning (5)', '#rl (4)', '#rlhf (2)', '#robotics (1)', '#science', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-10 04:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-10 04:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-10 04:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    