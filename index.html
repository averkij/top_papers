
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 16 papers. May 26.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">26 мая</span> | <span id="title-articles-count">16 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-23.html">⬅️ <span id="prev-date">23.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-27.html">➡️ <span id="next-date">27.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'};
        let feedDateNext = {'ru': '27.05', 'en': '05/27', 'zh': '5月27日'};
        let feedDatePrev = {'ru': '23.05', 'en': '05/23', 'zh': '5月23日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.17612', 'title': 'Distilling LLM Agent into Small Models with Retrieval and Code Tools', 'url': 'https://huggingface.co/papers/2505.17612', 'abstract': 'Agent Distillation transfers reasoning and task-solving capabilities from large language models to smaller models using enhanced prompts and self-consistent actions, matching performance of larger models on various reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.', 'score': 21, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '258cfb9a5b51fa42', 'authors': ['Minki Kang', 'Jongwon Jeong', 'Seanie Lee', 'Jaewoong Cho', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST', 'KRAFTON'], 'pdf_title_img': 'assets/pdf/title_img/2505.17612.jpg', 'data': {'categories': ['#math', '#small_models', '#agents', '#transfer_learning', '#training', '#hallucinations', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Передача навыков агента: от больших моделей к малым', 'desc': 'Метод Agent Distillation позволяет передавать навыки рассуждения и решения задач от больших языковых моделей (LLM) к меньшим моделям. Этот подход использует улучшенные промпты и самосогласованную генерацию действий. Agent Distillation позволяет маленьким моделям достигать производительности, сравнимой с более крупными моделями, на различных задачах, требующих рассуждений. Метод был протестирован на восьми задачах в фактологических и математических областях.'}, 'en': {'title': 'Empowering Small Models with Big Model Intelligence', 'desc': 'Agent Distillation is a method that helps smaller language models (sLMs) learn reasoning and task-solving skills from larger language models (LLMs). It uses improved prompts and self-consistent actions to enhance the performance of sLMs on reasoning tasks, making them competitive with larger models. The approach addresses challenges like hallucination in sLMs when faced with rare facts or complex computations. By evaluating on various reasoning tasks, the study shows that even small models can perform well, paving the way for more efficient AI applications.'}, 'zh': {'title': '代理蒸馏：小型模型的推理能力提升', 'desc': '本论文提出了一种名为代理蒸馏的框架，旨在将大型语言模型（LLM）的推理和任务解决能力转移到较小的语言模型（sLM）中。通过使用增强的提示和自一致性动作，代理蒸馏能够在多个推理任务上实现与大型模型相当的性能。我们的方法包括引入一种新的提示方法和改进小型代理在测试时的鲁棒性。实验结果表明，参数量为0.5B到3B的小型模型可以在事实和数学领域的推理任务中与更大的模型竞争。'}}}, {'id': 'https://huggingface.co/papers/2505.17941', 'title': 'VeriThinker: Learning to Verify Makes Reasoning Model Efficient', 'url': 'https://huggingface.co/papers/2505.17941', 'abstract': 'VeriThinker reduces the length of complex reasoning chains in Large Reasoning Models (LRMs) by fine-tuning them on a verification task, thereby decreasing inference costs without significantly sacrificing accuracy.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker', 'score': 12, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'cfc0e5dae345ea81', 'authors': ['Zigeng Chen', 'Xinyin Ma', 'Gongfan Fang', 'Ruonan Yu', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.17941.jpg', 'data': {'categories': ['#math', '#optimization', '#inference', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное сжатие цепочек рассуждений без потери точности', 'desc': 'VeriThinker - это новый подход к сжатию цепочек рассуждений в крупных моделях рассуждений (LRM). Метод использует дообучение LRM на вспомогательной задаче верификации, что позволяет моделям лучше оценивать необходимость дальнейших шагов самоанализа. Эксперименты показывают, что VeriThinker значительно сокращает длину цепочек рассуждений, сохраняя или даже немного улучшая точность. Подход также демонстрирует возможность обобщения на спекулятивные рассуждения без дополнительного обучения.'}, 'en': {'title': 'Streamlining Reasoning with VeriThinker', 'desc': 'VeriThinker is a method designed to enhance Large Reasoning Models (LRMs) by reducing the length of their reasoning chains. It achieves this by fine-tuning the models on a verification task instead of directly on the original reasoning tasks. This approach helps the models become more efficient by minimizing unnecessary steps in their reasoning process, which lowers inference costs. Experimental results show that VeriThinker not only shortens reasoning chains but also improves accuracy in various tasks, demonstrating its effectiveness in optimizing LRM performance.'}, 'zh': {'title': 'VeriThinker：优化推理链，提升效率与准确性', 'desc': 'VeriThinker是一种新方法，通过在验证任务上微调大型推理模型（LRMs），减少复杂推理链的长度，从而降低推理成本而不显著牺牲准确性。传统方法直接在原始推理任务上微调模型，而VeriThinker则创新性地仅通过辅助验证任务进行微调。通过训练LRMs准确验证推理解决方案的正确性，模型能够更好地判断后续自我反思步骤的必要性，有效抑制过度思考。实验结果表明，VeriThinker显著减少了推理链的长度，同时保持或略微提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2505.16211', 'title': 'AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.16211', 'abstract': 'AudioTrust evaluates the trustworthiness of Audio Large Language Models across multifaceted dimensions, using a comprehensive dataset and specific metrics to assess their performance in real-world audio scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement and expanding applications of Audio Large Language Models (ALLMs) demand a rigorous understanding of their trustworthiness. However, systematic research on evaluating these models, particularly concerning risks unique to the audio modality, remains largely unexplored. Existing evaluation frameworks primarily focus on the text modality or address only a restricted set of safety dimensions, failing to adequately account for the unique characteristics and application scenarios inherent to the audio modality. We introduce AudioTrust-the first multifaceted trustworthiness evaluation framework and benchmark specifically designed for ALLMs. AudioTrust facilitates assessments across six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. To comprehensively evaluate these dimensions, AudioTrust is structured around 18 distinct experimental setups. Its core is a meticulously constructed dataset of over 4,420 audio/text samples, drawn from real-world scenarios (e.g., daily conversations, emergency calls, voice assistant interactions), specifically designed to probe the multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully designs 9 audio-specific evaluation metrics, and we employ a large-scale automated pipeline for objective and scalable scoring of model outputs. Experimental results reveal the trustworthiness boundaries and limitations of current state-of-the-art open-source and closed-source ALLMs when confronted with various high-risk audio scenarios, offering valuable insights for the secure and trustworthy deployment of future audio models. Our platform and benchmark are available at https://github.com/JusperLee/AudioTrust.', 'score': 10, 'issue_id': 3946, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '1849951d3588375e', 'authors': ['Kai Li', 'Can Shen', 'Yile Liu', 'Jirui Han', 'Kelong Zheng', 'Xuechao Zou', 'Zhe Wang', 'Xingjian Du', 'Shun Zhang', 'Hanjun Luo', 'Yingbin Jin', 'Xinxin Xing', 'Ziyang Ma', 'Yue Liu', 'Xiaojun Jia', 'Yifan Zhang', 'Junfeng Fang', 'Kun Wang', 'Yibo Yan', 'Haoyang Li', 'Yiming Li', 'Xiaobin Zhuang', 'Yang Liu', 'Haibo Hu', 'Zhuo Chen', 'Zhizheng Wu', 'Xiaolin Hu', 'Eng-Siong Chng', 'XiaoFeng Wang', 'Wenyuan Xu', 'Wei Dong', 'Xinfeng Li'], 'affiliations': ['ACM Member', 'BJTU', 'BNBU', 'Bytedance', 'CAS', 'HUST', 'Hong Kong Polytechnic University', 'Hong Kong University of Science and Technology (Guangzhou)', 'Independent Researcher', 'Nanyang Technological University', 'National University of Singapore', 'QHU', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong (Shenzhen)', 'Tsinghua University', 'University of Rochester', 'Waseda University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16211.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#security', '#ethics', '#open_source', '#dataset', '#audio'], 'emoji': '🎙️', 'ru': {'title': 'AudioTrust: Комплексная оценка надежности аудио ИИ', 'desc': 'AudioTrust - это первая многогранная система оценки надежности Аудио Больших Языковых Моделей (АБЛМ). Она оценивает АБЛМ по шести ключевым параметрам: справедливость, галлюцинации, безопасность, конфиденциальность, устойчивость и аутентификация. Система использует набор данных из более чем 4420 аудио/текстовых образцов из реальных сценариев и 9 специфических метрик оценки. Результаты экспериментов выявляют границы надежности и ограничения современных АБЛМ в различных высокорисковых аудиосценариях.'}, 'en': {'title': 'Evaluating Trust in Audio Large Language Models with AudioTrust', 'desc': 'AudioTrust is a novel framework designed to evaluate the trustworthiness of Audio Large Language Models (ALLMs) across multiple dimensions. It addresses the unique challenges and risks associated with audio data, which are often overlooked in existing evaluation methods that focus primarily on text. The framework includes a comprehensive dataset of over 4,420 audio/text samples and employs 18 experimental setups to assess six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. By utilizing nine audio-specific metrics and an automated scoring pipeline, AudioTrust provides insights into the limitations of current ALLMs, guiding their secure deployment in real-world applications.'}, 'zh': {'title': '音频模型信任评估新标准', 'desc': 'AudioTrust是一个专门为音频大型语言模型（ALLMs）设计的多维信任评估框架。它通过一个包含4420多个音频/文本样本的数据集，评估模型在公平性、幻觉、安全性、隐私、鲁棒性和认证等六个关键维度的表现。该框架采用了18种不同的实验设置和9个音频特定的评估指标，以确保全面评估ALLMs的信任worthiness。实验结果揭示了当前最先进的开源和闭源ALLMs在高风险音频场景下的信任边界和局限性，为未来音频模型的安全和可信部署提供了重要见解。'}}}, {'id': 'https://huggingface.co/papers/2505.18129', 'title': 'One RL to See Them All: Visual Triple Unified Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.18129', 'abstract': 'A unified reinforcement learning system, V-Triune, combines visual reasoning and perception tasks in vision-language models through a single training pipeline, achieving significant improvements across various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has significantly advanced the reasoning capabilities of vision-language models (VLMs). However, the use of RL beyond reasoning tasks remains largely unexplored, especially for perceptionintensive tasks like object detection and grounding. We propose V-Triune, a Visual Triple Unified Reinforcement Learning system that enables VLMs to jointly learn visual reasoning and perception tasks within a single training pipeline. V-Triune comprises triple complementary components: Sample-Level Data Formatting (to unify diverse task inputs), Verifier-Level Reward Computation (to deliver custom rewards via specialized verifiers) , and Source-Level Metric Monitoring (to diagnose problems at the data-source level). We further introduce a novel Dynamic IoU reward, which provides adaptive, progressive, and definite feedback for perception tasks handled by V-Triune. Our approach is instantiated within off-the-shelf RL training framework using open-source 7B and 32B backbone models. The resulting model, dubbed Orsta (One RL to See Them All), demonstrates consistent improvements across both reasoning and perception tasks. This broad capability is significantly shaped by its training on a diverse dataset, constructed around four representative visual reasoning tasks (Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding, Detection, Counting, and OCR). Subsequently, Orsta achieves substantial gains on MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1 across its various 7B and 32B model variants, with performance benefits extending to a wide range of downstream tasks. These results highlight the effectiveness and scalability of our unified RL approach for VLMs. The V-Triune system, along with the Orsta models, is publicly available at https://github.com/MiniMax-AI.', 'score': 9, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': 'e690c5668b7f4cd0', 'authors': ['Yan Ma', 'Linge Du', 'Xuyang Shen', 'Shaoxiang Chen', 'Pengfei Li', 'Qibing Ren', 'Lizhuang Ma', 'Yuchao Dai', 'Pengfei Liu', 'Junjie Yan'], 'affiliations': ['Google DeepMind', 'MiniMax-AI', 'OpenAI'], 'pdf_title_img': 'assets/pdf/title_img/2505.18129.jpg', 'data': {'categories': ['#rl', '#dataset', '#multimodal', '#optimization', '#training', '#open_source', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Единая система обучения с подкреплением для визуального ИИ', 'desc': 'V-Triune - это система обучения с подкреплением, объединяющая задачи визуального рассуждения и восприятия в визуально-языковых моделях через единый процесс обучения. Система включает три компонента: форматирование данных на уровне выборки, вычисление наград на уровне верификатора и мониторинг метрик на уровне источника. Введена новая динамическая награда IoU для задач восприятия. Результирующая модель Orsta демонстрирует значительные улучшения как в задачах рассуждения, так и в задачах восприятия.'}, 'en': {'title': 'Unifying Visual Reasoning and Perception in One RL System', 'desc': 'The paper introduces V-Triune, a unified reinforcement learning system designed to enhance vision-language models (VLMs) by integrating visual reasoning and perception tasks into a single training framework. It features three key components: Sample-Level Data Formatting for input unification, Verifier-Level Reward Computation for tailored reward systems, and Source-Level Metric Monitoring for data diagnostics. A novel Dynamic IoU reward mechanism is also proposed, providing adaptive feedback for perception tasks. The resulting model, Orsta, shows significant performance improvements across various reasoning and perception benchmarks, demonstrating the effectiveness of this unified approach.'}, 'zh': {'title': '统一强化学习，提升视觉推理与感知能力', 'desc': 'V-Triune是一个统一的强化学习系统，旨在通过单一的训练流程结合视觉推理和感知任务。该系统包含三个互补的组件，分别是样本级数据格式化、验证器级奖励计算和源级指标监控，以支持多样化的任务输入和定制化的奖励反馈。我们还引入了一种新的动态IoU奖励，为感知任务提供适应性和渐进性的反馈。通过在多样化数据集上训练，V-Triune显著提升了视觉语言模型在推理和感知任务上的表现。'}}}, {'id': 'https://huggingface.co/papers/2505.15692', 'title': 'Thought-Augmented Policy Optimization: Bridging External Guidance and\n  Internal Capabilities', 'url': 'https://huggingface.co/papers/2505.15692', 'abstract': 'A novel RL framework, TAPO, integrates external guidance to enhance model performance and exploration compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has emerged as an effective method for training reasoning models. However, existing RL approaches typically bias the model\'s output distribution toward reward-maximizing paths without introducing external knowledge. This limits their exploration capacity and results in a narrower reasoning capability boundary compared to base models. To address this limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel framework that augments RL by incorporating external high-level guidance ("thought patterns"). By adaptively integrating structured thoughts during training, TAPO effectively balances model-internal exploration and external guidance exploitation. Extensive experiments show that our approach significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva Math. Notably, these high-level thought patterns, abstracted from only 500 prior samples, generalize effectively across various tasks and models. This highlights TAPO\'s potential for broader applications across multiple tasks and domains. Our further analysis reveals that introducing external guidance produces powerful reasoning models with superior explainability of inference behavior and enhanced output readability.', 'score': 9, 'issue_id': 3946, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '5b1bedaf6be49ffa', 'authors': ['Jinyang Wu', 'Chonghua Liao', 'Mingkuan Feng', 'Shuai Zhang', 'Zhengqi Wen', 'Pengpeng Shao', 'Huazhe Xu', 'Jianhua Tao'], 'affiliations': ['Beijing National Research Center for Information Science and Technology', 'Department of Automation, Tsinghua University', 'Institution for Interdisciplinary Information Sciences, Tsinghua University', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15692.jpg', 'data': {'categories': ['#reasoning', '#training', '#interpretability', '#rl', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'TAPO: Усиление обучения с подкреплением внешними мыслительными паттернами', 'desc': 'TAPO - это новая система обучения с подкреплением, которая включает внешние подсказки для улучшения производительности и исследовательской способности моделей. В отличие от существующих методов, TAPO интегрирует структурированные мыслительные паттерны во время обучения, эффективно балансируя между внутренним исследованием модели и использованием внешних указаний. Эксперименты показывают значительное превосходство TAPO над GRPO на различных математических задачах. Анализ также выявил, что внедрение внешних указаний приводит к созданию мощных моделей рассуждения с улучшенной объяснимостью и читаемостью выводов.'}, 'en': {'title': 'Enhancing Reinforcement Learning with Thought Patterns', 'desc': 'The paper introduces TAPO (Thought-Augmented Policy Optimization), a new reinforcement learning framework that enhances model performance by integrating external guidance. Traditional RL methods often limit exploration by focusing solely on reward-maximizing paths, which restricts the reasoning capabilities of the models. TAPO addresses this issue by incorporating structured thought patterns during training, allowing for a better balance between internal exploration and external guidance. Experimental results demonstrate that TAPO significantly outperforms existing methods, leading to improved reasoning models that are more explainable and readable.'}, 'zh': {'title': 'TAPO：增强探索与推理的新框架', 'desc': '本文提出了一种新的强化学习框架TAPO，通过整合外部指导来提升模型性能和探索能力。传统的强化学习方法往往只关注最大化奖励路径，缺乏外部知识的引入，限制了模型的探索能力。TAPO通过在训练过程中适应性地整合结构化思维，平衡了模型内部的探索与外部指导的利用。实验结果表明，TAPO在多个任务上显著优于现有方法，展示了其在广泛应用中的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.17558', 'title': 'Teaching with Lies: Curriculum DPO on Synthetic Negatives for\n  Hallucination Detection', 'url': 'https://huggingface.co/papers/2505.17558', 'abstract': "The use of carefully crafted hallucinations in a curriculum learning approach within the DPO alignment procedure significantly enhances LLMs' hallucination detection abilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.", 'score': 7, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '9faa21418742a88c', 'authors': ['Shrey Pandit', 'Ashwin Vinod', 'Liu Leqi', 'Ying Ding'], 'affiliations': ['The University of Texas at Austin'], 'pdf_title_img': 'assets/pdf/title_img/2505.17558.jpg', 'data': {'categories': ['#alignment', '#benchmark', '#rlhf', '#training', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Обучение LLM распознавать галлюцинации с помощью самих галлюцинаций', 'desc': 'Статья описывает новый метод улучшения способности больших языковых моделей (LLM) обнаруживать галлюцинации. Авторы используют тщательно сконструированные галлюцинации в качестве негативных примеров в процедуре DPO-выравнивания. Метод включает стратегию обучения с учебным планом, постепенно переходя от легких образцов к более сложным. Эксперименты показывают, что модели HaluCheck, обученные этим методом, значительно превосходят существующие модели в обнаружении галлюцинаций.'}, 'en': {'title': 'Enhancing Hallucination Detection in LLMs through Curriculum Learning', 'desc': "This paper presents a novel approach to improve large language models' (LLMs) ability to detect hallucinations by using specially designed negative examples in a curriculum learning framework. The authors recognize that hallucinated texts are often more deceptive than standard negative samples, and they leverage this insight in the DPO alignment procedure. By gradually increasing the difficulty of training samples, the method ensures that LLMs learn to identify hallucinations more effectively over time. Experimental results show that the proposed HaluCheck models achieve significant performance gains, particularly on challenging benchmarks, and demonstrate strong performance even in zero-shot scenarios."}, 'zh': {'title': '利用课程学习提升幻觉检测能力', 'desc': '本文提出了一种在DPO对齐过程中使用精心设计的幻觉样本的课程学习方法，以提高大型语言模型（LLMs）对幻觉的检测能力。我们认识到，幻觉样本通常比传统的负样本具有更高的欺骗性，因此将这些幻觉样本作为负例使用。通过逐步引入更难的样本，我们的课程学习策略确保了稳定的增量学习。实验结果表明，使用课程DPO方法和高质量负样本训练的HaluCheck模型在多个指标上显著提高了模型性能，尤其在MedHallu和HaluEval等困难基准上提升了多达24%。'}}}, {'id': 'https://huggingface.co/papers/2505.16483', 'title': 'Teaching Large Language Models to Maintain Contextual Faithfulness via\n  Synthetic Tasks and Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.16483', 'abstract': 'CANOE improves LLM faithfulness in generation tasks using synthetic QA data and Dual-GRPO reinforcement learning without human annotations.  \t\t\t\t\tAI-generated summary \t\t\t\t Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to improve the faithfulness of LLMs in both short-form and long-form generation tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different downstream tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.', 'score': 6, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '3bd78fedbb109d9a', 'authors': ['Shuzheng Si', 'Haozhe Zhao', 'Cheng Gao', 'Yuzhuo Bai', 'Zhitong Wang', 'Bofei Gao', 'Kangyang Luo', 'Wenhao Li', 'Yufei Huang', 'Gang Chen', 'Fanchao Qi', 'Minjia Zhang', 'Baobao Chang', 'Maosong Sun'], 'affiliations': ['DeepLang AI', 'Peking University', 'Tsinghua University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.16483.jpg', 'data': {'categories': ['#rl', '#dataset', '#rlhf', '#optimization', '#training', '#synthetic'], 'emoji': '🛶', 'ru': {'title': 'Достоверность без разметки: CANOE улучшает генерацию текста языковыми моделями', 'desc': 'CANOE - это новый подход к улучшению достоверности генерации текста языковыми моделями без использования размеченных данных. Метод основан на синтетических вопросно-ответных парах и обучении с подкреплением по алгоритму Dual-GRPO. CANOE применяет три специальных правила для вознаграждения на основе синтетических данных, оптимизируя одновременно генерацию коротких и длинных ответов. Эксперименты показали, что CANOE значительно повышает достоверность языковых моделей на 11 различных задачах, превосходя даже самые передовые модели вроде GPT-4.'}, 'en': {'title': 'Enhancing LLM Faithfulness with CANOE and Synthetic Data', 'desc': 'The paper presents CANOE, a framework designed to enhance the faithfulness of large language models (LLMs) in generating text. It achieves this by creating synthetic question-answering (QA) data, which serves as high-quality training material without requiring human annotations. The framework employs a novel reinforcement learning approach called Dual-GRPO, which uses rule-based rewards to optimize both short-form and long-form text generation. Experimental results demonstrate that CANOE significantly improves LLM performance across various tasks, surpassing even state-of-the-art models like GPT-4o.'}, 'zh': {'title': 'CANOE：提升大型语言模型的可信度', 'desc': 'CANOE是一个系统框架，旨在提高大型语言模型（LLMs）在生成任务中的可信度，而无需人工标注。该方法通过合成短形式问答（QA）数据，构建高质量的训练数据，并使用双重GRPO强化学习方法来优化生成过程。双重GRPO结合了基于规则的奖励机制，确保在短形式和长形式生成任务中都能有效提升模型的表现。实验结果表明，CANOE在11个不同的下游任务中显著提高了LLMs的可信度，甚至超越了最先进的模型，如GPT-4o和OpenAI o1。'}}}, {'id': 'https://huggingface.co/papers/2505.15389', 'title': 'Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study', 'url': 'https://huggingface.co/papers/2505.15389', 'abstract': 'VLMs are more vulnerable to harmful meme-based prompts than to synthetic images, and while multi-turn interactions offer some protection, significant vulnerabilities remain.  \t\t\t\t\tAI-generated summary \t\t\t\t Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet most evaluations rely on artificial images. This study asks: How safe are current VLMs when confronted with meme images that ordinary users share? To investigate this question, we introduce MemeSafetyBench, a 50,430-instance benchmark pairing real meme images with both harmful and benign instructions. Using a comprehensive safety taxonomy and LLM-based instruction generation, we assess multiple VLMs across single and multi-turn interactions. We investigate how real-world memes influence harmful outputs, the mitigating effects of conversational context, and the relationship between model scale and safety metrics. Our findings demonstrate that VLMs show greater vulnerability to meme-based harmful prompts than to synthetic or typographic images. Memes significantly increase harmful responses and decrease refusals compared to text-only inputs. Though multi-turn interactions provide partial mitigation, elevated vulnerability persists. These results highlight the need for ecologically valid evaluations and stronger safety mechanisms.', 'score': 6, 'issue_id': 3945, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '102a2cdaf1ccf7d5', 'authors': ['DongGeon Lee', 'Joonwon Jang', 'Jihae Jeong', 'Hwanjo Yu'], 'affiliations': ['LG AI Research', 'POSTECH'], 'pdf_title_img': 'assets/pdf/title_img/2505.15389.jpg', 'data': {'categories': ['#security', '#ethics', '#benchmark', '#multimodal'], 'emoji': '🛡️', 'ru': {'title': 'Мемы vs ИИ: неожиданная угроза безопасности визуально-языковых моделей', 'desc': 'Исследование показывает, что визуально-языковые модели (VLM) более уязвимы к вредоносным мемам, чем к синтетическим изображениям. Авторы представили MemeSafetyBench - набор данных из 50 430 мемов с вредными и безобидными инструкциями для оценки безопасности VLM. Многоэтапные взаимодействия частично снижают риски, но значительная уязвимость сохраняется. Результаты подчеркивают необходимость экологически валидных оценок и усиления механизмов безопасности для VLM.'}, 'en': {'title': 'Meme Vulnerability: A Call for Safer VLMs', 'desc': 'This paper investigates the safety of vision-language models (VLMs) when exposed to real-world meme images, which are often shared by users. The authors introduce a benchmark called MemeSafetyBench, consisting of over 50,000 instances of meme images paired with harmful and benign instructions. The study finds that VLMs are more susceptible to harmful prompts from memes compared to synthetic images, and while multi-turn interactions can offer some protection, vulnerabilities remain significant. The results emphasize the importance of realistic evaluations and the need for improved safety measures in VLMs.'}, 'zh': {'title': '恶搞图像对视觉语言模型的安全威胁', 'desc': '本研究探讨了视觉语言模型（VLMs）在面对用户分享的恶搞图像时的安全性。我们引入了MemeSafetyBench，这是一个包含50,430个实例的基准，结合了真实的恶搞图像和有害与无害的指令。研究发现，VLMs对恶搞图像的有害提示比对合成图像更脆弱，且多轮对话虽然提供了一定的保护，但仍然存在显著的脆弱性。我们的结果强调了需要进行生态有效的评估和更强的安全机制。'}}}, {'id': 'https://huggingface.co/papers/2505.17826', 'title': 'Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models', 'url': 'https://huggingface.co/papers/2505.17826', 'abstract': 'Trinity-RFT is a flexible and scalable framework for reinforcement fine-tuning of large language models, supporting various interaction modes and data pipelines.  \t\t\t\t\tAI-generated summary \t\t\t\t Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT, (2) seamless integration for agent-environment interaction with high efficiency and robustness, and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for exploring advanced reinforcement learning paradigms. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples demonstrating the utility and user-friendliness of the proposed framework.', 'score': 5, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6f6fdf1b20859c44', 'authors': ['Xuchen Pan', 'Yanxi Chen', 'Yushuo Chen', 'Yuchang Sun', 'Daoyuan Chen', 'Wenhao Zhang', 'Yuexiang Xie', 'Yilun Huang', 'Yilei Zhang', 'Dawei Gao', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['alibaba-inc.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.17826.jpg', 'data': {'categories': ['#rl', '#rlhf', '#agi', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Trinity-RFT: универсальная платформа для дообучения языковых моделей', 'desc': 'Trinity-RFT - это гибкая и масштабируемая платформа для дообучения больших языковых моделей с помощью обучения с подкреплением. Она поддерживает различные режимы взаимодействия и обработки данных, включая синхронные/асинхронные, on-policy/off-policy и онлайн/офлайн подходы. Архитектура Trinity-RFT состоит из универсального ядра RFT, модуля интеграции агента и окружения, а также оптимизированных конвейеров данных. Платформа легко адаптируется под различные сценарии применения и позволяет исследовать продвинутые парадигмы обучения с подкреплением.'}, 'en': {'title': 'Empowering Language Models with Flexible Reinforcement Fine-Tuning', 'desc': 'Trinity-RFT is a versatile framework designed for reinforcement fine-tuning (RFT) of large language models. It features a decoupled architecture that supports various modes of RFT, including synchronous and asynchronous, as well as on-policy and off-policy approaches. The framework ensures efficient and robust interactions between agents and environments, while also providing optimized data pipelines for RFT tasks. This makes Trinity-RFT adaptable to a wide range of applications, serving as a comprehensive platform for exploring advanced reinforcement learning techniques.'}, 'zh': {'title': 'Trinity-RFT：灵活的强化微调框架', 'desc': 'Trinity-RFT是一个灵活且可扩展的框架，专门用于大型语言模型的强化微调。它采用解耦设计，包含一个RFT核心，能够统一和概括同步/异步、在线/离线等多种强化微调模式。该框架支持高效且稳健的智能体与环境的交互，并优化了数据管道以适应强化微调的需求。Trinity-RFT易于适应不同的应用场景，是探索先进强化学习范式的统一平台。'}}}, {'id': 'https://huggingface.co/papers/2505.17225', 'title': 'Reasoning Model is Stubborn: Diagnosing Instruction Overriding in\n  Reasoning Models', 'url': 'https://huggingface.co/papers/2505.17225', 'abstract': 'A diagnostic set examines and categorizes reasoning rigidity in large language models, identifying patterns where models ignore instructions and default to familiar reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, . Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models.', 'score': 5, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '41036303d3b75082', 'authors': ['Doohyuk Jang', 'Yoonjeon Kim', 'Chanjae Park', 'Hyun Ryu', 'Eunho Yang'], 'affiliations': ['AITRICS', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2505.17225.jpg', 'data': {'categories': ['#math', '#dataset', '#data', '#interpretability', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Преодоление жесткости мышления в ИИ: новый подход к диагностике языковых моделей', 'desc': 'Статья представляет диагностический набор для анализа и категоризации жесткости рассуждений в больших языковых моделях. Исследователи выявили тенденцию моделей игнорировать инструкции и использовать знакомые паттерны рассуждений. Были определены три режима контаминации: перегрузка интерпретации, недоверие к входным данным и частичное внимание к инструкциям. Набор данных включает модифицированные варианты математических тестов и головоломок, требующих отклонения от привычных стратегий рассуждения.'}, 'en': {'title': 'Unraveling Reasoning Rigidity in Language Models', 'desc': 'This paper investigates a phenomenon called reasoning rigidity in large language models, where these models often ignore user instructions and revert to familiar reasoning patterns. The authors introduce a diagnostic set designed to identify and categorize this behavior, which can lead to incorrect conclusions in tasks requiring precise adherence to instructions. They highlight three specific modes of contamination: Interpretation Overload, Input Distrust, and Partial Instruction Attention, which describe how models distort or overlook given instructions. By releasing this diagnostic set, the authors aim to support further research aimed at reducing reasoning rigidity in language models.'}, 'zh': {'title': '揭示语言模型的推理僵化现象', 'desc': '本文探讨了大型语言模型中的推理僵化现象，即模型在面对明确指令时，仍然倾向于使用熟悉的推理模式。我们引入了一个专家策划的诊断集，以系统地研究这一行为，特别是在数学和逻辑难题等领域。该数据集包含经过修改的数学基准和重新设计的难题，旨在促使模型偏离常规推理策略。通过分析，我们识别出三种主要的推理僵化模式，帮助未来的研究更好地解决这一问题。'}}}, {'id': 'https://huggingface.co/papers/2505.17508', 'title': 'On the Design of KL-Regularized Policy Gradient Algorithms for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.17508', 'abstract': 'A regularized policy gradient framework is introduced to explore KL divergence formulations for enhancing the reasoning capabilities of LLMs in online reinforcement learning, demonstrating improved training stability and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Policy gradient algorithms have been successfully applied to enhance the reasoning capabilities of large language models (LLMs). Despite the widespread use of Kullback-Leibler (KL) regularization in policy gradient algorithms to stabilize training, the systematic exploration of how different KL divergence formulations can be estimated and integrated into surrogate loss functions for online reinforcement learning (RL) presents a nuanced and systematically explorable design space. In this paper, we propose regularized policy gradient (RPG), a systematic framework for deriving and analyzing KL-regularized policy gradient methods in the online RL setting. We derive policy gradients and corresponding surrogate loss functions for objectives regularized by both forward and reverse KL divergences, considering both normalized and unnormalized policy distributions. Furthermore, we present derivations for fully differentiable loss functions as well as REINFORCE-style gradient estimators, accommodating diverse algorithmic needs. We conduct extensive experiments on RL for LLM reasoning using these methods, showing improved or competitive results in terms of training stability and performance compared to strong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at https://github.com/complex-reasoning/RPG.', 'score': 4, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '6ae63edcb7127847', 'authors': ['Yifan Zhang', 'Yifeng Liu', 'Huizhuo Yuan', 'Yang Yuan', 'Quanquan Gu', 'Andrew C Yao'], 'affiliations': ['IIIS, Tsinghua University', 'Shanghai Qi Zhi Institute', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2505.17508.jpg', 'data': {'categories': ['#rl', '#rlhf', '#optimization', '#training', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Регуляризация градиента политики для улучшения рассуждений языковых моделей', 'desc': 'Статья представляет новый фреймворк под названием RPG (regularized policy gradient) для улучшения способностей больших языковых моделей к рассуждению в онлайн-обучении с подкреплением. Авторы исследуют различные формулировки KL-дивергенции для регуляризации градиента политики. Эксперименты показывают, что предложенный метод повышает стабильность обучения и производительность по сравнению с сильными базовыми алгоритмами. Фреймворк RPG предоставляет систематический подход к анализу и разработке KL-регуляризованных методов градиента политики.'}, 'en': {'title': 'Enhancing LLM Reasoning with Regularized Policy Gradients', 'desc': 'This paper introduces a regularized policy gradient framework that utilizes Kullback-Leibler (KL) divergence to improve the reasoning abilities of large language models (LLMs) in online reinforcement learning (RL). It systematically explores various KL divergence formulations to enhance training stability and performance through surrogate loss functions. The authors derive policy gradients for both forward and reverse KL divergences, accommodating different types of policy distributions. Extensive experiments demonstrate that their proposed methods achieve better or comparable results against established baselines in RL tasks involving LLMs.'}, 'zh': {'title': '正则化策略梯度：提升LLM推理能力的关键', 'desc': '本文提出了一种正则化策略梯度框架，用于探索KL散度的不同形式，以增强大型语言模型（LLMs）在在线强化学习中的推理能力。我们系统地分析了如何将不同的KL散度估计整合到替代损失函数中，从而提高训练的稳定性和性能。通过对正向和反向KL散度的正则化目标，我们推导了相应的策略梯度和损失函数，并考虑了标准化和非标准化的策略分布。实验结果表明，与现有的强基线算法相比，我们的方法在训练稳定性和性能上都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.17417', 'title': 'Speechless: Speech Instruction Training Without Speech for Low Resource\n  Languages', 'url': 'https://huggingface.co/papers/2505.17417', 'abstract': 'The rapid growth of voice assistants powered by large language models (LLM) has highlighted a need for speech instruction data to train these systems. Despite the abundance of speech recognition data, there is a notable scarcity of speech instruction data, which is essential for fine-tuning models to understand and execute spoken commands. Generating high-quality synthetic speech requires a good text-to-speech (TTS) model, which may not be available to low resource languages. Our novel approach addresses this challenge by halting synthesis at the semantic representation level, bypassing the need for TTS. We achieve this by aligning synthetic semantic representations with the pre-trained Whisper encoder, enabling an LLM to be fine-tuned on text instructions while maintaining the ability to understand spoken instructions during inference. This simplified training process is a promising approach to building voice assistant for low-resource languages.', 'score': 4, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '9d72b20aca0789ee', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Huy Hoang Ha', 'Tuan Le Duc Anh', 'Shreyas Gopal', 'Yue Heng Yeo', 'Warren Keng Hoong Low', 'Eng Siong Chng', 'Jia Qi Yip'], 'affiliations': ['CCDS, Nanyang Technological University, Singapore', 'Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.17417.jpg', 'data': {'categories': ['#multilingual', '#dataset', '#low_resource', '#data', '#synthetic', '#training', '#audio'], 'emoji': '🗣️', 'ru': {'title': 'Голосовые ассистенты для редких языков: обучение без TTS', 'desc': 'Статья описывает новый подход к обучению голосовых ассистентов для языков с ограниченными ресурсами. Авторы предлагают метод, позволяющий обойти необходимость в качественной системе text-to-speech, останавливая синтез на уровне семантического представления. Это достигается путем выравнивания синтетических семантических представлений с предобученным энкодером Whisper. Такой подход позволяет обучать языковую модель на текстовых инструкциях, сохраняя способность понимать устные команды при инференсе.'}, 'en': {'title': 'Empowering Voice Assistants for Low-Resource Languages', 'desc': 'This paper addresses the challenge of training voice assistants in low-resource languages, where there is a lack of speech instruction data. It proposes a novel method that generates synthetic speech by stopping at the semantic representation level, eliminating the need for a text-to-speech (TTS) model. By aligning these semantic representations with the pre-trained Whisper encoder, the approach allows for fine-tuning large language models (LLMs) on text instructions while still being able to process spoken commands. This method simplifies the training process and enhances the development of voice assistants for languages with limited resources.'}, 'zh': {'title': '为低资源语言构建语音助手的新方法', 'desc': '本论文探讨了为语音助手训练所需的语音指令数据的不足问题。尽管语音识别数据丰富，但语音指令数据却相对稀缺，这对模型理解和执行口头命令至关重要。我们提出了一种新方法，通过在语义表示层面停止合成，避免了对文本到语音（TTS）模型的依赖。该方法通过将合成的语义表示与预训练的Whisper编码器对齐，使得大型语言模型（LLM）能够在文本指令上进行微调，同时在推理过程中理解口头指令。'}}}, {'id': 'https://huggingface.co/papers/2505.17561', 'title': 'Model Already Knows the Best Noise: Bayesian Active Noise Selection via\n  Attention in Video Diffusion Model', 'url': 'https://huggingface.co/papers/2505.17561', 'abstract': 'ANSE enhances video diffusion models by selecting noise seeds based on model confidence, improving video quality and temporal coherence with minimal increase in inference time.  \t\t\t\t\tAI-generated summary \t\t\t\t The choice of initial noise significantly affects the quality and prompt alignment of video diffusion models, where different noise seeds for the same prompt can lead to drastically different generations. While recent methods rely on externally designed priors such as frequency filters or inter-frame smoothing, they often overlook internal model signals that indicate which noise seeds are inherently preferable. To address this, we propose ANSE (Active Noise Selection for Generation), a model-aware framework that selects high-quality noise seeds by quantifying attention-based uncertainty. At its core is BANSA (Bayesian Active Noise Selection via Attention), an acquisition function that measures entropy disagreement across multiple stochastic attention samples to estimate model confidence and consistency. For efficient inference-time deployment, we introduce a Bernoulli-masked approximation of BANSA that enables score estimation using a single diffusion step and a subset of attention layers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video quality and temporal coherence with only an 8% and 13% increase in inference time, respectively, providing a principled and generalizable approach to noise selection in video diffusion. See our project page: https://anse-project.github.io/anse-project/', 'score': 3, 'issue_id': 3945, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 мая', 'en': 'May 23', 'zh': '5月23日'}, 'hash': '224b15e182587a84', 'authors': ['Kwanyoung Kim', 'Sanghyun Kim'], 'affiliations': ['Samsung Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.17561.jpg', 'data': {'categories': ['#video', '#inference', '#diffusion', '#optimization'], 'emoji': '🎬', 'ru': {'title': 'Умный выбор шума для лучшего видео-синтеза', 'desc': 'Статья представляет ANSE - метод улучшения видео-диффузионных моделей путем выбора начальных шумовых сидов на основе уверенности модели. В основе лежит функция BANSA, оценивающая энтропию разногласий между стохастическими выборками внимания. Для эффективного применения во время инференса предложена аппроксимация BANSA с использованием маскирования по Бернулли. Эксперименты показали улучшение качества видео и временной согласованности при минимальном увеличении времени инференса.'}, 'en': {'title': 'Smart Noise Selection for Better Video Generation', 'desc': "The paper introduces ANSE, a method that enhances video diffusion models by intelligently selecting noise seeds based on the model's confidence. It highlights the importance of initial noise in generating high-quality videos, as different seeds can lead to varying results. ANSE utilizes an acquisition function called BANSA, which measures uncertainty through attention-based entropy to identify the best noise seeds. This approach improves video quality and temporal coherence while only slightly increasing the time needed for inference."}, 'zh': {'title': '主动选择噪声，提升视频生成质量', 'desc': 'ANSE（主动噪声选择生成）通过基于模型信心选择噪声种子，增强了视频扩散模型的性能。该方法利用注意力机制量化不确定性，从而选择高质量的噪声种子，显著提高视频质量和时间一致性。核心算法BANSA（基于注意力的贝叶斯主动噪声选择）通过测量多个随机注意力样本之间的熵不一致性来估计模型的信心和一致性。实验结果表明，ANSE在推理时间仅增加8%和13%的情况下，显著改善了视频生成的质量和一致性。'}}}, {'id': 'https://huggingface.co/papers/2505.16270', 'title': 'Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning', 'url': 'https://huggingface.co/papers/2505.16270', 'abstract': "The Transformer Copilot framework enhances large language model performance through a Copilot model that refines the Pilot's logits based on a Mistake Log, leading to consistent performance improvements across various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability.", 'score': 3, 'issue_id': 3945, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 мая', 'en': 'May 22', 'zh': '5月22日'}, 'hash': '242b4420fd3d9c4f', 'authors': ['Jiaru Zou', 'Yikun Ban', 'Zihao Li', 'Yunzhe Qi', 'Ruizhong Qiu', 'Ling Yang', 'Jingrui He'], 'affiliations': ['Princeton University', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.16270.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#architecture', '#transfer_learning', '#training'], 'emoji': '🚀', 'ru': {'title': 'Transformer Copilot: Учимся на ошибках для повышения эффективности ИИ', 'desc': 'Представлена новая архитектура Transformer Copilot, которая улучшает работу больших языковых моделей. Основная идея заключается в использовании модели-второго пилота (Copilot), которая корректирует логиты основной модели (Pilot) на основе журнала ошибок (Mistake Log). Эта система позволяет модели учиться на своих прошлых ошибках, аналогично тому, как учатся люди. Эксперименты на 12 тестовых наборах показали значительное улучшение производительности до 34.5% при минимальных вычислительных затратах.'}, 'en': {'title': 'Enhancing Language Models with Reflective Learning', 'desc': "The Transformer Copilot framework improves the performance of large language models by using a Copilot model that refines the Pilot's outputs based on a Mistake Log. This Mistake Log tracks the model's errors during fine-tuning, allowing the Copilot to learn from these mistakes, similar to how humans learn. The framework includes a novel design for the Copilot, a joint training approach where both models learn together, and a fused inference method that enhances the Pilot's predictions. Experiments show that this approach can boost performance by up to 34.5% across various tasks with minimal additional computational cost."}, 'zh': {'title': '提升语言模型性能的副驾驶框架', 'desc': 'Transformer Copilot框架通过一个副驾驶模型来提升大型语言模型的性能。这个副驾驶模型根据错误日志来优化主模型的输出，从而在多个基准测试中实现了持续的性能提升。我们引入了错误日志的概念，以系统地跟踪模型的学习行为和重复错误。通过这种方式，副驾驶模型能够在训练过程中不断学习，从而提高生成的准确性和质量。'}}}, {'id': 'https://huggingface.co/papers/2505.17091', 'title': 'Large Language Models Implicitly Learn to See and Hear Just By Reading', 'url': 'https://huggingface.co/papers/2505.17091', 'abstract': 'Auto-regressive text LLMs trained on text can develop internal capabilities for understanding images and audio, enabling them to perform classification tasks across different modalities without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.', 'score': 3, 'issue_id': 3945, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 мая', 'en': 'May 20', 'zh': '5月20日'}, 'hash': '7bb07d5dfb6680e6', 'authors': ['Prateek Verma', 'Mert Pilanci'], 'affiliations': ['Department of Electrical Engineering Stanford University Stanford, CA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.17091.jpg', 'data': {'categories': ['#cv', '#multimodal', '#optimization', '#architecture', '#transfer_learning', '#audio'], 'emoji': '🧠', 'ru': {'title': 'Языковые модели обретают зрение и слух через чтение', 'desc': 'Исследование показывает, что авторегрессионные языковые модели, обученные на текстовых данных, способны развивать внутренние механизмы для понимания изображений и аудио. Это позволяет им выполнять задачи классификации в разных модальностях без дополнительной настройки. Авторы демонстрируют, что текстовые веса модели могут использоваться для классификации аудио и изображений на различных датасетах. Данное открытие расширяет представление о мощных внутренних схемах, формируемых языковыми моделями, и их потенциальном применении в различных задачах без необходимости обучения с нуля.'}, 'en': {'title': 'Unlocking Multi-Modal Understanding with Text LLMs', 'desc': "This paper explores how auto-regressive language models (LLMs) trained solely on text can develop the ability to understand and classify images and audio without needing additional fine-tuning. The authors demonstrate that these text-based models can process inputs like image patches and audio waveforms, producing embeddings or category labels similar to those used in traditional classification tasks. They validate their findings by applying the model to audio classification tasks on datasets like FSD-50K and GTZAN, as well as image classification on CIFAR-10 and Fashion-MNIST. This research highlights the potential of leveraging text LLMs' internal capabilities for multi-modal applications, reducing the need for training separate models for each modality."}, 'zh': {'title': '文本模型的跨模态理解能力', 'desc': '这篇论文展示了一个有趣的发现：通过对文本进行训练的自回归语言模型，能够内在地发展出理解图像和音频的能力。这样，模型在没有微调的情况下，就能进行跨模态的分类任务。我们的方法通过输入图像块、音频波形或标记，生成典型的分类管道所需的嵌入或类别标签。研究表明，文本模型的权重在音频和图像分类任务中具有广泛的适用性，推动了文本语言模型学习强大内部电路的概念。'}}}, {'id': 'https://huggingface.co/papers/2505.17063', 'title': 'Synthetic Data RL: Task Definition Is All You Need', 'url': 'https://huggingface.co/papers/2505.17063', 'abstract': 'Synthetic Data RL enhances foundation models through reinforcement learning using only synthetic data, achieving performance comparable to models trained with full human-labeled data.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.', 'score': 1, 'issue_id': 3947, 'pub_date': '2025-05-18', 'pub_date_card': {'ru': '18 мая', 'en': 'May 18', 'zh': '5月18日'}, 'hash': 'a46b0456dc458ebe', 'authors': ['Yiduo Guo', 'Zhen Guo', 'Chuanwei Huang', 'Zi-Ang Wang', 'Zekai Zhang', 'Haofei Yu', 'Huishuai Zhang', 'Yikang Shen'], 'affiliations': ['MIT', 'MIT-IBM', 'Peking University', 'UIUC'], 'pdf_title_img': 'assets/pdf/title_img/2505.17063.jpg', 'data': {'categories': ['#rlhf', '#rl', '#optimization', '#training', '#synthetic'], 'emoji': '🤖', 'ru': {'title': 'Усиление языковых моделей без человеческих данных', 'desc': 'Статья представляет новый метод под названием Synthetic Data RL, который улучшает языковые модели с помощью обучения с подкреплением, используя только синтетические данные. Этот подход генерирует пары вопросов и ответов на основе определения задачи и извлеченных документов, адаптирует сложность вопросов и выбирает их для обучения. Метод показывает значительные улучшения производительности на различных задачах по сравнению с базовыми моделями и другими методами. Synthetic Data RL позволяет масштабировать адаптацию моделей без необходимости в больших объемах размеченных человеком данных.'}, 'en': {'title': 'Reinforcement Learning with Synthetic Data: A Game Changer for Model Training!', 'desc': "Synthetic Data RL is a novel approach that enhances foundation models using reinforcement learning (RL) without the need for extensive human-labeled data. It generates synthetic question and answer pairs based on task definitions, allowing for effective model fine-tuning. The method adapts question difficulty according to the model's performance, optimizing the training process. Results show significant performance improvements on various benchmarks, demonstrating that this approach can achieve results comparable to traditional RL methods that rely on human data."}, 'zh': {'title': '合成数据强化学习：高效提升模型性能的创新方法', 'desc': '本文介绍了一种名为合成数据强化学习（Synthetic Data RL）的方法，该方法通过使用合成数据来增强基础模型的性能。该方法生成与任务定义相关的问题和答案对，并根据模型的解决能力调整问题的难度。通过这种方式，Synthetic Data RL在多个数据集上实现了显著的性能提升，甚至接近于使用全人类标注数据的强化学习模型。此方法减少了对人类数据标注的依赖，使得模型适应变得更加高效和可扩展。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi (1)', '#alignment (1)', '#architecture (2)', '#audio (3)', '#benchmark (4)', '#cv (1)', '#data (2)', '#dataset (5)', '#diffusion (1)', '#ethics (2)', '#games', '#graphs', '#hallucinations (3)', '#healthcare', '#inference (2)', '#interpretability (2)', '#leakage', '#long_context', '#low_resource (1)', '#machine_translation', '#math (3)', '#multilingual (1)', '#multimodal (3)', '#open_source (2)', '#optimization (9)', '#plp', '#rag', '#reasoning (6)', '#rl (6)', '#rlhf (6)', '#robotics', '#science', '#security (2)', '#small_models (1)', '#story_generation', '#survey', '#synthetic (3)', '#training (11)', '#transfer_learning (3)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-05-26 04:18',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-26 04:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-26 04:18')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    