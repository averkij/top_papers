
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 72 papers. May 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 Ğ¼Ğ°Ñ</span> | <span id="title-articles-count">72 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-27.html">â¬…ï¸ <span id="prev-date">27.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-29.html">â¡ï¸ <span id="next-date">29.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 Ğ¼Ğ°Ñ', 'en': 'May 28', 'zh': '5æœˆ28æ—¥'};
        let feedDateNext = {'ru': '29.05', 'en': '05/29', 'zh': '5æœˆ29æ—¥'};
        let feedDatePrev = {'ru': '27.05', 'en': '05/27', 'zh': '5æœˆ27æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.19897', 'title': 'ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows', 'url': 'https://huggingface.co/papers/2505.19897', 'abstract': "ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers' workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.", 'score': 79, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '7903b97e8a51b382', 'authors': ['Qiushi Sun', 'Zhoumianze Liu', 'Chang Ma', 'Zichen Ding', 'Fangzhi Xu', 'Zhangyue Yin', 'Haiteng Zhao', 'Zhenyu Wu', 'Kanzhi Cheng', 'Zhaoyang Liu', 'Jianing Wang', 'Qintong Li', 'Xiangru Tang', 'Tianbao Xie', 'Xiachong Feng', 'Xiang Li', 'Ben Kao', 'Wenhai Wang', 'Biqing Qi', 'Lingpeng Kong', 'Zhiyong Wu'], 'affiliations': ['East China Normal University', 'Fudan University', 'Nanjing University', 'Peking University', 'Shanghai AI Laboratory', 'The University of Hong Kong', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19897.jpg', 'data': {'categories': ['#science', '#multimodal', '#agents', '#benchmark'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'ScienceBoard: Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ ÑÑ€ĞµĞ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…', 'desc': 'ScienceBoard Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½ÑƒÑ ÑÑ€ĞµĞ´Ñƒ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ½Ñ‹Ğ¼ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ±Ğ¸Ğ¾Ñ…Ğ¸Ğ¼Ğ¸Ñ, Ğ°ÑÑ‚Ñ€Ğ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¸ Ğ³ĞµĞ¾Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 169 Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸Ğ· Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¸Ñ€Ğ°. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ½Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ»Ğ¸ÑˆÑŒ 15% Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Unlocking the Potential of LLMs in Scientific Workflows', 'desc': 'The paper introduces ScienceBoard, a platform designed to evaluate the performance of Large Language Model (LLM)-based agents in scientific workflows. It features a realistic environment with dynamic tasks across various scientific domains, allowing agents to interact with professional software. Despite the advancements in LLMs, evaluations reveal that these agents struggle with complex tasks, achieving only a 15% success rate. The study highlights the need for improved design principles to enhance the capabilities of these agents in aiding scientific discovery.'}, 'zh': {'title': 'ç§‘å­¦å‘ç°çš„æ–°åŠ©æ‰‹ï¼šScienceBoard', 'desc': 'ScienceBoardæ˜¯ä¸€ä¸ªç°å®çš„ç§‘å­¦å·¥ä½œæµç¨‹ç¯å¢ƒå’ŒåŸºå‡†ï¼Œç”¨äºè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†çš„æ€§èƒ½ï¼Œå±•ç¤ºå®ƒä»¬åœ¨å¤æ‚ç§‘å­¦ä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è¯¥å¹³å°æä¾›äº†ä¸€ä¸ªå¤šé¢†åŸŸçš„åŠ¨æ€ç¯å¢ƒï¼Œå…è®¸ä»£ç†é€šè¿‡ä¸åŒçš„æ¥å£è‡ªä¸»äº’åŠ¨ï¼Œä»¥åŠ é€Ÿå¤æ‚çš„ç ”ç©¶ä»»åŠ¡å’Œå®éªŒã€‚å°½ç®¡ä¸€äº›ä»£ç†åœ¨è¯„ä¼°ä¸­è¡¨ç°å‡ºä¸€å®šçš„æ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤æ‚å·¥ä½œæµç¨‹ä¸­çš„æˆåŠŸç‡ä»…ä¸º15%ï¼Œæ˜¾ç¤ºå‡ºä»éœ€æ”¹è¿›ã€‚é€šè¿‡æ·±å…¥åˆ†æï¼Œæœ¬æ–‡ä¸ºè§£å†³å½“å‰ä»£ç†çš„å±€é™æ€§å’Œè®¾è®¡æ›´æœ‰æ•ˆçš„ä»£ç†æä¾›äº†å®è´µçš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21327', 'title': 'MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2505.21327', 'abstract': "MME-Reasoning evaluates the logical reasoning capabilities of multimodal large language models, revealing significant limitations and performance imbalances across inductive, deductive, and abductive reasoning types.  \t\t\t\t\tAI-generated summary \t\t\t\t Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce MME-Reasoning, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (i.e., inductive, deductive, and abductive) in its questions. We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of state-of-the-art MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. These findings highlight the critical limitations and performance imbalances of current MLLMs in diverse logical reasoning scenarios, providing comprehensive and systematic insights into the understanding and evaluation of reasoning capabilities.", 'score': 70, 'issue_id': 3991, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'a022fb524c5969bf', 'authors': ['Jiakang Yuan', 'Tianshuo Peng', 'Yilei Jiang', 'Yiting Lu', 'Renrui Zhang', 'Kaituo Feng', 'Chaoyou Fu', 'Tao Chen', 'Lei Bai', 'Bo Zhang', 'Xiangyu Yue'], 'affiliations': ['Fudan University', 'MMLab, The Chinese University of Hong Kong', 'Nanjing University', 'Shanghai AI Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21327.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ² Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': "MME-Reasoning - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ½Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ, Ğ´ĞµĞ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ Ğ°Ğ±Ğ´ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑÑŒ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ»Ğ¾Ğ³Ğ¸ĞºĞµ, Ğ° Ğ½Ğµ Ğ½Ğ° Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸ Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ…. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ…, Ñ Ğ·Ğ°Ğ¼ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ¸ÑĞ¼Ğ¸ Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ², Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº 'Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ' Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ», Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ."}, 'en': {'title': 'Unveiling the Reasoning Gaps in Multimodal AI', 'desc': 'The paper introduces MME-Reasoning, a benchmark designed to assess the logical reasoning abilities of multimodal large language models (MLLMs). It categorizes reasoning into three types: inductive, deductive, and abductive, addressing gaps in existing evaluations that often overlook these distinctions. The study reveals that even advanced MLLMs struggle with logical reasoning tasks, showing significant performance imbalances across the different reasoning types. Additionally, the paper analyzes common methods aimed at improving reasoning, highlighting the persistent limitations of current MLLMs in effectively handling diverse logical reasoning challenges.'}, 'zh': {'title': 'è¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹çš„é€»è¾‘æ¨ç†èƒ½åŠ›', 'desc': 'MME-Reasoning æ˜¯ä¸€ä¸ªè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€»è¾‘æ¨ç†èƒ½åŠ›çš„åŸºå‡†ï¼Œæ­ç¤ºäº†åœ¨å½’çº³ã€æ¼”ç»å’Œæº¯å› æ¨ç†ç±»å‹ä¸Šçš„æ˜¾è‘—å±€é™æ€§å’Œæ€§èƒ½ä¸å¹³è¡¡ã€‚å°½ç®¡å¤šæ¨¡æ€æ¨ç†å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æœªèƒ½å…¨é¢è¯„ä¼°å…¶æ¨ç†èƒ½åŠ›ï¼Œç¼ºä¹å¯¹é€»è¾‘æ¨ç†ç±»å‹çš„æ˜ç¡®åˆ†ç±»ã€‚æˆ‘ä»¬è®¾è®¡äº† MME-Reasoningï¼Œæ¶µç›–æ‰€æœ‰ä¸‰ç§æ¨ç†ç±»å‹çš„é—®é¢˜ï¼Œç¡®ä¿æ¯ä¸ªé—®é¢˜æœ‰æ•ˆè¯„ä¼°æ¨ç†èƒ½åŠ›ï¼Œè€Œéæ„ŸçŸ¥æŠ€èƒ½æˆ–çŸ¥è¯†å¹¿åº¦ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„ MLLMs åœ¨å…¨é¢çš„é€»è¾‘æ¨ç†è¯„ä¼°ä¸­è¡¨ç°æœ‰é™ï¼Œä¸”åœ¨ä¸åŒæ¨ç†ç±»å‹ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21497', 'title': 'Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers', 'url': 'https://huggingface.co/papers/2505.21497', 'abstract': "Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.", 'score': 63, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '7f740f76be754bce', 'authors': ['Wei Pang', 'Kevin Qinghong Lin', 'Xiangru Jian', 'Xi He', 'Philip Torr'], 'affiliations': ['National University of Singapore', 'University of Oxford', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2505.21497.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#open_source', '#agents', '#science'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ², ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ½ĞµĞ´Ğ°Ğ²Ğ½Ğ¸Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ°Ğ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ PosterAgent - Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Parser Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Planner Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ° Ğ¸ Painter-Commenter Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ. ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Qwen-2.5 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ° 87% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing Academic Poster Generation with PosterAgent', 'desc': 'This paper addresses the challenge of generating academic posters from lengthy scientific documents by introducing a benchmark and metric suite for evaluation. It presents PosterAgent, a multi-agent pipeline that includes a Parser for structuring content, a Planner for layout design, and a Painter-Commenter loop for refining visuals based on feedback. The study evaluates the effectiveness of generated posters using metrics like visual quality, textual coherence, and the ability to convey core content through quizzes. The results show that their open-source approach significantly outperforms existing models while being more efficient in token usage, paving the way for future advancements in automated poster generation.'}, 'zh': {'title': 'è‡ªåŠ¨åŒ–å­¦æœ¯æµ·æŠ¥ç”Ÿæˆçš„æ–°çºªå…ƒ', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„å­¦æœ¯æµ·æŠ¥ç”ŸæˆåŸºå‡†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œæ—¨åœ¨å°†é•¿ç¯‡æ–‡æ¡£å‹ç¼©ä¸ºè§†è§‰ä¸Šè¿è´¯çš„å•é¡µæµ·æŠ¥ã€‚æˆ‘ä»¬æå‡ºäº†PosterAgentï¼Œä¸€ä¸ªå¤šä»£ç†ç®¡é“ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è§£æã€è§„åˆ’å’Œç»˜åˆ¶æµ·æŠ¥å†…å®¹ã€‚é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è¾“å‡ºï¼Œæˆ‘ä»¬å‘ç°äººç±»è®¾è®¡çš„æµ·æŠ¥åœ¨è§†è§‰è¯­ä¹‰ä¸Šæ›´å…·å¸å¼•åŠ›ï¼Œè€ŒGPT-4oæ¨¡å‹è™½ç„¶å¤–è§‚ç¾è§‚ï¼Œä½†æ–‡æœ¬è´¨é‡å’Œä¿¡æ¯ä¼ è¾¾èƒ½åŠ›è¾ƒå·®ã€‚æˆ‘ä»¬çš„å¼€æºå˜ä½“åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰ç³»ç»Ÿï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘äº†æ‰€éœ€çš„è®¡ç®—èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18445', 'title': 'OmniConsistency: Learning Style-Agnostic Consistency from Paired\n  Stylization Data', 'url': 'https://huggingface.co/papers/2505.18445', 'abstract': "OmniConsistency, using large-scale Diffusion Transformers, enhances stylization consistency and generalization in image-to-image pipelines without style degradation.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have advanced image stylization significantly, yet two core challenges persist: (1) maintaining consistent stylization in complex scenes, particularly identity, composition, and fine details, and (2) preventing style degradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional stylization consistency highlights the performance gap between open-source methods and proprietary models. To bridge this gap, we propose OmniConsistency, a universal consistency plugin leveraging large-scale Diffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context consistency learning framework trained on aligned image pairs for robust generalization; (2) a two-stage progressive learning strategy decoupling style learning from consistency preservation to mitigate style degradation; and (3) a fully plug-and-play design compatible with arbitrary style LoRAs under the Flux framework. Extensive experiments show that OmniConsistency significantly enhances visual coherence and aesthetic quality, achieving performance comparable to commercial state-of-the-art model GPT-4o.", 'score': 57, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': '0a5e56835e542da2', 'authors': ['Yiren Song', 'Cheng Liu', 'Mike Zheng Shou'], 'affiliations': ['Show Lab, National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2505.18445.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#diffusion', '#cv', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¸Ğ»Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'OmniConsistency - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğµ Ğ”Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ (DiTs) Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ñ… Ğ²Ñ‹Ñ€Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. ĞŸĞ»Ğ°Ğ³Ğ¸Ğ½ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑÑ‰ÑƒÑ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ğ»Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸. OmniConsistency ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ²Ñ‹Ğ¼Ğ¸ LoRA Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑÑÑ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Achieving Consistent and High-Quality Image Stylization with OmniConsistency', 'desc': 'OmniConsistency is a novel approach that improves the consistency and generalization of image stylization using large-scale Diffusion Transformers. It addresses two main challenges in image-to-image pipelines: ensuring consistent stylization across complex scenes and preventing degradation of style when using style LoRAs. The method introduces a learning framework that focuses on maintaining consistency while allowing for flexible style application. Experimental results demonstrate that OmniConsistency achieves visual quality and coherence comparable to leading commercial models.'}, 'zh': {'title': 'OmniConsistencyï¼šæå‡å›¾åƒé£æ ¼ä¸€è‡´æ€§çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'OmniConsistency æ˜¯ä¸€ç§åˆ©ç”¨å¤§è§„æ¨¡æ‰©æ•£å˜æ¢å™¨ï¼ˆDiffusion Transformersï¼‰æ¥å¢å¼ºå›¾åƒåˆ°å›¾åƒç®¡é“ä¸­çš„é£æ ¼ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•è§£å†³äº†åœ¨å¤æ‚åœºæ™¯ä¸­ä¿æŒä¸€è‡´é£æ ¼å’Œé˜²æ­¢é£æ ¼é€€åŒ–çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚OmniConsistency æä¾›äº†ä¸€ç§åŸºäºå¯¹é½å›¾åƒå¯¹çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨ä¸¤é˜¶æ®µçš„æ¸è¿›å­¦ä¹ ç­–ç•¥æ¥åˆ†ç¦»é£æ ¼å­¦ä¹ ä¸ä¸€è‡´æ€§ä¿æŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniConsistency æ˜¾è‘—æé«˜äº†è§†è§‰è¿è´¯æ€§å’Œç¾å­¦è´¨é‡ï¼Œæ€§èƒ½å¯ä¸å•†ä¸šæœ€å…ˆè¿›æ¨¡å‹ GPT-4o ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20292', 'title': 'OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for\n  Subject-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20292', 'abstract': "Subject-to-Video (S2V) generation aims to create videos that faithfully incorporate reference content, providing enhanced flexibility in the production of videos. To establish the infrastructure for S2V generation, we propose OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and (ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V benchmarks inherited from VBench that focus on global and coarse-grained assessment of generated videos, OpenS2V-Eval focuses on the model's ability to generate subject-consistent videos with natural subject appearance and identity fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven major categories of S2V, which incorporate both real and synthetic test data. Furthermore, to accurately align human preferences with S2V benchmarks, we propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to separately quantify subject consistency, naturalness, and text relevance in generated videos. Building on this, we conduct a comprehensive evaluation of 16 representative S2V models, highlighting their strengths and weaknesses across different content. Moreover, we create the first open-source large-scale S2V generation dataset OpenS2V-5M, which consists of five million high-quality 720P subject-text-video triples. Specifically, we ensure subject-information diversity in our dataset by (1) segmenting subjects and building pairing information via cross-video associations and (2) prompting GPT-Image-1 on raw frames to synthesize multi-view representations. Through OpenS2V-Nexus, we deliver a robust infrastructure to accelerate future S2V generation research.", 'score': 49, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'e2a8d12789199cde', 'authors': ['Shenghai Yuan', 'Xianyi He', 'Yufan Deng', 'Yang Ye', 'Jinfa Huang', 'Bin Lin', 'Chongyang Ma', 'Jiebo Luo', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Rabbitpre AI', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20292.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#synthetic', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'OpenS2V-Nexus: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ OpenS2V-Nexus - Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ (Subject-to-Video, S2V). ĞĞ½Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ OpenS2V-Eval - Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¸ OpenS2V-5M - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² ÑÑƒĞ±ÑŠĞµĞºÑ‚-Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ°, ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 16 Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… S2V Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‰Ğ°Ñ Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹.'}, 'en': {'title': 'Revolutionizing Video Generation with Subject Fidelity', 'desc': 'The paper introduces Subject-to-Video (S2V) generation, which focuses on creating videos that accurately reflect reference content. It presents OpenS2V-Nexus, a framework that includes OpenS2V-Eval, a detailed benchmark for evaluating video generation, and OpenS2V-5M, a large dataset of five million subject-text-video pairs. Unlike previous benchmarks, OpenS2V-Eval emphasizes the generation of videos that maintain subject consistency and natural appearance. The authors also propose three new metrics to assess generated videos based on subject fidelity, naturalness, and relevance to the input text, facilitating a comprehensive evaluation of various S2V models.'}, 'zh': {'title': 'æ„å»ºè§†é¢‘ç”Ÿæˆçš„æ–°åŸºå‡†ä¸æ•°æ®é›†', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†Subject-to-Video (S2V) ç”Ÿæˆçš„åŸºç¡€è®¾æ–½OpenS2V-Nexusï¼Œæ—¨åœ¨åˆ›å»ºå¿ å®äºå‚è€ƒå†…å®¹çš„è§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†OpenS2V-Evalï¼Œä¸€ä¸ªç»†ç²’åº¦çš„åŸºå‡†ï¼Œä¸“æ³¨äºç”Ÿæˆå…·æœ‰è‡ªç„¶å¤–è§‚å’Œèº«ä»½ä¿çœŸåº¦çš„ä¸€è‡´è§†é¢‘ã€‚ä¸ºäº†è¯„ä¼°ç”Ÿæˆè§†é¢‘çš„è´¨é‡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼Œåˆ†åˆ«é‡åŒ–ä¸»é¢˜ä¸€è‡´æ€§ã€è‡ªç„¶æ€§å’Œæ–‡æœ¬ç›¸å…³æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å«äº”ç™¾ä¸‡ä¸ªé«˜è´¨é‡720Pä¸»é¢˜-æ–‡æœ¬-è§†é¢‘ä¸‰å…ƒç»„çš„å¼€æ”¾æºä»£ç æ•°æ®é›†OpenS2V-5Mï¼Œä»¥æ”¯æŒæœªæ¥çš„S2Vç”Ÿæˆç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19641', 'title': 'SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond', 'url': 'https://huggingface.co/papers/2505.19641', 'abstract': 'SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the potential of Reinforcement Learning (RL) to enhance reasoning abilities in Large Language Models (LLMs). While open-source replication efforts have primarily focused on mathematical and coding domains, methods and resources for developing general reasoning capabilities remain underexplored. This gap is partly due to the challenge of collecting diverse and verifiable reasoning data suitable for RL. We hypothesize that logical reasoning is critical for developing general reasoning capabilities, as logic forms a fundamental building block of reasoning. In this work, we present SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale, encompassing 35 diverse logical reasoning tasks. The SynLogic approach enables controlled synthesis of data with adjustable difficulty and quantity. Importantly, all examples can be verified by simple rules, making them ideally suited for RL with verifiable rewards. In our experiments, we validate the effectiveness of RL training on the SynLogic dataset based on 7B and 32B models. SynLogic leads to state-of-the-art logical reasoning performance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B by 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and coding tasks improves the training efficiency of these domains and significantly enhances reasoning generalization. Notably, our mixed training model outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These findings position SynLogic as a valuable resource for advancing the broader reasoning capabilities of LLMs. We open-source both the data synthesis pipeline and the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.', 'score': 41, 'issue_id': 3996, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '4c75ba0f126de4b9', 'authors': ['Junteng Liu', 'Yuanxiang Fan', 'Zhuo Jiang', 'Han Ding', 'Yongyi Hu', 'Chi Zhang', 'Yiqi Shi', 'Shitong Weng', 'Aili Chen', 'Shiqi Chen', 'Yunan Huang', 'Mozhi Zhang', 'Pengyu Zhao', 'Junjie Yan', 'Junxian He'], 'affiliations': ['MiniMax', 'The City University of Hong Kong', 'The Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2505.19641.jpg', 'data': {'categories': ['#training', '#rl', '#dataset', '#reasoning', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'SynLogic: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ˜Ğ˜ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ', 'desc': 'SynLogic - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¿Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»ĞµĞ³ĞºĞ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¸Ñ… Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynLogic, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ ÑÑ€ĞµĞ´Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ². Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… SynLogic Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±Ñ‰Ğ¸Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing AI Reasoning with SynLogic Framework', 'desc': 'SynLogic is a framework designed to improve the logical reasoning skills of Large Language Models (LLMs) using Reinforcement Learning (RL). It generates a diverse set of logical reasoning tasks, allowing for controlled data synthesis that can be adjusted in difficulty and quantity. The framework not only enhances the reasoning capabilities of LLMs but also improves their generalization across different domains by mixing logical reasoning data with mathematical and coding tasks. The results show that models trained with SynLogic achieve state-of-the-art performance, demonstrating its potential as a valuable resource for advancing AI reasoning.'}, 'zh': {'title': 'SynLogicï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„å…³é”®', 'desc': 'SynLogicæ˜¯ä¸€ä¸ªæ•°æ®åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶ç”Ÿæˆå¤šæ ·åŒ–çš„é€»è¾‘æ¨ç†æ•°æ®ï¼Œæ¶µç›–35ä¸ªä¸åŒçš„æ¨ç†ä»»åŠ¡ï¼Œå¹¶å…è®¸æ ¹æ®éš¾åº¦å’Œæ•°é‡è¿›è¡Œæ§åˆ¶åˆæˆã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨SynLogicæ•°æ®è¿›è¡ŒRLè®­ç»ƒï¼Œèƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨ç†æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šç°æœ‰çš„å¼€æºæ•°æ®é›†ã€‚é€šè¿‡å°†SynLogicæ•°æ®ä¸æ•°å­¦å’Œç¼–ç ä»»åŠ¡æ··åˆè®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†è¿™äº›é¢†åŸŸçš„è®­ç»ƒæ•ˆç‡å’Œæ¨ç†æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20325', 'title': 'Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence', 'url': 'https://huggingface.co/papers/2505.20325', 'abstract': 'The Guided by Gut (GG) framework enhances LLM reasoning efficiently using intrinsic signals and token-level confidence, outperforming PRM-based methods with faster inference and lower memory usage.  \t\t\t\t\tAI-generated summary \t\t\t\t Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM) reasoning often incur substantial computational costs, primarily due to extensive reliance on external Process Reward Models (PRMs) or sampling methods like Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient self-guided TTS framework that achieves PRM-level performance without costly external verifier models. Our method employs a lightweight tree search guided solely by intrinsic LLM signals, token-level confidence and step novelty. One critical innovation is improving the reliability of internal confidence estimates via a targeted reinforcement learning fine-tuning phase. Empirical evaluations on challenging mathematical reasoning benchmarks demonstrate that GG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching or surpassing significantly larger models (e.g., 32B-70B parameters), while reducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG achieves comparable accuracy with 8x faster inference speeds and 4-5x lower memory usage. Additionally, GG reduces KV cache memory usage by approximately 50% compared to the BoN strategy, facilitating more efficient and practical deployment of TTS techniques.', 'score': 39, 'issue_id': 4006, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '3055ff28f0a7f0f6', 'authors': ['Amirhosein Ghasemabadi', 'Keith G. Mills', 'Baochun Li', 'Di Niu'], 'affiliations': ['ECE Department, University of Alberta', 'ECE Department, University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.20325.jpg', 'data': {'categories': ['#training', '#rl', '#math', '#optimization', '#small_models', '#inference', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Guided by Gut (GG) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). GG Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ LLM Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ PRM, Ğ½Ğ¾ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¼ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ´ĞµÑ€ĞµĞ²Ñƒ, ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ LLM. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GG Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU Ğ´Ğ¾ 10 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'Efficient Reasoning with Guided by Gut: Smarter, Faster, Smaller!', 'desc': 'The Guided by Gut (GG) framework improves the reasoning capabilities of Large Language Models (LLMs) by using internal signals and confidence levels at the token level. Unlike traditional Test-Time Scaling (TTS) methods that depend on external Process Reward Models (PRMs), GG operates efficiently without these costly verifiers. It utilizes a lightweight tree search and enhances internal confidence through reinforcement learning fine-tuning. Empirical results show that GG allows smaller models to achieve high accuracy while significantly reducing memory usage and increasing inference speed compared to PRM-based approaches.'}, 'zh': {'title': 'é«˜æ•ˆæ¨ç†ï¼Œè¶…è¶Šè§„æ¨¡çš„GGæ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º"Guided by Gut (GG)"çš„æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ•ˆç‡ã€‚GGæ¡†æ¶é€šè¿‡åˆ©ç”¨å†…åœ¨ä¿¡å·å’Œä»¤ç‰Œçº§ç½®ä¿¡åº¦ï¼Œé¿å…äº†å¯¹å¤–éƒ¨è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„ä¾èµ–ï¼Œä»è€Œå®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„å†…å­˜ä½¿ç”¨ã€‚é€šè¿‡è½»é‡çº§çš„æ ‘æœç´¢å’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ŒGGèƒ½å¤Ÿåœ¨è¾ƒå°çš„æ¨¡å‹ä¸Šè¾¾åˆ°ä¸æ›´å¤§æ¨¡å‹ç›¸å½“çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶æ˜¾è‘—é™ä½GPUå†…å­˜æ¶ˆè€—ã€‚å®éªŒè¯æ˜ï¼ŒGGåœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„é«˜æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17813', 'title': "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM\n  Reasoning", 'url': 'https://huggingface.co/papers/2505.17813', 'abstract': 'Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.', 'score': 38, 'issue_id': 3992, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': 'd8d9d938b84c5ea6', 'authors': ['Michael Hassid', 'Gabriel Synnaeve', 'Yossi Adi', 'Roy Schwartz'], 'affiliations': ['FAIR Team, Meta', 'The Hebrew University of Jerusalem'], 'pdf_title_img': 'assets/pdf/title_img/2505.17813.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training'], 'emoji': 'âš¡', 'ru': {'title': 'ĞšĞ¾Ñ€Ğ¾Ñ‡Ğµ Ğ¼Ñ‹ÑĞ»ÑŒ - Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ²Ñ‹Ğ²Ğ¾Ğ´: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM) Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ short-m@k, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑÑ€ĞµĞ´Ğ¸ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… m Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, Ñ‡ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ, Ğ¿Ñ€Ğ¸ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…. Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾, Ğ°Ğ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ° ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Shorter Chains, Smarter Reasoning!', 'desc': 'This paper investigates the effectiveness of reasoning in large language models (LLMs) by comparing long and short reasoning chains. The authors find that shorter reasoning chains can yield significantly more accurate answers, with improvements of up to 34.5% compared to longer chains. They introduce a new method called short-m@k, which allows for parallel processing of multiple reasoning chains and selects the final answer based on majority voting. Their results indicate that shorter reasoning processes not only enhance performance but also reduce computational costs and inference time, challenging the traditional belief that longer reasoning leads to better outcomes.'}, 'zh': {'title': 'çŸ­æ€ç»´é“¾ï¼Œæå‡æ¨ç†èƒ½åŠ›ï¼', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ¨ç†ä»»åŠ¡ä¸­ä¾èµ–é•¿æ€ç»´é“¾çš„å‡è®¾ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè¾ƒçŸ­çš„æ¨ç†é“¾åœ¨å›ç­”é—®é¢˜æ—¶æ›´å¯èƒ½äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œå‡†ç¡®ç‡æ¯”æœ€é•¿é“¾é«˜å‡º34.5%ã€‚åŸºäºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ–¹æ³•short-m@kï¼Œé€šè¿‡å¹¶è¡Œç”Ÿæˆkä¸ªç‹¬ç«‹çš„æ€ç»´è¿‡ç¨‹ï¼Œå¹¶åœ¨ç¬¬ä¸€ä¸ªmä¸ªå®Œæˆååœæ­¢è®¡ç®—ï¼Œæœ€ç»ˆç­”æ¡ˆé€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰å‡ºã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒçŸ­æ€ç»´é“¾çš„è®­ç»ƒå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼ŒæŒ‘æˆ˜äº†é•¿æ€ç»´é“¾å¿…ç„¶å¸¦æ¥æ›´å¥½æ¨ç†èƒ½åŠ›çš„ä¼ ç»Ÿè§‚å¿µã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21189', 'title': 'Exploring the Latent Capacity of LLMs for One-Step Text Generation', 'url': 'https://huggingface.co/papers/2505.21189', 'abstract': 'LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.  \t\t\t\t\tAI-generated summary \t\t\t\t A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.', 'score': 37, 'issue_id': 3996, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '993d2720f6fed612', 'authors': ['Gleb Mezentsev', 'Ivan Oseledets'], 'affiliations': ['AIRI Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2505.21189.jpg', 'data': {'categories': ['#data', '#long_context', '#architecture', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'ĞœĞ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ² Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ²Ğ° ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°. Ğ­Ñ‚Ğ¾ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½ĞµĞ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ LLM Ğº Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ‚Ğ¾ĞºĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²Ğ° ÑÑ‚Ğ¸Ñ… ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€ÑƒÑ Ğ¾Ğ½Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‚. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ²ÑĞ·Ğ½Ñ‹Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Multi-Token Generation in LLMs Without Autoregression', 'desc': 'This paper investigates the ability of large language models (LLMs) to generate long text segments in a single forward pass using learned embeddings. It demonstrates that LLMs can produce hundreds of accurate tokens without relying on autoregressive methods, which typically require iterative decoding. The study reveals that by using just two learned embeddings, LLMs can reconstruct extensive text, showcasing a previously underexplored capability. Additionally, the authors analyze the properties of these embeddings, suggesting that they encode meaningful information and could lead to the development of a dedicated encoder for improved text generation.'}, 'zh': {'title': 'æ¢ç´¢LLMsçš„å¤šæ ‡è®°ç”Ÿæˆèƒ½åŠ›', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ²¡æœ‰è‡ªå›å½’çš„æƒ…å†µä¸‹ç”Ÿæˆé•¿æ–‡æœ¬çš„èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå†»ç»“çš„LLMså¯ä»¥ä»…é€šè¿‡ä¸¤ä¸ªå­¦ä¹ åˆ°çš„åµŒå…¥ï¼Œåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­ç”Ÿæˆæ•°ç™¾ä¸ªå‡†ç¡®çš„æ ‡è®°ã€‚è¿™æ­ç¤ºäº†LLMsåœ¨å¤šæ ‡è®°ç”Ÿæˆæ–¹é¢çš„æ½œåŠ›ï¼Œè¶…å‡ºäº†ä¼ ç»Ÿçš„è¿­ä»£è§£ç æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è¿™äº›åµŒå…¥çš„è¡Œä¸ºï¼Œå¹¶æä¾›äº†å®ƒä»¬æ‰€ç¼–ç ä¿¡æ¯çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19000', 'title': 'VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied\n  Iterative Policy Optimization', 'url': 'https://huggingface.co/papers/2505.19000', 'abstract': "A Verifier-guided Iterative Policy Optimization method enhances Video-LLMs' reasoning capabilities by integrating a Rollout-Aware Verifier between GRPO and DPO phases, leading to faster and more effective optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.", 'score': 35, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': '058fcf46b0f20cc6', 'authors': ['Yunxin Li', 'Xinyu Chen', 'Zitao Li', 'Zhenyu Liu', 'Longyue Wang', 'Wenhan Luo', 'Baotian Hu', 'Min Zhang'], 'affiliations': ['Alibaba International Group', 'Division of AMC and Department of ECE, HKUST', 'Harbin Institute of Technology, Shenzhen, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.19000.jpg', 'data': {'categories': ['#reasoning', '#training', '#video', '#optimization', '#rl', '#rlhf'], 'emoji': 'ğŸ¥', 'ru': {'title': 'VerIPO: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ VerIPO Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Verifier Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ğ°Ğ·Ğ°Ğ¼Ğ¸ GRPO Ğ¸ DPO Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. VerIPO Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ñ‡Ñ‚Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² 7 Ñ€Ğ°Ğ· Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ GRPO. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ VerIPO Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Video Reasoning with Verifier-guided Optimization', 'desc': 'This paper introduces VerIPO, a new method for improving Video Large Language Models (Video-LLMs) using a Verifier-guided Iterative Policy Optimization approach. It addresses the limitations of existing Reinforcement Fine-Tuning methods by incorporating a Rollout-Aware Verifier that enhances the quality of reasoning chains during training. By creating high-quality contrastive data, this method allows for faster and more effective optimization, achieving results that are significantly better than traditional methods. Experimental findings show that VerIPO not only speeds up the training process but also improves the contextual consistency and length of reasoning outputs in video tasks.'}, 'zh': {'title': 'éªŒè¯è€…å¼•å¯¼çš„è¿­ä»£ä¼˜åŒ–ï¼Œæå‡è§†é¢‘æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºVerIPOçš„éªŒè¯è€…å¼•å¯¼è¿­ä»£ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œæ—¨åœ¨æå‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨GRPOå’ŒDPOé˜¶æ®µä¹‹é—´å¼•å…¥ä¸€ä¸ªå›æ»šæ„ŸçŸ¥éªŒè¯å™¨ï¼Œå½¢æˆGRPO-éªŒè¯å™¨-DPOè®­ç»ƒå¾ªç¯ï¼Œä»è€Œå®ç°æ›´å¿«ä¸”æ›´æœ‰æ•ˆçš„ä¼˜åŒ–ã€‚éªŒè¯å™¨åˆ©ç”¨å°å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ¨ç†é€»è¾‘ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å¯¹æ¯”æ•°æ®ï¼Œä¿ƒè¿›äº†é•¿é“¾æ¨ç†çš„ç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVerIPOåœ¨ä¼˜åŒ–é€Ÿåº¦å’Œæ¨ç†è´¨é‡ä¸Šå‡æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„GRPOæ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16459', 'title': 'MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks', 'url': 'https://huggingface.co/papers/2505.16459', 'abstract': 'The MMMR benchmark evaluates multi-modal reasoning in MLLMs by assessing thinking quality through diverse reasoning types and a modular evaluation pipeline.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems.', 'score': 35, 'issue_id': 3991, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'd18f80036a817d7c', 'authors': ['Guiyao Tie', 'Xueyang Zhou', 'Tianhe Gu', 'Ruihang Zhang', 'Chaoran Hu', 'Sizhe Zhang', 'Mengqu Sun', 'Yan Zhang', 'Pan Zhou', 'Lichao Sun'], 'affiliations': ['Huazhong University of Science and Technology', 'Lehigh University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16459.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MMMR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MMMR Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM). MMMR Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 1083 ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… ÑˆĞµÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (RTEP). Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ MLLM Ñ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ±ĞµĞ· Ğ½Ğ¸Ñ…, Ğ½Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑ‚Ñ€Ğ°Ğ´Ğ°ÑÑ‚ Ğ¾Ñ‚ Ğ½ĞµÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Evaluating Multi-Modal Reasoning: The MMMR Benchmark', 'desc': 'The MMMR benchmark is designed to evaluate the reasoning abilities of Multi-Modal Large Language Models (MLLMs) by focusing on their thinking quality across various reasoning types. It includes a challenging dataset with 1,083 questions that require complex reasoning, and a modular evaluation pipeline to assess reasoning quality beyond just accuracy. The study finds that while MLLMs with intermediate thinking traces perform better than those without, they still exhibit issues like inconsistency and overthinking. This benchmark aims to bridge the gap between accuracy and reasoning quality, providing a structured approach for future advancements in multi-modal reasoning systems.'}, 'zh': {'title': 'å¤šæ¨¡æ€æ¨ç†çš„æ–°åŸºå‡†ï¼šMMMR', 'desc': 'MMMRåŸºå‡†æµ‹è¯•è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡å¤šæ ·çš„æ¨ç†ç±»å‹å’Œæ¨¡å—åŒ–è¯„ä¼°æµç¨‹æ¥è¯„ä¼°æ€ç»´è´¨é‡ã€‚å°½ç®¡MLLMsåœ¨è¯­è¨€ã€è§†è§‰å’Œç»“æ„åŒ–è¾“å…¥çš„ç»Ÿä¸€å¤„ç†ä¸Šå–å¾—äº†è¿›å±•ï¼Œä½†å…¶æ¨ç†èƒ½åŠ›ä»ç„¶ä¸å¤Ÿæ¸…æ™°ï¼Œç¼ºä¹æ ‡å‡†åŒ–çš„è¯„ä¼°åŸºå‡†ã€‚MMMRåŒ…å«ä¸€ä¸ªé«˜éš¾åº¦çš„æ•°æ®é›†å’Œä¸€ä¸ªæ¨ç†è¿½è¸ªè¯„ä¼°ç®¡é“ï¼Œæ—¨åœ¨è¶…è¶Šå‡†ç¡®æ€§è¯„ä¼°ï¼Œå…³æ³¨æ¨ç†çš„ç›¸å…³æ€§ã€ä¸€è‡´æ€§å’Œç»“æ„åŒ–é”™è¯¯æ³¨é‡Šã€‚é€šè¿‡å®è¯ç»“æœï¼ŒMMMRæ­ç¤ºäº†å‡†ç¡®æ€§ä¸æ¨ç†è´¨é‡ä¹‹é—´çš„å·®è·ï¼Œä¸ºæœªæ¥æ¨¡å‹çš„å‘å±•æä¾›äº†å¯æ“ä½œçš„è¯„ä¼°æ¡†æ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21496', 'title': 'UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents', 'url': 'https://huggingface.co/papers/2505.21496', 'abstract': 'In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.', 'score': 33, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'db6e0d226a2c500e', 'authors': ['Han Xiao', 'Guozhi Wang', 'Yuxiang Chai', 'Zimu Lu', 'Weifeng Lin', 'Hao He', 'Lue Fan', 'Liuyang Bian', 'Rui Hu', 'Liang Liu', 'Shuai Ren', 'Yafei Wen', 'Xiaoxin Chen', 'Aojun Zhou', 'Hongsheng Li'], 'affiliations': ['CPII under InnoHK', 'CUHK MMLab', 'vivo AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21496.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#synthetic', '#open_source', '#dataset', '#agents', '#training', '#data'], 'emoji': 'ğŸ§', 'ru': {'title': 'UI-Genie: ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ÑÑ Ğ˜Ğ˜-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ²', 'desc': 'UI-Genie - ÑÑ‚Ğ¾ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²ÑƒÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°, Ñ€ĞµÑˆĞ°ÑÑ‰Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ UI-Genie-RM Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ñ‚ĞµĞºÑÑ‚, Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ ÑĞ°Ğ¼Ğ¾ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ”Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… UI-Genie-RM-517k Ğ¸ UI-Genie-Agent-16k Ğ±ĞµĞ· Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UI-Genie Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ´Ğ»Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ°.'}, 'en': {'title': 'UI-Genie: Revolutionizing GUI Agents with Self-Improvement and Reward Models', 'desc': 'This paper presents UI-Genie, a framework designed to improve GUI agents by tackling the challenges of verifying outcomes and scaling high-quality training data. It introduces a reward model, UI-Genie-RM, which uses an image-text interleaved architecture to effectively process historical data and combine different levels of rewards. The framework also includes innovative data generation strategies to create training data without manual effort, such as rule-based verification and hard negative mining. Experimental results indicate that UI-Genie outperforms existing methods in GUI agent tasks, showcasing the effectiveness of its self-improvement approach.'}, 'zh': {'title': 'UI-Genieï¼šè‡ªæˆ‘æ”¹è¿›çš„GUIä»£ç†æ¡†æ¶', 'desc': 'æœ¬æ–‡ä»‹ç»äº†UI-Genieï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªæˆ‘æ”¹è¿›çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³GUIä»£ç†ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šè½¨è¿¹ç»“æœçš„éªŒè¯å›°éš¾å’Œé«˜è´¨é‡è®­ç»ƒæ•°æ®çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¥–åŠ±æ¨¡å‹å’Œè‡ªæˆ‘æ”¹è¿›ç®¡é“æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å¥–åŠ±æ¨¡å‹UI-Genie-RMé‡‡ç”¨å›¾åƒ-æ–‡æœ¬äº¤é”™æ¶æ„ï¼Œæœ‰æ•ˆå¤„ç†å†å²ä¸Šä¸‹æ–‡ï¼Œå¹¶ç»Ÿä¸€äº†åŠ¨ä½œçº§å’Œä»»åŠ¡çº§å¥–åŠ±ã€‚è‡ªæˆ‘æ”¹è¿›ç®¡é“é€šè¿‡å¥–åŠ±å¼•å¯¼æ¢ç´¢å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„ç»“æœéªŒè¯ï¼Œé€æ­¥æ‰©å±•å¯è§£å†³çš„å¤æ‚GUIä»»åŠ¡ï¼Œä»è€Œæå‡ä»£ç†å’Œå¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18875', 'title': 'Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via\n  Semantic-Aware Permutation', 'url': 'https://huggingface.co/papers/2505.18875', 'abstract': 'SVG2 is a training-free framework that enhances video generation efficiency and quality by accurately identifying and processing critical tokens using semantic-aware permutation and dynamic budget control.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates top-p dynamic budget control and customized kernel implementations, achieving up to 2.30x and 1.89x speedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan 2.1, respectively.', 'score': 32, 'issue_id': 3990, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': 'bc68e232d8897ad4', 'authors': ['Shuo Yang', 'Haocheng Xi', 'Yilong Zhao', 'Muyang Li', 'Jintao Zhang', 'Han Cai', 'Yujun Lin', 'Xiuyu Li', 'Chenfeng Xu', 'Kelly Peng', 'Jianfei Chen', 'Song Han', 'Kurt Keutzer', 'Ion Stoica'], 'affiliations': ['MIT', 'University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2505.18875.jpg', 'data': {'categories': ['#diffusion', '#training', '#video', '#optimization'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'SVG2 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºÑƒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². SVG2 Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ k-means Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ñƒ, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ±ÑĞ´Ğ¶ĞµÑ‚Ğ° top-p Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ´Ñ€Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2.30x Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Maximizing Video Generation Efficiency with SVG2', 'desc': 'SVG2 is a novel framework designed to improve the efficiency and quality of video generation without the need for extensive training. It focuses on accurately identifying critical tokens through semantic-aware permutation, which groups tokens based on their meanings rather than just their positions. This method reduces computational waste by ensuring that critical tokens are processed together, optimizing GPU usage. By implementing dynamic budget control, SVG2 achieves significant speed improvements while maintaining high video quality, demonstrating a balance between performance and resource efficiency.'}, 'zh': {'title': 'SVG2ï¼šæå‡è§†é¢‘ç”Ÿæˆæ•ˆç‡ä¸è´¨é‡çš„åˆ›æ–°æ¡†æ¶', 'desc': 'SVG2æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œé€šè¿‡å‡†ç¡®è¯†åˆ«å’Œå¤„ç†å…³é”®æ ‡è®°ï¼Œæå‡è§†é¢‘ç”Ÿæˆçš„æ•ˆç‡å’Œè´¨é‡ã€‚å®ƒé‡‡ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„æ’åˆ—å’ŒåŠ¨æ€é¢„ç®—æ§åˆ¶ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨è®¡ç®—é¢„ç®—ä¸‹ç”Ÿæˆè´¨é‡ä¸ä½³çš„é—®é¢˜ã€‚SVG2é€šè¿‡k-meansèšç±»å’Œé‡æ–°æ’åˆ—æ ‡è®°ï¼Œç¡®ä¿äº†ç²¾ç¡®çš„èšç±»è¡¨ç¤ºï¼Œä»è€Œæé«˜äº†è¯†åˆ«å‡†ç¡®æ€§ï¼Œå¹¶å‡å°‘äº†è®¡ç®—æµªè´¹ã€‚è¯¥æ¡†æ¶åœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è¾¾2.30å€çš„åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21333', 'title': 'MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios', 'url': 'https://huggingface.co/papers/2505.21333', 'abstract': 'MLLMs achieve modest accuracy in video OCR due to motion blur, temporal variations, and visual effects; MME-VideoOCR benchmark reveals limitations in spatio-temporal reasoning and language bias.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios.', 'score': 29, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '25639980b7f7add5', 'authors': ['Yang Shi', 'Huanqian Wang', 'Wulin Xie', 'Huanyao Zhang', 'Lijie Zhao', 'Yi-Fan Zhang', 'Xinfeng Li', 'Chaoyou Fu', 'Zhuoer Wen', 'Wenting Liu', 'Zhuoran Zhang', 'Xinlong Chen', 'Bohan Zeng', 'Sihan Yang', 'Yuanxing Zhang', 'Pengfei Wan', 'Haotian Wang', 'Wenjing Yang'], 'affiliations': ['CASIA', 'CUHKSZ', 'Kuaishou', 'NTU', 'PKU', 'THU', 'XJTU'], 'pdf_title_img': 'assets/pdf/title_img/2505.21333.jpg', 'data': {'categories': ['#multimodal', '#games', '#benchmark', '#reasoning', '#video'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ OCR Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM) Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR) Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ·-Ğ·Ğ° Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ„Ñ„ĞµĞºÑ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº MME-VideoOCR, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 10 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 25 Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ MLLM Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ OCR. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 18 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ³Ğ¾ OCR Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing Video OCR: Bridging the Gap in MLLM Performance', 'desc': 'This paper discusses the challenges faced by Multimodal Large Language Models (MLLMs) in performing Optical Character Recognition (OCR) on videos. It highlights that factors like motion blur and temporal variations significantly reduce their accuracy compared to static images. To address these issues, the authors introduce the MME-VideoOCR benchmark, which includes a variety of tasks designed to test spatio-temporal reasoning and language understanding in video contexts. The evaluation of 18 MLLMs reveals that even the best models struggle with comprehensive video comprehension, particularly in scenarios requiring integration of information across multiple frames.'}, 'zh': {'title': 'æå‡è§†é¢‘OCRçš„å¤šæ¨¡æ€åŸºå‡†æŒ‘æˆ˜', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒçš„å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è§†é¢‘OCRä¸­æ•ˆæœæ˜¾è‘—ä¸‹é™ã€‚è¿™æ˜¯ç”±äºè§†é¢‘å†…å®¹ä¸­çš„è¿åŠ¨æ¨¡ç³Šã€æ—¶é—´å˜åŒ–å’Œè§†è§‰æ•ˆæœç­‰å› ç´ å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MME-VideoOCRåŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§è§†é¢‘OCRåº”ç”¨åœºæ™¯ï¼ŒåŒ…å«10ä¸ªä»»åŠ¡ç±»åˆ«å’Œ25ä¸ªå…·ä½“ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„MLLMsåœ¨å¤„ç†éœ€è¦æ•´ä½“è§†é¢‘ç†è§£çš„ä»»åŠ¡æ—¶èƒ½åŠ›æœ‰é™ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶ç©ºæ¨ç†å’Œè·¨å¸§ä¿¡æ¯æ•´åˆæ–¹é¢ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20355', 'title': 'GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.20355', 'abstract': "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git", 'score': 28, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'd4035428ea14ce6b', 'authors': ['Yeonjoon Jung', 'Daehyun Ahn', 'Hyungjun Kim', 'Taesu Kim', 'Eunhyeok Park'], 'affiliations': ['POSTECH', 'SqueezeBits'], 'pdf_title_img': 'assets/pdf/title_img/2505.20355.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#optimization'], 'emoji': 'ğŸ§©', 'ru': {'title': 'GraLoRA: Ğ“Ñ€Ğ°Ğ½ÑƒĞ»ÑÑ€Ğ½Ğ°Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ğ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Granular Low-Rank Adaptation (GraLoRA). GraLoRA Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Low-Rank Adaptation (LoRA), ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ½Ğ³Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ĞµÑĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ½Ğ° Ğ¿Ğ¾Ğ´Ğ±Ğ»Ğ¾ĞºĞ¸, ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ¾ ÑĞ²Ğ¾Ğ¸Ğ¼ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€Ğ°Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ¾Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ GraLoRA Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ LoRA Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ¸ Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¼Ñ‹ÑĞ»Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 8.5% Ğ² Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞµ Pass@1 Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ HumanEval+.'}, 'en': {'title': 'GraLoRA: Unlocking the Power of Fine-Tuning with Granular Adaptation', 'desc': 'This paper introduces Granular Low-Rank Adaptation (GraLoRA), a new method designed to improve the performance of Low-Rank Adaptation (LoRA) in fine-tuning generative models. LoRA is effective but struggles with overfitting when the rank is increased, leading to poor accuracy compared to full fine-tuning. GraLoRA addresses this issue by dividing weight matrices into smaller sub-blocks, allowing each to have its own low-rank adapter, which enhances gradient propagation and reduces entanglement. Experimental results demonstrate that GraLoRA significantly outperforms LoRA and other methods, achieving notable improvements in various benchmarks without increasing computational costs.'}, 'zh': {'title': 'é¢—ç²’ä½ç§©é€‚åº”ï¼šè¶…è¶ŠLoRAçš„é«˜æ•ˆå¾®è°ƒ', 'desc': 'ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ˜¯ä¸€ç§æµè¡Œçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œå› å…¶ç®€å•æœ‰æ•ˆè€Œå—åˆ°é‡è§†ã€‚å°½ç®¡æœ€è¿‘æœ‰æ‰€æ”¹è¿›ï¼ŒLoRAä»ç„¶é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æ€§é™åˆ¶ï¼šå½“ç“¶é¢ˆåŠ å®½æ—¶å®¹æ˜“è¿‡æ‹Ÿåˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç»“æ„ï¼Œç§°ä¸ºé¢—ç²’ä½ç§©é€‚åº”ï¼ˆGraLoRAï¼‰ï¼Œå®ƒå°†æƒé‡çŸ©é˜µåˆ’åˆ†ä¸ºå­å—ï¼Œæ¯ä¸ªå­å—éƒ½æœ‰è‡ªå·±çš„ä½ç§©é€‚é…å™¨ï¼Œä»è€Œå…‹æœäº†LoRAçš„å±€é™æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒGraLoRAåœ¨ä»£ç ç”Ÿæˆå’Œå¸¸è¯†æ¨ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜äºLoRAï¼Œå…·æœ‰æ›´å¼ºçš„å¯æ‰©å±•æ€§å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21374', 'title': 'Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?', 'url': 'https://huggingface.co/papers/2505.21374', 'abstract': 'Video-Holmes benchmark evaluates complex video reasoning capabilities of MLLMs using suspense short films and reveals significant challenges in information integration compared to human experts.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.', 'score': 26, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '0683137770e562ab', 'authors': ['Junhao Cheng', 'Yuying Ge', 'Teng Wang', 'Yixiao Ge', 'Jing Liao', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.21374.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#benchmark', '#video'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ¨ĞµÑ€Ğ»Ğ¾Ğº Ğ¥Ğ¾Ğ»Ğ¼Ñ Ğ´Ğ»Ñ Ğ˜Ğ˜: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Video-Holmes - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¶Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ»ÑŒĞ¼Ñ‹-ÑĞ°ÑĞ¿ĞµĞ½Ñ Ğ¸ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1837 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ 270 Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… 7 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²Ñ‹ÑĞ²Ğ¸Ğ» Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ›ÑƒÑ‡ÑˆĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Gemini-2.5-Pro Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ 45%, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞ¸Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Video-Holmes: A New Benchmark for Complex Video Reasoning', 'desc': 'The Video-Holmes benchmark assesses the complex video reasoning abilities of Multimodal Language Models (MLLMs) using suspense short films. It highlights the challenges these models face in integrating information compared to human experts, particularly in real-world reasoning scenarios. The benchmark includes 1,837 questions based on 270 annotated films, requiring models to connect multiple visual clues across different segments. Despite advancements in reasoning techniques, the evaluation shows that even the best models struggle with accuracy, achieving only 45%, indicating significant room for improvement in multimodal reasoning.'}, 'zh': {'title': 'Video-Holmesï¼šæ¿€åŠ±æ¨¡å‹æ›´åƒäººç±»æ¨ç†çš„åŸºå‡†æµ‹è¯•', 'desc': 'Video-HolmesåŸºå‡†æµ‹è¯•è¯„ä¼°äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤æ‚è§†é¢‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ‚¬ç–‘çŸ­ç‰‡æ¥æ­ç¤ºä¸äººç±»ä¸“å®¶ç›¸æ¯”çš„ä¿¡æ¯æ•´åˆæŒ‘æˆ˜ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ª270éƒ¨æ‰‹åŠ¨æ³¨é‡Šçš„æ‚¬ç–‘çŸ­ç‰‡çš„1837ä¸ªé—®é¢˜ï¼Œè®¾è®¡äº†ä¸ƒä¸ªä»»åŠ¡ï¼Œè¦æ±‚æ¨¡å‹ä¸»åŠ¨å¯»æ‰¾å’Œè¿æ¥åˆ†æ•£åœ¨ä¸åŒè§†é¢‘ç‰‡æ®µä¸­çš„å¤šä¸ªç›¸å…³è§†è§‰çº¿ç´¢ã€‚å°½ç®¡ç°æœ‰çš„MLLMsåœ¨è§†è§‰æ„ŸçŸ¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¿¡æ¯æ•´åˆä¸Šå´é¢ä¸´é‡å¤§å›°éš¾ï¼Œè®¸å¤šæ¨¡å‹çš„å‡†ç¡®ç‡ä½äº40%ã€‚æˆ‘ä»¬å¸Œæœ›Video-Holmesèƒ½å¤Ÿæ¿€åŠ±æ¨¡å‹æ›´åƒäººç±»è¿›è¡Œæ¨ç†ï¼Œå¹¶å¼ºè°ƒè¿™ä¸€é¢†åŸŸçš„æŒç»­æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21297', 'title': 'rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale\n  Verified Dataset', 'url': 'https://huggingface.co/papers/2505.21297', 'abstract': 'A large-scale dataset called rStar-Coder enhances code reasoning in LLMs by providing verified code problems and solutions, leading to improved performance on various benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.', 'score': 22, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '1fdb1a9edd20c73b', 'authors': ['Yifei Liu', 'Li Lyna Zhang', 'Yi Zhu', 'Bingcheng Dong', 'Xudong Zhou', 'Ning Shang', 'Fan Yang', 'Mao Yang'], 'affiliations': ['Dalian University of Technology', 'Microsoft Research Asia', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21297.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#reasoning', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'rStar-Coder: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ğ¾ ĞºĞ¾Ğ´Ğµ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ rStar-Coder - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ğ¾ ĞºĞ¾Ğ´Ğµ. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 418 Ñ‚Ñ‹ÑÑÑ‡ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ, 580 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ rStar-Coder Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑĞ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Qwen Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ² Ğ¸Ğ¼ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ¼Ñ‹Ñ… Ñ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ LLM Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¾ ĞºĞ¾Ğ´Ğµ, Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ğ³Ğ¾Ñ€Ğ°Ğ·Ğ´Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ LiveCodeBench Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Qwen2.5-14B ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ° ÑĞ²Ğ¾Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ñ 23.3% Ğ´Ğ¾ 62.5% Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° rStar-Coder.'}, 'en': {'title': 'Unlocking Code Reasoning with rStar-Coder', 'desc': 'The paper presents rStar-Coder, a large-scale dataset designed to enhance code reasoning capabilities in large language models (LLMs). It addresses the challenge of limited high-difficulty datasets by providing 418,000 verified code problems and 580,000 long-reasoning solutions, complete with diverse test cases. The dataset is created through a three-step process that includes curating competitive programming problems, synthesizing input-output test cases, and verifying solutions. Experiments show that models trained on rStar-Coder significantly outperform existing benchmarks, demonstrating its effectiveness in improving code reasoning tasks.'}, 'zh': {'title': 'æå‡ä»£ç æ¨ç†èƒ½åŠ›çš„rStar-Coderæ•°æ®é›†', 'desc': 'rStar-Coderæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†åŒ…å«418,000ä¸ªç«äº‰çº§åˆ«çš„ä»£ç é—®é¢˜å’Œ580,000ä¸ªé•¿æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œé…å¤‡ä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹ï¼Œæ¶µç›–ä¸åŒéš¾åº¦ã€‚é€šè¿‡ä¸‰é¡¹æ ¸å¿ƒè´¡çŒ®ï¼ŒrStar-Coderæä¾›äº†å¯éªŒè¯çš„è¾“å…¥è¾“å‡ºæµ‹è¯•æ¡ˆä¾‹ï¼Œç¡®ä¿äº†è§£å†³æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨rStar-Coderçš„æ•°æ®é›†ï¼ŒQwenæ¨¡å‹åœ¨å¤šä¸ªä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18943', 'title': 'MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent\n  Systems', 'url': 'https://huggingface.co/papers/2505.18943', 'abstract': "MetaMind, a multi-agent framework inspired by metacognition, enhances LLMs' ability to perform Theory of Mind tasks by decomposing social understanding into hypothesis generation, refinement, and response generation, achieving human-like performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.", 'score': 20, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': '718f1062d34a47a7', 'authors': ['Xuanming Zhang', 'Yuxuan Chen', 'Min-Hsuan Yeh', 'Yixuan Li'], 'affiliations': ['Tsinghua University', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.18943.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#agents', '#reasoning', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MetaMind: Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'MetaMind - ÑÑ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ. ĞĞ½Ğ° Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ¸Ñ… ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². MetaMind Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ½Ğ° 35.7% Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°: Ğ°Ğ³ĞµĞ½Ñ‚ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ÑƒĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ°Ğ²Ğ´Ğ¾Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ, ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑƒĞ¼ĞµÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.'}, 'en': {'title': 'Empowering AI with Human-like Social Intelligence', 'desc': 'MetaMind is a multi-agent framework that enhances large language models (LLMs) by improving their ability to understand human social interactions through Theory of Mind (ToM) tasks. It breaks down social understanding into three key stages: generating hypotheses about mental states, refining these hypotheses with cultural and ethical considerations, and producing contextually appropriate responses. This approach allows LLMs to achieve human-like performance in social reasoning, showing significant improvements in real-world scenarios and ToM reasoning tasks. The framework demonstrates the importance of each component in achieving a balance between contextual relevance and social appropriateness, paving the way for more empathetic AI interactions.'}, 'zh': {'title': 'MetaMindï¼šæå‡AIçš„ç¤¾ä¼šæ™ºèƒ½', 'desc': 'MetaMindæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œçµæ„Ÿæ¥æºäºå…ƒè®¤çŸ¥ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¿ƒæ™ºç†è®ºä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚è¯¥æ¡†æ¶å°†ç¤¾ä¼šç†è§£åˆ†è§£ä¸ºä¸‰ä¸ªåä½œé˜¶æ®µï¼šé¦–å…ˆï¼Œå¿ƒæ™ºç†è®ºä»£ç†ç”Ÿæˆç”¨æˆ·å¿ƒç†çŠ¶æ€çš„å‡è®¾ï¼›å…¶æ¬¡ï¼Œé¢†åŸŸä»£ç†åˆ©ç”¨æ–‡åŒ–è§„èŒƒå’Œä¼¦ç†çº¦æŸæ¥ç»†åŒ–è¿™äº›å‡è®¾ï¼›æœ€åï¼Œå“åº”ä»£ç†ç”Ÿæˆç¬¦åˆä¸Šä¸‹æ–‡çš„é€‚å½“å›åº”ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒMetaMindåœ¨ä¸‰ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œé¦–æ¬¡ä½¿LLMsåœ¨å…³é”®çš„å¿ƒæ™ºç†è®ºä»»åŠ¡ä¸Šè¾¾åˆ°äººç±»æ°´å¹³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21334', 'title': 'HoliTom: Holistic Token Merging for Fast Video Large Language Models', 'url': 'https://huggingface.co/papers/2505.21334', 'abstract': "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Video large language models (video LLMs) excel at video comprehension but face significant computational inefficiency due to redundant video tokens. Existing token pruning methods offer solutions. However, approaches operating within the LLM (inner-LLM pruning), such as FastV, incur intrinsic computational overhead in shallow layers. In contrast, methods performing token pruning before the LLM (outer-LLM pruning) primarily address spatial redundancy within individual frames or limited temporal windows, neglecting the crucial global temporal dynamics and correlations across longer video sequences. This leads to sub-optimal spatio-temporal reduction and does not leverage video compressibility fully. Crucially, the synergistic potential and mutual influence of combining these strategies remain unexplored. To further reduce redundancy, we introduce HoliTom, a novel training-free holistic token merging framework. HoliTom employs outer-LLM pruning through global redundancy-aware temporal segmentation, followed by spatial-temporal merging to reduce visual tokens by over 90%, significantly alleviating the LLM's computational burden. Complementing this, we introduce a robust inner-LLM token similarity-based merging approach, designed for superior performance and compatibility with outer-LLM pruning. Evaluations demonstrate our method's promising efficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational costs to 6.9% of FLOPs while maintaining 99.1% of the original performance. Furthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a 1.32x acceleration in decoding throughput, highlighting the practical benefits of our integrated pruning approach for efficient video LLMs inference.", 'score': 17, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'b07c0c110f38f89c', 'authors': ['Kele Shao', 'Keda Tao', 'Can Qin', 'Haoxuan You', 'Yang Sui', 'Huan Wang'], 'affiliations': ['Columbia University', 'Rice University', 'Salesforce AI Research', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21334.jpg', 'data': {'categories': ['#inference', '#video', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'HoliTom: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-LLM', 'desc': 'HoliTom - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾-LLM Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ½ĞµÑˆĞ½ÑÑ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ LLM Ñ‡ĞµÑ€ĞµĞ· Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğ¼ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ°. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ±ĞµĞ· ÑƒÑ‰ĞµÑ€Ğ±Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ ÑƒĞ´Ğ°ĞµÑ‚ÑÑ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ğ·Ğ°Ğ¿ÑÑ‚Ğ¾Ğ¹ Ğ´Ğ¾ 6,9% Ğ¾Ñ‚ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾, ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ² 99,1% Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'HoliTom: Efficient Video LLMs through Smart Token Pruning', 'desc': 'HoliTom is a novel framework designed to enhance the efficiency of video large language models (LLMs) by reducing computational redundancy in video tokens. It combines outer-LLM pruning, which segments video data globally to identify and eliminate redundant tokens, with inner-LLM token merging based on similarity to further optimize performance. This dual approach allows for a significant reduction in visual tokens by over 90%, while still maintaining high performance levels, achieving 99.1% of the original output. The method also improves processing speed, reducing computational costs to just 6.9% of FLOPs and accelerating decoding throughput by 1.32 times, making it a practical solution for efficient video LLM inference.'}, 'zh': {'title': 'HoliTomï¼šé«˜æ•ˆè§†é¢‘LLMçš„å…¨æ–°å‰ªæç­–ç•¥', 'desc': 'HoliTomæ˜¯ä¸€ç§æ–°é¢–çš„è®­ç»ƒæ— å…³çš„æ•´ä½“ä»¤ç‰Œåˆå¹¶æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å…¨çƒæ—¶é—´åˆ†å‰²è¿›è¡Œå¤–éƒ¨LLMå‰ªæï¼Œæ˜¾è‘—å‡å°‘è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆè§†é¢‘LLMï¼‰çš„è®¡ç®—æ•ˆç‡é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†å¤–éƒ¨LLMå‰ªæå’Œå†…éƒ¨LLMåŸºäºä»¤ç‰Œç›¸ä¼¼æ€§çš„åˆå¹¶ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå‡å°‘è§†è§‰ä»¤ç‰Œè¶…è¿‡90%ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒHoliTomæœ‰æ•ˆç¼“è§£äº†LLMçš„è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ä¿æŒäº†99.1%çš„åŸå§‹æ€§èƒ½ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬å’Œè§£ç é€Ÿåº¦ä¸Šå‡æœ‰æ˜¾è‘—æå‡ï¼Œå±•ç¤ºäº†å…¶åœ¨é«˜æ•ˆè§†é¢‘LLMæ¨ç†ä¸­çš„å®é™…åº”ç”¨ä»·å€¼ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17952', 'title': 'Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL', 'url': 'https://huggingface.co/papers/2505.17952', 'abstract': 'Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.', 'score': 17, 'issue_id': 3999, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '6dbd0f852449229e', 'authors': ['Che Liu', 'Haozhe Wang', 'Jiazhen Pan', 'Zhongwei Wan', 'Yong Dai', 'Fangzhen Lin', 'Wenjia Bai', 'Daniel Rueckert', 'Rossella Arcucci'], 'affiliations': ['Fudan University', 'HKUST', 'Imperial College London', 'Ohio State University', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.17952.jpg', 'data': {'categories': ['#healthcare', '#reasoning', '#benchmark', '#rl', '#dataset', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ˜Ğ˜: Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ²Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ AlphaMed - Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (LLM), Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚ÑƒÑ Ğ¸ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ (RL). ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ² Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ñ‚Ğ¼ĞµÑ‡Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… LLM.'}, 'en': {'title': 'Reinforcement Learning Powers Medical Reasoning Without Supervision', 'desc': 'This paper introduces AlphaMed, a medical large language model (LLM) that enhances reasoning capabilities through reinforcement learning (RL) without the need for supervised fine-tuning (SFT) or costly chain-of-thought (CoT) data. AlphaMed utilizes minimalist rule-based rewards on publicly available multiple-choice question-answering datasets, achieving state-of-the-art performance on various medical QA benchmarks. The study reveals that the quality and diversity of the dataset significantly influence reasoning performance, demonstrating that informative datasets can effectively promote reasoning skills. Additionally, the authors highlight the limitations of current evaluation methods and advocate for the development of more challenging benchmarks to better assess reasoning in medical contexts.'}, 'zh': {'title': 'é€šè¿‡å¼ºåŒ–å­¦ä¹ æå‡åŒ»å­¦æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†AlphaMedï¼Œè¿™æ˜¯é¦–ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°æ¨ç†èƒ½åŠ›çš„åŒ»å­¦å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–é—­æºæ¨¡å‹çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ•°æ®ã€‚AlphaMedåœ¨å…­ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„SFT+RLè®­ç»ƒæ¨¡å‹ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ•°æ®é›†çš„ä¿¡æ¯é‡æ˜¯æ¨ç†æ€§èƒ½çš„å…³é”®é©±åŠ¨å› ç´ ï¼Œä½¿ç”¨ä¿¡æ¯ä¸°å¯Œçš„å¤šé¡¹é€‰æ‹©é—®ç­”æ•°æ®è¿›è¡Œç®€çº¦çš„RLèƒ½å¤Ÿæœ‰æ•ˆè¯±å¯¼æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œå½“å‰è¯„ä¼°æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´å…·æŒ‘æˆ˜æ€§å’Œæ¨ç†å¯¼å‘çš„åŒ»å­¦é—®ç­”åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.14064', 'title': 'NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI', 'url': 'https://huggingface.co/papers/2505.14064', 'abstract': 'NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t In many real-world applications, deployed models encounter inputs that differ from the data seen during training. Out-of-distribution detection identifies whether an input stems from an unseen distribution, while open-world recognition flags such inputs to ensure the system remains robust as ever-emerging, previously unknown categories appear and must be addressed without retraining. Foundation and vision-language models are pre-trained on large and diverse datasets with the expectation of broad generalization across domains, including medical imaging. However, benchmarking these models on test sets with only a few common outlier types silently collapses the evaluation back to a closed-set problem, masking failures on rare or truly novel conditions encountered in clinical use.   We therefore present NOVA, a challenging, real-life evaluation-only benchmark of sim900 brain MRI scans that span 281 rare pathologies and heterogeneous acquisition protocols. Each case includes rich clinical narratives and double-blinded expert bounding-box annotations. Together, these enable joint assessment of anomaly localisation, visual captioning, and diagnostic reasoning. Because NOVA is never used for training, it serves as an extreme stress-test of out-of-distribution generalisation: models must bridge a distribution gap both in sample appearance and in semantic space. Baseline results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and Qwen2.5-VL-72B) reveal substantial performance drops across all tasks, establishing NOVA as a rigorous testbed for advancing models that can detect, localize, and reason about truly unknown anomalies.', 'score': 16, 'issue_id': 3999, 'pub_date': '2025-05-20', 'pub_date_card': {'ru': '20 Ğ¼Ğ°Ñ', 'en': 'May 20', 'zh': '5æœˆ20æ—¥'}, 'hash': '168ea71152c0b54f', 'authors': ['Cosmin I. Bercea', 'Jun Li', 'Philipp Raffler', 'Evamaria O. Riedel', 'Lena Schmitzer', 'Angela Kurz', 'Felix Bitzer', 'Paula RoÃŸmÃ¼ller', 'Julian Canisius', 'Mirjam L. Beyrle', 'Che Liu', 'Wenjia Bai', 'Bernhard Kainz', 'Julia A. Schnabel', 'Benedikt Wiestler'], 'affiliations': ['FAU Erlangen-NÃ¼rnberg', 'Helmholtz Center Munich', 'Imperial College London', 'Kings College London', 'Klinikum Rechts der Isar', 'Technical University of Munich'], 'pdf_title_img': 'assets/pdf/title_img/2505.14064.jpg', 'data': {'categories': ['#healthcare', '#cv', '#reasoning', '#benchmark', '#interpretability'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'NOVA: Ğ­ĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ½Ğ° Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ˜Ğ˜ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'NOVA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑÑ… ĞœĞ Ğ¢ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ·Ğ³Ğ°. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 900 ÑĞ½Ğ¸Ğ¼ĞºĞ¾Ğ² Ñ 281 Ñ€ĞµĞ´ĞºĞ¾Ğ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹ Ğ¸ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. NOVA ÑĞ»ÑƒĞ¶Ğ¸Ñ‚ ÑĞºÑÑ‚Ñ€ĞµĞ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑÑ‚Ñ€ĞµÑÑ-Ñ‚ĞµÑÑ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµÑ‚ÑŒ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² ĞºĞ°Ğº Ğ²Ğ¾ Ğ²Ğ½ĞµÑˆĞ½ĞµĞ¼ Ğ²Ğ¸Ğ´Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼.'}, 'en': {'title': 'NOVA: Stress-Testing AI for Rare MRI Pathologies', 'desc': 'NOVA is a benchmark designed to evaluate vision-language models specifically on rare MRI pathologies that are clinically relevant. It focuses on out-of-distribution detection and open-world recognition, which are crucial for models to handle unseen data effectively. By using a dataset of 900 brain MRI scans with 281 rare conditions, NOVA tests models on their ability to generalize beyond their training data. The results from leading models show significant performance drops, highlighting the need for improved capabilities in detecting and reasoning about unknown anomalies in medical imaging.'}, 'zh': {'title': 'NOVAï¼šæŒ‘æˆ˜è§†è§‰-è¯­è¨€æ¨¡å‹çš„ç¨€æœ‰ç—…ç†è¯†åˆ«', 'desc': 'NOVAæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨ç¨€æœ‰ä¸´åºŠç›¸å…³MRIç—…ç†æ–¹é¢çš„åŸºå‡†ï¼ŒæŒ‘æˆ˜æ¨¡å‹åœ¨åˆ†å¸ƒå¤–å’Œå¼€æ”¾ä¸–ç•Œè¯†åˆ«èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«900ä¸ªè„‘éƒ¨MRIæ‰«æï¼Œæ¶µç›–281ç§ç¨€æœ‰ç—…ç†å’Œä¸åŒçš„è·å–åè®®ã€‚æ¯ä¸ªæ¡ˆä¾‹éƒ½åŒ…æ‹¬ä¸°å¯Œçš„ä¸´åºŠå™è¿°å’ŒåŒç›²ä¸“å®¶çš„è¾¹ç•Œæ¡†æ³¨é‡Šï¼Œæ”¯æŒå¼‚å¸¸å®šä½ã€è§†è§‰æè¿°å’Œè¯Šæ–­æ¨ç†çš„è”åˆè¯„ä¼°ã€‚NOVAä»æœªç”¨äºè®­ç»ƒï¼Œå› æ­¤å®ƒæ˜¯å¯¹æ¨¡å‹åœ¨åˆ†å¸ƒå¤–æ³›åŒ–èƒ½åŠ›çš„æé™å‹åŠ›æµ‹è¯•ï¼Œæ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨å¤„ç†æœªçŸ¥å¼‚å¸¸æ—¶çš„æ€§èƒ½ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21505', 'title': "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective", 'url': 'https://huggingface.co/papers/2505.21505', 'abstract': "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.  \t\t\t\t\tAI-generated summary \t\t\t\t Multilingual Alignment is an effective and representative paradigm to enhance LLMs' multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs' mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons~(including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs' internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of ''Spontaneous Multilingual Alignment''. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.", 'score': 15, 'issue_id': 3994, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'b77701cf90420b4c', 'authors': ['Shimao Zhang', 'Zhejian Lai', 'Xiang Liu', 'Shuaijie She', 'Xiao Liu', 'Yeyun Gong', 'Shujian Huang', 'Jiajun Chen'], 'affiliations': ['Microsoft Research Asia', 'National Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21505.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#alignment'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ñ‚Ğ°Ğ¹Ğ½ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°Ñ… LLM', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾-Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ, Ğ¾Ğ±Ñ‰ĞµĞµ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ, Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸ Ñ†ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM.'}, 'en': {'title': 'Enhancing Multilingual Capabilities in LLMs through Neuron Identification', 'desc': "This research introduces a new algorithm for identifying neurons in large language models (LLMs) that are specific to certain languages as well as those that are language-agnostic. It explores how these neurons contribute to the model's ability to understand and generate text in multiple languages, enhancing multilingual alignment. The study categorizes the internal processes of LLMs into four key areas: understanding multiple languages, reasoning in a shared semantic space, transforming outputs across languages, and managing vocabulary. By analyzing the behavior of different types of neurons, the research provides insights into how LLMs can better support low-resource languages through learned multilingual capabilities."}, 'zh': {'title': 'ç»†ç²’åº¦ç¥ç»å…ƒè¯†åˆ«ï¼Œæå‡å¤šè¯­è¨€èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ›´ç»†ç²’åº¦çš„ç¥ç»å…ƒè¯†åˆ«ç®—æ³•ï¼Œç”¨äºæ£€æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„è¯­è¨€ç‰¹å®šç¥ç»å…ƒå’Œè¯­è¨€æ— å…³ç¥ç»å…ƒã€‚æˆ‘ä»¬åˆ†æäº†å¤šè¯­è¨€ç†è§£ã€å…±äº«è¯­ä¹‰æ¨ç†ã€å¤šè¯­è¨€è¾“å‡ºè½¬æ¢å’Œè¯æ±‡ç©ºé—´è¾“å‡ºç­‰æ–¹é¢å¯¹å¤šè¯­è¨€å¯¹é½å’Œèƒ½åŠ›çš„å½±å“ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå­˜åœ¨åœ¨å¤„ç†ä¸åŒè¯­è¨€æ—¶é€‰æ‹©æ€§æ¿€æ´»çš„è¯­è¨€ç‰¹å®šç¥ç»å…ƒï¼Œè¿™ä¸ºæ·±å…¥ç†è§£LLMsåœ¨å¤šè¯­è¨€åœºæ™¯ä¸­çš„æœºåˆ¶æä¾›äº†æ–°è§†è§’ã€‚é€šè¿‡å¯¹ä¸åŒç±»å‹ç¥ç»å…ƒçš„ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬ä¸ºæ›´å¥½åœ°ç†è§£LLMsçš„å¤šè¯­è¨€å¯¹é½å’Œèƒ½åŠ›æä¾›äº†å®è¯ç»“æœå’Œæœ‰ä»·å€¼çš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20275', 'title': 'ImgEdit: A Unified Image Editing Dataset and Benchmark', 'url': 'https://huggingface.co/papers/2505.20275', 'abstract': 'Recent advancements in generative models have enabled high-fidelity text-to-image generation. However, open-source image-editing models still lag behind their proprietary counterparts, primarily due to limited high-quality data and insufficient benchmarks. To overcome these limitations, we introduce ImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2 million carefully curated edit pairs, which contain both novel and complex single-turn edits, as well as challenging multi-turn tasks. To ensure the data quality, we employ a multi-stage pipeline that integrates a cutting-edge vision-language model, a detection model, a segmentation model, alongside task-specific in-painting procedures and strict post-processing. ImgEdit surpasses existing datasets in both task novelty and data quality. Using ImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to process the reference image and editing prompt, which outperforms existing open-source models on multiple tasks, highlighting the value of ImgEdit and model design. For comprehensive evaluation, we introduce ImgEdit-Bench, a benchmark designed to evaluate image editing performance in terms of instruction adherence, editing quality, and detail preservation. It includes a basic testsuite, a challenging single-turn suite, and a dedicated multi-turn suite. We evaluate both open-source and proprietary models, as well as ImgEdit-E1, providing deep analysis and actionable insights into the current behavior of image-editing models. The source data are publicly available on https://github.com/PKU-YuanGroup/ImgEdit.', 'score': 14, 'issue_id': 3990, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'ab1a7940b7d4c31c', 'authors': ['Yang Ye', 'Xianyi He', 'Zongjian Li', 'Bin Lin', 'Shenghai Yuan', 'Zhiyuan Yan', 'Bohan Hou', 'Li Yuan'], 'affiliations': ['Peking University, Shenzhen Graduate School', 'Peng Cheng Laboratory', 'Rabbitpre AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20275.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#open_source', '#optimization', '#cv', '#data'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'ImgEdit: Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ ImgEdit - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ 1,2 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ImgEdit-E1, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ImgEdit-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ImgEdit-E1 Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Open-Source Image Editing with ImgEdit Dataset', 'desc': 'This paper presents ImgEdit, a new dataset designed to improve open-source image-editing models by providing 1.2 million high-quality image edit pairs. The dataset includes both simple and complex editing tasks, ensuring a wide range of challenges for model training. To maintain high data quality, a multi-stage pipeline is used, incorporating advanced models for vision-language processing, detection, and segmentation. The authors also introduce ImgEdit-E1, an editing model that outperforms existing open-source models, and ImgEdit-Bench, a benchmark for evaluating image editing performance across various tasks.'}, 'zh': {'title': 'ImgEditï¼šé«˜è´¨é‡å›¾åƒç¼–è¾‘çš„çªç ´', 'desc': 'æœ€è¿‘ç”Ÿæˆæ¨¡å‹çš„è¿›å±•ä½¿å¾—é«˜ä¿çœŸæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œå¼€æºå›¾åƒç¼–è¾‘æ¨¡å‹ä»ç„¶è½åäºä¸“æœ‰æ¨¡å‹ï¼Œä¸»è¦æ˜¯ç”±äºé«˜è´¨é‡æ•°æ®çš„ç¼ºä¹å’ŒåŸºå‡†æµ‹è¯•ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ImgEditï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„é«˜è´¨é‡å›¾åƒç¼–è¾‘æ•°æ®é›†ï¼ŒåŒ…å«120ä¸‡ä¸ªç²¾å¿ƒç­–åˆ’çš„ç¼–è¾‘å¯¹ï¼Œæ¶µç›–æ–°é¢–å’Œå¤æ‚çš„å•è½®ç¼–è¾‘ä»¥åŠå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šè½®ä»»åŠ¡ã€‚æˆ‘ä»¬ä½¿ç”¨å¤šé˜¶æ®µæµç¨‹ç¡®ä¿æ•°æ®è´¨é‡ï¼Œæ•´åˆäº†å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€æ£€æµ‹æ¨¡å‹ã€åˆ†å‰²æ¨¡å‹ä»¥åŠç‰¹å®šä»»åŠ¡çš„ä¿®å¤ç¨‹åºå’Œä¸¥æ ¼çš„åå¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20322', 'title': 'Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering\n  Target Atoms', 'url': 'https://huggingface.co/papers/2505.20322', 'abstract': 'Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.', 'score': 13, 'issue_id': 3990, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '63cd6df71ddaefa4', 'authors': ['Mengru Wang', 'Ziwen Xu', 'Shengyu Mao', 'Shumin Deng', 'Zhaopeng Tu', 'Huajun Chen', 'Ningyu Zhang'], 'affiliations': ['NUS-NCS Joint Lab, Singapore', 'National University of Singapore', 'Tencent AI Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20322.jpg', 'data': {'categories': ['#rl', '#reasoning', '#security', '#training', '#alignment'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Steering Target Atoms (STA) Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ STA Ğ² Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾ÑĞ¾Ğ±ÑƒÑ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing Control in Language Models with Steering Target Atoms', 'desc': 'This paper introduces a new method called Steering Target Atoms (STA) to improve the control over language model generation. It addresses the challenge of intertwined internal representations in large models, which can hinder precise steering and lead to unintended consequences. By isolating and manipulating specific knowledge components, STA enhances the safety and reliability of model outputs. The experiments show that this approach is effective, especially in adversarial situations, and it also improves reasoning control in large models.'}, 'zh': {'title': 'æå‡è¯­è¨€æ¨¡å‹å®‰å…¨æ€§çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºå¼•å¯¼ç›®æ ‡åŸå­ï¼ˆSTAï¼‰ï¼Œæ—¨åœ¨æé«˜è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚é€šè¿‡ä½¿ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆ†ç¦»å’Œæ“æ§é«˜ç»´ç©ºé—´ä¸­çš„çŸ¥è¯†ç»„ä»¶ï¼Œä»è€Œå¢å¼ºå¯¹æ¨¡å‹è¡Œä¸ºçš„æ§åˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSTAåœ¨å¯¹æŠ—æ€§åœºæ™¯ä¸­è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§å’Œçµæ´»æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è¿™ä¸€å¼•å¯¼ç­–ç•¥åº”ç”¨äºå¤§å‹æ¨ç†æ¨¡å‹ï¼ŒéªŒè¯äº†å…¶åœ¨ç²¾ç¡®æ¨ç†æ§åˆ¶ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17332', 'title': 'SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for\n  Enterprise Use', 'url': 'https://huggingface.co/papers/2505.17332', 'abstract': "SweEval is a benchmark for evaluating Large Language Models' compliance with ethical guidelines and cultural nuances when instructed to include offensive language.  \t\t\t\t\tAI-generated summary \t\t\t\t Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity.", 'score': 13, 'issue_id': 4006, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'faf1d1381f867cc1', 'authors': ['Hitesh Laxmichand Patel', 'Amit Agarwal', 'Arion Das', 'Bhargava Kumar', 'Srikant Panda', 'Priyaranjan Pattnayak', 'Taki Hasan Rafi', 'Tejaswini Kumar', 'Dong-Kyu Chae'], 'affiliations': ['Columbia University', 'Hanyang University', 'Indian Institute of Information Technology Ranchi', 'Oracle AI', 'TD Securities'], 'pdf_title_img': 'assets/pdf/title_img/2505.17332.jpg', 'data': {'categories': ['#alignment', '#multilingual', '#open_source', '#ethics', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½ĞµÑ†ĞµĞ½Ğ·ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ»ĞµĞºÑĞ¸ĞºĞ¸', 'desc': 'SweEval - ÑÑ‚Ğ¾ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑĞ¾Ğ±Ğ»ÑĞ´Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ½ĞµÑ†ĞµĞ½Ğ·ÑƒÑ€Ğ½Ğ¾Ğ¹ Ğ»ĞµĞºÑĞ¸ĞºĞ¾Ğ¹. ĞĞ½ ÑĞ¸Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ğ½Ğ°Ğ¼Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ¾ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ÑƒĞ³Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚, Ğ½Ğ°ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ LLM ÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ‚Ğ°ĞºĞ¸Ğ¼ Ğ½ĞµÑƒĞ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. SweEval Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.'}, 'en': {'title': 'Evaluating Ethical Compliance in Language Models with SweEval', 'desc': "SweEval is a benchmark designed to evaluate how well Large Language Models (LLMs) adhere to ethical guidelines when faced with prompts that include offensive language. It simulates real-world scenarios by varying the tone and context of the instructions, allowing researchers to assess the models' responses to inappropriate requests. The benchmark focuses on the models' ability to recognize and handle unsafe language while maintaining cultural sensitivity and compliance with ethical standards. By providing a dataset and code, SweEval aims to foster the development of AI systems that are both effective and ethically responsible in enterprise applications."}, 'zh': {'title': 'SweEvalï¼šè¯„ä¼°è¯­è¨€æ¨¡å‹çš„ä¼¦ç†åˆè§„æ€§', 'desc': 'SweEvalæ˜¯ä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å†’çŠ¯æ€§è¯­è¨€æ—¶çš„ä¼¦ç†åˆè§„æ€§å’Œæ–‡åŒ–ç»†å¾®å·®åˆ«ã€‚éšç€ä¼ä¸šå®¢æˆ·è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé‡è¦çš„æ²Ÿé€šä»»åŠ¡ï¼Œç¡®ä¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿç†è§£ä¸åŒçš„æ–‡åŒ–å’Œè¯­è¨€èƒŒæ™¯å˜å¾—è‡³å…³é‡è¦ã€‚è¯¥åŸºå‡†é€šè¿‡æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼Œè¯„ä¼°æ¨¡å‹åœ¨é¢å¯¹ä¸å½“æŒ‡ä»¤æ—¶çš„ååº”ï¼Œä»¥åŠå®ƒä»¬åœ¨ä¼¦ç†æ¡†æ¶å’Œè¯­è¨€ç†è§£èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å‘å¸ƒäº†æ•°æ®é›†å’Œä»£ç ï¼Œä»¥æ¨åŠ¨æ„å»ºç¬¦åˆä¼¦ç†çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21493', 'title': 'Reinforcing General Reasoning without Verifiers', 'url': 'https://huggingface.co/papers/2505.21493', 'abstract': 'The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.', 'score': 11, 'issue_id': 4003, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'f16e69476bc70236', 'authors': ['Xiangxin Zhou', 'Zichen Liu', 'Anya Sims', 'Haonan Wang', 'Tianyu Pang', 'Chongxuan Li', 'Liang Wang', 'Min Lin', 'Chao Du'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'Renmin University of China', 'Sea AI Lab, Singapore', 'University of Chinese Academy of Sciences', 'University of Oxford'], 'pdf_title_img': 'assets/pdf/title_img/2505.21493.jpg', 'data': {'categories': ['#optimization', '#math', '#rl', '#benchmark', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VeriFree: Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM Ğ±ĞµĞ· Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ´Ğ»Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VeriFree. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸-Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğµ. VeriFree Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹, ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ğµ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ Ğ²ĞµÑ€Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MMLU-Pro Ğ¸ GPQA. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ DeepSeek-R1-Zero Ğ½Ğ° Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ…Ğ¸Ğ¼Ğ¸Ñ, Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ğ° Ğ¸ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ°.'}, 'en': {'title': 'VeriFree: Reinventing Reinforcement Learning for Language Models Without Verifiers', 'desc': 'This paper introduces a new method called VeriFree for training large language models (LLMs) without the need for a separate verifier. Traditional reinforcement learning (RL) approaches rely on rule-based verification, which limits their application in complex real-world domains. VeriFree simplifies the process by directly maximizing the likelihood of generating correct answers, eliminating the need for an additional verifier model. The results show that VeriFree not only reduces computational demands but also performs as well or better than existing verifier-based methods across various benchmarks.'}, 'zh': {'title': 'æ— éªŒè¯å™¨çš„å¼ºåŒ–å­¦ä¹ æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œä½¿ç”¨DeepSeek-R1-Zeroé£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£ç å’Œæ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä»…é™äºå¯ä»¥è¿›è¡ŒåŸºäºè§„åˆ™çš„ç­”æ¡ˆéªŒè¯çš„ä»»åŠ¡ï¼Œæ— æ³•è‡ªç„¶æ‰©å±•åˆ°åŒ–å­¦ã€åŒ»ç–—ã€å·¥ç¨‹ã€æ³•å¾‹ã€ç”Ÿç‰©ã€å•†ä¸šå’Œç»æµç­‰ç°å®ä¸–ç•Œé¢†åŸŸã€‚ç›®å‰çš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨é¢å¤–çš„LLMä½œä¸ºæ¨¡å‹éªŒè¯å™¨ï¼Œä½†è¿™ä¼šå¼•å…¥å¯¹å¼ºéªŒè¯å™¨LLMçš„ä¾èµ–ã€å¥–åŠ±é»‘å®¢æ”»å‡»çš„é£é™©ä»¥åŠåœ¨è®­ç»ƒæœŸé—´ç»´æŠ¤éªŒè¯å™¨æ¨¡å‹çš„å®é™…è´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶å°†DeepSeek-R1-Zeroé£æ ¼çš„è®­ç»ƒæ‰©å±•åˆ°ä¸€èˆ¬æ¨ç†é¢†åŸŸï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éªŒè¯å™¨çš„æ–¹æ³•ï¼ˆVeriFreeï¼‰ï¼Œè¯¥æ–¹æ³•ç»•è¿‡ç­”æ¡ˆéªŒè¯ï¼Œç›´æ¥ä½¿ç”¨RLæœ€å¤§åŒ–ç”Ÿæˆå‚è€ƒç­”æ¡ˆçš„æ¦‚ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21491', 'title': 'Frame In-N-Out: Unbounded Controllable Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.21491', 'abstract': 'Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines.', 'score': 11, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'cf3b062e968f421f', 'authors': ['Boyang Wang', 'Xuweiyi Chen', 'Matheus Gadelha', 'Zezhou Cheng'], 'affiliations': ['Adobe Research', 'University of Virginia'], 'pdf_title_img': 'assets/pdf/title_img/2505.21491.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#games', '#diffusion', '#architecture', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€Ğµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸, Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ Frame In Ğ¸ Frame Out, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑĞ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸, Ğ²Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ² ĞºĞ°Ğ´Ñ€ Ğ¸Ğ»Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¸Ğ· Ğ½ĞµĞ³Ğ¾. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ» Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Diffusion Transformer Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Mastering Video Generation with Motion Control', 'desc': 'This paper addresses key challenges in video generation, specifically focusing on controllability, temporal coherence, and detail synthesis. It introduces a novel technique called Frame In and Frame Out, allowing users to manipulate objects in a video scene based on specified motion trajectories. The authors present a new dataset and evaluation protocol tailored for this task, along with a Diffusion Transformer architecture that preserves identity while enabling motion control. Results demonstrate that their method significantly improves upon existing video generation models.'}, 'zh': {'title': 'æå‡è§†é¢‘ç”Ÿæˆçš„å¯æ§æ€§ä¸ä¸€è‡´æ€§', 'desc': 'æœ¬è®ºæ–‡å…³æ³¨è§†é¢‘ç”Ÿæˆä¸­çš„å¯æ§æ€§ã€æ—¶é—´ä¸€è‡´æ€§å’Œç»†èŠ‚åˆæˆç­‰å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºâ€œå¸§è¿›å¸§å‡ºâ€çš„ç”µå½±æŠ€æœ¯ï¼Œå…è®¸ç”¨æˆ·æ§åˆ¶å›¾åƒä¸­çš„å¯¹è±¡è‡ªç„¶åœ°ç¦»å¼€æˆ–è¿›å…¥åœºæ™¯ã€‚ä¸ºæ”¯æŒè¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŠè‡ªåŠ¨ç­–åˆ’çš„æ–°æ•°æ®é›†ï¼Œå¹¶åˆ¶å®šäº†é’ˆå¯¹è¯¥è®¾ç½®çš„ç»¼åˆè¯„ä¼°åè®®ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21457', 'title': 'Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO', 'url': 'https://huggingface.co/papers/2505.21457', 'abstract': "Active vision, also known as active perception, refers to the process of actively selecting where and how to look in order to gather task-relevant information. It is a critical component of efficient perception and decision-making in humans and advanced embodied agents. Recently, the use of Multimodal Large Language Models (MLLMs) as central planning and decision-making modules in robotic systems has gained extensive attention. However, despite the importance of active perception in embodied intelligence, there is little to no exploration of how MLLMs can be equipped with or learn active perception capabilities. In this paper, we first provide a systematic definition of MLLM-based active perception tasks. We point out that the recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a special case of active perception; however, it still suffers from low search efficiency and inaccurate region selection. To address these issues, we propose ACTIVE-O3, a purely reinforcement learning based training framework built on top of GRPO, designed to equip MLLMs with active perception capabilities. We further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across both general open-world tasks, such as small-object and dense object grounding, and domain-specific scenarios, including small object detection in remote sensing and autonomous driving, as well as fine-grained interactive segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot reasoning abilities on the V* Benchmark, without relying on any explicit reasoning data. We hope that our work can provide a simple codebase and evaluation protocol to facilitate future research on active perception in MLLMs.", 'score': 11, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '6d338aff50466f43', 'authors': ['Muzhi Zhu', 'Hao Zhong', 'Canyu Zhao', 'Zongze Du', 'Zheng Huang', 'Mingyu Liu', 'Hao Chen', 'Cheng Zou', 'Jingdong Chen', 'Ming Yang', 'Chunhua Shen'], 'affiliations': ['Ant Group, China', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21457.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': 'ğŸ‘ï¸', 'ru': {'title': 'ACTIVE-O3: ĞĞ°Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ MLLM Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ACTIVE-O3 - Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ Ğ´Ğ»Ñ MLLM Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ACTIVE-O3 Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ACTIVE-O3 Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ñ‚ĞµÑÑ‚Ğµ V*.'}, 'en': {'title': 'Empowering MLLMs with Active Perception for Smarter Decision-Making', 'desc': 'This paper introduces ACTIVE-O3, a reinforcement learning framework designed to enhance Multimodal Large Language Models (MLLMs) with active perception capabilities. Active perception involves strategically selecting where to focus attention to gather relevant information, which is crucial for effective decision-making in robotics. The authors highlight the limitations of the existing GPT-o3 model in terms of search efficiency and region selection accuracy. By establishing a benchmark suite for evaluating ACTIVE-O3, the paper aims to advance research in active perception for MLLMs, demonstrating strong performance in various tasks without needing explicit reasoning data.'}, 'zh': {'title': 'æå‡æœºå™¨äººä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›çš„åˆ›æ–°æ¡†æ¶', 'desc': 'ä¸»åŠ¨è§†è§‰ï¼Œä¹Ÿç§°ä¸ºä¸»åŠ¨æ„ŸçŸ¥ï¼Œæ˜¯æŒ‡ä¸»åŠ¨é€‰æ‹©è§‚å¯Ÿçš„æ–¹å¼å’Œä½ç½®ï¼Œä»¥è·å–ä¸ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ã€‚æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æœºå™¨äººç³»ç»Ÿä¸­çš„ä¸»åŠ¨æ„ŸçŸ¥èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒæ¡†æ¶ACTIVE-O3ã€‚æˆ‘ä»¬å®šä¹‰äº†MLLMsçš„ä¸»åŠ¨æ„ŸçŸ¥ä»»åŠ¡ï¼Œå¹¶æŒ‡å‡ºç°æœ‰æ¨¡å‹åœ¨æœç´¢æ•ˆç‡å’ŒåŒºåŸŸé€‰æ‹©ä¸Šå­˜åœ¨ä¸è¶³ã€‚é€šè¿‡å»ºç«‹ç»¼åˆåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒACTIVE-O3åœ¨å¤šä¸ªä»»åŠ¡ä¸­å±•ç¤ºäº†å¼ºå¤§çš„é›¶-shotæ¨ç†èƒ½åŠ›ï¼Œæ¨åŠ¨äº†ä¸»åŠ¨æ„ŸçŸ¥çš„ç ”ç©¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16901', 'title': 'Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks', 'url': 'https://huggingface.co/papers/2505.16901', 'abstract': "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", 'score': 11, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'a91f0c74f1c3c191', 'authors': ['Hongyuan Tao', 'Ying Zhang', 'Zhenhao Tang', 'Hongen Peng', 'Xukun Zhu', 'Bingchang Liu', 'Yingguang Yang', 'Ziyin Zhang', 'Zhaogui Xu', 'Haipeng Zhang', 'Linchao Zhu', 'Rui Wang', 'Hang Yu', 'Jianguo Li', 'Peng Di'], 'affiliations': ['Ant Group, Hangzhou, China', 'Shanghai Jiaotong University, Shanghai, China', 'ShanghaiTech University, Shanghai, China', 'Zhejiang University, Hangzhou, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.16901.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#games', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“Ñ€Ğ°Ñ„Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ´Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Code Graph Models (CGM) - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ´Ğ° Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ. CGM Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ĞºĞ¾Ğ´Ğ° Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². Ğ’ ÑĞ¾Ñ‡ĞµÑ‚Ğ°Ğ½Ğ¸Ğ¸ Ñ Ğ±ĞµĞ·Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ¾Ğ¼ graph RAG, Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ SWE-bench Lite, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Empowering Code Generation with Open-Source Graph Models', 'desc': 'This paper presents a novel approach to enhance repository-level code generation tasks using open-source Code Graph Models (CGMs). By integrating code graph structures into the attention mechanisms of Large Language Models (LLMs), the authors demonstrate that these models can effectively understand the relationships and dependencies within codebases. This method eliminates the need for agent-based solutions, which often compromise data privacy and customization. The results show a significant improvement in performance, achieving a top ranking among open-source models on the SWE-bench Lite benchmark.'}, 'zh': {'title': 'å¼€æºä»£ç å›¾æ¨¡å‹æå‡ä»£ç ç”Ÿæˆæ€§èƒ½', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§å¼€æºä»£ç å›¾æ¨¡å‹ï¼ˆCode Graph Models, CGMsï¼‰ï¼Œæ—¨åœ¨æå‡ä»£ç ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚é€šè¿‡å°†ä»£ç å›¾ç»“æ„æ•´åˆåˆ°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼ŒCGMsèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä»£ç åº“ä¸­çš„å‡½æ•°å’Œæ–‡ä»¶ã€‚ä¸ä¾èµ–ä»£ç†çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä»£ç†ï¼Œç¡®ä¿äº†æ•°æ®éšç§å’Œæ¨¡å‹å®šåˆ¶çš„çµæ´»æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å¼€æºQwen2.5-72Bæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨SWE-bench LiteåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†43.00%çš„è§£å†³ç‡ï¼Œè¡¨ç°ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21500', 'title': 'ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in\n  Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.21500', 'abstract': "Vision-language models (VLMs) have demonstrated remarkable capabilities in understanding and reasoning about visual content, but significant challenges persist in tasks requiring cross-viewpoint understanding and spatial reasoning. We identify a critical limitation: current VLMs excel primarily at egocentric spatial reasoning (from the camera's perspective) but fail to generalize to allocentric viewpoints when required to adopt another entity's spatial frame of reference. We introduce ViewSpatial-Bench, the first comprehensive benchmark designed specifically for multi-viewpoint spatial localization recognition evaluation across five distinct task types, supported by an automated 3D annotation pipeline that generates precise directional labels. Comprehensive evaluation of diverse VLMs on ViewSpatial-Bench reveals a significant performance disparity: models demonstrate reasonable performance on camera-perspective tasks but exhibit reduced accuracy when reasoning from a human viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset, we achieve an overall performance improvement of 46.24% across tasks, highlighting the efficacy of our approach. Our work establishes a crucial benchmark for spatial intelligence in embodied AI systems and provides empirical evidence that modeling 3D spatial relationships enhances VLMs' corresponding spatial comprehension capabilities.", 'score': 10, 'issue_id': 3993, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'c8cec2407a7def0e', 'authors': ['Dingming Li', 'Hongxing Li', 'Zixuan Wang', 'Yuchen Yan', 'Hang Zhang', 'Siqi Chen', 'Guiyang Hou', 'Shengpei Jiang', 'Wenqi Zhang', 'Yongliang Shen', 'Weiming Lu', 'Yueting Zhuang'], 'affiliations': ['The Chinese University of Hong Kong', 'University of Electronic Science and Technology of China', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21500.jpg', 'data': {'categories': ['#benchmark', '#cv', '#reasoning', '#3d', '#games'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ViewSpatial-Bench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº Ğ·Ñ€ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ vision-language Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ½Ğ¾ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°Ğ»Ğ»Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ğ¼Ğ¸. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿ÑÑ‚ÑŒ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ 3D-ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹. Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ»Ğ° ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° 46.24%.'}, 'en': {'title': 'Enhancing VLMs with Multi-Viewpoint Spatial Reasoning', 'desc': "This paper addresses the limitations of vision-language models (VLMs) in understanding spatial relationships from different viewpoints. It highlights that while VLMs perform well in egocentric spatial reasoning, they struggle with allocentric perspectives, which are essential for tasks requiring understanding from another entity's viewpoint. The authors introduce ViewSpatial-Bench, a new benchmark for evaluating multi-viewpoint spatial localization, along with a 3D annotation pipeline for accurate directional labeling. By fine-tuning VLMs on this dataset, they demonstrate a significant performance improvement, emphasizing the importance of 3D spatial modeling in enhancing VLMs' spatial reasoning capabilities."}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç†è§£å’Œæ¨ç†è§†è§‰å†…å®¹æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨éœ€è¦è·¨è§†è§’ç†è§£å’Œç©ºé—´æ¨ç†çš„ä»»åŠ¡ä¸­ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„VLMsä¸»è¦åœ¨è‡ªæˆ‘ä¸­å¿ƒçš„ç©ºé—´æ¨ç†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦é‡‡ç”¨å…¶ä»–å®ä½“çš„ç©ºé—´å‚è€ƒæ¡†æ¶æ—¶ï¼Œæ— æ³•å¾ˆå¥½åœ°æ¨å¹¿åˆ°ä»–å¿ƒä¸­å¿ƒè§†è§’ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ViewSpatial-Benchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå¤šè§†è§’ç©ºé—´å®šä½è¯†åˆ«è¯„ä¼°è®¾è®¡çš„ç»¼åˆåŸºå‡†ï¼Œæ”¯æŒè‡ªåŠ¨åŒ–çš„3Dæ ‡æ³¨æµç¨‹ç”Ÿæˆç²¾ç¡®çš„æ–¹å‘æ ‡ç­¾ã€‚é€šè¿‡åœ¨æˆ‘ä»¬çš„å¤šè§†è§’ç©ºé—´æ•°æ®é›†ä¸Šå¾®è°ƒVLMsï¼Œæˆ‘ä»¬åœ¨å„é¡¹ä»»åŠ¡ä¸Šå®ç°äº†46.24%çš„æ•´ä½“æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21473', 'title': 'DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via\n  Next-Detail Prediction', 'url': 'https://huggingface.co/papers/2505.21473', 'abstract': "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods.", 'score': 9, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'd4df44fe3325795e', 'authors': ['Yiheng Liu', 'Liao Qu', 'Huichao Zhang', 'Xu Wang', 'Yi Jiang', 'Yiming Gao', 'Hu Ye', 'Xian Li', 'Shuai Wang', 'Daniel K. Du', 'Shu Cheng', 'Zehuan Yuan', 'Xinglong Wu'], 'affiliations': ['ByteDance Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.21473.jpg', 'data': {'categories': ['#benchmark', '#cv', '#training', '#architecture', '#diffusion'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğº Ñ‡Ğ°ÑÑ‚Ğ½Ğ¾Ğ¼Ñƒ', 'desc': 'DetailFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ñ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑƒÑ…ÑƒĞ´ÑˆĞ°ÑÑ‰Ğ¸Ğ¼ÑÑ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°Ñ‚ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ñ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ñ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ² 8 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'DetailFlow: Efficient 1D Image Generation with Coarse-to-Fine Refinement', 'desc': 'This paper introduces DetailFlow, a new method for generating images using a 1D autoregressive approach. It employs a next-detail prediction strategy that allows the model to start with a broad image structure and gradually add finer details. By using a resolution-aware token sequence and a parallel inference mechanism, DetailFlow significantly improves generation speed and reduces the number of tokens needed for high-quality image synthesis. The results show that DetailFlow outperforms existing models in both image quality and efficiency, achieving better performance with fewer resources.'}, 'zh': {'title': 'DetailFlowï¼šé«˜æ•ˆçš„è‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDetailFlowçš„ç²—åˆ°ç»†çš„1Dè‡ªå›å½’å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ä¸‹ä¸€ä¸ªç»†èŠ‚é¢„æµ‹ç­–ç•¥æ¥å»ºæ¨¡å›¾åƒã€‚é€šè¿‡å­¦ä¹ ä¸€ä¸ªåˆ†è¾¨ç‡æ„ŸçŸ¥çš„æ ‡è®°åºåˆ—ï¼Œå¹¶ä½¿ç”¨é€æ­¥é™çº§çš„å›¾åƒè¿›è¡Œç›‘ç£ï¼ŒDetailFlowä½¿ç”Ÿæˆè¿‡ç¨‹èƒ½å¤Ÿä»å…¨å±€ç»“æ„å¼€å§‹ï¼Œé€æ­¥ç»†åŒ–ç»†èŠ‚ã€‚è¯¥ç²—åˆ°ç»†çš„1Dæ ‡è®°åºåˆ—ä¸è‡ªå›å½’æ¨ç†æœºåˆ¶å¾ˆå¥½åœ°å¯¹é½ï¼Œä¸ºè‡ªå›å½’æ¨¡å‹ç”Ÿæˆå¤æ‚è§†è§‰å†…å®¹æä¾›äº†ä¸€ç§æ›´è‡ªç„¶å’Œé«˜æ•ˆçš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDetailFlowåœ¨å›¾åƒåˆæˆè´¨é‡å’Œæ•ˆç‡ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19099', 'title': 'SeePhys: Does Seeing Help Thinking? -- Benchmarking Vision-Based Physics\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.19099', 'abstract': "SeePhys, a multimodal benchmark, highlights challenges in LLMs' visual reasoning and physics-grounded problem-solving capabilities, especially in interpreting diagrams and reducing reliance on textual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t We present SeePhys, a large-scale multimodal benchmark for LLM reasoning grounded in physics questions ranging from middle school to PhD qualifying exams. The benchmark covers 7 fundamental domains spanning the physics discipline, incorporating 21 categories of highly heterogeneous diagrams. In contrast to prior works where visual elements mainly serve auxiliary purposes, our benchmark features a substantial proportion of vision-essential problems (75\\%) that mandate visual information extraction for correct solutions. Through extensive evaluation, we observe that even the most advanced visual reasoning models (e.g., Gemini-2.5-pro and o4-mini) achieve sub-60\\% accuracy on our benchmark. These results reveal fundamental challenges in current large language models' visual understanding capabilities, particularly in: (i) establishing rigorous coupling between diagram interpretation and physics reasoning, and (ii) overcoming their persistent reliance on textual cues as cognitive shortcuts.", 'score': 8, 'issue_id': 3991, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'e7a5589d299cea0e', 'authors': ['Kun Xiang', 'Heng Li', 'Terry Jingchen Zhang', 'Yinya Huang', 'Zirong Liu', 'Peixin Qu', 'Jixi He', 'Jiaqi Chen', 'Yu-Jie Yuan', 'Jianhua Han', 'Hang Xu', 'Hanhui Li', 'Mrinmaya Sachan', 'Xiaodan Liang'], 'affiliations': ['ETH Zurich', 'Huawei Noahs Ark Lab', 'Sun Yat-sen University', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#benchmark', '#cv', '#multimodal'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'SeePhys: Ğ²Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ LLM Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ', 'desc': 'SeePhys - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ Ñ„Ğ¸Ğ·Ğ¸ĞºĞµ. ĞĞ½ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 7 Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¾Ğ² Ñ„Ğ¸Ğ·Ğ¸ĞºĞ¸ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 21 ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ 75% Ğ·Ğ°Ğ´Ğ°Ñ‡, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ñ… Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ ÑĞ°Ğ¼Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ½ĞµĞµ 60% Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ.'}, 'en': {'title': 'SeePhys: Bridging Visual Reasoning and Physics in LLMs', 'desc': 'SeePhys is a new benchmark designed to test how well large language models (LLMs) can solve physics problems that involve visual reasoning. It includes a wide range of questions, from middle school to PhD level, and features many different types of diagrams that are crucial for finding the right answers. Unlike previous benchmarks, SeePhys requires models to extract visual information for 75% of the problems, making it essential for them to interpret diagrams accurately. The results show that even the best models struggle with these tasks, highlighting significant gaps in their ability to connect visual data with physics reasoning without relying heavily on text.'}, 'zh': {'title': 'SeePhysï¼šæŒ‘æˆ˜è§†è§‰æ¨ç†ä¸ç‰©ç†é—®é¢˜è§£å†³çš„åŸºå‡†', 'desc': 'SeePhysæ˜¯ä¸€ä¸ªå¤§å‹çš„å¤šæ¨¡æ€åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç‰©ç†é—®é¢˜ä¸Šçš„æ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–ä»ä¸­å­¦åˆ°åšå£«èµ„æ ¼è€ƒè¯•çš„èŒƒå›´ã€‚è¯¥åŸºå‡†æ¶‰åŠç‰©ç†å­¦çš„7ä¸ªåŸºæœ¬é¢†åŸŸï¼Œå¹¶åŒ…å«21ç±»é«˜åº¦å¼‚è´¨çš„å›¾è¡¨ã€‚ä¸ä»¥å¾€çš„ç ”ç©¶ä¸åŒï¼ŒSeePhysä¸­75%çš„é—®é¢˜éœ€è¦æå–è§†è§‰ä¿¡æ¯æ‰èƒ½å¾—å‡ºæ­£ç¡®ç­”æ¡ˆï¼Œæ˜¾ç¤ºå‡ºè§†è§‰ä¿¡æ¯åœ¨è§£å†³é—®é¢˜ä¸­çš„é‡è¦æ€§ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è§†è§‰æ¨ç†æ¨¡å‹ï¼Œå…¶å‡†ç¡®ç‡ä¹Ÿæœªèƒ½è¶…è¿‡60%ï¼Œæ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§†è§‰ç†è§£æ–¹é¢çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20287', 'title': 'MotionPro: A Precise Motion Controller for Image-to-Video Generation', 'url': 'https://huggingface.co/papers/2505.20287', 'abstract': 'MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.  \t\t\t\t\tAI-generated summary \t\t\t\t Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically rely on large Gaussian kernels to extend motion trajectories as condition without explicitly defining movement region, leading to coarse motion control and failing to disentangle object and camera moving. To alleviate these, we present MotionPro, a precise motion controller that novelly leverages region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify target motion category (i.e., object or camera moving), respectively. Technically, MotionPro first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories to simulate inference scenario. Instead of extending flow through large Gaussian kernels, our region-wise trajectory approach enables more precise control by directly utilizing trajectories within local regions, thereby effectively characterizing fine-grained movements. A motion mask is simultaneously derived from the predicted flow maps to capture the holistic motion dynamics of the movement regions. To pursue natural motion control, MotionPro further strengthens video denoising by incorporating both region-wise trajectories and motion mask through feature modulation. More remarkably, we meticulously construct a benchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level I2V motion control. Extensive experiments conducted on WebVid-10M and MC-Bench demonstrate the effectiveness of MotionPro. Please refer to our project page for more results: https://zhw-zhang.github.io/MotionPro-page/.', 'score': 7, 'issue_id': 4004, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '032f4425f4fd7152', 'authors': ['Zhongwei Zhang', 'Fuchen Long', 'Zhaofan Qiu', 'Yingwei Pan', 'Wu Liu', 'Ting Yao', 'Tao Mei'], 'affiliations': ['HiDream.ai Inc.', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.20287.jpg', 'data': {'categories': ['#benchmark', '#video', '#games', '#optimization'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹', 'desc': 'MotionPro - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ¸ Ğ¼Ğ°ÑĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ½ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ ĞºĞ°Ğ¼ĞµÑ€Ñ‹. MotionPro Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ¼ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ³Ğ°ÑƒÑÑĞ¾Ğ²Ñ‹Ñ… ÑĞ´ĞµÑ€. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ¾Ğ±Ñ‰ĞµĞ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ´Ğ²Ğ¸Ğ¶ÑƒÑ‰Ğ¸Ñ…ÑÑ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Precision in Motion Control for Image-to-Video Generation', 'desc': 'MotionPro is a novel approach for image-to-video generation that enhances motion control by using region-wise trajectories and motion masks. This method allows for precise regulation of motion synthesis, effectively distinguishing between object and camera movements. Instead of relying on large Gaussian kernels, MotionPro utilizes localized trajectories to achieve fine-grained control over motion dynamics. Additionally, it incorporates a motion mask to improve video denoising and has been evaluated using a newly created benchmark dataset, MC-Bench, demonstrating its effectiveness in motion control tasks.'}, 'zh': {'title': 'ç²¾ç¡®è¿åŠ¨æ§åˆ¶ï¼Œæå‡å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆ', 'desc': 'MotionPro æ˜¯ä¸€ç§ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶å™¨ï¼Œåˆ©ç”¨åŒºåŸŸè½¨è¿¹å’Œè¿åŠ¨æ©ç æ¥æ”¹å–„å›¾åƒåˆ°è§†é¢‘çš„ç”Ÿæˆã€‚å®ƒé€šè¿‡ä¼°è®¡æ¯ä¸ªè®­ç»ƒè§†é¢‘çš„æµåŠ¨å›¾ï¼Œå¹¶ä»ä¸­é‡‡æ ·åŒºåŸŸè½¨è¿¹ï¼Œæ¥å®ç°ç»†ç²’åº¦çš„è¿åŠ¨åˆæˆã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒMotionPro ç›´æ¥ä½¿ç”¨å±€éƒ¨åŒºåŸŸå†…çš„è½¨è¿¹ï¼Œä»è€Œå®ç°æ›´ç²¾ç¡®çš„è¿åŠ¨æ§åˆ¶ã€‚è¯¥æ–¹æ³•è¿˜é€šè¿‡ç‰¹å¾è°ƒåˆ¶å¢å¼ºè§†é¢‘å»å™ªï¼Œæ„å»ºäº†ä¸€ä¸ªåŸºå‡†æ•°æ®é›† MC-Benchï¼Œç”¨äºè¯„ä¼°ç»†ç²’åº¦å’Œå¯¹è±¡çº§çš„è¿åŠ¨æ§åˆ¶æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21494', 'title': 'Adversarial Attacks against Closed-Source MLLMs via Feature Optimal\n  Alignment', 'url': 'https://huggingface.co/papers/2505.21494', 'abstract': "Multimodal large language models (MLLMs) remain vulnerable to transferable adversarial examples. While existing methods typically achieve targeted attacks by aligning global features-such as CLIP's [CLS] token-between adversarial and target samples, they often overlook the rich local information encoded in patch tokens. This leads to suboptimal alignment and limited transferability, particularly for closed-source models. To address this limitation, we propose a targeted transferable adversarial attack method based on feature optimal alignment, called FOA-Attack, to improve adversarial transfer capability. Specifically, at the global level, we introduce a global feature loss based on cosine similarity to align the coarse-grained features of adversarial samples with those of target samples. At the local level, given the rich local representations within Transformers, we leverage clustering techniques to extract compact local patterns to alleviate redundant local features. We then formulate local feature alignment between adversarial and target samples as an optimal transport (OT) problem and propose a local clustering optimal transport loss to refine fine-grained feature alignment. Additionally, we propose a dynamic ensemble model weighting strategy to adaptively balance the influence of multiple models during adversarial example generation, thereby further improving transferability. Extensive experiments across various models demonstrate the superiority of the proposed method, outperforming state-of-the-art methods, especially in transferring to closed-source MLLMs. The code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack.", 'score': 6, 'issue_id': 3992, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '9308405d203b4ba9', 'authors': ['Xiaojun Jia', 'Sensen Gao', 'Simeng Qin', 'Tianyu Pang', 'Chao Du', 'Yihao Huang', 'Xinfeng Li', 'Yiming Li', 'Bo Li', 'Yang Liu'], 'affiliations': ['MBZUAI, United Arab Emirates', 'Nanyang Technological University, Singapore', 'Sea AI Lab, Singapore', 'University of Illinois Urbana-Champaign, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.21494.jpg', 'data': {'categories': ['#multimodal', '#security', '#training'], 'emoji': 'ğŸ¯', 'ru': {'title': 'Ğ£ÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°Ñ‚Ğ°ĞºĞ° Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ FOA-Attack. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ½Ğ° Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾ÑĞ¸Ğ½ÑƒÑĞ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ°. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½ÑĞ°Ğ¼Ğ±Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Adversarial Transferability with FOA-Attack', 'desc': 'This paper addresses the vulnerability of multimodal large language models (MLLMs) to transferable adversarial examples. The authors introduce a new attack method called FOA-Attack, which focuses on aligning both global and local features to enhance the transferability of adversarial samples. By utilizing cosine similarity for global feature alignment and optimal transport for local feature alignment, the method improves the effectiveness of attacks on closed-source models. The proposed approach outperforms existing methods in various experiments, demonstrating its robustness in generating transferable adversarial examples.'}, 'zh': {'title': 'æå‡å¯¹æŠ—æ ·æœ¬è½¬ç§»èƒ½åŠ›çš„FOA-Attackæ–¹æ³•', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å®¹æ˜“å—åˆ°å¯è½¬ç§»çš„å¯¹æŠ—æ ·æœ¬æ”»å‡»ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é€šè¿‡å¯¹é½å…¨å±€ç‰¹å¾æ¥å®ç°ç›®æ ‡æ”»å‡»ï¼Œä½†å¿½è§†äº†å±€éƒ¨ä¿¡æ¯çš„ä¸°å¯Œæ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç‰¹å¾æœ€ä¼˜å¯¹é½çš„é’ˆå¯¹æ€§å¯è½¬ç§»å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼Œç§°ä¸ºFOA-Attackã€‚é€šè¿‡å¼•å…¥å…¨å±€ç‰¹å¾æŸå¤±å’Œå±€éƒ¨èšç±»æœ€ä¼˜ä¼ è¾“æŸå¤±ï¼Œæˆ‘ä»¬æ˜¾è‘—æé«˜äº†å¯¹æŠ—æ ·æœ¬çš„è½¬ç§»èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20793', 'title': 'Rendering-Aware Reinforcement Learning for Vector Graphics Generation', 'url': 'https://huggingface.co/papers/2505.20793', 'abstract': 'RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.  \t\t\t\t\tAI-generated summary \t\t\t\t Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF(Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.', 'score': 6, 'issue_id': 4004, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'bba0f264ad66c8c3', 'authors': ['Juan A. Rodriguez', 'Haotian Zhang', 'Abhay Puri', 'Aarash Feizi', 'Rishav Pramanik', 'Pascal Wichmann', 'Arnab Mondal', 'Mohammad Reza Samsami', 'Rabiul Awal', 'Perouz Taslakian', 'Spandana Gella', 'Sai Rajeswar', 'David Vazquez', 'Christopher Pal', 'Marco Pedersoli'], 'affiliations': ['Apple', 'Canada CIFAR AI Chair', 'Columbia University', 'Google Research', 'Independent Scholar', 'McGill University', 'Mila', 'Polytechnique MontrÃ©al', 'ServiceNow Research', 'Stony Brook University', 'Ã‰TS MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2505.20793.jpg', 'data': {'categories': ['#cv', '#rag', '#transfer_learning', '#rl', '#games'], 'emoji': 'ğŸ¨', 'ru': {'title': 'RLRF: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SVG Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ Ğ¾Ñ‚ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ°', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ RLRF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ SVG Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° (VLM). ĞĞ½ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¾Ñ‚Ñ€ĞµĞ½Ğ´ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ SVG Ñ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ SVG. RLRF Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ñ€ĞµÑˆĞ°Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ SVG.'}, 'en': {'title': 'Enhancing SVG Generation with Reinforcement Learning from Rendering Feedback', 'desc': "The paper presents RLRF, a novel reinforcement learning method that improves the generation of Scalable Vector Graphics (SVG) using rendering feedback. By comparing rendered SVG outputs to original images, RLRF provides evaluative feedback that enhances the model's ability to produce accurate and efficient SVGs. This approach leverages the strengths of vision-language models (VLMs) to capture both global semantics and detailed visual patterns, addressing limitations of previous methods that lacked direct rendering feedback. Ultimately, RLRF demonstrates superior performance over traditional supervised fine-tuning, leading to high-quality SVG generation with better structural understanding and generalization capabilities."}, 'zh': {'title': 'åˆ©ç”¨æ¸²æŸ“åé¦ˆæå‡SVGç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•', 'desc': 'RLRFæ˜¯ä¸€ç§åˆ©ç”¨æ¸²æŸ“åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç”Ÿæˆå¯ç¼©æ”¾çŸ¢é‡å›¾å½¢ï¼ˆSVGï¼‰çš„èƒ½åŠ›ã€‚é€šè¿‡å°†ç”Ÿæˆçš„SVGä¸åŸå§‹å›¾åƒè¿›è¡Œæ¯”è¾ƒï¼ŒRLRFèƒ½å¤Ÿæä¾›è§†è§‰ä¿çœŸåº¦åé¦ˆï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®å’Œé«˜æ•ˆçš„SVGã€‚è¯¥æ–¹æ³•è§£å†³äº†ç°æœ‰VLMåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§‚å¯Ÿæ¸²æŸ“å›¾åƒçš„é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†SVGç”Ÿæˆçš„è´¨é‡å’Œç»“æ„ç†è§£èƒ½åŠ›ã€‚RLRFåœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒæ–¹æ³•ï¼Œå±•ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17613', 'title': 'MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation', 'url': 'https://huggingface.co/papers/2505.17613', 'abstract': 'MMMG is a comprehensive benchmark for multimodal generation, offering 49 tasks and 937 instructions to align automatic evaluation with human judgment, revealing areas for improvement in reasoning and audio generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Automatically evaluating multimodal generation presents a significant challenge, as automated metrics often struggle to align reliably with human evaluation, especially for complex tasks that involve multiple modalities. To address this, we present MMMG, a comprehensive and human-aligned benchmark for multimodal generation across 4 modality combinations (image, audio, interleaved text and image, interleaved text and audio), with a focus on tasks that present significant challenges for generation models, while still enabling reliable automatic evaluation through a combination of models and programs. MMMG encompasses 49 tasks (including 29 newly developed ones), each with a carefully designed evaluation pipeline, and 937 instructions to systematically assess reasoning, controllability, and other key capabilities of multimodal generation models. Extensive validation demonstrates that MMMG is highly aligned with human evaluation, achieving an average agreement of 94.3%. Benchmarking results on 24 multimodal generation models reveal that even though the state-of-the-art model, GPT Image, achieves 78.3% accuracy for image generation, it falls short on multimodal reasoning and interleaved generation. Furthermore, results suggest considerable headroom for improvement in audio generation, highlighting an important direction for future research.', 'score': 6, 'issue_id': 4006, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': 'd49545e0846c379a', 'authors': ['Jihan Yao', 'Yushi Hu', 'Yujie Yi', 'Bin Han', 'Shangbin Feng', 'Guang Yang', 'Bingbing Wen', 'Ranjay Krishna', 'Lucy Lu Wang', 'Yulia Tsvetkov', 'Noah A. Smith', 'Banghua Zhu'], 'affiliations': ['Allen Institute for AI', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.17613.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#survey', '#multimodal'], 'emoji': 'ğŸ­', 'ru': {'title': 'MMMG: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'MMMG - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 49 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ 937 Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞĞ½ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ° ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ĞµĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ°ÑƒĞ´Ğ¸Ğ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‡ĞµÑ€ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…ÑÑ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 24 Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ğ¸Ğ· Ğ½Ğ¸Ñ… Ğ¸Ğ¼ĞµÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑÑ„ĞµÑ€Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'MMMG: Bridging the Gap Between Human and Machine Evaluation in Multimodal Generation', 'desc': 'MMMG is a new benchmark designed to evaluate multimodal generation tasks, which involve combining different types of data like images, audio, and text. It includes 49 tasks and 937 instructions that help ensure automatic evaluations match human judgments, especially in complex scenarios. The benchmark focuses on assessing key capabilities such as reasoning and controllability in generation models. Results show that while some models perform well in specific areas, there is still significant room for improvement, particularly in audio generation.'}, 'zh': {'title': 'å¤šæ¨¡æ€ç”Ÿæˆçš„å…¨é¢åŸºå‡†ä¸è¯„ä¼°', 'desc': 'MMMGæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç”ŸæˆåŸºå‡†ï¼Œæä¾›49ä¸ªä»»åŠ¡å’Œ937æ¡æŒ‡ä»¤ï¼Œæ—¨åœ¨å°†è‡ªåŠ¨è¯„ä¼°ä¸äººç±»åˆ¤æ–­å¯¹é½ï¼Œæ­ç¤ºæ¨ç†å’ŒéŸ³é¢‘ç”Ÿæˆçš„æ”¹è¿›ç©ºé—´ã€‚è¯¥åŸºå‡†æ¶µç›–å››ç§æ¨¡æ€ç»„åˆï¼ˆå›¾åƒã€éŸ³é¢‘ã€äº¤é”™æ–‡æœ¬å’Œå›¾åƒã€äº¤é”™æ–‡æœ¬å’ŒéŸ³é¢‘ï¼‰ï¼Œä¸“æ³¨äºå¯¹ç”Ÿæˆæ¨¡å‹æå‡ºé‡å¤§æŒ‘æˆ˜çš„ä»»åŠ¡ï¼ŒåŒæ—¶é€šè¿‡æ¨¡å‹å’Œç¨‹åºçš„ç»“åˆå®ç°å¯é çš„è‡ªåŠ¨è¯„ä¼°ã€‚MMMGçš„è¯„ä¼°ç®¡é“ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œèƒ½å¤Ÿç³»ç»Ÿåœ°è¯„ä¼°å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„æ¨ç†ã€å¯æ§æ€§ç­‰å…³é”®èƒ½åŠ›ã€‚å¹¿æ³›çš„éªŒè¯è¡¨æ˜ï¼ŒMMMGä¸äººç±»è¯„ä¼°é«˜åº¦ä¸€è‡´ï¼Œå¹³å‡ä¸€è‡´æ€§è¾¾åˆ°94.3%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21178', 'title': 'Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.21178', 'abstract': 'As test-time scaling becomes a pivotal research frontier in Large Language Models (LLMs) development, contemporary and advanced post-training methodologies increasingly focus on extending the generation length of long Chain-of-Thought (CoT) responses to enhance reasoning capabilities toward DeepSeek R1-like performance. However, recent studies reveal a persistent overthinking phenomenon in state-of-the-art reasoning models, manifesting as excessive redundancy or repetitive thinking patterns in long CoT responses. To address this issue, in this paper, we propose a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR. Specifically, the first stage, using more training steps, aims to incentivize the model\'s reasoning capabilities via Group Relative Policy Optimization with clip-higher and dynamic sampling components (GRPO++), and the second stage, using fewer training steps, explicitly enforces conciseness and improves efficiency via Length-aware Group Relative Policy Optimization (L-GRPO). Significantly, ConciseR only optimizes response length once all rollouts of a sample are correct, following the "walk before you run" principle. Extensive experimental results demonstrate that our ConciseR model, which generates more concise CoT reasoning responses, outperforms recent state-of-the-art reasoning models with zero RL paradigm across AIME 2024, MATH-500, AMC 2023, Minerva, and Olympiad benchmarks.', 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'e8fe10bbb758cc6a', 'authors': ['Mingyang Song', 'Mao Zheng'], 'affiliations': ['Tencent Hunyuan'], 'pdf_title_img': 'assets/pdf/title_img/2505.21178.jpg', 'data': {'categories': ['#optimization', '#training', '#rlhf', '#rl', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ - ÑĞµÑÑ‚Ñ€Ğ° Ñ‚Ğ°Ğ»Ğ°Ğ½Ñ‚Ğ°: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ ConciseR Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ¿ ÑƒÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ğ° Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ¿ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ConciseR Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'ConciseR: Streamlining Reasoning in Large Language Models', 'desc': 'This paper introduces ConciseR, a two-stage reinforcement learning framework designed to improve the reasoning capabilities of Large Language Models (LLMs) by generating more concise Chain-of-Thought (CoT) responses. The first stage enhances reasoning through Group Relative Policy Optimization with dynamic sampling, while the second stage focuses on enforcing conciseness using Length-aware Group Relative Policy Optimization. The approach addresses the issue of overthinking in LLMs, which often leads to redundant responses in long CoT outputs. Experimental results show that ConciseR outperforms existing state-of-the-art models across various reasoning benchmarks, demonstrating its effectiveness in producing efficient and concise reasoning.'}, 'zh': {'title': 'ç®€æ´æ¨ç†ï¼Œæå‡LLMsæ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºConciseRçš„ä¸¤é˜¶æ®µå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›å¹¶å®ç°ç®€æ´çš„æ¨ç†ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ­¥éª¤ï¼Œé‡‡ç”¨æ”¹è¿›çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPO++ï¼‰æ¥æ¿€åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç¬¬äºŒé˜¶æ®µåˆ™é€šè¿‡é•¿åº¦æ„ŸçŸ¥çš„ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆL-GRPOï¼‰æ¥å¼ºåˆ¶æ‰§è¡Œç®€æ´æ€§ï¼Œæé«˜æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒConciseRåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç”Ÿæˆçš„æ¨ç†å“åº”æ›´ä¸ºç®€æ´ï¼Œè¶…è¶Šäº†æœ€æ–°çš„æ¨ç†æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20426', 'title': 'MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness', 'url': 'https://huggingface.co/papers/2505.20426', 'abstract': "Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/", 'score': 5, 'issue_id': 4002, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '29aacf61c54d1cda', 'authors': ['Yunlong Tang', 'Pinxin Liu', 'Mingqian Feng', 'Zhangyun Tan', 'Rui Mao', 'Chao Huang', 'Jing Bi', 'Yunzhong Xiao', 'Susan Liang', 'Hang Hua', 'Ali Vosoughi', 'Luchuan Song', 'Zeliang Zhang', 'Chenliang Xu'], 'affiliations': ['Carnegie Mellon University', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2505.20426.jpg', 'data': {'categories': ['#3d', '#cv', '#interpretability', '#multimodal', '#reasoning', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'MMPerspective: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ÑƒĞ±ĞµĞ¶ Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMPerspective - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (MLLM). Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ‚Ñ€ĞµÑ… Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ…: Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 2,711 Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. ĞÑ†ĞµĞ½ĞºĞ° 43 ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… MLLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾ ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ĞµĞµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¾Ğ¼ Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹.'}, 'en': {'title': 'Evaluating Perspective Understanding in Multimodal Models', 'desc': 'This paper introduces MMPerspective, a benchmark designed to evaluate how well multimodal large language models (MLLMs) understand perspective geometry. It includes 10 tasks that assess three key areas: how models perceive perspective, their reasoning abilities, and their robustness to changes. The benchmark features over 2,700 images and more than 5,000 question-answer pairs that test various skills like recognizing vanishing points and understanding 3D spatial relationships. The study finds that while MLLMs perform well on basic tasks, they struggle with complex reasoning and maintaining spatial accuracy when faced with transformations, revealing important insights into their limitations and potential improvements.'}, 'zh': {'title': 'MMPerspectiveï¼šè¯„ä¼°è§†è§’ç†è§£çš„åŸºå‡†', 'desc': 'ç†è§£è§†è§’å¯¹äººç±»è§†è§‰æ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œä½†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§’å‡ ä½•æ–¹é¢çš„ç†è§£ç¨‹åº¦å°šä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºäº†MMPerspectiveï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡10ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡ç³»ç»Ÿåœ°è¯„ä¼°MLLMså¯¹è§†è§’çš„ç†è§£ã€‚åŸºå‡†åŒ…å«2711ä¸ªçœŸå®å’Œåˆæˆå›¾åƒå®ä¾‹ï¼Œä»¥åŠ5083ä¸ªé—®é¢˜-ç­”æ¡ˆå¯¹ï¼Œæ¢è®¨å…³é”®èƒ½åŠ›ï¼Œå¦‚æ¶ˆå¤±ç‚¹æ„ŸçŸ¥ã€è§†è§’ç±»å‹æ¨ç†å’Œ3Dç©ºé—´ä¸­çš„çº¿å…³ç³»ç†è§£ç­‰ã€‚é€šè¿‡å¯¹43ä¸ªæœ€å…ˆè¿›çš„MLLMsè¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹åœ¨è¡¨é¢æ„ŸçŸ¥ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»„åˆæ¨ç†å’Œä¿æŒç©ºé—´ä¸€è‡´æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—å±€é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19433', 'title': 'Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic\n  Capabilities in LLM Compression', 'url': 'https://huggingface.co/papers/2505.19433', 'abstract': "Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.", 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '1fb4272abd1ee9c8', 'authors': ['Peijie Dong', 'Zhenheng Tang', 'Xiang Liu', 'Lujun Li', 'Xiaowen Chu', 'Bo Li'], 'affiliations': ['The Hong Kong University of Science and Technology', 'The Hong Kong University of Science and Technology (GuangZhou)'], 'pdf_title_img': 'assets/pdf/title_img/2505.19433.jpg', 'data': {'categories': ['#agents', '#optimization', '#inference', '#benchmark'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ACBench: Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ¶Ğ°Ñ‚Ñ‹Ñ… LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº ACBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ½Ğ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ACBench Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 12 Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¿Ğ¾ 4 Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€ÑƒĞ½Ğ¸Ğ½Ğ³Ğ°, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ 15 Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ 4-Ğ±Ğ¸Ñ‚Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑÑ… Ğ½Ğ° 10-15%. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ERank, Top-k Ranking Correlation Ğ¸ Energy Ğ´Ğ»Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°.'}, 'en': {'title': 'Optimizing LLM Compression for Real-World Agentic Tasks', 'desc': 'This paper presents the Agent Compression Benchmark (ACBench), which evaluates how post-training compression affects the agentic capabilities of large language models (LLMs). Unlike existing benchmarks that focus solely on language modeling and understanding, ACBench assesses 12 tasks across four key capabilities, including workflow generation and long-context retrieval. The study explores various compression techniques, such as quantization and pruning, across multiple LLMs of different sizes. Results indicate that while some compression methods maintain performance in certain tasks, they can significantly reduce accuracy in real-world applications, highlighting the need for careful optimization in LLM deployment.'}, 'zh': {'title': 'ä»£ç†å‹ç¼©åŸºå‡†ï¼šä¼˜åŒ–LLMçš„èƒ½åŠ›ä¸æ•ˆç‡', 'desc': 'åè®­ç»ƒå‹ç¼©æŠ€æœ¯å¯ä»¥å‡å°‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®¡ç®—å’Œå†…å­˜æˆæœ¬ï¼Œä»è€Œå®ç°æ›´é«˜æ•ˆçš„èµ„æºéƒ¨ç½²ã€‚ç°æœ‰çš„å‹ç¼©åŸºå‡†ä¸»è¦å…³æ³¨è¯­è¨€å»ºæ¨¡å’Œè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ï¼Œå¿½è§†äº†æ¨¡å‹åœ¨å·¥ä½œæµã€å·¥å…·ä½¿ç”¨ã€é•¿ä¸Šä¸‹æ–‡ç†è§£å’Œå®é™…åº”ç”¨ç­‰æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä»£ç†å‹ç¼©åŸºå‡†ï¼ˆACBenchï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢è¯„ä¼°å‹ç¼©å¯¹LLMsä»£ç†èƒ½åŠ›å½±å“çš„åŸºå‡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ4ä½é‡åŒ–åœ¨å·¥ä½œæµç”Ÿæˆå’Œå·¥å…·ä½¿ç”¨æ–¹é¢ä¿æŒè‰¯å¥½æ€§èƒ½ï¼Œä½†åœ¨å®é™…åº”ç”¨å‡†ç¡®æ€§ä¸Šä¸‹é™äº†10%-15%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18134', 'title': 'VideoGameBench: Can Vision-Language Models complete popular video games?', 'url': 'https://huggingface.co/papers/2505.18134', 'abstract': "VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) have achieved strong results on coding and math benchmarks that are challenging for humans, yet their ability to perform tasks that come naturally to humans--such as perception, spatial navigation, and memory management--remains understudied. Real video games are crafted to be intuitive for humans to learn and master by leveraging innate inductive biases, making them an ideal testbed for evaluating such capabilities in VLMs. To this end, we introduce VideoGameBench, a benchmark consisting of 10 popular video games from the 1990s that VLMs directly interact with in real-time. VideoGameBench challenges models to complete entire games with access to only raw visual inputs and a high-level description of objectives and controls, a significant departure from existing setups that rely on game-specific scaffolding and auxiliary information. We keep three of the games secret to encourage solutions that generalize to unseen environments. Our experiments show that frontier vision-language models struggle to progress beyond the beginning of each game. We find inference latency to be a major limitation of frontier models in the real-time setting; therefore, we introduce VideoGameBench Lite, a setting where the game pauses while waiting for the LM's next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization of the human skills mentioned above into this benchmark motivates progress in these research directions.", 'score': 5, 'issue_id': 4002, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '8dbd2cb973419464', 'authors': ['Alex L. Zhang', 'Thomas L. Griffiths', 'Karthik R. Narasimhan', 'Ofir Press'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.18134.jpg', 'data': {'categories': ['#games', '#cv', '#multimodal', '#benchmark', '#video'], 'emoji': 'ğŸ®', 'ru': {'title': 'Ğ’Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ñ‹ ĞºĞ°Ğº Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°', 'desc': 'VideoGameBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¸Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 10 Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… Ğ¸Ğ³Ñ€ 1990-Ñ… Ğ³Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ¹ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ¶Ğµ Ğ½Ğ° Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°Ñ… Ğ¸Ğ³Ñ€. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Evaluating Human-Like Skills in AI through Video Games', 'desc': "VideoGameBench is a new benchmark designed to assess the capabilities of vision-language models (VLMs) in real-time video game interactions using only visual inputs and high-level objectives. It highlights the gap in VLMs' performance on tasks that require human-like skills such as perception and spatial navigation, which are essential for mastering video games. The benchmark includes 10 classic video games from the 1990s, challenging models to complete them without additional game-specific information. Results show that even advanced models struggle significantly, indicating the need for further research to enhance VLMs' abilities in these areas."}, 'zh': {'title': 'è¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¸¸æˆèƒ½åŠ›', 'desc': 'VideoGameBench æ˜¯ä¸€ä¸ªè¯„ä¼°è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨å®æ—¶è§†é¢‘æ¸¸æˆäº’åŠ¨ä¸­èƒ½åŠ›çš„åŸºå‡†ã€‚è¯¥åŸºå‡†ä½¿ç”¨ä»…æœ‰çš„è§†è§‰è¾“å…¥å’Œé«˜å±‚æ¬¡ç›®æ ‡ï¼Œå¼ºè°ƒäº†äººç±»æŠ€èƒ½çš„æŒ‘æˆ˜ã€‚å°½ç®¡ VLM åœ¨ç¼–ç å’Œæ•°å­¦åŸºå‡†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ„ŸçŸ¥ã€ç©ºé—´å¯¼èˆªå’Œè®°å¿†ç®¡ç†ç­‰äººç±»è‡ªç„¶ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ä»ç„¶ç¼ºä¹ç ”ç©¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå‰æ²¿çš„ VLM åœ¨æ¸¸æˆçš„å¼€å§‹é˜¶æ®µå°±é‡åˆ°äº†å›°éš¾ï¼Œæ¨ç†å»¶è¿Ÿæ˜¯å…¶åœ¨å®æ—¶ç¯å¢ƒä¸­çš„ä¸»è¦é™åˆ¶ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.11277', 'title': 'Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs', 'url': 'https://huggingface.co/papers/2505.11277', 'abstract': "AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.", 'score': 5, 'issue_id': 3995, 'pub_date': '2025-05-16', 'pub_date_card': {'ru': '16 Ğ¼Ğ°Ñ', 'en': 'May 16', 'zh': '5æœˆ16æ—¥'}, 'hash': 'ec7d9a17f0e241f5', 'authors': ['Yaorui Shi', 'Shihan Li', 'Chang Wu', 'Zhiyuan Liu', 'Junfeng Fang', 'Hengxing Cai', 'An Zhang', 'Xiang Wang'], 'affiliations': ['DP Technology', 'National University of Singapore', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.11277.jpg', 'data': {'categories': ['#optimization', '#rag', '#rl', '#reasoning', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'AutoRefine: Ğ£Ğ¼Ğ½Ğ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜', 'desc': 'AutoRefine - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ¸ÑĞºĞ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. AutoRefine Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ¿Ñ‹ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°Ğ¼Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞ°, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ, Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ AutoRefine Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Refine Your Search, Enhance Your Answers!', 'desc': "AutoRefine is a reinforcement learning framework designed to improve the performance of large language models (LLMs) in complex question-answering tasks. It enhances retrieval-augmented reasoning by allowing LLMs to iteratively refine their knowledge and optimize their search processes. The framework introduces a 'search-and-refine-during-think' approach, where the model filters and organizes information before generating answers. Experiments show that AutoRefine outperforms existing methods, especially in scenarios requiring multi-hop reasoning, by producing higher-quality searches and synthesizing evidence more effectively."}, 'zh': {'title': 'AutoRefineï¼šæå‡æ¨ç†èƒ½åŠ›çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'AutoRefineæ˜¯ä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¿­ä»£ä¼˜åŒ–çŸ¥è¯†å’Œæœç´¢è¿‡ç¨‹æ¥å¢å¼ºæ£€ç´¢å¢å¼ºæ¨ç†ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†â€œæ€è€ƒæ—¶æœç´¢ä¸ç²¾ç‚¼â€çš„æ–°èŒƒå¼ï¼Œåœ¨è¿ç»­çš„æœç´¢è°ƒç”¨ä¹‹é—´å¼•å…¥äº†æ˜ç¡®çš„çŸ¥è¯†ç²¾ç‚¼æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ç”Ÿæˆç­”æ¡ˆä¹‹å‰è¿‡æ»¤ã€æç‚¼å’Œç»„ç»‡è¯æ®ã€‚é€šè¿‡ç»“åˆç‰¹å®šçš„æ£€ç´¢å¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼ŒAutoRefineåœ¨å¤æ‚çš„å¤šè·³é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoRefineèƒ½å¤Ÿè¿›è¡Œæ›´é¢‘ç¹ä¸”é«˜è´¨é‡çš„æœç´¢ï¼Œæœ‰æ•ˆåˆæˆè¯æ®ï¼Œä»è€Œæé«˜æ¨ç†çš„å‡†ç¡®æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21097', 'title': 'Thinker: Learning to Think Fast and Slow', 'url': 'https://huggingface.co/papers/2505.21097', 'abstract': 'A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.', 'score': 4, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '90232cf610bff733', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.21097.jpg', 'data': {'categories': ['#inference', '#reasoning', '#training', '#math', '#optimization', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ²Ğ¾Ğ¹Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¸Ğ· Ğ¿ÑĞ¸Ñ…Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸ Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ ÑÑ‚Ğ°Ğ¿Ğ°: Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºÑƒ, Ğ¼ĞµĞ´Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ 24.9% Ğ´Ğ¾ 27.9% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-1.5B Ğ¸ Ñ 45.9% Ğ´Ğ¾ 49.8% Ğ´Ğ»Ñ DeepSeek-R1-Qwen-1.5B. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ²Ğ¸Ğ´ĞµÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ deliberative Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ - ÑÑ‚Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ, Ğ½Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ñ€ÑƒĞ³ Ğ´Ñ€ÑƒĞ³Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ²Ñ‹Ğ¸Ğ³Ñ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¾Ñ‚ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Enhancing LLM Accuracy through Structured Reasoning', 'desc': 'This paper presents a novel four-stage modification to question-answering tasks for Large Language Models (LLMs), inspired by Dual Process Theory. The stages include Fast Thinking, Verification, Slow Thinking, and Summarization, which help separate intuitive responses from more deliberate reasoning. By implementing this structured approach, the authors demonstrate significant improvements in accuracy for math and coding tasks, with notable efficiency in token usage. The results indicate that enhancing both intuitive and deliberative reasoning can lead to better performance in LLMs.'}, 'zh': {'title': 'ä¼˜åŒ–é—®ç­”ä»»åŠ¡ï¼Œæå‡æ¨¡å‹æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºåŒé‡è¿‡ç¨‹ç†è®ºçš„å››é˜¶æ®µé—®ç­”ä»»åŠ¡ä¿®æ”¹ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦å’Œç¼–ç¨‹é¢†åŸŸçš„å‡†ç¡®æ€§ã€‚è¯¥æ–¹æ³•å°†æ€ç»´è¿‡ç¨‹åˆ†ä¸ºå¿«é€Ÿæ€è€ƒã€éªŒè¯ã€æ…¢é€Ÿæ€è€ƒå’Œæ€»ç»“å››ä¸ªé˜¶æ®µï¼Œä»¥æ”¹å–„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°å’Œç²¾ç‚¼å…¶åˆå§‹å›ç­”ï¼Œä»è€Œå‡å°‘å†—ä½™å’Œä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„å¹³å‡å‡†ç¡®ç‡ï¼Œè¯æ˜äº†ç›´è§‰å’Œæ·±æ€ç†Ÿè™‘çš„æ¨ç†æ˜¯äº’è¡¥çš„ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21070', 'title': 'Minute-Long Videos with Dual Parallelisms', 'url': 'https://huggingface.co/papers/2505.21070', 'abstract': 'Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54times lower latency and 1.48times lower memory cost on 8timesRTX 4090 GPUs.', 'score': 4, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'c777be11d4844462', 'authors': ['Zeqing Wang', 'Bowen Zheng', 'Xingyi Yang', 'Yuecong Xu', 'Xinchao Wang'], 'affiliations': ['Huazhong University of Science and Technology', 'National University of Singapore', 'Xidian University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21070.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#video', '#inference'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Diffusion Transformer (DiT), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ÑƒÑ DualParal. ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ´ĞµÑ Ğ·Ğ°ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ÑÑ Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ ÑĞ»Ğ¾ĞµĞ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ GPU Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ±Ğ»Ğ¾Ñ‡Ğ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ 1025 ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ´Ğ¾ 6,54 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµĞ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ¸ 1,48 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ½Ğ° 8 GPU RTX 4090.'}, 'en': {'title': 'Revolutionizing Video Generation with DualPar Efficiency!', 'desc': 'The paper presents a new method called DualPar for improving the efficiency of video generation using Diffusion Transformer (DiT) models. It addresses the high latency and memory costs associated with generating long videos by distributing the workload across multiple GPUs. The method uses a block-wise denoising approach to maintain synchronized noise levels while allowing for parallel processing of frames and model layers. Additionally, it introduces a feature cache and coordinated noise initialization to optimize performance, resulting in faster and more efficient video generation without artifacts.'}, 'zh': {'title': 'é«˜æ•ˆç”Ÿæˆæ— é™é•¿è§†é¢‘çš„åˆ›æ–°ç­–ç•¥', 'desc': 'æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é•¿è§†é¢‘ç”Ÿæˆæ—¶çš„å¤„ç†å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å¸ƒå¼æ¨ç†ç­–ç•¥ï¼Œç§°ä¸ºDualParalï¼Œé€šè¿‡åœ¨å¤šä¸ªGPUä¸Šå¹¶è¡Œå¤„ç†æ—¶é—´å¸§å’Œæ¨¡å‹å±‚æ¥æé«˜æ•ˆç‡ã€‚ä¸ºäº†å…‹æœæ‰©æ•£æ¨¡å‹å¯¹å™ªå£°æ°´å¹³åŒæ­¥çš„è¦æ±‚ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å—çº§å»å™ªæ–¹æ¡ˆï¼Œä½¿å¾—æ¯ä¸ªGPUå¤„ç†ç‰¹å®šçš„å¸§å—å’Œå±‚ï¼ŒåŒæ—¶å®ç°å¼‚æ­¥è®¡ç®—å’Œé€šä¿¡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿæˆé«˜è´¨é‡è§†é¢‘æ—¶æ˜¾è‘—é™ä½äº†å»¶è¿Ÿå’Œå†…å­˜æ¶ˆè€—ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆæ— é™é•¿çš„è§†é¢‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20561', 'title': 'Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning', 'url': 'https://huggingface.co/papers/2505.20561', 'abstract': 'BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.', 'score': 4, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '4b7b0470932e5c49', 'authors': ['Shenao Zhang', 'Yaqing Wang', 'Yinxiao Liu', 'Tianqi Liu', 'Peter Grabowski', 'Eugene Ie', 'Zhaoran Wang', 'Yunxuan Li'], 'affiliations': ['Google', 'Google DeepMind', 'Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.20561.jpg', 'data': {'categories': ['#rl', '#training', '#rlhf', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'BARL: Ğ£Ğ¼Ğ½ĞµĞµ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµĞ¼, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ĞµĞµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´Ğ°ĞµĞ¼', 'desc': 'BARL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ‘Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¾Ğ³Ğ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². BARL Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´Ğ°ĞµĞ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BARL Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ñ€Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing LLMs with Reflective Exploration through BARL', 'desc': 'The paper introduces BARL, a Bayes-Adaptive Reinforcement Learning framework designed to improve the performance of Large Language Models (LLMs) by incorporating reflective reasoning and efficient exploration strategies. Traditional Markovian RL methods limit exploration to the training phase and rely solely on current state information, which may hinder the emergence of reflective reasoning during training. BARL addresses this limitation by optimizing expected returns using a Bayesian approach, allowing the model to adaptively explore and exploit based on updated beliefs about the environment. Empirical results show that BARL significantly enhances token efficiency and effectiveness in reasoning tasks compared to standard Markovian RL methods.'}, 'zh': {'title': 'BARLï¼šæå‡LLMæ€§èƒ½çš„è´å¶æ–¯è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ æ¡†æ¶', 'desc': 'BARLæ˜¯ä¸€ç§è´å¶æ–¯è‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆåæ€æ¨ç†å’Œé«˜æ•ˆæ¢ç´¢ï¼Œæå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ä¼ ç»Ÿçš„é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ é™åˆ¶äº†æ¢ç´¢è¿‡ç¨‹ï¼Œä»…ä¾èµ–å½“å‰çŠ¶æ€çš„å†å²ä¸Šä¸‹æ–‡æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œå¯¼è‡´åæ€æ¨ç†çš„å‡ºç°å’Œå…¶åœ¨æµ‹è¯•æ—¶çš„å¥½å¤„ä¸æ˜ç¡®ã€‚BARLæ¡†æ¶é€šè¿‡ä¼˜åŒ–é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„åéªŒåˆ†å¸ƒï¼Œé¼“åŠ±æ¨¡å‹åœ¨å¥–åŠ±æœ€å¤§åŒ–å’Œä¿¡æ¯æ”¶é›†ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒBARLåœ¨åˆæˆå’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ä¼˜äºä¼ ç»Ÿçš„é©¬å°”å¯å¤«å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„ä»¤ç‰Œæ•ˆç‡å’Œæ›´æœ‰æ•ˆçš„æ¢ç´¢èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20289', 'title': 'VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual\n  Tool Selection', 'url': 'https://huggingface.co/papers/2505.20289', 'abstract': "VisTA, a reinforcement learning framework, enhances visual reasoning by autonomously selecting and combining tools from a diverse library without extensive human supervision.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce VisTA, a new reinforcement learning framework that empowers visual agents to dynamically explore, select, and combine tools from a diverse library based on empirical performance. Existing methods for tool-augmented reasoning either rely on training-free prompting or large-scale fine-tuning; both lack active tool exploration and typically assume limited tool diversity, and fine-tuning methods additionally demand extensive human supervision. In contrast, VisTA leverages end-to-end reinforcement learning to iteratively refine sophisticated, query-specific tool selection strategies, using task outcomes as feedback signals. Through Group Relative Policy Optimization (GRPO), our framework enables an agent to autonomously discover effective tool-selection pathways without requiring explicit reasoning supervision. Experiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate that VisTA achieves substantial performance gains over training-free baselines, especially on out-of-distribution examples. These results highlight VisTA's ability to enhance generalization, adaptively utilize diverse tools, and pave the way for flexible, experience-driven visual reasoning systems.", 'score': 4, 'issue_id': 3991, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '06184525a85012f4', 'authors': ['Zeyi Huang', 'Yuyang Ji', 'Anirudh Sundara Rajan', 'Zefan Cai', 'Wen Xiao', 'Junjie Hu', 'Yong Jae Lee'], 'affiliations': ['Microsoft', 'University of Wisconsin-Madison'], 'pdf_title_img': 'assets/pdf/title_img/2505.20289.jpg', 'data': {'categories': ['#reasoning', '#agents', '#benchmark', '#optimization', '#cv', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VisTA: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'VisTA - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ. ĞĞ½Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ğ¸ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², VisTA Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞºĞ²Ğ¾Ğ·Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ VisTA Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ… Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸.'}, 'en': {'title': 'Empowering Visual Agents with Autonomous Tool Selection', 'desc': 'VisTA is a reinforcement learning framework designed to improve visual reasoning by allowing agents to autonomously select and combine tools from a diverse library. Unlike traditional methods that either require extensive human supervision or lack active exploration, VisTA uses end-to-end reinforcement learning to refine tool selection strategies based on task outcomes. The framework employs Group Relative Policy Optimization (GRPO) to enable agents to discover effective pathways for tool selection without explicit reasoning guidance. Experiments show that VisTA significantly outperforms existing methods, particularly in challenging scenarios, demonstrating its potential for enhancing generalization and adaptability in visual reasoning tasks.'}, 'zh': {'title': 'VisTAï¼šè‡ªä¸»é€‰æ‹©å·¥å…·çš„è§†è§‰æ¨ç†æ–°æ¡†æ¶', 'desc': 'VisTAæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è‡ªä¸»é€‰æ‹©å’Œç»„åˆå¤šæ ·åŒ–å·¥å…·æ¥å¢å¼ºè§†è§‰æ¨ç†èƒ½åŠ›ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒVisTAä¸ä¾èµ–äºè®­ç»ƒå‰æç¤ºæˆ–å¤§è§„æ¨¡å¾®è°ƒï¼Œè€Œæ˜¯é€šè¿‡ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ æ¥ä¼˜åŒ–å·¥å…·é€‰æ‹©ç­–ç•¥ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºåé¦ˆä¿¡å·ï¼Œå…è®¸æ™ºèƒ½ä½“è‡ªä¸»å‘ç°æœ‰æ•ˆçš„å·¥å…·é€‰æ‹©è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisTAåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åˆ†å¸ƒå¤–ç¤ºä¾‹æ—¶ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨å¢å¼ºæ³›åŒ–èƒ½åŠ›å’Œçµæ´»åˆ©ç”¨å¤šæ ·åŒ–å·¥å…·æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19314', 'title': 'SoloSpeech: Enhancing Intelligibility and Quality in Target Speech\n  Extraction through a Cascaded Generative Pipeline', 'url': 'https://huggingface.co/papers/2505.19314', 'abstract': "Target Speech Extraction (TSE) aims to isolate a target speaker's voice from a mixture of multiple speakers by leveraging speaker-specific cues, typically provided as auxiliary audio (a.k.a. cue audio). Although recent advancements in TSE have primarily employed discriminative models that offer high perceptual quality, these models often introduce unwanted artifacts, reduce naturalness, and are sensitive to discrepancies between training and testing environments. On the other hand, generative models for TSE lag in perceptual quality and intelligibility. To address these challenges, we present SoloSpeech, a novel cascaded generative pipeline that integrates compression, extraction, reconstruction, and correction processes. SoloSpeech features a speaker-embedding-free target extractor that utilizes conditional information from the cue audio's latent space, aligning it with the mixture audio's latent space to prevent mismatches. Evaluated on the widely-used Libri2Mix dataset, SoloSpeech achieves the new state-of-the-art intelligibility and quality in target speech extraction and speech separation tasks while demonstrating exceptional generalization on out-of-domain data and real-world scenarios.", 'score': 4, 'issue_id': 3990, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'd58bb66dfe2fc291', 'authors': ['Helin Wang', 'Jiarui Hai', 'Dongchao Yang', 'Chen Chen', 'Kai Li', 'Junyi Peng', 'Thomas Thebaud', 'Laureano Moro Velazquez', 'Jesus Villalba', 'Najim Dehak'], 'affiliations': ['Brno University of Technology', 'Johns Hopkins University', 'Nanyang Technological University', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19314.jpg', 'data': {'categories': ['#audio'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'SoloSpeech: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'SoloSpeech - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸ Ğ¸Ğ· ÑĞ¼ĞµÑĞ¸ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ². ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ĞºĞ°ÑĞºĞ°Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ğ°Ñ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ - ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ±ĞµĞ· ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰ĞµĞ³Ğ¾ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²ÑĞ¿Ğ¾Ğ¼Ğ¾Ğ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°ÑƒĞ´Ğ¸Ğ¾. SoloSpeech Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ Libri2Mix, Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ.'}, 'en': {'title': 'SoloSpeech: Revolutionizing Target Speech Extraction with Generative Techniques', 'desc': "This paper introduces SoloSpeech, a new approach for Target Speech Extraction (TSE) that effectively isolates a target speaker's voice from a mix of multiple speakers. Unlike traditional discriminative models that can produce artifacts and lack naturalness, SoloSpeech employs a cascaded generative pipeline that includes processes for compression, extraction, reconstruction, and correction. It utilizes a speaker-embedding-free target extractor that aligns the latent spaces of cue audio and mixture audio, enhancing performance and reducing discrepancies. Evaluated on the Libri2Mix dataset, SoloSpeech sets a new benchmark for intelligibility and quality in TSE, showing strong generalization capabilities in diverse real-world scenarios."}, 'zh': {'title': 'SoloSpeechï¼šæå‡ç›®æ ‡è¯­éŸ³æå–çš„æ–°æ–¹æ³•', 'desc': 'ç›®æ ‡è¯­éŸ³æå–ï¼ˆTSEï¼‰æ—¨åœ¨ä»å¤šä¸ªè¯´è¯è€…çš„æ··åˆéŸ³é¢‘ä¸­åˆ†ç¦»å‡ºç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ï¼Œé€šå¸¸åˆ©ç”¨ä½œä¸ºè¾…åŠ©éŸ³é¢‘çš„è¯´è¯è€…ç‰¹å¾çº¿ç´¢ã€‚å°½ç®¡è¿‘æœŸçš„TSEè¿›å±•ä¸»è¦é‡‡ç”¨äº†é«˜æ„ŸçŸ¥è´¨é‡çš„åˆ¤åˆ«æ¨¡å‹ï¼Œä½†è¿™äº›æ¨¡å‹å¸¸å¸¸å¼•å…¥ä¸å¿…è¦çš„ä¼ªå½±ï¼Œé™ä½è‡ªç„¶æ€§ï¼Œå¹¶å¯¹è®­ç»ƒå’Œæµ‹è¯•ç¯å¢ƒä¹‹é—´çš„å·®å¼‚æ•æ„Ÿã€‚å¦ä¸€æ–¹é¢ï¼Œç”Ÿæˆæ¨¡å‹åœ¨æ„ŸçŸ¥è´¨é‡å’Œå¯æ‡‚æ€§æ–¹é¢æ»åã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SoloSpeechï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„çº§è”ç”Ÿæˆç®¡é“ï¼Œé›†æˆäº†å‹ç¼©ã€æå–ã€é‡å»ºå’Œä¿®æ­£è¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨ç›®æ ‡è¯­éŸ³æå–å’Œè¯­éŸ³åˆ†ç¦»ä»»åŠ¡ä¸­å®ç°æ–°çš„æœ€å…ˆè¿›çš„å¯æ‡‚æ€§å’Œè´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20321', 'title': 'BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases', 'url': 'https://huggingface.co/papers/2505.20321', 'abstract': 'BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.', 'score': 4, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '3f6bd45ed0456607', 'authors': ['Mathew J. Koretsky', 'Maya Willey', 'Adi Asija', 'Owen Bianchi', 'Chelsea X. Alvarado', 'Tanay Nayak', 'Nicole Kuznetsov', 'Sungwon Kim', 'Mike A. Nalls', 'Daniel Khashabi', 'Faraz Faghri'], 'affiliations': ['Center for Alzheimers Disease and Related Dementias, NIA, NIH', 'DataTecnica LLC', 'Johns Hopkins University', 'Laboratory of Neurogenetics, NIA, NIH'], 'pdf_title_img': 'assets/pdf/title_img/2505.20321.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multimodal', '#benchmark', '#reasoning'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾-SQL Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ´Ğ»Ñ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½Ñ‹', 'desc': 'BiomedSQL - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ² SQL, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºÑƒÑ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 68 000 Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ/SQL-Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ/Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ BigQuery. ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ĞµĞ², Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ¼.'}, 'en': {'title': 'Bridging the Gap in Biomedical Text-to-SQL Reasoning', 'desc': 'BiomedSQL is a benchmark designed to assess scientific reasoning in text-to-SQL tasks specifically within the biomedical domain. It highlights the challenges faced by existing models in translating qualitative scientific questions into executable SQL queries, especially when implicit reasoning is required. The benchmark includes 68,000 question/SQL query/answer triples based on a comprehensive biomedical knowledge base, which necessitates understanding complex domain-specific criteria. Evaluation of various large language models (LLMs) shows significant performance gaps, indicating the need for improved systems that can effectively support scientific discovery through advanced reasoning capabilities.'}, 'zh': {'title': 'BiomedSQLï¼šæ¨åŠ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ç§‘å­¦æ¨ç†', 'desc': 'BiomedSQLæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ–‡æœ¬åˆ°SQLç”Ÿæˆä¸­çš„ç§‘å­¦æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸã€‚å®ƒåŒ…å«68,000ä¸ªé—®é¢˜ã€SQLæŸ¥è¯¢å’Œç­”æ¡ˆçš„ä¸‰å…ƒç»„ï¼ŒåŸºäºä¸€ä¸ªæ•´åˆäº†åŸºå› -ç–¾ç—…å…³è”ã€ç»„å­¦æ•°æ®å› æœæ¨æ–­å’Œè¯ç‰©æ‰¹å‡†è®°å½•çš„çŸ¥è¯†åº“ã€‚ç°æœ‰çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿåœ¨å°†ç§‘å­¦é—®é¢˜è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„SQLæ—¶ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦éšå«é¢†åŸŸæ¨ç†æ—¶ï¼Œè¡¨ç°ä¸ä½³ã€‚é€šè¿‡å¯¹å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼ŒBiomedSQLæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œä¸ºæ”¯æŒç§‘å­¦å‘ç°çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿçš„è¿›æ­¥å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17005', 'title': 'R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.17005', 'abstract': "R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) are powerful but prone to hallucinations due to static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting external information, but current methods often are costly, generalize poorly, or ignore the internal knowledge of the model. In this paper, we introduce R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage both internal and external knowledge sources. R1-Searcher++ employs a two-stage training strategy: an initial SFT Cold-start phase for preliminary format learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses outcome-supervision to encourage exploration, incorporates a reward mechanism for internal knowledge utilization, and integrates a memorization mechanism to continuously assimilate retrieved information, thereby enriching the model's internal knowledge. By leveraging internal knowledge and external search engine, the model continuously improves its capabilities, enabling efficient retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++ outperforms previous RAG and reasoning methods and achieves efficient retrieval. The code is available at https://github.com/RUCAIBox/R1-Searcher-plus.", 'score': 4, 'issue_id': 3996, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'c185506804508a79', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Wenqing Tian', 'Zhipeng Chen', 'Yuhuan Wu', 'Jiahao Zhao', 'Yingqian Min', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['Beijing Institute of Technology', 'DataCanvas Alaya NeW', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.17005.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#reasoning', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM: Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ', 'desc': 'R1-Searcher++ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼ Ğ¸ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¾Ğ±Ñ€ĞµÑ‚ĞµĞ½Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. R1-Searcher++ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ¼, Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ ÑĞ²Ğ¾Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing LLMs with Adaptive Knowledge Integration', 'desc': "R1-Searcher++ is a new framework that enhances Large Language Models (LLMs) by combining internal knowledge with external information through a two-stage training process. The first stage, called SFT Cold-start, helps the model learn basic formats, while the second stage uses Reinforcement Learning (RL) to improve knowledge acquisition dynamically. This RL phase encourages the model to explore and rewards it for effectively using its internal knowledge, while also allowing it to memorize and integrate new information from external sources. As a result, R1-Searcher++ significantly boosts the model's reasoning efficiency and performance compared to existing methods."}, 'zh': {'title': 'R1-Searcher++ï¼šæ™ºèƒ½æ•´åˆå†…å¤–éƒ¨çŸ¥è¯†çš„æ¡†æ¶', 'desc': 'R1-Searcher++æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œå¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ•´åˆå†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚è¯¥æ¡†æ¶é¦–å…ˆè¿›è¡Œå†·å¯åŠ¨çš„åˆå§‹æ ¼å¼å­¦ä¹ ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡ŒåŠ¨æ€çŸ¥è¯†è·å–ã€‚å¼ºåŒ–å­¦ä¹ é˜¶æ®µé‡‡ç”¨ç»“æœç›‘ç£ï¼Œé¼“åŠ±æ¨¡å‹æ¢ç´¢ï¼ŒåŒæ—¶å¼•å…¥å¥–åŠ±æœºåˆ¶ä»¥ä¿ƒè¿›å†…éƒ¨çŸ¥è¯†çš„åˆ©ç”¨ï¼Œå¹¶ç»“åˆè®°å¿†æœºåˆ¶ä¸æ–­å¸æ”¶æ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR1-Searcher++åœ¨æ£€ç´¢å¢å¼ºæ¨ç†æ•ˆç‡å’Œæ€§èƒ½ä¸Šä¼˜äºä¹‹å‰çš„RAGå’Œæ¨ç†æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21471', 'title': 'Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration', 'url': 'https://huggingface.co/papers/2505.21471', 'abstract': 'The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, ExtAgents, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $boldsymbol{inftyBench+}, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls within or exceeds the context window$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.', 'score': 3, 'issue_id': 4001, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '3daf58c369674282', 'authors': ['Zijun Liu', 'Zhennan Wan', 'Peng Li', 'Ming Yan', 'Ji Zhang', 'Fei Huang', 'Yang Liu'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University', 'Institute for AI Industry Research (AIR), Tsinghua University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2505.21471.jpg', 'data': {'categories': ['#long_context', '#inference', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ExtAgents: Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ±ĞµĞ· ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ ExtAgents, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (LLM). Ğ­Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºĞ½Ğ° LLM. ExtAgents Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ¾Ğ². Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ñƒ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Scalable Knowledge Integration with ExtAgents Framework', 'desc': 'The ExtAgents framework improves how large language models (LLMs) integrate external knowledge during inference without needing to extend their context window. It addresses key challenges in knowledge synchronization and reasoning that limit the effectiveness of existing methods. By using a multi-agent approach, ExtAgents allows for better scalability and performance in tasks that require significant external information. This method not only enhances results in multi-hop question answering but also maintains efficiency through parallel processing.'}, 'zh': {'title': 'ExtAgentsï¼šæå‡å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†æ•´åˆçš„å¯æ‰©å±•æ€§', 'desc': 'ExtAgentsæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†æ—¶æ•´åˆå¤–éƒ¨çŸ¥è¯†çš„å¯æ‰©å±•æ€§ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰çŸ¥è¯†åŒæ­¥å’Œæ¨ç†è¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªæ ¸å¿ƒç“¶é¢ˆï¼Œä»è€Œåœ¨ä¸å¢åŠ ä¸Šä¸‹æ–‡çª—å£çš„æƒ…å†µä¸‹ï¼Œæå‡äº†æ€§èƒ½ã€‚é€šè¿‡åœ¨å¤šè·³é—®ç­”æµ‹è¯•å’Œå…¶ä»–å…¬å…±æµ‹è¯•é›†ä¸Šçš„åŸºå‡†æµ‹è¯•ï¼ŒExtAgentsæ˜¾è‘—æé«˜äº†åœ¨ç›¸åŒå¤–éƒ¨çŸ¥è¯†è¾“å…¥ä¸‹çš„è¡¨ç°ã€‚è¯¥æ–¹æ³•è¿˜å› å…¶é«˜å¹¶è¡Œæ€§è€Œä¿æŒäº†é«˜æ•ˆç‡ï¼Œæœªæ¥åœ¨åè°ƒLLMæ™ºèƒ½ä½“ä»¥å¢åŠ å¤–éƒ¨çŸ¥è¯†è¾“å…¥æ–¹é¢çš„ç ”ç©¶å°†æœ‰åŠ©äºå®é™…åº”ç”¨ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21205', 'title': 'Sci-Fi: Symmetric Constraint for Frame Inbetweening', 'url': 'https://huggingface.co/papers/2505.21205', 'abstract': "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines.", 'score': 3, 'issue_id': 3990, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'a20a9eb43b42208f', 'authors': ['Liuhan Chen', 'Xiaodong Cun', 'Xiaoyu Li', 'Xianyi He', 'Shenghai Yuan', 'Jie Chen', 'Ying Shan', 'Li Yuan'], 'affiliations': ['ARC Lab, Tencent PCG', 'GVC Lab, Great Bay University', 'Rabbitpre Intelligence', 'Shenzhen Graduate School, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21205.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#video', '#training'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ¡Ğ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğµ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¿Ñ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Sci-Fi, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ°ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ñ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ (I2V-DM). Sci-Fi Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ EF-Net Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ°Ğ´Ñ€Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ½Ğ°Ğ´ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Achieving Harmony in Video Frame Synthesis with Sci-Fi', 'desc': 'This paper presents a new approach called Sci-Fi for generating intermediate video sequences from given start and end frames. The authors identify a limitation in existing methods that treat the start and end frame constraints equally, which can lead to poor video quality. Sci-Fi introduces a novel mechanism using a lightweight module, EF-Net, to enhance the influence of the end frame, ensuring it has a similar impact as the start frame. The results show that Sci-Fi produces smoother and more consistent transitions in generated videos compared to current state-of-the-art techniques.'}, 'zh': {'title': 'å®ç°èµ·å§‹å¸§ä¸ç»“æŸå¸§çš„å¯¹ç§°çº¦æŸ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºSci-Fiï¼Œç”¨äºåœ¨ç»™å®šçš„èµ·å§‹å¸§å’Œç»“æŸå¸§ä¹‹é—´åˆæˆä¸­é—´è§†é¢‘åºåˆ—ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºå¤§å‹é¢„è®­ç»ƒçš„å›¾åƒåˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆI2V-DMsï¼‰ï¼Œä½†åœ¨å¼•å…¥ç»“æŸå¸§çº¦æŸæ—¶å­˜åœ¨è®¾è®¡ç¼ºé™·ã€‚æˆ‘ä»¬å‘ç°ï¼Œç®€å•åœ°ä½¿ç”¨ä¸èµ·å§‹å¸§ç›¸åŒçš„æœºåˆ¶æ¥å¼•å…¥ç»“æŸå¸§çº¦æŸï¼Œå¯èƒ½æ— æ³•æœ‰æ•ˆå½±å“ä¸­é—´å†…å®¹ï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆå¸§çš„è¿åŠ¨ä¸ä¸€è‡´æˆ–å¤–è§‚å´©æºƒã€‚Sci-Fié€šè¿‡å¼•å…¥ä¸€ç§æ”¹è¿›çš„æœºåˆ¶å’Œè½»é‡çº§æ¨¡å—EF-Netï¼Œä½¿ç»“æŸå¸§çº¦æŸçš„å½±å“åŠ›ä¸èµ·å§‹å¸§ç›¸å½“ï¼Œä»è€Œå®ç°æ›´å’Œè°çš„è¿‡æ¸¡æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20650', 'title': 'FinTagging: An LLM-ready Benchmark for Extracting and Structuring\n  Financial Information', 'url': 'https://huggingface.co/papers/2505.20650', 'abstract': 'FinTagging evaluates LLMs for structured information extraction and semantic alignment in XBRL financial reporting, revealing challenges in fine-grained concept alignment.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce FinTagging, the first full-scope, table-aware XBRL benchmark designed to evaluate the structured information extraction and semantic alignment capabilities of large language models (LLMs) in the context of XBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL tagging as flat multi-class classification and focus solely on narrative text, FinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for financial entity extraction and FinCL for taxonomy-driven concept alignment. It requires models to jointly extract facts and align them with the full 10k+ US-GAAP taxonomy across both unstructured text and structured tables, enabling realistic, fine-grained evaluation. We assess a diverse set of LLMs under zero-shot settings, systematically analyzing their performance on both subtasks and overall tagging accuracy. Our results reveal that, while LLMs demonstrate strong generalization in information extraction, they struggle with fine-grained concept alignment, particularly in disambiguating closely related taxonomy entries. These findings highlight the limitations of existing LLMs in fully automating XBRL tagging and underscore the need for improved semantic reasoning and schema-aware modeling to meet the demands of accurate financial disclosure. Code is available at our GitHub repository and data is at our Hugging Face repository.', 'score': 3, 'issue_id': 4008, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': '71aeb73f235b2c8a', 'authors': ['Yan Wang', 'Yang Ren', 'Lingfei Qian', 'Xueqing Peng', 'Keyi Wang', 'Yi Han', 'Dongji Feng', 'Xiao-Yang Liu', 'Jimin Huang', 'Qianqian Xie'], 'affiliations': ['Columbia University', 'Gustavus Adolphus College', 'The Fin AI'], 'pdf_title_img': 'assets/pdf/title_img/2505.20650.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#alignment', '#dataset', '#reasoning', '#data'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'FinTagging: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ LLM Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ XBRL', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ FinTagging - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ XBRL. FinTagging Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ XBRL Ğ½Ğ° Ğ´Ğ²Ğ° Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ: FinNI Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ FinCL Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ LLM Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾Ğ±ĞµĞ¸Ñ… Ğ¿Ğ¾Ğ´Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ¾Ğ±Ñ‰ÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ñ LLM Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¾Ğ½Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ±Ğ»Ğ¸Ğ·ĞºĞ¸Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸.'}, 'en': {'title': 'FinTagging: Enhancing LLMs for Accurate Financial Reporting', 'desc': 'FinTagging is a benchmark designed to evaluate how well large language models (LLMs) can extract structured information and align semantics in XBRL financial reports. It breaks down the tagging process into two tasks: extracting financial entities and aligning them with a detailed taxonomy. The benchmark assesses LLMs in zero-shot settings, focusing on their ability to handle both unstructured text and structured tables. Results show that while LLMs excel at extracting information, they face challenges in accurately aligning closely related concepts within the taxonomy, indicating a need for better semantic reasoning in financial reporting.'}, 'zh': {'title': 'FinTaggingï¼šæå‡è´¢åŠ¡æŠ¥å‘Šä¿¡æ¯æå–ä¸è¯­ä¹‰å¯¹é½çš„åŸºå‡†', 'desc': 'FinTaggingæ˜¯ä¸€ä¸ªå…¨æ–°çš„åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨XBRLè´¢åŠ¡æŠ¥å‘Šä¸­çš„ç»“æ„åŒ–ä¿¡æ¯æå–å’Œè¯­ä¹‰å¯¹é½èƒ½åŠ›ã€‚å®ƒå°†XBRLæ ‡è®°é—®é¢˜åˆ†è§£ä¸ºä¸¤ä¸ªå­ä»»åŠ¡ï¼šFinNIç”¨äºæå–è´¢åŠ¡å®ä½“ï¼ŒFinCLç”¨äºåŸºäºåˆ†ç±»æ³•çš„æ¦‚å¿µå¯¹é½ã€‚é€šè¿‡åœ¨æ— ç›‘ç£è®¾ç½®ä¸‹è¯„ä¼°å¤šç§LLMsï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹åœ¨ä¿¡æ¯æå–æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç»†ç²’åº¦æ¦‚å¿µå¯¹é½ä¸Šå­˜åœ¨å›°éš¾ï¼Œå°¤å…¶æ˜¯åœ¨åŒºåˆ†ç›¸ä¼¼çš„åˆ†ç±»æ³•æ¡ç›®æ—¶ã€‚ç ”ç©¶ç»“æœå¼ºè°ƒäº†ç°æœ‰LLMsåœ¨è‡ªåŠ¨åŒ–XBRLæ ‡è®°æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ”¹è¿›è¯­ä¹‰æ¨ç†å’Œæ¨¡å¼æ„ŸçŸ¥å»ºæ¨¡ä»¥æ»¡è¶³å‡†ç¡®è´¢åŠ¡æŠ«éœ²çš„è¦æ±‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19973', 'title': 'DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response', 'url': 'https://huggingface.co/papers/2505.19973', 'abstract': 'DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.', 'score': 3, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'c4a0e825312e2a47', 'authors': ['Bilel Cherif', 'Tamas Bisztray', 'Richard A. Dubniczky', 'Aaesha Aldahmani', 'Saeed Alshehhi', 'Norbert Tihanyi'], 'affiliations': ['EÃ¶tvÃ¶s LorÃ¡nd University, Budapest, Hungary', 'Technology Innovation Institute, Abu Dhabi, UAE', 'University of Oslo, Oslo, Norway'], 'pdf_title_img': 'assets/pdf/title_img/2505.19973.jpg', 'data': {'categories': ['#science', '#hallucinations', '#benchmark', '#dataset', '#optimization', '#reasoning'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸ĞºĞ¸', 'desc': 'DFIR-Metric - ÑÑ‚Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ğ¾Ğ¹ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸ĞºĞ¸. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚ĞµÑÑ‚Ñ‹ Ğ½Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Benchmark Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ 14 LLM Ğ¿Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Task Understanding Score (TUS), Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸ Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒÑ.'}, 'en': {'title': 'Evaluating AI in Digital Forensics: The DFIR-Metric Benchmark', 'desc': 'The paper introduces DFIR-Metric, a benchmark designed to evaluate Large Language Models (LLMs) in the field of Digital Forensics and Incident Response (DFIR). It consists of three main components: a Knowledge Assessment with expert-reviewed questions, Realistic Forensic Challenges that test reasoning and evidence correlation, and Practical Analysis using real forensic cases. The study also presents a new metric called the Task Understanding Score (TUS) to assess model performance in low-accuracy situations. By evaluating 14 LLMs, this benchmark aims to provide a reliable framework for improving AI applications in digital forensics.'}, 'zh': {'title': 'DFIR-Metricï¼šæ•°å­—å–è¯ä¸­çš„è¯­è¨€æ¨¡å‹è¯„ä¼°æ–°æ ‡å‡†', 'desc': 'DFIR-Metric æ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­—å–è¯é¢†åŸŸè¡¨ç°çš„åŸºå‡†å·¥å…·ã€‚å®ƒåŒ…å«ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†ï¼šçŸ¥è¯†è¯„ä¼°ã€ç°å®å–è¯æŒ‘æˆ˜å’Œå®é™…åˆ†ææ¡ˆä¾‹ï¼Œæ—¨åœ¨å…¨é¢æµ‹è¯•æ¨¡å‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹ 14 ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä¼°ï¼Œç ”ç©¶è€…åˆ†æäº†å®ƒä»¬åœ¨å‡†ç¡®æ€§å’Œä¸€è‡´æ€§æ–¹é¢çš„è¡¨ç°ã€‚æ–°å¼•å…¥çš„ä»»åŠ¡ç†è§£åˆ†æ•°ï¼ˆTUSï¼‰å¯ä»¥æ›´æœ‰æ•ˆåœ°è¯„ä¼°æ¨¡å‹åœ¨æ¥è¿‘é›¶å‡†ç¡®ç‡çš„åœºæ™¯ä¸­çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.18657', 'title': 'MLLMs are Deeply Affected by Modality Bias', 'url': 'https://huggingface.co/papers/2505.18657', 'abstract': 'MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have shown promising results in integrating diverse modalities such as texts and images. MLLMs are heavily influenced by modality bias, often relying on language while under-utilizing other modalities like visual inputs. This position paper argues that MLLMs are deeply affected by modality bias. Firstly, we diagnose the current state of modality bias, highlighting its manifestations across various tasks. Secondly, we propose a systematic research road-map related to modality bias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and offer actionable suggestions for future research to mitigate it. To substantiate these findings, we conduct experiments that demonstrate the influence of each factor: 1. Data Characteristics: Language data is compact and abstract, while visual data is redundant and complex, creating an inherent imbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The dominance of pretrained language models in MLLMs leads to overreliance on language and neglect of visual information. 3. Training Objectives: Current objectives often fail to promote balanced cross-modal alignment, resulting in shortcut learning biased toward language. These findings highlight the need for balanced training strategies and model architectures to better integrate multiple modalities in MLLMs. We call for interdisciplinary efforts to tackle these challenges and drive innovation in MLLM research. Our work provides a fresh perspective on modality bias in MLLMs and offers insights for developing more robust and generalizable multimodal systems-advancing progress toward Artificial General Intelligence.', 'score': 3, 'issue_id': 3998, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 Ğ¼Ğ°Ñ', 'en': 'May 24', 'zh': '5æœˆ24æ—¥'}, 'hash': '530ba33aeb74da52', 'authors': ['Xu Zheng', 'Chenfei Liao', 'Yuqian Fu', 'Kaiyu Lei', 'Yuanhuiyi Lyu', 'Lutao Jiang', 'Bin Ren', 'Jialei Chen', 'Jiawen Wang', 'Chengxin Li', 'Linfeng Zhang', 'Danda Pani Paudel', 'Xuanjing Huang', 'Yu-Gang Jiang', 'Nicu Sebe', 'Dacheng Tao', 'Luc Van Gool', 'Xuming Hu'], 'affiliations': ['CSE, HKUST', 'China University of Mining & Technology, Beijing', 'College of Computing & Data Science, Nanyang Technological University', 'Fudan University', 'HKUST(GZ)', 'INSAIT, Sofia University St. Kliment Ohridski', 'Nagoya University', 'SPIC Energy Science and Technology Research Institute', 'Shanghai Jiao Tong University', 'Tongji University', 'University of Pisa, IT', 'University of Trento, IT', 'Xian Jiaotong University'], 'pdf_title_img': 'assets/pdf/title_img/2505.18657.jpg', 'data': {'categories': ['#interpretability', '#training', '#agi', '#multimodal', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… (MLLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¾Ñ‚Ğ´Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°Ğ´ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸: Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½ĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€ĞµĞ´Ğ²Ğ·ÑÑ‚Ğ¾ÑÑ‚Ğ¸ Ğ² MLLM. Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğº Ğ¼ĞµĞ¶Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¼ ÑƒÑĞ¸Ğ»Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼.'}, 'en': {'title': 'Balancing Modalities: Overcoming Bias in MLLMs', 'desc': 'This paper discusses the issue of modality bias in Multimodal Large Language Models (MLLMs), where these models tend to favor language inputs over visual ones. It highlights how this bias affects the integration of different modalities, leading to imbalanced learning and performance across tasks. The authors diagnose the current state of modality bias, propose a research roadmap to address it, and identify key factors contributing to this bias, such as data characteristics and training objectives. They emphasize the need for balanced training strategies and architectures to improve multimodal integration and advance towards more generalizable AI systems.'}, 'zh': {'title': 'å…‹æœæ¨¡æ€åè§ï¼Œå®ç°å¤šæ¨¡æ€å¹³è¡¡æ•´åˆ', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆæ–‡æœ¬å’Œå›¾åƒç­‰ä¸åŒæ¨¡æ€æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬å­˜åœ¨æ¨¡æ€åè§ï¼Œé€šå¸¸æ›´ä¾èµ–è¯­è¨€è€Œå¿½è§†è§†è§‰è¾“å…¥ã€‚è¿™ç§åè§å½±å“äº†å¤šæ¨¡æ€çš„å¹³è¡¡æ•´åˆï¼Œå¯¼è‡´å­¦ä¹ åŠ¨æ€çš„ä¸å¹³è¡¡ã€‚æœ¬æ–‡è¯Šæ–­äº†æ¨¡æ€åè§çš„ç°çŠ¶ï¼Œæå‡ºäº†ç³»ç»Ÿçš„ç ”ç©¶è·¯çº¿å›¾ï¼Œå¹¶è¯†åˆ«äº†å½±å“æ¨¡æ€åè§çš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬å»ºè®®æœªæ¥çš„ç ”ç©¶åº”å…³æ³¨å¹³è¡¡çš„è®­ç»ƒç­–ç•¥å’Œæ¨¡å‹æ¶æ„ï¼Œä»¥ä¿ƒè¿›å¤šæ¨¡æ€çš„æœ‰æ•ˆæ•´åˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21499', 'title': 'AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery', 'url': 'https://huggingface.co/papers/2505.21499', 'abstract': "AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.", 'score': 2, 'issue_id': 3997, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'd5affb6081e51a29', 'authors': ['Haowei Wang', 'Junjie Wang', 'Xiaojun Jia', 'Rupeng Zhang', 'Mingyang Li', 'Zhe Liu', 'Yang Liu', 'Qing Wang'], 'affiliations': ['Institute of Software, Chinese Academy of Sciences, Beijing, China', 'Nanyang Technological University, Singapore', 'State Key Laboratory of Intelligent Game, Beijing, China', 'University of Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.21499.jpg', 'data': {'categories': ['#cv', '#agents', '#security'], 'emoji': 'ğŸ•µï¸', 'ru': {'title': 'Ğ ĞµĞºĞ»Ğ°Ğ¼Ğ° ĞºĞ°Ğº Ğ¾Ñ€ÑƒĞ¶Ğ¸Ğµ: Ğ½Ğ¾Ğ²Ğ°Ñ ÑƒĞ³Ñ€Ğ¾Ğ·Ğ° Ğ´Ğ»Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² ÑĞµÑ‚Ğ¸', 'desc': 'AdInject - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ°Ñ‚Ğ°ĞºĞ¸ Ğ½Ğ° Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹ Ğ´Ğ»Ñ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ñ‡ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‰Ğ¸ĞºĞ°, Ğ±ĞµĞ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ° Ğº Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¾ Ğ½Ğ°Ğ¼ĞµÑ€ĞµĞ½Ğ¸ÑÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ. AdInject Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ¹ Ñ€ĞµĞºĞ»Ğ°Ğ¼Ñ‹ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰ÑƒÑ Ğ² Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… ÑĞ»ÑƒÑ‡Ğ°ÑÑ… 100% ÑƒÑĞ¿ĞµÑ…Ğ°, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° ÑĞµÑ€ÑŒĞµĞ·Ğ½ÑƒÑ ÑƒÑĞ·Ğ²Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµĞ±-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'AdInject: Harnessing Ads for Real-World Attacks on Web Agents', 'desc': "AdInject is a new method that shows how internet advertising can be used to attack vision-language model-based web agents. This method highlights the security weaknesses of these agents when they operate in uncontrolled online environments. Unlike previous attacks that required detailed knowledge of the agent's workings, AdInject works in a black-box setting, making it more applicable to real-world scenarios. The research demonstrates that by cleverly designing malicious ads, attackers can significantly mislead web agents, achieving high success rates in their attacks."}, 'zh': {'title': 'åˆ©ç”¨å¹¿å‘Šæ³¨å…¥æ”»å‡»ç½‘ç»œä»£ç†çš„å®‰å…¨æ¼æ´', 'desc': 'AdInjectæ˜¯ä¸€ç§æ–°é¢–çš„é»‘ç®±æ”»å‡»æ–¹æ³•ï¼Œåˆ©ç”¨äº’è”ç½‘å¹¿å‘Šå‘åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„ç½‘ç»œä»£ç†æ³¨å…¥æ¶æ„å†…å®¹ï¼Œæ˜¾ç¤ºå‡ºç½‘ç»œä»£ç†å®‰å…¨æ€§çš„é‡è¦æ¼æ´ã€‚è¯¥æ–¹æ³•åœ¨æ›´ç°å®çš„å¨èƒæ¨¡å‹ä¸‹è¿è¡Œï¼Œä¸éœ€è¦å¯¹ç”¨æˆ·æ„å›¾çš„å…·ä½“äº†è§£ï¼Œä¸”å‡è®¾ä»£ç†ä¸ºé»‘ç®±ã€‚AdInjecté€šè¿‡è®¾è®¡è¯¯å¯¼æ€§çš„å¹¿å‘Šå†…å®¹ï¼Œè¯±ä½¿ä»£ç†ç‚¹å‡»ï¼Œå¹¶ä½¿ç”¨åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„å¹¿å‘Šå†…å®¹ä¼˜åŒ–æŠ€æœ¯ï¼Œä½¿å¹¿å‘Šå†…å®¹ä¸ç›®æ ‡ç½‘ç«™çš„ä¸Šä¸‹æ–‡æ›´ç›¸å…³ï¼Œä»è€Œæé«˜æ”»å‡»æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAdInjectåœ¨å¤§å¤šæ•°åœºæ™¯ä¸­çš„æ”»å‡»æˆåŠŸç‡è¶…è¿‡60%ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹æ¥è¿‘100%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20286', 'title': 'Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal\n  Predefinition and Maximal Self-Evolution', 'url': 'https://huggingface.co/papers/2505.20286', 'abstract': 'Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce Alita--a generalist agent designed with the principle of "Simplicity is the ultimate sophistication," enabling scalable agentic reasoning through minimal predefinition and maximal self-evolution. For minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. For Maximal self-evolution, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at https://github.com/CharlesQ9/Alita{https://github.com/CharlesQ9/Alita}.', 'score': 2, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'cbcb892a82c3101a', 'authors': ['Jiahao Qiu', 'Xuan Qi', 'Tongcheng Zhang', 'Xinzhe Juan', 'Jiacheng Guo', 'Yifu Lu', 'Yimin Wang', 'Zixin Yao', 'Qihan Ren', 'Xun Jiang', 'Xing Zhou', 'Dongrui Liu', 'Ling Yang', 'Yue Wu', 'Kaixuan Huang', 'Shilong Liu', 'Hongru Wang', 'Mengdi Wang'], 'affiliations': ['AI Lab, Princeton University', 'IIIS, Tsinghua University', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong', 'Tianqiao and Chrissy Chen Institute', 'University of Michigan'], 'pdf_title_img': 'assets/pdf/title_img/2505.20286.jpg', 'data': {'categories': ['#open_source', '#agi', '#agents', '#benchmark', '#reasoning'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ° - ĞºĞ»ÑÑ‡ Ğº ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ñƒ', 'desc': 'ĞĞ»Ğ¸Ñ‚Ğ° - ÑÑ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ñ, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ°Ğ¼Ğ¾Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ»Ñ‹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ĞµĞ¹, Ğ´Ğ»Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ»Ğ¸Ñ‚Ğ° Ğ¾ÑĞ½Ğ°Ñ‰ĞµĞ½Ğ° Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ Ğ½Ğ°Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ‰Ğµ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ². ĞĞ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… GAIA, Mathvista Ğ¸ PathVQA, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¸Ğµ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹.'}, 'en': {'title': "Simplicity Fuels Alita's Generalist Intelligence", 'desc': 'Alita is a generalist agent that excels in various tasks by focusing on simplicity and self-evolution. It minimizes the need for predefined tools, allowing it to adapt and generalize better across different domains. By using task-related model context protocols, Alita can autonomously develop and refine its capabilities, enhancing its problem-solving skills. Its performance on benchmarks like GAIA demonstrates its effectiveness, achieving high accuracy rates compared to more complex systems.'}, 'zh': {'title': 'ç®€çº¦è®¾è®¡ï¼Œå¼ºå¤§æ™ºèƒ½ï¼', 'desc': 'Alitaæ˜¯ä¸€ç§ä»¥ç®€çº¦ä¸ºé©±åŠ¨çš„é€šç”¨æ™ºèƒ½ä½“ï¼Œé€šè¿‡æœ€å°çš„é¢„å®šä¹‰å’Œè‡ªæˆ‘è¿›åŒ–åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é«˜æ€§èƒ½ã€‚ä¸ä¾èµ–æ‰‹åŠ¨é¢„å®šä¹‰å·¥å…·çš„ç°æœ‰æ¡†æ¶ä¸åŒï¼ŒAlitaä»…é…å¤‡ä¸€ä¸ªç›´æ¥è§£å†³é—®é¢˜çš„ç»„ä»¶ï¼Œä½¿å…¶è®¾è®¡æ›´åŠ ç®€æ´ã€‚å®ƒé€šè¿‡ç”Ÿæˆä»»åŠ¡ç›¸å…³çš„æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPsï¼‰æ¥å¢å¼ºè‡ªæˆ‘è¿›åŒ–èƒ½åŠ›ï¼Œä»è€Œå®ç°å¯æ‰©å±•çš„æ™ºèƒ½æ¨ç†ã€‚Alitaåœ¨GAIAåŸºå‡†éªŒè¯æ•°æ®é›†ä¸Šè¾¾åˆ°äº†75.15%çš„pass@1å’Œ87.27%çš„pass@3å‡†ç¡®ç‡ï¼Œè¡¨ç°ä¼˜äºè®¸å¤šå¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19650', 'title': 'Modality Curation: Building Universal Embeddings for Advanced Multimodal\n  Information Retrieval', 'url': 'https://huggingface.co/papers/2505.19650', 'abstract': 'UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.', 'score': 2, 'issue_id': 3997, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'ef6a1b7516769657', 'authors': ['Fanheng Kong', 'Jingyuan Zhang', 'Yahui Liu', 'Hongzhi Zhang', 'Shi Feng', 'Xiaocui Yang', 'Daling Wang', 'Yu Tian', 'Victoria W.', 'Fuzheng Zhang', 'Guorui Zhou'], 'affiliations': ['Kuaishou Technology', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19650.jpg', 'data': {'categories': ['#dataset', '#data', '#benchmark', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'UNITE: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ²Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ UNITE - ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Modal-Aware Masked Contrastive Learning (MAMCL) Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¼ÑĞ³Ñ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºĞ·ĞµĞ¼Ğ¿Ğ»ÑÑ€Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. UNITE Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°.'}, 'en': {'title': 'UNITE: Bridging Modalities for Superior Information Retrieval', 'desc': 'UNITE is a framework designed to improve multimodal information retrieval (MIR) by focusing on data curation and modality-aware training. It addresses the challenges of aligning different types of data, which often have varying characteristics. The framework introduces Modal-Aware Masked Contrastive Learning (MAMCL) to enhance the learning process by reducing conflicts between different modalities. By conducting extensive experiments, UNITE demonstrates significant improvements in retrieval performance across various benchmarks, setting a new standard in the field.'}, 'zh': {'title': 'UNITEï¼šå¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢çš„æ–°çªç ´', 'desc': 'UNITEæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢ï¼ˆMIRï¼‰æŒ‘æˆ˜çš„é€šç”¨æ¡†æ¶ï¼Œä¸»è¦é€šè¿‡æ•°æ®æ•´ç†å’Œæ¨¡æ€æ„ŸçŸ¥è®­ç»ƒæ¥è§£å†³é—®é¢˜ã€‚è¯¥ç ”ç©¶é¦–æ¬¡ç³»ç»Ÿåˆ†æäº†æ¨¡æ€ç‰¹å®šæ•°æ®å±æ€§å¦‚ä½•å½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†æ¨¡æ€æ„ŸçŸ¥æ©è”½å¯¹æ¯”å­¦ä¹ ï¼ˆMAMCLï¼‰ï¼Œä»¥å‡è½»ä¸åŒæ¨¡æ€å®ä¾‹ä¹‹é—´çš„ç«äº‰å…³ç³»ã€‚é€šè¿‡åœ¨å¤šä¸ªå¤šæ¨¡æ€æ£€ç´¢åŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼ŒUNITEå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥å¤šæ¨¡æ€ç³»ç»Ÿçš„ç ”ç©¶æä¾›äº†åŸºç¡€è“å›¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17908', 'title': 'ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback', 'url': 'https://huggingface.co/papers/2505.17908', 'abstract': 'ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to enable robust and scalable general-purpose generation, built on the ComfyUI platform. ComfyMind introduces two core innovations: Semantic Workflow Interface (SWI) that abstracts low-level node graphs into callable functional modules described in natural language, enabling high-level composition and reducing structural errors; Search Tree Planning mechanism with localized feedback execution, which models generation as a hierarchical decision process and allows adaptive correction at each stage. Together, these components improve the stability and flexibility of complex generative workflows. We evaluate ComfyMind on three public benchmarks: ComfyBench, GenEval, and Reason-Edit, which span generation, editing, and reasoning tasks. Results show that ComfyMind consistently outperforms existing open-source baselines and achieves performance comparable to GPT-Image-1. ComfyMind paves a promising path for the development of open-source general-purpose generative AI systems. Project page: https://github.com/LitaoGuo/ComfyMind', 'score': 2, 'issue_id': 3995, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '002e547984f03ef8', 'authors': ['Litao Guo', 'Xinli Xu', 'Luozhou Wang', 'Jiantao Lin', 'Jinsong Zhou', 'Zixin Zhang', 'Bolan Su', 'Ying-Cong Chen'], 'affiliations': ['HKUST(GZ)'], 'pdf_title_img': 'assets/pdf/title_img/2505.17908.jpg', 'data': {'categories': ['#multimodal', '#open_source', '#games', '#training', '#reasoning', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ComfyMind: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'ComfyMind - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°, Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ ComfyUI. ĞĞ½Ğ° Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ˜Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ Ğ°Ğ±Ğ¾Ñ‡ĞµĞ³Ğ¾ ĞŸÑ€Ğ¾Ñ†ĞµÑÑĞ° (SWI) Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ² ÑƒĞ·Ğ»Ğ¾Ğ² Ğ² Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ”ĞµÑ€ĞµĞ²Ğ° ĞŸĞ¾Ğ¸ÑĞºĞ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğº Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹. ComfyMind Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Empowering Generative Workflows with ComfyMind', 'desc': 'ComfyMind is a collaborative AI system that enhances generative workflows by introducing a Semantic Workflow Interface and a Search Tree Planning mechanism. The Semantic Workflow Interface simplifies complex tasks by allowing users to describe workflows in natural language, which reduces errors and improves usability. The Search Tree Planning mechanism organizes the generation process into a hierarchical structure, enabling adaptive corrections and localized feedback during execution. Overall, ComfyMind demonstrates superior performance in generation, editing, and reasoning tasks compared to existing open-source systems, making it a significant advancement in general-purpose generative AI.'}, 'zh': {'title': 'ComfyMindï¼šæå‡ç”Ÿæˆå·¥ä½œæµç¨‹çš„åä½œAIç³»ç»Ÿ', 'desc': 'ComfyMindæ˜¯ä¸€ä¸ªåŸºäºComfyUIçš„åä½œAIç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºç”Ÿæˆå·¥ä½œæµç¨‹ã€‚å®ƒå¼•å…¥äº†è¯­ä¹‰å·¥ä½œæµæ¥å£å’Œæœç´¢æ ‘è§„åˆ’æœºåˆ¶ï¼Œä½¿å¾—ç”Ÿæˆã€ç¼–è¾‘å’Œæ¨ç†ä»»åŠ¡çš„æ€§èƒ½è¶…è¶Šç°æœ‰çš„å¼€æºç³»ç»Ÿã€‚é€šè¿‡å°†ä½çº§èŠ‚ç‚¹å›¾æŠ½è±¡ä¸ºè‡ªç„¶è¯­è¨€æè¿°çš„å¯è°ƒç”¨åŠŸèƒ½æ¨¡å—ï¼ŒComfyMindå‡å°‘äº†ç»“æ„é”™è¯¯å¹¶æé«˜äº†é«˜å±‚æ¬¡çš„ç»„åˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæœç´¢æ ‘è§„åˆ’æœºåˆ¶å…è®¸åœ¨æ¯ä¸ªé˜¶æ®µè¿›è¡Œè‡ªé€‚åº”ä¿®æ­£ï¼Œä»è€Œæé«˜äº†å¤æ‚ç”Ÿæˆå·¥ä½œæµç¨‹çš„ç¨³å®šæ€§å’Œçµæ´»æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16673', 'title': 'R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO', 'url': 'https://huggingface.co/papers/2505.16673', 'abstract': 'Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.  \t\t\t\t\tAI-generated summary \t\t\t\t In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.', 'score': 2, 'issue_id': 3994, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'e90d190149bd97ed', 'authors': ['Huanjin Yao', 'Qixiang Yin', 'Jingyi Zhang', 'Min Yang', 'Yibo Wang', 'Wenhao Wu', 'Fei Su', 'Li Shen', 'Minghui Qiu', 'Dacheng Tao', 'Jiaxing Huang'], 'affiliations': ['Beijing University of Posts and Telecommunications', 'ByteDance', 'Nanyang Technological University', 'The University of Sydney', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16673.jpg', 'data': {'categories': ['#rl', '#training', '#multimodal', '#benchmark', '#optimization', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ MLLM Ñ‡ĞµÑ€ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµĞ¼Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': 'Share-GRPO - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (MLLM). ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. Share-GRPO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ğ³Ğ¾Ğ´. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° ÑˆĞµÑÑ‚Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ… Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°.'}, 'en': {'title': 'Enhancing MLLMs with Share-GRPO: Expanding Questions and Sharing Reasoning', 'desc': 'This paper introduces Share-GRPO, a new reinforcement learning method designed to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). It addresses challenges like sparse rewards and advantage vanishing by expanding the question space and sharing diverse reasoning paths. The approach encourages MLLMs to explore various reasoning trajectories and share insights across different question variants. By hierarchically computing advantages, Share-GRPO enhances the stability of policy training and demonstrates superior performance on multiple reasoning benchmarks.'}, 'zh': {'title': 'Share-GRPOï¼šæå‡å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•Share-GRPOï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡æ‰©å±•é—®é¢˜ç©ºé—´å’Œå…±äº«å¤šæ ·çš„æ¨ç†è½¨è¿¹ï¼ŒShare-GRPOæœ‰æ•ˆåœ°è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¨€ç–å¥–åŠ±å’Œä¼˜åŠ¿æ¶ˆå¤±é—®é¢˜ã€‚è¯¥æ–¹æ³•é¦–å…ˆé€šè¿‡æ•°æ®è½¬æ¢æŠ€æœ¯æ‰©å±•ç»™å®šé—®é¢˜çš„ç©ºé—´ï¼Œç„¶åé¼“åŠ±MLLMåœ¨æ‰©å±•çš„é—®é¢˜ç©ºé—´ä¸­æ¢ç´¢å¤šæ ·çš„æ¨ç†è½¨è¿¹ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­å…±äº«è¿™äº›è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒShare-GRPOåœ¨ä¼˜åŠ¿è®¡ç®—ä¸­å…±äº«å¥–åŠ±ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†ç›¸å¯¹ä¼˜åŠ¿çš„ä¼°è®¡å‡†ç¡®æ€§ï¼Œå¢å¼ºäº†ç­–ç•¥è®­ç»ƒçš„ç¨³å®šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20162', 'title': 'Capability-Based Scaling Laws for LLM Red-Teaming', 'url': 'https://huggingface.co/papers/2505.20162', 'abstract': "Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.", 'score': 1, 'issue_id': 4003, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'f1f248fb099b6803', 'authors': ['Alexander Panfilov', 'Paul Kassianik', 'Maksym Andriushchenko', 'Jonas Geiping'], 'affiliations': ['Cisco Systems Inc.', 'ELLIS Institute TÃ¼bingen', 'EPFL', 'Foundation AI', 'Max Planck Institute for Intelligent Systems', 'TÃ¼bingen AI Center'], 'pdf_title_img': 'assets/pdf/title_img/2505.20162.jpg', 'data': {'categories': ['#rlhf', '#benchmark', '#security', '#alignment'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'ĞŸÑ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ² Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑÑ… - ĞºĞ»ÑÑ‡ Ğº Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ñ‚Ğ°Ğº Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ñ€ĞµĞ·ĞºĞ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ÑÑ, ĞºĞ¾Ğ³Ğ´Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰ĞµĞ³Ğ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 500 Ğ¿Ğ°Ñ€Ğ°Ğ¼Ğ¸ Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğ¹-Ñ†ĞµĞ»ÑŒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ‚Ğ¸Ğ¿Ğ° jailbreak, Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ñ€Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ‚ĞµĞ½Ğ´ĞµĞ½Ñ†Ğ¸Ğ¸: Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑĞ²Ğ»ÑÑÑ‚ÑÑ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ°Ñ‚Ğ°ĞºÑƒÑÑ‰Ğ¸Ğ¼Ğ¸, ÑƒÑĞ¿ĞµÑ… Ğ°Ñ‚Ğ°ĞºĞ¸ Ñ€ĞµĞ·ĞºĞ¾ Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»Ğ¸, Ğ¸ ÑƒÑĞ¿ĞµÑ… Ğ°Ñ‚Ğ°ĞºĞ¸ ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒÑ Ğ½Ğ° ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ°Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ° MMLU-Pro. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°.'}, 'en': {'title': 'Bridging the Capability Gap: Rethinking Red-Teaming for Advanced AI', 'desc': "This paper explores the effectiveness of red-teaming, which is a method used to identify vulnerabilities in large language models (LLMs). It finds that as LLMs become more capable, traditional red-teaming strategies may fail, especially when the target model is stronger than the attacker. The study evaluates over 500 pairs of attackers and targets, revealing that attack success rates drop significantly when the target model's capabilities exceed those of the attacker. The authors propose a scaling law to predict attack success based on the capability gap, emphasizing the need for new strategies to ensure the safe deployment of advanced AI models."}, 'zh': {'title': 'èƒ½åŠ›å·®è·å†³å®šæ”»å‡»æˆåŠŸç‡', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çº¢é˜Ÿæµ‹è¯•ä¸­çš„åº”ç”¨ï¼Œå¼ºè°ƒäº†æ”»å‡»è€…ä¸ç›®æ ‡æ¨¡å‹èƒ½åŠ›å·®è·çš„é‡è¦æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå½“ç›®æ ‡æ¨¡å‹çš„èƒ½åŠ›è¶…è¿‡æ”»å‡»è€…æ—¶ï¼Œæ”»å‡»æˆåŠŸç‡ä¼šæ˜¾è‘—ä¸‹é™ã€‚é€šè¿‡åˆ†æ500å¤šä¸ªæ”»å‡»è€…ä¸ç›®æ ‡å¯¹ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›‘æµ‹å’Œè¯„ä¼°ç­–ç•¥ï¼Œä»¥åº”å¯¹æœªæ¥çš„é£é™©ã€‚ç»“æœè¡¨æ˜ï¼Œèƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹åœ¨æ”»å‡»ä¸­è¡¨ç°æ›´å¥½ï¼Œè€Œå›ºå®šèƒ½åŠ›çš„æ”»å‡»è€…ï¼ˆå¦‚äººç±»ï¼‰å¯èƒ½åœ¨é¢å¯¹æœªæ¥æ¨¡å‹æ—¶å˜å¾—æ— æ•ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19377', 'title': 'Absolute Coordinates Make Motion Generation Easy', 'url': 'https://huggingface.co/papers/2505.19377', 'abstract': 'Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.  \t\t\t\t\tAI-generated summary \t\t\t\t State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications.', 'score': 1, 'issue_id': 3994, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'd9415ccd47a86548', 'authors': ['Zichong Meng', 'Zeyu Han', 'Xiaogang Peng', 'Yiming Xie', 'Huaizu Jiang'], 'affiliations': ['Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.19377.jpg', 'data': {'categories': ['#training', '#multimodal', '#games', '#optimization', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ñ‹ ÑÑƒÑÑ‚Ğ°Ğ²Ğ¾Ğ² Ğ² Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆÑƒÑ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ¶Ğµ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Transformer. ĞŸĞ¾Ğ´Ñ…Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¾Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑ€ÑˆĞ¸Ğ½ Ğ¼ĞµÑˆĞ° SMPL-H Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Revolutionizing Text-to-Motion with Global Coordinates', 'desc': 'This paper introduces a new approach to text-to-motion generation by using absolute joint coordinates in global space instead of the traditional local-relative motion representation. This change enhances motion fidelity, improves text alignment, and allows for better scalability, even when using a simple Transformer model. The authors demonstrate that their method supports various downstream tasks without needing complex reengineering or additional guidance. Overall, this work lays a solid foundation for future advancements in motion generation and related applications.'}, 'zh': {'title': 'å…¨å±€ç»å¯¹åæ ‡æå‡æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆçš„æ•ˆæœ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€ç©ºé—´ä¸­çš„ç»å¯¹å…³èŠ‚åæ ‡æ¥æé«˜è¿åŠ¨çš„çœŸå®æ„Ÿã€æ–‡æœ¬å¯¹é½å’Œå¯æ‰©å±•æ€§ã€‚ä¼ ç»Ÿçš„è¿åŠ¨è¡¨ç¤ºæ–¹æ³•ä¾èµ–äºç›¸å¯¹è¿åŠ¨ï¼Œè™½ç„¶ç®€åŒ–äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯¹æ‰©æ•£æ¨¡å‹çš„åº”ç”¨é€ æˆäº†é™åˆ¶ã€‚é€šè¿‡ç³»ç»Ÿåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†è¿™ç§æ–°çš„è¡¨ç¤ºæ–¹æ³•åœ¨è¿åŠ¨çœŸå®æ„Ÿå’Œæ–‡æœ¬å¯¹é½æ–¹é¢æ˜¾è‘—æå‡ï¼Œä¸”èƒ½å¤Ÿæ”¯æŒä¸‹æ¸¸ä»»åŠ¡ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä»æ–‡æœ¬ç›´æ¥ç”Ÿæˆè¿åŠ¨çš„SMPL-Hç½‘æ ¼é¡¶ç‚¹æ–¹é¢çš„è‰¯å¥½æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œè¿åŠ¨ç›¸å…³åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19235', 'title': 'CoreMatching: A Co-adaptive Sparse Inference Framework with Token and\n  Neuron Pruning for Comprehensive Acceleration of Vision-Language Models', 'url': 'https://huggingface.co/papers/2505.19235', 'abstract': 'A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-Language Models (VLMs) excel across diverse tasks but suffer from high inference costs in time and memory. Token sparsity mitigates inefficiencies in token usage, while neuron sparsity reduces high-dimensional computations, both offering promising solutions to enhance efficiency. Recently, these two sparsity paradigms have evolved largely in parallel, fostering the prevailing assumption that they function independently. However, a fundamental yet underexplored question remains: Do they truly operate in isolation, or is there a deeper underlying interplay that has yet to be uncovered? In this paper, we conduct the first comprehensive investigation into this question. By introducing and analyzing the matching mechanism between Core Neurons and Core Tokens, we found that key neurons and tokens for inference mutually influence and reinforce each other. Building on this insight, we propose CoreMatching, a co-adaptive sparse inference framework, which leverages the synergy between token and neuron sparsity to enhance inference efficiency. Through theoretical analysis and efficiency evaluations, we demonstrate that the proposed method surpasses state-of-the-art baselines on ten image understanding tasks and three hardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs reduction and a 10x overall speedup. Code is released at https://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.', 'score': 1, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': '0d7373d1050c8170', 'authors': ['Qinsi Wang', 'Hancheng Ye', 'Ming-Yu Chung', 'Yudong Liu', 'Yueqian Lin', 'Martin Kuo', 'Mingyuan Ma', 'Jianyi Zhang', 'Yiran Chen'], 'affiliations': ['Department of Electrical and Computer Engineering, Duke University, North Carolina, USA'], 'pdf_title_img': 'assets/pdf/title_img/2505.19235.jpg', 'data': {'categories': ['#architecture', '#inference', '#cv', '#optimization'], 'emoji': 'ğŸš€', 'ru': {'title': 'CoreMatching: ÑĞ¸Ğ½ĞµÑ€Ğ³Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ CoreMatching Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ², Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ Ğ¸Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğµ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ´ĞµÑÑÑ‚Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ñ‚Ñ€ĞµÑ… Ğ°Ğ¿Ğ¿Ğ°Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…. ĞĞ° NVIDIA Titan Xp Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ¾ 5-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ FLOPS Ğ¸ 10-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Unlocking Efficiency: Synergizing Token and Neuron Sparsity in VLMs', 'desc': 'This paper introduces a new framework called CoreMatching that improves the efficiency of vision-language models (VLMs) by combining two types of sparsity: token sparsity and neuron sparsity. Token sparsity helps reduce the number of tokens used, while neuron sparsity minimizes the computations needed for high-dimensional data. The authors explore the interaction between key neurons and tokens, revealing that they influence each other, which leads to better performance. By leveraging this synergy, CoreMatching significantly enhances inference efficiency, achieving remarkable speedups and reductions in computational load across various tasks and devices.'}, 'zh': {'title': 'æ ¸å¿ƒåŒ¹é…ï¼šæå‡è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†æ•ˆç‡çš„å…³é”®', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ ¸å¿ƒåŒ¹é…æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨æ ‡è®°å’Œç¥ç»å…ƒç¨€ç–æ€§ä¹‹é—´çš„ååŒä½œç”¨ï¼Œæé«˜äº†è§†è§‰è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚ä¼ ç»Ÿçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ¨ç†æ—¶æ¶ˆè€—å¤§é‡æ—¶é—´å’Œå†…å­˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œå…³é”®çš„ç¥ç»å…ƒå’Œæ ‡è®°åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›¸äº’å½±å“ï¼Œå¢å¼ºäº†å½¼æ­¤çš„æ•ˆæœã€‚åŸºäºè¿™ä¸€å‘ç°ï¼ŒCoreMatchingæ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆç»“åˆè¿™ä¸¤ç§ç¨€ç–æ€§ï¼Œä»è€Œåœ¨å¤šä¸ªä»»åŠ¡å’Œè®¾å¤‡ä¸Šè¶…è¶Šç°æœ‰çš„åŸºå‡†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19094', 'title': 'SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.19094', 'abstract': 'SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.  \t\t\t\t\tAI-generated summary \t\t\t\t DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.', 'score': 1, 'issue_id': 4005, 'pub_date': '2025-05-25', 'pub_date_card': {'ru': '25 Ğ¼Ğ°Ñ', 'en': 'May 25', 'zh': '5æœˆ25æ—¥'}, 'hash': 'af44c678d01cadc9', 'authors': ['Chuming Shen', 'Wei Wei', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['School of Computer Science & Technology, Huazhong University of Science and Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.19094.jpg', 'data': {'categories': ['#benchmark', '#reasoning', '#multimodal', '#optimization', '#dataset', '#rl'], 'emoji': 'ğŸ”', 'ru': {'title': 'SATORI: ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ VQA Ñ‡ĞµÑ€ĞµĞ· Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»ĞµĞ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'SATORI - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° (VQA), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ VQA Ğ½Ğ° Ñ‚Ñ€Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ»Ğ¾ĞºĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°. SATORI Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ€Ğ°ÑÑĞµĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ 15.7% Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ½Ğ° ÑĞµĞ¼Ğ¸ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… VQA.'}, 'en': {'title': 'SATORI: Enhancing VQA with Structured Stages and Focused Rewards', 'desc': 'The paper introduces SATORI, a method that improves Visual Question Answering (VQA) by breaking it down into three clear stages: global image captioning, region localization, and answer prediction. This structured approach allows for explicit rewards at each stage, which helps the model focus on important areas of the image, thus enhancing accuracy. By addressing the issues of visual focus and policy-gradient variance, SATORI achieves significant performance gains, with improvements of up to 15.7% in accuracy over previous methods. Additionally, the authors provide a new dataset, VQA-Verify, to support the training of their model with annotated captions and bounding boxes.'}, 'zh': {'title': 'SATORIï¼šæå‡è§†è§‰é—®ç­”çš„å…³é”®åŒºåŸŸå…³æ³¨ä¸å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSATORIçš„æ–¹æ³•ï¼Œå°†è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡åˆ†è§£ä¸ºä¸‰ä¸ªå¯éªŒè¯çš„é˜¶æ®µï¼šå…¨å±€å›¾åƒæè¿°ã€åŒºåŸŸå®šä½å’Œç­”æ¡ˆé¢„æµ‹ã€‚æ¯ä¸ªé˜¶æ®µéƒ½æä¾›æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œä»¥å¢å¼ºå¯¹å…³é”®åŒºåŸŸçš„å…³æ³¨å¹¶å‡å°‘ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®ã€‚é€šè¿‡å¼•å…¥VQA-Verifyæ•°æ®é›†ï¼Œæœ¬æ–‡ä¸ºè®­ç»ƒæä¾›äº†å¸¦æœ‰ç­”æ¡ˆå¯¹é½çš„æè¿°å’Œè¾¹ç•Œæ¡†çš„12kæ³¨é‡Šæ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸ƒä¸ªVQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSATORIæ–¹æ³•çš„å‡†ç¡®ç‡æé«˜äº†æœ€å¤š15.7%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17855', 'title': 'Explaining Sources of Uncertainty in Automated Fact-Checking', 'url': 'https://huggingface.co/papers/2505.17855', 'abstract': 'CLUE generates natural language explanations for a language model\'s uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding sources of a model\'s uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes using numerical uncertainty or hedges ("I\'m not sure, but ..."), which do not explain uncertainty that arises from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of model uncertainty by (i) identifying relationships between spans of text that expose claim-evidence or inter-evidence conflicts and agreements that drive the model\'s predictive uncertainty in an unsupervised way, and (ii) generating explanations via prompting and attention steering that verbalize these critical interactions. Across three language models and two fact-checking datasets, we show that CLUE produces explanations that are more faithful to the model\'s uncertainty and more consistent with fact-checking decisions than prompting for uncertainty explanations without span-interaction guidance. Human evaluators judge our explanations to be more helpful, more informative, less redundant, and more logically consistent with the input than this baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and generalises readily to other tasks that require reasoning over complex information.', 'score': 1, 'issue_id': 4001, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '331b47ce3c56f5bf', 'authors': ['Jingyi Sun', 'Greta Warren', 'Irina Shklovski', 'Isabelle Augenstein'], 'affiliations': ['University of Copenhagen'], 'pdf_title_img': 'assets/pdf/title_img/2505.17855.jpg', 'data': {'categories': ['#interpretability', '#multimodal', '#training', '#reasoning', '#data'], 'emoji': 'ğŸ”', 'ru': {'title': 'CLUE: ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CLUE - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ğ½ĞµĞ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ. ĞĞ½ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ½Ñ„Ğ»Ğ¸ĞºÑ‚Ñ‹ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° Ğ½ĞµÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸ÑÑ…. CLUE Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ² Ğ¸ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼, Ğ²ĞµÑ€Ğ±Ğ°Ğ»Ğ¸Ğ·ÑƒÑ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ñ‡Ğ°ÑÑ‚ÑĞ¼Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ CLUE Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹ Ğ¸ Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ñ‹ Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'CLUE: Clear Explanations for Model Uncertainty', 'desc': "CLUE is a framework that generates natural language explanations for a language model's uncertainty by analyzing conflicts and agreements in text spans. It identifies how different pieces of evidence relate to each other, revealing the reasons behind the model's uncertainty in its predictions. This approach enhances the clarity of explanations, making them more useful for tasks like fact-checking. CLUE operates without needing any modifications to the model, allowing it to be easily integrated into existing systems."}, 'zh': {'title': 'CLUEï¼šæ­ç¤ºè¯­è¨€æ¨¡å‹ä¸ç¡®å®šæ€§çš„æ™ºèƒ½è§£é‡Šå·¥å…·', 'desc': 'CLUEæ˜¯ä¸€ä¸ªç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„æ¡†æ¶ï¼Œæ—¨åœ¨æ­ç¤ºè¯­è¨€æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚å®ƒé€šè¿‡è¯†åˆ«æ–‡æœ¬ç‰‡æ®µä¹‹é—´çš„å†²çªå’Œä¸€è‡´æ€§ï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ¨¡å‹çš„é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚ä¸ä»¥å¾€çš„æ•°å€¼ä¸ç¡®å®šæ€§æ–¹æ³•ä¸åŒï¼ŒCLUEèƒ½å¤Ÿæä¾›æ›´æ¸…æ™°çš„è§£é‡Šï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹å®æ ¸æŸ¥ç­‰ä»»åŠ¡ä¸­ã€‚è¯¥æ¡†æ¶æ— éœ€å¾®è°ƒæˆ–æ¶æ„æ›´æ”¹ï¼Œé€‚ç”¨äºä»»ä½•ç™½ç›’è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒå¤æ‚ä¿¡æ¯çš„æ¨ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17190', 'title': 'Tropical Attention: Neural Algorithmic Reasoning for Combinatorial\n  Algorithms', 'url': 'https://huggingface.co/papers/2505.17190', 'abstract': 'Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax.', 'score': 1, 'issue_id': 3997, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': 'e7c2b885aa2f16a0', 'authors': ['Baran Hashemi', 'Kurt Pasque', 'Chris Teska', 'Ruriko Yoshida'], 'affiliations': ['Naval Postgraduate School, Monterey, California', 'Origins Data Science Lab, Technical University of Munich, Munich, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2505.17190.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#architecture', '#security', '#math', '#reasoning'], 'emoji': 'ğŸŒ´', 'ru': {'title': 'Ğ¢Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¾ÑÑ‚Ñ€Ğ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': "Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ 'Ğ¢Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ', ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ÑÑ…ĞµĞ¼Ñ‹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ² Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ softmax Ğ¿Ğ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ¾ÑÑ‚ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ°Ñ‚Ğ°ĞºĞ°Ğ¼."}, 'en': {'title': 'Tropical Attention: Enhancing Algorithmic Reasoning with Sharp Structures', 'desc': 'This paper introduces Tropical attention, a new attention mechanism designed for combinatorial optimization problems that traditionally use dynamic programming. Unlike existing models that use softmax-normalized attention, which can blur important structures in data, Tropical attention operates within the max-plus semiring, preserving the sharp characteristics of value functions. The authors demonstrate that Tropical attention can effectively approximate tropical circuits used in dynamic programming algorithms, leading to improved performance on out-of-distribution tasks. Additionally, they highlight the robustness of Tropical transformers against adversarial attacks, making them a strong alternative to softmax-based models in algorithmic reasoning.'}, 'zh': {'title': 'çƒ­å¸¦æ³¨æ„åŠ›ï¼šæå‡ç®—æ³•æ¨ç†çš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºçƒ­å¸¦æ³¨æ„åŠ›ï¼Œæ—¨åœ¨è§£å†³ç»„åˆä¼˜åŒ–é—®é¢˜ä¸­çš„åŠ¨æ€è§„åˆ’ç®—æ³•ã€‚ä¼ ç»Ÿçš„ç¥ç»ç®—æ³•æ¨ç†æ¨¡å‹ä½¿ç”¨softmaxå½’ä¸€åŒ–çš„ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œè¿™ä¼šæ¨¡ç³Šæ‰é‡è¦çš„å‡ ä½•ç»“æ„ã€‚çƒ­å¸¦æ³¨æ„åŠ›åœ¨çƒ­å¸¦å‡ ä½•çš„æœ€å¤§åŠ æ³•åŠç¯ä¸­åŸç”Ÿæ“ä½œï¼Œèƒ½å¤Ÿæ›´å¥½åœ°è¿‘ä¼¼åŠ¨æ€è§„åˆ’ç±»å‹çš„ç”µè·¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œçƒ­å¸¦å˜æ¢å™¨åœ¨ç®—æ³•æ¨ç†ä»»åŠ¡ä¸­åœ¨é•¿åº¦æ³›åŒ–å’Œä»·å€¼æ³›åŒ–æ–¹é¢çš„è¡¨ç°ä¼˜äºsoftmaxåŸºçº¿ï¼Œå¹¶ä¸”åœ¨å¯¹æŠ—æ”»å‡»ä¸‹ä¿æŒç¨³å®šã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.16340', 'title': 'Improving Chemical Understanding of LLMs via SMILES Parsing', 'url': 'https://huggingface.co/papers/2505.16340', 'abstract': 'Large language models (LLMs) are increasingly recognized as powerful tools for scientific discovery, particularly in molecular science. A fundamental requirement for these models is the ability to accurately understand molecular structures, commonly encoded in the SMILES representation. However, current LLMs struggle to interpret SMILES, even failing to carry out basic tasks such as counting molecular rings. To address this limitation, we introduce CLEANMOL, a novel framework that formulates SMILES parsing into a suite of clean and deterministic tasks explicitly designed to promote graph-level molecular comprehension. These tasks span from subgraph matching to global graph matching, providing structured supervision aligned with molecular structural properties. We construct a molecular pretraining dataset with adaptive difficulty scoring and pre-train open-source LLMs on these tasks. Our results show that CLEANMOL not only enhances structural comprehension but also achieves the best or competes with the baseline on the Mol-Instructions benchmark.', 'score': 1, 'issue_id': 3993, 'pub_date': '2025-05-22', 'pub_date_card': {'ru': '22 Ğ¼Ğ°Ñ', 'en': 'May 22', 'zh': '5æœˆ22æ—¥'}, 'hash': '22065ebe729018b8', 'authors': ['Yunhui Jang', 'Jaehyung Kim', 'Sungsoo Ahn'], 'affiliations': ['KAIST', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2505.16340.jpg', 'data': {'categories': ['#benchmark', '#graphs', '#science', '#dataset', '#open_source', '#data', '#multimodal'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'CLEANMOL: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ CLEANMOL - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). CLEANMOL Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€ SMILES-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ». ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ LLM Ğ½Ğ° ÑÑ‚Ğ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CLEANMOL ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Mol-Instructions.'}, 'en': {'title': 'CLEANMOL: Enhancing LLMs for Molecular Understanding', 'desc': 'This paper presents CLEANMOL, a new framework aimed at improving how large language models (LLMs) understand molecular structures represented in SMILES format. The authors identify that existing LLMs struggle with basic molecular tasks, such as counting rings in molecules. CLEANMOL addresses this by breaking down SMILES parsing into clear, structured tasks that enhance graph-level comprehension of molecular properties. The framework includes a pretraining dataset with varying difficulty levels, leading to improved performance on molecular understanding benchmarks.'}, 'zh': {'title': 'CLEANMOLï¼šæå‡åˆ†å­ç»“æ„ç†è§£çš„åˆ›æ–°æ¡†æ¶', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åˆ†å­ç§‘å­¦çš„ç§‘å­¦å‘ç°ä¸­è¢«è¶Šæ¥è¶Šå¤šåœ°è®¤å¯ä¸ºå¼ºå¤§çš„å·¥å…·ã€‚ä¸ºäº†ä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®ç†è§£åˆ†å­ç»“æ„ï¼Œæˆ‘ä»¬æå‡ºäº†CLEANMOLæ¡†æ¶ï¼Œå°†SMILESè§£æè½¬åŒ–ä¸ºä¸€ç³»åˆ—æ¸…æ™°ä¸”ç¡®å®šçš„ä»»åŠ¡ï¼Œä»¥ä¿ƒè¿›å›¾çº§åˆ†å­ç†è§£ã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­å›¾åŒ¹é…å’Œå…¨å±€å›¾åŒ¹é…ï¼Œæä¾›ä¸åˆ†å­ç»“æ„ç‰¹æ€§ç›¸ä¸€è‡´çš„ç»“æ„åŒ–ç›‘ç£ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒCLEANMOLä¸ä»…å¢å¼ºäº†ç»“æ„ç†è§£èƒ½åŠ›ï¼Œè¿˜åœ¨Mol-InstructionsåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.15561', 'title': 'Do RAG Systems Suffer From Positional Bias?', 'url': 'https://huggingface.co/papers/2505.15561', 'abstract': "Retrieval Augmented Generation enhances LLM accuracy by adding passages retrieved from an external corpus to the LLM prompt. This paper investigates how positional bias - the tendency of LLMs to weight information differently based on its position in the prompt - affects not only the LLM's capability to capitalize on relevant passages, but also its susceptibility to distracting passages. Through extensive experiments on three benchmarks, we show how state-of-the-art retrieval pipelines, while attempting to retrieve relevant passages, systematically bring highly distracting ones to the top ranks, with over 60% of queries containing at least one highly distracting passage among the top-10 retrieved passages. As a result, the impact of the LLM positional bias, which in controlled settings is often reported as very prominent by related works, is actually marginal in real scenarios since both relevant and distracting passages are, in turn, penalized. Indeed, our findings reveal that sophisticated strategies that attempt to rearrange the passages based on LLM positional preferences do not perform better than random shuffling.", 'score': 1, 'issue_id': 3996, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 Ğ¼Ğ°Ñ', 'en': 'May 21', 'zh': '5æœˆ21æ—¥'}, 'hash': 'ca5f3f3552697daa', 'authors': ['Florin Cuconasu', 'Simone Filice', 'Guy Horowitz', 'Yoelle Maarek', 'Fabrizio Silvestri'], 'affiliations': ['Sapienza University of Rome', 'Technology Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2505.15561.jpg', 'data': {'categories': ['#interpretability', '#rag', '#benchmark', '#hallucinations'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ² RAG: Ğ½Ğµ Ñ‚Ğ°Ğº ÑÑ‚Ñ€Ğ°ÑˆĞ½Ğ¾, ĞºĞ°Ğº ĞºĞ°Ğ¶ĞµÑ‚ÑÑ', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Retrieval Augmented Generation. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ¼ĞµÑ‰Ğ°ÑÑ‚ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ²ĞµÑ€Ñ…Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ²Ğ»Ğ¸ÑÑ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ĞµĞµ 60% Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ ĞºĞ°Ğº Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ Ğ¾Ğ´Ğ¸Ğ½ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚Ğ²Ğ»ĞµĞºĞ°ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ‚Ñ€Ñ‹Ğ²Ğ¾Ğº ÑÑ€ĞµĞ´Ğ¸ Ñ‚Ğ¾Ğ¿-10 Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°ÑÑĞ°Ğ¶ĞµĞ¹. Ğ’ Ğ¸Ñ‚Ğ¾Ğ³Ğµ, ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ¿ĞµÑ€ĞµÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚Ñ€Ñ‹Ğ²ĞºĞ¾Ğ² Ğ½Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ² Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼.'}, 'en': {'title': 'Navigating Positional Bias in Retrieval Augmented Generation', 'desc': "This paper explores how Retrieval Augmented Generation (RAG) can improve the accuracy of large language models (LLMs) by incorporating relevant passages from an external corpus. It specifically examines the effect of positional bias, which is how LLMs prioritize information based on its location in the input prompt. The study finds that while retrieval systems aim to provide relevant information, they often inadvertently highlight distracting passages, affecting the LLM's performance. Ultimately, the research shows that attempts to optimize passage arrangement based on positional bias do not yield better results than random ordering, indicating a need for improved retrieval strategies."}, 'zh': {'title': 'ä½ç½®åå·®å½±å“æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ•ˆæœ', 'desc': 'æœ¬è®ºæ–‡ç ”ç©¶äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¦‚ä½•é€šè¿‡å°†å¤–éƒ¨è¯­æ–™åº“ä¸­çš„æ®µè½æ·»åŠ åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æç¤ºä¸­æ¥æé«˜å…¶å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ¢è®¨äº†ä½ç½®åå·®ï¼Œå³LLMæ ¹æ®ä¿¡æ¯åœ¨æç¤ºä¸­çš„ä½ç½®ä¸åŒè€ŒåŠ æƒçš„å€¾å‘ï¼Œå¦‚ä½•å½±å“å…¶åˆ©ç”¨ç›¸å…³æ®µè½çš„èƒ½åŠ›ä»¥åŠå¯¹å¹²æ‰°æ®µè½çš„æ•æ„Ÿæ€§ã€‚é€šè¿‡åœ¨ä¸‰ä¸ªåŸºå‡†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°å°½ç®¡æ£€ç´¢ç®¡é“æ—¨åœ¨è·å–ç›¸å…³æ®µè½ï¼Œä½†å´ç³»ç»Ÿæ€§åœ°å°†é«˜åº¦å¹²æ‰°çš„æ®µè½æ’åœ¨å‰åˆ—ï¼Œè¶…è¿‡60%çš„æŸ¥è¯¢åœ¨å‰10ä¸ªæ£€ç´¢æ®µè½ä¸­åŒ…å«è‡³å°‘ä¸€ä¸ªé«˜åº¦å¹²æ‰°çš„æ®µè½ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œè¯•å›¾æ ¹æ®LLMä½ç½®åå¥½é‡æ–°æ’åˆ—æ®µè½çš„å¤æ‚ç­–ç•¥å¹¶æ²¡æœ‰æ¯”éšæœºæ‰“ä¹±è¡¨ç°æ›´å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.21501', 'title': 'Vision Transformers with Self-Distilled Registers', 'url': 'https://huggingface.co/papers/2505.21501', 'abstract': 'Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly "absorb" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher\'s inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing.', 'score': 0, 'issue_id': 4005, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 Ğ¼Ğ°Ñ', 'en': 'May 27', 'zh': '5æœˆ27æ—¥'}, 'hash': 'e71640ad11c559f5', 'authors': ['Yinjie Chen', 'Zipeng Yan', 'Chong Zhou', 'Bo Dai', 'Andrew F. Luo'], 'affiliations': ['Nanyang Technological University', 'University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.21501.jpg', 'data': {'categories': ['#architecture', '#cv', '#optimization', '#training'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Vision Transformers Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Post Hoc Registers (PH-Reg) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Vision Transformers (ViT). Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ViT Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. PH-Reg Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºÑƒÑ Ğ¸ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºÑƒÑ ÑĞµÑ‚Ğ¸ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ViT, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ½ĞµĞ¸Ğ·Ğ¼ĞµĞ½Ğ½Ñ‹Ğ¼, Ğ° ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ° Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑÑ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸. ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ Ğ°Ñ€Ñ‚ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ² ÑƒÑ‡ĞµĞ½Ğ¸ĞºĞ°.'}, 'en': {'title': 'Enhancing Vision Transformers with Post Hoc Registers', 'desc': "This paper introduces Post Hoc Registers (PH-Reg), a self-distillation technique designed to enhance Vision Transformers (ViTs) by integrating register tokens. These register tokens help to mitigate the issue of artifact tokens that can negatively impact the model's performance in tasks requiring precise localization. The method allows for the incorporation of these tokens into pre-trained ViTs without the need for extensive retraining or additional labeled data. By leveraging a frozen teacher network and optimizing a small subset of the student network's weights, PH-Reg effectively improves segmentation and depth prediction capabilities."}, 'zh': {'title': 'è‡ªè’¸é¦ï¼šæå‡è§†è§‰å˜æ¢å™¨æ€§èƒ½çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPost Hoc Registersï¼ˆPH-Regï¼‰çš„è‡ªè’¸é¦æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°†æ³¨å†Œä»¤ç‰Œé›†æˆåˆ°é¢„è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰ä¸­ï¼Œå‡å°‘ä¼ªå½±ä»¤ç‰Œçš„å½±å“ï¼Œä»è€Œæå‡åˆ†å‰²å’Œæ·±åº¦é¢„æµ‹çš„æ€§èƒ½ã€‚ViTåœ¨è§†è§‰å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼ªå½±ä»¤ç‰Œä¼šå¹²æ‰°å±€éƒ¨è¯­ä¹‰ï¼Œé™ä½æ¨¡å‹åœ¨ç»†ç²’åº¦å®šä½å’Œç»“æ„ä¸€è‡´æ€§ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚PH-Regæ–¹æ³•ä¸éœ€è¦ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ˜¯åˆ©ç”¨ç°æœ‰çš„é¢„è®­ç»ƒViTï¼Œé€šè¿‡æ·»åŠ éšæœºåˆå§‹åŒ–çš„æ³¨å†Œä»¤ç‰Œæ¥ä¼˜åŒ–å­¦ç”Ÿç½‘ç»œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘ä¼ªå½±ä»¤ç‰Œçš„æ•°é‡ï¼Œæå‡æ¨¡å‹åœ¨é›¶æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹ä¸‹çš„è¡¨ç°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20279', 'title': 'VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D\n  Reconstruction', 'url': 'https://huggingface.co/papers/2505.20279', 'abstract': 'VLM-3R, a framework for Vision-Language Models, incorporates 3D reconstructive instruction tuning to process monocular video frames and perform embodied reasoning with robust visual-spatial and temporal contextual understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t The rapid advancement of Large Multimodal Models (LMMs) for 2D images and videos has motivated extending these models to understand 3D scenes, aiming for human-like visual-spatial intelligence. Nevertheless, achieving deep spatial understanding comparable to human capabilities poses significant challenges in model encoding and data acquisition. Existing methods frequently depend on external depth sensors for geometry capture or utilize off-the-shelf algorithms for pre-constructing 3D maps, thereby limiting their scalability, especially with prevalent monocular video inputs and for time-sensitive applications. In this work, we introduce VLM-3R, a unified framework for Vision-Language Models (VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes monocular video frames by employing a geometry encoder to derive implicit 3D tokens that represent spatial understanding. Leveraging our Spatial-Visual-View Fusion and over 200K curated 3D reconstructive instruction tuning question-answer (QA) pairs, VLM-3R effectively aligns real-world spatial context with language instructions. This enables monocular 3D spatial assistance and embodied reasoning. To facilitate the evaluation of temporal reasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark, featuring over 138.6K QA pairs across five distinct tasks focused on evolving spatial relationships. Extensive experiments demonstrate that our model, VLM-3R, not only facilitates robust visual-spatial reasoning but also enables the understanding of temporal 3D context changes, excelling in both accuracy and scalability.', 'score': 0, 'issue_id': 4008, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': 'fb0a1eeef2e6b7ac', 'authors': ['Zhiwen Fan', 'Jian Zhang', 'Renjie Li', 'Junge Zhang', 'Runjin Chen', 'Hezhen Hu', 'Kevin Wang', 'Huaizhi Qu', 'Dilin Wang', 'Zhicheng Yan', 'Hongyu Xu', 'Justin Theiss', 'Tianlong Chen', 'Jiachen Li', 'Zhengzhong Tu', 'Zhangyang Wang', 'Rakesh Ranjan'], 'affiliations': ['Meta', 'TAMU', 'UCR', 'UNC', 'UT Austin', 'XMU'], 'pdf_title_img': 'assets/pdf/title_img/2505.20279.jpg', 'data': {'categories': ['#games', '#benchmark', '#multimodal', '#3d', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'VLM-3R: ĞŸÑ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'VLM-3R - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞ²Ğ½Ñ‹Ñ… 3D-Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 200 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ğ°Ñ€ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ 3D-Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. VLM-3R Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ 3D-ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Empowering Vision-Language Models with 3D Spatial Intelligence', 'desc': 'VLM-3R is a new framework designed for Vision-Language Models that enhances the understanding of 3D scenes from monocular video inputs. It uses 3D reconstructive instruction tuning to create implicit 3D tokens, which help the model grasp spatial relationships and context. By integrating over 200,000 curated question-answer pairs, VLM-3R aligns visual information with language instructions, enabling effective embodied reasoning. The framework also introduces a benchmark for evaluating temporal reasoning, demonstrating improved accuracy and scalability in understanding dynamic spatial contexts.'}, 'zh': {'title': 'VLM-3Rï¼šå®ç°äººç±»èˆ¬çš„è§†è§‰ç©ºé—´æ™ºèƒ½', 'desc': 'VLM-3Ræ˜¯ä¸€ä¸ªè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œç»“åˆäº†3Dé‡å»ºæŒ‡ä»¤è°ƒä¼˜ï¼Œèƒ½å¤Ÿå¤„ç†å•ç›®è§†é¢‘å¸§å¹¶è¿›è¡Œå…·èº«æ¨ç†ã€‚è¯¥æ¨¡å‹é€šè¿‡å‡ ä½•ç¼–ç å™¨æå–éšå¼3Dæ ‡è®°ï¼Œå¢å¼ºäº†å¯¹ç©ºé—´ç†è§£çš„èƒ½åŠ›ã€‚VLM-3Råˆ©ç”¨è¶…è¿‡20ä¸‡ä¸ªç²¾å¿ƒç­–åˆ’çš„3Dé‡å»ºæŒ‡ä»¤é—®ç­”å¯¹ï¼Œæœ‰æ•ˆåœ°å°†ç°å®ä¸–ç•Œçš„ç©ºé—´ä¸Šä¸‹æ–‡ä¸è¯­è¨€æŒ‡ä»¤å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVLM-3Råœ¨è§†è§‰-ç©ºé—´æ¨ç†å’Œæ—¶é—´3Dä¸Šä¸‹æ–‡å˜åŒ–ç†è§£æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è‰¯å¥½çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20052', 'title': 'Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations', 'url': 'https://huggingface.co/papers/2505.20052', 'abstract': 'A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein language models (PLMs) have emerged as powerful tools to detect complex patterns of protein sequences. However, the capability of PLMs to fully capture information on protein sequences might be limited by focusing on single pre-training tasks. Although adding data modalities or supervised objectives can improve the performance of PLMs, pre-training often remains focused on denoising corrupted sequences. To push the boundaries of PLMs, our research investigated a multi-task pre-training strategy. We developed Ankh3, a model jointly optimized on two objectives: masked language modeling with multiple masking probabilities and protein sequence completion relying only on protein sequences as input. This multi-task pre-training demonstrated that PLMs can learn richer and more generalizable representations solely from protein sequences. The results demonstrated improved performance in downstream tasks, such as secondary structure prediction, fluorescence, GB1 fitness, and contact prediction. The integration of multiple tasks gave the model a more comprehensive understanding of protein properties, leading to more robust and accurate predictions.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '74f830b28865c630', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.20052.jpg', 'data': {'categories': ['#training', '#optimization', '#healthcare', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'ĞœĞ½Ğ¾Ğ³Ğ¾Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°ÑĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (PLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ankh3 Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»Ğ¸Ğ» PLM Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ğµ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ»ĞºĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, Ñ„Ğ»ÑƒĞ¾Ñ€ĞµÑÑ†ĞµĞ½Ñ†Ğ¸Ğ¸, Ñ„Ğ¸Ñ‚Ğ½ĞµÑĞ° GB1 Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Unlocking Protein Insights with Multi-Task Learning', 'desc': 'This paper presents a multi-task pre-training strategy for protein language models (PLMs) to enhance their performance on various protein prediction tasks. The proposed model, Ankh3, is optimized on two objectives: masked language modeling with varying masking probabilities and protein sequence completion, both using only protein sequences. By leveraging multiple tasks during pre-training, the model learns richer and more generalizable representations of protein sequences. The results show significant improvements in downstream tasks, indicating that this approach provides a deeper understanding of protein properties, leading to better prediction accuracy.'}, 'zh': {'title': 'å¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œæå‡è›‹ç™½è´¨é¢„æµ‹èƒ½åŠ›', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å¤šä»»åŠ¡é¢„è®­ç»ƒç­–ç•¥ï¼Œç”¨äºæé«˜è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆPLMsï¼‰åœ¨ä¸‹æ¸¸è›‹ç™½è´¨é¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡åŒæ—¶ä¼˜åŒ–å¤šä¸ªç›®æ ‡ï¼Œæ¨¡å‹èƒ½å¤Ÿä»è›‹ç™½è´¨åºåˆ—ä¸­å­¦ä¹ æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚æˆ‘ä»¬å¼€å‘çš„Ankh3æ¨¡å‹ç»“åˆäº†å¤šç§æ©ç è¯­è¨€å»ºæ¨¡å’Œè›‹ç™½è´¨åºåˆ—è¡¥å…¨ä»»åŠ¡ï¼Œå±•ç¤ºäº†PLMsåœ¨ä»…ä¾èµ–è›‹ç™½è´¨åºåˆ—æ—¶çš„å­¦ä¹ èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äºŒçº§ç»“æ„é¢„æµ‹ã€è§å…‰ã€GB1é€‚åº”æ€§å’Œæ¥è§¦é¢„æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.20036', 'title': 'Beyond Simple Concatenation: Fairly Assessing PLM Architectures for\n  Multi-Chain Protein-Protein Interactions Prediction', 'url': 'https://huggingface.co/papers/2505.20036', 'abstract': 'The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatenation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Protein-protein interactions (PPIs) are fundamental to numerous cellular processes, and their characterization is vital for understanding disease mechanisms and guiding drug discovery. While protein language models (PLMs) have demonstrated remarkable success in predicting protein structure and function, their application to sequence-based PPI binding affinity prediction remains relatively underexplored. This gap is often attributed to the scarcity of high-quality, rigorously refined datasets and the reliance on simple strategies for concatenating protein representations. In this work, we address these limitations. First, we introduce a meticulously curated version of the PPB-Affinity dataset of a total of 8,207 unique protein-protein interaction entries, by resolving annotation inconsistencies and duplicate entries for multi-chain protein interactions. This dataset incorporates a stringent, less than or equal to 30%, sequence identity threshold to ensure robust splitting into training, validation, and test sets, minimizing data leakage. Second, we propose and systematically evaluate four architectures for adapting PLMs to PPI binding affinity prediction: embeddings concatenation (EC), sequences concatenation (SC), hierarchical pooling (HP), and pooled attention addition (PAD). These architectures were assessed using two training methods: full fine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM features. Our comprehensive experiments across multiple leading PLMs (ProtT5, ESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures consistently outperform conventional concatenation methods, achieving up to 12% increase in terms of Spearman correlation. These results highlight the necessity of sophisticated architectural designs to fully exploit the capabilities of PLMs for nuanced PPI binding affinity prediction.', 'score': 0, 'issue_id': 4000, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '778086e609851ecb', 'authors': ['Hazem Alsamkary', 'Mohamed Elshaffei', 'Mohamed Soudy', 'Sara Ossman', 'Abdallah Amr', 'Nehal Adel Abdelsalam', 'Mohamed Elkerdawy', 'Ahmed Elnaggar'], 'affiliations': ['Proteinea Inc'], 'pdf_title_img': 'assets/pdf/title_img/2505.20036.jpg', 'data': {'categories': ['#training', '#optimization', '#leakage', '#architecture', '#dataset', '#science'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ»Ğ¾Ğº-Ğ±ĞµĞ»ĞºĞ¾Ğ²Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… PPB-Affinity Ğ¸ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ‡ĞµÑ‚Ñ‹Ñ€Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ»ĞºĞ¾Ğ² Ğº Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ»Ğ¾Ğº-Ğ±ĞµĞ»Ğ¾Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ¾Ğ¼ Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸. ĞĞ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 8207 ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑÑ… Ğ±ĞµĞ»Ğ¾Ğº-Ğ±ĞµĞ»Ğ¾Ğº Ñ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ¾Ğ¼ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ â‰¤30% Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ ÑƒÑ‚ĞµÑ‡ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ğ»Ğ¸ÑÑŒ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ»ĞºĞ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ProtT5, ESM2, Ankh, Ankh2 Ğ¸ ESM3.'}, 'en': {'title': 'Unlocking Protein Interactions: Advanced Architectures for Better Predictions', 'desc': 'This study presents the PPB-Affinity dataset, which contains 8,207 unique entries of protein-protein interactions, refined to eliminate inconsistencies and duplicates. The authors evaluate four different architectural designs for adapting protein language models (PLMs) to predict binding affinity, focusing on hierarchical pooling and pooled attention addition methods. Their experiments show that these advanced architectures significantly outperform traditional concatenation methods, achieving up to a 12% improvement in Spearman correlation. This work emphasizes the importance of using sophisticated model designs to enhance the predictive power of PLMs in understanding protein interactions.'}, 'zh': {'title': 'æå‡è›‹ç™½è´¨ç›¸äº’ä½œç”¨é¢„æµ‹çš„æ¶æ„è®¾è®¡', 'desc': 'æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„PPB-Affinityæ•°æ®é›†ï¼Œå¹¶è¯„ä¼°äº†å››ç§æ¶æ„è®¾è®¡ï¼Œä»¥é€‚åº”è›‹ç™½è´¨è¯­è¨€æ¨¡å‹é¢„æµ‹è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç»“åˆäº²å’ŒåŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå±‚æ¬¡æ± åŒ–å’Œæ± åŒ–æ³¨æ„åŠ›åŠ æ³•æ¶æ„çš„è¡¨ç°ä¼˜äºç®€å•çš„è¿æ¥æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥ä¸¥æ ¼çš„æ•°æ®é›†æ ‡å‡†ï¼Œç¡®ä¿äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†çš„æœ‰æ•ˆåˆ†å‰²ï¼Œå‡å°‘äº†æ•°æ®æ³„æ¼çš„é£é™©ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤æ‚çš„æ¶æ„è®¾è®¡å¯¹äºå……åˆ†åˆ©ç”¨è›‹ç™½è´¨è¯­è¨€æ¨¡å‹åœ¨PPIç»“åˆäº²å’ŒåŠ›é¢„æµ‹ä¸­çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.19954', 'title': 'An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning', 'url': 'https://huggingface.co/papers/2505.19954', 'abstract': "A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.  \t\t\t\t\tAI-generated summary \t\t\t\t The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions.", 'score': 0, 'issue_id': 4001, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 Ğ¼Ğ°Ñ', 'en': 'May 26', 'zh': '5æœˆ26æ—¥'}, 'hash': '3af67df3af862fe0', 'authors': ['Andrew Zamai', 'Nathanael Fijalkow', 'Boris Mansencal', 'Laurent Simon', 'Eloi Navet', 'Pierrick Coupe'], 'affiliations': ['Univ. Bordeaux, CNRS, Bordeaux INP, LaBRI, UMR 5800, F-33400 Talence, France'], 'pdf_title_img': 'assets/pdf/title_img/2505.19954.jpg', 'data': {'categories': ['#3d', '#interpretability', '#cv', '#healthcare', '#multimodal', '#rl', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°: Ğ˜Ğ˜ Ñ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ´ĞµĞ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ´ĞµĞ¼ĞµĞ½Ñ†Ğ¸ÑÑ…. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D ĞœĞ Ğ¢ Ğ¼Ğ¾Ğ·Ğ³Ğ° Ğ² Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹. Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ². ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² LLM, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'Enhancing Diagnostic Clarity with Causally Grounded Explanations', 'desc': "This paper presents a new framework that improves the clarity of deep learning models used for diagnosing neurodegenerative dementias. It combines modular pipelines that convert brain MRIs into textual reports with reinforcement learning to enhance the reasoning capabilities of Large Language Models (LLMs). By generating explanations that are causally linked to neuroimaging data, the framework provides insights into the model's decision-making process. This approach not only maintains high diagnostic accuracy but also offers transparent rationales that assist clinicians in differentiating between various dementia types."}, 'zh': {'title': 'æå‡ç¥ç»é€€è¡Œæ€§ç—´å‘†è¯Šæ–­é€æ˜åº¦çš„æ™ºèƒ½æ¡†æ¶', 'desc': 'æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œç»“åˆæ¨¡å—åŒ–ç®¡é“å’Œå¼ºåŒ–å­¦ä¹ ï¼Œæ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç¥ç»é€€è¡Œæ€§ç—´å‘†è¯Šæ–­ä¸­çš„é€æ˜åº¦ã€‚æˆ‘ä»¬é¦–å…ˆå°†3D T1åŠ æƒè„‘MRIè½¬æ¢ä¸ºæ–‡æœ¬æ”¾å°„å­¦æŠ¥å‘Šï¼Œç„¶ååˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å¸®åŠ©ä¸´åºŠåŒ»ç”Ÿè¿›è¡Œå‰é¢å¶ç—´å‘†äºšå‹ã€é˜¿å°”èŒ¨æµ·é»˜ç—…å’Œæ­£å¸¸è¡°è€çš„é‰´åˆ«è¯Šæ–­ã€‚é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ä»¬é¼“åŠ±è¯­è¨€æ¨¡å‹è¿›è¡Œè¯Šæ–­æ¨ç†ï¼Œä»è€Œç”ŸæˆåŸºäºç¥ç»å½±åƒå­¦å‘ç°çš„ç»“æ„åŒ–è¯Šæ–­ç†ç”±ã€‚ä¸ä¼ ç»Ÿçš„åéªŒå¯è§£é‡Šæ€§æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”Ÿæˆå› æœè§£é‡Šï¼Œæ—¢ä¿æŒäº†æ·±åº¦å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹æ€§èƒ½ï¼Œåˆæä¾›äº†æ”¯æŒè¯Šæ–­ç»“è®ºçš„åˆç†æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2505.17639', 'title': 'PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and\n  Retrieval', 'url': 'https://huggingface.co/papers/2505.17639', 'abstract': 'Mixture-of-experts (MoE) architectures enable scaling large language models (LLMs) to vast parameter counts without a proportional rise in computational costs. However, the significant memory demands of large MoE models hinder their deployment across various computational environments, from cloud servers to consumer devices. This study first demonstrates pronounced task-specific specialization in expert activation patterns within MoE layers. Building on this, we introduce PreMoe, a novel framework that enables efficient deployment of massive MoE models in memory-constrained environments. PreMoe features two main components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER). PEP employs a new metric, the task-conditioned expected selection score (TCESS), derived from router logits to quantify expert importance for specific tasks, thereby identifying a minimal set of critical experts. TAER leverages these task-specific expert importance profiles for efficient inference. It pre-computes and stores compact expert patterns for diverse tasks. When a user query is received, TAER rapidly identifies the most relevant stored task pattern and reconstructs the model by loading only the small subset of experts crucial for that task. This approach dramatically reduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B maintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\% expert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning (87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and 81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64 (390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly available at https://github.com/JarvisPei/PreMoe.', 'score': 0, 'issue_id': 4004, 'pub_date': '2025-05-23', 'pub_date_card': {'ru': '23 Ğ¼Ğ°Ñ', 'en': 'May 23', 'zh': '5æœˆ23æ—¥'}, 'hash': '42ac16b897bccbaa', 'authors': ['Zehua Pei', 'Ying Zhang', 'Hui-Ling Zhen', 'Xianzhi Yu', 'Wulong Liu', 'Sinno Jialin Pan', 'Mingxuan Yuan', 'Bei Yu'], 'affiliations': ['Noahs Ark Lab, Huawei', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.17639.jpg', 'data': {'categories': ['#inference', '#optimization', '#architecture', '#open_source', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¸Ğ³Ğ°Ğ½Ñ‚ÑĞºĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PreMoe - Ğ½Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¼ĞµÑĞ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (MoE) Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. PreMoe Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ½ÑƒÑ Ğ¾Ğ±Ñ€ĞµĞ·ĞºÑƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² (PEP) Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñƒ (TAER). PEP Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ TCESS Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ², Ğ° TAER Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµÑ‚ Ğ¸ Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ PreMoe Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Efficiently Scaling MoE Models with PreMoe', 'desc': 'This paper presents PreMoe, a framework designed to efficiently deploy large Mixture-of-Experts (MoE) models in environments with limited memory. It introduces two key components: probabilistic expert pruning (PEP) and task-adaptive expert retrieval (TAER), which work together to optimize expert selection based on task-specific needs. PEP uses a new metric to determine the importance of experts for particular tasks, allowing for a significant reduction in the number of experts needed. TAER enhances inference speed by pre-computing expert patterns, ensuring that only the most relevant experts are loaded for each user query, thus minimizing memory usage while maintaining high accuracy.'}, 'zh': {'title': 'é«˜æ•ˆéƒ¨ç½²å¤§è§„æ¨¡MoEæ¨¡å‹çš„åˆ›æ–°æ–¹æ¡ˆ', 'desc': 'æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å¯ä»¥åœ¨ä¸æ˜¾è‘—å¢åŠ è®¡ç®—æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‚æ•°æ•°é‡ã€‚ç„¶è€Œï¼Œå¤§å‹MoEæ¨¡å‹çš„å†…å­˜éœ€æ±‚ä½¿å…¶åœ¨å„ç§è®¡ç®—ç¯å¢ƒä¸­çš„éƒ¨ç½²å—åˆ°é™åˆ¶ã€‚æœ¬æ–‡é¦–å…ˆå±•ç¤ºäº†MoEå±‚ä¸­ä¸“å®¶æ¿€æ´»æ¨¡å¼çš„ä»»åŠ¡ç‰¹å®šä¸“ä¸šåŒ–ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PreMoeæ¡†æ¶ï¼Œé€šè¿‡æ¦‚ç‡ä¸“å®¶ä¿®å‰ªï¼ˆPEPï¼‰å’Œä»»åŠ¡è‡ªé€‚åº”ä¸“å®¶æ£€ç´¢ï¼ˆTAERï¼‰æ¥å®ç°å¤§è§„æ¨¡MoEæ¨¡å‹åœ¨å†…å­˜å—é™ç¯å¢ƒä¸­çš„é«˜æ•ˆéƒ¨ç½²ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (10)', '#agi (2)', '#alignment (6)', '#architecture (11)', '#audio (1)', '#benchmark (44)', '#cv (15)', '#data (8)', '#dataset (17)', '#diffusion (6)', '#ethics (1)', '#games (10)', '#graphs (1)', '#hallucinations (3)', '#healthcare (4)', '#inference (9)', '#interpretability (8)', '#leakage (1)', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math (4)', '#multilingual (2)', '#multimodal (27)', '#open_source (13)', '#optimization (30)', '#plp', '#rag (4)', '#reasoning (37)', '#rl (17)', '#rlhf (4)', '#robotics', '#science (7)', '#security (5)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (3)', '#training (26)', '#transfer_learning (1)', '#video (11)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-05-28 21:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-28 21:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-28 21:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    