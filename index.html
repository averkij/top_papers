
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 40 papers. October 29.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">29 октября</span> | <span id="title-articles-count">40 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-10-28.html">⬅️ <span id="prev-date">28.10</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-10-30.html">➡️ <span id="next-date">30.10</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-10.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '29 октября', 'en': 'October 29', 'zh': '10月29日'};
        let feedDateNext = {'ru': '30.10', 'en': '10/30', 'zh': '10月30日'};
        let feedDatePrev = {'ru': '28.10', 'en': '10/28', 'zh': '10月28日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2510.24668', 'title': 'InteractComp: Evaluating Search Agents With Ambiguous Queries', 'url': 'https://huggingface.co/papers/2510.24668', 'abstract': "InteractComp evaluates search agents' ability to recognize and resolve query ambiguity through interaction, revealing significant gaps in current models' capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.", 'score': 82, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '7db93e4c4cf42dec', 'authors': ['Mingyi Deng', 'Lijun Huang', 'Yani Fan', 'Jiayi Zhang', 'Fashen Ren', 'Jinyi Bai', 'Fuzhen Yang', 'Dayi Miao', 'Zhaoyang Yu', 'Yifan Wu', 'Yanfei Zhang', 'Fengwei Teng', 'Yingjia Wan', 'Song Hu', 'Yude Li', 'Xin Jin', 'Conghao Hu', 'Haoyu Li', 'Qirui Fu', 'Tai Zhong', 'Xinyu Wang', 'Xiangru Tang', 'Nan Tang', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['Agent Universe', 'DeepWisdom', 'McGill University', 'Renmin University of China', 'The Hong Kong University of Science and Technology (Guangzhou)', 'University of California, Los Angeles', 'Yale University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24668.jpg', 'data': {'categories': ['#alignment', '#interpretability', '#benchmark', '#agents', '#reasoning'], 'emoji': '❓', 'ru': {'title': 'Поисковые агенты не умеют задавать уточняющие вопросы', 'desc': 'Исследователи представили бенчмарк InteractComp для оценки способности поисковых AI-агентов распознавать неоднозначные запросы и уточнять их через диалог с пользователем. Тестирование 17 моделей показало критический провал: лучшая модель достигла только 13.73% точности при неполном контексте против 71.50% с полной информацией. Проблема заключается не в недостатке рассуждений, а в систематической самоуверенности моделей, которые не осознают необходимость задавать вопросы. При принудительном взаимодействии результаты резко улучшаются, что демонстрирует скрытый потенциал современных LLM, который текущие стратегии не используют.'}, 'en': {'title': "Bridging the Gap: Enhancing Search Agents' Interaction Skills", 'desc': 'InteractComp is a new benchmark that tests how well search agents can identify and clarify ambiguous user queries through interaction. Current models often assume that user queries are clear and complete, which is not the case in real-world scenarios. The benchmark includes 210 carefully designed questions that create genuine ambiguity, requiring agents to engage with users to resolve it. Results show that while search performance has improved, the ability to interact and clarify queries has stagnated, highlighting a significant area for development in search agent capabilities.'}, 'zh': {'title': '提升搜索代理的互动能力', 'desc': 'InteractComp是一个新的基准，用于评估搜索代理识别和解决查询模糊性的能力。现有的搜索模型通常假设用户的查询是完整且明确的，但实际上用户的查询往往是不完整的，需要通过互动来澄清。我们设计了210个专家策划的问题，旨在通过互动来解决真实的模糊性。评估结果显示，尽管搜索性能有所提高，但互动能力却停滞不前，这表明当前策略未能有效利用潜在能力。'}}}, {'id': 'https://huggingface.co/papers/2510.24701', 'title': 'Tongyi DeepResearch Technical Report', 'url': 'https://huggingface.co/papers/2510.24701', 'abstract': "Tongyi DeepResearch, a large language model with agentic capabilities, achieves top performance in various deep research tasks through an end-to-end training framework and automated data synthesis.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.", 'score': 68, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'a6f467c756fc29b7', 'authors': ['Tongyi DeepResearch Team', 'Baixuan Li', 'Bo Zhang', 'Dingchu Zhang', 'Fei Huang', 'Guangyu Li', 'Guoxin Chen', 'Huifeng Yin', 'Jialong Wu', 'Jingren Zhou', 'Kuan Li', 'Liangcai Su', 'Litu Ou', 'Liwen Zhang', 'Pengjun Xie', 'Rui Ye', 'Wenbiao Yin', 'Xinmiao Yu', 'Xinyu Wang', 'Xixi Wu', 'Xuanzhong Chen', 'Yida Zhao', 'Zhen Zhang', 'Zhengwei Tao', 'Zhongwang Zhang', 'Zile Qiao', 'Chenxi Wang', 'Donglei Yu', 'Gang Fu', 'Haiyang Shen', 'Jiayin Yang', 'Jun Lin', 'Junkai Zhang', 'Kui Zeng', 'Li Yang', 'Hailong Yin', 'Maojia Song', 'Ming Yan', 'Peng Xia', 'Qian Xiao', 'Rui Min', 'Ruixue Ding', 'Runnan Fang', 'Shaowei Chen', 'Shen Huang', 'Shihang Wang', 'Shihao Cai', 'Weizhou Shen', 'Xiaobin Wang', 'Xin Guan', 'Xinyu Geng', 'Yingcheng Shi', 'Yuning Wu', 'Zhuo Chen', 'Zijian Li', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24701.jpg', 'data': {'categories': ['#long_context', '#synthetic', '#dataset', '#benchmark', '#training', '#open_source', '#agi', '#agents'], 'emoji': '🔍', 'ru': {'title': 'Автономный AI-агент для глубоких исследований', 'desc': 'Tongyi DeepResearch - это большая языковая модель с агентными способностями, специализирующаяся на долгосрочных исследовательских задачах с глубоким поиском информации. Модель обучается через end-to-end фреймворк, включающий агентное mid-training и post-training, что позволяет масштабируемо рассуждать и искать информацию в сложных задачах. Ключевая особенность - полностью автоматический пайплайн синтеза данных без дорогостоящей человеческой разметки. При 30.5 миллиардах параметров, из которых активируется только 3.3 миллиарда на токен, модель достигает state-of-the-art результатов на множестве бенчмарков для агентных исследований.'}, 'en': {'title': 'Empowering Autonomous Deep Research with Tongyi DeepResearch', 'desc': 'Tongyi DeepResearch is a large language model designed for complex research tasks that require deep information-seeking capabilities. It utilizes an end-to-end training framework that includes both mid-training and post-training phases to enhance its autonomous research abilities. The model features a fully automated data synthesis pipeline, eliminating the need for expensive human annotations, which allows for scalable training across various tasks. With 30.5 billion parameters, Tongyi DeepResearch achieves top performance on multiple benchmarks, demonstrating its effectiveness in deep research applications.'}, 'zh': {'title': '自主深度研究的未来', 'desc': 'Tongyi DeepResearch 是一种具有自主能力的大型语言模型，专门用于长时间的信息检索研究任务。它通过端到端的训练框架和自动化的数据合成，能够在复杂任务中实现可扩展的推理和信息获取。该模型具有 305 亿个参数，且每个令牌仅激活 33 亿个参数，表现出色，达到了多项深度研究基准的最先进水平。我们将模型、框架和完整解决方案开源，以支持社区的发展。'}}}, {'id': 'https://huggingface.co/papers/2510.24699', 'title': 'AgentFold: Long-Horizon Web Agents with Proactive Context Management', 'url': 'https://huggingface.co/papers/2510.24699', 'abstract': "AgentFold, a novel proactive context management paradigm for LLM-based web agents, achieves superior performance on long-horizon tasks through dynamic context folding, surpassing larger models and proprietary agents.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.", 'score': 52, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'c4184e162b0c817e', 'authors': ['Rui Ye', 'Zhongwang Zhang', 'Kuan Li', 'Huifeng Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liangcai Su', 'Liwen Zhang', 'Zile Qiao', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Siheng Chen', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24699.jpg', 'data': {'categories': ['#long_context', '#benchmark', '#training', '#agents'], 'emoji': '🗂️', 'ru': {'title': 'Динамическое сворачивание контекста для эффективных веб-агентов', 'desc': 'AgentFold — это новая парадигма управления контекстом для веб-агентов на основе LLM, вдохновленная процессом ретроспективной консолидации в человеческом познании. Вместо пассивного накопления истории действий, агент активно «складывает» свой контекст, выполняя гранулярное сжатие важных деталей или глубокую консолидацию целых подзадач. Такой подход позволяет избежать переполнения контекста шумом и потери критической информации, что характерно для традиционных ReAct-агентов. Модель AgentFold-30B показывает результаты, превосходящие модели в 20 раз большего размера и proprietary агенты вроде OpenAI o4-mini на бенчмарках длинных задач.'}, 'en': {'title': 'Dynamic Context Management for Superior LLM Performance', 'desc': 'AgentFold is a new approach for managing context in large language model (LLM) web agents, designed to improve their performance on long tasks. It addresses the issue of context saturation found in existing agents by dynamically folding context, which helps retain important details while reducing noise. This method mimics human cognitive processes, allowing the agent to actively manage its historical information rather than just accumulating it. The results show that AgentFold outperforms larger models and proprietary agents on key benchmarks with minimal fine-tuning.'}, 'zh': {'title': 'AgentFold：长任务中的上下文管理新范式', 'desc': 'AgentFold是一种新颖的主动上下文管理范式，专为基于大语言模型（LLM）的网络代理设计。它通过动态上下文折叠技术，在长时间任务中表现优异，超越了更大模型和专有代理。与传统的ReAct代理相比，AgentFold有效解决了上下文饱和的问题，避免了重要细节的丢失。通过将上下文视为动态的认知工作空间，AgentFold能够在多个层面上管理历史轨迹，从而实现更高效的信息处理。'}}}, {'id': 'https://huggingface.co/papers/2510.23763', 'title': 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context', 'url': 'https://huggingface.co/papers/2510.23763', 'abstract': 'RoboOmni, a Perceiver-Thinker-Talker-Executor framework using end-to-end omni-modal LLMs, improves robotic manipulation by inferring user intentions from spoken dialogue, environmental sounds, and visual cues.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.', 'score': 50, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'b37a82d0c1d4c433', 'authors': ['Siyin Wang', 'Jinlan Fu', 'Feihong Liu', 'Xinzhe He', 'Huangxuan Wu', 'Junhao Shi', 'Kexin Huang', 'Zhaoye Fei', 'Jingjing Gong', 'Zuxuan Wu', 'Yugang Jiang', 'See-Kiong Ng', 'Tat-Seng Chua', 'Xipeng Qiu'], 'affiliations': ['Fudan University', 'National University of Singapore', 'Shanghai Innovation Institute'], 'pdf_title_img': 'assets/pdf/title_img/2510.23763.jpg', 'data': {'categories': ['#agents', '#dataset', '#optimization', '#games', '#multimodal', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Робот, который понимает намерения без прямых команд', 'desc': 'Статья представляет RoboOmni — систему для роботизированной манипуляции, которая понимает намерения пользователя из разговоров, звуков окружения и визуальных сигналов, а не из явных команд. Фреймворк основан на omni-modal LLM и объединяет распознавание намерений, подтверждение через диалог и выполнение действий. Авторы создали датасет OmniAction с 140 тысячами эпизодов для обучения проактивному распознаванию намерений. Эксперименты показали, что RoboOmni превосходит baseline-модели по точности, скорости и способности к проактивной помощи.'}, 'en': {'title': 'RoboOmni: Understanding Intentions for Smarter Robot Interaction', 'desc': 'RoboOmni is a new framework that enhances robotic manipulation by understanding user intentions through various inputs like speech, sounds, and visual information. Unlike traditional models that depend on clear instructions, RoboOmni can infer what users want based on context, making it more effective in real-world situations. It combines different types of data using advanced multimodal large language models (MLLMs) to recognize intentions and execute actions seamlessly. The framework is trained on a large dataset called OmniAction, which helps it perform better in both simulated and real environments.'}, 'zh': {'title': 'RoboOmni：智能机器人主动理解用户意图的全新框架', 'desc': 'RoboOmni是一个基于端到端全模态大语言模型的框架，旨在通过推断用户意图来改善机器人操作。该框架结合了语音对话、环境声音和视觉线索，能够在没有明确指令的情况下进行有效的协作。RoboOmni通过跨模态上下文指令来识别意图，并支持直接的语音交互。实验结果表明，RoboOmni在成功率、推理速度和主动协助方面优于传统的文本和自动语音识别基线。'}}}, {'id': 'https://huggingface.co/papers/2510.23691', 'title': 'Game-TARS: Pretrained Foundation Models for Scalable Generalist\n  Multimodal Game Agents', 'url': 'https://huggingface.co/papers/2510.23691', 'abstract': 'Game-TARS, a generalist game agent trained with a unified action space, achieves superior performance across various domains and benchmarks through large-scale pre-training and efficient reasoning strategies.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.', 'score': 43, 'issue_id': 6668, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': '9070d487a826ae6f', 'authors': ['Zihao Wang', 'Xujing Li', 'Yining Ye', 'Junjie Fang', 'Haoming Wang', 'Longxiang Liu', 'Shihao Liang', 'Junting Lu', 'Zhiyong Wu', 'Jiazhan Feng', 'Wanjun Zhong', 'Zili Li', 'Yu Wang', 'Yu Miao', 'Bo Zhou', 'Yuanfan Li', 'Hao Wang', 'Zhongkai Zhao', 'Faming Wu', 'Zhengxuan Jiang', 'Weihao Tan', 'Heyuan Yao', 'Shi Yan', 'Xiangyang Li', 'Yitao Liang', 'Yujia Qin', 'Guang Shi'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2510.23691.jpg', 'data': {'categories': ['#agents', '#training', '#games', '#benchmark', '#multimodal', '#reasoning'], 'emoji': '🎮', 'ru': {'title': 'Универсальный игровой агент через единое пространство действий', 'desc': 'Game-TARS — это универсальный игровой агент, обученный на едином пространстве действий, основанном на нативных действиях клавиатуры и мыши. Модель прошла предобучение на более чем 500 миллиардах токенов с использованием разнородных траекторий из игр, операционных систем и веб-приложений. Ключевые техники включают continual loss с затуханием для снижения каузальной путаницы и стратегию Sparse-Thinking для эффективного рассуждения. Агент показал двукратное превосходство над предыдущими SOTA моделями в задачах Minecraft и превзошёл GPT-5, Gemini-2.5-Pro и Claude-4-Sonnet в FPS бенчмарках.'}, 'en': {'title': 'Game-TARS: A Unified Agent for Diverse Gaming Excellence', 'desc': 'Game-TARS is a versatile game agent designed to operate across different gaming environments using a unified action space that mimics human keyboard and mouse inputs. It undergoes extensive pre-training on a massive dataset of over 500 billion tokens, which includes varied gameplay experiences and multimodal information. The agent employs innovative techniques like a decaying continual loss to minimize confusion in decision-making and a Sparse-Thinking strategy to optimize reasoning efficiency. Experimental results indicate that Game-TARS significantly outperforms previous models in various gaming tasks, showcasing its potential as a generalist agent capable of adapting to diverse computer-based activities.'}, 'zh': {'title': 'Game-TARS：通用游戏代理的未来', 'desc': 'Game-TARS是一种通用游戏代理，采用统一的可扩展动作空间进行训练，能够在多个领域和基准测试中表现出色。它通过大规模的持续预训练，结合多模态数据，提升了在操作系统、网页和模拟游戏等异构领域的性能。关键技术包括逐渐减小的持续损失，以减少因果混淆，以及高效的稀疏思维策略，平衡推理深度和推理成本。实验结果表明，Game-TARS在开放世界的Minecraft任务中成功率是之前最佳模型的两倍，并在未见过的网页3D游戏中接近新手玩家的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.24717', 'title': 'Uniform Discrete Diffusion with Metric Path for Video Generation', 'url': 'https://huggingface.co/papers/2510.24717', 'abstract': 'URSA, a discrete generative model, bridges the gap with continuous approaches in video generation by using a Linearized Metric Path and Resolution-dependent Timestep Shifting, achieving high-resolution and long-duration synthesis with fewer inference steps.  \t\t\t\t\tAI-generated summary \t\t\t\t Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA', 'score': 34, 'issue_id': 6670, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '2191a1dd1f4af127', 'authors': ['Haoge Deng', 'Ting Pan', 'Fan Zhang', 'Yang Liu', 'Zhuoyan Luo', 'Yufeng Cui', 'Wenxuan Wang', 'Chunhua Shen', 'Shiguang Shan', 'Zhaoxiang Zhang', 'Xinlong Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Key Laboratory of Intelligent Information Processing, ICT, CAS', 'National Laboratory of Pattern Recognition, CASIA', 'University of Chinese Academy of Sciences', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24717.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#architecture', '#video'], 'emoji': '🎥', 'ru': {'title': 'URSA: Новая эра дискретной генерации видео', 'desc': 'В статье представлена модель URSA, которая улучшает генерацию видео с использованием дискретных подходов, приближая их к непрерывным методам. URSA использует линейный метрический путь и механизм сдвига временных шагов, что позволяет создавать видео высокого разрешения и длительности с меньшим количеством шагов вывода. Также предложена стратегия асинхронной временной настройки, которая объединяет различные задачи в одной модели, такие как интерполяция и генерация видео из изображений. Эксперименты показывают, что URSA превосходит существующие дискретные методы и достигает результатов, сопоставимых с передовыми непрерывными методами диффузии.'}, 'en': {'title': 'URSA: Bridging Discrete and Continuous Video Generation', 'desc': 'URSA is a new discrete generative model designed to improve video generation by addressing the limitations of traditional discrete methods. It uses a Linearized Metric Path and Resolution-dependent Timestep Shifting to enhance the quality and duration of generated videos while minimizing the number of inference steps needed. The model refines discrete spatiotemporal tokens iteratively, allowing for high-resolution outputs and long video sequences. Additionally, URSA incorporates an asynchronous temporal fine-tuning strategy to handle various tasks like interpolation and image-to-video generation within a single framework.'}, 'zh': {'title': 'URSA：高效的视频生成新方法', 'desc': 'URSA是一种离散生成模型，通过线性度量路径和分辨率依赖的时间步移机制，缩小了视频生成中离散方法与连续方法之间的差距。它将视频生成任务视为离散时空标记的迭代全局优化，从而实现高分辨率和长时长的视频合成。URSA的设计使其在生成高质量图像和长时间视频时，所需的推理步骤显著减少。此外，URSA还引入了一种异步时间微调策略，能够在单一模型中统一多种任务，包括插值和图像到视频的生成。'}}}, {'id': 'https://huggingface.co/papers/2510.24694', 'title': 'Repurposing Synthetic Data for Fine-grained Search Agent Supervision', 'url': 'https://huggingface.co/papers/2510.24694', 'abstract': 'Entity-aware Group Relative Policy Optimization (E-GRPO) enhances search agents by incorporating entity information into the reward function, improving accuracy and efficiency in knowledge-intensive tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative "near-miss" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent\'s reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these "near-misses". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.', 'score': 20, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '57a3b452eab7371c', 'authors': ['Yida Zhao', 'Kuan Li', 'Xixi Wu', 'Liwen Zhang', 'Dingchu Zhang', 'Baixuan Li', 'Maojia Song', 'Zhuo Chen', 'Chenxi Wang', 'Xinyu Wang', 'Kewei Tu', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24694.jpg', 'data': {'categories': ['#training', '#optimization', '#rl', '#agents', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Учимся на «почти правильных» ответах: награды за найденные сущности', 'desc': 'Статья представляет Entity-aware Group Relative Policy Optimization (E-GRPO) — улучшенный метод обучения LLM-агентов для решения задач, требующих поиска и обработки знаний. В отличие от традиционного GRPO, который использует только финальный результат для обучения, E-GRPO учитывает промежуточную информацию об упомянутых сущностях. Метод назначает частичные награды за «почти правильные» ответы пропорционально числу корректно найденных сущностей, что позволяет эффективнее учиться на ошибках. Эксперименты показывают, что E-GRPO превосходит базовый GRPO по точности и эффективности, требуя меньше обращений к инструментам поиска.'}, 'en': {'title': 'Enhancing Search Agents with Entity-Aware Learning', 'desc': 'Entity-aware Group Relative Policy Optimization (E-GRPO) improves the training of search agents by integrating entity information into the reward system, which enhances their performance on complex tasks. Traditional methods like Group Relative Policy Optimization (GRPO) overlook valuable entity data, leading to a loss of learning opportunities from near-miss samples. E-GRPO addresses this by providing partial rewards based on the match rate of identified entities, allowing agents to learn from their mistakes more effectively. Experimental results show that E-GRPO not only boosts accuracy but also promotes more efficient reasoning, requiring fewer resources to achieve better outcomes.'}, 'zh': {'title': '实体感知优化，提升搜索代理的智能！', 'desc': 'E-GRPO（实体感知群体相对策略优化）通过将实体信息纳入奖励函数，增强了搜索代理的能力，从而提高了在知识密集型任务中的准确性和效率。传统的训练方法如GRPO忽视了丰富的实体信息，依赖稀疏的基于结果的奖励，导致无法有效区分有价值的“近乎正确”样本。我们通过利用训练中被丢弃的实体，提出了一种新的密集实体感知奖励函数，使模型能够从这些“近乎正确”的样本中有效学习。实验结果表明，E-GRPO在多种问答和深度研究基准上显著优于GRPO基线，且在提高准确性的同时，减少了工具调用次数，展现出更高效的推理策略。'}}}, {'id': 'https://huggingface.co/papers/2510.24563', 'title': 'OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents', 'url': 'https://huggingface.co/papers/2510.24563', 'abstract': "OSWorld-MCP is a benchmark that evaluates multimodal agents' tool invocation, GUI operation, and decision-making abilities, highlighting the importance of assessing tool usage in real-world scenarios.  \t\t\t\t\tAI-generated summary \t\t\t\t With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.", 'score': 20, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '31a5246f81b64759', 'authors': ['Hongrui Jia', 'Jitong Liao', 'Xi Zhang', 'Haiyang Xu', 'Tianbao Xie', 'Chaoya Jiang', 'Ming Yan', 'Si Liu', 'Wei Ye', 'Fei Huang'], 'affiliations': ['Beijing Zhongguancun Academy', 'Peking University', 'Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24563.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#benchmark', '#optimization', '#open_source', '#agents'], 'emoji': '🛠️', 'ru': {'title': 'Новый стандарт оценки AI-агентов: не только GUI, но и умение пользоваться инструментами', 'desc': 'OSWorld-MCP — это первый комплексный бенчмарк для оценки мультимодальных агентов в реальных условиях, который измеряет способности к вызову инструментов, работе с графическим интерфейсом и принятию решений. Исследователи создали 158 высококачественных инструментов для 7 популярных приложений с помощью автоматизированной генерации кода и ручной проверки. Эксперименты показали, что использование MCP-инструментов улучшает показатели успеха задач для современных моделей, например, с 8.3% до 20.4% для OpenAI o3. Однако даже лучшие модели используют инструменты только в 36.3% случаев, что указывает на значительный потенциал для улучшения.'}, 'en': {'title': 'Revolutionizing Multimodal Agent Evaluation with OSWorld-MCP', 'desc': "OSWorld-MCP is a new benchmark designed to evaluate multimodal agents on their ability to invoke tools, operate GUIs, and make decisions in real-world scenarios. It addresses the gap in previous assessments that primarily focused on GUI interactions, providing a fair comparison by including tool invocation capabilities. The benchmark features a unique automated code-generation pipeline that creates and validates 158 high-quality tools for common applications. Results show that while integrating MCP tools significantly improves task success rates, there is still a need for enhancement in tool invocation rates among leading models, emphasizing the benchmark's importance in advancing multimodal agent evaluation."}, 'zh': {'title': '评估多模态智能体的新标准', 'desc': 'OSWorld-MCP是一个基准测试，旨在评估多模态智能体在工具调用、图形用户界面（GUI）操作和决策能力方面的表现。该研究强调了在真实场景中评估工具使用的重要性，尤其是通过模型上下文协议（MCP）实现的工具调用能力。通过设计自动化代码生成管道，研究团队创建了158个高质量工具，并对其功能和适用性进行了严格验证。评估结果表明，使用MCP工具的智能体在任务成功率上有显著提升，显示出工具调用能力的关键性。'}}}, {'id': 'https://huggingface.co/papers/2510.24657', 'title': 'Group Relative Attention Guidance for Image Editing', 'url': 'https://huggingface.co/papers/2510.24657', 'abstract': "Group Relative Attention Guidance enhances image editing quality by modulating token deltas in Diffusion-in-Transformer models, providing fine-grained control over editing intensity.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.", 'score': 19, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '05adb00252472e87', 'authors': ['Xuanpu Zhang', 'Xuesong Niu', 'Ruidong Chen', 'Dan Song', 'Jianhao Zeng', 'Penghui Du', 'Haoxiang Cao', 'Kai Wu', 'An-an Liu'], 'affiliations': ['Kolors Team, Kuaishou Technology', 'Tianjin University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24657.jpg', 'data': {'categories': ['#architecture', '#open_source', '#training', '#optimization', '#cv', '#diffusion'], 'emoji': '🎚️', 'ru': {'title': 'Точная настройка силы редактирования через модуляцию токенов', 'desc': 'Исследователи предложили метод Group Relative Attention Guidance (GRAG) для улучшения контроля над интенсивностью редактирования изображений в Diffusion-in-Transformer моделях. Они обнаружили, что Query и Key токены в механизме MM-Attention имеют общий bias-вектор, который представляет базовое поведение модели при редактировании. GRAG использует это наблюдение для модуляции дельт между токенами и их bias, что позволяет тонко управлять фокусом модели на исходном изображении относительно инструкции редактирования. Метод интегрируется всего четырьмя строками кода и обеспечивает более плавный и точный контроль по сравнению с классическим Classifier-Free Guidance.'}, 'en': {'title': 'Fine-Tune Your Edits with Group Relative Attention Guidance!', 'desc': "This paper introduces Group Relative Attention Guidance (GRAG), a method that improves image editing quality in Diffusion-in-Transformer models by adjusting token deltas. The authors identify that the Query and Key tokens in the model share a layer-dependent bias, which influences the model's editing behavior. By reweighting the delta values of tokens, GRAG allows for fine-tuned control over the intensity of image edits based on specific instructions. The results show that GRAG can be easily integrated into existing frameworks and provides smoother and more precise editing compared to traditional methods."}, 'zh': {'title': '群体相对注意力引导提升图像编辑质量', 'desc': '本文提出了一种名为群体相对注意力引导（Group Relative Attention Guidance, GRAG）的方法，旨在提高基于扩散-变换器模型的图像编辑质量。通过调节不同标记的增量值，GRAG能够实现对编辑强度的细粒度控制，克服了现有方法在编辑程度上的局限性。研究表明，GRAG可以与现有图像编辑框架轻松集成，并且只需少量代码即可实现。实验结果显示，GRAG在编辑质量上优于常用的无分类器引导方法，提供了更平滑和精确的编辑控制。'}}}, {'id': 'https://huggingface.co/papers/2510.24697', 'title': 'WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling\n  Info-Rich Seeking', 'url': 'https://huggingface.co/papers/2510.24697', 'abstract': 'WebLeaper framework improves information seeking efficiency and effectiveness by constructing high-coverage tasks and generating efficient solution trajectories using tree-structured reasoning and curated Wikipedia tables.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.', 'score': 18, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '4986235d4df6a5c7', 'authors': ['Zhengwei Tao', 'Haiyang Shen', 'Baixuan Li', 'Wenbiao Yin', 'Jialong Wu', 'Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Rui Ye', 'Liwen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24697.jpg', 'data': {'categories': ['#dataset', '#survey', '#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': '🌳', 'ru': {'title': 'WebLeaper: Эффективный поиск информации через древовидное рассуждение', 'desc': 'Статья представляет WebLeaper — фреймворк для улучшения эффективности информационного поиска агентами на основе LLM. Авторы формулируют задачу поиска как древовидную структуру рассуждений, что позволяет охватить больше целевых сущностей в ограниченном контексте. Используя таблицы из Wikipedia, предложены три варианта синтеза задач (Basic, Union и Reverse-Union) для повышения качества поиска. Эксперименты на пяти бенчмарках показали улучшение как точности, так и скорости поиска по сравнению с базовыми методами.'}, 'en': {'title': 'WebLeaper: Boosting Information Seeking Efficiency with Tree-Structured Reasoning', 'desc': 'The WebLeaper framework enhances the efficiency and effectiveness of information seeking (IS) by creating high-coverage tasks and generating efficient solution paths through tree-structured reasoning. It addresses the issue of low search efficiency in current IS agents, which often struggle due to the limited availability of target entities in training tasks. By utilizing curated Wikipedia tables, WebLeaper synthesizes IS tasks in three variants to improve both the efficiency and efficacy of the search process. Extensive experiments show that this approach leads to significant improvements in performance across multiple IS benchmarks.'}, 'zh': {'title': 'WebLeaper：提升信息检索效率与效果的框架', 'desc': 'WebLeaper框架通过构建高覆盖率的信息检索任务和生成高效的解决方案路径，提升了信息检索的效率和效果。该框架将信息检索视为一个树结构推理问题，从而在有限的上下文中嵌入更多的目标实体。通过利用精心策划的维基百科表格，WebLeaper提出了三种合成信息检索任务的变体，以系统性地提高信息检索的效率和有效性。实验结果表明，该方法在多个基准测试中相较于强基线模型在效果和效率上均有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2510.24698', 'title': 'ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking', 'url': 'https://huggingface.co/papers/2510.24698', 'abstract': 'ParallelMuse enhances problem-solving by efficiently reusing paths and compressing reasoning in deep information-seeking agents, improving performance and reducing token consumption.  \t\t\t\t\tAI-generated summary \t\t\t\t Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.', 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '5622c4489fe55493', 'authors': ['Baixuan Li', 'Dingchu Zhang', 'Jialong Wu', 'Wenbiao Yin', 'Zhengwei Tao', 'Yida Zhao', 'Liwen Zhang', 'Haiyang Shen', 'Runnan Fang', 'Pengjun Xie', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24698.jpg', 'data': {'categories': ['#training', '#optimization', '#benchmark', '#agents', '#reasoning'], 'emoji': '🔀', 'ru': {'title': 'Параллельное мышление для AI-агентов: быстрее думаем, меньше тратим', 'desc': 'Исследователи представили ParallelMuse — систему, которая улучшает работу AI-агентов при решении сложных задач, требующих глубокого поиска информации. Метод работает в два этапа: сначала умно переиспользует уже найденные пути решения вместо повторного поиска с нуля, а затем сжимает длинные цепочки рассуждений без потери важной информации. Это позволяет агентам параллельно исследовать больше вариантов решения и лучше их комбинировать при формировании финального ответа. В результате производительность улучшается до 62%, при этом расход токенов снижается на 10-30%.'}, 'en': {'title': 'Enhancing Problem-Solving Efficiency with ParallelMuse', 'desc': 'ParallelMuse is a novel approach that improves the efficiency of deep information-seeking agents by reusing reasoning paths and compressing the information they generate. It addresses the challenges of traditional parallel thinking, which often leads to inefficiencies and difficulties in managing long-term reasoning. The method consists of two main stages: the first enhances exploration by reusing paths based on their functionality, while the second compresses reasoning to streamline the answer generation process. Experiments show that ParallelMuse can significantly boost performance while reducing the number of tokens used during exploration.'}, 'zh': {'title': 'ParallelMuse：高效推理与探索的结合', 'desc': 'ParallelMuse 是一种增强深度信息搜索代理问题解决能力的方法。它通过高效重用路径和压缩推理过程，显著提高了性能并减少了令牌消耗。该方法分为两个阶段：第一阶段通过不确定性引导的路径重用来提高探索效率，第二阶段则利用推理冗余来无损压缩与答案推导相关的信息。实验结果表明，ParallelMuse 在多个开源代理和基准测试中实现了高达62%的性能提升，同时探索性令牌消耗减少了10%到30%。'}}}, {'id': 'https://huggingface.co/papers/2510.24695', 'title': 'AgentFrontier: Expanding the Capability Frontier of LLM Agents with\n  ZPD-Guided Data Synthesis', 'url': 'https://huggingface.co/papers/2510.24695', 'abstract': "A ZPD-guided data synthesis approach enhances large language model capabilities by training them on tasks just beyond their current abilities, leading to state-of-the-art performance on complex benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.", 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '041127f22b52b58f', 'authors': ['Xuanzhong Chen', 'Zile Qiao', 'Guoxin Chen', 'Liangcai Su', 'Zhen Zhang', 'Xinyu Wang', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou', 'Yong Jiang'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2510.24695.jpg', 'data': {'categories': ['#dataset', '#training', '#benchmark', '#synthetic', '#data', '#agents', '#reasoning'], 'emoji': '🎯', 'ru': {'title': 'Обучение AI в зоне ближайшего развития', 'desc': 'Исследователи предложили подход к синтезу данных для обучения LLM, основанный на педагогической концепции зоны ближайшего развития (ZPD). Метод использует автоматический пайплайн AgentFrontier Engine, который генерирует задачи на границе возможностей модели — те, что она не может решить самостоятельно, но способна освоить с помощью. Обученная таким образом модель AgentFrontier-30B-A3B показала результаты уровня state-of-the-art на сложных бенчмарках, превзойдя некоторые проприетарные решения. Подход демонстрирует масштабируемый путь к созданию более способных AI-агентов через целенаправленный синтез обучающих данных.'}, 'en': {'title': 'Unlocking LLM Potential with ZPD-Guided Data Synthesis', 'desc': "This paper presents a novel approach to enhance large language models (LLMs) by using a data synthesis method based on the Zone of Proximal Development (ZPD). The ZPD concept helps identify tasks that LLMs can learn to solve with some guidance, allowing for targeted training on these challenging tasks. The authors introduce the AgentFrontier Engine, which automates the creation of high-quality training data that aligns with the LLM's current capabilities. By applying this method, the trained model achieves state-of-the-art performance on complex benchmarks, demonstrating the effectiveness of ZPD-guided data synthesis in advancing LLM capabilities."}, 'zh': {'title': '基于ZPD的数据合成，提升语言模型能力！', 'desc': '本文提出了一种基于最近发展区（ZPD）理论的数据合成方法，以提升大型语言模型（LLM）的能力。通过训练模型在其能力边界附近的任务，模型能够在复杂基准测试中实现最先进的表现。我们介绍了AgentFrontier引擎，这是一种自动化管道，能够合成高质量的多学科数据，帮助模型在知识密集型数据上进行持续预训练和复杂推理任务的后续训练。我们的实验表明，基于ZPD指导的数据合成方法为构建更强大的LLM代理提供了一条可扩展且有效的路径。'}}}, {'id': 'https://huggingface.co/papers/2510.24693', 'title': 'STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D\n  Intelligence', 'url': 'https://huggingface.co/papers/2510.24693', 'abstract': 'STAR-Bench measures audio 4D intelligence by evaluating sound dynamics in time and 3D space, revealing gaps in fine-grained perceptual reasoning among existing models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.', 'score': 17, 'issue_id': 6669, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'cb1bd8a9c7ac0b88', 'authors': ['Zihan Liu', 'Zhikang Niu', 'Qiuyang Xiao', 'Zhisheng Zheng', 'Ruoqi Yuan', 'Yuhang Zang', 'Yuhang Cao', 'Xiaoyi Dong', 'Jianze Liang', 'Xie Chen', 'Leilei Sun', 'Dahua Lin', 'Jiaqi Wang'], 'affiliations': ['Beihang University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'Shanghai Jiao Tong University', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.24693.jpg', 'data': {'categories': ['#interpretability', '#data', '#audio', '#reasoning', '#benchmark', '#open_source'], 'emoji': '🎧', 'ru': {'title': '4D аудио-интеллект: новый уровень понимания звука в пространстве и времени', 'desc': 'Статья представляет STAR-Bench — новый бенчмарк для оценки «4D аудио-интеллекта» у мультимодальных LLM, то есть способности моделей понимать динамику звука во времени и трёхмерном пространстве. Существующие аудио-бенчмарки тестируют в основном семантику, которую можно восстановить из текстовых описаний, скрывая проблемы с детальным перцептивным анализом. STAR-Bench включает задачи на базовое акустическое восприятие (шесть атрибутов) и комплексное пространственно-временное рассуждение (переупорядочивание сегментов, локализацию, траектории). Тестирование 19 моделей показало значительное отставание от людей: закрытые модели испытывают трудности с детальным восприятием, а открытые отстают во всех аспектах — восприятии, знаниях и рассуждениях.'}, 'en': {'title': 'STAR-Bench: Advancing Audio 4D Intelligence Evaluation', 'desc': 'The paper introduces STAR-Bench, a new benchmark designed to evaluate audio 4D intelligence, which involves understanding sound dynamics over time and in three-dimensional space. It highlights the limitations of existing audio benchmarks that primarily focus on semantic understanding derived from text, thereby neglecting fine-grained perceptual reasoning. STAR-Bench incorporates both foundational acoustic perception and holistic spatio-temporal reasoning tasks, revealing significant performance gaps between current models and human capabilities. The findings indicate that both closed-source and open-source models struggle with fine-grained perception, underscoring the need for improved model development to enhance understanding of complex auditory environments.'}, 'zh': {'title': 'STAR-Bench：音频四维智能的新标准', 'desc': 'STAR-Bench 是一个用于测量音频四维智能的基准，评估声音在时间和三维空间中的动态表现。该研究揭示了现有模型在细粒度感知推理方面的不足，尤其是在多模态大语言模型和大音频语言模型的快速发展背景下。STAR-Bench 结合了基础声学感知和整体时空推理的设置，使用高质量样本来确保评估的准确性。通过对19个模型的评估，发现它们与人类的表现存在显著差距，强调了未来模型在理解物理世界方面的改进方向。'}}}, {'id': 'https://huggingface.co/papers/2510.24514', 'title': 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs', 'url': 'https://huggingface.co/papers/2510.24514', 'abstract': "Latent Sketchpad enhances Multimodal Large Language Models with an internal visual scratchpad, enabling generative visual thought and improved reasoning performance.  \t\t\t\t\tAI-generated summary \t\t\t\t While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.", 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'c64dd82987acd606', 'authors': ['Huanyu Zhang', 'Wenshan Wu', 'Chengzu Li', 'Ning Shang', 'Yan Xia', 'Yangyu Huang', 'Yifan Zhang', 'Li Dong', 'Zhang Zhang', 'Liang Wang', 'Tieniu Tan', 'Furu Wei'], 'affiliations': ['CASIA', 'Cambridge', 'MSR', 'NJU', 'UCAS'], 'pdf_title_img': 'assets/pdf/title_img/2510.24514.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#cv', '#multimodal', '#interpretability'], 'emoji': '✏️', 'ru': {'title': 'Визуальное мышление для AI: когда нейросеть учится рисовать свои мысли', 'desc': 'Статья представляет Latent Sketchpad — фреймворк, который добавляет мультимодальным LLM внутренний визуальный «блокнот для набросков». Подобно тому, как люди используют рисование для визуального мышления, модель может генерировать внутренние визуальные представления в процессе рассуждений. Система включает Context-Aware Vision Head для создания визуальных латентов и Sketch Decoder для их преобразования в понятные изображения. Эксперименты показывают, что такой подход улучшает способность моделей к планированию и рассуждениям в сложных визуальных задачах.'}, 'en': {'title': 'Empowering MLLMs with Visual Thinking through Latent Sketchpad', 'desc': 'Latent Sketchpad is a novel framework that enhances Multimodal Large Language Models (MLLMs) by introducing an internal visual scratchpad for generative visual thought. This approach allows MLLMs to not only understand visual information but also engage in complex visual planning and imagination, similar to human sketching. By integrating visual generation into the autoregressive reasoning process, the model can seamlessly combine text and visual representations, improving its reasoning capabilities. The framework has been evaluated on a new dataset, MazePlanning, demonstrating superior performance across various MLLMs, thus paving the way for improved human-computer interaction.'}, 'zh': {'title': '增强视觉思维，提升推理能力的框架', 'desc': 'Latent Sketchpad 是一种增强多模态大型语言模型（MLLM）的框架，提供了一个内部视觉草图板，支持生成性视觉思维和改进的推理能力。该框架通过将视觉生成直接集成到模型的自回归推理过程中，使得模型能够在文本推理与视觉潜在生成之间交替进行。我们引入了两个关键组件：上下文感知视觉头和预训练的草图解码器，前者生成视觉表示，后者将其转化为人类可理解的图像。实验结果表明，Latent Sketchpad 在推理性能上与基础模型相当，甚至更优，拓展了模型的文本推理能力至视觉思维，开启了更丰富的人机交互和应用机会。'}}}, {'id': 'https://huggingface.co/papers/2510.23642', 'title': 'VisCoder2: Building Multi-Language Visualization Coding Agents', 'url': 'https://huggingface.co/papers/2510.23642', 'abstract': 'VisCoder2, a family of multi-language visualization models, outperforms open-source baselines and approaches proprietary models by leveraging VisCode-Multi-679K and VisPlotBench for iterative self-debugging and multi-turn correction.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.', 'score': 17, 'issue_id': 6668, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': 'ddd9603391399242', 'authors': ['Yuansheng Ni', 'Songcheng Cai', 'Xiangchao Chen', 'Jiarong Liang', 'Zhiheng Lyu', 'Jiaqi Deng', 'Kai Zou', 'Ping Nie', 'Fei Yuan', 'Xiang Yue', 'Wenhu Chen'], 'affiliations': ['Carnegie Mellon University', 'Independent Researcher', 'Korea Advanced Institute of Science & Technology', 'Netmind.ai', 'University of Waterloo'], 'pdf_title_img': 'assets/pdf/title_img/2510.23642.jpg', 'data': {'categories': ['#open_source', '#science', '#dataset', '#multilingual', '#benchmark', '#agents'], 'emoji': '📊', 'ru': {'title': 'Мультиязычный AI-агент для визуализации данных с самоотладкой', 'desc': 'Представлена семья моделей VisCoder2 для генерации кода визуализаций на 12 языках программирования. Модели обучены на датасете VisCode-Multi-679K, содержащем 679 тысяч валидных примеров с диалогами многошаговой коррекции ошибок. Для оценки создан бенчмарк VisPlotBench с исполняемыми задачами и механизмом итеративной самоотладки. VisCoder2 превосходит открытые аналоги и приближается к проприетарным моделям вроде GPT-4.1, достигая 82.4% успешного выполнения кода на масштабе 32B параметров.'}, 'en': {'title': 'Revolutionizing Visualization Code Generation with VisCoder2', 'desc': 'VisCoder2 is a new set of models designed to improve the generation of visualization code across multiple programming languages. It uses a large dataset called VisCode-Multi-679K, which includes 679,000 validated examples and supports multi-turn corrections, allowing for better iterative debugging. The models are evaluated using VisPlotBench, which provides a structured way to assess their performance on both initial code generation and subsequent corrections. Results show that VisCoder2 outperforms existing open-source models and approaches the capabilities of proprietary systems, achieving a high execution pass rate, especially in complex programming scenarios.'}, 'zh': {'title': 'VisCoder2：多语言可视化的未来', 'desc': 'VisCoder2是一种多语言可视化模型家族，通过利用VisCode-Multi-679K和VisPlotBench实现迭代自我调试和多轮修正，超越了开源基准并接近专有模型的性能。该模型解决了现有编码代理在实际工作流程中面临的语言覆盖有限、执行不可靠和缺乏迭代修正机制的问题。我们引入了一个包含679K验证和可执行可视化样本的大规模监督数据集，以及一个系统评估基准，支持初始生成和多轮自我调试。实验结果表明，VisCoder2在执行通过率上达到了82.4%，尤其在符号或依赖编译器的语言中表现突出。'}}}, {'id': 'https://huggingface.co/papers/2510.24711', 'title': 'Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance', 'url': 'https://huggingface.co/papers/2510.24711', 'abstract': 'ProMoE, an MoE framework with conditional and prototypical routing, enhances expert specialization in Diffusion Transformers, achieving state-of-the-art performance on ImageNet.  \t\t\t\t\tAI-generated summary \t\t\t\t Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.', 'score': 16, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '628b0204fe7a35ef', 'authors': ['Yujie Wei', 'Shiwei Zhang', 'Hangjie Yuan', 'Yujin Han', 'Zhekai Chen', 'Jiayu Wang', 'Difan Zou', 'Xihui Liu', 'Yingya Zhang', 'Yu Liu', 'Hongming Shan'], 'affiliations': ['Fudan University', 'MMLab', 'The University of Hong Kong', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24711.jpg', 'data': {'categories': ['#architecture', '#open_source', '#optimization', '#cv', '#benchmark', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'Специализация экспертов в диффузионных моделях через двухэтапную маршрутизацию', 'desc': 'Статья представляет ProMoE — новый подход к использованию Mixture-of-Experts в Diffusion Transformers для генерации изображений. Авторы обнаружили, что визуальные токены отличаются от языковых избыточностью и функциональной неоднородностью, что мешает специализации экспертов. Предложенная двухэтапная маршрутизация сначала разделяет токены на условные и безусловные, а затем использует прототипы для уточнения назначения на основе семантического содержания. ProMoE достигает лучших результатов на ImageNet благодаря контрастивной функции потерь, которая усиливает когерентность внутри экспертов и разнообразие между ними.'}, 'en': {'title': 'ProMoE: Enhancing Expert Specialization in Vision with Smart Routing', 'desc': "ProMoE is a new framework that improves the Mixture-of-Experts (MoE) approach in Diffusion Transformers, focusing on better expert specialization. It introduces a two-step routing mechanism that categorizes image tokens into conditional and unconditional sets, enhancing how experts are utilized based on the tokens' roles. By using prototypical routing, the framework refines the assignment of tokens to experts, ensuring that similar tokens are grouped together for more effective learning. The results show that ProMoE achieves top performance on the ImageNet dataset, demonstrating its effectiveness in visual tasks compared to previous methods."}, 'zh': {'title': 'ProMoE：提升扩散变换器专家专业化的创新框架', 'desc': 'ProMoE是一种混合专家（MoE）框架，旨在提高扩散变换器（Diffusion Transformers）中的专家专业化。该框架采用了条件和原型路由的两步路由机制，能够有效地将图像标记分为条件和无条件集合，从而优化专家的分配。通过原型路由，ProMoE能够根据语义内容对条件图像标记进行精细分配，增强了专家之间的多样性和内部一致性。实验结果表明，ProMoE在ImageNet基准测试中表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2510.24320', 'title': 'Critique-RL: Training Language Models for Critiquing through Two-Stage\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2510.24320', 'abstract': "Critique-RL is an online reinforcement learning approach for developing critiquing language models without strong supervision, using a two-stage optimization strategy to improve both the critic's discriminability and helpfulness.  \t\t\t\t\tAI-generated summary \t\t\t\t Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.", 'score': 16, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '0c26fc5b04f76bff', 'authors': ['Zhiheng Xi', 'Jixuan Huang', 'Xin Guo', 'Boyang Hong', 'Dingwen Yang', 'Xiaoran Fan', 'Shuo Li', 'Zehui Chen', 'Junjie Ye', 'Siyu Yuan', 'Zhengyin Du', 'Xuesong Yao', 'Yufei Xu', 'Jiecao Chen', 'Rui Zheng', 'Tao Gui', 'Qi Zhang', 'Xuanjing Huang'], 'affiliations': ['ByteDance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24320.jpg', 'data': {'categories': ['#rl', '#reasoning', '#optimization', '#training', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Critique-RL: Улучшение языковых моделей без сильного надзора', 'desc': 'Critique-RL — это метод обучения с подкреплением для создания языковых моделей, которые могут критиковать без сильного надзора. Он использует двухэтапную стратегию оптимизации, чтобы улучшить как способность критика различать, так и его полезность. В первом этапе усиливается способность критика различать с помощью прямых сигналов вознаграждения, а во втором этапе вводятся косвенные вознаграждения для улучшения полезности критика. Эксперименты показывают, что Critique-RL значительно улучшает производительность моделей на различных задачах.'}, 'en': {'title': 'Empowering Language Models with Self-Supervised Critique', 'desc': "Critique-RL is a novel online reinforcement learning method designed to enhance critiquing language models without the need for strong supervision. It employs a two-stage optimization strategy where an actor generates responses and a critic evaluates them, allowing for iterative refinement. The first stage focuses on improving the critic's ability to distinguish high-quality responses using direct reward signals, while the second stage enhances the critic's helpfulness through indirect rewards based on the actor's improvements. Experimental results demonstrate that Critique-RL significantly boosts performance, achieving notable gains in both in-domain and out-of-domain tasks."}, 'zh': {'title': 'Critique-RL：无强监督的评估语言模型优化方法', 'desc': 'Critique-RL是一种在线强化学习方法，用于在没有强监督的情况下开发评估语言模型。该方法采用两阶段优化策略，旨在提高评估者的区分能力和有用性。在第一阶段，通过直接的基于规则的奖励信号来增强评估者的区分能力；在第二阶段，基于生成者的反馈引入间接奖励，以提高评估者的有用性，同时通过适当的正则化保持其区分能力。实验结果表明，Critique-RL在多个任务和模型上显著提升了性能，展示了其潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.21978', 'title': 'Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in\n  Large Reasoning Models', 'url': 'https://huggingface.co/papers/2510.21978', 'abstract': 'RECAP, a dynamic objective reweighting strategy, enhances reinforcement learning with verifiable rewards by preserving general knowledge and improving reasoning through flexible reward trade-offs.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.', 'score': 12, 'issue_id': 6684, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '5ed5e8a9d936f8c4', 'authors': ['Hoang Phan', 'Xianjun Yang', 'Kevin Yao', 'Jingyu Zhang', 'Shengjie Bi', 'Xiaocheng Tang', 'Madian Khabsa', 'Lijuan Liu', 'Deren Lei'], 'affiliations': ['Johns Hopkins University', 'Meta Superintelligence Labs', 'New York University'], 'pdf_title_img': 'assets/pdf/title_img/2510.21978.jpg', 'data': {'categories': ['#optimization', '#training', '#benchmark', '#reasoning', '#rl', '#rlhf'], 'emoji': '⚖️', 'ru': {'title': 'Динамическое перевзвешивание целей для сохранения знаний при обучении с подкреплением', 'desc': 'Статья представляет RECAP — метод для решения проблемы деградации базовых навыков моделей при reinforcement learning с верифицируемыми наградами (RLVR). Традиционные подходы с KL-дивергенцией и experience replay не гарантируют сохранение широких знаний модели. RECAP динамически перевзвешивает задачи обучения на основе краткосрочных сигналов сходимости, смещая фокус от насыщенных целей к проблемным. Эксперименты на моделях Qwen2.5-VL показали, что метод сохраняет общие способности моделей и улучшает качество рассуждений.'}, 'en': {'title': 'Dynamic Reweighting for Enhanced Reinforcement Learning', 'desc': 'RECAP is a novel strategy designed to enhance reinforcement learning with verifiable rewards by dynamically adjusting the focus on different training objectives. It addresses the issue of capability regression, where models lose foundational skills during extended training periods. By implementing a flexible reward trade-off system, RECAP ensures that models maintain general knowledge while improving their reasoning abilities. The method is easy to integrate into existing reinforcement learning frameworks, requiring no additional model training or extensive tuning.'}, 'zh': {'title': 'RECAP：动态重加权，提升强化学习推理能力', 'desc': 'RECAP是一种动态目标重加权策略，旨在通过灵活的奖励权衡来增强强化学习中的可验证奖励。该方法解决了在长时间训练中模型能力退化的问题，确保模型保留基础知识并改善推理能力。通过在线适应的重加权机制，RECAP能够根据收敛和不稳定的短期信号调整训练重点，从而避免过度关注饱和目标。实验结果表明，RECAP不仅能保持模型的通用能力，还能通过更灵活的任务奖励权衡来提升推理效果。'}}}, {'id': 'https://huggingface.co/papers/2510.22037', 'title': 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,\n  Finetuning, and Decoding the Curse of Multilinguality', 'url': 'https://huggingface.co/papers/2510.22037', 'abstract': "The study introduces ATLAS, a multilingual scaling law that improves out-of-sample generalization and provides insights into cross-lingual transfer, optimal scaling, and computational crossover points for model training.  \t\t\t\t\tAI-generated summary \t\t\t\t Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.", 'score': 11, 'issue_id': 6669, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '23b87c7cd2ed6296', 'authors': ['Shayne Longpre', 'Sneha Kudugunta', 'Niklas Muennighoff', 'I-Hung Hsu', 'Isaac Caswell', 'Alex Pentland', 'Sercan Arik', 'Chen-Yu Lee', 'Sayna Ebrahimi'], 'affiliations': ['Google Cloud AI', 'Google DeepMind', 'MIT', 'Stanford University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2510.22037.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#transfer_learning', '#dataset', '#training'], 'emoji': '🌍', 'ru': {'title': 'Законы масштабирования для многоязычного AI', 'desc': 'ATLAS — это новый закон масштабирования для многоязычных языковых моделей, основанный на 774 экспериментах с моделями от 10M до 8B параметров и более чем 400 языками. Исследование показывает, как языки передают знания друг другу, создавая матрицу взаимной пользы для 1444 языковых пар. ATLAS помогает определить оптимальное соотношение размера модели и данных при добавлении новых языков, а также точки, когда выгоднее обучать модель с нуля или дообучать существующую. Работа закладывает научную основу для демократизации AI за пределами англоцентричного подхода.'}, 'en': {'title': 'Unlocking Multilingual AI: The ATLAS Approach', 'desc': 'The paper presents ATLAS, a new multilingual scaling law designed to enhance out-of-sample generalization in AI models. It is based on extensive research involving 774 multilingual training experiments across a wide range of model sizes and languages. The study reveals important insights into cross-lingual transfer dynamics and provides a framework for optimal scaling of model size and data when incorporating multiple languages. Additionally, it identifies key points for deciding between pretraining from scratch or fine-tuning existing multilingual models, aiming to make AI more accessible for diverse languages.'}, 'zh': {'title': 'ATLAS：多语言扩展法则的创新', 'desc': '本研究介绍了ATLAS，一个多语言扩展法则，旨在提高模型的外部样本泛化能力，并提供跨语言迁移、最佳扩展和模型训练的计算交叉点的见解。我们进行了774个多语言训练实验，涵盖了10M到8B的模型参数，400多种训练语言和48种评估语言。ATLAS在单语言和多语言预训练中表现优于现有的扩展法则，外部样本泛化能力提高了0.3 R^2以上。我们的分析揭示了多语言学习动态、语言间的迁移特性以及多语言的挑战，为跨语言的扩展法则提供了科学基础。'}}}, {'id': 'https://huggingface.co/papers/2510.24702', 'title': 'Agent Data Protocol: Unifying Datasets for Diverse, Effective\n  Fine-tuning of LLM Agents', 'url': 'https://huggingface.co/papers/2510.24702', 'abstract': 'The agent data protocol (ADP) standardizes diverse agent training datasets, enabling improved performance across various tasks without domain-specific tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an "interlingua" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.', 'score': 10, 'issue_id': 6680, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '0143eedf3ffbbc3a', 'authors': ['Yueqi Song', 'Ketan Ramaneti', 'Zaid Sheikh', 'Ziru Chen', 'Boyu Gou', 'Tianbao Xie', 'Yiheng Xu', 'Danyang Zhang', 'Apurva Gandhi', 'Fan Yang', 'Joseph Liu', 'Tianyue Ou', 'Zhihao Yuan', 'Frank Xu', 'Shuyan Zhou', 'Xingyao Wang', 'Xiang Yue', 'Tao Yu', 'Huan Sun', 'Yu Su', 'Graham Neubig'], 'affiliations': ['All Hands AI', 'Carnegie Mellon University', 'Duke University', 'Fujitsu Research', 'The Ohio State University', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2510.24702.jpg', 'data': {'categories': ['#open_source', '#agents', '#training', '#dataset', '#optimization', '#benchmark'], 'emoji': '🔗', 'ru': {'title': 'Единый язык данных для обучения AI-агентов', 'desc': 'Исследователи представили Agent Data Protocol (ADP) — универсальный формат для стандартизации разнородных датасетов для обучения AI-агентов. Проблема заключалась не в отсутствии данных, а в их фрагментации по различным форматам и интерфейсам. ADP работает как «межъязык», позволяя объединить данные из разных источников для единого процесса обучения агентов, охватывая задачи от использования API до программирования. Supervised finetuning на 13 объединённых датасетах показал прирост производительности ~20% по сравнению с базовыми моделями, достигая state-of-the-art результатов без специальной настройки под домен.'}, 'en': {'title': 'Standardizing Agent Training for Better Performance', 'desc': 'The paper introduces the Agent Data Protocol (ADP), which standardizes various agent training datasets to enhance performance across multiple tasks without needing specific adjustments for each domain. It identifies that the main issue in agent training is not the lack of data, but rather the fragmentation of data across different formats and tools. ADP acts as a universal language that connects these diverse datasets, making it easier to train agents on a wide range of tasks like coding and browsing. Experiments show that using ADP leads to an average performance improvement of about 20% on various benchmarks, promoting more efficient and reproducible agent training.'}, 'zh': {'title': '代理数据协议：提升多任务性能的关键', 'desc': '本文提出了代理数据协议（ADP），旨在标准化不同格式的代理训练数据集，从而提高多任务的性能。我们认为，数据碎片化是训练代理的主要瓶颈，而不是数据源的缺乏。ADP作为一种轻量级的表示语言，可以在多种格式的代理数据集之间充当“中介”，并简化下游的训练流程。实验表明，使用ADP格式统一的13个现有代理训练数据集，经过微调后，模型性能平均提升约20%，在多个标准基准上达到了最先进的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.17439', 'title': 'From Spatial to Actions: Grounding Vision-Language-Action Model in\n  Spatial Foundation Priors', 'url': 'https://huggingface.co/papers/2510.17439', 'abstract': 'FALCON enhances vision-language-action models by integrating rich 3D spatial tokens into the action head, improving spatial reasoning and modality transferability.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.', 'score': 10, 'issue_id': 6677, 'pub_date': '2025-10-20', 'pub_date_card': {'ru': '20 октября', 'en': 'October 20', 'zh': '10月20日'}, 'hash': '11b29adeb1a77d6e', 'authors': ['Zhengshen Zhang', 'Hao Li', 'Yalun Dai', 'Zhengbang Zhu', 'Lei Zhou', 'Chenchen Liu', 'Dong Wang', 'Francis E. H. Tay', 'Sijin Chen', 'Ziwei Liu', 'Yuxiao Liu', 'Xinghang Li', 'Pan Zhou'], 'affiliations': ['ByteDance Seed', 'NTU', 'NUS', 'SMU', 'THU'], 'pdf_title_img': 'assets/pdf/title_img/2510.17439.jpg', 'data': {'categories': ['#3d', '#reasoning', '#architecture', '#benchmark', '#alignment', '#transfer_learning', '#multimodal'], 'emoji': '🧠', 'ru': {'title': 'FALCON: Прорыв в пространственном мышлении для VLA моделей', 'desc': 'В статье представлена новая модель FALCON, которая улучшает пространственное мышление в системах Vision-Language-Action (VLA) за счёт интеграции 3D пространственных токенов. Это позволяет моделям лучше адаптироваться к реальному миру, где требуется понимание трёхмерного пространства. FALCON использует пространственные модели, которые обеспечивают сильные геометрические приоритеты, и может работать с дополнительной информацией о глубине или позе без необходимости в переобучении. В результате, FALCON демонстрирует высокую производительность в различных задачах и условиях, превосходя существующие аналоги.'}, 'en': {'title': 'FALCON: Bridging 3D Spatial Understanding with Action in VLA Models', 'desc': 'FALCON is a new approach that improves vision-language-action (VLA) models by incorporating detailed 3D spatial tokens into the action decision-making process. This integration helps the models better understand spatial relationships and enhances their ability to transfer knowledge across different modalities. Unlike previous methods that struggled with spatial reasoning or required complex setups, FALCON uses spatial foundation models to extract geometric information from standard RGB images. The design allows for improved performance in various tasks without needing extensive retraining, making it more adaptable and effective in real-world scenarios.'}, 'zh': {'title': 'FALCON：提升空间推理与模态转移的创新模型', 'desc': 'FALCON是一种新颖的视觉-语言-动作模型，旨在通过将丰富的3D空间标记整合到动作头中来增强空间推理能力和模态可转移性。现有的视觉-语言-动作模型通常基于2D编码器，导致空间推理存在差距，限制了模型的泛化和适应能力。FALCON利用空间基础模型，从RGB图像中提取强几何先验，并通过空间增强动作头处理空间标记，以保持语言推理的完整性。经过在多个模拟基准和真实任务中的全面评估，FALCON在性能上超越了竞争对手，展现出卓越的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2510.20661', 'title': 'UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale\n  High-Quality Dataset', 'url': 'https://huggingface.co/papers/2510.20661', 'abstract': 'A new dataset and frequency-aware post-training method improve fine-grained detail synthesis in ultra-high-resolution text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce UltraHR-100K, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning on detail-critical denoising steps, and (ii) Soft-Weighting Frequency Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at https://github.com/NJU-PCALab/UltraHR-100k{here}.', 'score': 9, 'issue_id': 6678, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': 'b8ec9bcd7c4829c5', 'authors': ['Chen Zhao', 'En Ci', 'Yunzhe Xu', 'Tiehan Fan', 'Shanyan Guan', 'Yanhao Ge', 'Jian Yang', 'Ying Tai'], 'affiliations': ['State Key Laboratory of Novel Software Technology, Nanjing University, China', 'vivo Mobile Communication Co., Ltd., China'], 'pdf_title_img': 'assets/pdf/title_img/2510.20661.jpg', 'data': {'categories': ['#training', '#benchmark', '#synthetic', '#dataset', '#diffusion', '#cv'], 'emoji': '🔬', 'ru': {'title': 'Совершенство в деталях: частотная настройка для сверхвысокого разрешения', 'desc': 'Статья представляет новый датасет UltraHR-100K из 100 тысяч ультравысокого разрешения изображений (более 3K) с детальными описаниями для обучения text-to-image моделей. Авторы предлагают метод post-training, который улучшает генерацию мелких деталей через специальную выборку временных шагов (DOTS) и регуляризацию частотных компонентов с помощью преобразования Фурье (SWFR). Подход позволяет diffusion моделям лучше сохранять высокочастотные детали, что критично для ультравысокого разрешения. Эксперименты показывают значительное улучшение качества детализации и общей точности генерируемых изображений.'}, 'en': {'title': 'Enhancing Ultra-High-Resolution Image Generation with New Dataset and Training Techniques', 'desc': 'This paper presents a new dataset called UltraHR-100K, which contains 100,000 ultra-high-resolution images with detailed captions, aimed at improving text-to-image (T2I) generation. The authors address two main challenges in T2I models: the lack of a high-quality dataset and the need for better training methods for fine details. They introduce a frequency-aware post-training technique that includes Detail-Oriented Timestep Sampling (DOTS) and Soft-Weighting Frequency Regularization (SWFR) to enhance detail synthesis. Experimental results show that their methods significantly improve the quality and fidelity of generated images in ultra-high resolutions.'}, 'zh': {'title': '提升超高分辨率图像生成的细节质量', 'desc': '本文提出了一种新的数据集和后训练方法，以改善超高分辨率文本到图像生成模型中的细节合成。我们引入了UltraHR-100K数据集，包含10万张高质量的超高分辨率图像，配有丰富的描述，确保内容多样性和视觉真实感。为了解决细节合成的问题，我们提出了一种频率感知的后训练方法，包括细节导向时间步采样和软加权频率正则化，以增强细节生成。实验结果表明，我们的方法显著提高了超高分辨率图像生成的细节质量和整体真实感。'}}}, {'id': 'https://huggingface.co/papers/2510.24081', 'title': 'Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+\n  Languages and Cultures', 'url': 'https://huggingface.co/papers/2510.24081', 'abstract': 'Global PIQA is a multilingual commonsense reasoning benchmark that highlights performance gaps of large language models across different cultures and languages.  \t\t\t\t\tAI-generated summary \t\t\t\t To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded.', 'score': 8, 'issue_id': 6682, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': '87e621d34cc9f647', 'authors': ['Tyler A. Chang', 'Catherine Arnett', 'Abdelrahman Eldesokey', 'Abdelrahman Sadallah', 'Abeer Kashar', 'Abolade Daud', 'Abosede Grace Olanihun', 'Adamu Labaran Mohammed', 'Adeyemi Praise', 'Adhikarinayum Meerajita Sharma', 'Aditi Gupta', 'Afitab Iyigun', 'Afonso Simplício', 'Ahmed Essouaied', 'Aicha Chorana', 'Akhil Eppa', 'Akintunde Oladipo', 'Akshay Ramesh', 'Aleksei Dorkin', 'Alfred Malengo Kondoro', 'Alham Fikri Aji', 'Ali Eren Çetintaş', 'Allan Hanbury', 'Alou Dembele', 'Alp Niksarli', 'Álvaro Arroyo', 'Amin Bajand', 'Amol Khanna', 'Ana Chkhaidze', 'Ana Condez', 'Andiswa Mkhonto', 'Andrew Hoblitzell', 'Andrew Tran', 'Angelos Poulis', 'Anirban Majumder', 'Anna Vacalopoulou', 'Annette Kuuipolani Kanahele Wong', 'Annika Simonsen', 'Anton Kovalev', 'Ashvanth. S', 'Ayodeji Joseph Lana', 'Barkin Kinay', 'Bashar Alhafni', 'Benedict Cibalinda Busole', 'Bernard Ghanem', 'Bharti Nathani', 'Biljana Stojanovska Đurić', 'Bola Agbonile', 'Bragi Bergsson', 'Bruce Torres Fischer', 'Burak Tutar', 'Burcu Alakuş Çınar', 'Cade J. Kanoniakapueo Kane', 'Can Udomcharoenchaikit', 'Catherine Arnett', 'Chadi Helwe', 'Chaithra Reddy Nerella', 'Chen Cecilia Liu', 'Chiamaka Glory Nwokolo', 'Cristina España-Bonet', 'Cynthia Amol', 'DaeYeop Lee', 'Dana Arad', 'Daniil Dzenhaliou', 'Daria Pugacheva', 'Dasol Choi', 'Daud Abolade', 'David Liu', 'David Semedo', 'Deborah Popoola', 'Deividas Mataciunas', 'Delphine Nyaboke', 'Dhyuthy Krishna Kumar', 'Diogo Glória-Silva', 'Diogo Tavares', 'Divyanshu Goyal', 'DongGeon Lee', 'Ebele Nwamaka Anajemba', 'Egonu Ngozi Grace', 'Elena Mickel', 'Elena Tutubalina', 'Elias Herranen', 'Emile Anand', 'Emmanuel Habumuremyi', 'Emuobonuvie Maria Ajiboye', 'Eryawan Presma Yulianrifat', 'Esther Adenuga', 'Ewa Rudnicka', 'Faith Olabisi Itiola', 'Faran Taimoor Butt', 'Fathima Thekkekara', 'Fatima Haouari', 'Filbert Aurelian Tjiaranata', 'Firas Laakom', 'Francesca Grasso', 'Francesco Orabona', 'Francesco Periti', 'Gbenga Kayode Solomon', 'Gia Nghia Ngo', 'Gloria Udhehdhe-oze', 'Gonçalo Martins', 'Gopi Naga Sai Ram Challagolla', 'Guijin Son', 'Gulnaz Abdykadyrova', 'Hafsteinn Einarsson', 'Hai Hu', 'Hamidreza Saffari', 'Hamza Zaidi', 'Haopeng Zhang', 'Harethah Abu Shairah', 'Harry Vuong', 'Hele-Andra Kuulmets', 'Houda Bouamor', 'Hwanjo Yu', 'Iben Nyholm Debess', 'İbrahim Ethem Deveci', 'Ikhlasul Akmal Hanif', 'Ikhyun Cho', 'Inês Calvo', 'Inês Vieira', 'Isaac Manzi', 'Ismail Daud', 'Itay Itzhak', 'Iuliia', 'Alekseenko', 'Ivan Belashkin', 'Ivan Spada', 'Ivan Zhelyazkov', 'Jacob Brinton', 'Jafar Isbarov', 'Jaka Čibej', 'Jan Čuhel', 'Jan Kocoń', 'Jauza Akbar Krito', 'Jebish Purbey', 'Jennifer Mickel', 'Jennifer Za', 'Jenny Kunz', 'Jihae Jeong', 'Jimena Tena Dávalos', 'Jinu Lee', 'João Magalhães', 'John Yi', 'Jongin Kim', 'Joseph Chataignon', 'Joseph Marvin Imperial', 'Jubeerathan Thevakumar', 'Judith Land', 'Junchen Jiang', 'Jungwhan Kim', 'Kairit Sirts', 'Kamesh R', 'Kamesh V', 'Kanda Patrick Tshinu', 'Kätriin Kukk', 'Kaustubh Ponkshe', 'Kavsar Huseynova', 'Ke He', 'Kelly Buchanan', 'Kengatharaiyer Sarveswaran', 'Kerem Zaman', 'Khalil Mrini', 'Kian Kyars', 'Krister Kruusmaa', 'Kusum Chouhan', 'Lainitha Krishnakumar', 'Laura Castro Sánchez', 'Laura Porrino Moscoso', 'Leshem Choshen', 'Levent Sencan', 'Lilja Øvrelid', 'Lisa Alazraki', 'Lovina Ehimen-Ugbede', 'Luheerathan Thevakumar', 'Luxshan Thavarasa', 'Mahnoor Malik', 'Mamadou K. Keita', 'Mansi Jangid', 'Marco De Santis', 'Marcos García', 'Marek Suppa', "Mariam D'Ciofalo", 'Marii Ojastu', 'Maryam Sikander', 'Mausami Narayan', 'Maximos Skandalis', 'Mehak Mehak', 'Mehmet İlteriş Bozkurt', 'Melaku Bayu Workie', 'Menan Velayuthan', 'Michael Leventhal', 'Michał Marcińczuk', 'Mirna Potočnjak', 'Mohammadamin Shafiei', 'Mridul Sharma', 'Mrityunjaya Indoria', 'Muhammad Ravi Shulthan Habibi', 'Murat Kolić', 'Nada Galant', 'Naphat Permpredanun', 'Narada Maugin', 'Nicholas Kluge Corrêa', 'Nikola Ljubešić', 'Nirmal Thomas', 'Nisansa de Silva', 'Nisheeth Joshi', 'Nitish Ponkshe', 'Nizar Habash', 'Nneoma C. Udeze', 'Noel Thomas', 'Noémi Ligeti-Nagy', 'Nouhoum Coulibaly', 'Nsengiyumva Faustin', 'Odunayo Kareemat Buliaminu', 'Odunayo Ogundepo', 'Oghojafor Godswill Fejiro', 'Ogundipe Blessing Funmilola', "Okechukwu God'spraise", 'Olanrewaju Samuel', 'Olaoye Deborah Oluwaseun', 'Olasoji Akindejoye', 'Olga Popova', 'Olga Snissarenko', 'Onyinye Anulika Chiemezie', 'Orkun Kinay', 'Osman Tursun', 'Owoeye Tobiloba Moses', 'Oyelade Oluwafemi Joshua', 'Oyesanmi Fiyinfoluwa', 'Pablo Gamallo', 'Pablo Rodríguez Fernández', 'Palak Arora', 'Pedro Valente', 'Peter Rupnik', 'Philip Oghenesuowho Ekiugbo', 'Pramit Sahoo', 'Prokopis Prokopidis', 'Pua Niau-Puhipau', 'Quadri Yahya', 'Rachele Mignone', 'Raghav Singhal', 'Ram Mohan Rao Kadiyala', 'Raphael Merx', 'Rapheal Afolayan', 'Ratnavel Rajalakshmi', 'Rishav Ghosh', 'Romina Oji', 'Ron Kekeha Solis', 'Rui Guerra', 'Rushikesh Zawar', "Sa'ad Nasir Bashir", 'Saeed Alzaabi', 'Sahil Sandeep', 'Sai Pavan Batchu', 'SaiSandeep Kantareddy', 'Salsabila Zahirah Pranida', 'Sam Buchanan', 'Samuel Rutunda', 'Sander Land', 'Sarah Sulollari', 'Sardar Ali', 'Saroj Sapkota', 'Saulius Tautvaisas', 'Sayambhu Sen', 'Sayantani Banerjee', 'Sebastien Diarra', 'SenthilNathan. M', 'Sewoong Lee', 'Shaan Shah', 'Shankar Venkitachalam', 'Sharifa Djurabaeva', 'Sharon Ibejih', 'Shivanya Shomir Dutta', 'Siddhant Gupta', 'Silvia Paniagua Suárez', 'Sina Ahmadi', 'Sivasuthan Sukumar', 'Siyuan Song', 'Snegha A.', 'Sokratis Sofianopoulos', 'Sona Elza Simon', 'Sonja Benčina', 'Sophie Gvasalia', 'Sphurti Kirit More', 'Spyros Dragazis', 'Stephan P. Kaufhold', 'Suba. S', 'Sultan AlRashed', 'Surangika Ranathunga', 'Taiga Someya', 'Taja Kuzman Pungeršek', 'Tal Haklay', "Tasi'u Jibril", 'Tatsuya Aoyama', 'Tea Abashidze', 'Terenz Jomar Dela Cruz', 'Terra Blevins', 'Themistoklis Nikas', 'Theresa Dora Idoko', 'Thu Mai Do', 'Tilek Chubakov', 'Tommaso Gargiani', 'Uma Rathore', 'Uni Johannesen', 'Uwuma Doris Ugwu', 'Vallerie Alexandra Putra', 'Vanya Bannihatti Kumar', 'Varsha Jeyarajalingam', 'Varvara Arzt', 'Vasudevan Nedumpozhimana', 'Viktoria Ondrejova', 'Viktoryia Horbik', 'Vishnu Vardhan Reddy Kummitha', 'Vuk Dinić', 'Walelign Tewabe Sewunetie', 'Winston Wu', 'Xiaojing Zhao', 'Yacouba Diarra', 'Yaniv Nikankin', 'Yash Mathur', 'Yixi Chen', 'Yiyuan Li', 'Yolanda Xavier', 'Yonatan Belinkov', 'Yusuf Ismail Abayomi', 'Zaid Alyafeai', 'Zhengyang Shan', 'Zhi Rui Tam', 'Zilu Tang', 'Zuzana Nadova', 'Baber Abbasi', 'Stella Biderman', 'David Stap', 'Duygu Ataman', 'Fabian Schmidt', 'Hila Gonen', 'Jiayi Wang', 'David Ifeoluwa Adelani'], 'affiliations': ['EleutherAI', 'UC San Diego'], 'pdf_title_img': 'assets/pdf/title_img/2510.24081.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#ethics', '#multilingual', '#low_resource', '#benchmark'], 'emoji': '🌍', 'ru': {'title': 'Культурное разнообразие обнажает слабости языковых моделей', 'desc': 'Global PIQA — это мультиязычный бенчмарк для оценки здравого смысла LLM, созданный вручную исследователями из 65 стран для более чем 100 языков. Датасет охватывает 14 языковых семей и 23 системы письма, при этом более 50% примеров содержат культурно-специфичные элементы, такие как местная еда, обычаи и традиции. Современные LLM показывают хорошие результаты в среднем, но демонстрируют значительное падение точности (до 37%) на низкоресурсных языках по сравнению со случайным угадыванием в 50%. Исследование подчеркивает, что повседневные знания, укорененные в различных культурах, остаются проблемной областью для AI-моделей наравне со сложными рассуждениями и экспертными знаниями.'}, 'en': {'title': 'Bridging Cultural Gaps in Language Models with Global PIQA', 'desc': 'Global PIQA is a new benchmark designed to evaluate the commonsense reasoning abilities of large language models (LLMs) across more than 100 languages and cultures. It was created by a diverse team of researchers and includes culturally-specific examples that reflect local customs and traditions. The study reveals that while LLMs perform well overall, they struggle significantly with lower-resource languages, showing a notable accuracy gap. This benchmark not only assesses LLM performance but also emphasizes the importance of understanding cultural diversity in language processing.'}, 'zh': {'title': '全球PIQA：跨文化常识推理的新基准', 'desc': 'Global PIQA是一个多语言的常识推理基准，旨在揭示大型语言模型在不同文化和语言之间的表现差距。该基准由来自65个国家的335名研究人员手工构建，涵盖超过100种语言，涉及五大洲、14个语言家族和23种书写系统。Global PIQA中的示例超过50%涉及当地的食物、习俗和传统等文化特征，显示出文化特定性的重要性。研究发现，尽管最先进的语言模型在整体上表现良好，但在低资源语言中的表现较弱，准确率差距可达37%。'}}}, {'id': 'https://huggingface.co/papers/2510.23925', 'title': 'Latent Chain-of-Thought for Visual Reasoning', 'url': 'https://huggingface.co/papers/2510.23925', 'abstract': 'The proposed method reformulates reasoning in Large Vision-Language Models as posterior inference using amortized variational inference and a sparse reward function, improving effectiveness, generalization, and interpretability.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.', 'score': 8, 'issue_id': 6684, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': '0868d2155ce6b87b', 'authors': ['Guohao Sun', 'Hang Hua', 'Jian Wang', 'Jiebo Luo', 'Sohail Dianat', 'Majid Rabbani', 'Raghuveer Rao', 'Zhiqiang Tao'], 'affiliations': ['DEVCOM Army Research Laboratory', 'Rochester Institute of Technology', 'Snap Inc.', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2510.23925.jpg', 'data': {'categories': ['#interpretability', '#training', '#multimodal', '#benchmark', '#reasoning', '#rl', '#rlhf'], 'emoji': '🧠', 'ru': {'title': 'Байесовский подход к обучению рассуждению в мультимодальных LLM', 'desc': 'Статья предлагает новый метод обучения больших мультимодальных моделей (LVLMs) рассуждению через формулировку задачи как байесовского вывода с использованием вариационной инференции. Вместо традиционных алгоритмов типа PPO авторы применяют reinforcement learning с разреженной наградой на уровне токенов, что стимулирует разнообразные цепочки рассуждений (chain-of-thought). Для эффективного выбора оптимальных объяснений используется стратегия масштабирования через маргинальное правдоподобие вместо затратных методов Best-of-N. Экспериментально показано улучшение производительности на семи бенчмарках по эффективности, обобщающей способности и интерпретируемости.'}, 'en': {'title': 'Revolutionizing Reasoning in LVLMs with Amortized Inference', 'desc': 'This paper presents a new approach to improve Large Vision-Language Models (LVLMs) by treating reasoning as posterior inference through amortized variational inference. The authors highlight the limitations of traditional training methods, which often struggle with generalization and rely on biased reward models. To overcome these issues, they introduce a sparse reward function that promotes diverse reasoning paths and enhances interpretability. Their method shows significant improvements in performance across various reasoning tasks, demonstrating better effectiveness and generalization capabilities.'}, 'zh': {'title': '提升大型视觉语言模型的推理能力', 'desc': '本文提出了一种新方法，将大型视觉语言模型（LVLMs）的推理过程重新表述为后验推断，采用了摊销变分推断和稀疏奖励函数。这种方法提高了模型的有效性、泛化能力和可解释性。通过引入多样性寻求的强化学习算法，提出了一种新的稀疏奖励函数，以促进多样化的高可能性链式推理。实验结果表明，该方法在七个推理基准测试中显著提升了现有LVLM的表现。'}}}, {'id': 'https://huggingface.co/papers/2510.24684', 'title': 'SPICE: Self-Play In Corpus Environments Improves Reasoning', 'url': 'https://huggingface.co/papers/2510.24684', 'abstract': "SPICE, a reinforcement learning framework, uses self-play in a corpus environment to continuously improve a model's reasoning capabilities through adversarial dynamics and document grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Self-improving systems require environmental interaction for continuous adaptation. We introduce SPICE (Self-Play In Corpus Environments), a reinforcement learning framework where a single model acts in two roles: a Challenger that mines documents from a large corpus to generate diverse reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics, the Challenger creates an automatic curriculum at the frontier of the Reasoner's capability, while corpus grounding provides the rich, near-inexhaustible external signal necessary for sustained improvement. Unlike existing ungrounded self-play methods that offer more limited benefits, SPICE achieves consistent gains across mathematical (+8.9%) and general reasoning (+9.8%) benchmarks on multiple model families. Our analysis reveals how document grounding is a key ingredient in SPICE to continuously generate its own increasingly challenging goals and achieve them, enabling sustained self-improvement.", 'score': 5, 'issue_id': 6681, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'ee9331019ee63467', 'authors': ['Bo Liu', 'Chuanyang Jin', 'Seungone Kim', 'Weizhe Yuan', 'Wenting Zhao', 'Ilia Kulikov', 'Xian Li', 'Sainbayar Sukhbaatar', 'Jack Lanchantin', 'Jason Weston'], 'affiliations': ['FAIR at Meta', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2510.24684.jpg', 'data': {'categories': ['#optimization', '#rl', '#benchmark', '#reasoning', '#rlhf'], 'emoji': '🎭', 'ru': {'title': 'Две роли одной модели: самообучение через adversarial игру с документами', 'desc': 'SPICE - это фреймворк для reinforcement learning, где одна модель играет две роли: Challenger ищет документы в корпусе и генерирует задачи на reasoning, а Reasoner их решает. Благодаря adversarial динамике между этими ролями создаётся автоматический curriculum learning на границе возможностей модели. Ключевая идея - использование документов из корпуса как неисчерпаемого источника сигнала для непрерывного self-improvement. Метод показывает стабильный рост качества на математических (+8.9%) и общих (+9.8%) бенчмарках для reasoning.'}, 'en': {'title': 'SPICE: Self-Play for Continuous Reasoning Improvement', 'desc': "SPICE is a reinforcement learning framework that enhances a model's reasoning abilities through self-play in a document-rich environment. It features two roles: a Challenger that generates diverse reasoning tasks from a large corpus, and a Reasoner that attempts to solve these tasks. This adversarial setup creates a dynamic learning environment where the Challenger continuously adapts the difficulty of tasks based on the Reasoner's performance. By grounding the learning process in real documents, SPICE achieves significant improvements in reasoning tasks compared to traditional self-play methods."}, 'zh': {'title': 'SPICE：自我提升的强化学习框架', 'desc': 'SPICE是一种强化学习框架，通过自我对弈在语料环境中不断提升模型的推理能力。该框架中，单一模型扮演挑战者和推理者两个角色，挑战者从大语料库中挖掘文档以生成多样的推理任务，而推理者则负责解决这些任务。通过对抗动态，挑战者为推理者创造了一个自动化的课程，使其不断面对更具挑战性的目标。SPICE在多个模型家族的数学和一般推理基准测试中均取得了显著的提升，表明文档基础是其持续自我改进的关键因素。'}}}, {'id': 'https://huggingface.co/papers/2510.22768', 'title': 'MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion', 'url': 'https://huggingface.co/papers/2510.22768', 'abstract': 'MMPersuade is a framework for studying multimodal persuasion in Large Vision-Language Models, revealing insights into their susceptibility and the effectiveness of various persuasive strategies across different contexts.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.', 'score': 5, 'issue_id': 6682, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '549393a3e78ca331', 'authors': ['Haoyi Qiu', 'Yilun Zhou', 'Pranav Narayanan Venkit', 'Kung-Hsiang Huang', 'Jiaxin Zhang', 'Nanyun Peng', 'Chien-Sheng Wu'], 'affiliations': ['Salesforce AI Research', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2510.22768.jpg', 'data': {'categories': ['#dataset', '#ethics', '#multimodal', '#alignment', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Как изображения делают AI-модели более внушаемыми', 'desc': 'В статье представлен MMPersuade - фреймворк для изучения мультимодального убеждения в больших vision-language моделях (LVLMs). Исследование показывает, что модели значительно более восприимчивы к убеждению при использовании изображений и видео по сравнению с текстом, особенно в контексте дезинформации. Разные стратегии убеждения работают по-разному: взаимность эффективна в коммерческих контекстах, а логика и авторитетность - в противоборствующих ситуациях. Фреймворк помогает создавать более устойчивые и этически aligned модели, способные противостоять манипулятивному контенту.'}, 'en': {'title': 'Understanding Persuasion in Large Vision-Language Models', 'desc': 'MMPersuade is a framework designed to explore how Large Vision-Language Models (LVLMs) respond to persuasive multimodal inputs, such as images and videos. The study reveals that these models can be significantly influenced by such inputs, especially in scenarios involving misinformation. It also finds that while having stated preferences can reduce susceptibility to persuasion, multimodal content still holds a persuasive edge. By analyzing the effectiveness of various persuasive strategies, MMPersuade aims to enhance the robustness and ethical alignment of LVLMs when they encounter persuasive content.'}, 'zh': {'title': '多模态说服的研究新框架', 'desc': 'MMPersuade是一个研究大型视觉语言模型（LVLMs）在多模态说服中的框架。该框架揭示了这些模型在不同情境下的易受影响性和各种说服策略的有效性。研究表明，多模态输入显著提高了说服效果，尤其是在错误信息的情况下。通过分析说服效果和模型的易受影响性，MMPersuade为开发更稳健、符合用户偏好和伦理的模型提供了基础。'}}}, {'id': 'https://huggingface.co/papers/2510.24645', 'title': 'FunReason-MT Technical Report: Overcoming the Complexity Barrier in\n  Multi-Turn Function Calling', 'url': 'https://huggingface.co/papers/2510.24645', 'abstract': 'FunReason-MT is a novel data synthesis framework that enhances multi-turn function calling in large language models by addressing challenges in environment interaction, query synthesis, and chain-of-thought generation, achieving state-of-the-art performance on the Berkeley Function-Calling Leaderboard.  \t\t\t\t\tAI-generated summary \t\t\t\t Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted model training, isolation of tool architecture, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models, outperforming most close-source models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.', 'score': 4, 'issue_id': 6668, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'e3e4dc7959a10fb7', 'authors': ['Zengzhuang Xu', 'Bingguang Hao', 'Zechuan Wang', 'Yuntao Wen', 'Maolin Wang', 'Yang Liu', 'Long Chen', 'Dong Wang', 'Yicheng Chen', 'Cunyin Peng', 'Chenyi Zhuang', 'Jinjie Gu', 'Leilei Gan', 'Xiangyu Zhao', 'Shi Gu'], 'affiliations': ['AWorld Team, Inclusion AI', 'City University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2510.24645.jpg', 'data': {'categories': ['#dataset', '#training', '#optimization', '#data', '#agents', '#transfer_learning'], 'emoji': '🔧', 'ru': {'title': 'Учим LLM правильно пользоваться инструментами в несколько шагов', 'desc': 'FunReason-MT — это новый фреймворк для синтеза данных, который улучшает способность больших языковых моделей выполнять многошаговые вызовы функций. Метод решает три ключевые проблемы: взаимодействие с окружением через графы Environment-API, синтез сложных запросов и генерацию chain-of-thought рассуждений. Модель размером всего 4B параметров, обученная на данных FunReason-MT, достигла state-of-the-art результатов на Berkeley Function-Calling Leaderboard, превзойдя большинство проприетарных моделей. Фреймворк обеспечивает надежный источник качественных данных для обучения AI-агентов работе с внешними инструментами.'}, 'en': {'title': 'Empowering AI with Enhanced Multi-Turn Function Calling', 'desc': "FunReason-MT is a new framework designed to improve how large language models (LLMs) perform multi-turn function calling, which is essential for interacting with external tools. It tackles issues related to generating high-quality training data by using advanced techniques like Environment-API Graph Interactions and Tool-Query Synthesis. This framework helps in creating better training scenarios that reflect real-world complexities, enhancing the model's ability to understand and generate logical sequences. The results show that models trained with FunReason-MT data achieve top performance on the Berkeley Function-Calling Leaderboard, indicating its effectiveness in advancing AI capabilities."}, 'zh': {'title': '提升多轮函数调用的智能框架', 'desc': 'FunReason-MT 是一个新颖的数据合成框架，旨在提升大型语言模型在多轮函数调用中的表现。它通过解决环境交互、查询合成和思维链生成等挑战，成功在伯克利函数调用排行榜上取得了领先的性能。该框架采用环境-API图交互、先进的工具-查询合成和引导迭代链等技术，生成高质量的多轮数据。评估结果显示，基于 FunReason-MT 生成数据的 4B 模型在同类模型中表现优异，证明了其在智能学习中的可靠性和稳健性。'}}}, {'id': 'https://huggingface.co/papers/2510.24448', 'title': 'Rethinking Visual Intelligence: Insights from Video Pretraining', 'url': 'https://huggingface.co/papers/2510.24448', 'abstract': 'Video Diffusion Models (VDMs) show higher data efficiency than large language models across various visual tasks, suggesting video pretraining can enhance visual foundation models.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.', 'score': 4, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'cb963e7271205da4', 'authors': ['Pablo Acuaviva', 'Aram Davtyan', 'Mariam Hassan', 'Sebastian Stapf', 'Ahmad Rahimi', 'Alexandre Alahi', 'Paolo Favaro'], 'affiliations': ['Computer Vision Group University of Bern Bern, Switzerland', 'VITA Lab, EPFL Lausanne, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2510.24448.jpg', 'data': {'categories': ['#multimodal', '#transfer_learning', '#games', '#diffusion', '#benchmark', '#video'], 'emoji': '🎬', 'ru': {'title': 'Видео-диффузия побеждает языковые модели в визуальных задачах', 'desc': 'Исследователи сравнили Video Diffusion Models (VDMs) и большие языковые модели (LLM) в визуальных задачах. VDMs, предобученные на видеоданных, показали более высокую эффективность обучения на малых данных благодаря лучшему пониманию пространственно-временной структуры. Модели тестировались на различных бенчмарках, включая ARC-AGI, ConceptARC, визуальные игры и планирование маршрутов. Результаты показывают, что предобучение на видео дает важные индуктивные смещения для создания визуальных foundation models.'}, 'en': {'title': 'Unlocking Visual Potential with Video Diffusion Models', 'desc': 'This paper explores the effectiveness of Video Diffusion Models (VDMs) in improving visual tasks through video pretraining. Unlike large language models (LLMs), which excel in language tasks, VDMs leverage spatiotemporal data to enhance their understanding of structure and dynamics. The authors conducted experiments comparing pretrained VDMs and LLMs, finding that VDMs showed superior data efficiency across various benchmarks. The results suggest that video pretraining can significantly advance the development of visual foundation models.'}, 'zh': {'title': '视频预训练提升视觉模型效率', 'desc': '视频扩散模型（VDMs）在各种视觉任务中显示出比大型语言模型更高的数据效率，表明视频预训练可以增强视觉基础模型的能力。尽管大型语言模型在语言领域的预训练取得了成功，但在视觉领域，模型仍然面临组合理解和样本效率等挑战。我们研究VDMs作为弥合这一差距的有前景的方向，认为其在时空数据上的预训练赋予了模型强大的结构和动态的归纳偏置。我们的实验结果表明，VDMs在多个基准测试中表现出比语言模型更高的数据效率，支持了视频预训练对视觉基础模型的进展。'}}}, {'id': 'https://huggingface.co/papers/2510.24591', 'title': 'ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers?', 'url': 'https://huggingface.co/papers/2510.24591', 'abstract': "ReplicationBench evaluates AI agents' ability to replicate astrophysics research papers, providing insights into their faithfulness and correctness in scientific research tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Frontier AI agents show increasing promise as scientific research assistants, and may eventually be useful for extended, open-ended research workflows. However, in order to use agents for novel research, we must first assess the underlying faithfulness and correctness of their work. To evaluate agents as research assistants, we introduce ReplicationBench, an evaluation framework that tests whether agents can replicate entire research papers drawn from the astrophysics literature. Astrophysics, where research relies heavily on archival data and computational study while requiring little real-world experimentation, is a particularly useful testbed for AI agents in scientific research. We split each paper into tasks which require agents to replicate the paper's core contributions, including the experimental setup, derivations, data analysis, and codebase. Each task is co-developed with the original paper authors and targets a key scientific result, enabling objective evaluation of both faithfulness (adherence to original methods) and correctness (technical accuracy of results). ReplicationBench is extremely challenging for current frontier language models: even the best-performing language models score under 20%. We analyze ReplicationBench trajectories in collaboration with domain experts and find a rich, diverse set of failure modes for agents in scientific research. ReplicationBench establishes the first benchmark of paper-scale, expert-validated astrophysics research tasks, reveals insights about agent performance generalizable to other domains of data-driven science, and provides a scalable framework for measuring AI agents' reliability in scientific research.", 'score': 3, 'issue_id': 6667, 'pub_date': '2025-10-28', 'pub_date_card': {'ru': '28 октября', 'en': 'October 28', 'zh': '10月28日'}, 'hash': 'd655b0884e7b15c0', 'authors': ['Christine Ye', 'Sihan Yuan', 'Suchetha Cooray', 'Steven Dillmann', 'Ian L. V. Roque', 'Dalya Baron', 'Philipp Frank', 'Sergio Martin-Alvarez', 'Nolan Koblischke', 'Frank J Qu', 'Diyi Yang', 'Risa Wechsler', 'Ioana Ciuca'], 'affiliations': ['Stanford University', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2510.24591.jpg', 'data': {'categories': ['#benchmark', '#science', '#agents'], 'emoji': '🔭', 'ru': {'title': 'Проверка AI-агентов на репликацию научных исследований', 'desc': 'Исследователи представили ReplicationBench — бенчмарк для оценки способности AI-агентов воспроизводить научные работы по астрофизике. Каждая задача включает проверку верности оригинальным методам и корректности технических результатов, при этом все задачи разработаны совместно с авторами оригинальных статей. Даже лучшие современные LLM показывают результат ниже 20%, демонстрируя множество разнообразных ошибок при работе с научными исследованиями. Этот бенчмарк предоставляет первую масштабируемую систему для измерения надежности AI-агентов в научных исследованиях и может быть применен в других областях науки, основанных на данных.'}, 'en': {'title': 'Evaluating AI Agents in Astrophysics Research Replication', 'desc': "ReplicationBench is a framework designed to evaluate AI agents' ability to replicate research papers in astrophysics, focusing on their faithfulness and correctness. It breaks down each paper into specific tasks that require the agents to reproduce key contributions, such as experimental setups and data analyses, in collaboration with the original authors. The framework reveals that even advanced language models struggle with these tasks, scoring below 20%, highlighting various failure modes in their performance. This benchmark not only assesses AI reliability in astrophysics but also offers insights applicable to other scientific fields."}, 'zh': {'title': '评估 AI 代理在科学研究中的可靠性', 'desc': 'ReplicationBench 是一个评估 AI 代理在复制天体物理学研究论文能力的框架。它通过将论文分解为多个任务，测试代理是否能够准确复现论文的核心贡献，包括实验设置、推导、数据分析和代码库。该框架与原论文作者共同开发，确保评估的客观性，关注代理的忠实性和正确性。尽管当前的前沿语言模型在此任务中表现不佳，ReplicationBench 仍为科学研究中的 AI 代理提供了一个可扩展的可靠性测量框架。'}}}, {'id': 'https://huggingface.co/papers/2510.22876', 'title': 'Batch Speculative Decoding Done Right', 'url': 'https://huggingface.co/papers/2510.22876', 'abstract': 'Batch speculative decoding improves LLM inference throughput by managing ragged tensors to maintain output equivalence and reduce realignment overhead.  \t\t\t\t\tAI-generated summary \t\t\t\t Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3times throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.', 'score': 2, 'issue_id': 6687, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '7d222fe69620ec33', 'authors': ['Ranran Haoran Zhang', 'Soumik Dey', 'Ashirbad Mishra', 'Hansi Wu', 'Binbin Li', 'Rui Zhang'], 'affiliations': ['The Pennsylvania State University', 'eBay Inc'], 'pdf_title_img': 'assets/pdf/title_img/2510.22876.jpg', 'data': {'categories': ['#inference', '#optimization', '#training'], 'emoji': '🎯', 'ru': {'title': 'Правильная групповая обработка для ускорения inference в LLM', 'desc': 'Статья рассматривает проблему batch speculative decoding для ускорения inference в LLM. Основная сложность возникает при обработке неровных тензоров (ragged tensors), когда разные последовательности в батче принимают разное количество предложенных токенов от draft-модели. Авторы показывают, что существующие реализации нарушают эквивалентность выходных данных из-за неправильной обработки этой проблемы. Предложенный метод EXSPEC динамически группирует последовательности одинаковой длины, обеспечивая до 3x улучшение throughput при размере батча 8 с сохранением 95% эквивалентности выходов.'}, 'en': {'title': 'Boosting LLM Inference with Batch Speculative Decoding', 'desc': 'This paper presents a method called Batch Speculative Decoding that enhances the efficiency of large language model (LLM) inference by addressing the challenges posed by ragged tensors. It highlights how speculative decoding can be applied in batches to improve throughput while ensuring that the output remains equivalent to standard autoregressive generation. The authors identify synchronization issues that can lead to output discrepancies and propose two solutions: EQSPEC, which focuses on correctness, and EXSPEC, which optimizes realignment overhead. Their approach demonstrates significant performance improvements, achieving up to three times the throughput at batch size 8 while maintaining a high level of output equivalence.'}, 'zh': {'title': '批量推测解码：提升LLM推理效率的关键', 'desc': '本文提出了一种批量推测解码的方法，以提高大型语言模型（LLM）的推理吞吐量。通过管理不规则张量，保持输出等价性并减少重新对齐的开销，解决了批量处理中的不规则张量问题。我们展示了现有批量实现如何因处理不当而违反输出等价性，并提出了两种新方法来保证正确性和减少开销。实验结果表明，在批量大小为8时，我们的方法在吞吐量上提高了最多3倍，同时保持了95%的输出等价性。'}}}, {'id': 'https://huggingface.co/papers/2510.22795', 'title': 'SAO-Instruct: Free-form Audio Editing using Natural Language\n  Instructions', 'url': 'https://huggingface.co/papers/2510.22795', 'abstract': 'SAO-Instruct, a generative model based on Stable Audio Open, allows flexible audio editing using natural language instructions, outperforming existing methods in both objective and subjective evaluations.  \t\t\t\t\tAI-generated summary \t\t\t\t Generative models have made significant progress in synthesizing high-fidelity audio from short textual descriptions. However, editing existing audio using natural language has remained largely underexplored. Current approaches either require the complete description of the edited audio or are constrained to predefined edit instructions that lack flexibility. In this work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of editing audio clips using any free-form natural language instruction. To train our model, we create a dataset of audio editing triplets (input audio, edit instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual editing pipeline. Although partially trained on synthetic data, our model generalizes well to real in-the-wild audio clips and unseen edit instructions. We demonstrate that SAO-Instruct achieves competitive performance on objective metrics and outperforms other audio editing approaches in a subjective listening study. To encourage future research, we release our code and model weights.', 'score': 2, 'issue_id': 6681, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '2dd95fced620d404', 'authors': ['Michael Ungersböck', 'Florian Grötschla', 'Luca A. Lanzendörfer', 'June Young Yi', 'Changho Choi', 'Roger Wattenhofer'], 'affiliations': ['ETH Zurich', 'Korea University', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2510.22795.jpg', 'data': {'categories': ['#dataset', '#open_source', '#audio', '#synthetic'], 'emoji': '🎨', 'ru': {'title': 'Редактирование аудио свободными текстовыми командами', 'desc': 'SAO-Instruct — это генеративная модель на основе Stable Audio Open, которая позволяет редактировать аудио с помощью произвольных текстовых инструкций на естественном языке. Авторы создали датасет из триплетов (исходное аудио, инструкция редактирования, результат) используя техники Prompt-to-Prompt и DDPM инversion. Модель демонстрирует хорошую генерализацию на реальных аудиоклипах, несмотря на частичное обучение на синтетических данных. По результатам объективных метрик и субъективного прослушивания SAO-Instruct превосходит существующие методы редактирования аудио.'}, 'en': {'title': 'Revolutionizing Audio Editing with Natural Language Instructions', 'desc': 'SAO-Instruct is a generative model designed for flexible audio editing using natural language instructions. Unlike previous methods that require detailed descriptions or fixed editing commands, this model allows users to provide any free-form instruction for audio modification. It is trained on a dataset of audio editing triplets, which helps it learn to generate high-quality edited audio from various inputs. The model not only performs well on objective metrics but also excels in subjective evaluations, making it a significant advancement in audio editing technology.'}, 'zh': {'title': '灵活音频编辑，尽在SAO-Instruct', 'desc': 'SAO-Instruct是一种基于Stable Audio Open的生成模型，能够灵活地使用自然语言指令进行音频编辑。与现有方法相比，它在客观和主观评估中表现更佳。该模型通过创建音频编辑三元组的数据集进行训练，能够处理任意自由形式的编辑指令。尽管部分训练使用了合成数据，但模型在真实音频片段和未见过的编辑指令上也能很好地泛化。'}}}, {'id': 'https://huggingface.co/papers/2510.22099', 'title': 'Generalization or Memorization: Dynamic Decoding for Mode Steering', 'url': 'https://huggingface.co/papers/2510.22099', 'abstract': "A framework using the Information Bottleneck principle and Dynamic Mode Steering algorithm improves the reliability of Large Language Models by balancing generalization and memorization.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.", 'score': 2, 'issue_id': 6667, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '7a4136db7043277f', 'authors': ['Xuanming Zhang'], 'affiliations': ['Department of Computer Science, University of Wisconsin-Madison, Madison, USA', 'Stanford University, Palo Alto, USA'], 'pdf_title_img': 'assets/pdf/title_img/2510.22099.jpg', 'data': {'categories': ['#reasoning', '#interpretability', '#inference', '#training'], 'emoji': '🧭', 'ru': {'title': 'Управление режимами мышления: как научить LLM обобщать, а не заучивать', 'desc': 'Исследователи предлагают новый подход к проблеме, когда большие языковые модели то блестяще обобщают знания, то просто воспроизводят заученные фразы из обучающих данных. На основе принципа Information Bottleneck они создали теоретическую модель, где обобщение понимается как сжатие информации до важных для задачи элементов, а механическое запоминание — как неспособность к такому сжатию. Разработанный алгоритм Dynamic Mode Steering использует легковесный линейный классификатор для определения момента, когда модель полагается на заученные данные, и динамически корректирует её вычисления в сторону обобщающих паттернов. Эксперименты показали, что метод значительно улучшает логическую последовательность и фактическую точность ответов LLM.'}, 'en': {'title': 'Balancing Generalization and Memorization for Reliable LLMs', 'desc': "This paper presents a framework that enhances the reliability of Large Language Models (LLMs) by addressing their tendency to both generalize and memorize training data. It introduces the Information Bottleneck principle to differentiate between effective generalization, which compresses information, and problematic memorization, which retains too much detail. The Dynamic Mode Steering algorithm is developed to dynamically adjust the model's focus during inference, promoting generalization while minimizing reliance on memorization. Experimental results show that this approach improves logical consistency and factual accuracy in LLM outputs, making them more reliable for critical applications."}, 'zh': {'title': '提升大型语言模型可靠性的创新框架', 'desc': '本文提出了一种新的框架，利用信息瓶颈原理和动态模式引导算法来提高大型语言模型的可靠性。该框架旨在平衡模型的泛化能力和记忆能力，解决模型在高风险应用中的不可靠性问题。通过理论模型，泛化被定义为学习压缩的、与任务相关的表示，而记忆则被视为未能进行压缩。实验结果表明，动态模式引导算法显著提高了模型的逻辑一致性和事实准确性，提供了一种增强大型语言模型可靠性的原则性方法。'}}}, {'id': 'https://huggingface.co/papers/2510.23667', 'title': 'Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free\n  Structural Topology Optimization', 'url': 'https://huggingface.co/papers/2510.23667', 'abstract': 'OAT, a deep-learning framework combining autoencoder, neural-field decoder, and latent-diffusion model, achieves fast and general topology optimization with high performance across various conditions and resolutions.  \t\t\t\t\tAI-generated summary \t\t\t\t Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology.', 'score': 1, 'issue_id': 6678, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '7fce06878cedad5b', 'authors': ['Amin Heyrani Nobari', 'Lyle Regenwetter', 'Cyril Picard', 'Ligong Han', 'Faez Ahmed'], 'affiliations': ['Massachusetts Institute of Technology, Cambridge, MA, 02139', 'Red Hat AI, MIT-IBM Watson AI Lab, Cambridge, MA, 02139'], 'pdf_title_img': 'assets/pdf/title_img/2510.23667.jpg', 'data': {'categories': ['#inference', '#benchmark', '#dataset', '#diffusion', '#optimization', '#open_source', '#architecture'], 'emoji': '🏗️', 'ru': {'title': 'OAT: универсальная AI-система для мгновенной оптимизации любых конструкций', 'desc': 'Представлена OAT — framework на основе deep learning для топологической оптимизации инженерных конструкций, который объединяет autoencoder, neural-field декодер и latent-diffusion модель. Система обучена на датасете OpenTO из 2.2 миллионов оптимизированных структур и работает с произвольными разрешениями, соотношениями сторон и граничными условиями. OAT снижает среднюю податливость конструкций до 90% по сравнению с предыдущими методами и выполняет inference менее чем за секунду на одном GPU. Это первая foundation model для топологической оптимизации, которая работает с любыми параметрами без необходимости дополнительной оптимизации.'}, 'en': {'title': 'Revolutionizing Topology Optimization with OAT', 'desc': 'The paper presents Optimize Any Topology (OAT), a novel deep-learning framework designed for efficient structural topology optimization. OAT integrates an autoencoder, a neural-field decoder, and a latent-diffusion model to predict optimal layouts for various engineering constraints and configurations. Unlike previous methods that are limited to fixed grids and specific conditions, OAT can handle arbitrary aspect ratios and resolutions, significantly improving performance. The framework demonstrates a remarkable reduction in compliance and fast inference times, making it a versatile tool for physics-aware design optimization.'}, 'zh': {'title': '优化任意拓扑的快速解决方案', 'desc': 'OAT是一种深度学习框架，结合了自编码器、神经场解码器和潜在扩散模型，能够快速且高效地进行拓扑优化。它解决了传统方法在固定网格和边界条件上的局限性，支持任意比例、分辨率和负载的布局预测。OAT在多个基准测试中表现出色，相较于之前的最佳模型，平均合规性降低了90%。该框架为物理感知的拓扑优化提供了一个通用、快速且无分辨率限制的解决方案，并为逆向设计的生成建模研究提供了大规模数据集。'}}}, {'id': 'https://huggingface.co/papers/2510.22590', 'title': 'ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs', 'url': 'https://huggingface.co/papers/2510.22590', 'abstract': 'ATOM is a few-shot, scalable approach for constructing and updating Temporal Knowledge Graphs from unstructured text, improving exhaustivity, stability, and latency.  \t\t\t\t\tAI-generated summary \t\t\t\t In today\'s rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained "atomic" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.', 'score': 1, 'issue_id': 6687, 'pub_date': '2025-10-26', 'pub_date_card': {'ru': '26 октября', 'en': 'October 26', 'zh': '10月26日'}, 'hash': '2f15e1bddecd9786', 'authors': ['Yassir Lairgi', 'Ludovic Moncla', 'Khalid Benabdeslem', 'Rémy Cazabet', 'Pierre Cléau'], 'affiliations': ['GAUC, Lyon, France', 'LIRIS, INSA Lyon, Université Claude Bernard Lyon 1, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.22590.jpg', 'data': {'categories': ['#dataset', '#games', '#data', '#transfer_learning', '#optimization', '#multimodal'], 'emoji': '⚛️', 'ru': {'title': 'ATOM: Адаптивное построение временных графов знаний из текста', 'desc': 'Статья представляет метод ATOM для создания и обновления временных графов знаний из неструктурированного текста с использованием few-shot подхода. Ключевая идея заключается в разбиении документов на минимальные атомарные факты, что повышает полноту и стабильность извлечения информации. Метод использует двойное временное моделирование, различающее момент наблюдения данных и период их валидности, а затем объединяет атомарные графы параллельно. Эксперименты показывают улучшение полноты на 18%, стабильности на 17% и снижение задержки более чем на 90% по сравнению с базовыми методами.'}, 'en': {'title': 'ATOM: Revolutionizing Temporal Knowledge Graphs with Few-Shot Learning', 'desc': "ATOM is a novel approach designed to create and update Temporal Knowledge Graphs (TKGs) from unstructured text using few-shot learning techniques. It addresses the limitations of traditional static knowledge graph construction by focusing on the dynamic nature of real-world data, allowing for continuous updates. By breaking down documents into minimal 'atomic' facts, ATOM enhances the exhaustivity and stability of knowledge extraction. The method employs dual-time modeling to differentiate between the observation and validity of information, resulting in significant improvements in performance metrics such as exhaustivity, stability, and latency."}, 'zh': {'title': 'ATOM：动态知识图谱构建的新方法', 'desc': 'ATOM是一种少量样本、可扩展的方法，用于从非结构化文本中构建和更新时间知识图谱。它通过将输入文档拆分为最小的、自包含的“原子”事实，来提高知识提取的全面性和稳定性。ATOM采用双时间建模，区分信息被观察的时间和信息有效的时间，从而构建原子时间知识图谱。实验证明，ATOM在全面性、稳定性和延迟方面均显著优于传统方法，展示了其在动态知识图谱构建中的强大扩展潜力。'}}}, {'id': 'https://huggingface.co/papers/2510.22373', 'title': 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations', 'url': 'https://huggingface.co/papers/2510.22373', 'abstract': "VisJudge-Bench is a benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality, revealing gaps compared to human experts and demonstrating improvements with the VisJudge model.  \t\t\t\t\tAI-generated summary \t\t\t\t Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.", 'score': 1, 'issue_id': 6674, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': 'e827dfbea1866daf', 'authors': ['Yupeng Xie', 'Zhiyang Zhang', 'Yifan Wu', 'Sirong Lu', 'Jiayi Zhang', 'Zhaoyang Yu', 'Jinlin Wang', 'Sirui Hong', 'Bang Liu', 'Chenglin Wu', 'Yuyu Luo'], 'affiliations': ['DeepWisdom', 'The Hong Kong University of Science and Technology (Guangzhou)', 'Université de Montréal & Mila'], 'pdf_title_img': 'assets/pdf/title_img/2510.22373.jpg', 'data': {'categories': ['#benchmark', '#interpretability', '#multimodal', '#optimization'], 'emoji': '📊', 'ru': {'title': 'VisJudge: учим AI понимать красоту графиков', 'desc': 'Статья представляет VisJudge-Bench — первый комплексный бенчмарк для оценки способности multimodal LLM оценивать эстетику и качество визуализаций данных. Benchmark содержит 3090 образцов с экспертными аннотациями, охватывающих 32 типа графиков из реальных сценариев. Тестирование показало, что даже продвинутые модели типа GPT-5 значительно уступают человеческим экспертам в оценке визуализаций. Предложенная модель VisJudge сокращает разрыв с человеческой оценкой на 19.8% по метрике MAE и улучшает корреляцию с экспертами на 58.7%.'}, 'en': {'title': 'Bridging the Gap in Visualization Quality Assessment with VisJudge-Bench', 'desc': 'VisJudge-Bench is a new benchmark designed to evaluate how well multimodal large language models (MLLMs) can assess the aesthetics and quality of visualizations. It includes over 3,000 expert-annotated examples that cover various types of visualizations, making it a comprehensive tool for testing. The findings show that even advanced models like GPT-5 struggle to match human expert evaluations, with notable gaps in accuracy and correlation. To improve this, the VisJudge model was introduced, which significantly enhances performance in aesthetic assessments, demonstrating a marked reduction in error rates compared to existing MLLMs.'}, 'zh': {'title': '可视化评估的新基准与模型', 'desc': 'VisJudge-Bench是一个用于评估多模态大型语言模型（MLLMs）在可视化美学和质量评估方面表现的基准。该基准包含3090个来自真实场景的专家标注样本，涵盖32种图表类型的单个可视化、多重可视化和仪表板。研究表明，即使是最先进的MLLM（如GPT-5），在判断上与人类专家相比仍存在显著差距。为此，我们提出了VisJudge模型，专门用于可视化美学和质量评估，实验结果显示其在与人类判断的一致性上有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2510.22319', 'title': 'GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via\n  Regulated Clipping', 'url': 'https://huggingface.co/papers/2510.22319', 'abstract': 'GRPO-Guard enhances GRPO-based reinforcement learning by normalizing importance ratios and reweighting gradients, mitigating over-optimization in flow-matching models without heavy KL regularization.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.', 'score': 1, 'issue_id': 6681, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '4224bc436e599f94', 'authors': ['Jing Wang', 'Jiajun Liang', 'Jie Liu', 'Henglin Liu', 'Gongye Liu', 'Jun Zheng', 'Wanyuan Pang', 'Ao Ma', 'Zhenyu Xie', 'Xintao Wang', 'Meng Wang', 'Pengfei Wan', 'Xiaodan Liang'], 'affiliations': ['CUHK MMLab', 'HKUST', 'Kuaishou Technology', 'Shenzhen Campus of Sun Yat-Sen University', 'Tsinghua University', 'UCAS', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2510.22319.jpg', 'data': {'categories': ['#optimization', '#rlhf', '#training', '#rl'], 'emoji': '🛡️', 'ru': {'title': 'Защита от переобучения в reinforcement learning для диффузионных моделей', 'desc': 'Статья представляет GRPO-Guard — улучшенный метод обучения с подкреплением для flow-matching моделей. Авторы обнаружили, что стандартный GRPO страдает от смещения распределения importance ratios, что приводит к неявной переоптимизации: proxy reward растёт, но качество изображений и соответствие текстовым промптам ухудшаются. GRPO-Guard решает проблему через нормализацию коэффициентов важности и перевзвешивание градиентов, обеспечивая стабильное обучение без тяжёлой KL-регуляризации. Эксперименты на SD3.5M и Flux.1-dev показывают значительное снижение переоптимизации при сохранении качества генерации.'}, 'en': {'title': 'Stabilizing Reinforcement Learning with GRPO-Guard', 'desc': 'The paper introduces GRPO-Guard, an enhancement to GRPO-based reinforcement learning that addresses the issue of over-optimization in flow-matching models. It normalizes importance ratios and reweights gradients to ensure that policy updates are stable and effective, preventing the model from becoming overly confident in its predictions. By correcting the distribution of importance ratios, GRPO-Guard allows for better clipping of gradients, which helps maintain the quality of generated outputs. Experimental results show that this method improves performance across various tasks without the need for heavy KL regularization.'}, 'zh': {'title': 'GRPO-Guard：稳定优化，减轻过度优化的利器', 'desc': 'GRPO-Guard 是一种增强 GRPO 基于强化学习的方法，通过归一化重要性比率和重新加权梯度，减轻流匹配模型中的过度优化问题，而无需重度的 KL 正则化。在 GRPO 框架中，策略更新依赖于重要性比率的裁剪，以限制过于自信的正负梯度。然而，实际中我们观察到重要性比率分布的系统性偏移，导致正优势样本无法进入裁剪区域，从而使机制无法有效约束过度自信的更新。GRPO-Guard 通过引入比率归一化和梯度重新加权策略，稳定了优化过程，显著减少了隐性过度优化，同时保持或提高了生成质量。'}}}, {'id': 'https://huggingface.co/papers/2510.21323', 'title': 'VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a\n  Unified Concept Set', 'url': 'https://huggingface.co/papers/2510.21323', 'abstract': 'VL-SAE, a sparse autoencoder, enhances vision-language alignment by correlating neurons to unified concepts, improving interpretability and performance in tasks like zero-shot image classification and hallucination elimination.  \t\t\t\t\tAI-generated summary \t\t\t\t The alignment of vision-language representations endows current Vision-Language Models (VLMs) with strong multi-modal reasoning capabilities. However, the interpretability of the alignment component remains uninvestigated due to the difficulty in mapping the semantics of multi-modal representations into a unified concept set. To address this problem, we propose VL-SAE, a sparse autoencoder that encodes vision-language representations into its hidden activations. Each neuron in its hidden layer correlates to a concept represented by semantically similar images and texts, thereby interpreting these representations with a unified concept set. To establish the neuron-concept correlation, we encourage semantically similar representations to exhibit consistent neuron activations during self-supervised training. First, to measure the semantic similarity of multi-modal representations, we perform their alignment in an explicit form based on cosine similarity. Second, we construct the VL-SAE with a distance-based encoder and two modality-specific decoders to ensure the activation consistency of semantically similar representations. Experiments across multiple VLMs (e.g., CLIP, LLaVA) demonstrate the superior capability of VL-SAE in interpreting and enhancing the vision-language alignment. For interpretation, the alignment between vision and language representations can be understood by comparing their semantics with concepts. For enhancement, the alignment can be strengthened by aligning vision-language representations at the concept level, contributing to performance improvements in downstream tasks, including zero-shot image classification and hallucination elimination. Codes are available at https://github.com/ssfgunner/VL-SAE.', 'score': 1, 'issue_id': 6667, 'pub_date': '2025-10-24', 'pub_date_card': {'ru': '24 октября', 'en': 'October 24', 'zh': '10月24日'}, 'hash': '41279ff25d4d0ecb', 'authors': ['Shufan Shen', 'Junshu Sun', 'Qingming Huang', 'Shuhui Wang'], 'affiliations': ['Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2510.21323.jpg', 'data': {'categories': ['#alignment', '#multimodal', '#interpretability', '#cv', '#hallucinations', '#training'], 'emoji': '🔍', 'ru': {'title': 'Разреженный автоэнкодер для понимания и усиления связи между зрением и языком', 'desc': 'VL-SAE — это разреженный автоэнкодер, который повышает интерпретируемость визуально-языкового выравнивания в VLM моделях путём сопоставления нейронов с единым набором концептов. Каждый нейрон скрытого слоя коррелирует с концептом, представленным семантически похожими изображениями и текстами. Модель обучается самообучением, обеспечивая согласованную активацию нейронов для семантически близких представлений разных модальностей. Эксперименты показывают улучшение производительности в задачах zero-shot классификации изображений и устранения галлюцинаций в моделях типа CLIP и LLaVA.'}, 'en': {'title': 'Enhancing Vision-Language Alignment with VL-SAE', 'desc': 'The paper introduces VL-SAE, a sparse autoencoder designed to improve the alignment between vision and language representations in multi-modal models. By correlating neurons in its hidden layer to unified concepts derived from semantically similar images and texts, VL-SAE enhances both interpretability and performance. The model employs self-supervised training to ensure consistent neuron activations for similar representations, using cosine similarity for semantic alignment. Experiments show that VL-SAE significantly boosts the effectiveness of vision-language models in tasks like zero-shot image classification and reducing hallucinations.'}, 'zh': {'title': 'VL-SAE：提升视觉-语言对齐的稀疏自编码器', 'desc': 'VL-SAE是一种稀疏自编码器，通过将视觉和语言表示关联到统一的概念，增强了视觉-语言对齐的能力。它的隐藏层中的每个神经元与语义相似的图像和文本所代表的概念相关联，从而提高了模型的可解释性和性能。通过自监督训练，VL-SAE鼓励语义相似的表示在神经元激活上保持一致。实验表明，VL-SAE在零样本图像分类和消除幻觉等任务中表现出色，显著提升了视觉-语言模型的对齐能力。'}}}, {'id': 'https://huggingface.co/papers/2510.20155', 'title': 'PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D\n  Part Understanding', 'url': 'https://huggingface.co/papers/2510.20155', 'abstract': "PartNeXt, a high-quality, textured 3D dataset with fine-grained part labels, improves performance in class-agnostic part segmentation and 3D part-centric question answering, highlighting gaps in open-vocabulary part grounding.  \t\t\t\t\tAI-generated summary \t\t\t\t Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset's superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.", 'score': 1, 'issue_id': 6672, 'pub_date': '2025-10-23', 'pub_date_card': {'ru': '23 октября', 'en': 'October 23', 'zh': '10月23日'}, 'hash': '338d5df27efaaad5', 'authors': ['Penghao Wang', 'Yiyang He', 'Xin Lv', 'Yukai Zhou', 'Lan Xu', 'Jingyi Yu', 'Jiayuan Gu'], 'affiliations': ['ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2510.20155.jpg', 'data': {'categories': ['#dataset', '#3d', '#benchmark', '#cv'], 'emoji': '🧩', 'ru': {'title': 'PartNeXt: текстурированные 3D модели с детальной разметкой частей для продвинутого понимания объектов', 'desc': 'Исследователи представили PartNeXt — новый датасет из более чем 23,000 высококачественных текстурированных 3D моделей с детальной иерархической разметкой частей объектов в 50 категориях. Датасет решает проблемы предыдущих наборов данных, таких как PartNet, которые использовали модели без текстур и требовали экспертной аннотации. На PartNeXt протестировали две задачи: class-agnostic сегментацию частей объектов и 3D question answering о частях объектов, где современные методы показали существенные пробелы в понимании. Обучение Point-SAM на PartNeXt продемонстрировало значительное улучшение качества по сравнению с PartNet, подтверждая превосходство нового датасета.'}, 'en': {'title': 'PartNeXt: Elevating 3D Understanding with Textured Models and Fine-Grained Labels', 'desc': "PartNeXt is a new dataset designed to enhance the understanding of 3D objects by providing high-quality, textured models with detailed part labels. It addresses limitations of previous datasets like PartNet, which lacked texture and relied on expert annotations, making them less scalable. The dataset is evaluated on two key tasks: class-agnostic part segmentation and 3D part-centric question answering, revealing challenges in current models' ability to handle fine-grained parts. By offering a diverse and well-annotated resource, PartNeXt aims to advance research in 3D part segmentation and improve the performance of machine learning models in this area."}, 'zh': {'title': 'PartNeXt：推动3D理解的新数据集', 'desc': 'PartNeXt是一个高质量的3D数据集，包含超过23,000个带有细粒度部件标签的纹理模型，旨在提升无类别部件分割和3D部件中心问答的性能。与以往的数据集相比，PartNeXt克服了依赖无纹理几何体和专家注释的局限性，提供了更好的可扩展性和可用性。通过在两个任务上进行基准测试，PartNeXt展示了其在细粒度部件处理和开放词汇部件定位方面的优势。该数据集的多任务评估和纹理感知标签为结构化3D理解的研究开辟了新的方向。'}}}, {'id': 'https://huggingface.co/papers/2510.23828', 'title': "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural\n  Processing of Figurative Language", 'url': 'https://huggingface.co/papers/2510.23828', 'abstract': 'We present a comprehensive evaluation of the ability of large language models (LLMs) to process culturally grounded language, specifically to understand and pragmatically use figurative expressions that encode local knowledge and cultural nuance. Using figurative language as a proxy for cultural nuance and local knowledge, we design evaluation tasks for contextual understanding, pragmatic use, and connotation interpretation in Arabic and English. We evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms, multidialectal Arabic proverbs, and English proverbs. Our results show a consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower than for English proverbs, and performance for Egyptian idioms is 10.28% lower than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07% relative to understanding, though providing contextual idiomatic sentences improves accuracy by 10.66%. Models also struggle with connotative meaning, reaching at most 85.58% agreement with human annotators on idioms with 100% inter-annotator agreement. These findings demonstrate that figurative language serves as an effective diagnostic for cultural reasoning: while LLMs can often interpret figurative meaning, they face challenges in using it appropriately. To support future research, we release Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation.', 'score': 0, 'issue_id': 6687, 'pub_date': '2025-10-27', 'pub_date_card': {'ru': '27 октября', 'en': 'October 27', 'zh': '10月27日'}, 'hash': 'd4ec15f710d0fa0b', 'authors': ['Mena Attia', 'Aashiq Muhamed', 'Mai Alkhamissi', 'Thamar Solorio', 'Mona Diab'], 'affiliations': ['Carnegie Mellon University', 'MBZUAI'], 'pdf_title_img': 'assets/pdf/title_img/2510.23828.jpg', 'data': {'categories': ['#dataset', '#low_resource', '#interpretability', '#alignment', '#multilingual', '#benchmark'], 'emoji': '🎭', 'ru': {'title': 'Культурные нюансы — ахиллесова пята больших языковых моделей', 'desc': 'Исследователи провели комплексную оценку способности LLM понимать и использовать образный язык, который отражает культурные особенности и локальные знания. Они протестировали 22 открытые и закрытые модели на египетских идиомах, арабских пословицах разных диалектов и английских пословицах. Результаты показали чёткую иерархию: модели справляются с английскими пословицами лучше всего, на 4.29% хуже с арабскими, и на 10.28% хуже с египетскими идиомами. Особенно сложной оказалась задача прагматического использования образных выражений — точность упала на 14.07%, а согласие с человеческими аннотаторами по коннотациям достигло максимум 85.58%, что демонстрирует ограниченность культурного понимания у современных LLM.'}, 'en': {'title': 'Unlocking Cultural Nuance: Evaluating LLMs with Figurative Language', 'desc': 'This paper evaluates how well large language models (LLMs) understand and use culturally specific language, focusing on figurative expressions that reflect local knowledge. The authors created tasks to assess contextual understanding, pragmatic application, and connotation interpretation in both Arabic and English. They tested 22 different LLMs on various idioms and proverbs, revealing that models perform better on English than Arabic, with significant drops in accuracy for pragmatic tasks. The study highlights that while LLMs can grasp figurative meanings, they struggle with appropriate usage, indicating the need for improved cultural reasoning in AI.'}, 'zh': {'title': '大型语言模型与文化语言的挑战', 'desc': '本文评估了大型语言模型（LLMs）处理文化相关语言的能力，特别是理解和使用隐喻表达的能力。我们设计了评估任务，考察模型在阿拉伯语和英语中的上下文理解、实用使用和内涵解释。结果显示，阿拉伯谚语的平均准确率比英语谚语低4.29%，而埃及成语的表现更低，低于阿拉伯谚语10.28%。这些发现表明，隐喻语言是文化推理的有效诊断工具，尽管LLMs能够解释隐喻意义，但在适当使用方面仍面临挑战。'}}}, {'id': 'https://huggingface.co/papers/2510.22264', 'title': 'PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding', 'url': 'https://huggingface.co/papers/2510.22264', 'abstract': 'PatenTEB is a comprehensive benchmark for patent text embeddings with 15 tasks, and the patembed model family demonstrates strong generalization across various patent-specific challenges.  \t\t\t\t\tAI-generated summary \t\t\t\t Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.', 'score': 0, 'issue_id': 6676, 'pub_date': '2025-10-25', 'pub_date_card': {'ru': '25 октября', 'en': 'October 25', 'zh': '10月25日'}, 'hash': '8e4bb95367ac7cbc', 'authors': ['Iliass Ayaou', 'Denis Cavallucci'], 'affiliations': ['INSA Strasbourg, France'], 'pdf_title_img': 'assets/pdf/title_img/2510.22264.jpg', 'data': {'categories': ['#benchmark', '#training', '#open_source', '#multimodal', '#transfer_learning', '#dataset'], 'emoji': '📜', 'ru': {'title': 'PatenTEB: специализированный бенчмарк для патентных эмбеддингов нового поколения', 'desc': 'Исследователи представили PatenTEB — комплексный бенчмарк для оценки text embeddings патентных документов, включающий 15 задач и более 2 миллионов примеров. Бенчмарк учитывает специфические сложности патентной области: асимметричный поиск фрагментов в документах, специализированную выборку hard negatives и разделение данных по доменам. Авторы разработали семейство моделей patembed с параметрами от 67M до 344M и контекстом до 4096 токенов, обученных с использованием multi-task learning. Модели показали state-of-the-art результаты на внешних бенчмарках, демонстрируя отличную генерализацию благодаря multi-task обучению и предобучению на патентных данных.'}, 'en': {'title': 'Revolutionizing Patent Analysis with PatenTEB and Patembed Models', 'desc': 'PatenTEB is a new benchmark designed specifically for evaluating patent text embeddings across 15 diverse tasks, including retrieval and classification. The patembed model family, which ranges from 67M to 344M parameters, shows impressive generalization capabilities in handling patent-specific challenges. By utilizing techniques like domain-stratified splits and hard negative mining, PatenTEB addresses gaps in existing benchmarks that do not cater to the unique aspects of patent texts. The results indicate that multi-task training and domain-pretrained models significantly enhance performance on various tasks, demonstrating the effectiveness of this approach in patent analysis.'}, 'zh': {'title': '专利文本嵌入的全面基准与强大模型', 'desc': 'PatenTEB是一个全面的专利文本嵌入基准，包含15个任务，旨在解决专利特有的挑战。该基准通过领域分层划分、特定领域的困难负样本挖掘，以及系统覆盖不对称片段与文档匹配场景，提供了2.06百万个示例。patembed模型系列通过多任务训练开发，参数范围从6700万到3.44亿，支持最长4096个标记的上下文。外部验证显示，patembed模型在多个任务上表现出色，证明了多任务训练和领域预训练初始化的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (14)', '#agi (1)', '#alignment (5)', '#architecture (5)', '#audio (2)', '#benchmark (27)', '#cv (6)', '#data (4)', '#dataset (18)', '#diffusion (6)', '#ethics (2)', '#games (4)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (3)', '#interpretability (9)', '#leakage', '#long_context (2)', '#low_resource (3)', '#machine_translation', '#math', '#multilingual (4)', '#multimodal (12)', '#open_source (10)', '#optimization (17)', '#plp', '#rag', '#reasoning (16)', '#rl (6)', '#rlhf (5)', '#robotics', '#science (2)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (20)', '#transfer_learning (6)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-10-29 22:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-10-29 22:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-10-29 22:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    