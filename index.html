
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 18 papers. August 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°</span> | <span id="title-articles-count">18 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-08-22.html">â¬…ï¸ <span id="prev-date">22.08</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-08-26.html">â¡ï¸ <span id="next-date">26.08</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-08.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 25', 'zh': '8æœˆ25æ—¥'};
        let feedDateNext = {'ru': '26.08', 'en': '08/26', 'zh': '8æœˆ26æ—¥'};
        let feedDatePrev = {'ru': '22.08', 'en': '08/22', 'zh': '8æœˆ22æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2508.16153', 'title': 'AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs', 'url': 'https://huggingface.co/papers/2508.16153', 'abstract': 'A novel memory-augmented reinforcement learning paradigm enables adaptive LLM agents to continually learn without fine-tuning, using episodic memory and a neural case-selection policy.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches 66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds 4.7% to 9.6% absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly.', 'score': 51, 'issue_id': 5518, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': '6286f08d6e4a4187', 'authors': ['Huichi Zhou', 'Yihang Chen', 'Siyuan Guo', 'Xue Yan', 'Kin Hei Lee', 'Zihan Wang', 'Ka Yiu Lee', 'Guchun Zhang', 'Kun Shao', 'Linyi Yang', 'Jun Wang'], 'affiliations': ['AI Centre, UCL', 'Huawei Noahs Ark Lab, UK', 'Institute of Automation, CAS', 'Jilin University'], 'pdf_title_img': 'assets/pdf/title_img/2508.16153.jpg', 'data': {'categories': ['#agi', '#optimization', '#rl', '#agents'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ LLM-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒÑÑ‚Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¸Ñ… Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¸ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ĞºĞ°Ğº Ğ¼Ğ°Ñ€ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ (M-MDP). Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ AgentFly Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'Empowering LLMs with Memory for Continuous Learning', 'desc': 'This paper presents a new approach for adaptive Large Language Model (LLM) agents that allows them to learn continuously without the need for fine-tuning. The method utilizes a Memory-augmented Markov Decision Process (M-MDP) that incorporates episodic memory and a neural case-selection policy to make decisions based on past experiences. By leveraging memory-based online reinforcement learning, the agents can adapt to new information efficiently, improving their performance on various tasks. The results show that this approach outperforms traditional training methods, making it a promising direction for developing versatile LLM agents capable of ongoing learning.'}, 'zh': {'title': 'è®°å¿†å¢å¼ºçš„è‡ªé€‚åº”å­¦ä¹ ï¼ŒæŒç»­è¿›æ­¥æ— å¾®è°ƒ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è®°å¿†å¢å¼ºå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œä½¿è‡ªé€‚åº”çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†èƒ½å¤Ÿåœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹æŒç»­å­¦ä¹ ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åŸºäºè®°å¿†çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ å®ç°äº†ä½æˆæœ¬çš„æŒç»­é€‚åº”ï¼Œé¿å…äº†é™æ€çš„æ‰‹å·¥åæ€æµç¨‹å’Œè®¡ç®—å¯†é›†å‹çš„æ¢¯åº¦æ›´æ–°ã€‚æˆ‘ä»¬å°†å…¶å½¢å¼åŒ–ä¸ºè®°å¿†å¢å¼ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆM-MDPï¼‰ï¼Œå¹¶ä½¿ç”¨ç¥ç»æ¡ˆä¾‹é€‰æ‹©ç­–ç•¥æ¥æŒ‡å¯¼è¡ŒåŠ¨å†³ç­–ã€‚æˆ‘ä»¬çš„ä»£ç†æ¨¡å‹åœ¨æ·±åº¦ç ”ç©¶ç¯å¢ƒä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„åŸºäºè®­ç»ƒçš„æ–¹æ³•ï¼Œå±•ç¤ºäº†åœ¨å¼€æ”¾å¼æŠ€èƒ½è·å–å’Œæ·±åº¦ç ”ç©¶åœºæ™¯ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.08240', 'title': 'ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks', 'url': 'https://huggingface.co/papers/2508.08240', 'abstract': "ODYSSEY is a unified mobile manipulation framework for quadruped robots that integrates high-level task planning with low-level whole-body control, addressing challenges in egocentric perception, generalization, and coordination in unstructured environments.  \t\t\t\t\tAI-generated summary \t\t\t\t Language-guided long-horizon mobile manipulation has long been a grand challenge in embodied semantic reasoning, generalizable manipulation, and adaptive locomotion. Three fundamental limitations hinder progress: First, although large language models have improved spatial reasoning and task planning through semantic priors, existing implementations remain confined to tabletop scenarios, failing to address the constrained perception and limited actuation ranges of mobile platforms. Second, current manipulation strategies exhibit insufficient generalization when confronted with the diverse object configurations encountered in open-world environments. Third, while crucial for practical deployment, the dual requirement of maintaining high platform maneuverability alongside precise end-effector control in unstructured settings remains understudied.   In this work, we present ODYSSEY, a unified mobile manipulation framework for agile quadruped robots equipped with manipulators, which seamlessly integrates high-level task planning with low-level whole-body control. To address the challenge of egocentric perception in language-conditioned tasks, we introduce a hierarchical planner powered by a vision-language model, enabling long-horizon instruction decomposition and precise action execution. At the control level, our novel whole-body policy achieves robust coordination across challenging terrains. We further present the first benchmark for long-horizon mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through successful sim-to-real transfer, we demonstrate the system's generalization and robustness in real-world deployments, underscoring the practicality of legged manipulators in unstructured environments. Our work advances the feasibility of generalized robotic assistants capable of complex, dynamic tasks. Our project page: https://kaijwang.github.io/odyssey.github.io/", 'score': 34, 'issue_id': 5522, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '313edb98990f0684', 'authors': ['Kaijun Wang', 'Liqin Lu', 'Mingyu Liu', 'Jianuo Jiang', 'Zeju Li', 'Bolin Zhang', 'Wancai Zheng', 'Xinyi Yu', 'Hao Chen', 'Chunhua Shen'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen', 'Zhejiang University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.08240.jpg', 'data': {'categories': ['#robotics', '#transfer_learning', '#optimization', '#reasoning', '#agents', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²: Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'ODYSSEY - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ¾Ğ½Ğ¾Ğ³Ğ¸Ñ… Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ², Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑÑÑ‰Ğ°Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾ÑƒÑ€Ğ¾Ğ²Ğ½ĞµĞ²Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²ÑĞµĞ¼ Ñ‚ĞµĞ»Ğ¾Ğ¼. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ñ, Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞĞ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ Ñ‚ĞµĞ»Ğ° Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ°, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ°Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ÑƒÑ ĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ²ĞµÑ€Ñ…Ğ½Ğ¾ÑÑ‚ÑÑ…. ODYSSEY Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ¸Ñ€Ğµ, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ¾Ğ½Ğ¾Ğ³Ğ¸Ñ… Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ² Ğ½ĞµÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ….'}, 'en': {'title': 'Empowering Quadruped Robots for Complex Mobile Manipulation', 'desc': 'ODYSSEY is a new framework designed for quadruped robots that combines high-level planning with low-level control to improve mobile manipulation. It addresses key challenges such as understanding tasks through language, adapting to various object configurations, and coordinating movements in complex environments. The framework uses a vision-language model for better perception and task execution, allowing robots to follow long-term instructions effectively. Additionally, it includes a benchmark for testing these capabilities in real-world scenarios, showcasing the potential of robots to perform complex tasks in dynamic settings.'}, 'zh': {'title': 'å››è¶³æœºå™¨äººç§»åŠ¨æ“æ§çš„æ–°çªç ´', 'desc': 'ODYSSEYæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç§»åŠ¨æ“æ§æ¡†æ¶ï¼Œä¸“ä¸ºå››è¶³æœºå™¨äººè®¾è®¡ï¼Œç»“åˆäº†é«˜å±‚æ¬¡çš„ä»»åŠ¡è§„åˆ’å’Œä½å±‚æ¬¡çš„å…¨èº«æ§åˆ¶ã€‚è¯¥æ¡†æ¶è§£å†³äº†åœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­è‡ªæˆ‘ä¸­å¿ƒæ„ŸçŸ¥ã€æ³›åŒ–å’Œåè°ƒç­‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹çš„åˆ†å±‚è§„åˆ’å™¨ï¼Œèƒ½å¤Ÿå®ç°é•¿æ—¶é—´æŒ‡ä»¤çš„åˆ†è§£å’Œç²¾ç¡®æ‰§è¡Œã€‚é€šè¿‡æˆåŠŸçš„æ¨¡æ‹Ÿåˆ°ç°å®è½¬ç§»ï¼Œå±•ç¤ºäº†è¯¥ç³»ç»Ÿåœ¨çœŸå®ç¯å¢ƒä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼Œæ¨åŠ¨äº†å¤æ‚åŠ¨æ€ä»»åŠ¡çš„æœºå™¨äººåŠ©æ‰‹çš„å¯è¡Œæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.14029', 'title': 'Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR', 'url': 'https://huggingface.co/papers/2508.14029', 'abstract': "An online self-play strategy with variational problem synthesis for RLVR training maintains policy entropy and improves Pass@k performance on reasoning benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.", 'score': 27, 'issue_id': 5520, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': '615692d5b0adab3d', 'authors': ['Xiao Liang', 'Zhongzhi Li', 'Yeyun Gong', 'Yelong Shen', 'Ying Nian Wu', 'Zhijiang Guo', 'Weizhu Chen'], 'affiliations': ['Hong Kong University of Science and Technology', 'Hong Kong University of Science and Technology (Guangzhou)', 'Microsoft', 'School of Artificial Intelligence, Chinese Academy of Sciences', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2508.14029.jpg', 'data': {'categories': ['#rl', '#benchmark', '#optimization', '#rlhf', '#reasoning', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¡Ğ°Ğ¼Ğ¾ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· ÑĞ¸Ğ½Ñ‚ĞµĞ· Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ğ°Ğ¼Ğ¸ (RLVR) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Self-play with Variational problem Synthesis (SvS) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¸. Ğ­Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ Pass@k Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ ÑĞ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ RLVR. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 12 Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°.'}, 'en': {'title': 'Enhancing Reasoning Performance with Self-Play and Variational Synthesis', 'desc': "This paper introduces a novel online self-play strategy called Self-play with Variational problem Synthesis (SvS) for training Reinforcement Learning with Verifiable Rewards (RLVR) in large language models. The authors identify that traditional RLVR training can lead to reduced policy entropy, which negatively impacts the diversity of generated outputs and overall performance on reasoning tasks. By augmenting and updating training problems, SvS maintains policy entropy and enhances the model's ability to generate diverse and accurate responses. The results show significant improvements in Pass@k performance on various reasoning benchmarks, demonstrating the effectiveness and robustness of the proposed method."}, 'zh': {'title': 'è‡ªæˆ‘å¯¹å¼ˆç­–ç•¥æå‡æ¨ç†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨çº¿è‡ªæˆ‘å¯¹å¼ˆç­–ç•¥ï¼Œç»“åˆå˜åˆ†é—®é¢˜åˆæˆï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡å¢å¼ºå’Œæ›´æ–°è®­ç»ƒé—®é¢˜ï¼Œè§£å†³äº†ä¼ ç»ŸRLVRè®­ç»ƒä¸­ç­–ç•¥ç†µé™ä½çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†ç”Ÿæˆå¤šæ ·æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆç»´æŒäº†ç­–ç•¥ç†µï¼Œå¹¶æ˜¾è‘—æå‡äº†åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­çš„Pass@kè¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç­–ç•¥åœ¨ä¸åŒè§„æ¨¡çš„æ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13013', 'title': 'EgoTwin: Dreaming Body and View in First Person', 'url': 'https://huggingface.co/papers/2508.13013', 'abstract': "EgoTwin, a diffusion transformer framework, addresses viewpoint alignment and causal interplay in joint egocentric video and human motion generation using a head-centric motion representation and cybernetics-inspired interaction mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework.", 'score': 14, 'issue_id': 5521, 'pub_date': '2025-08-18', 'pub_date_card': {'ru': '18 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 18', 'zh': '8æœˆ18æ—¥'}, 'hash': '3573d981a35e3de1', 'authors': ['Jingqiao Xiu', 'Fangzhou Hong', 'Yicong Li', 'Mengze Li', 'Wentao Wang', 'Sirui Han', 'Liang Pan', 'Ziwei Liu'], 'affiliations': ['Hong Kong University of Science and Technology', 'Nanyang Technological University', 'National University of Singapore', 'Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2508.13013.jpg', 'data': {'categories': ['#diffusion', '#video', '#multimodal', '#dataset', '#architecture'], 'emoji': 'ğŸ‘€', 'ru': {'title': 'EgoTwin: Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°', 'desc': 'EgoTwin - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ° Ğ¸ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ½Ğ¾-ÑĞ»ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ñ‹ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¸Ğ±ĞµÑ€Ğ½ĞµÑ‚Ğ¸ĞºĞ¾Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ñ€Ğ¸Ğ¿Ğ»ĞµÑ‚Ğ¾Ğ² Ñ‚ĞµĞºÑÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° EgoTwin.'}, 'en': {'title': 'EgoTwin: Bridging Video and Motion in First-Person View', 'desc': "EgoTwin is a new framework that combines video generation and human motion modeling in a first-person perspective. It tackles two main challenges: ensuring that the camera movement matches the head movement of the person and making sure that the generated human actions correspond to the visual changes in the video. The framework uses a special representation that focuses on the head's motion and a mechanism inspired by cybernetics to understand how video and motion influence each other. By creating a large dataset and new evaluation methods, EgoTwin shows significant improvements in generating realistic egocentric videos and human movements."}, 'zh': {'title': 'EgoTwinï¼šè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ä¸äººç±»è¿åŠ¨ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'EgoTwinæ˜¯ä¸€ä¸ªæ‰©æ•£å˜æ¢å™¨æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”åˆè‡ªæˆ‘ä¸­å¿ƒè§†é¢‘å’Œäººç±»è¿åŠ¨ç”Ÿæˆä¸­çš„è§†è§’å¯¹é½å’Œå› æœå…³ç³»é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä»¥å¤´éƒ¨ä¸ºä¸­å¿ƒçš„è¿åŠ¨è¡¨ç¤ºï¼Œç»“åˆå—ç½‘ç»œæ§åˆ¶å¯å‘çš„äº¤äº’æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰è§†é¢‘ä¸è¿åŠ¨ä¹‹é—´çš„å› æœå…³ç³»ã€‚é€šè¿‡å»ºç«‹ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼ŒEgoTwinéœ€è¦å¤„ç†ç›¸æœºè½¨è¿¹ä¸äººç±»è¿åŠ¨çš„å¤´éƒ¨è½¨è¿¹ä¹‹é—´çš„å¯¹é½ï¼Œä»¥åŠåˆæˆçš„äººç±»è¿åŠ¨ä¸ç›¸é‚»è§†é¢‘å¸§çš„è§†è§‰åŠ¨æ€ä¹‹é—´çš„å› æœå…³ç³»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEgoTwinåœ¨è§†é¢‘ä¸è¿åŠ¨çš„ä¸€è‡´æ€§ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13650', 'title': 'CRISP: Persistent Concept Unlearning via Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2508.13650', 'abstract': "CRISP is a parameter-efficient method using sparse autoencoders to permanently remove unwanted knowledge from large language models while preserving their utility.  \t\t\t\t\tAI-generated summary \t\t\t\t As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.", 'score': 11, 'issue_id': 5520, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': 'bfe846ca6ab823e6', 'authors': ['Tomer Ashuach', 'Dana Arad', 'Aaron Mueller', 'Martin Tutek', 'Yonatan Belinkov'], 'affiliations': ['Boston University', 'TakeLab, University of Zagreb', 'Technion Israel Institute of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2508.13650.jpg', 'data': {'categories': ['#inference', '#ethics', '#training', '#security'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'CRISP - ÑÑ‚Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸Ğ· Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ²Ğ½Ğ¾ÑÑ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ĞµÑ‘ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹. CRISP Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¸Ğ´ĞµĞ½Ñ‚Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ² Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑĞ»Ğ¾ÑÑ… Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ñ… Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ CRISP Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ·Ğ°Ğ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'CRISP: Permanently Unlearning Harmful Knowledge in LLMs', 'desc': "CRISP is a novel method designed to efficiently remove unwanted knowledge from large language models (LLMs) using sparse autoencoders (SAEs). Unlike previous approaches that only intervene at inference time, CRISP makes permanent changes to the model's parameters, ensuring that harmful knowledge cannot be easily restored. The method identifies important features across different layers of the model and suppresses their activations, effectively achieving concept unlearning. Our experiments demonstrate that CRISP outperforms existing techniques in safety-critical tasks, maintaining the model's overall performance while ensuring a clear distinction between harmful and benign knowledge."}, 'zh': {'title': 'CRISPï¼šé«˜æ•ˆçš„çŸ¥è¯†å»é™¤æ–¹æ³•', 'desc': 'CRISPæ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°æ–¹æ³•ï¼Œåˆ©ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ°¸ä¹…æ€§åœ°å»é™¤ä¸å¿…è¦çš„çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„å®ç”¨æ€§ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›ä½¿ç”¨ï¼Œé€‰æ‹©æ€§å»é™¤ä¸å¿…è¦çŸ¥è¯†çš„éœ€æ±‚å˜å¾—å°¤ä¸ºé‡è¦ã€‚CRISPèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å¤šä¸ªå±‚æ¬¡ä¸­çš„æ˜¾è‘—SAEç‰¹å¾ï¼Œå¹¶æŠ‘åˆ¶å…¶æ¿€æ´»ï¼Œä»è€Œå®ç°æŒä¹…çš„æ¦‚å¿µé—å¿˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCRISPåœ¨å®‰å…¨å…³é”®çš„é—å¿˜ä»»åŠ¡ä¸­ä¼˜äºä¹‹å‰çš„æ–¹æ³•ï¼ŒæˆåŠŸå»é™¤äº†æœ‰å®³çŸ¥è¯†ï¼ŒåŒæ—¶ä¿ç•™äº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›å’Œé¢†åŸŸå†…èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.07877', 'title': 'Selective Contrastive Learning for Weakly Supervised Affordance\n  Grounding', 'url': 'https://huggingface.co/papers/2508.07877', 'abstract': "The method uses selective prototypical and pixel contrastive objectives to learn affordance-relevant cues from third-person demonstrations, improving upon traditional weakly supervised affordance grounding by focusing on both part and object levels.  \t\t\t\t\tAI-generated summary \t\t\t\t Facilitating an entity's interaction with objects requires accurately identifying parts that afford specific actions. Weakly supervised affordance grounding (WSAG) seeks to imitate human learning from third-person demonstrations, where humans intuitively grasp functional parts without needing pixel-level annotations. To achieve this, grounding is typically learned using a shared classifier across images from different perspectives, along with distillation strategies incorporating part discovery process. However, since affordance-relevant parts are not always easily distinguishable, models primarily rely on classification, often focusing on common class-specific patterns that are unrelated to affordance. To address this limitation, we move beyond isolated part-level learning by introducing selective prototypical and pixel contrastive objectives that adaptively learn affordance-relevant cues at both the part and object levels, depending on the granularity of the available information. Initially, we find the action-associated objects in both egocentric (object-focused) and exocentric (third-person example) images by leveraging CLIP. Then, by cross-referencing the discovered objects of complementary views, we excavate the precise part-level affordance clues in each perspective. By consistently learning to distinguish affordance-relevant regions from affordance-irrelevant background context, our approach effectively shifts activation from irrelevant areas toward meaningful affordance cues. Experimental results demonstrate the effectiveness of our method. Codes are available at github.com/hynnsk/SelectiveCL.", 'score': 10, 'issue_id': 5520, 'pub_date': '2025-08-11', 'pub_date_card': {'ru': '11 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 11', 'zh': '8æœˆ11æ—¥'}, 'hash': '1be5d42feb9fc4d7', 'authors': ['WonJun Moon', 'Hyun Seok Seong', 'Jae-Pil Heo'], 'affiliations': ['Sungkyunkwan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.07877.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#cv'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ»Ğ°Ğ±Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞµĞ»ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ Ğ¿Ğ¸ĞºÑĞµĞ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ†ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² ĞºĞ°Ğº Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ², Ñ‚Ğ°Ğº Ğ¸ Ñ†ĞµĞ»Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ CLIP Ğ´Ğ»Ñ Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸ ÑĞºĞ·Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑÑ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ² ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğ¸ Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing Affordance Grounding with Selective Learning Techniques', 'desc': 'This paper presents a novel approach to weakly supervised affordance grounding (WSAG) by utilizing selective prototypical and pixel contrastive objectives. The method enhances the learning of affordance-relevant cues from third-person demonstrations, focusing on both object and part levels. By leveraging CLIP to identify action-associated objects from different perspectives, the model effectively distinguishes meaningful affordance regions from irrelevant background. Experimental results show that this approach significantly improves the accuracy of identifying parts that facilitate specific actions, addressing limitations of traditional classification methods.'}, 'zh': {'title': 'é€šè¿‡é€‰æ‹©æ€§å­¦ä¹ æå‡åŠŸèƒ½åŸºç¡€å®šä½çš„å‡†ç¡®æ€§', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©æ€§åŸå‹å’Œåƒç´ å¯¹æ¯”ç›®æ ‡ï¼Œä»ç¬¬ä¸‰äººç§°ç¤ºèŒƒä¸­å­¦ä¹ ä¸åŠŸèƒ½ç›¸å…³çš„çº¿ç´¢ï¼Œä»è€Œæ”¹è¿›ä¼ ç»Ÿçš„å¼±ç›‘ç£åŠŸèƒ½åŸºç¡€å®šä½ã€‚è¯¥æ–¹æ³•å…³æ³¨äºç‰©ä½“å’Œéƒ¨ä»¶çš„å±‚é¢ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«ä¸ç‰¹å®šåŠ¨ä½œç›¸å…³çš„éƒ¨ä»¶ã€‚æˆ‘ä»¬åˆ©ç”¨CLIPæŠ€æœ¯åœ¨ä¸åŒè§†è§’çš„å›¾åƒä¸­æ‰¾åˆ°ä¸åŠ¨ä½œç›¸å…³çš„ç‰©ä½“ï¼Œå¹¶é€šè¿‡äº¤å‰å‚è€ƒå‘ç°çš„ç‰©ä½“æ¥æŒ–æ˜æ¯ä¸ªè§†è§’ä¸­çš„ç²¾ç¡®éƒ¨ä»¶åŠŸèƒ½çº¿ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŠŸèƒ½åŸºç¡€å®šä½ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.16402', 'title': "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions", 'url': 'https://huggingface.co/papers/2508.16402', 'abstract': 'AetherCode is a new benchmark for evaluating Large Language Models in competitive programming, offering more challenging and expert-validated test cases than existing benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.', 'score': 9, 'issue_id': 5517, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': 'e6c79bec20f431d6', 'authors': ['Zihan Wang', 'Jiaze Chen', 'Zhicheng Liu', 'Markus Mak', 'Yidi Du', 'Geonsik Moon', 'Luoqi Xu', 'Aaron Tua', 'Kunshuo Peng', 'Jiayi Lu', 'Mingfei Xia', 'Boqian Zou', 'Chenyang Ran', 'Guang Tian', 'Shoutai Zhu', 'Yeheng Duan', 'Zhenghui Kang', 'Zhenxing Lin', 'Shangshu Li', 'Qiang Luo', 'Qingshen Long', 'Zhiyong Chen', 'Yihan Xiao', 'Yurong Wu', 'Daoguang Zan', 'Yuyi Fu', 'Mingxuan Wang', 'Ming Ding'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2508.16402.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#reasoning'], 'emoji': 'ğŸ†', 'ru': {'title': 'AetherCode: ĞĞ¾Ğ²Ğ°Ñ Ğ¿Ğ»Ğ°Ğ½ĞºĞ° Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ˜Ğ˜-Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸ÑÑ‚Ğ¾Ğ²', 'desc': 'AetherCode - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸. ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ ÑĞ»ÑƒÑ‡Ğ°Ğ¸, Ñ‡ĞµĞ¼ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¸. AetherCode Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµÑÑ‚Ğ¸Ğ¶Ğ½Ñ‹Ñ… ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº IOI Ğ¸ ICPC, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ Ğ¾Ñ…Ğ²Ğ°Ñ‚ Ğ¸ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½ÑƒÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ. Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ñ‚ĞµÑÑ‚Ğ¾Ğ², ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ ĞºÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸.'}, 'en': {'title': 'AetherCode: Raising the Bar for LLM Evaluation in Competitive Programming', 'desc': 'AetherCode is a benchmark designed to evaluate Large Language Models (LLMs) specifically in the context of competitive programming. It addresses the limitations of existing benchmarks by providing more difficult and expertly validated test cases, which better reflect the skills of elite human programmers. The benchmark includes problems sourced from prestigious competitions like IOI and ICPC, ensuring a wider range of challenges. By combining automated generation with human curation, AetherCode aims to deliver a more accurate assessment of LLM capabilities in coding and reasoning tasks.'}, 'zh': {'title': 'AetherCodeï¼šæå‡å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ ‡å‡†çš„åŸºå‡†', 'desc': 'AetherCodeæ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç«äº‰ç¼–ç¨‹ä¸­çš„è¡¨ç°ã€‚ä¸ç°æœ‰åŸºå‡†ç›¸æ¯”ï¼Œå®ƒæä¾›äº†æ›´å…·æŒ‘æˆ˜æ€§å’Œç»è¿‡ä¸“å®¶éªŒè¯çš„æµ‹è¯•æ¡ˆä¾‹ã€‚å½“å‰çš„è¯„ä¼°å¾€å¾€å¤¸å¤§äº†æ¨¡å‹çš„èƒ½åŠ›ï¼Œæ©ç›–äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸é¡¶å°–äººç±»ç¨‹åºå‘˜ä¹‹é—´çš„å·®è·ã€‚AetherCodeé€šè¿‡ä»é¡¶çº§ç¼–ç¨‹ç«èµ›ä¸­æå–é—®é¢˜ï¼Œç»“åˆè‡ªåŠ¨ç”Ÿæˆå’Œäººå·¥ç­–åˆ’çš„æµ‹è¯•å¥—ä»¶ï¼Œç¡®ä¿äº†è¯„ä¼°çš„ä¸¥æ ¼æ€§å’Œå¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.16292', 'title': 'Do What? Teaching Vision-Language-Action Models to Reject the Impossible', 'url': 'https://huggingface.co/papers/2508.16292', 'abstract': 'A unified framework, Instruct-Verify-and-Act (IVA), enhances Vision-Language-Action (VLA) models to detect and respond to false-premise instructions by leveraging contextually augmented datasets.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Vision-Language-Action (VLA) models have demonstrated strong performance on a range of robotic tasks. These models rely on multimodal inputs, with language instructions playing a crucial role -- not only in predicting actions, but also in robustly interpreting user intent, even when the requests are impossible to fulfill. In this work, we investigate how VLAs can recognize, interpret, and respond to false-premise instructions: natural language commands that reference objects or conditions absent from the environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that (i) detects when an instruction cannot be executed due to a false premise, (ii) engages in language-based clarification or correction, and (iii) grounds plausible alternatives in perception and action. Towards this end, we construct a large-scale instruction tuning setup with structured language prompts and train a VLA model capable of handling both accurate and erroneous requests. Our approach leverages a contextually augmented, semi-synthetic dataset containing paired positive and false-premise instructions, enabling robust detection and natural language correction. Our experiments show that IVA improves false premise detection accuracy by 97.56% over baselines, while increasing successful responses in false-premise scenarios by 50.78%.', 'score': 5, 'issue_id': 5518, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': 'fad295352d591bab', 'authors': ['Wen-Han Hsieh', 'Elvis Hsieh', 'Dantong Niu', 'Trevor Darrell', 'Roei Herzig', 'David M. Chan'], 'affiliations': ['University of California, Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2508.16292.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#robotics', '#alignment', '#dataset'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğµ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ñ‹ ÑƒÑ‡Ğ°Ñ‚ÑÑ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ ĞµĞ´Ğ¸Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Instruct-Verify-and-Act (IVA) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Vision-Language-Action (VLA) Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞµ. IVA Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ñ€ĞµĞ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»ĞºĞ°Ğ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½ĞµĞ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹, ÑƒÑ‚Ğ¾Ñ‡Ğ½ÑÑ‚ÑŒ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³ Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾ÑÑ‹Ğ»Ğ¾Ğº Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ² Ñ‚Ğ°ĞºĞ¸Ñ… ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ….'}, 'en': {'title': 'Enhancing VLA Models to Handle False-Premise Instructions with IVA', 'desc': 'The paper introduces a new framework called Instruct-Verify-and-Act (IVA) that improves Vision-Language-Action (VLA) models in handling incorrect instructions. It focuses on detecting false-premise commands, which are instructions that refer to non-existent objects or conditions. The IVA framework not only identifies these erroneous instructions but also engages in clarifying or correcting them through natural language. By using a specially designed dataset for training, the model significantly enhances its ability to respond accurately to both valid and invalid requests, achieving remarkable improvements in detection and response rates.'}, 'zh': {'title': 'æŒ‡ä»¤éªŒè¯ä¸è¡ŒåŠ¨ï¼šæå‡VLAæ¨¡å‹çš„æ™ºèƒ½å“åº”èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œç§°ä¸ºæŒ‡ä»¤éªŒè¯ä¸è¡ŒåŠ¨ï¼ˆIVAï¼‰ï¼Œæ—¨åœ¨å¢å¼ºè§†è§‰-è¯­è¨€-è¡ŒåŠ¨ï¼ˆVLAï¼‰æ¨¡å‹çš„èƒ½åŠ›ï¼Œä»¥æ£€æµ‹å’Œå“åº”é”™è¯¯å‰æçš„æŒ‡ä»¤ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨ä¸Šä¸‹æ–‡å¢å¼ºçš„æ•°æ®é›†ï¼Œå¸®åŠ©æ¨¡å‹è¯†åˆ«æ— æ³•æ‰§è¡Œçš„æŒ‡ä»¤ï¼Œå¹¶è¿›è¡Œè¯­è¨€ä¸Šçš„æ¾„æ¸…æˆ–ä¿®æ­£ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒä¼˜è®¾ç½®ï¼Œè®­ç»ƒå‡ºèƒ½å¤Ÿå¤„ç†å‡†ç¡®å’Œé”™è¯¯è¯·æ±‚çš„VLAæ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIVAåœ¨é”™è¯¯å‰ææ£€æµ‹å‡†ç¡®ç‡ä¸Šæé«˜äº†97.56%ï¼Œå¹¶åœ¨é”™è¯¯å‰æåœºæ™¯ä¸­æˆåŠŸå“åº”çš„æ¯”ä¾‹å¢åŠ äº†50.78%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.15746', 'title': 'End-to-End Agentic RAG System Training for Traceable Diagnostic\n  Reasoning', 'url': 'https://huggingface.co/papers/2508.15746', 'abstract': "Deep-DxSearch, an agentic RAG system trained with reinforcement learning, enhances medical diagnosis accuracy by integrating a large-scale retrieval corpus and tailored rewards.  \t\t\t\t\tAI-generated summary \t\t\t\t Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.", 'score': 5, 'issue_id': 5519, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 21', 'zh': '8æœˆ21æ—¥'}, 'hash': '7a7f46f22403e421', 'authors': ['Qiaoyu Zheng', 'Yuze Sun', 'Chaoyi Wu', 'Weike Zhao', 'Pengcheng Qiu', 'Yongguo Yu', 'Kun Sun', 'Yanfeng Wang', 'Ya Zhang', 'Weidi Xie'], 'affiliations': ['Shanghai AI Laboratory, Shanghai, China', 'Shanghai Jiao Tong University, Shanghai, China', 'Xinhua Hospital affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.15746.jpg', 'data': {'categories': ['#optimization', '#interpretability', '#healthcare', '#rl', '#reasoning', '#rag', '#hallucinations'], 'emoji': 'ğŸ©º', 'ru': {'title': 'Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ', 'desc': 'Deep-DxSearch - ÑÑ‚Ğ¾ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ RAG, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞĞ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ ĞºĞ¾Ñ€Ğ¿ÑƒÑ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4 Ğ¸ DeepSeek-R1, ĞºĞ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ñ€ĞµĞ´ĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹. Deep-DxSearch Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ² Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ğ²Ñ€Ğ°Ñ‡Ğ°Ğ¼ Ğ² Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞµ.'}, 'en': {'title': 'Revolutionizing Medical Diagnosis with Deep-DxSearch', 'desc': 'Deep-DxSearch is a novel retrieval-augmented generation (RAG) system that uses reinforcement learning (RL) to improve the accuracy of medical diagnoses. It integrates a large-scale retrieval corpus of patient records and medical knowledge, allowing the model to perform better reasoning in diagnostic scenarios. By framing the large language model (LLM) as an agent and the retrieval corpus as its environment, the system utilizes tailored rewards to enhance its performance in terms of format, reasoning structure, and diagnostic accuracy. Experimental results show that Deep-DxSearch significantly outperforms existing models, providing clinicians with more reliable and precise preliminary diagnoses.'}, 'zh': {'title': 'æå‡åŒ»ç–—è¯Šæ–­å‡†ç¡®æ€§çš„æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ', 'desc': 'Deep-DxSearch æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ä»£ç† RAG ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜åŒ»ç–—è¯Šæ–­çš„å‡†ç¡®æ€§ã€‚å®ƒç»“åˆäº†å¤§è§„æ¨¡çš„æ£€ç´¢è¯­æ–™åº“å’Œå®šåˆ¶çš„å¥–åŠ±æœºåˆ¶ï¼Œä»¥æ”¯æŒåŸºäºæ£€ç´¢çš„æ¨ç†ã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ„å»ºåŒ…å«æ‚£è€…è®°å½•å’Œå¯é åŒ»å­¦çŸ¥è¯†æ¥æºçš„æ£€ç´¢è¯­æ–™åº“ï¼Œå¢å¼ºäº†è¯Šæ–­åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeep-DxSearch åœ¨å¤šç§æ•°æ®ä¸­å¿ƒçš„è¯Šæ–­å‡†ç¡®æ€§ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„æç¤ºå·¥ç¨‹å’Œæ— è®­ç»ƒçš„ RAG æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.15881', 'title': 'TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference', 'url': 'https://huggingface.co/papers/2508.15881', 'abstract': "Tensor-Parallel Latent Attention (TPLA) enhances tensor parallelism efficiency by partitioning latent representations and input dimensions, preserving the benefits of compressed key-value caches while maintaining strong representational capacity.  \t\t\t\t\tAI-generated summary \t\t\t\t Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.", 'score': 5, 'issue_id': 5516, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 21', 'zh': '8æœˆ21æ—¥'}, 'hash': '603c38aea1aa4a8f', 'authors': ['Xiaojuan Tang', 'Fanxu Meng', 'Pingzhi Tang', 'Yuxuan Wang', 'Di Yin', 'Xing Sun', 'Muhan Zhang'], 'affiliations': ['Institute for Artificial Intelligence, Peking University', 'State Key Laboratory of General Artificial Intelligence, BIGAI', 'Tencent Youtu Lab, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.15881.jpg', 'data': {'categories': ['#architecture', '#optimization', '#training', '#benchmark', '#long_context'], 'emoji': 'âš¡', 'ru': {'title': 'TPLA: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ - Tensor-Parallel Latent Attention (TPLA). TPLA Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ğ¼Ğ¸, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑĞ¶Ğ°Ñ‚Ğ¾Ğ³Ğ¾ ĞºÑÑˆĞ° ĞºĞ»ÑÑ‡ĞµĞ¹ Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ñ€Ğ°ÑĞ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. TPLA ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼ Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹.'}, 'en': {'title': 'Boosting Tensor Parallelism with TPLA', 'desc': 'Tensor-Parallel Latent Attention (TPLA) improves the efficiency of tensor parallelism by dividing latent representations and input dimensions across multiple devices. This method retains the advantages of compressed key-value caches while ensuring that each attention head can still utilize the full latent representation, thus enhancing its representational capacity. TPLA is compatible with models that have been pre-trained using Multi-Head Latent Attention (MLA), allowing for efficient tensor-parallel decoding without the need for retraining. By applying orthogonal transforms before partitioning, TPLA minimizes cross-shard interference, leading to significant speedups in processing time while maintaining accuracy on various benchmarks.'}, 'zh': {'title': 'å¼ é‡å¹¶è¡Œæ½œåœ¨æ³¨æ„åŠ›ï¼šæå‡æ•ˆç‡ä¸è¡¨ç°çš„å®Œç¾ç»“åˆ', 'desc': 'Tensor-Parallel Latent Attention (TPLA) æ˜¯ä¸€ç§æé«˜å¼ é‡å¹¶è¡Œæ•ˆç‡çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨è®¾å¤‡ä¹‹é—´åˆ’åˆ†æ½œåœ¨è¡¨ç¤ºå’Œè¾“å…¥ç»´åº¦ï¼Œä¿æŒå‹ç¼©çš„é”®å€¼ç¼“å­˜çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ä¸å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼ˆMLAï¼‰ç›¸æ¯”ï¼ŒTPLA å…è®¸æ¯ä¸ªå¤´åœ¨ä¸åŒè®¾å¤‡ä¸Šç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶é€šè¿‡å…¨å½’çº¦ç»„åˆç»“æœï¼Œä»è€Œæé«˜äº†æ•ˆç‡ã€‚TPLA å…¼å®¹ä½¿ç”¨ MLA é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œæ”¯æŒ MLA é£æ ¼çš„é¢„å¡«å……ï¼Œå¹¶å®ç°é«˜æ•ˆçš„å¼ é‡å¹¶è¡Œè§£ç ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚é€šè¿‡åœ¨ TP åˆ‡ç‰‡ä¹‹å‰åº”ç”¨ç®€å•çš„æ­£äº¤å˜æ¢ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘è·¨åˆ†ç‰‡å¹²æ‰°ï¼Œç¡®ä¿åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å®ç°æ˜¾è‘—çš„åŠ é€Ÿã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.16279', 'title': 'AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications', 'url': 'https://huggingface.co/papers/2508.16279', 'abstract': 'AgentScope enhances agentic applications by providing flexible tool-based interactions, unified interfaces, and advanced infrastructure based on the ReAct paradigm, supporting efficient and safe development and deployment.  \t\t\t\t\tAI-generated summary \t\t\t\t Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications.', 'score': 4, 'issue_id': 5518, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': '5a3094b08e023d2b', 'authors': ['Dawei Gao', 'Zitao Li', 'Yuexiang Xie', 'Weirui Kuang', 'Liuyi Yao', 'Bingchen Qian', 'Zhijian Ma', 'Yue Cui', 'Haohao Luo', 'Shen Li', 'Lu Yi', 'Yi Yu', 'Shiqi He', 'Zhiling Luo', 'Wenmeng Zhou', 'Zhicheng Zhang', 'Xuguang He', 'Ziqian Chen', 'Weikai Liao', 'Farruh Isakulovich Kushnazarov', 'Yaliang Li', 'Bolin Ding', 'Jingren Zhou'], 'affiliations': ['Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2508.16279.jpg', 'data': {'categories': ['#agents', '#architecture'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'AgentScope: Ğ³Ğ¸Ğ±ĞºĞ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ˜Ğ˜-Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'AgentScope 1.0 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€ĞºĞ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑÑ‹ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸ Ğ´Ğ»Ñ Ğ³Ğ¸Ğ±ĞºĞ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸. AgentScope Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğµ ReAct Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² production-ÑÑ€ĞµĞ´Ğµ.'}, 'en': {'title': 'Empowering Agentic Applications with AgentScope', 'desc': 'AgentScope is a framework designed to improve the development of agentic applications by offering flexible interactions and a unified interface. It leverages the ReAct paradigm to enhance agent capabilities, allowing them to effectively use tools and interact with their environments. The latest version introduces foundational components and extensible modules that simplify the integration of new models and methodologies. Additionally, it includes features like a visual evaluation module and a runtime sandbox to ensure safe and efficient deployment of agents in real-world scenarios.'}, 'zh': {'title': 'AgentScopeï¼šæ„å»ºçµæ´»é«˜æ•ˆçš„æ™ºèƒ½ä»£ç†åº”ç”¨', 'desc': 'AgentScope æ˜¯ä¸€ä¸ªå¢å¼ºæ™ºèƒ½ä»£ç†åº”ç”¨ç¨‹åºçš„å·¥å…·ï¼Œæä¾›çµæ´»çš„å·¥å…·äº¤äº’å’Œç»Ÿä¸€çš„æ¥å£ã€‚å®ƒåŸºäº ReAct ç†è®ºï¼Œæ”¯æŒé«˜æ•ˆå’Œå®‰å…¨çš„å¼€å‘ä¸éƒ¨ç½²ã€‚æ–°ç‰ˆæœ¬ 1.0 å¼•å…¥äº†é‡è¦æ”¹è¿›ï¼Œå¸®åŠ©å¼€å‘è€…è½»æ¾åˆ©ç”¨æœ€æ–°çš„æ¨¡å‹å’Œæ¨¡å—ã€‚AgentScope è¿˜æä¾›äº†å¯æ‰©å±•çš„è¯„ä¼°æ¨¡å—å’Œå®‰å…¨çš„è¿è¡Œæ—¶æ²™ç®±ï¼Œç¡®ä¿ä»£ç†çš„å®‰å…¨æ‰§è¡Œã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.14037', 'title': 'Distilled-3DGS:Distilled 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2508.14037', 'abstract': 'A knowledge distillation framework for 3D Gaussian Splatting improves rendering quality and storage efficiency by aggregating outputs from multiple teacher models to optimize a lightweight student model.  \t\t\t\t\tAI-generated summary \t\t\t\t 3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view synthesis (NVS). However, it suffers from a significant drawback: achieving high-fidelity rendering typically necessitates a large number of 3D Gaussians, resulting in substantial memory consumption and storage requirements. To address this challenge, we propose the first knowledge distillation framework for 3DGS, featuring various teacher models, including vanilla 3DGS, noise-augmented variants, and dropout-regularized versions. The outputs of these teachers are aggregated to guide the optimization of a lightweight student model. To distill the hidden geometric structure, we propose a structural similarity loss to boost the consistency of spatial geometric distributions between the student and teacher model. Through comprehensive quantitative and qualitative evaluations across diverse datasets, the proposed Distilled-3DGS, a simple yet effective framework without bells and whistles, achieves promising rendering results in both rendering quality and storage efficiency compared to state-of-the-art methods. Project page: https://distilled3dgs.github.io . Code: https://github.com/lt-xiang/Distilled-3DGS .', 'score': 3, 'issue_id': 5531, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': '44772dad89d5ac79', 'authors': ['Lintao Xiang', 'Xinkai Chen', 'Jianhuang Lai', 'Guangcong Wang'], 'affiliations': ['Sun Yat-Sen University', 'The University of Manchester', 'Vision, Graphics, and Group, Great Bay University'], 'pdf_title_img': 'assets/pdf/title_img/2508.14037.jpg', 'data': {'categories': ['#optimization', '#3d', '#training'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³ Ñ‡ĞµÑ€ĞµĞ· Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ² 3D Gaussian Splatting', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ 3D Gaussian Splatting, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ¾Ğ¹ ÑƒÑ‡ĞµĞ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ½Ğ¾Ğ³Ğ¾ ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Distilled-3DGS Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ½Ğ³Ğ° Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Optimizing 3D Gaussian Splatting with Knowledge Distillation', 'desc': 'This paper introduces a knowledge distillation framework specifically designed for 3D Gaussian Splatting (3DGS), which enhances rendering quality while reducing storage needs. By utilizing multiple teacher models, including standard and noise-augmented versions, the framework optimizes a more compact student model. A novel structural similarity loss is implemented to ensure that the spatial geometric distributions of the student closely match those of the teachers. The results demonstrate that the proposed Distilled-3DGS framework achieves superior rendering performance and efficiency compared to existing methods.'}, 'zh': {'title': 'çŸ¥è¯†è’¸é¦æå‡3Dé«˜æ–¯æ¸²æŸ“æ•ˆç‡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§çŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œç”¨äº3Dé«˜æ–¯ç‚¹äº‘æ¸²æŸ“ï¼Œä»¥æé«˜æ¸²æŸ“è´¨é‡å’Œå­˜å‚¨æ•ˆç‡ã€‚è¯¥æ¡†æ¶é€šè¿‡èšåˆå¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œä¼˜åŒ–ä¸€ä¸ªè½»é‡çº§çš„å­¦ç”Ÿæ¨¡å‹ã€‚ä¸ºäº†è§£å†³é«˜ä¿çœŸæ¸²æŸ“æ‰€éœ€çš„å¤§é‡3Dé«˜æ–¯ä½“ç§¯å¸¦æ¥çš„å†…å­˜æ¶ˆè€—é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ç»“æ„ç›¸ä¼¼æ€§æŸå¤±ï¼Œä»¥å¢å¼ºå­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¹‹é—´çš„ç©ºé—´å‡ ä½•åˆ†å¸ƒä¸€è‡´æ€§ã€‚ç»è¿‡å¤šç§æ•°æ®é›†çš„è¯„ä¼°ï¼Œæå‡ºçš„Distilled-3DGSåœ¨æ¸²æŸ“è´¨é‡å’Œå­˜å‚¨æ•ˆç‡ä¸Šå‡è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.16359', 'title': 'RotaTouille: Rotation Equivariant Deep Learning for Contours', 'url': 'https://huggingface.co/papers/2508.16359', 'abstract': 'RotaTouille is a deep learning framework that achieves rotation and cyclic shift equivariance for contour data using complex-valued circular convolution, enabling effective performance in shape classification, reconstruction, and contour regression.  \t\t\t\t\tAI-generated summary \t\t\t\t Contours or closed planar curves are common in many domains. For example, they appear as object boundaries in computer vision, isolines in meteorology, and the orbits of rotating machinery. In many cases when learning from contour data, planar rotations of the input will result in correspondingly rotated outputs. It is therefore desirable that deep learning models be rotationally equivariant. In addition, contours are typically represented as an ordered sequence of edge points, where the choice of starting point is arbitrary. It is therefore also desirable for deep learning methods to be equivariant under cyclic shifts. We present RotaTouille, a deep learning framework for learning from contour data that achieves both rotation and cyclic shift equivariance through complex-valued circular convolution. We further introduce and characterize equivariant non-linearities, coarsening layers, and global pooling layers to obtain invariant representations for downstream tasks. Finally, we demonstrate the effectiveness of RotaTouille through experiments in shape classification, reconstruction, and contour regression.', 'score': 1, 'issue_id': 5528, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': '6bd890386e0d4116', 'authors': ['Odin Hoff Gardaa', 'Nello Blaser'], 'affiliations': ['University of Bergen'], 'pdf_title_img': 'assets/pdf/title_img/2508.16359.jpg', 'data': {'categories': ['#optimization', '#games', '#architecture', '#cv'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ’Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ´Ğ²Ğ¸Ğ³ Ğ½Ğµ Ğ¿Ğ¾Ğ¼ĞµÑ…Ğ°: RotaTouille Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ²', 'desc': 'RotaTouille - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸. ĞĞ½ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğº Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğ·Ğ½Ğ°Ñ‡Ğ½Ğ¾Ğ¹ ĞºÑ€ÑƒĞ³Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸. Ğ¤Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚Ğ¸, ÑĞ»Ğ¾Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹. RotaTouille Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ€Ğ¼, Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¸ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ÑƒÑ€Ğ¾Ğ².'}, 'en': {'title': 'RotaTouille: Mastering Contours with Equivariance', 'desc': 'RotaTouille is a deep learning framework designed to handle contour data by ensuring that the model is equivariant to both rotations and cyclic shifts. This is achieved through the use of complex-valued circular convolution, which allows the model to maintain performance across various transformations of the input data. The framework also introduces new components like equivariant non-linearities and coarsening layers to create invariant representations for tasks such as shape classification and reconstruction. Experiments show that RotaTouille effectively improves the learning process for contour-related applications.'}, 'zh': {'title': 'RotaTouilleï¼šè½®å»“æ•°æ®çš„æ—‹è½¬ä¸å¾ªç¯ç§»ä½ä¸å˜æ€§', 'desc': 'RotaTouilleæ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†è½®å»“æ•°æ®ï¼Œèƒ½å¤Ÿå®ç°æ—‹è½¬å’Œå¾ªç¯ç§»ä½çš„ä¸å˜æ€§ã€‚å®ƒé€šè¿‡å¤å€¼åœ†å·ç§¯æ¥è¾¾åˆ°è¿™ä¸€ç›®æ ‡ï¼Œä»è€Œåœ¨å½¢çŠ¶åˆ†ç±»ã€é‡å»ºå’Œè½®å»“å›å½’ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è½®å»“æ•°æ®åœ¨è®¡ç®—æœºè§†è§‰å’Œæ°”è±¡ç­‰å¤šä¸ªé¢†åŸŸä¸­éå¸¸å¸¸è§ï¼Œå› æ­¤æ¨¡å‹çš„æ—‹è½¬ä¸å˜æ€§å’Œå¾ªç¯ç§»ä½ä¸å˜æ€§æ˜¯éå¸¸é‡è¦çš„ã€‚RotaTouilleè¿˜å¼•å…¥äº†ä¸å˜çš„éçº¿æ€§å±‚ã€ç²—åŒ–å±‚å’Œå…¨å±€æ± åŒ–å±‚ï¼Œä»¥ä¾¿ä¸ºåç»­ä»»åŠ¡æä¾›ä¸å˜çš„è¡¨ç¤ºã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.16072', 'title': 'InMind: Evaluating LLMs in Capturing and Applying Individual Human\n  Reasoning Styles', 'url': 'https://huggingface.co/papers/2508.16072', 'abstract': "InMind evaluates LLMs' ability to capture and apply personalized reasoning styles in social deduction games, highlighting limitations in current models' adaptive reasoning.  \t\t\t\t\tAI-generated summary \t\t\t\t LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs' capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction.", 'score': 0, 'issue_id': 5518, 'pub_date': '2025-08-22', 'pub_date_card': {'ru': '22 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 22', 'zh': '8æœˆ22æ—¥'}, 'hash': '1987a0ba86bfb9ad', 'authors': ['Zizhen Li', 'Chuanhao Li', 'Yibin Wang', 'Qi Chen', 'Diping Song', 'Yukang Feng', 'Jianwen Sun', 'Jiaxin Ai', 'Fanrui Zhang', 'Mingzhu Sun', 'Kaipeng Zhang'], 'affiliations': ['Fudan University', 'Johns Hopkins University', 'Nankai University', 'Shanghai AI Laboratory', 'Shanghai Innovation Institute', 'University of Science and Technology of China', 'Wuhan University'], 'pdf_title_img': 'assets/pdf/title_img/2508.16072.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#games', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞÑ†ĞµĞ½ĞºĞ° Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² Ğ˜Ğ˜: Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ InMind - ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) ÑƒĞ»Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¸Ğ»Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¸Ğ³Ñ€Ğ°Ñ… Ñ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´ĞµĞ´ÑƒĞºÑ†Ğ¸ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GPT-4, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ Ğ½Ğ° Ğ»ĞµĞºÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¼ÑÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼. ĞœĞ¾Ğ´ĞµĞ»Ğ¸, ÑƒÑĞ¸Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº DeepSeek-R1, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğº Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ÑÑ‚Ğ¸Ğ»Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ñ… LLM Ğ² Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¸.'}, 'en': {'title': 'InMind: Evaluating Personalized Reasoning in AI for Social Deduction Games', 'desc': 'The paper introduces InMind, a framework for evaluating large language models (LLMs) on their ability to understand and apply personalized reasoning styles in social deduction games (SDGs). It highlights that while LLMs perform well in human-centric reasoning tasks, they often fail to adapt to the unique reasoning strategies that different players use in similar situations. By analyzing gameplay data and player reflections, InMind assesses both static alignment and dynamic adaptation of reasoning styles. The study finds that many general-purpose LLMs struggle with individualized reasoning, while some enhanced models show potential for better adaptation to diverse reasoning styles.'}, 'zh': {'title': 'è¯„ä¼°ä¸ªæ€§åŒ–æ¨ç†é£æ ¼çš„InMindæ¡†æ¶', 'desc': 'InMindæ˜¯ä¸€ä¸ªè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼èƒ½åŠ›çš„æ¡†æ¶ã€‚å½“å‰çš„æ¨¡å‹åœ¨é€‚åº”æ€§æ¨ç†æ–¹é¢å­˜åœ¨å±€é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸ªä½“åŒ–æ¨ç†é£æ ¼æ—¶ã€‚é€šè¿‡åœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­å¼•å…¥ç»“æ„åŒ–çš„æ¸¸æˆæ•°æ®å’Œç­–ç•¥è¿½è¸ªï¼ŒInMindèƒ½å¤Ÿæ›´å¥½åœ°è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›å…ˆè¿›çš„LLMsè¡¨ç°å‡ºä¸€å®šçš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä»ç„¶åœ¨ä¸ªæ€§åŒ–å’ŒåŠ¨æ€é€‚åº”æ€§æ¨ç†æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.15868', 'title': 'CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning', 'url': 'https://huggingface.co/papers/2508.15868', 'abstract': 'A contrastive learning approach with annotated Chain-of-Thought enhances the reasoning performance of Large Language Models through stable fine-tuning and robust contrastive signals.  \t\t\t\t\tAI-generated summary \t\t\t\t Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., , to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of  in terms of robustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code is available at https://github.com/WNQzhu/CARFT.', 'score': 0, 'issue_id': 5525, 'pub_date': '2025-08-21', 'pub_date_card': {'ru': '21 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 21', 'zh': '8æœˆ21æ—¥'}, 'hash': '47b82f30fedd191b', 'authors': ['Wenqiao Zhu', 'Ji Liu', 'Rongjuncheng Zhang', 'Haipang Wu', 'Yulun Zhang'], 'affiliations': ['HiThink Research', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2508.15868.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ CoT: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT). ĞœĞµÑ‚Ğ¾Ğ´, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ CARFT, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ĞµĞ¼, Ğ¸Ğ·Ğ±ĞµĞ³Ğ°Ñ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ… Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚ĞºĞ¾Ğ². CARFT Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ CoT Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ½Ñ‹Ğµ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Enhancing LLM Reasoning with Contrastive Learning and Chain-of-Thought', 'desc': 'This paper introduces a new method called Contrastive learning with annotated Chain-of-Thought (CoT) for improving the reasoning abilities of Large Language Models (LLMs). The authors highlight the limitations of existing Reinforcement Learning (RL) methods, which often overlook the importance of annotated CoT and can lead to unstable training. Their approach stabilizes the fine-tuning process by using contrastive signals that leverage both annotated and unsupervised learning. Experimental results show that this method significantly enhances the robustness and performance of LLMs compared to traditional techniques.'}, 'zh': {'title': 'å¯¹æ¯”å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¢å¼ºå‹å¼ºåŒ–å¾®è°ƒæ–¹æ³•ï¼Œæ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æ³¨é‡Šçš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¥ç¨³å®šå¾®è°ƒè¿‡ç¨‹ï¼Œå¹¶é€šè¿‡æ–°é¢–çš„å¯¹æ¯”ä¿¡å·æ¥æŒ‡å¯¼è®­ç»ƒã€‚ä¸ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å¯ç”¨çš„æ³¨é‡ŠCoTï¼ŒåŒæ—¶é¿å…æ¨¡å‹å´©æºƒå’Œè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é²æ£’æ€§ã€æ€§èƒ½å’Œæ•ˆç‡æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13797', 'title': 'Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing', 'url': 'https://huggingface.co/papers/2508.13797', 'abstract': 'Sketch3DVE is a sketch-based 3D-aware video editing method that uses dense stereo estimation, point cloud editing, and a video diffusion model to handle sparse inputs and viewpoint changes in video editing.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing. Homepage and code: http://http://geometrylearning.com/Sketch3DVE/', 'score': 0, 'issue_id': 5528, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': '214d175c6e1f5020', 'authors': ['Feng-Lin Liu', 'Shi-Yang Li', 'Yan-Pei Cao', 'Hongbo Fu', 'Lin Gao'], 'affiliations': ['Hong Kong University of Science and Technology, China', 'Institute of Computing Technology, Chinese Academy of Sciences, China', 'University of Chinese Academy of Sciences, China', 'VAST, China'], 'pdf_title_img': 'assets/pdf/title_img/2508.13797.jpg', 'data': {'categories': ['#video', '#games', '#diffusion', '#3d'], 'emoji': 'âœï¸', 'ru': {'title': 'Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑĞºĞ¸Ğ·Ğ¾Ğ²', 'desc': 'Sketch3DVE - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑĞºĞ¸Ğ·Ğ¾Ğ², ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 3D-ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ ÑÑ†ĞµĞ½Ñ‹. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½ÑƒÑ ÑÑ‚ĞµÑ€ĞµĞ¾Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°ĞºĞ° Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ñ€Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑÑ… Ñ‚Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ğ·Ğ¾Ñ€Ğ°, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¿Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ñ‹ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ·ÑƒĞ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Sketch3DVE Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ğ¼Ğ°Ğ½Ğ¸Ğ¿ÑƒĞ»ÑÑ†Ğ¸Ñ 3D-Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°.'}, 'en': {'title': 'Revolutionizing Video Editing with 3D Sketch Manipulation', 'desc': 'Sketch3DVE is a novel method for editing videos that allows users to manipulate 3D structures using sketches. It addresses the challenges of viewpoint changes and sparse inputs by employing dense stereo estimation to create a point cloud representation of the video. The method enables precise editing by propagating changes from the first frame to subsequent frames while maintaining the integrity of unedited areas. By integrating a video diffusion model, Sketch3DVE produces realistic outputs that align new edits with the original 3D scene.'}, 'zh': {'title': 'Sketch3DVEï¼šåˆ›æ–°çš„3Dè§†é¢‘ç¼–è¾‘æ–¹æ³•', 'desc': 'Sketch3DVEæ˜¯ä¸€ç§åŸºäºè‰å›¾çš„3Dè§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†è§†é¢‘ç¼–è¾‘ä¸­çš„ç¨€ç–è¾“å…¥å’Œè§†è§’å˜åŒ–ã€‚è¯¥æ–¹æ³•é€šè¿‡å¯†é›†ç«‹ä½“ä¼°è®¡å’Œç‚¹äº‘ç¼–è¾‘ï¼Œç”Ÿæˆä¸åŸå§‹è§†é¢‘ä¸€è‡´çš„æ–°è§†è§’å†…å®¹ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è‰å›¾è¿›è¡Œç²¾ç¡®çš„å‡ ä½•æ§åˆ¶ï¼ŒåŒæ—¶æ”¯æŒå…¶ä»–åŸºäºæ©è†œçš„å›¾åƒç¼–è¾‘æ–¹æ³•ã€‚æœ€ç»ˆï¼ŒSketch3DVEç»“åˆè§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œç”ŸæˆçœŸå®æ„Ÿå¼ºçš„ç¼–è¾‘è§†é¢‘ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘ç¼–è¾‘ä¸­çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.13562', 'title': 'Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose\n  Inverse Kinematics', 'url': 'https://huggingface.co/papers/2508.13562', 'abstract': 'Learnable SMPLify replaces iterative optimization in SMPLify with a neural network for faster and more generalized 3D human pose and shape estimation.  \t\t\t\t\tAI-generated summary \t\t\t\t In 3D human pose and shape estimation, SMPLify remains a robust baseline that solves inverse kinematics (IK) through iterative optimization. However, its high computational cost limits its practicality. Recent advances across domains have shown that replacing iterative optimization with data-driven neural networks can achieve significant runtime improvements without sacrificing accuracy. Motivated by this trend, we propose Learnable SMPLify, a neural framework that replaces the iterative fitting process in SMPLify with a single-pass regression model. The design of our framework targets two core challenges in neural IK: data construction and generalization. To enable effective training, we propose a temporal sampling strategy that constructs initialization-target pairs from sequential frames. To improve generalization across diverse motions and unseen poses, we propose a human-centric normalization scheme and residual learning to narrow the solution space. Learnable SMPLify supports both sequential inference and plug-in post-processing to refine existing image-based estimators. Extensive experiments demonstrate that our method establishes itself as a practical and simple baseline: it achieves nearly 200x faster runtime compared to SMPLify, generalizes well to unseen 3DPW and RICH, and operates in a model-agnostic manner when used as a plug-in tool on LucidAction. The code is available at https://github.com/Charrrrrlie/Learnable-SMPLify.', 'score': 0, 'issue_id': 5523, 'pub_date': '2025-08-19', 'pub_date_card': {'ru': '19 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 19', 'zh': '8æœˆ19æ—¥'}, 'hash': 'b08c5280107ed4d1', 'authors': ['Yuchen Yang', 'Linfeng Dong', 'Wei Wang', 'Zhihang Zhong', 'Xiao Sun'], 'affiliations': ['Fudan University', 'Shanghai Artificial Intelligence Laboratory', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2508.13562.jpg', 'data': {'categories': ['#3d', '#data', '#training', '#optimization'], 'emoji': 'ğŸƒ', 'ru': {'title': 'Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¸ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµĞ¼Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° 3D Ğ¿Ğ¾Ğ·Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Learnable SMPLify - Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ 3D Ğ¿Ğ¾Ğ·Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞĞ½ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² SMPLify Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ½ÑƒÑ Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒÑĞºĞ¾Ñ€ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ. Ğ”Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ, Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ½Ğ° Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ 200-ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğµ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ SMPLify Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Speeding Up 3D Pose Estimation with Neural Networks', 'desc': 'Learnable SMPLify introduces a neural network approach to 3D human pose and shape estimation, replacing the traditional iterative optimization used in SMPLify. This method significantly reduces computational time while maintaining accuracy, making it more practical for real-world applications. The framework addresses key challenges in neural inverse kinematics, such as data construction and generalization, by utilizing a temporal sampling strategy and a human-centric normalization scheme. Extensive testing shows that Learnable SMPLify is nearly 200 times faster than its predecessor and effectively generalizes to new poses and motions.'}, 'zh': {'title': 'Learnable SMPLifyï¼šåŠ é€Ÿ3Däººä½“å§¿æ€ä¼°è®¡çš„ç¥ç»ç½‘ç»œ', 'desc': 'Learnable SMPLify æ˜¯ä¸€ç§æ–°çš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç”¨äºæ›´å¿«å’Œæ›´é€šç”¨çš„ 3D äººä½“å§¿æ€å’Œå½¢çŠ¶ä¼°è®¡ã€‚å®ƒç”¨å•æ¬¡å›å½’æ¨¡å‹æ›¿ä»£äº† SMPLify ä¸­çš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œä»è€Œæ˜¾è‘—æé«˜äº†è¿è¡Œé€Ÿåº¦ã€‚è¯¥æ¡†æ¶é€šè¿‡æ—¶é—´é‡‡æ ·ç­–ç•¥æ„å»ºåˆå§‹åŒ–-ç›®æ ‡å¯¹ï¼Œå¹¶é‡‡ç”¨äººæœ¬å½’ä¸€åŒ–å’Œæ®‹å·®å­¦ä¹ æ¥æ”¹å–„å¯¹ä¸åŒåŠ¨ä½œå’Œæœªè§å§¿æ€çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒLearnable SMPLify çš„è¿è¡Œé€Ÿåº¦æ¯” SMPLify å¿«è¿‘ 200 å€ï¼Œå¹¶ä¸”åœ¨å¤šç§æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2508.10390', 'title': 'Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts', 'url': 'https://huggingface.co/papers/2508.10390', 'abstract': 'A hybrid framework combining LLMs and human oversight is proposed to clean datasets and detect jailbreak attacks, with new strategies to enhance jailbreak success.  \t\t\t\t\tAI-generated summary \t\t\t\t Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.', 'score': 0, 'issue_id': 5518, 'pub_date': '2025-08-14', 'pub_date_card': {'ru': '14 Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°', 'en': 'August 14', 'zh': '8æœˆ14æ—¥'}, 'hash': 'df08ead155e89f8d', 'authors': ['Chiyu Zhang', 'Lu Zhou', 'Xiaogang Xu', 'Jiafei Wu', 'Liming Fang', 'Zhe Liu'], 'affiliations': ['Collaborative Innovation Center of Novel Software Technology and Industrialization', 'Nanjing University of Aeronautics and Astronautics', 'The Chinese University of Hong Kong', 'Zhejiang Lab'], 'pdf_title_img': 'assets/pdf/title_img/2508.10390.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#data', '#security', '#dataset'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° Ğ¸ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸Ñ jailbreak-Ğ°Ñ‚Ğ°Ğº', 'desc': 'ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰Ğ°Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ°Ñ‚Ğ°Ğº Ğ¿Ğ¾ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ñƒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ (jailbreak). Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° MDH Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ Ğ¸ Ğ¾Ñ‡Ğ¸Ñ‰Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°. Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ D-Attack Ğ¸ DH-CoT Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ jailbreak-Ğ°Ñ‚Ğ°Ğº Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°Ñ‚ÑŒ jailbreak-Ğ°Ñ‚Ğ°ĞºĞ¸, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ÑĞ»ÑƒÑ‡Ğ°ÑÑ… Ğ½ĞµÑĞ²Ğ½Ğ¾ Ğ²Ñ€ĞµĞ´Ğ¾Ğ½Ğ¾ÑĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Dataset Integrity and Jailbreak Detection with Hybrid LLMs', 'desc': 'This paper introduces a hybrid framework called MDH that combines large language models (LLMs) with minimal human oversight to improve the detection of malicious content in datasets and evaluate jailbreak attacks. The framework addresses the limitations of existing methods, which either require extensive manual work or suffer from inconsistent accuracy. By integrating LLMs with human assistance, the proposed approach enhances the cleaning of datasets and the identification of harmful prompts. Additionally, the authors present two innovative strategies, D-Attack and DH-CoT, to increase the success rate of jailbreak attempts by utilizing context simulation and hijacked reasoning processes.'}, 'zh': {'title': 'æ··åˆæ¡†æ¶æå‡æ•°æ®é›†æ¸…ç†ä¸è¶Šç‹±æ£€æµ‹', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ··åˆæ¡†æ¶ï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œäººå·¥ç›‘ç£ï¼Œç”¨äºæ¸…ç†æ•°æ®é›†å’Œæ£€æµ‹è¶Šç‹±æ”»å‡»ã€‚ç°æœ‰çš„æ¶æ„å†…å®¹æ£€æµ‹æ–¹æ³•è¦ä¹ˆä¾èµ–äººå·¥æ ‡æ³¨ï¼Œè€—æ—¶è€—åŠ›ï¼Œè¦ä¹ˆä¾èµ–LLMsï¼Œä½†åœ¨è¯†åˆ«æœ‰å®³ç±»å‹æ—¶å‡†ç¡®æ€§ä¸ä¸€è‡´ã€‚æˆ‘ä»¬æå‡ºçš„MDHæ¡†æ¶é€šè¿‡æœ€å°åŒ–äººå·¥å¹²é¢„ï¼Œç»“åˆLLMæ ‡æ³¨ï¼Œæé«˜äº†æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç²¾å¿ƒè®¾è®¡çš„å¼€å‘è€…æ¶ˆæ¯å¯ä»¥æ˜¾è‘—æé«˜è¶Šç‹±æˆåŠŸç‡ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°ç­–ç•¥ï¼šD-Attackå’ŒDH-CoTã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (3)', '#agi (1)', '#alignment (1)', '#architecture (4)', '#audio', '#benchmark (6)', '#cv (2)', '#data (2)', '#dataset (3)', '#diffusion (2)', '#ethics (1)', '#games (3)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (1)', '#optimization (11)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (4)', '#rlhf (1)', '#robotics (2)', '#science', '#security (2)', '#small_models', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-08-25 20:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-08-25 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-08-25 20:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    