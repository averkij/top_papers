
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. July 4.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">4 июля</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-03.html">⬅️ <span id="prev-date">03.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-07.html">➡️ <span id="next-date">07.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '4 июля', 'en': 'July 4', 'zh': '7月4日'};
        let feedDateNext = {'ru': '07.07', 'en': '07/07', 'zh': '7月7日'};
        let feedDatePrev = {'ru': '03.07', 'en': '07/03', 'zh': '7月3日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.02592', 'title': 'WebSailor: Navigating Super-human Reasoning for Web Agent', 'url': 'https://huggingface.co/papers/2507.02592', 'abstract': "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.", 'score': 41, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '0a8cc61c0251e5da', 'authors': ['Kuan Li', 'Zhongwang Zhang', 'Huifeng Yin', 'Liwen Zhang', 'Litu Ou', 'Jialong Wu', 'Wenbiao Yin', 'Baixuan Li', 'Zhengwei Tao', 'Xinyu Wang', 'Weizhou Shen', 'Junkai Zhang', 'Dingchu Zhang', 'Xixi Wu', 'Yong Jiang', 'Ming Yan', 'Pengjun Xie', 'Fei Huang', 'Jingren Zhou'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2507.02592.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#training', '#agents', '#rl'], 'emoji': '🧭', 'ru': {'title': 'WebSailor: навигация в океане неопределенности для ИИ', 'desc': 'Статья представляет новый метод обучения языковых моделей под названием WebSailor. Он направлен на преодоление когнитивных ограничений человека в задачах поиска сложной информации. WebSailor использует генерацию задач с высокой неопределенностью и обучение с подкреплением для развития способности моделей систематически снижать неопределенность. Результаты показывают, что WebSailor позволяет открытым моделям достичь производительности проприетарных систем в сложных информационных задачах.'}, 'en': {'title': 'Empowering Open-Source Agents to Compete with the Best', 'desc': 'This paper discusses advancements in training large language models (LLMs) to surpass human cognitive limitations. It highlights the success of proprietary systems like DeepResearch, which excel in complex information-seeking tasks due to their unique reasoning abilities. The authors introduce WebSailor, a post-training methodology that enhances LLMs by generating high-uncertainty tasks and employing a novel reinforcement learning algorithm called Duplicating Sampling Policy Optimization (DUPO). The results show that WebSailor significantly improves the performance of open-source agents, enabling them to compete with proprietary models in challenging information retrieval scenarios.'}, 'zh': {'title': '超越认知局限，提升信息检索能力', 'desc': '本论文探讨了超越人类认知局限性在大型语言模型（LLM）训练中的重要性。我们提出的WebSailor方法通过系统性地减少在广阔信息环境中导航时的极端不确定性，来提升模型的推理能力。该方法结合了结构化采样和信息模糊化等技术，生成新的高不确定性任务，并采用高效的强化学习算法进行训练。实验结果表明，WebSailor在复杂的信息检索任务中显著优于所有开源代理，缩小了与专有代理的能力差距。'}}}, {'id': 'https://huggingface.co/papers/2507.02813', 'title': 'LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with\n  TriMap Video Diffusion', 'url': 'https://huggingface.co/papers/2507.02813', 'abstract': 'Recovering 3D structures with open-vocabulary scene understanding from 2D images is a fundamental but daunting task. Recent developments have achieved this by performing per-scene optimization with embedded language information. However, they heavily rely on the calibrated dense-view reconstruction paradigm, thereby suffering from severe rendering artifacts and implausible semantic synthesis when limited views are available. In this paper, we introduce a novel generative framework, coined LangScene-X, to unify and generate 3D consistent multi-modality information for reconstruction and understanding. Powered by the generative capability of creating more consistent novel observations, we can build generalizable 3D language-embedded scenes from only sparse views. Specifically, we first train a TriMap video diffusion model that can generate appearance (RGBs), geometry (normals), and semantics (segmentation maps) from sparse inputs through progressive knowledge integration. Furthermore, we propose a Language Quantized Compressor (LQC), trained on large-scale image datasets, to efficiently encode language embeddings, enabling cross-scene generalization without per-scene retraining. Finally, we reconstruct the language surface fields by aligning language information onto the surface of 3D scenes, enabling open-ended language queries. Extensive experiments on real-world data demonstrate the superiority of our LangScene-X over state-of-the-art methods in terms of quality and generalizability. Project Page: https://liuff19.github.io/LangScene-X.', 'score': 36, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '726c080e7ea88c4b', 'authors': ['Fangfu Liu', 'Hao Li', 'Jiawei Chi', 'Hanyang Wang', 'Minghui Yang', 'Fudong Wang', 'Yueqi Duan'], 'affiliations': ['Ant Group', 'NTU', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.02813.jpg', 'data': {'categories': ['#open_source', '#3d', '#games', '#multimodal', '#diffusion'], 'emoji': '🏙️', 'ru': {'title': 'Генерация 3D-сцен с языковым пониманием по нескольким изображениям', 'desc': 'LangScene-X - это новая генеративная система для восстановления и понимания 3D-сцен с открытым словарем на основе 2D-изображений. Она использует видео-диффузионную модель TriMap для генерации согласованной мультимодальной информации (RGB, нормали, сегментация) из ограниченного числа ракурсов. Система включает языковой квантованный компрессор (LQC) для эффективного кодирования языковых эмбеддингов, что позволяет обобщать на новые сцены без переобучения. LangScene-X превосходит современные методы по качеству и обобщаемости при реконструкции 3D-сцен с языковой привязкой.'}, 'en': {'title': 'Revolutionizing 3D Reconstruction with Language-Embedded Insights', 'desc': 'This paper presents LangScene-X, a new framework for creating 3D structures from 2D images using language information. It addresses the limitations of previous methods that relied on dense-view reconstructions, which often resulted in poor quality when only limited views were available. LangScene-X utilizes a TriMap video diffusion model to generate consistent visual and semantic data from sparse inputs, enhancing the reconstruction process. Additionally, it introduces a Language Quantized Compressor to efficiently encode language embeddings, allowing for better generalization across different scenes without needing to retrain for each one.'}, 'zh': {'title': 'LangScene-X：从稀疏视图生成一致的3D场景', 'desc': '本文提出了一种新颖的生成框架LangScene-X，用于从稀疏视图中恢复3D结构并进行场景理解。该框架结合了语言信息和多模态数据，能够生成一致的3D场景。我们首先训练了一个TriMap视频扩散模型，从稀疏输入中生成外观、几何和语义信息。通过引入语言量化压缩器（LQC），我们实现了跨场景的泛化，避免了逐场景的重新训练。'}}}, {'id': 'https://huggingface.co/papers/2507.02025', 'title': 'IntFold: A Controllable Foundation Model for General and Specialized\n  Biomolecular Structure Prediction', 'url': 'https://huggingface.co/papers/2507.02025', 'abstract': 'We introduce IntFold, a controllable foundation model for both general and specialized biomolecular structure prediction. IntFold demonstrates predictive accuracy comparable to the state-of-the-art AlphaFold3, while utilizing a superior customized attention kernel. Beyond standard structure prediction, IntFold can be adapted to predict allosteric states, constrained structures, and binding affinity through the use of individual adapters. Furthermore, we introduce a novel confidence head to estimate docking quality, offering a more nuanced assessment for challenging targets such as antibody-antigen complexes. Finally, we share insights gained during the training process of this computationally intensive model.', 'score': 30, 'issue_id': 4642, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '5e8a0f59d9493778', 'authors': ['The IntFold Team', 'Leon Qiao', 'Wayne Bai', 'He Yan', 'Gary Liu', 'Nova Xi', 'Xiang Zhang'], 'affiliations': ['IntelliGen AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.02025.jpg', 'data': {'categories': ['#architecture', '#training', '#healthcare'], 'emoji': '🧬', 'ru': {'title': 'IntFold: Гибкий инструмент для точного прогнозирования биомолекулярных структур', 'desc': 'IntFold - это новая управляемая базовая модель для предсказания как общих, так и специализированных биомолекулярных структур. Она демонстрирует точность прогнозирования, сравнимую с современной моделью AlphaFold3, используя улучшенное пользовательское ядро внимания. IntFold может быть адаптирована для предсказания аллостерических состояний, ограниченных структур и аффинности связывания с помощью индивидуальных адаптеров. Модель также включает новый блок оценки уверенности для определения качества докинга, что особенно полезно для сложных целей, таких как комплексы антитело-антиген.'}, 'en': {'title': 'IntFold: Advancing Biomolecular Structure Prediction with Precision and Flexibility', 'desc': 'IntFold is a new foundation model designed for predicting the structures of biomolecules, both in general and specialized contexts. It achieves high accuracy in predictions, rivaling the leading model AlphaFold3, by employing a unique attention mechanism. The model is versatile, allowing for adaptations to predict various states and properties of biomolecules, such as allosteric states and binding affinities, through the use of specific adapters. Additionally, IntFold includes a confidence head that assesses the quality of docking predictions, particularly for complex targets like antibody-antigen interactions, and shares valuable insights from its training process.'}, 'zh': {'title': 'IntFold：生物分子结构预测的新突破', 'desc': '我们介绍了IntFold，这是一种可控的基础模型，用于一般和专业的生物分子结构预测。IntFold的预测准确性与最先进的AlphaFold3相当，同时采用了更优的定制注意力核。除了标准的结构预测，IntFold还可以通过使用单独的适配器来预测变构状态、受限结构和结合亲和力。此外，我们引入了一种新颖的置信度头，以评估对接质量，为抗体-抗原复合物等具有挑战性的目标提供更细致的评估。'}}}, {'id': 'https://huggingface.co/papers/2507.02321', 'title': 'Heeding the Inner Voice: Aligning ControlNet Training via Intermediate\n  Features Feedback', 'url': 'https://huggingface.co/papers/2507.02321', 'abstract': 'InnerControl enforces spatial consistency across all diffusion steps by training lightweight convolutional probes to improve control fidelity and generation quality in text-to-image diffusion models.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth).', 'score': 28, 'issue_id': 4646, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'f340ac71ebc152be', 'authors': ['Nina Konovalova', 'Maxim Nikolaev', 'Andrey Kuznetsov', 'Aibek Alanov'], 'affiliations': ['AIRI, Russia', 'HSE University, Russia', 'Innopolis, Russia', 'Sber, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2507.02321.jpg', 'data': {'categories': ['#diffusion', '#cv', '#alignment', '#training'], 'emoji': '🎨', 'ru': {'title': 'Точный контроль генерации изображений на всех этапах диффузии', 'desc': 'InnerControl - это новый метод обучения для улучшения пространственного контроля в диффузионных моделях генерации изображений по тексту. Он использует легковесные сверточные зонды для реконструкции входных сигналов управления на всех этапах диффузии. Метод минимизирует расхождение между предсказанными и целевыми условиями на протяжении всего процесса, что улучшает точность контроля и качество генерации. В сочетании с существующими техниками InnerControl достигает передовых результатов для различных методов кондиционирования.'}, 'en': {'title': 'Enhancing Spatial Consistency in Diffusion Models with InnerControl', 'desc': 'InnerControl is a novel training strategy designed to enhance spatial consistency in text-to-image diffusion models. It utilizes lightweight convolutional probes to reconstruct control signals from intermediate features during the denoising process. By applying an alignment loss that minimizes the difference between predicted and target conditions at every diffusion step, InnerControl significantly improves control fidelity and overall generation quality. This approach, when combined with existing methods like ControlNet++, achieves state-of-the-art results across various conditioning techniques.'}, 'zh': {'title': 'InnerControl：提升扩散模型的空间一致性与生成质量', 'desc': 'InnerControl是一种训练策略，旨在增强文本到图像扩散模型在所有扩散步骤中的空间一致性。通过训练轻量级卷积探针，InnerControl能够在每个去噪步骤中从中间UNet特征中重建输入控制信号（如边缘和深度）。这种方法有效地提取信号，即使在高度噪声的潜在空间中，也能为训练提供伪真实控制。通过最小化整个扩散过程中的预测条件与目标条件之间的差异，InnerControl显著提高了控制的准确性和生成的质量。'}}}, {'id': 'https://huggingface.co/papers/2507.01352', 'title': 'Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy', 'url': 'https://huggingface.co/papers/2507.01352', 'abstract': 'Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.', 'score': 25, 'issue_id': 4638, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '955bbdefa8606d12', 'authors': ['Chris Yuhao Liu', 'Liang Zeng', 'Yuzhen Xiao', 'Jujie He', 'Jiacai Liu', 'Chaojie Wang', 'Rui Yan', 'Wei Shen', 'Fuxiang Zhang', 'Jiacheng Xu', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['2050 Research, Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.01352.jpg', 'data': {'categories': ['#open_source', '#rlhf', '#training', '#alignment', '#data', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'Синергия человека и ИИ для создания передовых моделей вознаграждения', 'desc': 'Статья представляет новый набор данных предпочтений SynPref-40M, содержащий 40 миллионов пар предпочтений, созданный с помощью двухэтапного процесса, сочетающего человеческую аннотацию и масштабируемость ИИ. На основе этих данных авторы разработали набор моделей вознаграждения Skywork-Reward-V2 с параметрами от 0.6B до 8B. Модели демонстрируют высокую эффективность в различных задачах, включая соответствие человеческим предпочтениям, объективную корректность и безопасность. Исследование подчеркивает важность качественной курации данных и синергии человека и ИИ для улучшения моделей вознаграждения в обучении с подкреплением на основе обратной связи от человека (RLHF).'}, 'en': {'title': 'Unlocking Human Preferences with Skywork-Reward-V2', 'desc': 'This paper addresses the limitations of current reward models (RMs) in reinforcement learning from human feedback (RLHF), which struggle to accurately reflect complex human preferences. The authors propose a new large-scale preference dataset, SynPref-40M, containing 40 million preference pairs, to improve the training of RMs. They introduce a two-stage human-AI curation pipeline that combines human annotation with AI scalability to ensure high-quality data. The resulting Skywork-Reward-V2 models, trained on a refined subset of this dataset, demonstrate superior performance across various benchmarks, showcasing the importance of quality data in enhancing reward model effectiveness.'}, 'zh': {'title': '提升奖励模型的质量与性能', 'desc': '本论文探讨了奖励模型在从人类反馈中进行强化学习的重要性。当前的开放奖励模型在评估基准上表现不佳，无法有效捕捉人类偏好的复杂性。为了解决这一问题，作者提出了一个包含4000万对偏好的大规模数据集SynPref-40M，并设计了一个人机协作的两阶段数据处理流程。通过高质量的数据标注和AI的自动化处理，作者训练了Skywork-Reward-V2系列奖励模型，展示了其在多项基准测试中的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.23918', 'title': 'Thinking with Images for Multimodal Reasoning: Foundations, Methods, and\n  Future Frontiers', 'url': 'https://huggingface.co/papers/2506.23918', 'abstract': 'Recent progress in multimodal reasoning has been significantly advanced by textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning within language. This text-centric approach, however, treats vision as a static, initial context, creating a fundamental "semantic gap" between rich perceptual data and discrete symbolic thought. Human cognition often transcends language, utilizing vision as a dynamic mental sketchpad. A similar evolution is now unfolding in AI, marking a fundamental paradigm shift from models that merely think about images to those that can truly think with images. This emerging paradigm is characterized by models leveraging visual information as intermediate steps in their thought process, transforming vision from a passive input into a dynamic, manipulable cognitive workspace. In this survey, we chart this evolution of intelligence along a trajectory of increasing cognitive autonomy, which unfolds across three key stages: from external tool exploration, through programmatic manipulation, to intrinsic imagination. To structure this rapidly evolving field, our survey makes four key contributions. (1) We establish the foundational principles of the think with image paradigm and its three-stage framework. (2) We provide a comprehensive review of the core methods that characterize each stage of this roadmap. (3) We analyze the critical landscape of evaluation benchmarks and transformative applications. (4) We identify significant challenges and outline promising future directions. By providing this structured overview, we aim to offer a clear roadmap for future research towards more powerful and human-aligned multimodal AI.', 'score': 21, 'issue_id': 4640, 'pub_date': '2025-06-30', 'pub_date_card': {'ru': '30 июня', 'en': 'June 30', 'zh': '6月30日'}, 'hash': '8526b6b1e8d4b31e', 'authors': ['Zhaochen Su', 'Peng Xia', 'Hangyu Guo', 'Zhenhua Liu', 'Yan Ma', 'Xiaoye Qu', 'Jiaqi Liu', 'Yanshu Li', 'Kaide Zeng', 'Zhengyuan Yang', 'Linjie Li', 'Yu Cheng', 'Heng Ji', 'Junxian He', 'Yi R. Fung'], 'affiliations': ['Microsoft', 'The Chinese University of Hong Kong', 'The Hong Kong University of Science and Technology', 'UIUC', 'UNC-Chapel Hill'], 'pdf_title_img': 'assets/pdf/title_img/2506.23918.jpg', 'data': {'categories': ['#benchmark', '#multimodal', '#reasoning', '#alignment', '#survey'], 'emoji': '🧠', 'ru': {'title': 'От мышления об изображениях к мышлению изображениями: новая эра мультимодального ИИ', 'desc': 'Эта статья описывает новую парадигму в мультимодальном искусственном интеллекте, где визуальная информация используется как динамический инструмент мышления, а не просто статичный входной контекст. Авторы выделяют три ключевых этапа развития этой парадигмы: от внешнего исследования инструментов через программную манипуляцию к внутреннему воображению. В работе представлен всесторонний обзор методов, характерных для каждого этапа, а также анализ критически важных эталонных тестов и трансформационных приложений. Статья также определяет значительные проблемы и намечает перспективные направления будущих исследований в области мультимодального ИИ.'}, 'en': {'title': 'Transforming Vision into Dynamic Thought in AI', 'desc': "This paper discusses the advancements in multimodal reasoning, particularly focusing on the 'think with images' paradigm. It highlights the limitations of traditional text-based reasoning, which treats visual data as static, leading to a disconnect between perception and thought. The authors propose a three-stage framework that evolves from using images as tools to integrating them into cognitive processes, allowing for dynamic manipulation of visual information. The survey aims to provide a structured overview of this emerging field, outlining foundational principles, core methods, evaluation benchmarks, and future research directions."}, 'zh': {'title': '从图像思考到思考图像的转变', 'desc': '这篇论文探讨了多模态推理的最新进展，特别是文本链式思维（CoT）在语言推理中的应用。作者指出，传统的文本中心方法将视觉视为静态背景，导致感知数据与符号思维之间存在“语义差距”。论文提出了一种新的思维模式，强调视觉信息在思维过程中的动态作用，使其成为可操作的认知工作空间。通过建立三阶段框架，论文为未来的多模态人工智能研究提供了清晰的路线图。'}}}, {'id': 'https://huggingface.co/papers/2507.02652', 'title': 'Decoupled Planning and Execution: A Hierarchical Reasoning Framework for\n  Deep Search', 'url': 'https://huggingface.co/papers/2507.02652', 'abstract': 'Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.', 'score': 12, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'e833cc483ac9b10c', 'authors': ['Jiajie Jin', 'Xiaoxi Li', 'Guanting Dong', 'Yuyao Zhang', 'Yutao Zhu', 'Yang Zhao', 'Hongjin Qian', 'Zhicheng Dou'], 'affiliations': ['Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2507.02652.jpg', 'data': {'categories': ['#optimization', '#reasoning', '#rag', '#benchmark', '#multimodal', '#agents'], 'emoji': '🧠', 'ru': {'title': 'HiRA: Иерархический подход к сложному информационному поиску', 'desc': 'Статья представляет HiRA - иерархическую систему для сложных информационных запросов. HiRA разделяет стратегическое планирование и специализированное выполнение задач, что позволяет эффективнее обрабатывать многоэтапные запросы. Система декомпозирует сложные задачи на подзадачи и назначает их специализированным агентам с внешними инструментами. Эксперименты показали, что HiRA превосходит современные RAG-системы и агентные подходы по качеству ответов и эффективности.'}, 'en': {'title': 'HiRA: Enhancing Search Efficiency through Hierarchical Reasoning', 'desc': 'This paper presents HiRA, a new framework designed to improve complex information retrieval tasks by separating high-level planning from detailed execution. Traditional methods often struggle because they use a single model for both tasks, which can lead to inefficiencies. HiRA addresses this by breaking down complex search tasks into smaller subtasks, each handled by specialized agents that have their own tools and reasoning abilities. The results show that HiRA outperforms existing systems in both the quality of answers and overall efficiency, demonstrating the benefits of this hierarchical approach.'}, 'zh': {'title': 'HiRA：分层框架提升搜索效率与质量', 'desc': '在现实世界的搜索场景中，复杂的信息需求需要深度推理和知识综合，而传统的检索增强生成（RAG）管道难以有效应对。当前的推理方法存在一个根本性限制：它们使用单一模型处理高层次规划和详细执行，导致推理效率低下和可扩展性有限。本文提出了HiRA，一个分层框架，将战略规划与专业执行分开。我们的研究表明，HiRA在复杂的跨模态深度搜索基准测试中显著优于现有的RAG和基于代理的系统，提升了答案质量和系统效率。'}}}, {'id': 'https://huggingface.co/papers/2507.02726', 'title': 'Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving', 'url': 'https://huggingface.co/papers/2507.02726', 'abstract': 'Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.', 'score': 11, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '42e132c4863440b8', 'authors': ['Matthieu Zimmer', 'Xiaotong Ji', 'Rasul Tutunov', 'Anthony Bordg', 'Jun Wang', 'Haitham Bou Ammar'], 'affiliations': ['Huawei Noahs Ark Lab', 'Imperial College London', 'Lagrange Center', 'UCL Centre for AI'], 'pdf_title_img': 'assets/pdf/title_img/2507.02726.jpg', 'data': {'categories': ['#architecture', '#reasoning', '#benchmark', '#agents', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Самогенерируемые цели улучшают автоматическое доказательство теорем', 'desc': 'Статья представляет новый подход к автоматическому доказательству теорем с использованием больших языковых моделей (LLM). Авторы предлагают фреймворк sG-MDP, в котором агенты генерируют и преследуют подцели на основе текущего состояния доказательства. Они применяют алгоритмы, подобные Monte Carlo Tree Search, для решения sG-MDP. Результатом является система Bourbaki (7B), которая достигает новых рекордных результатов на бенчмарке PutnamBench для моделей своего масштаба.'}, 'en': {'title': 'Empowering Reasoning with Self-Generated Goals in Theorem Proving', 'desc': 'This paper addresses the difficulties that large language models (LLMs) face in reasoning tasks, particularly in automated theorem proving (ATP) where rewards are sparse and proofs are complex. The authors propose a novel framework called self-generated goal-conditioned MDPs (sG-MDPs), which allows agents to create and pursue subgoals based on the current state of the proof. By structuring goal generation, the problem becomes easier to navigate and search. The framework is implemented in a system called Bourbaki (7B), which utilizes multiple LLMs to enhance subgoal generation and tactic synthesis, achieving state-of-the-art results on the challenging PutnamBench benchmark.'}, 'zh': {'title': '自生成目标助力推理挑战', 'desc': '本文探讨了大型语言模型（LLMs）在自动定理证明（ATP）中的推理挑战，尤其是在稀疏奖励和证明规模庞大的情况下。为了解决这些问题，提出了一种新的框架——自生成目标条件马尔可夫决策过程（sG-MDPs），使得智能体能够根据不断变化的证明状态生成和追求子目标。通过这种结构化的目标生成，问题变得更易于搜索。最后，应用类似蒙特卡洛树搜索（MCTS）的算法解决sG-MDP，并在Bourbaki（7B）系统中实现，成功在PutnamBench上解决了26个问题，创造了新的最先进结果。'}}}, {'id': 'https://huggingface.co/papers/2507.02754', 'title': 'Fast and Simplex: 2-Simplicial Attention in Triton', 'url': 'https://huggingface.co/papers/2507.02754', 'abstract': 'Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that 2-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.', 'score': 10, 'issue_id': 4638, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'a9492490e5a70bc4', 'authors': ['Aurko Roy', 'Timothy Chou', 'Sai Surya Duvvuri', 'Sijia Chen', 'Jiecao Yu', 'Xiaodong Wang', 'Manzil Zaheer', 'Rohan Anil'], 'affiliations': ['Department of Computer Science University of Texas at Austin', 'Meta Menlo Park, CA'], 'pdf_title_img': 'assets/pdf/title_img/2507.02754.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Повышение эффективности языковых моделей с помощью 2-симплициального внимания', 'desc': 'В статье исследуется архитектура 2-симплициального трансформера, обобщающая стандартное внимание на основе скалярного произведения до трилинейных функций. Авторы показывают, что эта архитектура достигает лучшей эффективности использования токенов по сравнению со стандартными трансформерами. При фиксированном бюджете токенов модели сопоставимого размера превосходят аналоги со скалярным произведением в задачах математики, программирования, рассуждений и логики. Количественно преимущества выражаются в изменении показателя степени в законах масштабирования для задач, связанных со знаниями и рассуждениями.'}, 'en': {'title': 'Enhancing Token Efficiency with 2-Simplicial Transformers', 'desc': 'This paper explores the limitations of current scaling laws in machine learning, particularly in the context of large language models that are no longer purely compute-bound due to their reliance on vast datasets. It introduces the 2-simplicial Transformer, a new architecture that enhances standard attention mechanisms by using trilinear functions, which improves token efficiency. The authors show that this new architecture allows models to perform better on various tasks, such as mathematics and reasoning, while using the same number of tokens. By quantifying the improvements, they reveal that the 2-simplicial attention modifies the scaling laws, leading to better performance in knowledge and reasoning tasks compared to traditional dot-product attention.'}, 'zh': {'title': '提升标记效率的2-单纯形变换器', 'desc': '最近的研究表明，训练损失与模型大小和标记数量呈幂律关系，达到计算最优模型需要同时扩大模型大小和标记数量。然而，这些缩放法则假设数据是无限的，并主要适用于计算受限的环境。随着现代大型语言模型越来越依赖于大规模互联网数据集，这种假设变得不再有效。因此，我们需要优先考虑标记效率的架构。'}}}, {'id': 'https://huggingface.co/papers/2507.02694', 'title': 'Can LLMs Identify Critical Limitations within Scientific Research? A\n  Systematic Evaluation on AI Research Papers', 'url': 'https://huggingface.co/papers/2507.02694', 'abstract': "Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.", 'score': 7, 'issue_id': 4641, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': 'd7b392be540c08ba', 'authors': ['Zhijian Xu', 'Yilun Zhao', 'Manasi Patwardhan', 'Lovekesh Vig', 'Arman Cohan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.02694.jpg', 'data': {'categories': ['#multimodal', '#rag', '#dataset', '#benchmark', '#synthetic', '#science'], 'emoji': '🔬', 'ru': {'title': 'ИИ на страже научной объективности: LLM как помощник в рецензировании', 'desc': 'Статья посвящена использованию больших языковых моделей (LLM) для помощи в процессе рецензирования научных работ, особенно в выявлении ограничений исследований. Авторы представляют таксономию типов ограничений в научных исследованиях и создают бенчмарк LimitGen для оценки способности LLM поддерживать ранние этапы обратной связи. LimitGen включает синтетический набор данных и реальные ограничения, написанные людьми. Исследователи улучшают способность LLM выявлять ограничения, дополняя их поиском по научной литературе.'}, 'en': {'title': 'Empowering Peer Review with AI: LimitGen for Identifying Research Limitations', 'desc': "This paper addresses the challenges of peer review in scientific research due to the increasing number of publications. It introduces a taxonomy of limitation types specifically for AI research, which helps in understanding the weaknesses of scientific papers. The authors present LimitGen, a benchmark designed to evaluate how well large language models (LLMs) can assist in identifying these limitations and provide feedback. By incorporating literature retrieval, the study enhances LLMs' ability to generate relevant and constructive critiques of research papers."}, 'zh': {'title': '提升同行评审的智能化支持', 'desc': '同行评审是科学研究的重要环节，但随着出版物数量的增加，这一过程面临越来越大的挑战。本文提出了一种全面的科学研究局限性分类法，特别关注人工智能领域。我们介绍了LimitGen，这是第一个评估大型语言模型（LLM）在支持早期反馈和补充人类评审方面能力的基准测试。通过结合文献检索，我们增强了LLM系统识别研究局限性的能力，从而能够提供更具体和建设性的反馈。'}}}, {'id': 'https://huggingface.co/papers/2507.02092', 'title': 'Energy-Based Transformers are Scalable Learners and Thinkers', 'url': 'https://huggingface.co/papers/2507.02092', 'abstract': 'Inference-time computation techniques, analogous to human System 2 Thinking, have recently become popular for improving model performances. However, most existing approaches suffer from several limitations: they are modality-specific (e.g., working only in text), problem-specific (e.g., verifiable domains like math and coding), or require additional supervision/training on top of unsupervised pretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question "Is it possible to generalize these System 2 Thinking approaches, and develop models that learn to think solely from unsupervised learning?" Interestingly, we find the answer is yes, by learning to explicitly verify the compatibility between inputs and candidate-predictions, and then re-framing prediction problems as optimization with respect to this verifier. Specifically, we train Energy-Based Transformers (EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy value to every input and candidate-prediction pair, enabling predictions through gradient descent-based energy minimization until convergence. Across both discrete (text) and continuous (visual) modalities, we find EBTs scale faster than the dominant Transformer++ approach during training, achieving an up to 35% higher scaling rate with respect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs improve performance with System 2 Thinking by 29% more than the Transformer++ on language tasks, and EBTs outperform Diffusion Transformers on image denoising while using fewer forward passes. Further, we find that EBTs achieve better results than existing models on most downstream tasks given the same or worse pretraining performance, suggesting that EBTs generalize better than existing approaches. Consequently, EBTs are a promising new paradigm for scaling both the learning and thinking capabilities of models.', 'score': 3, 'issue_id': 4642, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '9f33fd27885f443d', 'authors': ['Alexi Gladstone', 'Ganesh Nanduru', 'Md Mofijul Islam', 'Peixuan Han', 'Hyeonjeong Ha', 'Aman Chadha', 'Yilun Du', 'Heng Ji', 'Jundong Li', 'Tariq Iqbal'], 'affiliations': ['Amazon GenAI', 'Harvard University', 'Stanford University', 'UIUC', 'UVA'], 'pdf_title_img': 'assets/pdf/title_img/2507.02092.jpg', 'data': {'categories': ['#reasoning', '#inference', '#training', '#optimization', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'EBTs: Обучение мышлению через неконтролируемое обучение', 'desc': "Эта статья представляет новый класс моделей - Energy-Based Transformers (EBTs), которые обучаются проверять совместимость входных данных и предсказаний. EBTs переформулируют задачу предсказания как оптимизацию относительно верификатора, что позволяет им применять методы мышления 'Системы 2' без дополнительного обучения. Исследования показывают, что EBTs масштабируются быстрее, чем стандартные трансформеры, и демонстрируют лучшую производительность на задачах обработки текста и изображений. Авторы утверждают, что EBTs представляют собой многообещающую новую парадигму для масштабирования как обучающих, так и мыслительных способностей моделей."}, 'en': {'title': 'Energy-Based Transformers: Scaling Learning and Thinking in AI', 'desc': 'This paper introduces Energy-Based Transformers (EBTs), a new type of model that enhances inference-time computation by mimicking human System 2 Thinking. EBTs learn to verify the compatibility between inputs and predictions without needing additional supervision, making them more generalizable across different modalities and tasks. The authors demonstrate that EBTs can scale faster than existing models like Transformer++ and achieve superior performance in both language and image tasks. Overall, EBTs represent a significant advancement in the efficiency and effectiveness of machine learning models.'}, 'zh': {'title': '能量基础变换器：无监督学习的新思维方式', 'desc': '本文探讨了一种新的模型——能量基础变换器（EBTs），旨在通过无监督学习来实现更好的推理能力。EBTs通过显式验证输入与候选预测之间的兼容性，将预测问题重新框架为优化问题，从而提高模型的性能。研究表明，EBTs在训练过程中比传统的Transformer++方法具有更快的扩展速度，并在推理时在语言任务上提高了29%的性能。总体而言，EBTs在大多数下游任务中表现优于现有模型，显示出更好的泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2506.22813', 'title': 'Selecting and Merging: Towards Adaptable and Scalable Named Entity\n  Recognition with Large Language Models', 'url': 'https://huggingface.co/papers/2506.22813', 'abstract': "Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.", 'score': 3, 'issue_id': 4644, 'pub_date': '2025-06-28', 'pub_date_card': {'ru': '28 июня', 'en': 'June 28', 'zh': '6月28日'}, 'hash': '762c8f77cac2babf', 'authors': ['Zhuojun Ding', 'Wei Wei', 'Chenghao Fan'], 'affiliations': ['School of Computer Science & Technology, Huazhong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.22813.jpg', 'data': {'categories': ['#optimization', '#benchmark', '#transfer_learning', '#training', '#multimodal'], 'emoji': '🧩', 'ru': {'title': 'Динамическое объединение экспертов для адаптивного извлечения информации', 'desc': 'Статья представляет новый подход SaM для адаптации моделей извлечения информации к различным предметным областям. Вместо обучения единой модели, SaM динамически выбирает и объединяет предобученные экспертные модели на этапе вывода. Этот метод улучшает обобщение на новые домены без дополнительного обучения и обеспечивает масштабируемость. Эксперименты показывают, что SaM превосходит единую модель в среднем на 10% по различным бенчмаркам.'}, 'en': {'title': 'Dynamic Expert Selection for Enhanced Domain Adaptation', 'desc': 'The paper introduces the SaM framework, which enhances the performance of large language models in information extraction tasks by dynamically selecting and merging expert models tailored to specific domains. Instead of training a single model for all domains, SaM identifies domain-specific experts based on their relevance and performance, allowing for better adaptation to target tasks. This approach not only improves generalization across various domains but also offers scalability by enabling the addition or removal of experts without retraining. Experimental results show that the SaM framework outperforms traditional unified models by an average of 10%, highlighting its effectiveness in optimizing task-specific performance.'}, 'zh': {'title': '动态选择与合并专家模型，提升跨领域性能', 'desc': '监督微调（SFT）广泛应用于将大型语言模型（LLM）与信息提取（IE）任务对齐，例如命名实体识别（NER）。然而，标注这些细粒度标签和训练特定领域的模型成本高昂。现有方法通常在多个领域训练统一模型，但这种方法缺乏适应性和可扩展性，因为并非所有训练数据都能惠及目标领域。我们提出了SaM框架，在推理时动态选择和合并专家模型，从而提高了跨领域的泛化能力，而无需额外训练。'}}}, {'id': 'https://huggingface.co/papers/2507.02778', 'title': 'Self-Correction Bench: Revealing and Addressing the Self-Correction\n  Blind Spot in LLMs', 'url': 'https://huggingface.co/papers/2507.02778', 'abstract': 'Self-Correction Bench measures the self-correction blind spot in large language models, finding that training primarily on error-free responses contributes to this issue; appending "Wait" notably improves their ability to correct errors in their outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic \'Self-Correction Blind Spot\' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.', 'score': 2, 'issue_id': 4645, 'pub_date': '2025-07-03', 'pub_date_card': {'ru': '3 июля', 'en': 'July 3', 'zh': '7月3日'}, 'hash': '4b54d8d384329494', 'authors': ['Ken Tsui'], 'affiliations': ['Independent'], 'pdf_title_img': 'assets/pdf/title_img/2507.02778.jpg', 'data': {'categories': ['#reasoning', '#training', '#alignment', '#benchmark', '#hallucinations'], 'emoji': '🔍', 'ru': {'title': 'Самокоррекция: ключ к надежности LLM', 'desc': 'Исследование выявляет, что у больших языковых моделей (LLM) есть проблема с самокоррекцией, так как они обучаются на данных без ошибок. Это приводит к тому, что модели не могут исправлять собственные ошибки, хотя и могут находить ошибки в пользовательском вводе. Введение фразы "Wait" помогает моделям активировать способность к самокоррекции, что значительно уменьшает количество ошибок. Работа подчеркивает важность улучшения надежности LLM через обучение на данных с ошибками и их исправлениями.'}, 'en': {'title': 'Unlocking Self-Correction in Language Models', 'desc': "This paper introduces Self-Correction Bench, a framework designed to measure the self-correction capabilities of large language models (LLMs). It identifies a significant issue known as the 'Self-Correction Blind Spot', where LLMs fail to correct errors in their own outputs despite being able to recognize errors in user inputs. The study reveals that this blind spot is largely due to the training data, which often consists of error-free examples rather than sequences that include error corrections. Notably, the simple addition of the word 'Wait' to prompts can significantly enhance the models' self-correction abilities, indicating that these capabilities can be activated with the right cues."}, 'zh': {'title': '激活自我纠正，提升语言模型的可靠性', 'desc': '这篇论文介绍了自我纠正基准（Self-Correction Bench），用于测量大型语言模型（LLM）在自我纠正方面的盲点。研究发现，主要在无错误的响应上进行训练会导致模型在自身输出中无法纠正相同的错误。通过对14个模型进行测试，发现平均有64.5%的盲点率。简单地在输出中添加“等待”一词可以将盲点减少89.3%，这表明模型具备自我纠正的能力，但需要激活。'}}}, {'id': 'https://huggingface.co/papers/2507.01663', 'title': 'AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM\n  Post-Training', 'url': 'https://huggingface.co/papers/2507.01663', 'abstract': 'Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs.', 'score': 2, 'issue_id': 4639, 'pub_date': '2025-07-02', 'pub_date_card': {'ru': '2 июля', 'en': 'July 2', 'zh': '7月2日'}, 'hash': '8a3f43a4a9e735d7', 'authors': ['Zhenyu Han', 'Ansheng You', 'Haibo Wang', 'Kui Luo', 'Guang Yang', 'Wenqi Shi', 'Menglong Chen', 'Sicheng Zhang', 'Zeshun Lan', 'Chunshi Deng', 'Huazhong Ji', 'Wenjie Liu', 'Yu Huang', 'Yixiang Zhang', 'Chenyi Pan', 'Jing Wang', 'Xin Huang', 'Chunsheng Li', 'Jianping Wu'], 'affiliations': ['Huawei'], 'pdf_title_img': 'assets/pdf/title_img/2507.01663.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization', '#rl'], 'emoji': '🔄', 'ru': {'title': 'AsyncFlow: Асинхронное обучение с подкреплением для больших языковых моделей', 'desc': 'AsyncFlow - это новая асинхронная потоковая система обучения с подкреплением для эффективной пост-обработки больших языковых моделей. Она вводит распределенный модуль хранения и передачи данных, обеспечивающий унифицированное управление данными и детальное планирование в потоковом режиме. AsyncFlow использует асинхронный рабочий процесс на основе модели производитель-потребитель для минимизации простоев вычислений. Система показывает среднее улучшение пропускной способности в 1,59 раза по сравнению с современными аналогами.'}, 'en': {'title': 'AsyncFlow: Revolutionizing RL for Large Language Models', 'desc': 'This paper introduces AsyncFlow, a new framework for reinforcement learning (RL) that enhances the post-training phase of large language models (LLMs). It addresses scalability issues found in traditional RL frameworks by implementing a distributed data storage and transfer system, which allows for efficient data management and scheduling. The framework also features an asynchronous workflow that reduces idle computation time by optimizing the timing of parameter updates. Overall, AsyncFlow is designed to be modular and customizable, making it easier to integrate with various training and inference engines while improving throughput significantly.'}, 'zh': {'title': 'AsyncFlow：高效的异步流式强化学习框架', 'desc': '强化学习（RL）在大型语言模型（LLM）的后训练阶段变得至关重要。传统的任务共存RL框架面临可扩展性瓶颈，而任务分离的RL框架在复杂数据流和资源闲置方面存在挑战。为了解决这些问题，我们提出了AsyncFlow，一个高效的异步流式RL框架，能够实现自动化的管道重叠和动态负载平衡。我们的实验表明，与最先进的基线相比，AsyncFlow在吞吐量上平均提高了1.59倍。'}}}, {'id': 'https://huggingface.co/papers/2507.01004', 'title': 'ZeCO: Zero Communication Overhead Sequence Parallelism for Linear\n  Attention', 'url': 'https://huggingface.co/papers/2507.01004', 'abstract': 'A new zero communication overhead sequence parallelism method called ZeCO enables efficient training of large language models with ultra-long sequences across multiple devices.  \t\t\t\t\tAI-generated summary \t\t\t\t Linear attention mechanisms deliver significant advantages for Large Language Models (LLMs) by providing linear computational complexity, enabling efficient processing of ultra-long sequences (e.g., 1M context). However, existing Sequence Parallelism (SP) methods, essential for distributing these workloads across devices, become the primary bottleneck due to substantial communication overhead. In this paper, we introduce ZeCO (Zero Communication Overhead) sequence parallelism for linear attention models, a new SP method designed to overcome these limitations and achieve end-to-end near-linear scalability for long sequence training. For example, training a model with a 1M sequence length across 64 devices using ZeCO takes roughly the same time as training with an 16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new collective communication primitive. All-Scan provides each SP rank with precisely the initial operator state it requires while maintaining a minimal communication footprint, effectively eliminating communication overhead. Theoretically, we prove the optimaity of ZeCO, showing that it introduces only negligible time and space overhead. Empirically, we compare the communication costs of different sequence parallelism strategies and demonstrate that All-Scan achieves the fastest communication in SP scenarios. Specifically, on 256 GPUs with an 8M sequence length, ZeCO achieves a 60\\% speedup compared to the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a clear path toward efficiently training next-generation LLMs on previously intractable sequence lengths.', 'score': 2, 'issue_id': 4645, 'pub_date': '2025-07-01', 'pub_date_card': {'ru': '1 июля', 'en': 'July 1', 'zh': '7月1日'}, 'hash': 'c104abc218e38a97', 'authors': ['Yuhong Chou', 'Zehao Liu', 'Ruijie Zhu', 'Xinyi Wan', 'Tianjian Li', 'Congying Chu', 'Qian Liu', 'Jibin Wu', 'Zejun Ma'], 'affiliations': ['Institute of Automation, Chinese Academy of Sciences', 'National University of Singapore', 'The Hong Kong Polytechnic University', 'TikTok', 'UC Santa Cruz'], 'pdf_title_img': 'assets/pdf/title_img/2507.01004.jpg', 'data': {'categories': ['#training', '#architecture', '#long_context', '#optimization'], 'emoji': '🚀', 'ru': {'title': 'ZeCO: Революция в обучении языковых моделей с ультрадлинными последовательностями', 'desc': 'ZeCO - это новый метод параллелизма последовательностей с нулевыми накладными расходами на коммуникацию, разработанный для эффективного обучения больших языковых моделей с ультрадлинными последовательностями на нескольких устройствах. В основе ZeCO лежит новый примитив коллективной коммуникации All-Scan, который обеспечивает каждому рангу SP точное начальное состояние оператора при минимальных затратах на коммуникацию. Теоретически доказана оптимальность ZeCO, показывающая, что он вносит лишь незначительные временные и пространственные накладные расходы. Эмпирически продемонстрировано, что на 256 GPU с длиной последовательности 8M ZeCO достигает ускорения на 60% по сравнению с современными методами SP.'}, 'en': {'title': 'ZeCO: Revolutionizing Long Sequence Training with Zero Communication Overhead', 'desc': 'This paper presents ZeCO, a novel sequence parallelism method that eliminates communication overhead during the training of large language models (LLMs) with ultra-long sequences. By utilizing linear attention mechanisms, ZeCO allows for efficient processing of sequences up to 1 million tokens across multiple devices without the typical bottlenecks caused by communication delays. The core innovation, All-Scan, enables each device to access the necessary operator state with minimal communication, resulting in near-linear scalability for long sequence training. Empirical results show that ZeCO significantly outperforms existing methods, achieving a 60% speedup on 256 GPUs with an 8M sequence length, paving the way for training next-generation LLMs.'}, 'zh': {'title': 'ZeCO：高效训练超长序列的大型语言模型', 'desc': '本文介绍了一种新的零通信开销序列并行方法ZeCO，旨在高效训练具有超长序列的大型语言模型。ZeCO通过引入All-Scan这一新的集体通信原语，显著减少了设备间的通信开销，从而实现了接近线性的可扩展性。与传统的序列并行方法相比，ZeCO在多个设备上处理1M序列时的训练时间与在单个设备上处理16k序列的时间相当。实验结果表明，ZeCO在256个GPU上处理8M序列时，相较于现有的最佳序列并行方法实现了60%的速度提升。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi', '#alignment (4)', '#architecture (6)', '#audio', '#benchmark (6)', '#cv (1)', '#data (1)', '#dataset (2)', '#diffusion (2)', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (5)', '#open_source (2)', '#optimization (7)', '#plp', '#rag (2)', '#reasoning (7)', '#rl (3)', '#rlhf (1)', '#robotics', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (10)', '#transfer_learning (1)', '#video'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-07-04 13:26',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-04 13:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-04 13:26')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    