
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 24 papers. May 13.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["–º–∏–Ω—É—Ç—É", "–º–∏–Ω—É—Ç—ã", "–º–∏–Ω—É—Ç"],
                hour: ["—á–∞—Å", "—á–∞—Å–∞", "—á–∞—Å–æ–≤"],
                day: ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π"],
                justNow: "—Ç–æ–ª—å–∫–æ —á—Ç–æ",
                ago: "–Ω–∞–∑–∞–¥"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["ÂàÜÈíü", "ÂàÜÈíü", "ÂàÜÈíü"],
                hour: ["Â∞èÊó∂", "Â∞èÊó∂", "Â∞èÊó∂"],
                day: ["Â§©", "Â§©", "Â§©"],
                justNow: "ÂàöÂàö",
                ago: "Ââç"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "—Å—Ç–∞—Ç–µ–π";
            } else if (lastDigit === 1) {
                word = "—Å—Ç–∞—Ç—å—è";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "—Å—Ç–∞—Ç—å–∏";
            } else {
                word = "—Å—Ç–∞—Ç–µ–π";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ÁØáËÆ∫Êñá"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">üî∫</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">13 –º–∞—è</span> | <span id="title-articles-count">24 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-05-12.html">‚¨ÖÔ∏è <span id="prev-date">12.05</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-05-14.html">‚û°Ô∏è <span id="next-date">14.05</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-05.html">üìà <span id='top-month-label'>–ú–µ—Å—è—Ü</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">üîÄ <span id="sort-label-text">–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">—Ä–µ–π—Ç–∏–Ω–≥—É</option>
                    <option value="pub_date">–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏</option>
                    <option value="issue_id">–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">üè∑Ô∏è –§–∏–ª—å—Ç—Ä</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A‚à™B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A‚à©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">üßπ</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ‚úñÔ∏è <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '13 –º–∞—è', 'en': 'May 13', 'zh': '5Êúà13Êó•'};
        let feedDateNext = {'ru': '14.05', 'en': '05/14', 'zh': '5Êúà14Êó•'};
        let feedDatePrev = {'ru': '12.05', 'en': '05/12', 'zh': '5Êúà12Êó•'};
        let filterLabel = {'ru': '–§–∏–ª—å—Ç—Ä', 'en': 'Topics', 'zh': '‰∏ªÈ¢òÁ≠õÈÄâ'}
        let publishedLabel = {'ru': '—Å—Ç–∞—Ç—å—è –æ—Ç ', 'en': 'published on ', 'zh': 'ÂèëË°®‰∫é'}
        let sortLabel = {'ru': '–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ', 'en': 'Sort by', 'zh': 'ÊéíÂ∫èÊñπÂºè'}
        let paperLabel = {'ru': '–°—Ç–∞—Ç—å—è', 'en': 'Paper', 'zh': 'ËÆ∫Êñá'}
        let topMonthLabel = {'ru': '–ú–µ—Å—è—Ü', 'en': 'Month', 'zh': 'ÊúàÂ∫¶ËÆ∫Êñá'}
        let topDayLabel = {'ru': '–î–µ–Ω—å', 'en': 'Day', 'zh': 'Êó•Â∫¶ËÆ∫Êñá'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2505.07062', 'title': 'Seed1.5-VL Technical Report', 'url': 'https://huggingface.co/papers/2505.07062', 'abstract': 'We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)', 'score': 83, 'issue_id': 3722, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'c3406b40cc21820d', 'authors': ['Dong Guo', 'Faming Wu', 'Feida Zhu', 'Fuxing Leng', 'Guang Shi', 'Haobin Chen', 'Haoqi Fan', 'Jian Wang', 'Jianyu Jiang', 'Jiawei Wang', 'Jingji Chen', 'Jingjia Huang', 'Kang Lei', 'Liping Yuan', 'Lishu Luo', 'Pengfei Liu', 'Qinghao Ye', 'Rui Qian', 'Shen Yan', 'Shixiong Zhao', 'Shuai Peng', 'Shuangye Li', 'Sihang Yuan', 'Sijin Wu', 'Tianheng Cheng', 'Weiwei Liu', 'Wenqian Wang', 'Xianhan Zeng', 'Xiao Liu', 'Xiaobo Qin', 'Xiaohan Ding', 'Xiaojun Xiao', 'Xiaoying Zhang', 'Xuanwei Zhang', 'Xuehan Xiong', 'Yanghua Peng', 'Yangrui Chen', 'Yanwei Li', 'Yanxu Hu', 'Yi Lin', 'Yiyuan Hu', 'Yiyuan Zhang', 'Youbin Wu', 'Yu Li', 'Yudong Liu', 'Yue Ling', 'Yujia Qin', 'Zanbo Wang', 'Zhiwu He', 'Aoxue Zhang', 'Bairen Yi', 'Bencheng Liao', 'Can Huang', 'Can Zhang', 'Chaorui Deng', 'Chaoyi Deng', 'Cheng Lin', 'Cheng Yuan', 'Chenggang Li', 'Chenhui Gou', 'Chenwei Lou', 'Chengzhi Wei', 'Chundian Liu', 'Chunyuan Li', 'Deyao Zhu', 'Donghong Zhong', 'Feng Li', 'Feng Zhang', 'Gang Wu', 'Guodong Li', 'Guohong Xiao', 'Haibin Lin', 'Haihua Yang', 'Haoming Wang', 'Heng Ji', 'Hongxiang Hao', 'Hui Shen', 'Huixia Li', 'Jiahao Li', 'Jialong Wu', 'Jianhua Zhu', 'Jianpeng Jiao', 'Jiashi Feng', 'Jiaze Chen', 'Jianhui Duan', 'Jihao Liu', 'Jin Zeng', 'Jingqun Tang', 'Jingyu Sun', 'Joya Chen', 'Jun Long', 'Junda Feng', 'Junfeng Zhan', 'Junjie Fang', 'Junting Lu', 'Kai Hua', 'Kai Liu', 'Kai Shen', 'Kaiyuan Zhang', 'Ke Shen', 'Ke Wang', 'Keyu Pan', 'Kun Zhang', 'Kunchang Li', 'Lanxin Li', 'Lei Li', 'Lei Shi', 'Li Han', 'Liang Xiang', 'Liangqiang Chen', 'Lin Chen', 'Lin Li', 'Lin Yan', 'Liying Chi', 'Longxiang Liu', 'Mengfei Du', 'Mingxuan Wang', 'Ningxin Pan', 'Peibin Chen', 'Pengfei Chen', 'Pengfei Wu', 'Qingqing Yuan', 'Qingyao Shuai', 'Qiuyan Tao', 'Renjie Zheng', 'Renrui Zhang', 'Ru Zhang', 'Rui Wang', 'Rui Yang', 'Rui Zhao', 'Shaoqiang Xu', 'Shihao Liang', 'Shipeng Yan', 'Shu Zhong', 'Shuaishuai Cao', 'Shuangzhi Wu', 'Shufan Liu', 'Shuhan Chang', 'Songhua Cai', 'Tenglong Ao', 'Tianhao Yang', 'Tingting Zhang', 'Wanjun Zhong', 'Wei Jia', 'Wei Weng', 'Weihao Yu', 'Wenhao Huang', 'Wenjia Zhu', 'Wenli Yang', 'Wenzhi Wang', 'Xiang Long', 'XiangRui Yin', 'Xiao Li', 'Xiaolei Zhu', 'Xiaoying Jia', 'Xijin Zhang', 'Xin Liu', 'Xinchen Zhang', 'Xinyu Yang', 'Xiongcai Luo', 'Xiuli Chen', 'Xuantong Zhong', 'Xuefeng Xiao', 'Xujing Li', 'Yan Wu', 'Yawei Wen', 'Yifan Du', 'Yihao Zhang', 'Yining Ye', 'Yonghui Wu', 'Yu Liu', 'Yu Yue', 'Yufeng Zhou', 'Yufeng Yuan', 'Yuhang Xu', 'Yuhong Yang', 'Yun Zhang', 'Yunhao Fang', 'Yuntao Li', 'Yurui Ren', 'Yuwen Xiong', 'Zehua Hong', 'Zehua Wang', 'Zewei Sun', 'Zeyu Wang', 'Zhao Cai', 'Zhaoyue Zha', 'Zhecheng An', 'Zhehui Zhao', 'Zhengzhuo Xu', 'Zhipeng Chen', 'Zhiyong Wu', 'Zhuofan Zheng', 'Zihao Wang', 'Zilong Huang', 'Ziyu Zhu', 'Zuquan Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07062.jpg', 'data': {'categories': ['#agi', '#multimodal', '#survey', '#architecture', '#training', '#reasoning', '#data'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –≤—ã–¥–∞—é—â–∏–º–∏—Å—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—è–º–∏', 'desc': 'Seed1.5-VL - —ç—Ç–æ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, —Å–æ—á–µ—Ç–∞—é—â–∞—è –∑—Ä–µ–Ω–∏–µ –∏ —è–∑—ã–∫ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ 532 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å 20 –º–ª—Ä–¥ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –∫–æ–º–ø–∞–∫—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —à–∏—Ä–æ–∫–æ–º —Å–ø–µ–∫—Ç—Ä–µ –∑–∞–¥–∞—á, –¥–æ—Å—Ç–∏–≥–∞—è state-of-the-art –Ω–∞ 38 –∏–∑ 60 –ø—É–±–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤. Seed1.5-VL –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –≤ –∑–∞–¥–∞—á–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞–º–∏, –∏–≥—Ä–æ–≤–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –∏ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –≤–µ–¥—É—â–∏–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.'}, 'en': {'title': 'Empowering Multimodal Understanding with Seed1.5-VL', 'desc': 'Seed1.5-VL is a vision-language foundation model that enhances multimodal understanding and reasoning. It features a compact architecture with a 532M-parameter vision encoder and a 20B parameter Mixture-of-Experts (MoE) language model, achieving state-of-the-art results on many benchmarks. The model excels in agent-centric tasks like GUI control and gameplay, outperforming other leading systems. Additionally, it showcases strong reasoning capabilities, making it effective for complex multimodal challenges such as visual puzzles.'}, 'zh': {'title': 'Seed1.5-VLÔºöÂ§öÊ®°ÊÄÅÁêÜËß£‰∏éÊé®ÁêÜÁöÑÊñ∞Á™ÅÁ†¥', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜSeed1.5-VLÔºåËøôÊòØ‰∏ÄÁßçÊó®Âú®ÊèêÂçáÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÊé®ÁêÜÁöÑËßÜËßâ-ËØ≠Ë®ÄÂü∫Á°ÄÊ®°Âûã„ÄÇSeed1.5-VLÁî±‰∏Ä‰∏™532MÂèÇÊï∞ÁöÑËßÜËßâÁºñÁ†ÅÂô®Âíå‰∏Ä‰∏™ÂÖ∑Êúâ20BÊ¥ªË∑ÉÂèÇÊï∞ÁöÑ‰∏ìÂÆ∂Ê∑∑ÂêàÊ®°ÂûãÔºàMoE LLMÔºâÁªÑÊàê„ÄÇÂ∞ΩÁÆ°ÂÖ∂Êû∂ÊûÑÁõ∏ÂØπÁ¥ßÂáëÔºå‰ΩÜÂú®Â§ö‰∏™ÂÖ¨ÂÖ±VLMÂü∫ÂáÜÊµãËØï‰∏≠Ë°®Áé∞Âá∫Ëâ≤ÔºåÂú®60‰∏™ÂÖ¨ÂÖ±Âü∫ÂáÜ‰∏≠Êúâ38‰∏™ËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÊÄßËÉΩ„ÄÇÊ≠§Â§ñÔºåÂú®‰ª•‰ª£ÁêÜ‰∏∫‰∏≠ÂøÉÁöÑ‰ªªÂä°‰∏≠ÔºåÂ¶ÇÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÊéßÂà∂ÂíåÊ∏∏ÊàèÁé©Ê≥ïÔºåSeed1.5-VLË∂ÖË∂ä‰∫ÜÈ¢ÜÂÖàÁöÑÂ§öÊ®°ÊÄÅÁ≥ªÁªüÔºåÂåÖÊã¨OpenAI CUAÂíåClaude 3.7„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07608', 'title': 'MiMo: Unlocking the Reasoning Potential of Language Model -- From\n  Pretraining to Posttraining', 'url': 'https://huggingface.co/papers/2505.07608', 'abstract': "We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.", 'score': 52, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9db5f7b72add3369', 'authors': ['Xiaomi LLM-Core Team', ':', 'Bingquan Xia', 'Bowen Shen', 'Cici', 'Dawei Zhu', 'Di Zhang', 'Gang Wang', 'Hailin Zhang', 'Huaqiu Liu', 'Jiebao Xiao', 'Jinhao Dong', 'Liang Zhao', 'Peidian Li', 'Peng Wang', 'Shihua Yu', 'Shimao Chen', 'Weikun Wang', 'Wenhan Ma', 'Xiangwei Deng', 'Yi Huang', 'Yifan Song', 'Zihan Jiang', 'Bowen Ye', 'Can Cai', 'Chenhong He', 'Dong Zhang', 'Duo Zhang', 'Guoan Wang', 'Hao Tian', 'Haochen Zhao', 'Heng Qu', 'Hongshen Xu', 'Jun Shi', 'Kainan Bao', 'QingKai Fang', 'Kang Zhou', 'Kangyang Zhou', 'Lei Li', 'Menghang Zhu', 'Nuo Chen', 'Qiantong Wang', 'Shaohui Liu', 'Shicheng Li', 'Shuhao Gu', 'Shuhuai Ren', 'Shuo Liu', 'Sirui Deng', 'Weiji Zhuang', 'Weiwei Lv', 'Wenyu Yang', 'Xin Zhang', 'Xing Yong', 'Xing Zhang', 'Xingchen Song', 'Xinzhe Xu', 'Xu Wang', 'Yihan Yan', 'Yu Tu', 'Yuanyuan Tian', 'Yudong Wang', 'Yue Yu', 'Zhenru Lin', 'Zhichao Song', 'Zihao Yue'], 'affiliations': ['Xiaomi LLM-Core Team'], 'pdf_title_img': 'assets/pdf/title_img/2505.07608.jpg', 'data': {'categories': ['#plp', '#reasoning', '#optimization', '#dataset', '#math', '#rl', '#data', '#training'], 'emoji': 'üß†', 'ru': {'title': 'MiMo-7B: –ú–æ—â–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': 'MiMo-7B - —ç—Ç–æ –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∑–∞–¥–∞—á —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ç—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –ù–∞ —ç—Ç–∞–ø–µ –ø–æ—Å—Ç-–æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è–ª–æ—Å—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –Ω–∞–±–æ—Ä–µ –∏–∑ 130 —Ç—ã—Å—è—á –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ–º—ã—Ö –∑–∞–¥–∞—á –ø–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é. –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å MiMo-7B-RL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'MiMo-7B: Revolutionizing Reasoning with Advanced Training Techniques', 'desc': 'MiMo-7B is a large language model specifically designed for reasoning tasks, optimized through both pre-training and post-training processes. In the pre-training phase, it utilizes an advanced data preprocessing pipeline and a three-stage data mixing strategy to enhance its reasoning capabilities, training on a massive dataset of 25 trillion tokens. The post-training phase involves reinforcement learning with a curated dataset of 130,000 math and programming problems, addressing sparse-reward challenges with a code-reward scheme and strategic data resampling. Evaluations demonstrate that MiMo-7B-Base excels in reasoning tasks, outperforming larger models, while the final RL-tuned version, MiMo-7B-RL, achieves outstanding results in mathematics, coding, and general reasoning tasks.'}, 'zh': {'title': 'MiMo-7BÔºöÊé®ÁêÜ‰ªªÂä°ÁöÑÂº∫Â§ßËØ≠Ë®ÄÊ®°Âûã', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜMiMo-7BÔºåËøôÊòØ‰∏Ä‰∏™‰∏ì‰∏∫Êé®ÁêÜ‰ªªÂä°ËÆæËÆ°ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºå‰ºòÂåñ‰∫ÜÈ¢ÑËÆ≠ÁªÉÂíåÂêéËÆ≠ÁªÉÈò∂ÊÆµ„ÄÇÂú®È¢ÑËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊàë‰ª¨Â¢ûÂº∫‰∫ÜÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊµÅÁ®ãÔºåÂπ∂ÈááÁî®‰∏âÈò∂ÊÆµÊï∞ÊçÆÊ∑∑ÂêàÁ≠ñÁï•Ôºå‰ª•ÊèêÂçáÂü∫Á°ÄÊ®°ÂûãÁöÑÊé®ÁêÜËÉΩÂäõ„ÄÇMiMo-7B-BaseÂú®25‰∏á‰∫ø‰∏™Ê†áËÆ∞‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Â¢ûÂä†‰∫ÜÂ§öÊ†áËÆ∞È¢ÑÊµãÁõÆÊ†áÔºå‰ª•ÊèêÈ´òÊÄßËÉΩÂíåÂä†ÈÄüÊé®ÁêÜÈÄüÂ∫¶„ÄÇÂú®ÂêéËÆ≠ÁªÉÈò∂ÊÆµÔºåÊàë‰ª¨Êï¥ÁêÜ‰∫Ü130K‰∏™ÂèØÈ™åËØÅÁöÑÊï∞Â≠¶ÂíåÁºñÁ®ãÈóÆÈ¢òÊï∞ÊçÆÈõÜÔºåÁªìÂêàÊµãËØïÈöæÂ∫¶È©±Âä®ÁöÑ‰ª£Á†ÅÂ•ñÂä±Êú∫Âà∂ÔºåËß£ÂÜ≥Á®ÄÁñèÂ•ñÂä±ÈóÆÈ¢òÔºåÂπ∂ÈááÁî®ÊàòÁï•ÊÄßÊï∞ÊçÆÈáçÈááÊ†∑Êù•Á®≥ÂÆöËÆ≠ÁªÉ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07747', 'title': 'Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets', 'url': 'https://huggingface.co/papers/2505.07747', 'abstract': 'While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.', 'score': 46, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'd9ffe741ebae4acb', 'authors': ['Weiyu Li', 'Xuanyang Zhang', 'Zheng Sun', 'Di Qi', 'Hao Li', 'Wei Cheng', 'Weiwei Cai', 'Shihao Wu', 'Jiarui Liu', 'Zihao Wang', 'Xiao Chen', 'Feipeng Tian', 'Jianxiong Pan', 'Zeming Li', 'Gang Yu', 'Xiangyu Zhang', 'Daxin Jiang', 'Ping Tan'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07747.jpg', 'data': {'categories': ['#open_source', '#dataset', '#architecture', '#data', '#diffusion', '#transfer_learning', '#3d', '#benchmark'], 'emoji': 'üßä', 'ru': {'title': '–û—Ç–∫—Ä—ã—Ç–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç Step1X-3D - –æ—Ç–∫—Ä—ã—Ç—É—é —Å–∏—Å—Ç–µ–º—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 3D-–æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ê–≤—Ç–æ—Ä—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–≤ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ 3D-–º–æ–¥–µ–ª–µ–π. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã –≤–∫–ª—é—á–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π VAE-DiT –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–µ–æ–º–µ—Ç—Ä–∏–∏ –∏ –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä. Step1X-3D –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –º–µ—Ç–æ–¥—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∏–∑ 2D-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫ 3D-—Å–∏–Ω—Ç–µ–∑—É.'}, 'en': {'title': 'Revolutionizing 3D Generation with Step1X-3D', 'desc': 'The paper introduces Step1X-3D, a framework designed to improve 3D generation in artificial intelligence. It tackles challenges like limited data and algorithmic issues by creating a high-quality dataset and employing a two-stage architecture that combines a geometry generator and a texture synthesis module. The framework allows for better detail preservation and consistency in 3D assets by integrating techniques from 2D generation. By providing open-source resources, it aims to enhance research and development in controllable 3D asset generation.'}, 'zh': {'title': 'Step1X-3DÔºöÂºÄÂàõÂèØÊéß3DÁîüÊàêÁöÑÊñ∞Ê†áÂáÜ', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜStep1X-3DÔºåËøôÊòØ‰∏Ä‰∏™ÂºÄÊîæÊ°ÜÊû∂ÔºåÊó®Âú®Ëß£ÂÜ≥3DÁîüÊàê‰∏≠ÁöÑÊï∞ÊçÆÁ®ÄÁº∫„ÄÅÁÆóÊ≥ïÈôêÂà∂ÂíåÁîüÊÄÅÁ≥ªÁªüÁ¢éÁâáÂåñÁ≠âÊåëÊàò„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøá‰∏•Ê†ºÁöÑÊï∞ÊçÆÊï¥ÁêÜÊµÅÁ®ãÔºåÂ§ÑÁêÜË∂ÖËøá500‰∏áËµÑ‰∫ßÔºåÂàõÂª∫‰∫Ü‰∏Ä‰∏™200‰∏áÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºåÂπ∂ÈááÁî®Ê†áÂáÜÂåñÁöÑÂá†‰ΩïÂíåÁ∫πÁêÜÂ±ûÊÄß„ÄÇÂÆÉÁªìÂêà‰∫ÜÊ∑∑ÂêàVAE-DiTÂá†‰ΩïÁîüÊàêÂô®ÂíåÂü∫‰∫éÊâ©Êï£ÁöÑÁ∫πÁêÜÂêàÊàêÊ®°ÂùóÔºåËÉΩÂ§üÁîüÊàêÈ´òË¥®ÈáèÁöÑ3DÊ®°Âûã„ÄÇStep1X-3DËøòÊîØÊåÅÂ∞Ü2DÊéßÂà∂ÊäÄÊúØÁõ¥Êé•ËΩ¨ÁßªÂà∞3DÂêàÊàêÔºåÊé®Âä®‰∫ÜÂèØÊéß3DËµÑ‰∫ßÁîüÊàêÁöÑÊñ∞Ê†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07787', 'title': 'Learning from Peers in Reasoning Models', 'url': 'https://huggingface.co/papers/2505.07787', 'abstract': 'Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP\'s robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .', 'score': 34, 'issue_id': 3722, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '350f28f20ab516fc', 'authors': ['Tongxu Luo', 'Wenyu Du', 'Jiaxi Bi', 'Stephen Chung', 'Zhengyang Tang', 'Hao Yang', 'Min Zhang', 'Benyou Wang'], 'affiliations': ['DualityRL', 'Huawei', 'The Chinese University of Hong Kong, Shenzhen', 'USTB'], 'pdf_title_img': 'assets/pdf/title_img/2505.07787.jpg', 'data': {'categories': ['#optimization', '#math', '#training', '#small_models', '#reasoning', '#dataset'], 'emoji': 'üß†', 'ru': {'title': '–ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑—É–º: –∫–∞–∫ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É—á–∞—Ç—Å—è –¥—Ä—É–≥ —É –¥—Ä—É–≥–∞', 'desc': "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∫—Ä—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (LRM) –º–æ–≥—É—Ç –ø–æ–ø–∞–¥–∞—Ç—å –≤ '–ª–æ–≤—É—à–∫—É –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–µ—Ñ–∏–∫—Å–∞', –∫–æ–≥–¥–∞ –ø–ª–æ—Ö–æ–µ –Ω–∞—á–∞–ª–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –º–µ—à–∞–µ—Ç —Å–∞–º–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏. –î–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –º–µ—Ç–æ–¥ '–û–±—É—á–µ–Ω–∏–µ —É —Å–≤–µ—Ä—Å—Ç–Ω–∏–∫–æ–≤' (LeaP), –ø–æ–∑–≤–æ–ª—è—é—â–∏–π –º–æ–¥–µ–ª—è–º –æ–±–º–µ–Ω–∏–≤–∞—Ç—å—Å—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –≤–æ –≤—Ä–µ–º—è –≤—ã–≤–æ–¥–∞. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ —Å–µ—Ä–∏—é –º–æ–¥–µ–ª–µ–π LeaP-T, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –ø–æ –æ–±–æ–±—â–µ–Ω–∏—é –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LeaP."}, 'en': {'title': 'Empowering LRMs through Peer Collaboration', 'desc': "This paper introduces a new approach called Learning from Peers (LeaP) to enhance the self-correction capabilities of Large Reasoning Models (LRMs). It identifies a challenge known as the 'Prefix Dominance Trap', where poor initial reasoning hinders recovery. LeaP allows models to share intermediate reasoning insights through a routing mechanism, promoting collaborative learning among reasoning paths. The results show that LeaP significantly improves performance on various benchmarks, demonstrating effective error correction and adaptability to different task difficulties."}, 'zh': {'title': 'Âêå‰º¥Â≠¶‰π†ÔºöÊèêÂçáÊé®ÁêÜÊ®°ÂûãÁöÑËá™ÊàëÁ∫†Ê≠£ËÉΩÂäõ', 'desc': 'Â§ßÂûãÊé®ÁêÜÊ®°ÂûãÔºàLRMsÔºâÂÖ∑ÊúâËá™ÊàëÁ∫†Ê≠£ÁöÑËÉΩÂäõÔºå‰ΩÜÂΩìÊé®ÁêÜËøáÁ®ã‰ª•Áü≠ËÄåÂ∑ÆÁöÑÂºÄÂ§¥ÂºÄÂßãÊó∂ÔºåÊ®°ÂûãÂæàÈöæÊÅ¢Â§ç„ÄÇÊàë‰ª¨Áß∞ËøôÁßçÁé∞Ë±°‰∏∫‚ÄúÂâçÁºÄ‰∏ªÂØºÈô∑Èò±‚Äù„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‚Äú‰ªéÂêå‰º¥Â≠¶‰π†‚ÄùÔºàLeaPÔºâÔºåÈÄöËøáË∑ØÁî±Êú∫Âà∂ËÆ©ÊØè‰∏™Êé®ÁêÜË∑ØÂæÑÊÄªÁªìÂÖ∂‰∏≠Èó¥Êé®ÁêÜÂπ∂‰∏éÂÖ∂‰ªñË∑ØÂæÑÂÖ±‰∫´Ôºå‰ªéËÄåÂú®Êé®ÁêÜËøáÁ®ã‰∏≠ËûçÂÖ•Âêå‰º¥ÁöÑËßÅËß£„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåLeaPÊòæËëóÊèêÈ´ò‰∫ÜÊ®°ÂûãÁöÑË°®Áé∞ÔºåÂ∞§ÂÖ∂ÊòØÂú®Â§ÑÁêÜ‰∏çÂêå‰ªªÂä°ÈöæÂ∫¶Êó∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÈîôËØØÂÆπÂøçËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07447', 'title': 'Unified Continuous Generative Models', 'url': 'https://huggingface.co/papers/2505.07447', 'abstract': 'Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.', 'score': 31, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '8d18ef028e9905b1', 'authors': ['Peng Sun', 'Yi Jiang', 'Tao Lin'], 'affiliations': ['Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.07447.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#training', '#cv'], 'emoji': 'üîÑ', 'ru': {'title': '–ï–¥–∏–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: –æ—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã—Ö –¥–æ –º–∞–ª–æ—à–∞–≥–æ–≤—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –∏ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ –¥–∏—Ñ—Ñ—É–∑–∏—è –∏ flow-matching, —Å –º–∞–ª–æ—à–∞–≥–æ–≤—ã–º–∏ –ø–æ–¥—Ö–æ–¥–∞–º–∏ –≤—Ä–æ–¥–µ consistency models. –ò—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, UCGM-{T,S}, –¥–æ—Å—Ç–∏–≥–∞–µ—Ç state-of-the-art —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ ImageNet 256x256, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, UCGM-S —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, —Å–Ω–∏–∂–∞—è FID –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Unifying Generative Models for Superior Performance', 'desc': 'This paper presents a new framework called Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}) that combines different generative modeling techniques, specifically diffusion and consistency models. By integrating these methods, the framework allows for more efficient training and sampling, leading to improved performance in generating images. The authors demonstrate that their approach achieves state-of-the-art results on the ImageNet dataset, significantly reducing the number of steps needed for high-quality image generation. Overall, UCGM-{T,S} streamlines the process of training and sampling generative models, making it easier to achieve better outcomes with fewer resources.'}, 'zh': {'title': 'Áªü‰∏ÄÁîüÊàêÊ®°ÂûãÔºåÊèêÂçáÁîüÊàêÊÄßËÉΩÔºÅ', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÁªü‰∏ÄÁöÑËøûÁª≠ÁîüÊàêÊ®°ÂûãÊ°ÜÊû∂ÔºåÊó®Âú®Êï¥ÂêàÂ§öÊ≠•ÂíåÂ∞ëÊ≠•ÁîüÊàêÊñπÊ≥ïÁöÑËÆ≠ÁªÉÂíåÈááÊ†∑„ÄÇÈÄöËøáÂºïÂÖ•Áªü‰∏ÄÁöÑËÆ≠ÁªÉÂíåÈááÊ†∑Âô®ÔºàUCGM-{T,S}ÔºâÔºåÊàë‰ª¨ÂÆûÁé∞‰∫ÜÊúÄÂÖàËøõÁöÑÁîüÊàêÊÄßËÉΩ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÂú®ImageNetÊï∞ÊçÆÈõÜ‰∏äÔºåUCGM-TËÉΩÂ§üÂú®20Ê≠•ÂÜÖÂ∞ÜÂ§öÊ≠•Ê®°ÂûãÁöÑFIDÈôç‰ΩéÂà∞1.30ÔºåËÄåÂ∞ëÊ≠•Ê®°ÂûãÂú®‰ªÖ2Ê≠•ÂÜÖËææÂà∞1.42ÁöÑFID„ÄÇÊ≠§Â§ñÔºå‰ΩøÁî®UCGM-SÂØπÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãËøõË°åÊîπËøõÔºåFID‰ªé250Ê≠•ÁöÑ1.26ÈôçËá≥‰ªÖ40Ê≠•ÁöÑ1.06„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06548', 'title': 'REFINE-AF: A Task-Agnostic Framework to Align Language Models via\n  Self-Generated Instructions using Reinforcement Learning from Automated\n  Feedback', 'url': 'https://huggingface.co/papers/2505.06548', 'abstract': 'Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.', 'score': 26, 'issue_id': 3722, 'pub_date': '2025-05-10', 'pub_date_card': {'ru': '10 –º–∞—è', 'en': 'May 10', 'zh': '5Êúà10Êó•'}, 'hash': 'db28335cad79db53', 'authors': ['Aniruddha Roy', 'Pretam Ray', 'Abhilash Nandy', 'Somak Aditya', 'Pawan Goyal'], 'affiliations': ['Indian Institute of Technology, Kharagpur'], 'pdf_title_img': 'assets/pdf/title_img/2505.06548.jpg', 'data': {'categories': ['#optimization', '#open_source', '#training', '#small_models', '#rl', '#dataset', '#data'], 'emoji': 'ü§ñ', 'ru': {'title': '–ú–∞–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ RL —É–ª—É—á—à–∞—é—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö –Ø–ú', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–∞–ª—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLaMA 2-7B, LLama 2-13B, Mistral 7B) –¥–ª—è –ø–æ–ª—É–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω—è—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫, —É–º–µ–Ω—å—à–∞—é—â–∏–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –∏ –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π. –û–Ω–∏ —Ç–∞–∫–∂–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—Ç –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (RL) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å RL –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–æ–¥—ã –≤ 63-66% –∑–∞–¥–∞—á.'}, 'en': {'title': 'Empowering LLMs with Cost-Effective Instruction Generation', 'desc': 'This paper discusses the challenges of creating instruction data for training Large Language Models (LLMs) in Natural Language Processing (NLP). It introduces a semi-automated framework that utilizes smaller open-source LLMs, like LLaMA and Mistral, to generate this data with less human effort and cost. The authors also incorporate a Reinforcement Learning (RL) training algorithm, which significantly improves the performance of the generated instruction datasets. The results show that this approach enhances task performance in a majority of cases compared to earlier methods.'}, 'zh': {'title': 'È´òÊïàÁîüÊàêÊåá‰ª§Êï∞ÊçÆÔºåÊèêÂçáLLMsÊÄßËÉΩ', 'desc': 'ËøôÁØáËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÊåá‰ª§È©±Âä®ÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®„ÄÇ‰ΩúËÄÖÊèêÂá∫‰∫Ü‰∏ÄÁßçÂçäËá™Âä®ÂåñÊ°ÜÊû∂ÔºåÂà©Áî®ÂºÄÊ∫êÁöÑÂ∞èÂûãLLMsÔºàÂ¶ÇLLaMA 2-7B„ÄÅLLaMA 2-13BÂíåMistral 7BÔºâÊù•ÁîüÊàêÊåá‰ª§Êï∞ÊçÆÈõÜÔºå‰ªéËÄåÂáèÂ∞ë‰∫∫Â∑•Âπ≤È¢ÑÂíåÊàêÊú¨„ÄÇÈÄöËøáÂºïÂÖ•Âü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑËÆ≠ÁªÉÁÆóÊ≥ïÔºåÁ†îÁ©∂Ë°®ÊòéËøôÁßçÊñπÊ≥ïÂú®63-66%ÁöÑ‰ªªÂä°‰∏≠ÊòæËëóÊèêÈ´ò‰∫ÜÊÄßËÉΩ„ÄÇËØ•Á†îÁ©∂‰∏∫ÁîüÊàêÂ§öÊ†∑ÂåñÁöÑÊåá‰ª§Êï∞ÊçÆÊèê‰æõ‰∫Ü‰∏ÄÁßçÊõ¥È´òÊïàÁöÑËß£ÂÜ≥ÊñπÊ°à„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07293', 'title': 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong\n  Pretraining Data Selection', 'url': 'https://huggingface.co/papers/2505.07293', 'abstract': "Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.", 'score': 17, 'issue_id': 3725, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '7403ae602b400fc4', 'authors': ['Kai Hua', 'Steven Wu', 'Ge Zhang', 'Ke Shen'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2505.07293.jpg', 'data': {'categories': ['#reasoning', '#training', '#dataset', '#benchmark', '#data', '#optimization', '#small_models'], 'emoji': 'üß†', 'ru': {'title': '–£–ª—É—á—à–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –Ø–ú –∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º —á–µ—Ä–µ–∑ —É–º–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç–æ–¥ AttentionInfluence –¥–ª—è –æ—Ç–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö, —É–ª—É—á—à–∞—é—â–∏—Ö —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ —Å–ª–æ–∂–Ω—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–±–æ–ª—å—à—É—é –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ–≤–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏–ª–∏ —Ä–∞–∑–º–µ—Ç–∫–µ. –ê–≤—Ç–æ—Ä—ã –ø—Ä–∏–º–µ–Ω–∏–ª–∏ AttentionInfluence –∫ –∫–æ—Ä–ø—É—Å—É SmolLM –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è 7B-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑–∞–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.'}, 'en': {'title': 'Enhancing Reasoning in LLMs with AttentionInfluence', 'desc': 'This paper introduces AttentionInfluence, a novel method for selecting reasoning-intensive pretraining data for large language models (LLMs) without requiring human or LLM supervision. The approach leverages attention heads, which are critical for in-context reasoning, to identify and mask retrieval heads, allowing a smaller pretrained model to effectively select relevant data. By applying this method to a 1.3B-parameter model and the SmolLM corpus, the authors demonstrate significant performance improvements on various reasoning benchmarks. The results suggest that this technique enables smaller models to enhance the performance of larger models, providing a scalable solution for data selection in reasoning tasks.'}, 'zh': {'title': 'Êó†ÁõëÁù£Êé®ÁêÜÊï∞ÊçÆÈÄâÊã©ÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÔºåÁ†îÁ©∂ËÄÖ‰ª¨Ë∂äÊù•Ë∂äÂÖ≥Ê≥®Êî∂ÈõÜÊé®ÁêÜÂØÜÈõÜÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ª•ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂ§çÊùÇÊé®ÁêÜËÉΩÂäõ„ÄÇ‰ª•ÂæÄÁöÑÊñπÊ≥ïÈÄöÂ∏∏‰æùËµñ‰∫éÁõëÁù£ÂàÜÁ±ªÂô®Êù•ËØÜÂà´Ëøô‰∫õÊï∞ÊçÆÔºåËøôÈúÄË¶Å‰∫∫Á±ªÊàñLLMsËøõË°åÊ†áÊ≥®ÔºåÂ∏∏Â∏∏ÂºïÂÖ•È¢ÜÂüüÁâπÂÆöÁöÑÂÅèËßÅ„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫AttentionInfluenceÁöÑÊñπÊ≥ïÔºåËøôÊòØ‰∏ÄÁßçÁÆÄÂçïËÄåÊúâÊïàÁöÑÊó†ÁõëÁù£ËÆ≠ÁªÉÊñπÊ≥ï„ÄÇÈÄöËøáÁÆÄÂçïÁöÑÊ≥®ÊÑèÂäõÂ§¥Â±èËîΩÊìç‰ΩúÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ï‰ΩøÂæó‰∏Ä‰∏™Â∞èÂûãÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§ü‰Ωú‰∏∫Âº∫Â§ßÁöÑÊï∞ÊçÆÈÄâÊã©Âô®Ôºå‰ªéËÄåÂú®Êé®ÁêÜ‰ªªÂä°‰∏≠ÂèñÂæóÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07818', 'title': 'DanceGRPO: Unleashing GRPO on Visual Generation', 'url': 'https://huggingface.co/papers/2505.07818', 'abstract': 'Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.', 'score': 16, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '023078250e0d651f', 'authors': ['Zeyue Xue', 'Jie Wu', 'Yu Gao', 'Fangyuan Kong', 'Lingting Zhu', 'Mengzhao Chen', 'Zhiheng Liu', 'Wei Liu', 'Qiushan Guo', 'Weilin Huang', 'Ping Luo'], 'affiliations': ['ByteDance Seed', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.07818.jpg', 'data': {'categories': ['#alignment', '#optimization', '#video', '#multimodal', '#rl', '#diffusion', '#benchmark', '#rlhf'], 'emoji': 'üé®', 'ru': {'title': 'DanceGRPO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –æ–±—É—á–µ–Ω–∏–∏ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç DanceGRPO - —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ Group Relative Policy Optimization (GRPO) –∫ –∑–∞–¥–∞—á–∞–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. DanceGRPO —Å–æ–≤–º–µ—Å—Ç–∏–º —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–¥–∏–≥–º–∞–º–∏, –∑–∞–¥–∞—á–∞–º–∏, —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –º–æ–¥–µ–ª—è–º–∏ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è. –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö. DanceGRPO —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–∏–¥–µ–æ –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–µ –ª—É—á—à–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è.'}, 'en': {'title': 'DanceGRPO: Unifying Reinforcement Learning for Visual Generation', 'desc': 'This paper presents DanceGRPO, a novel framework that enhances visual content generation by integrating Group Relative Policy Optimization (GRPO) with generative models like diffusion models and rectified flows. It addresses key challenges in reinforcement learning (RL) for visual generation, such as instability during training and compatibility with modern sampling methods. DanceGRPO is versatile, supporting multiple tasks including text-to-image and video generation, and utilizes various foundational and reward models to improve output quality. The framework shows significant performance improvements over existing methods, making it a promising solution for aligning generative models with human preferences in visual synthesis.'}, 'zh': {'title': 'DanceGRPOÔºöËßÜËßâÁîüÊàêÁöÑÁªü‰∏ÄÂº∫ÂåñÂ≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ËÆ∫Êñá‰ªãÁªç‰∫ÜDanceGRPOÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™Â∞ÜÁæ§‰ΩìÁõ∏ÂØπÁ≠ñÁï•‰ºòÂåñÔºàGRPOÔºâÂ∫îÁî®‰∫éËßÜËßâÁîüÊàêÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇÂÆÉËß£ÂÜ≥‰∫ÜÁé∞ÊúâÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâÁöÑÊñπÊ≥ïÂú®Áé∞‰ª£Â∏∏ÂæÆÂàÜÊñπÁ®ãÔºàODEÔºâÈááÊ†∑„ÄÅËÆ≠ÁªÉÁ®≥ÂÆöÊÄßÂíåËßÜÈ¢ëÁîüÊàêÈ™åËØÅÊñπÈù¢ÁöÑÂ±ÄÈôêÊÄß„ÄÇDanceGRPOËÉΩÂ§üÂú®Êâ©Êï£Ê®°ÂûãÂíå‰øÆÊ≠£ÊµÅÁ≠âÂ§öÁßçÁîüÊàêËåÉÂºè‰∏≠Êó†ÁºùÈÄÇÂ∫îÔºåÂπ∂Âú®ÊñáÊú¨Âà∞ÂõæÂÉè„ÄÅÊñáÊú¨Âà∞ËßÜÈ¢ëÂíåÂõæÂÉèÂà∞ËßÜÈ¢ëÁ≠â‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫ÊòæËëóÁöÑÊÄßËÉΩÊèêÂçá„ÄÇËØ•Ê°ÜÊû∂Âú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠Ë∂ÖË∂ä‰∫ÜÂü∫Á∫øÔºåÂ±ïÁ§∫‰∫ÜÂú®ËßÜËßâÁîüÊàê‰ªªÂä°‰∏≠ÁªìÂêàÂº∫ÂåñÂ≠¶‰π†‰∏é‰∫∫Á±ªÂèçÈ¶àÁöÑÊΩúÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07263', 'title': 'Skywork-VL Reward: An Effective Reward Model for Multimodal\n  Understanding and Reasoning', 'url': 'https://huggingface.co/papers/2505.07263', 'abstract': 'We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.', 'score': 15, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9735af402c0df35f', 'authors': ['Xiaokun Wang', 'Chris', 'Jiangbo Pei', 'Wei Shen', 'Yi Peng', 'Yunzhuo Hao', 'Weijie Qiu', 'Ai Jian', 'Tianyidan Xie', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07263.jpg', 'data': {'categories': ['#optimization', '#multimodal', '#benchmark', '#alignment', '#open_source', '#training', '#architecture', '#dataset'], 'emoji': 'üåü', 'ru': {'title': '–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ò–ò-—Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ Skywork-VL Reward - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è. –û–Ω–∏ —Å–æ–∑–¥–∞–ª–∏ –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ Qwen2.5-VL-7B-Instruct —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≥–æ–ª–æ–≤—ã –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –∏ –º–Ω–æ–≥–æ—ç—Ç–∞–ø–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π. Skywork-VL Reward –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é Mixed Preference Optimization.'}, 'en': {'title': 'Skywork-VL Reward: Advancing Multimodal Understanding and Reasoning', 'desc': 'The paper introduces Skywork-VL Reward, a novel multimodal reward model designed to enhance understanding and reasoning across different types of data, such as text and images. It utilizes a large-scale dataset that captures diverse tasks, with feedback gathered from both traditional vision-language models and more sophisticated reasoning models. The architecture is built on Qwen2.5-VL-7B-Instruct, featuring a reward head and employing multi-stage fine-tuning with pairwise ranking loss to optimize performance. Experimental results demonstrate that this model not only excels in multimodal tasks but also improves training for Mixed Preference Optimization, marking a significant step forward in creating effective reward models for multimodal alignment.'}, 'zh': {'title': 'Skywork-VL RewardÔºöÂ§öÊ®°ÊÄÅÂØπÈΩêÁöÑÁ™ÅÁ†¥ÊÄßËøõÂ±ï', 'desc': 'Êàë‰ª¨ÊèêÂá∫‰∫ÜSkywork-VL RewardÔºåËøôÊòØ‰∏ÄÁßçÂ§öÊ®°ÊÄÅÂ•ñÂä±Ê®°ÂûãÔºåÊó®Âú®‰∏∫Â§öÊ®°ÊÄÅÁêÜËß£ÂíåÊé®ÁêÜ‰ªªÂä°Êèê‰æõÂ•ñÂä±‰ø°Âè∑„ÄÇÊàë‰ª¨ÁöÑÊäÄÊúØÊñπÊ≥ïÂåÖÊã¨‰∏§‰∏™ÂÖ≥ÈîÆÁªÑÊàêÈÉ®ÂàÜÔºöÈ¶ñÂÖàÔºåÊàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏Ä‰∏™Â§ßËßÑÊ®°ÁöÑÂ§öÊ®°ÊÄÅÂÅèÂ•ΩÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫ÜÂπøÊ≥õÁöÑ‰ªªÂä°ÂíåÂú∫ÊôØÔºåÊï∞ÊçÆÊù•Ëá™Ê†áÂáÜËßÜËßâ-ËØ≠Ë®ÄÊ®°ÂûãÂíåÂÖàËøõÁöÑËßÜËßâ-ËØ≠Ë®ÄÊé®ÁêÜÊ®°Âûã„ÄÇÂÖ∂Ê¨°ÔºåÊàë‰ª¨ËÆæËÆ°‰∫Ü‰∏ÄÁßçÂü∫‰∫éQwen2.5-VL-7B-InstructÁöÑÂ•ñÂä±Ê®°ÂûãÊû∂ÊûÑÔºåÈõÜÊàê‰∫ÜÂ•ñÂä±Â§¥ÔºåÂπ∂Âú®ÊàêÂØπÂÅèÂ•ΩÊï∞ÊçÆ‰∏äÂ∫îÁî®‰∫ÜÂ§öÈò∂ÊÆµÂæÆË∞É„ÄÇÂÆûÈ™åËØÑ‰º∞Ë°®ÊòéÔºåSkywork-VL RewardÂú®Â§öÊ®°ÊÄÅVL-RewardBench‰∏äËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûúÔºåÂπ∂Âú®ÊñáÊú¨Â•ñÂä±Âü∫ÂáÜ‰∏äË°®Áé∞Âá∫Á´û‰∫âÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.03733', 'title': 'WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch', 'url': 'https://huggingface.co/papers/2505.03733', 'abstract': "LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\\%, surpassing the performance of the best proprietary model.", 'score': 15, 'issue_id': 3722, 'pub_date': '2025-05-06', 'pub_date_card': {'ru': '6 –º–∞—è', 'en': 'May 6', 'zh': '5Êúà6Êó•'}, 'hash': '1d5e56d00ea8d485', 'authors': ['Zimu Lu', 'Yunqiao Yang', 'Houxing Ren', 'Haotian Hou', 'Han Xiao', 'Ke Wang', 'Weikang Shi', 'Aojun Zhou', 'Mingjie Zhan', 'Hongsheng Li'], 'affiliations': ['Multimedia Laboratory (MMLab), The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.03733.jpg', 'data': {'categories': ['#optimization', '#games', '#benchmark', '#training', '#agents', '#dataset'], 'emoji': 'üåê', 'ru': {'title': 'WebGen-Bench: –ù–æ–≤—ã–π —Ä—É–±–µ–∂ –≤ –æ—Ü–µ–Ω–∫–µ LLM-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏', 'desc': 'WebGen-Bench - —ç—Ç–æ –Ω–æ–≤—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ LLM-–∞–≥–µ–Ω—Ç–æ–≤ —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–µ –≤–µ–±-—Å–∞–π—Ç—ã —Å –Ω—É–ª—è. –û–Ω –≤–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–µ –ø–æ—á—Ç–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Ç–∏–ø—ã –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π. –ö–∞—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö GPT-4o –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –≤—Ä—É—á–Ω—É—é. –õ—É—á—à–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è - Bolt.diy —Å DeepSeek-R1 - –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —Ç–æ–ª—å–∫–æ 27,8% —Ç–æ—á–Ω–æ—Å—Ç–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –±–µ–Ω—á–º–∞—Ä–∫–∞.'}, 'en': {'title': 'Benchmarking LLMs for Website Code Generation', 'desc': 'This paper presents WebGen-Bench, a benchmark for evaluating LLM-based agents in generating multi-file website codebases. It includes a variety of instructions for website creation, developed by both human annotators and GPT-4o, covering a wide range of web application types. The quality of the generated websites is assessed using 647 test cases that check if the websites function as expected, with a web-navigation agent automating the testing process. The results show that even the best-performing code-agent framework, Bolt.diy with DeepSeek-R1, only achieves 27.8% accuracy, indicating the complexity of the task and the need for improved models.'}, 'zh': {'title': 'ËØÑ‰º∞LLM‰ª£ÁêÜÁîüÊàêÁΩëÁ´ô‰ª£Á†ÅÁöÑÊåëÊàò', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂü∫ÂáÜÊµãËØïWebGen-BenchÔºåÊó®Âú®ËØÑ‰º∞Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÁöÑ‰ª£ÁêÜÂú®‰ªéÈõ∂ÂºÄÂßãÂàõÂª∫Â§öÊñá‰ª∂ÁΩëÁ´ô‰ª£Á†ÅÂ∫ìÁöÑËÉΩÂäõ„ÄÇËØ•Âü∫ÂáÜÂåÖÂê´Â§öÊ†∑ÂåñÁöÑÁΩëÁ´ôÁîüÊàêÊåá‰ª§ÔºåÊ∂µÁõñ‰∫Ü‰∏âÂ§ßÁ±ªÂíåÂçÅ‰∏âÂ∞èÁ±ªÔºåÂá†‰πéÂåÖÊã¨ÊâÄÊúâÈáçË¶ÅÁ±ªÂûãÁöÑWebÂ∫îÁî®Á®ãÂ∫è„ÄÇ‰∏∫‰∫ÜËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑË¥®ÈáèÔºå‰ΩøÁî®GPT-4oÁîüÊàêÈíàÂØπÊØè‰∏™ÂäüËÉΩÁöÑÊµãËØïÁî®‰æãÔºåÂπ∂ÊâãÂä®ËøáÊª§ÂíåË∞ÉÊï¥ÔºåÊúÄÁªàÂΩ¢Êàê647‰∏™ÊµãËØïÁî®‰æã„ÄÇÈÄöËøáÂº∫Â§ßÁöÑÁΩëÈ°µÂØºËà™‰ª£ÁêÜËá™Âä®ÊâßË°åÊµãËØïÔºåËØÑ‰º∞ÁîüÊàêÁΩëÁ´ôÁöÑÂìçÂ∫îÊòØÂê¶Á¨¶ÂêàÈ¢ÑÊúüÁªìÊûúÔºåÁªìÊûúÊòæÁ§∫ÊúÄ‰Ω≥Ê®°ÂûãÁªÑÂêàÁöÑÂáÜÁ°ÆÁéá‰ªÖ‰∏∫27.8%ÔºåÊòæÁ§∫Âá∫Âü∫ÂáÜÁöÑÊåëÊàòÊÄß„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07796', 'title': 'Learning Dynamics in Continual Pre-Training for Large Language Models', 'url': 'https://huggingface.co/papers/2505.07796', 'abstract': 'Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.', 'score': 12, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'c63d617be0b4d13a', 'authors': ['Xingjin Wang', 'Howe Tissue', 'Lu Wang', 'Linjing Li', 'Daniel Dajun Zeng'], 'affiliations': ['Ritzz-AI', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07796.jpg', 'data': {'categories': ['#optimization', '#transfer_learning', '#training'], 'emoji': 'üìà', 'ru': {'title': '–†–∞—Å–∫—Ä—ã—Ç–∏–µ —Å–µ–∫—Ä–µ—Ç–æ–≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–°—Ç–∞—Ç—å—è –∏—Å—Å–ª–µ–¥—É–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ (CPT) –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –ê–≤—Ç–æ—Ä—ã –Ω–∞–±–ª—é–¥–∞—é—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–±—â–∏—Ö –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á–∞—Ö –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è. –û–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∑–∞–∫–æ–Ω CPT, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π —ç—Ñ—Ñ–µ–∫—Ç—ã —Å–º–µ—â–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–µ—Ä–∏ –Ω–∞ –ª—é–±–æ–º —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–π CPT.'}, 'en': {'title': 'Unlocking the Dynamics of Continual Pre-Training', 'desc': "This paper investigates the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on how performance in general and specific domains changes during training. The authors analyze the CPT loss curve, revealing that it represents a transition between different performance states influenced by distribution shifts and learning rate adjustments. They propose a scaling law that predicts loss across various training steps and learning rate schedules, providing insights into key factors like peak learning rate and replay ratio. The findings are validated through extensive experiments, showing the law's applicability across different datasets and training configurations."}, 'zh': {'title': 'ÊåÅÁª≠È¢ÑËÆ≠ÁªÉÁöÑÂä®ÊÄÅ‰∏é‰ºòÂåñÊ≥ïÂàô', 'desc': 'ÊåÅÁª≠È¢ÑËÆ≠ÁªÉÔºàCPTÔºâÊòØ‰∏ÄÁßçÂ∞ÜÂº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÂ∫îÁî®‰∫éÁâπÂÆö‰∏ãÊ∏∏‰ªªÂä°ÁöÑÊúâÊïàÊñπÊ≥ï„ÄÇÊú¨ÊñáÊé¢ËÆ®‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®CPTËøáÁ®ã‰∏≠ÁöÑÂ≠¶‰π†Âä®ÊÄÅÔºåÁâπÂà´ÂÖ≥Ê≥®Âú®ÊØè‰∏™ËÆ≠ÁªÉÊ≠•È™§‰∏≠ÔºåÈÄöÁî®ÊÄßËÉΩÂíå‰∏ãÊ∏∏È¢ÜÂüüÊÄßËÉΩÁöÑÊºîÂèò„ÄÇÊàë‰ª¨ËßÇÂØüÂà∞CPTÊçüÂ§±Êõ≤Á∫øÊú¨Ë¥®‰∏äÊèèËø∞‰∫Ü‰ªé‰∏Ä‰∏™Êõ≤Á∫øÂà∞Âè¶‰∏Ä‰∏™ÈöêËóèÊõ≤Á∫øÁöÑËøáÊ∏°ÔºåÂπ∂ÈÄöËøáËß£ËÄ¶ÂàÜÂ∏ÉÂèòÂåñÂíåÂ≠¶‰π†ÁéáÈÄÄÁÅ´ÁöÑÂΩ±ÂìçÊù•ËøõË°åÊèèËø∞„ÄÇÊàë‰ª¨Êé®ÂØºÂá∫‰∫Ü‰∏ÄÁßçCPTÁº©ÊîæÊ≥ïÂàôÔºåÁªìÂêà‰∫ÜËøô‰∏§‰∏™Âõ†Á¥†Ôºå‰ΩøÂæóËÉΩÂ§üÈ¢ÑÊµãÂú®‰ªª‰ΩïÔºàÊåÅÁª≠ÔºâËÆ≠ÁªÉÊ≠•È™§ÂíåÂ≠¶‰π†ÁéáË∞ÉÂ∫¶‰∏ãÁöÑÊçüÂ§±„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07596', 'title': 'Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent', 'url': 'https://huggingface.co/papers/2505.07596', 'abstract': 'Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.', 'score': 10, 'issue_id': 3723, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '10063104a79da512', 'authors': ['Ziyang Huang', 'Xiaowei Yuan', 'Yiming Ju', 'Jun Zhao', 'Kang Liu'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'Institute of Automation, Chinese Academy of Sciences', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2505.07596.jpg', 'data': {'categories': ['#reasoning', '#rag', '#agents', '#optimization', '#hallucinations', '#rl', '#training'], 'emoji': 'üß†', 'ru': {'title': '–£–º–Ω—ã–π –ø–æ–∏—Å–∫: –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å, –∞ –∫–æ–≥–¥–∞ –¥–æ–≤–µ—Ä–∏—Ç—å—Å—è —Å–µ–±–µ', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —É–ª—É—á—à–µ–Ω–∏—é —Ä–∞–±–æ—Ç—ã –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ retrieval-augmented generation (RAG). –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –∞–≥–µ–Ω—Ç–∞ IKEA, –∫–æ—Ç–æ—Ä—ã–π —Å–ø–æ—Å–æ–±–µ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≥—Ä–∞–Ω–∏—Ü—ã —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è –≥—Ä–∞–Ω–∏—Ü—ã –∑–Ω–∞–Ω–∏–π, –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ IKEA –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –≤–Ω–µ—à–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ö–æ—Ä–æ—à—É—é –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.'}, 'en': {'title': 'Optimizing Knowledge Use in Language Models with IKEA', 'desc': 'This paper presents the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), a novel approach to enhance the performance of Large Language Models (LLMs) by optimizing their retrieval processes. IKEA intelligently determines when to use its internal knowledge versus when to perform external searches, reducing unnecessary retrievals and improving inference speed. The model employs a unique reward function that encourages effective use of internal knowledge while still allowing for external retrieval when needed. Evaluations show that IKEA not only outperforms existing methods but also generalizes well across various knowledge reasoning tasks.'}, 'zh': {'title': 'Êô∫ËÉΩÊ£ÄÁ¥¢Ôºå‰ºòÂåñÁü•ËØÜÂà©Áî®', 'desc': 'Êú¨Êñá‰ªãÁªç‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂº∫ÂåñÂ≠¶‰π†Ê®°ÂûãÔºåÂêç‰∏∫IKEAÔºåÊó®Âú®ÊèêÈ´òÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÊ£ÄÁ¥¢ËÉΩÂäõ„ÄÇIKEAËÉΩÂ§üËØÜÂà´Ëá™Ë∫´Áü•ËØÜÁöÑËæπÁïåÔºåÂπ∂‰ºòÂÖà‰ΩøÁî®ÂÜÖÈÉ®Áü•ËØÜÔºåÂè™ÊúâÂú®ÂÜÖÈÉ®Áü•ËØÜ‰∏çË∂≥Êó∂Êâç‰ºöËøõË°åÂ§ñÈÉ®Ê£ÄÁ¥¢„ÄÇÈÄöËøáÂºïÂÖ•‰∏ÄÁßçÊñ∞ÁöÑÂ•ñÂä±ÂáΩÊï∞ÂíåËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåIKEAËÉΩÂ§üÊúâÊïàÂáèÂ∞ëÂÜó‰ΩôÊ£ÄÁ¥¢ÔºåÊèêÈ´òÂõûÁ≠îÁöÑÂáÜÁ°ÆÊÄß„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåIKEAÂú®Â§ö‰∏™Áü•ËØÜÊé®ÁêÜ‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇÔºåÊòæËëóÈôç‰Ωé‰∫ÜÊ£ÄÁ¥¢È¢ëÁéáÔºåÂπ∂Â±ïÁé∞Âá∫Âº∫Â§ßÁöÑÊ≥õÂåñËÉΩÂäõ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06176', 'title': "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills", 'url': 'https://huggingface.co/papers/2505.06176', 'abstract': 'Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.', 'score': 7, 'issue_id': 3733, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': '884e34691df4b88b', 'authors': ['Niladri Shekhar Dutt', 'Duygu Ceylan', 'Niloy J. Mitra'], 'affiliations': ['Adobe Research, UK', 'University College London, Adobe Research, UK', 'University College London, UK'], 'pdf_title_img': 'assets/pdf/title_img/2505.06176.jpg', 'data': {'categories': ['#synthetic', '#optimization', '#training', '#dataset', '#data', '#multimodal', '#interpretability', '#cv'], 'emoji': 'üñºÔ∏è', 'ru': {'title': '–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ—Ç—É—à—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π: MLLM –Ω–∞ —Å—Ç—Ä–∞–∂–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—Ç—É—à–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (MLLM). –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—ã—Ä—ã–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏, –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –∏ —Ä–µ–∞–ª–∏–∑–æ–≤—ã–≤–∞—Ç—å –∏—Ö —Å –ø–æ–º–æ—â—å—é –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. MLLM —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≥–æ–ª–æ–≤–æ–ª–æ–º–∫–∞—Ö –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ –∑–∞—Ç–µ–º –Ω–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è–º–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –ø–æ–Ω—è—Ç–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ–ø–µ—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.'}, 'en': {'title': 'Empowering Photo Retouching with Intelligent Editing Guidance', 'desc': 'This paper explores the use of a multimodal large language model (MLLM) for retouching raw photographs by critiquing and suggesting edits based on procedural image operations. The authors train the MLLM to understand image processing through visual puzzles, enabling it to plan and propose edit sequences that are user-friendly and maintain the integrity of the original image. By synthesizing a reasoning dataset from expert-edited photos, the model learns to generate understandable retouching operations that preserve object details and resolution. The results demonstrate that this approach offers better explainability and identity preservation compared to traditional generative editing methods.'}, 'zh': {'title': 'Âà©Áî®Â§öÊ®°ÊÄÅÊ®°ÂûãÊèêÂçáÁÖßÁâá‰øÆÈ•∞Ë¥®Èáè', 'desc': 'Êú¨ËÆ∫ÊñáÊé¢ËÆ®‰∫ÜÂ¶Ç‰ΩïÂà©Áî®Â§öÊ®°ÊÄÅÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàMLLMÔºâÊù•ÊîπËøõÂéüÂßãÁÖßÁâáÁöÑ‰øÆÈ•∞ËøáÁ®ã„ÄÇÊàë‰ª¨ËÆ≠ÁªÉMLLMÁêÜËß£ÂõæÂÉèÂ§ÑÁêÜÊìç‰ΩúÔºåÂπ∂ÈÄöËøáËß£ÂÜ≥ËßÜËßâÈöæÈ¢òÊù•Â¢ûÂº∫ÂÖ∂Êìç‰ΩúÊÑèËØÜ„ÄÇËØ•Ê®°ÂûãËÉΩÂ§üËßÑÂàíÂíåÂª∫ËÆÆÁºñËæëÂ∫èÂàóÔºåÁ°Æ‰øù‰øÆÈ•∞Êìç‰ΩúÂØπÁî®Êà∑ÂèØÁêÜËß£ÔºåÂπ∂‰øùÁïôÂØπË±°ÁªÜËäÇÂíåÂàÜËæ®Áéá„ÄÇÊàë‰ª¨ÁöÑÂÆûÈ™åÁªìÊûúË°®ÊòéÔºå‰∏éÁé∞ÊúâÁöÑÁîüÊàêÂíåÁ®ãÂ∫èÂåñÊñπÊ≥ïÁõ∏ÊØîÔºåËØ•ÊñπÊ≥ïÂú®ÂèØËß£ÈáäÊÄßÂíåË∫´‰ªΩ‰øùÁïôÊñπÈù¢ÂÖ∑ÊúâÊòéÊòæ‰ºòÂäø„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07819', 'title': 'H^{3}DP: Triply-Hierarchical Diffusion Policy for Visuomotor\n  Learning', 'url': 'https://huggingface.co/papers/2505.07819', 'abstract': 'Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce Triply-Hierarchical Diffusion Policy~(H^{\\mathbf{3}DP}), a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H^{3}DP contains 3 levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H^{3}DP yields a +27.5% average relative improvement over baselines across 44 simulation tasks and achieves superior performance in 4 challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '9285a87dc24d7d07', 'authors': ['Yiyang Lu', 'Yufeng Tian', 'Zhecheng Yuan', 'Xianbang Wang', 'Pu Hua', 'Zhengrong Xue', 'Huazhe Xu'], 'affiliations': ['Harbin Institute of Technology', 'Shanghai AI Lab', 'Shanghai Qi Zhi Institute', 'Tsinghua University IIIS'], 'pdf_title_img': 'assets/pdf/title_img/2505.07819.jpg', 'data': {'categories': ['#diffusion', '#agents', '#robotics', '#optimization'], 'emoji': 'ü§ñ', 'ru': {'title': '–ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ä–æ–±–æ—Ç–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –æ–±—É—á–µ–Ω–∏—é –≤–∏–∑—É–æ–º–æ—Ç–æ—Ä–Ω–æ–π –ø–æ–ª–∏—Ç–∏–∫–∏ –¥–ª—è —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ - –¢—Ä–∏–µ–¥–∏–Ω—É—é –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –î–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—É—é –ü–æ–ª–∏—Ç–∏–∫—É (H^3DP). –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é –∏–µ—Ä–∞—Ä—Ö–∏—é: –ø–æ—Å–ª–æ–π–Ω—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –≥–ª—É–±–∏–Ω—ã, –º–Ω–æ–≥–æ–º–∞—Å—à—Ç–∞–±–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –¥–∏—Ñ—Ñ—É–∑–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–µ–π—Å—Ç–≤–∏–π. H^3DP —É—Å–∏–ª–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –º–µ–∂–¥—É –≤–∏–∑—É–∞–ª—å–Ω—ã–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ–º –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –¥–µ–π—Å—Ç–≤–∏–π. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –≤ —Å–∏–º—É–ª—è—Ü–∏–∏, —Ç–∞–∫ –∏ –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏.'}, 'en': {'title': 'Enhancing Robotic Manipulation with Hierarchical Visual-Action Integration', 'desc': 'This paper presents a new framework called Triply-Hierarchical Diffusion Policy (H^{3}DP) for improving visuomotor policy learning in robotic manipulation. It addresses the gap in existing methods by integrating visual perception with action prediction through a structured hierarchical approach. The framework consists of three levels: organizing RGB-D data by depth, creating multi-scale visual representations, and using a diffusion process to generate actions that correspond to visual features. Experimental results show that H^{3}DP significantly outperforms previous methods, achieving a 27.5% improvement in simulation tasks and excelling in complex real-world scenarios.'}, 'zh': {'title': 'Â¢ûÂº∫ËßÜËßâ‰∏éÂä®‰ΩúÁîüÊàêÁöÑ‰∏âÂ±ÇÊ¨°Â≠¶‰π†Ê°ÜÊû∂', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑËßÜËßâËøêÂä®Á≠ñÁï•Â≠¶‰π†Ê°ÜÊû∂ÔºåÁß∞‰∏∫‰∏âÂ±ÇÊ¨°Êâ©Êï£Á≠ñÁï•ÔºàH^{\textbf{3}DPÔºâ„ÄÇËØ•Ê°ÜÊû∂ÈÄöËøáÂºïÂÖ•Â±ÇÊ¨°ÁªìÊûÑÔºåÂ¢ûÂº∫‰∫ÜËßÜËßâÁâπÂæÅ‰∏éÂä®‰ΩúÁîüÊàê‰πãÈó¥ÁöÑÁªìÂêà„ÄÇH^{3}DPÂåÖÂê´‰∏â‰∏™Â±ÇÊ¨°ÔºöÂü∫‰∫éÊ∑±Â∫¶‰ø°ÊÅØÁöÑËæìÂÖ•ÂàÜÂ±Ç„ÄÅÂ§öÂ∞∫Â∫¶ËßÜËßâË°®Á§∫ÂíåÂ±ÇÊ¨°Êù°‰ª∂Êâ©Êï£ËøáÁ®ã„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåH^{3}DPÂú®44‰∏™‰ªøÁúü‰ªªÂä°‰∏≠Áõ∏ËæÉ‰∫éÂü∫Á∫øÊñπÊ≥ïÂπ≥ÂùáÊèêÈ´ò‰∫Ü27.5%ÁöÑÊÄßËÉΩÔºåÂπ∂Âú®Âõõ‰∏™Â§çÊùÇÁöÑÂèåÊâãÁúüÂÆû‰∏ñÁïåÊìç‰Ωú‰ªªÂä°‰∏≠Ë°®Áé∞‰ºòÂºÇ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07260', 'title': 'UMoE: Unifying Attention and FFN with Shared Experts', 'url': 'https://huggingface.co/papers/2505.07260', 'abstract': 'Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.', 'score': 5, 'issue_id': 3729, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '6a820b0cd1c07ac4', 'authors': ['Yuanhang Yang', 'Chaozheng Wang', 'Jing Li'], 'affiliations': ['Hong Kong Polytechnic University, Hong Kong, China', 'Institute of Science Tokyo, Tokyo, Japan', 'The Chinese University of Hong Kong, Hong Kong, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.07260.jpg', 'data': {'categories': ['#architecture', '#training', '#optimization'], 'emoji': 'üß†', 'ru': {'title': '–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è MoE –≤ Transformer: –ø–æ–≤—ã—à–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (Sparse Mixture of Experts, MoE) –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π Transformer. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∑–∞–π–Ω MoE –¥–ª—è —Å–ª–æ–µ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É—è –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è. –ù–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –Ω–∞–∑–≤–∞–Ω–Ω–∞—è UMoE, –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞ —Å—á–µ—Ç MoE –≤ —Å–ª–æ—è—Ö –≤–Ω–∏–º–∞–Ω–∏—è. UMoE —Ç–∞–∫–∂–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–¥–µ–ª—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–º–∏ —Å–ª–æ—è–º–∏.'}, 'en': {'title': 'Unifying MoE for Enhanced Transformer Performance', 'desc': 'This paper presents a new approach called UMoE that combines Sparse Mixture of Experts (MoE) architectures with attention layers in Transformer models. Traditionally, MoE has been used in feed-forward network (FFN) layers, but this study shows how to effectively apply it to attention layers as well. The authors reformulate the attention mechanism to reveal similarities with FFN structures, allowing for better integration and performance. UMoE not only improves the efficiency of parameter sharing between FFN and attention components but also enhances overall model performance compared to previous methods.'}, 'zh': {'title': 'Áªü‰∏ÄÊ≥®ÊÑèÂäõ‰∏éÂâçÈ¶àÁΩëÁªúÁöÑÁ®ÄÁñè‰∏ìÂÆ∂Êû∂ÊûÑ', 'desc': 'Á®ÄÁñè‰∏ìÂÆ∂Ê∑∑ÂêàÔºàMoEÔºâÊû∂ÊûÑÊòØ‰∏ÄÁßçÊúâÂâçÊôØÁöÑÊñπÊ≥ïÔºåÁî®‰∫éÊâ©Â±ïTransformerÊ®°Âûã„ÄÇËôΩÁÑ∂Êó©ÊúüÁöÑÁ†îÁ©∂‰∏ªË¶ÅÂ∞ÜMoEÂ∫îÁî®‰∫éÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÂ±ÇÔºå‰ΩÜÊúÄËøëÁöÑÁ†îÁ©∂ÂºÄÂßãÊé¢Á¥¢Â∞ÜMoEÊâ©Â±ïÂà∞Ê≥®ÊÑèÂäõÂ±ÇÔºå‰ª•ÊèêÈ´òÊ®°ÂûãÊÄßËÉΩ„ÄÇÁé∞ÊúâÁöÑÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑMoEÂ±ÇÈúÄË¶Å‰∏ìÈó®ÁöÑÂÆûÁé∞ÔºåÂπ∂‰∏î‰∏éÂü∫‰∫éFFNÁöÑÂ±ÇÁõ∏ÊØîÔºåÊÄßËÉΩ‰∏çÂ∞ΩÂ¶Ç‰∫∫ÊÑè„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞È¢ñÁöÑÊ≥®ÊÑèÂäõÊú∫Âà∂ÈáçÊûÑÔºåÁªü‰∏Ä‰∫ÜÊ≥®ÊÑèÂäõÂ±ÇÂíåFFNÂ±Ç‰∏≠ÁöÑMoEËÆæËÆ°ÔºåÊèêÂá∫ÁöÑUMoEÊû∂ÊûÑÈÄöËøáÂü∫‰∫éÊ≥®ÊÑèÂäõÁöÑMoEÂ±ÇÂÆûÁé∞‰∫ÜÊõ¥‰ºòÁöÑÊÄßËÉΩÔºåÂêåÊó∂ÂÆûÁé∞‰∫ÜFFNÂíåÊ≥®ÊÑèÂäõÁªÑ‰ª∂‰πãÈó¥ÁöÑÈ´òÊïàÂèÇÊï∞ÂÖ±‰∫´„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.00612', 'title': 'Position: AI Competitions Provide the Gold Standard for Empirical Rigor\n  in GenAI Evaluation', 'url': 'https://huggingface.co/papers/2505.00612', 'abstract': 'In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.', 'score': 5, 'issue_id': 3733, 'pub_date': '2025-05-01', 'pub_date_card': {'ru': '1 –º–∞—è', 'en': 'May 1', 'zh': '5Êúà1Êó•'}, 'hash': '98b0a79b86c5cd15', 'authors': ['D. Sculley', 'Will Cukierski', 'Phil Culliton', 'Sohier Dane', 'Maggie Demkin', 'Ryan Holbrook', 'Addison Howard', 'Paul Mooney', 'Walter Reade', 'Megan Risdal', 'Nate Keating'], 'affiliations': ['Kaggle, Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.00612.jpg', 'data': {'categories': ['#leakage', '#benchmark', '#evaluation'], 'emoji': 'üèÜ', 'ru': {'title': '–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò - –∫–ª—é—á –∫ –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç –∫—Ä–∏–∑–∏—Å –≤ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò, —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –æ—Ç–º–µ—á–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —ç—Ç–∞–ª–æ–Ω–Ω–æ–π –∏—Å—Ç–∏–Ω—ã –∏ —Å–∏–ª—å–Ω—ã–º–∏ –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö. –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞–º —É—Ç–µ—á–∫–∏ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞–∫ –Ω–∞–∏–±–æ–ª–µ–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–º –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò. –°—Ç–∞—Ç—å—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –ø–æ –ò–ò –∫–∞–∫ –∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏ –≤ –æ—Ü–µ–Ω–∫–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ò–ò.'}, 'en': {'title': 'Rethinking Evaluation: AI Competitions as the Gold Standard for GenAI', 'desc': 'This paper discusses the challenges of evaluating Generative AI (GenAI) models, highlighting that traditional machine learning evaluation methods are inadequate. It points out that GenAI models have vast input and output possibilities, lack clear ground truth, and are influenced by previous outputs, complicating their assessment. The authors emphasize that issues like leakage and contamination are critical hurdles in GenAI evaluations. They propose that AI Competitions offer effective strategies to mitigate these problems and should be recognized as a benchmark for rigorous evaluation in the field.'}, 'zh': {'title': 'ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÊñ∞Ê†áÂáÜÔºö‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÁöÑ‰ª∑ÂÄº', 'desc': 'Âú®ËøôÁØáËÆ∫Êñá‰∏≠ÔºåÊàë‰ª¨ËßÇÂØüÂà∞ÁîüÊàêÊÄß‰∫∫Â∑•Êô∫ËÉΩÁöÑÂÆûËØÅËØÑ‰º∞Ê≠£Èù¢‰∏¥Âç±Êú∫ÔºåÂõ†‰∏∫‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ËØÑ‰º∞ÂíåÂü∫ÂáÜÁ≠ñÁï•Êó†Ê≥ïÊª°Ë∂≥Áé∞‰ª£ÁîüÊàêÊÄßAIÊ®°ÂûãÂíåÁ≥ªÁªüÁöÑËØÑ‰º∞ÈúÄÊ±Ç„ÄÇËøô‰∫õÊ®°ÂûãÈÄöÂ∏∏ÂÖ∑ÊúâÂá†‰πéÊó†ÈôêÁöÑËæìÂÖ•ÂíåËæìÂá∫Á©∫Èó¥ÔºåÁº∫‰πèÊòéÁ°ÆÁöÑÁúüÂÆûÁõÆÊ†áÔºåÂπ∂‰∏îÂú®È¢ÑÊµãÊó∂Âº∫ÁÉà‰æùËµñ‰∫é‰πãÂâçÊ®°ÂûãËæìÂá∫ÁöÑ‰∏ä‰∏ãÊñá„ÄÇÊ≠§Â§ñÔºåËÆ∫ÊñáÊåáÂá∫ÔºåÊ≥ÑÊºèÂíåÊ±°ÊüìÈóÆÈ¢òÊòØÁîüÊàêÊÄßAIËØÑ‰º∞‰∏≠ÊúÄÈáçË¶Å‰∏îÊúÄÈöæËß£ÂÜ≥ÁöÑÊåëÊàò„ÄÇÊàë‰ª¨ËÆ§‰∏∫Ôºå‰∫∫Â∑•Êô∫ËÉΩÁ´ûËµõÈ¢ÜÂüüÂ∑≤ÁªèÂèëÂ±ïÂá∫ÊúâÊïàÁöÑÊé™ÊñΩÊù•Â∫îÂØπÊ≥ÑÊºèÈóÆÈ¢òÔºåÂõ†Ê≠§Â∫îÂ∞ÜÂÖ∂ËßÜ‰∏∫ÁîüÊàêÊÄßAIËØÑ‰º∞ÁöÑÈªÑÈáëÊ†áÂáÜ„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07812', 'title': 'Continuous Visual Autoregressive Generation via Score Maximization', 'url': 'https://huggingface.co/papers/2505.07812', 'abstract': 'Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.', 'score': 3, 'issue_id': 3730, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': '5635f18df39cf275', 'authors': ['Chenze Shao', 'Fandong Meng', 'Jie Zhou'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2505.07812.jpg', 'data': {'categories': ['#optimization', '#data', '#diffusion', '#training', '#architecture'], 'emoji': 'üîÑ', 'ru': {'title': '–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –≤–∏–∑—É–∞–ª—å–Ω–æ–º—É –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–º—É –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é (VAR) –¥–ª—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ Continuous VAR, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Å—Ç—Ä–æ–≥–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö –æ—Ü–µ–Ω–∫–∏ (strictly proper scoring rules). –û—Å–Ω–æ–≤–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —É–¥–µ–ª—è–µ—Ç—Å—è —Ü–µ–ª–µ–≤—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ (energy score), –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ VAR, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø–æ—Ç–µ—Ä–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏.'}, 'en': {'title': 'Revolutionizing Visual Data Generation with Continuous VAR', 'desc': 'This paper presents a new approach called Continuous Visual AutoRegressive (VAR) modeling, which allows for the generation of visual data without converting it into discrete formats. Traditional methods often lose important information during quantization, but this framework uses strictly proper scoring rules to evaluate and optimize generative models directly in continuous space. By focusing on energy scores as training objectives, the method avoids the challenges of probabilistic predictions in continuous domains. Additionally, it shows that previous continuous autoregressive methods can be derived from this new framework, enhancing their applicability and performance.'}, 'zh': {'title': 'Êó†ÈáèÂåñÁöÑËøûÁª≠Ëá™ÂõûÂΩíÁîüÊàêÊñ∞Ê°ÜÊû∂', 'desc': '‰º†ÁªüËßÇÁÇπËÆ§‰∏∫Ëá™ÂõûÂΩíÊ®°Âûã‰∏ªË¶ÅÁî®‰∫éÂ§ÑÁêÜÁ¶ªÊï£Êï∞ÊçÆ„ÄÇÂú®Â§ÑÁêÜËøûÁª≠Êï∞ÊçÆÔºàÂ¶ÇËßÜËßâÊï∞ÊçÆÔºâÊó∂ÔºåËßÜËßâËá™ÂõûÂΩíÂª∫Ê®°ÔºàVARÔºâÈÄöÂ∏∏ÈúÄË¶ÅÈÄöËøáÈáèÂåñÊñπÊ≥ïÂ∞ÜÊï∞ÊçÆËΩ¨Êç¢‰∏∫Á¶ªÊï£Á©∫Èó¥ÔºåËøôÂèØËÉΩÂØºËá¥‰ø°ÊÅØÊçüÂ§±„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏ÄÁßçËøûÁª≠VARÊ°ÜÊû∂ÔºåËÉΩÂ§üÁõ¥Êé•ËøõË°åËßÜËßâËá™ÂõûÂΩíÁîüÊàêÔºåËÄåÊó†ÈúÄÂêëÈáèÈáèÂåñ„ÄÇËØ•Ê°ÜÊû∂ÁöÑÁêÜËÆ∫Âü∫Á°ÄÊòØ‰∏•Ê†ºÈÄÇÂΩìÁöÑËØÑÂàÜËßÑÂàôÔºåËøô‰∏∫ËØÑ‰º∞ÁîüÊàêÊ®°ÂûãÂ¶Ç‰ΩïÈÄºËøëÁúüÂÆûÂàÜÂ∏ÉÊèê‰æõ‰∫ÜÂº∫Â§ßÁöÑÁªüËÆ°Â∑•ÂÖ∑„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07793', 'title': 'Overflow Prevention Enhances Long-Context Recurrent LLMs', 'url': 'https://huggingface.co/papers/2505.07793', 'abstract': 'A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.', 'score': 3, 'issue_id': 3728, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'f4bfefd5343cbf0c', 'authors': ['Assaf Ben-Kish', 'Itamar Zimerman', 'M. Jehanzeb Mirza', 'James Glass', 'Leonid Karlinsky', 'Raja Giryes'], 'affiliations': ['IBM Research', 'MIT CSAIL', 'Tel Aviv University', 'Xero'], 'pdf_title_img': 'assets/pdf/title_img/2505.07793.jpg', 'data': {'categories': ['#benchmark', '#training', '#architecture', '#long_context'], 'emoji': 'üß†', 'ru': {'title': '–ß–∞–Ω–∫–∏ –ø–æ–±–µ–∂–¥–∞—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤', 'desc': '–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å—É–±-–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –¥–∞–∂–µ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö, –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑—É—é—Ç. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ —á–∞–Ω–∫–∞–º, –≤—ã–±–∏—Ä–∞—é—â–∏–π –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä—è–¥–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–µ LongBench. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞–≤—è—Ç –ø–æ–¥ —Å–æ–º–Ω–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.'}, 'en': {'title': 'Unlocking Long-Context Potential with Chunk-Based Inference', 'desc': 'This paper explores the efficiency of large language models (LLMs) that use recurrent sub-quadratic architectures for processing long contexts. The authors find that these models often do not fully utilize their long-context capabilities due to limitations in their fixed-size recurrent memory. They propose a chunk-based inference method that selectively processes the most relevant parts of the input, significantly enhancing performance on long-context tasks. Their results show substantial improvements in various models and challenge the assumption that recurrent models effectively leverage long-range dependencies.'}, 'zh': {'title': 'ÊèêÂçáÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÊïàÁéáÁöÑÊñ∞ÊñπÊ≥ï', 'desc': 'ÊúÄËøëÔºåÈïøÊñáÊú¨Ê®°ÂûãÔºàLLMsÔºâÂèëÂ±ïÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂ≠ê‰∫åÊ¨°Ê®°ÂûãÔºåÊó®Âú®ÊèêÈ´òÈïø‰∏ä‰∏ãÊñáÂ§ÑÁêÜÁöÑÊïàÁéá„ÄÇÊàë‰ª¨Á†îÁ©∂‰∫Ü‰∏ªË¶ÅÁöÑÈïø‰∏ä‰∏ãÊñáÊ®°ÂûãÔºåÈáçÁÇπÂÖ≥Ê≥®ÂÆÉ‰ª¨Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÈÄíÂΩíËÆ∞ÂøÜÂ¶Ç‰ΩïÂΩ±ÂìçÊÄßËÉΩ„ÄÇÂÆûÈ™åË°®ÊòéÔºåÂç≥‰ΩøËøô‰∫õÊ®°ÂûãÁªèËøáÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉÔºåÂÆÉ‰ª¨ÂØπÈïø‰∏ä‰∏ãÊñáÁöÑÂà©Áî®‰ªçÁÑ∂‰∏çË∂≥„ÄÇÊàë‰ª¨ÊèêÂá∫ÁöÑÂü∫‰∫éÂùóÁöÑÊé®ÁêÜÊñπÊ≥ïËÉΩÂ§üËØÜÂà´Âπ∂Â§ÑÁêÜËæìÂÖ•‰∏≠ÊúÄÁõ∏ÂÖ≥ÁöÑÈÉ®ÂàÜÔºå‰ªéËÄåÊúâÊïàÁºìËß£ÈÄíÂΩíËÆ∞ÂøÜÁöÑ‰∏çË∂≥ÔºåÂπ∂Âú®Â§ö‰∏™Èïø‰∏ä‰∏ãÊñá‰ªªÂä°‰∏≠ÂèñÂæóÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.06324', 'title': 'Document Attribution: Examining Citation Relationships using Large\n  Language Models', 'url': 'https://huggingface.co/papers/2505.06324', 'abstract': "As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.   To tackle this, our work proposes two techniques. (1) A zero-shot approach that frames attribution as a straightforward textual entailment task. Our method using flan-ul2 demonstrates an improvement of 0.27% and 2.4% over the best baseline of ID and OOD sets of AttributionBench, respectively. (2) We also explore the role of the attention mechanism in enhancing the attribution process. Using a smaller LLM, flan-t5-small, the F1 scores outperform the baseline across almost all layers except layer 4 and layers 8 through 11.", 'score': 3, 'issue_id': 3724, 'pub_date': '2025-05-09', 'pub_date_card': {'ru': '9 –º–∞—è', 'en': 'May 9', 'zh': '5Êúà9Êó•'}, 'hash': 'd1b4a407c1a67da8', 'authors': ['Vipula Rawte', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Nedim Lipka'], 'affiliations': ['Adobe Inc.', 'Adobe Research'], 'pdf_title_img': 'assets/pdf/title_img/2505.06324.jpg', 'data': {'categories': ['#training', '#interpretability', '#multimodal', '#hallucinations'], 'emoji': 'üîç', 'ru': {'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ LLM: –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –≤–∫–ª—é—á–µ–Ω–∏—è –¥–æ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è', 'desc': '–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–µ –∞—Ç—Ä–∏–±—É—Ü–∏–∏ –≤ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª—è—Ö (LLM) –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏. –ê–≤—Ç–æ—Ä—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –ø–æ–¥—Ö–æ–¥ —Å –Ω—É–ª–µ–≤—ã–º –æ–±—É—á–µ–Ω–∏–µ–º, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∫–ª—é—á–µ–Ω–∏–∏, –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è. –ü–µ—Ä–≤—ã–π –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π flan-ul2, –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 0,27% –∏ 2,4% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–µ–π –Ω–∞ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö AttributionBench. –í—Ç–æ—Ä–æ–π –º–µ—Ç–æ–¥, –ø—Ä–∏–º–µ–Ω—è—é—â–∏–π –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å flan-t5-small, –ø—Ä–µ–≤–∑–æ—à–µ–ª –±–∞–∑–æ–≤—É—é –ª–∏–Ω–∏—é –ø–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—é F1 –ø–æ—á—Ç–∏ –≤–æ –≤—Å–µ—Ö —Å–ª–æ—è—Ö, –∫—Ä–æ–º–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö.'}, 'en': {'title': 'Enhancing Trust in LLMs through Improved Attribution Techniques', 'desc': 'This paper addresses the challenges of trustworthiness and interpretability in Large Language Models (LLMs) when used for document-based tasks. It introduces two techniques for improving attribution, which is the process of linking model outputs back to their source documents. The first technique is a zero-shot approach that treats attribution as a textual entailment task, showing measurable improvements in performance. The second technique investigates how the attention mechanism in LLMs can enhance attribution accuracy, achieving better F1 scores in most layers of a smaller model.'}, 'zh': {'title': 'ÊèêÂçáÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÁöÑÂèØ‰ø°ÊÄß‰∏éÂèØËß£ÈáäÊÄß', 'desc': 'ÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÊñáÊ°£ÊëòË¶Å„ÄÅÈóÆÁ≠îÂíå‰ø°ÊÅØÊèêÂèñÁ≠â‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®Êó•ÁõäÂ¢ûÂ§öÔºåÁ°Æ‰øùËøô‰∫õÁ≥ªÁªüÁöÑÂèØ‰ø°ÊÄßÂíåÂèØËß£ÈáäÊÄßÂèòÂæóËá≥ÂÖ≥ÈáçË¶Å„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂΩíÂõ†ÊñπÊ≥ïÔºåÈÄöËøáËøΩË∏™ÁîüÊàêÁöÑËæìÂá∫ÂõûÂà∞ÂÖ∂Ê∫êÊñáÊ°£Êù•Ëß£ÂÜ≥Ëøô‰∏ÄÊåëÊàò„ÄÇÊàë‰ª¨ÊèêÂá∫‰∫Ü‰∏§ÁßçÊäÄÊúØÔºö‰∏ÄÁßçÊòØÈõ∂Ê†∑Êú¨ÊñπÊ≥ïÔºåÂ∞ÜÂΩíÂõ†ËßÜ‰∏∫ÁÆÄÂçïÁöÑÊñáÊú¨Ëï¥Âê´‰ªªÂä°ÔºåÂè¶‰∏ÄÁßçÊòØÊé¢Á¥¢Ê≥®ÊÑèÂäõÊú∫Âà∂Âú®Â¢ûÂº∫ÂΩíÂõ†ËøáÁ®ã‰∏≠ÁöÑ‰ΩúÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåÊàë‰ª¨ÁöÑÊñπÊ≥ïÂú®Â§ö‰∏™Âü∫ÂáÜÊµãËØï‰∏≠ÂùáÊúâÊòæËëóÊèêÂçá„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07291', 'title': 'INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2505.07291', 'abstract': 'We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.   To enable a training run with this unique infrastructure, we built various components from scratch: we introduce PRIME-RL, our training framework purpose-built for distributed asynchronous reinforcement learning, based on top of novel components such as TOPLOC, which verifies rollouts from untrusted inference workers, and SHARDCAST, which efficiently broadcasts policy weights from training nodes to inference workers.   Beyond infrastructure components, we propose modifications to the standard GRPO training recipe and data filtering techniques that were crucial to achieve training stability and ensure that our model successfully learned its training objective, thus improving upon QwQ-32B, the state of the art reasoning model in the 32B parameter range.   We open-source INTELLECT-2 along with all of our code and data, hoping to encourage and enable more open research in the field of decentralized training.', 'score': 2, 'issue_id': 3742, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'bf4c666ff6a739cf', 'authors': ['Prime Intellect Team', 'Sami Jaghouar', 'Justus Mattern', 'Jack Min Ong', 'Jannik Straube', 'Manveer Basra', 'Aaron Pazdera', 'Kushal Thaman', 'Matthew Di Ferrante', 'Felix Gabriel', 'Fares Obeid', 'Kemal Erdem', 'Michael Keiblinger', 'Johannes Hagemann'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.07291.jpg', 'data': {'categories': ['#open_source', '#training', '#optimization', '#rl', '#dataset', '#reasoning'], 'emoji': 'üåê', 'ru': {'title': '–ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è –∫—Ä—É–ø–Ω–æ–º–∞—Å—à—Ç–∞–±–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π', 'desc': 'INTELLECT-2 –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–≤—É—é –≥–ª–æ–±–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –¥–ª—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å 32 –º–∏–ª–ª–∏–∞—Ä–¥–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, INTELLECT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –Ω–∞ –¥–∏–Ω–∞–º–∏—á–Ω–æ–º, –≥–µ—Ç–µ—Ä–æ–≥–µ–Ω–Ω–æ–º —Ä–æ–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —É–∑–ª–æ–≤. –î–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–π —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±—ã–ª–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ PRIME-RL, —Å–∏—Å—Ç–µ–º—É –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ TOPLOC –∏ –º–µ—Ö–∞–Ω–∏–∑–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ SHARDCAST. –ê–≤—Ç–æ—Ä—ã —Ç–∞–∫–∂–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ GRPO –∏ –º–µ—Ç–æ–¥—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è.'}, 'en': {'title': 'Revolutionizing Reinforcement Learning with Distributed Training', 'desc': 'INTELLECT-2 is a groundbreaking reinforcement learning model that utilizes a globally distributed training approach for a 32 billion parameter language model. It employs asynchronous training across a diverse group of independent compute contributors, which is a shift from traditional centralized methods. The paper introduces innovative components like PRIME-RL for managing distributed training, TOPLOC for verifying data from untrusted sources, and SHARDCAST for efficient communication of model updates. By refining the GRPO training method and implementing effective data filtering, INTELLECT-2 achieves enhanced stability and performance, surpassing previous models in its category.'}, 'zh': {'title': 'INTELLECT-2ÔºöÂÖ®ÁêÉÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ÁöÑÂàõÊñ∞‰πãË∑Ø', 'desc': 'Êàë‰ª¨‰ªãÁªç‰∫ÜINTELLECT-2ÔºåËøôÊòØÁ¨¨‰∏Ä‰∏™ÂÖ®ÁêÉÂàÜÂ∏ÉÂºèÁöÑÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÔºå‰ΩøÁî®‰∫Ü320‰∫øÂèÇÊï∞ÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇ‰∏é‰º†ÁªüÁöÑÈõÜ‰∏≠ÂºèËÆ≠ÁªÉ‰∏çÂêåÔºåINTELLECT-2ÈÄöËøá‰∏Ä‰∏™Âä®ÊÄÅÁöÑ„ÄÅÂºÇÊûÑÁöÑËÆ°ÁÆóË¥°ÁåÆËÄÖÁæ§‰ΩìÔºåÈááÁî®ÂÆåÂÖ®ÂºÇÊ≠•ÁöÑÂº∫ÂåñÂ≠¶‰π†Êù•ËÆ≠ÁªÉÊé®ÁêÜÊ®°Âûã„ÄÇ‰∏∫‰∫ÜÊîØÊåÅËøôÁßçÁã¨ÁâπÁöÑÂü∫Á°ÄËÆæÊñΩÔºåÊàë‰ª¨‰ªéÂ§¥ÂºÄÂßãÊûÑÂª∫‰∫ÜÂ§ö‰∏™ÁªÑ‰ª∂ÔºåÂåÖÊã¨‰∏ì‰∏∫ÂàÜÂ∏ÉÂºèÂºÇÊ≠•Âº∫ÂåñÂ≠¶‰π†ËÆæËÆ°ÁöÑPRIME-RLËÆ≠ÁªÉÊ°ÜÊû∂Ôºå‰ª•ÂèäÈ™åËØÅ‰∏çÂèØ‰ø°Êé®ÁêÜÂ∑•‰ΩúËÄÖÁöÑTOPLOCÂíåÈ´òÊïàÂπøÊí≠Á≠ñÁï•ÊùÉÈáçÁöÑSHARDCAST„ÄÇÊàë‰ª¨ËøòÂØπÊ†áÂáÜÁöÑGRPOËÆ≠ÁªÉÊñπÊ≥ïÂíåÊï∞ÊçÆËøáÊª§ÊäÄÊúØËøõË°å‰∫Ü‰øÆÊîπÔºå‰ª•Á°Æ‰øùËÆ≠ÁªÉÁöÑÁ®≥ÂÆöÊÄßÔºåÂπ∂ÊàêÂäüÂÆûÁé∞Ê®°ÂûãÁöÑËÆ≠ÁªÉÁõÆÊ†áÔºå‰ªéËÄåÂú®320‰∫øÂèÇÊï∞ËåÉÂõ¥ÂÜÖË∂ÖË∂ä‰∫ÜÁé∞ÊúâÁöÑQwQ-32BÊé®ÁêÜÊ®°Âûã„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07233', 'title': 'DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for\n  Dynamic Reranking in Retrieval-Augmented Generation', 'url': 'https://huggingface.co/papers/2505.07233', 'abstract': 'Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG', 'score': 2, 'issue_id': 3734, 'pub_date': '2025-05-12', 'pub_date_card': {'ru': '12 –º–∞—è', 'en': 'May 12', 'zh': '5Êúà12Êó•'}, 'hash': 'b6c55b4c738d5230', 'authors': ['Jiashuo Sun', 'Xianrui Zhong', 'Sizhe Zhou', 'Jiawei Han'], 'affiliations': ['University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.07233.jpg', 'data': {'categories': ['#rag', '#optimization', '#interpretability', '#rl'], 'emoji': 'üîÑ', 'ru': {'title': '–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ò–ò-—Å–∏—Å—Ç–µ–º', 'desc': 'DynamicRAG - —ç—Ç–æ –Ω–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (RAG), –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –†–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º, –∏—Å–ø–æ–ª—å–∑—É—è –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏. –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –ø–æ—Ä—è–¥–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. DynamicRAG –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Å–µ–º–∏ –Ω–∞–±–æ—Ä–∞—Ö –¥–∞–Ω–Ω—ã—Ö, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—à–∏—Ä–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.'}, 'en': {'title': 'DynamicRAG: Optimizing Document Retrieval for Better Generation', 'desc': "This paper introduces DynamicRAG, a new framework for retrieval-augmented generation (RAG) systems that enhances the quality of generated responses by optimizing the reranking of retrieved documents. The reranker in DynamicRAG is designed to dynamically adjust both the order and the number of documents based on the specific query, addressing the challenge of selecting the optimal number of documents. By employing reinforcement learning, the reranker uses feedback from the quality of the language model's output to improve its decisions. The results show that DynamicRAG outperforms existing methods across multiple knowledge-intensive datasets, achieving state-of-the-art performance."}, 'zh': {'title': 'Âä®ÊÄÅË∞ÉÊï¥ÔºåÊèêÂçáÁîüÊàêË¥®ÈáèÁöÑRAGÊ°ÜÊû∂', 'desc': 'Ê£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÁ≥ªÁªüÁªìÂêà‰∫ÜÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂíåÂ§ñÈÉ®Áü•ËØÜÊ£ÄÁ¥¢ÔºåÈÄÇÁî®‰∫éÁü•ËØÜÂØÜÈõÜÂûã‰ªªÂä°„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑRAGÊ°ÜÊû∂DynamicRAGÔºåÂÖ∂‰∏≠ÁöÑÈáçÊéíÂ∫èÂô®ËÉΩÂ§üÊ†πÊçÆÊü•ËØ¢Âä®ÊÄÅË∞ÉÊï¥Ê£ÄÁ¥¢ÊñáÊ°£ÁöÑÈ°∫Â∫èÂíåÊï∞Èáè„ÄÇÊàë‰ª¨Â∞ÜÈáçÊéíÂ∫èÂô®Âª∫Ê®°‰∏∫‰∏Ä‰∏™ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†ÔºàRLÔºâ‰ºòÂåñÁöÑÊô∫ËÉΩ‰ΩìÔºåÂà©Áî®LLMËæìÂá∫Ë¥®Èáè‰Ωú‰∏∫Â•ñÂä±Êù•‰ºòÂåñÈáçÊéíÂ∫èÂÜ≥Á≠ñ„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåDynamicRAGÂú®‰∏É‰∏™Áü•ËØÜÂØÜÈõÜÂûãÊï∞ÊçÆÈõÜ‰∏äË°®Áé∞‰ºòÂºÇÔºåËææÂà∞‰∫ÜÊúÄÂÖàËøõÁöÑÁªìÊûú„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04918', 'title': 'Physics-Assisted and Topology-Informed Deep Learning for Weather\n  Prediction', 'url': 'https://huggingface.co/papers/2505.04918', 'abstract': "Although deep learning models have demonstrated remarkable potential in weather prediction, most of them overlook either the physics of the underlying weather evolution or the topology of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. With these considerations, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. In the 5.625^circ-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42. Code and checkpoint are available at https://github.com/Yumenomae/PASSAT_5p625.", 'score': 2, 'issue_id': 3728, 'pub_date': '2025-05-08', 'pub_date_card': {'ru': '8 –º–∞—è', 'en': 'May 8', 'zh': '5Êúà8Êó•'}, 'hash': '73efdd4a4328c88d', 'authors': ['Jiaqi Zheng', 'Qing Ling', 'Yerong Feng'], 'affiliations': ['Shenzhen Institute of Meteorological Innovation', 'Sun Yat-Sen University'], 'pdf_title_img': 'assets/pdf/title_img/2505.04918.jpg', 'data': {'categories': ['#data', '#optimization', '#dataset', '#graphs', '#architecture', '#training'], 'emoji': 'üåé', 'ru': {'title': 'PASSAT: –§–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã —Å —É—á–µ—Ç–æ–º —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –ó–µ–º–ª–∏', 'desc': 'PASSAT - —ç—Ç–æ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–≥–æ–¥—ã, —É—á–∏—Ç—ã–≤–∞—é—â–∞—è —Ñ–∏–∑–∏–∫—É –∞—Ç–º–æ—Å—Ñ–µ—Ä–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∏ —Ç–æ–ø–æ–ª–æ–≥–∏—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ó–µ–º–ª–∏. –ú–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–¥–≤–µ–∫—Ü–∏–∏ –∏ –ù–∞–≤—å–µ-–°—Ç–æ–∫—Å–∞ –Ω–∞ —Å—Ñ–µ—Ä–∏—á–µ—Å–∫–æ–º –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ñ–µ—Ä–∏—á–µ—Å–∫—É—é –≥—Ä–∞—Ñ–æ–≤—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ó–µ–º–ª–∏ –∏ –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã. PASSAT –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–æ–ª—è —Å–∫–æ—Ä–æ—Å—Ç–µ–π, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —É—Ä–∞–≤–Ω–µ–Ω–∏—è –∞–¥–≤–µ–∫—Ü–∏–∏. –í —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö –Ω–∞ –¥–∞–Ω–Ω—ã—Ö ERA5 —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 5.625¬∞ PASSAT –ø—Ä–µ–≤–∑–æ—à–ª–∞ –∫–∞–∫ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, —Ç–∞–∫ –∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —á–∏—Å–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –ø–æ–≥–æ–¥—ã IFS T42.'}, 'en': {'title': 'PASSAT: Bridging Physics and Topology for Superior Weather Prediction', 'desc': "The paper introduces PASSAT, a deep learning model designed for weather prediction that integrates physical principles and the Earth's surface topology. Unlike traditional models, PASSAT incorporates the advection process and the complex interactions between the Earth and atmosphere, using the advection and Navier-Stokes equations. It employs a spherical graph neural network to effectively model these interactions and generate essential initial velocity fields. The results show that PASSAT significantly outperforms existing deep learning models and the operational numerical weather prediction model IFS T42, demonstrating its effectiveness in accurately predicting weather patterns."}, 'zh': {'title': 'PASSATÔºöÁªìÂêàÁâ©ÁêÜ‰∏éÊãìÊâëÁöÑÂ§©Ê∞îÈ¢ÑÊµãÊñ∞Ê®°Âûã', 'desc': 'Êú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÂûãÁöÑÂ§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãPASSATÔºåËØ•Ê®°ÂûãÁªìÂêà‰∫ÜÁâ©ÁêÜÂ≠¶ÂíåÂú∞ÂΩ¢‰ø°ÊÅØ„ÄÇPASSATÈÄöËøáÂØπÊµÅËøáÁ®ãÂíåÂú∞ÁêÉ-Â§ßÊ∞îÁõ∏‰∫í‰ΩúÁî®Êù•ÊèèËø∞Â§©Ê∞îÊºîÂèòÔºåÂπ∂ËÄÉËôë‰∫ÜÂú∞ÁêÉË°®Èù¢ÁöÑÊãìÊâëÁªìÊûÑ„ÄÇËØ•Ê®°ÂûãÂú®ÁêÉÈù¢ÊµÅÂΩ¢‰∏äÊï∞ÂÄºÊ±ÇËß£ÂØπÊµÅÊñπÁ®ãÂíåÁ∫≥Áª¥-ÊñØÊâòÂÖãÊñØÊñπÁ®ãÔºåÂπ∂Âà©Áî®ÁêÉÈù¢ÂõæÁ•ûÁªèÁΩëÁªúÊçïÊçâÂú∞ÁêÉ-Â§ßÊ∞îÁöÑÁõ∏‰∫í‰ΩúÁî®„ÄÇÂÆûÈ™åÁªìÊûúË°®ÊòéÔºåPASSATÂú®5.625Â∫¶ÂàÜËæ®ÁéáÁöÑERA5Êï∞ÊçÆÈõÜ‰∏ä‰ºò‰∫éÁé∞ÊúâÁöÑÊ∑±Â∫¶Â≠¶‰π†Â§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãÂíåÊìç‰ΩúÊÄßÊï∞ÂÄºÂ§©Ê∞îÈ¢ÑÊµãÊ®°ÂûãIFS T42„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.04066', 'title': 'LLAMAPIE: Proactive In-Ear Conversation Assistants', 'url': 'https://huggingface.co/papers/2505.04066', 'abstract': 'We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.', 'score': 1, 'issue_id': 3739, 'pub_date': '2025-05-07', 'pub_date_card': {'ru': '7 –º–∞—è', 'en': 'May 7', 'zh': '5Êúà7Êó•'}, 'hash': 'f8bf204612751793', 'authors': ['Tuochao Chen', 'Nicholas Batchelder', 'Alisa Liu', 'Noah Smith', 'Shyamnath Gollakota'], 'affiliations': ['University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2505.04066.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#dataset', '#data'], 'emoji': 'üéß', 'ru': {'title': 'LlamaPIE: –ù–µ–∑–∞–º–µ—Ç–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∂–∏–≤—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤', 'desc': 'LlamaPIE - —ç—Ç–æ –ø–µ—Ä–≤—ã–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–º–µ—Ç–Ω—ã–µ, –∫—Ä–∞—Ç–∫–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏, –ø–µ—Ä–µ–¥–∞–≤–∞–µ–º—ã–µ —á–µ—Ä–µ–∑ —Å–ª—É—Ö–æ–≤—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π, —ç—Ç–æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ, –ø—Ä–µ–¥—É–≥–∞–¥—ã–≤–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –±–µ–∑ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. –ê–≤—Ç–æ—Ä—ã —Ä–µ—à–∞—é—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á, –≤–∫–ª—é—á–∞—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–º–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ. –û—Ü–µ–Ω–∫–∞ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å LlamaPIE –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –ø–æ–ª–µ–∑–Ω–æ–π, –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤–æ–π –ø–æ–º–æ—â–∏.'}, 'en': {'title': 'LlamaPIE: Enhancing Conversations with Proactive Assistance', 'desc': 'LlamaPIE is a novel real-time proactive assistant that enhances human conversations by providing discreet guidance through hearable devices. Unlike traditional models that wait for user prompts, LlamaPIE anticipates user needs and offers assistance without interrupting the flow of conversation. The system tackles challenges such as timing for responses, generating concise and relevant replies, and utilizing user context for personalized support, all while processing information on-device. Evaluations show that users prefer LlamaPIE over reactive models, indicating its effectiveness in improving live interactions.'}, 'zh': {'title': 'LlamaPIEÔºöÊèêÂçáÂØπËØùÁöÑ‰∏ªÂä®Âä©Êâã', 'desc': 'LlamaPIE ÊòØÈ¶ñ‰∏™ÂÆûÊó∂‰∏ªÂä®Âä©ÊâãÔºåÊó®Âú®ÈÄöËøáÂèØÁ©øÊà¥ËÆæÂ§áÂú®ÂØπËØù‰∏≠Êèê‰æõÁÆÄÊ¥ÅÁöÑÊåáÂØº„ÄÇ‰∏é‰º†ÁªüËØ≠Ë®ÄÊ®°Âûã‰∏çÂêåÔºåÂÆÉÂú®ÂêéÂè∞ËøêË°åÔºåËÉΩÂ§üÈ¢ÑÊµãÁî®Êà∑ÈúÄÊ±ÇËÄå‰∏çÊâìÊñ≠ÂØπËØù„ÄÇÊàë‰ª¨Ëß£ÂÜ≥‰∫ÜÂ§ö‰∏™ÊåëÊàòÔºåÂåÖÊã¨‰ΩïÊó∂ÂìçÂ∫î„ÄÅÂ¶Ç‰ΩïÁîüÊàêÁÆÄÊ¥ÅÁöÑÂõûÂ∫î‰ª•ÂèäÂ¶Ç‰ΩïÂà©Áî®Áî®Êà∑Áü•ËØÜÊèê‰æõ‰∏ä‰∏ãÊñáÊÑüÁü•ÁöÑÂ∏ÆÂä©„ÄÇÈÄöËøáÊûÑÂª∫ÂçäÂêàÊàêÂØπËØùÊï∞ÊçÆÈõÜÂíåÊèêÂá∫ÂèåÊ®°ÂûãÁÆ°ÈÅìÔºåÊàë‰ª¨Âú®ÁúüÂÆûÊï∞ÊçÆÈõÜ‰∏äËØÑ‰º∞‰∫ÜËØ•ÊñπÊ≥ïÁöÑÊúâÊïàÊÄßÔºåÁî®Êà∑Á†îÁ©∂ÊòæÁ§∫Âá∫ÂØπ‰∏ªÂä®Âä©ÊâãÁöÑÂº∫ÁÉàÂÅèÂ•Ω„ÄÇ'}}}, {'id': 'https://huggingface.co/papers/2505.07086', 'title': 'Multi-Objective-Guided Discrete Flow Matching for Controllable\n  Biological Sequence Design', 'url': 'https://huggingface.co/papers/2505.07086', 'abstract': "Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.", 'score': 0, 'issue_id': 3725, 'pub_date': '2025-05-11', 'pub_date_card': {'ru': '11 –º–∞—è', 'en': 'May 11', 'zh': '5Êúà11Êó•'}, 'hash': 'a2fce171208a1e7a', 'authors': ['Tong Chen', 'Yinuo Zhang', 'Sophia Tang', 'Pranam Chatterjee'], 'affiliations': ['Center of Computational Biology, Duke-NUS Medical School', 'Department of Biomedical Engineering, Duke University', 'Department of Biostatistics and Bioinformatics, Duke University', 'Department of Computer Science, Duke University', 'Department of Computer Science, Fudan University', 'Management and Technology Program, University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2505.07086.jpg', 'data': {'categories': ['#training', '#dataset', '#science', '#data', '#optimization'], 'emoji': 'üß¨', 'ru': {'title': '–ú–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤', 'desc': '–°—Ç–∞—Ç—å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ü–µ–ª–µ–≤—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏. MOG-DFM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤ –∏ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –∫ –ü–∞—Ä–µ—Ç–æ-—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–∞–º –º–µ–∂–¥—É –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å–∫–∞–ª—è—Ä–Ω—ã–º–∏ —Ü–µ–ª—è–º–∏. –ê–≤—Ç–æ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ–ø—Ç–∏–¥–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏ –∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –î–ù–ö —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏. MOG-DFM –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–µ–±—è –º–æ—â–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞ –±–∏–æ–º–æ–ª–µ–∫—É–ª—è—Ä–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.'}, 'en': {'title': 'Optimizing Biomolecule Design with MOG-DFM', 'desc': "This paper introduces Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a new framework for designing biological sequences that meet multiple, often conflicting, criteria. Unlike previous methods that focus on single objectives or use continuous embeddings, MOG-DFM efficiently navigates high-dimensional sequence spaces to find Pareto-efficient solutions. The framework employs a hybrid rank-directional scoring system and an adaptive hypercone filter to ensure consistent progress across various objectives. The authors demonstrate MOG-DFM's capabilities by generating optimized peptide binders and specific enhancer DNA sequences, showcasing its potential in biomolecule engineering."}, 'zh': {'title': 'Â§öÁõÆÊ†á‰ºòÂåñÔºåÂä©ÂäõÁîüÁâ©ÂàÜÂ≠êËÆæËÆ°', 'desc': 'Âú®ÁîüÁâ©ÂàÜÂ≠êÂ∑•Á®ã‰∏≠ÔºåËÆæËÆ°Êª°Ë∂≥Â§öÁßçÂäüËÉΩÂíåÁîüÁâ©Áâ©ÁêÜÊ†áÂáÜÁöÑÁîüÁâ©Â∫èÂàó‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÈáçË¶ÅÊåëÊàò„ÄÇÊú¨ÊñáÊèêÂá∫‰∫Ü‰∏ÄÁßçÂêç‰∏∫Â§öÁõÆÊ†áÂºïÂØºÁ¶ªÊï£ÊµÅÂåπÈÖçÔºàMOG-DFMÔºâÁöÑÊ°ÜÊû∂ÔºåËÉΩÂ§üÂú®Â§ö‰∏™Ê†áÈáèÁõÆÊ†á‰πãÈó¥ÂÆûÁé∞Â∏ïÁ¥ØÊâòÊúâÊïàÁöÑÊùÉË°°„ÄÇMOG-DFMÈÄöËøáËÆ°ÁÆóÊ∑∑ÂêàÊéíÂêçÊñπÂêëÂàÜÊï∞ÂíåÂ∫îÁî®Ëá™ÈÄÇÂ∫îË∂ÖÈî•ËøáÊª§Âô®ÔºåÊù•ÂºïÂØºÈ¢ÑËÆ≠ÁªÉÁöÑÁ¶ªÊï£Êó∂Èó¥ÊµÅÂåπÈÖçÁîüÊàêÂô®ËøõË°åÂ§öÁõÆÊ†á‰ºòÂåñ„ÄÇÊàë‰ª¨Â±ïÁ§∫‰∫ÜMOG-DFMÂú®ÁîüÊàê‰ºòÂåñÁöÑËÇΩÁªìÂêàÁâ©ÂíåÁâπÂÆöÂ¢ûÂº∫Â≠êÁ±ªDNAÂ∫èÂàóÊñπÈù¢ÁöÑÊúâÊïàÊÄßÔºåËØÅÊòé‰∫ÜÂÖ∂Âú®Â§öÂ±ûÊÄßÂºïÂØºÁöÑÁîüÁâ©ÂàÜÂ≠êÂ∫èÂàóËÆæËÆ°‰∏≠ÁöÑÂº∫Â§ßËÉΩÂäõ„ÄÇ'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (3)', '#agi (1)', '#alignment (2)', '#architecture (7)', '#audio', '#benchmark (7)', '#cv (2)', '#data (10)', '#dataset (12)', '#diffusion (5)', '#ethics', '#games (1)', '#graphs (1)', '#hallucinations (2)', '#healthcare', '#inference', '#interpretability (3)', '#leakage (1)', '#long_context (1)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (18)', '#plp (1)', '#rag (2)', '#reasoning (6)', '#rl (6)', '#rlhf (1)', '#robotics (1)', '#science (1)', '#security', '#small_models (4)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (18)', '#transfer_learning (2)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `üè∑Ô∏è ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            üî∫ ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'üîÑ ' + getTimeDiff('2025-05-13 22:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "—Ä–µ–π—Ç–∏–Ω–≥—É",
                    pub_date: "–¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
                    issue_id: "–¥–æ–±–∞–≤–ª–µ–Ω–∏—é –Ω–∞ HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "ËØÑÂàÜ",
                    pub_date: "ÂèëÂ∏ÉÊó•Êúü",
                    issue_id: "HF‰∏ä‰º†Êó•Êúü"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-05-13 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-05-13 22:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    