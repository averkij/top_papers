
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 19 papers. March 7.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">7 Ğ¼Ğ°Ñ€Ñ‚Ğ°</span> | <span id="title-articles-count">19 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-06.html">â¬…ï¸ <span id="prev-date">06.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-10.html">â¡ï¸ <span id="next-date">10.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '7 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 7', 'zh': '3æœˆ7æ—¥'};
        let feedDateNext = {'ru': '10.03', 'en': '03/10', 'zh': '3æœˆ10æ—¥'};
        let feedDatePrev = {'ru': '06.03', 'en': '03/06', 'zh': '3æœˆ6æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.04625', 'title': 'START: Self-taught Reasoner with Tools', 'url': 'https://huggingface.co/papers/2503.04625', 'abstract': "Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.", 'score': 42, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '8961f69e1eda24ad', 'authors': ['Chengpeng Li', 'Mingfeng Xue', 'Zhenru Zhang', 'Jiaxi Yang', 'Beichen Zhang', 'Xiang Wang', 'Bowen Yu', 'Binyuan Hui', 'Junyang Lin', 'Dayiheng Liu'], 'affiliations': ['Alibaba Group', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04625.jpg', 'data': {'categories': ['#long_context', '#rl', '#training', '#architecture', '#hallucinations', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'START: Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ START - Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ¼Ñ‹ÑĞ»ĞµĞ¹, Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹. START Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºÑƒ Hint-infer Ğ´Ğ»Ñ ÑÑ‚Ğ¸Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², Ğ¸ Hint Rejection Sampling Fine-Tuning Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ QwQ-32B Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. START Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ½Ğ°ÑƒĞºĞµ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ PhD Ğ¸ ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹.'}, 'en': {'title': 'Enhancing Reasoning with External Tools: Introducing START', 'desc': "This paper presents START, a new long Chain-of-thought reasoning model that integrates external tools to improve reasoning capabilities. Traditional large reasoning models often struggle with hallucinations and inefficiencies, but START addresses these issues by allowing the model to perform complex computations and self-debugging. The innovation lies in two techniques: Hint-infer, which uses designed hints to encourage tool usage, and Hint Rejection Sampling Fine-Tuning, which refines the model's reasoning paths. START demonstrates superior performance on various benchmarks, surpassing its predecessor and competing effectively with state-of-the-art models."}, 'zh': {'title': 'å·¥å…·æ•´åˆï¼Œæ¨ç†æ›´å¼ºï¼', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é•¿é“¾æ¨ç†æ¨¡å‹STARTï¼ˆè‡ªæˆ‘å­¦ä¹ æ¨ç†å™¨ä¸å·¥å…·ï¼‰ï¼Œå®ƒé€šè¿‡æ•´åˆå¤–éƒ¨å·¥å…·æ¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚STARTåˆ©ç”¨ä»£ç æ‰§è¡Œè¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥ã€æ¢ç´¢å¤šç§æ–¹æ³•å’Œè‡ªæˆ‘è°ƒè¯•ï¼Œä»è€Œå…‹æœäº†å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¸¸è§çš„å¹»è§‰å’Œä½æ•ˆé—®é¢˜ã€‚æ ¸å¿ƒåˆ›æ–°åœ¨äºè‡ªæˆ‘å­¦ä¹ æ¡†æ¶ï¼ŒåŒ…æ‹¬æç¤ºæ¨ç†ï¼ˆHint-inferï¼‰å’Œæç¤ºæ‹’ç»é‡‡æ ·å¾®è°ƒï¼ˆHint-RFTï¼‰ä¸¤ç§æŠ€æœ¯ï¼Œå‰è€…é€šè¿‡æ’å…¥è®¾è®¡çš„æç¤ºæ¥æ¿€å‘æ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ã€‚ç»è¿‡å¾®è°ƒï¼ŒSTARTåœ¨å¤šä¸ªç§‘å­¦é—®ç­”å’Œæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡æ˜¾è‘—é«˜äºåŸºç¡€æ¨¡å‹QwQ-32Bã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04724', 'title': 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM', 'url': 'https://huggingface.co/papers/2503.04724', 'abstract': 'Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .', 'score': 20, 'issue_id': 2589, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '3113e03ae6f4ed42', 'authors': ['Sambal Shikhar', 'Mohammed Irfan Kurpath', 'Sahal Shaji Mullappilly', 'Jean Lahoud', 'Fahad Khan', 'Rao Muhammad Anwer', 'Salman Khan', 'Hisham Cholakkal'], 'affiliations': ['LinkÃ¶ping University, Sweden', 'Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), UAE'], 'pdf_title_img': 'assets/pdf/title_img/2503.04724.jpg', 'data': {'categories': ['#multimodal', '#small_models', '#low_resource', '#audio', '#open_source', '#long_context'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'LLMVoX: Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'LLMVoX - ÑÑ‚Ğ¾ Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ñ€ĞµÑ‡Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ¸Ñ… Ğ¼Ğ¾Ğ´Ğ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸. ĞĞ½Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ½Ñ‚ĞµĞ· Ñ€ĞµÑ‡Ğ¸ Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ²ÑĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. LLMVoX Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€ĞµÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ Ñ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ğ¾Ğ¹ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ², Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Seamless Speech Synthesis with LLMVoX', 'desc': 'The paper introduces LLMVoX, a novel speech-to-speech dialogue system that utilizes a lightweight, 30M-parameter architecture to enhance multimodal interactions without compromising the linguistic capabilities of large language models (LLMs). Unlike existing systems that require extensive fine-tuning and often degrade conversational quality, LLMVoX operates efficiently with low latency and a significantly reduced Word Error Rate. It employs a multi-queue token streaming mechanism to decouple speech synthesis from LLM processing, enabling seamless dialogues of infinite length. Additionally, LLMVoX can adapt to new languages with minimal dataset adjustments and integrates with Vision-Language Models to support speech, text, and vision functionalities without extra multimodal training.'}, 'zh': {'title': 'LLMVoXï¼šé«˜æ•ˆè¯­éŸ³åˆæˆçš„æ–°é€‰æ‹©', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLLMVoXçš„è½»é‡çº§è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œå…·æœ‰3000ä¸‡å‚æ•°ï¼Œèƒ½å¤Ÿä¸ä»»ä½•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…¼å®¹ã€‚LLMVoXé€šè¿‡å¤šé˜Ÿåˆ—ä»¤ç‰Œæµç³»ç»Ÿï¼Œå°†è¯­éŸ³åˆæˆä¸LLMå¤„ç†è§£è€¦ï¼Œä»è€Œå®ç°ä½å»¶è¿Ÿå’Œé«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆã€‚ä¸ç°æœ‰çš„è¯­éŸ³å¢å¼ºLLMç›¸æ¯”ï¼ŒLLMVoXåœ¨è¯é”™è¯¯ç‡ä¸Šæ˜¾è‘—é™ä½ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼çš„å»¶è¿Ÿå’Œç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ã€‚è¯¥ç³»ç»Ÿè¿˜æ”¯æŒæ— ç¼çš„æ— é™é•¿åº¦å¯¹è¯ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡æ•°æ®é›†é€‚åº”è½»æ¾æ‰©å±•åˆ°æ–°è¯­è¨€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03803', 'title': 'EgoLife: Towards Egocentric Life Assistant', 'url': 'https://huggingface.co/papers/2503.03803', 'abstract': 'We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.', 'score': 15, 'issue_id': 2581, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 5', 'zh': '3æœˆ5æ—¥'}, 'hash': '52396234365a3fb0', 'authors': ['Jingkang Yang', 'Shuai Liu', 'Hongming Guo', 'Yuhao Dong', 'Xiamengwei Zhang', 'Sicheng Zhang', 'Pengyun Wang', 'Zitang Zhou', 'Binzhu Xie', 'Ziyue Wang', 'Bei Ouyang', 'Zhengyu Lin', 'Marco Cominelli', 'Zhongang Cai', 'Yuanhan Zhang', 'Peiyuan Zhang', 'Fangzhou Hong', 'Joerg Widmer', 'Francesco Gringoli', 'Lei Yang', 'Bo Li', 'Ziwei Liu'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.03803.jpg', 'data': {'categories': ['#video', '#long_context', '#dataset', '#open_source', '#agents', '#multimodal', '#data', '#benchmark'], 'emoji': 'ğŸ‘“', 'ru': {'title': 'EgoLife: Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ¾Ñ‡ĞºĞ¾Ğ²', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ EgoLife Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ñ‡ĞºĞ¾Ğ² Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ ÑĞ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… EgoLife Dataset, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ¸Ğ¹ 300 Ñ‡Ğ°ÑĞ¾Ğ² ÑĞ³Ğ¾Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸ ÑˆĞµÑÑ‚Ğ¸ ÑƒÑ‡Ğ°ÑÑ‚Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ·Ğ°Ğ´Ğ°Ñ‡ EgoLifeQA Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿Ğ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ğ¾Ğ¹ Ğ¶Ğ¸Ğ·Ğ½Ğ¸. Ğ”Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° EgoButler, Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‰Ğ°Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ EgoGPT Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ EgoRAG.'}, 'en': {'title': 'Empowering Daily Life with Egocentric AI Assistance', 'desc': 'The paper presents EgoLife, an AI-powered life assistant designed to enhance personal efficiency through wearable glasses that capture egocentric video data. A comprehensive dataset, the EgoLife Dataset, was created by recording daily activities of participants over a week, resulting in 300 hours of multimodal data with detailed annotations. The study introduces EgoLifeQA, a set of question-answering tasks that utilize this dataset to assist users with practical life questions and personalized recommendations. To tackle challenges in visual-audio model development and long-context question answering, the authors propose EgoButler, which includes EgoGPT for video understanding and EgoRAG for retrieval-based answering.'}, 'zh': {'title': 'æ™ºèƒ½ç”Ÿæ´»åŠ©æ‰‹ï¼Œæå‡ä¸ªäººæ•ˆç‡', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†EgoLifeé¡¹ç›®ï¼Œæ—¨åœ¨å¼€å‘ä¸€ä¸ªä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„ç”Ÿæ´»åŠ©æ‰‹ï¼Œé€šè¿‡AIé©±åŠ¨çš„å¯ç©¿æˆ´çœ¼é•œæå‡ä¸ªäººæ•ˆç‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ•°æ®æ”¶é›†ç ”ç©¶ï¼Œå…­åå‚ä¸è€…å…±åŒç”Ÿæ´»ä¸€å‘¨ï¼Œä½¿ç”¨AIçœ¼é•œè®°å½•æ—¥å¸¸æ´»åŠ¨ï¼Œå½¢æˆäº†EgoLifeæ•°æ®é›†ï¼ŒåŒ…å«300å°æ—¶çš„å¤šè§†è§’ã€å¤šæ¨¡æ€æ—¥å¸¸ç”Ÿæ´»æ•°æ®ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EgoLifeQAï¼Œä¸€ä¸ªé’ˆå¯¹ç”Ÿæ´»çš„é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ï¼Œæ—¨åœ¨æä¾›å®ç”¨çš„æ—¥å¸¸ç”Ÿæ´»å¸®åŠ©ã€‚ä¸ºäº†è§£å†³å…³é”®æŠ€æœ¯æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†EgoButlerç³»ç»Ÿï¼ŒåŒ…æ‹¬EgoGPTå’ŒEgoRAGï¼Œå‰è€…åœ¨è‡ªæˆ‘ä¸­å¿ƒè§†é¢‘ç†è§£ä¸Šè¡¨ç°å‡ºè‰²ï¼Œåè€…æ”¯æŒè¶…é•¿æ–‡æœ¬é—®é¢˜çš„å›ç­”ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.20258', 'title': 'LLM as a Broken Telephone: Iterative Generation Distorts Information', 'url': 'https://huggingface.co/papers/2502.20258', 'abstract': 'As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.', 'score': 14, 'issue_id': 2580, 'pub_date': '2025-02-27', 'pub_date_card': {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'}, 'hash': '5b26055854ae3d90', 'authors': ['Amr Mohamed', 'Mingmeng Geng', 'Michalis Vazirgiannis', 'Guokan Shang'], 'affiliations': ['Ecole Polytechnique', 'MBZUAI', 'SISSA'], 'pdf_title_img': 'assets/pdf/title_img/2502.20258.jpg', 'data': {'categories': ['#hallucinations', '#data', '#long_context', '#alignment', '#multimodal', '#training'], 'emoji': 'ğŸ“', 'ru': {'title': "Ğ­Ñ„Ñ„ĞµĞºÑ‚ 'Ğ¸ÑĞ¿Ğ¾Ñ€Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ°' Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…", 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ÑĞºĞ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ½ĞµĞ¸Ğ·Ğ±ĞµĞ¶Ğ½Ğ°, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ¼ÑĞ³Ñ‡ĞµĞ½Ğ° Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¾ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°, Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ LLM Ğ² Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ñ….'}, 'en': {'title': 'Mitigating Distortion in Iterative LLM Outputs', 'desc': "This paper explores how large language models (LLMs) can distort information when they repeatedly process their own outputs, similar to the 'broken telephone' effect seen in human communication. The researchers conducted translation-based experiments to observe how information degradation occurs over time, which is affected by the choice of language and the complexity of the generation chain. They found that while some level of distortion is unavoidable, it can be reduced by using specific prompting strategies. These insights highlight the potential risks of relying on LLMs for generating content in iterative processes, raising concerns about the accuracy of AI-generated information."}, 'zh': {'title': 'æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¿¡æ¯æ‰­æ›²ä¸å¯é æ€§', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åå¤å¤„ç†è‡ªèº«è¾“å‡ºæ—¶æ˜¯å¦ä¼šæ‰­æ›²ä¿¡æ¯ï¼Œç±»ä¼¼äºäººç±»æ²Ÿé€šä¸­çš„â€œç ´ç”µè¯â€æ•ˆåº”ã€‚é€šè¿‡åŸºäºç¿»è¯‘çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ä¿¡æ¯æ‰­æ›²ä¼šéšç€æ—¶é—´çš„æ¨ç§»è€Œç´¯ç§¯ï¼Œå—è¯­è¨€é€‰æ‹©å’Œé“¾æ¡å¤æ‚æ€§çš„å½±å“ã€‚å°½ç®¡ä¿¡æ¯é€€åŒ–æ˜¯ä¸å¯é¿å…çš„ï¼Œä½†é€šè¿‡æˆ˜ç•¥æ€§æç¤ºæŠ€æœ¯å¯ä»¥å‡è½»è¿™ç§å½±å“ã€‚ç ”ç©¶ç»“æœä¸ºAIä»‹å¯¼çš„ä¿¡æ¯ä¼ æ’­çš„é•¿æœŸå½±å“æä¾›äº†é‡è¦è§è§£ï¼Œæå‡ºäº†å…³äºLLMç”Ÿæˆå†…å®¹åœ¨è¿­ä»£å·¥ä½œæµç¨‹ä¸­å¯é æ€§çš„é‡è¦é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02972', 'title': 'LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation', 'url': 'https://huggingface.co/papers/2503.02972', 'abstract': 'Effective evaluation of the reasoning capabilities of large language models (LLMs) are susceptible to overestimation due to data exposure of evaluation benchmarks. We introduce a framework for producing linguistic reasoning problems that reduces the effect of memorisation in model performance estimates and apply this framework to develop LINGOLY-TOO, a challenging evaluation benchmark for linguistic reasoning. By developing orthographic templates, we dynamically obfuscate the writing systems of real languages to generate numerous question variations. These variations preserve the reasoning steps required for each solution while reducing the likelihood of specific problem instances appearing in model training data. Our experiments demonstrate that frontier models, including OpenAI o1-preview and DeepSeem R1, struggle with advanced reasoning. Our analysis also shows that LLMs exhibit noticeable variance in accuracy across permutations of the same problem, and on average perform better on questions appearing in their original orthography. Our findings highlight the opaque nature of response generation in LLMs and provide evidence that prior data exposure contributes to overestimating the reasoning capabilities of frontier models.', 'score': 12, 'issue_id': 2585, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '0eb195e2704f3d5d', 'authors': ['Jude Khouja', 'Karolina Korgul', 'Simi Hellsten', 'Lingyi Yang', 'Vlad Neacs', 'Harry Mayne', 'Ryan Kearns', 'Andrew Bean', 'Adam Mahdi'], 'affiliations': ['Asia-Pacific Linguistics Olympiad', 'Hong Kong Linguistics Olympiad', 'National University of Science and Technology POLITEHNICA Bucharest, Romania', 'United Kingdom Linguistics Olympiad', 'University of Glasgow, Glasgow, United Kingdom', 'University of Oxford, Oxford, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.02972.jpg', 'data': {'categories': ['#reasoning', '#dataset', '#hallucinations', '#benchmark'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ LLM Ğ±ĞµĞ· Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº LINGOLY-TOO, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ Ğ¾Ğ±Ñ„ÑƒÑĞºĞ°Ñ†Ğ¸ĞµĞ¹ ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ OpenAI Ğ¸ DeepSeem, Ğ¸ÑĞ¿Ñ‹Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼ÑÑ‚Ğ²Ğ¾ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğº Ğ¿ĞµÑ€ĞµĞ¾Ñ†ĞµĞ½ĞºĞµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ LLM Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Unmasking Reasoning: Evaluating LLMs Beyond Memorization', 'desc': 'This paper addresses the challenge of accurately evaluating the reasoning abilities of large language models (LLMs) by introducing a new framework that minimizes the impact of memorization on performance assessments. The authors present LINGOLY-TOO, a benchmark designed to test linguistic reasoning through dynamically generated questions that obfuscate the original writing systems of languages. By using orthographic templates, they create multiple variations of questions that maintain the necessary reasoning steps while reducing the chance of models having seen specific instances during training. The results indicate that leading models struggle with complex reasoning tasks and show significant performance differences based on the question format, revealing the influence of prior data exposure on their evaluation.'}, 'zh': {'title': 'æ­ç¤ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„çœŸå®é¢è²Œ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å‡å°‘ç”±äºæ•°æ®æš´éœ²å¯¼è‡´çš„è¯„ä¼°è¿‡é«˜çš„é—®é¢˜ã€‚æˆ‘ä»¬å¼€å‘äº†LINGOLY-TOOï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è¯­è¨€æ¨ç†è¯„ä¼°åŸºå‡†ï¼Œé€šè¿‡ä½¿ç”¨æ­£å­—æ³•æ¨¡æ¿åŠ¨æ€æ¨¡ç³ŠçœŸå®è¯­è¨€çš„ä¹¦å†™ç³»ç»Ÿï¼Œç”Ÿæˆå¤šç§é—®é¢˜å˜ä½“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå‰æ²¿æ¨¡å‹åœ¨é«˜çº§æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ï¼Œå¹¶ä¸”åœ¨ç›¸åŒé—®é¢˜çš„ä¸åŒæ’åˆ—ä¸­ï¼ŒLLMsçš„å‡†ç¡®æ€§å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†LLMså“åº”ç”Ÿæˆçš„å¤æ‚æ€§ï¼Œå¹¶æä¾›äº†è¯æ®è¡¨æ˜ï¼Œå…ˆå‰çš„æ•°æ®æš´éœ²ä¼šå¯¼è‡´å¯¹å‰æ²¿æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é«˜ä¼°ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04130', 'title': 'Token-Efficient Long Video Understanding for Multimodal LLMs', 'url': 'https://huggingface.co/papers/2503.04130', 'abstract': 'Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8times and the decoding latency by 2.4-2.9times for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm', 'score': 9, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'f1d1afacd41dc0c7', 'authors': ['Jindong Jiang', 'Xiuyu Li', 'Zhijian Liu', 'Muyang Li', 'Guo Chen', 'Zhiqi Li', 'De-An Huang', 'Guilin Liu', 'Zhiding Yu', 'Kurt Keutzer', 'Sungjin Ahn', 'Jan Kautz', 'Hongxu Yin', 'Yao Lu', 'Song Han', 'Wonmin Byeon'], 'affiliations': ['KAIST', 'MIT', 'NVIDIA', 'Nanjing University', 'Rutgers University', 'UC Berkeley'], 'pdf_title_img': 'assets/pdf/title_img/2503.04130.jpg', 'data': {'categories': ['#reasoning', '#benchmark', '#optimization', '#long_context', '#architecture', '#video', '#inference', '#multimodal'], 'emoji': 'ğŸŒªï¸', 'ru': {'title': 'STORM: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ', 'desc': 'STORM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-LLM, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞ¼Ğ¿Ğ¾Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mamba State Space Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ LLM. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑÑ‰Ğ¸Ğµ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºÑƒ Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ğ´Ñ€Ğ°Ğ¼Ğ¸ Ğ²Ğ¾ Ğ²ÑĞµĞ¹ Ğ²Ğ¸Ğ´ĞµĞ¾Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. STORM Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ LLM. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ STORM Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ Ğ½Ğ° 5% Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ´Ğ¾ 8 Ñ€Ğ°Ğ·.'}, 'en': {'title': 'STORM: Revolutionizing Video Understanding with Temporal Insights', 'desc': 'This paper presents STORM, a new architecture designed to enhance video understanding in multimodal large language models (LLMs) by incorporating a temporal encoder. Unlike previous methods that treat video frames independently, STORM uses the Mamba State Space Model to capture the dynamics between frames, resulting in richer representations of video content. The architecture also implements token reduction strategies that significantly lower computational costs and improve processing speed without losing important temporal information. Evaluations demonstrate that STORM outperforms existing models on long video benchmarks while achieving substantial reductions in computation and latency.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘ç†è§£çš„æ–°çªç ´ï¼šSTORMæ¨¡å‹', 'desc': 'æœ€è¿‘ï¼ŒåŸºäºè§†é¢‘çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMsï¼‰åœ¨è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†è®¸å¤šç°æœ‰æ–¹æ³•åœ¨è§†è§‰éª¨å¹²ä¸­ç‹¬ç«‹å¤„ç†å¸§ï¼Œç¼ºä¹æ˜ç¡®çš„æ—¶é—´å»ºæ¨¡ï¼Œé™åˆ¶äº†æ•æ‰åŠ¨æ€æ¨¡å¼çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†STORMï¼ˆæ—¶ç©ºä»¤ç‰Œå‡å°‘æ¨¡å‹ï¼‰ï¼Œå®ƒåœ¨å›¾åƒç¼–ç å™¨å’Œå¤§è¯­è¨€æ¨¡å‹ä¹‹é—´å¼•å…¥äº†ä¸“é—¨çš„æ—¶é—´ç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ—¶é—´ç¼–ç å™¨åˆ©ç”¨MambaçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå°†æ—¶é—´ä¿¡æ¯æ•´åˆåˆ°å›¾åƒä»¤ç‰Œä¸­ï¼Œç”Ÿæˆä¸°å¯Œçš„è¡¨ç¤ºï¼Œä¿ç•™æ•´ä¸ªè§†é¢‘åºåˆ—ä¸­çš„å¸§é—´åŠ¨æ€ã€‚é€šè¿‡è¿™äº›æŠ€æœ¯çš„æ•´åˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—éœ€æ±‚å’Œæ¨ç†å»¶è¿Ÿï¼Œå®ç°äº†å¯¹é•¿è§†é¢‘çš„é«˜æ•ˆç†è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04598', 'title': 'HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization', 'url': 'https://huggingface.co/papers/2503.04598', 'abstract': 'Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose HybridNorm, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.', 'score': 9, 'issue_id': 2579, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '78b05ed4e27a874b', 'authors': ['Zhijian Zhuo', 'Yutao Zeng', 'Ya Wang', 'Sijun Zhang', 'Jian Yang', 'Xiaoqing Li', 'Xun Zhou', 'Jinwen Ma'], 'affiliations': ['Capital University of Economics and Business', 'School of Mathematical Sciences, Peking University', 'SeedFoundation-Model, ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2503.04598.jpg', 'data': {'categories': ['#benchmark', '#optimization', '#training', '#architecture', '#open_source'], 'emoji': 'ğŸ”€', 'ru': {'title': 'HybridNorm: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ HybridNorm. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Pre-Norm Ğ¸ Post-Norm ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ QKV Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Post-Norm Ğ² FFN ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°. HybridNorm Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ñ…, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚, Ñ‡Ñ‚Ğ¾ HybridNorm Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'HybridNorm: The Best of Both Normalization Worlds for Transformers', 'desc': 'This paper introduces HybridNorm, a new normalization strategy for training deep transformer networks, which combines the strengths of Pre-Norm and Post-Norm methods. Pre-Norm structures help with training stability but often lead to lower performance, while Post-Norm structures typically perform better but can complicate training. HybridNorm applies QKV normalization in the attention mechanism and Post-Norm in the feed-forward network, resulting in improved training stability and performance. Experiments demonstrate that HybridNorm outperforms both Pre-Norm and Post-Norm across various benchmarks, making it a promising approach for enhancing large language models.'}, 'zh': {'title': 'HybridNormï¼šæå‡å˜æ¢å™¨æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ä¸æ€§èƒ½', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ··åˆå½’ä¸€åŒ–ç­–ç•¥ï¼Œç§°ä¸ºHybridNormï¼Œæ—¨åœ¨è§£å†³æ·±åº¦å˜æ¢å™¨ç½‘ç»œè®­ç»ƒä¸­çš„å±‚å½’ä¸€åŒ–ä½ç½®é—®é¢˜ã€‚HybridNormç»“åˆäº†Pre-Normå’ŒPost-Normçš„ä¼˜ç‚¹ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨QKVå½’ä¸€åŒ–ï¼Œè€Œåœ¨å‰é¦ˆç½‘ç»œä¸­ä½¿ç”¨Post-Normã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒHybridNormåœ¨ç¨ å¯†å’Œç¨€ç–æ¶æ„ä¸­å‡ä¼˜äºä¼ ç»Ÿçš„Pre-Normå’ŒPost-Normæ–¹æ³•ï¼Œæå‡äº†å¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºæ·±åº¦å˜æ¢å™¨æ¨¡å‹çš„è®­ç»ƒå’Œæ€§èƒ½æ”¹è¿›æä¾›äº†æ–°çš„æ€è·¯ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04094', 'title': 'PokÃ©Champ: an Expert-level Minimax Language Agent', 'url': 'https://huggingface.co/papers/2503.04094', 'abstract': "We introduce Pok\\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\\'emon battles. Built on a general framework for two-player competitive games, Pok\\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\\'eChamp consistently outperforms the previous best LLM-based bot, Pok\\'ellmon powered by GPT-4o, with a 64% win rate. Pok\\'eChamp attains a projected Elo of 1300-1500 on the Pok\\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.", 'score': 7, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'bffe348d8443d4c2', 'authors': ['Seth Karten', 'Andy Luu Nguyen', 'Chi Jin'], 'affiliations': ['Department of Computer Science, Princeton University', 'Department of Electrical and Computer Engineering, Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04094.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#agents', '#games'], 'emoji': 'ğŸ®', 'ru': {'title': 'PokeChamp: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ˜Ğ˜ Ğ´Ğ»Ñ Ğ¸Ğ³Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ PokeChamp - Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞºÑĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLM) Ğ´Ğ»Ñ Ğ±Ğ¾ĞµĞ² Ğ² PokÃ©mon. PokeChamp Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ LLM Ğ´Ğ»Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ°, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ¿Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸Ğ³Ñ€Ñ‹ Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ GPT-4o Ğ°Ğ³ĞµĞ½Ñ‚ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾Ğ±ĞµĞ´ Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ² ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ±Ğ¾Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ² Ñ‚Ğ¾Ğ¿-30% - 10% Ğ¸Ğ³Ñ€Ğ¾ĞºĞ¾Ğ² Ğ½Ğ° Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğµ PokÃ©mon Showdown. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ±Ğ¾ĞµĞ² PokÃ©mon Ğ¸ ÑĞµÑ€Ğ¸Ğ¸ Ñ‚ĞµÑÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ñ.'}, 'en': {'title': "Pok'eChamp: Elevating PokÃ©mon Battles with LLMs", 'desc': "Pok'eChamp is a minimax agent designed for PokÃ©mon battles that utilizes Large Language Models (LLMs) to improve its decision-making process. It replaces traditional components of the minimax algorithm with LLMs for player action sampling, opponent modeling, and value function estimation, allowing it to better understand gameplay history and human strategies. The agent demonstrates impressive performance, achieving a 76% win rate against top LLM-based bots and a 64% win rate with a smaller model, showcasing its effectiveness in competitive play. Additionally, Pok'eChamp compiles a large dataset of over 3 million PokÃ©mon battles to create benchmarks for evaluating battling skills and encourages further research in integrating LLMs with game-theoretic approaches."}, 'zh': {'title': 'PokÃ©Champï¼šå®å¯æ¢¦å¯¹æˆ˜ä¸­çš„æ™ºèƒ½ä»£ç†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†PokÃ©Champï¼Œä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æå°æå¤§ç®—æ³•ä»£ç†ï¼Œç”¨äºå®å¯æ¢¦å¯¹æˆ˜ã€‚è¯¥ä»£ç†åˆ©ç”¨LLMsçš„é€šç”¨èƒ½åŠ›ï¼Œå¢å¼ºäº†æå°æå¤§æ ‘æœç´¢ï¼Œæ›¿ä»£äº†ç©å®¶åŠ¨ä½œé‡‡æ ·ã€å¯¹æ‰‹å»ºæ¨¡å’Œä»·å€¼å‡½æ•°ä¼°è®¡ç­‰å…³é”®æ¨¡å—ã€‚PokÃ©Champåœ¨ä¸éœ€è¦é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨æ¸¸æˆå†å²å’Œäººç±»çŸ¥è¯†ï¼Œå‡å°‘æœç´¢ç©ºé—´å¹¶è§£å†³éƒ¨åˆ†å¯è§‚æµ‹æ€§é—®é¢˜ã€‚ç»è¿‡è¯„ä¼°ï¼ŒPokÃ©Champåœ¨å®å¯æ¢¦å¯¹æˆ˜ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œèµ¢å¾—äº†76%çš„èƒœç‡ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤šæ™ºèƒ½ä½“é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04222', 'title': 'FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion', 'url': 'https://huggingface.co/papers/2503.04222', 'abstract': 'We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0.', 'score': 7, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c7d793a0b91efd5d', 'authors': ['Ziyi Yang', 'Fanqi Wan', 'Longguang Zhong', 'Canbin Huang', 'Guosheng Liang', 'Xiaojun Quan'], 'affiliations': ['School of Computer Science and Engineering, Sun Yat-sen University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04222.jpg', 'data': {'categories': ['#training', '#dataset', '#benchmark', '#small_models', '#rlhf', '#transfer_learning', '#open_source', '#optimization'], 'emoji': 'ğŸ”€', 'ru': {'title': 'Ğ¡Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ¼Ğ¾Ñ‰Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ', 'desc': 'FuseChat-3.0 Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½ Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… LLM Ğ² Ğ±Ğ¾Ğ»ĞµĞµ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²Ğ° ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑÑ‚Ğ°Ğ¿Ğ°: ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‚Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ FuseChat-3.0 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸Ñ€Ğ¾ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼, Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Llama-3.1-8B-Instruct Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑÑ€ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ° 6,8 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ² Ğ¿Ğ¾ 14 Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼.'}, 'en': {'title': 'Fusing Strengths for Smarter, Smaller Models', 'desc': 'FuseChat-3.0 is a collection of large language models (LLMs) that combines the strengths of various larger source models into smaller, more efficient target models. The training process involves supervised fine-tuning to align the target models with the source models, followed by Direct Preference Optimization to enhance performance based on preferences from multiple sources. This innovative approach leads to significant improvements in tasks like instruction following, general knowledge, mathematics, and coding. The results show an average performance boost of 6.8 points across 14 benchmarks, with particularly impressive gains in instruction-following tasks.'}, 'zh': {'title': 'èåˆå¤šæºæ¨¡å‹ï¼Œæå‡è¯­è¨€ç†è§£èƒ½åŠ›', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†FuseChat-3.0ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ•´åˆä¸åŒæ¥æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼˜åŠ¿è€Œå¼€å‘çš„æ›´ç´§å‡‘çš„ç›®æ ‡LLMå¥—ä»¶ã€‚æºæ¨¡å‹åŒ…æ‹¬å¼ºå¤§çš„Gemma-2-27B-itã€Mistral-Large-Instruct-2407ã€Qwen-2.5-72B-Instructå’ŒLlama-3.1-70B-Instructã€‚ç›®æ ‡æ¨¡å‹åˆ™é›†ä¸­åœ¨ä¸‰ç§å¹¿æ³›ä½¿ç”¨çš„å°å‹å˜ä½“ä¸Šï¼Œä»¥åŠä¸¤ä¸ªè¶…ç´§å‡‘é€‰é¡¹ã€‚é€šè¿‡ä¸“é—¨çš„æ•°æ®æ„å»ºåè®®å’Œä¸¤é˜¶æ®µçš„è®­ç»ƒæµç¨‹ï¼ŒFuseChat-3.0åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.03983', 'title': 'Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities', 'url': 'https://huggingface.co/papers/2503.03983', 'abstract': 'Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.', 'score': 6, 'issue_id': 2581, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '952e4cb72ec34df8', 'authors': ['Sreyan Ghosh', 'Zhifeng Kong', 'Sonal Kumar', 'S Sakshi', 'Jaehyeon Kim', 'Wei Ping', 'Rafael Valle', 'Dinesh Manocha', 'Bryan Catanzaro'], 'affiliations': ['NVIDIA, Santa Clara, CA, USA', 'University of Maryland, College Park, MD, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.03983.jpg', 'data': {'categories': ['#audio', '#long_context', '#dataset', '#small_models', '#reasoning', '#synthetic', '#open_source', '#benchmark'], 'emoji': 'ğŸµ', 'ru': {'title': 'AF2: Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'Audio Flamingo 2 (AF2) - ÑÑ‚Ğ¾ ÑƒÑĞ¾Ğ²ĞµÑ€ÑˆĞµĞ½ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (ALM) Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğ¼Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ± Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ CLAP, ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Audio QA Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚ÑƒÑ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. AF2 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² ÑÑ€ĞµĞ´Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 20 Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ 3 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… LongAudio Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ALM Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Audio Flamingo 2', 'desc': 'This paper presents Audio Flamingo 2 (AF2), an advanced Audio-Language Model (ALM) designed to enhance audio understanding and reasoning. AF2 utilizes a custom CLAP model and synthetic Audio QA data to improve fine-grained audio reasoning, along with a multi-stage curriculum learning approach. The model achieves state-of-the-art results with a relatively small 3B parameter language model, outperforming larger models on over 20 benchmarks. Additionally, it introduces LongAudio, a new dataset for training on long audio segments, and demonstrates exceptional performance on the LongAudioBench for evaluating long audio understanding.'}, 'zh': {'title': 'éŸ³é¢‘ç†è§£çš„æ–°çªç ´ï¼šAudio Flamingo 2', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Audio Flamingo 2ï¼ˆAF2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰å…ˆè¿›éŸ³é¢‘ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆALMï¼‰ã€‚AF2åˆ©ç”¨äº†å®šåˆ¶çš„CLAPæ¨¡å‹ã€åˆæˆçš„éŸ³é¢‘é—®ç­”æ•°æ®ä»¥åŠå¤šé˜¶æ®µçš„è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚AF2åœ¨ä»…ä½¿ç”¨ä¸€ä¸ª3Bå‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œè¶…è¶Šäº†20å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å¤§å‹å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAF2é¦–æ¬¡æ‰©å±•äº†å¯¹é•¿éŸ³é¢‘ç‰‡æ®µï¼ˆ30ç§’åˆ°5åˆ†é’Ÿï¼‰çš„ç†è§£ï¼Œå¹¶æå‡ºäº†LongAudioæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒALMåœ¨é•¿éŸ³é¢‘æ ‡æ³¨å’Œé—®ç­”ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04644', 'title': 'IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval', 'url': 'https://huggingface.co/papers/2503.04644', 'abstract': 'We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.', 'score': 4, 'issue_id': 2585, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c4f19dc17abc0e8f', 'authors': ['Tingyu Song', 'Guo Gan', 'Mingsheng Shang', 'Yilun Zhao'], 'affiliations': ['Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences', 'School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences', 'Yale University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04644.jpg', 'data': {'categories': ['#science', '#healthcare', '#dataset', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'IFIR: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'IFIR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ² ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 2426 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ² 8 Ğ¿Ğ¾Ğ´Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²Ğ°Ñ… Ğ¸Ğ· 4 ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ²: Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹, Ğ¿Ñ€Ğ°Ğ²Ğ¾, Ğ·Ğ´Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ°Ñ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°. IFIR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ½Ğ° 15 Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹.'}, 'en': {'title': 'IFIR: Benchmarking Instruction-Following in Expert Domains', 'desc': 'The paper presents IFIR, a new benchmark for assessing how well information retrieval systems can follow instructions in specialized fields like finance, law, healthcare, and science. It consists of 2,426 examples that simulate real-world retrieval tasks requiring tailored instructions. The benchmark allows for a nuanced evaluation of retrieval models, particularly focusing on their ability to handle varying complexities of instructions. The authors also introduce a new evaluation method using large language models (LLMs) to measure performance, revealing that existing models struggle with complex, domain-specific tasks and providing insights for future improvements.'}, 'zh': {'title': 'IFIRï¼šä¸“å®¶é¢†åŸŸæŒ‡ä»¤è·Ÿéšæ£€ç´¢çš„é¦–ä¸ªåŸºå‡†', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†IFIRï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…¨é¢çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°ä¸“å®¶é¢†åŸŸä¸­çš„æŒ‡ä»¤è·Ÿéšä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ã€‚IFIRåŒ…å«2426ä¸ªé«˜è´¨é‡ç¤ºä¾‹ï¼Œæ¶µç›–é‡‘èã€æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®ç­‰å››ä¸ªä¸“ä¸šé¢†åŸŸçš„å…«ä¸ªå­é›†ã€‚æ¯ä¸ªå­é›†é’ˆå¯¹ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å®šé¢†åŸŸçš„æ£€ç´¢ä»»åŠ¡ï¼Œæ¨¡æ‹Ÿäº†éœ€è¦å®šåˆ¶æŒ‡ä»¤çš„çœŸå®åœºæ™¯ã€‚é€šè¿‡å¯¹15ä¸ªå‰æ²¿æ£€ç´¢æ¨¡å‹çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ¨¡å‹åœ¨æœ‰æ•ˆè·Ÿéšå¤æ‚çš„é¢†åŸŸç‰¹å®šæŒ‡ä»¤æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01901', 'title': 'Identifying Sensitive Weights via Post-quantization Integral', 'url': 'https://huggingface.co/papers/2503.01901', 'abstract': "Serving Large Language Models (LLMs) is costly. However, post-training weight quantization can address this problem by both compressing their sizes for limited memory and saving bandwidth for acceleration. As not all weight dimensions are equally important, those methods typically rely on a sensitivity metric, which indicates the element-wise influence of weights on loss function and is used to preprocess original weights for better quantization. In this work, we conduct an empirical study on the accuracy of the sensitivity metric, and find that existing gradient and Hessian based metrics are very inaccurate: they underestimate quantization's impact on the loss function by orders of magnitude, mainly due to the small convergence radius of local 2nd order approximation, \\ie, gradient and Hessian term in Taylor's formula. To tackle this problem, we propose Post-quantization Integral (PQI), an accurate metric to estimate posterior sensitivity in a fine-grained manner. To leverage this accurate metric, we further propose ReQuant, a simple yet powerful framework that mainly consists of two Dense-and-Sparse detach components: self-adaptive outlier selection and step-wise significant weights detach. Results show that ReQuant boosts state-of-the-art post-training quantization methods, with a pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.", 'score': 4, 'issue_id': 2585, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 28', 'zh': '2æœˆ28æ—¥'}, 'hash': '1045983ed982a00b', 'authors': ['Yuezhou Hu', 'Weiyu Huang', 'Zichen Liang', 'Chang Chen', 'Jintao Zhang', 'Jun Zhu', 'Jianfei Chen'], 'affiliations': ['Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center, THBI Lab, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01901.jpg', 'data': {'categories': ['#training', '#inference', '#optimization'], 'emoji': 'âš–ï¸', 'ru': {'title': 'Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ ReQuant. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ - Post-quantization Integral (PQI). ReQuant Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ PQI Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ¸ Ğ¿Ğ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ñ… Ğ²ĞµÑĞ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ReQuant ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ÑÑ‚-Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Llama 3.2 1B.'}, 'en': {'title': 'Enhancing LLM Efficiency with Accurate Quantization Metrics', 'desc': 'This paper addresses the high costs associated with serving large language models (LLMs) by introducing post-training weight quantization techniques. It highlights the inadequacy of existing sensitivity metrics, which fail to accurately predict the impact of quantization on model performance. The authors propose a new metric called Post-quantization Integral (PQI) that provides a more precise estimation of weight sensitivity. Additionally, they introduce ReQuant, a framework that enhances quantization methods by effectively selecting significant weights and improving overall model accuracy.'}, 'zh': {'title': 'æå‡é‡åŒ–ç²¾åº¦ï¼Œé™ä½æ¨¡å‹æˆæœ¬', 'desc': 'è¿™ç¯‡è®ºæ–‡è®¨è®ºäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœåŠ¡æ—¶çš„é«˜æˆæœ¬é—®é¢˜ã€‚é€šè¿‡åè®­ç»ƒæƒé‡é‡åŒ–ï¼Œå¯ä»¥å‹ç¼©æ¨¡å‹å¤§å°ï¼ŒèŠ‚çœå†…å­˜å’Œå¸¦å®½ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„åŸºäºæ¢¯åº¦å’Œæµ·æ£®çŸ©é˜µçš„æ•æ„Ÿåº¦åº¦é‡ä¸å¤Ÿå‡†ç¡®ï¼Œä½ä¼°äº†é‡åŒ–å¯¹æŸå¤±å‡½æ•°çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†åé‡åŒ–ç§¯åˆ†ï¼ˆPQIï¼‰ä½œä¸ºä¸€ç§æ›´ç²¾ç¡®çš„æ•æ„Ÿåº¦åº¦é‡ï¼Œå¹¶è¿›ä¸€æ­¥æå‡ºäº†ReQuantæ¡†æ¶ï¼Œä»¥æé«˜åè®­ç»ƒé‡åŒ–æ–¹æ³•çš„æ•ˆæœã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04725', 'title': 'L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling', 'url': 'https://huggingface.co/papers/2503.04725', 'abstract': "We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L^2M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.", 'score': 4, 'issue_id': 2582, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '51e8f1332666da31', 'authors': ['Zhuo Chen', 'Oriol MaynÃ© i Comas', 'Zhuotao Jin', 'Di Luo', 'Marin SoljaÄiÄ‡'], 'affiliations': ['Harvard University', 'Massachusetts Institute of Technology', 'NSF AI Institute for Artificial Intelligence and Fundamental Interactions', 'Polytechnic University of Catalonia', 'University of California, Los Angeles'], 'pdf_title_img': 'assets/pdf/title_img/2503.04725.jpg', 'data': {'categories': ['#training', '#architecture', '#math', '#long_context'], 'emoji': 'ğŸ“', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ - ĞºĞ»ÑÑ‡ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒÑÑ‰ĞµĞµÑÑ Ğ½Ğ° Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞºÑÑ‚Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğµ L^2M Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ²ÑĞ·Ñ‹Ğ²Ğ°ÑÑ‰ĞµĞµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ĞµĞµ ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ³Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ… Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ° ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ·Ğ°ĞºĞ»Ğ°Ğ´Ñ‹Ğ²Ğ°ĞµÑ‚ Ñ‚ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑĞ½Ğ¾Ğ²Ñƒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'Unlocking Long-Range Dependencies in Language Models', 'desc': "This paper introduces a new scaling law for bipartite mutual information that is crucial for understanding long-range dependencies in natural language processing. Unlike traditional two-point mutual information, this scaling law operates independently and is essential for effective long-context language modeling. The authors propose the Long-context Language Modeling (L^2M) condition, which connects a model's ability to handle long contexts with the size of its latent state for retaining past information. Experimental validation on transformers and state space models supports the theoretical framework, paving the way for advancements in large language models with extended context lengths."}, 'zh': {'title': 'é•¿ä¸Šä¸‹æ–‡å»ºæ¨¡çš„æ–°æ³•åˆ™', 'desc': 'æœ¬æ–‡ä¸¥è°¨åœ°å»ºç«‹äº†è‡ªç„¶è¯­è¨€ä¸­çš„åŒå‘äº’ä¿¡æ¯ç¼©æ”¾æ³•åˆ™ï¼Œè¯¥æ³•åˆ™æ§åˆ¶ç€é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸€ç¼©æ”¾æ³•åˆ™ä¸ä¼ ç»Ÿçš„ä¸¤ç‚¹äº’ä¿¡æ¯ä¸åŒï¼Œå¹¶ä¸”ç‹¬ç«‹ç¼©æ”¾ï¼Œæ˜¯ç†è§£é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡çš„å…³é”®ã€‚é€šè¿‡è¿™ä¸€ç¼©æ”¾æ³•åˆ™ï¼Œæˆ‘ä»¬æå‡ºäº†é•¿ä¸Šä¸‹æ–‡è¯­è¨€å»ºæ¨¡ï¼ˆL^2Mï¼‰æ¡ä»¶ï¼Œå°†æ¨¡å‹æœ‰æ•ˆå»ºæ¨¡é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„èƒ½åŠ›ä¸å…¶å­˜å‚¨è¿‡å»ä¿¡æ¯çš„æ½œåœ¨çŠ¶æ€å¤§å°çš„ç¼©æ”¾è”ç³»èµ·æ¥ã€‚æˆ‘ä»¬çš„ç»“æœé€šè¿‡å¯¹å˜æ¢å™¨å’ŒçŠ¶æ€ç©ºé—´æ¨¡å‹çš„å®éªŒå¾—åˆ°äº†éªŒè¯ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å‘æ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„å‘å±•å¥ å®šäº†ç†è®ºåŸºç¡€ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04606', 'title': 'The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation', 'url': 'https://huggingface.co/papers/2503.04606', 'abstract': 'Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a sim14,000times compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.', 'score': 4, 'issue_id': 2580, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': 'c635690f3edc1186', 'authors': ['Aoxiong Yin', 'Kai Shen', 'Yichong Leng', 'Xu Tan', 'Xinyu Zhou', 'Juncheng Li', 'Siliang Tang'], 'affiliations': ['College of Computer Science and Technology, Zhejiang University, Hangzhou, China', 'Moonshot AI, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04606.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#benchmark', '#long_context', '#architecture', '#video'], 'emoji': 'ğŸ¬', 'ru': {'title': 'LanDiff: Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ LanDiff - Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¾Ñ‡ĞµÑ‚Ğ°ÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. LanDiff Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€, ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²ÑƒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ LanDiff Ğ¾Ğ±ÑŠĞµĞ¼Ğ¾Ğ¼ 5 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ VBench T2V. Ğ¢Ğ°ĞºĞ¶Ğµ LanDiff Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ»ÑƒÑ‡ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'LanDiff: Bridging Language and Visuals for Superior Video Generation', 'desc': 'This paper introduces LanDiff, a novel framework for text-to-video (T2V) generation that combines the strengths of autoregressive language models and diffusion models. It addresses the limitations of each approach by using a coarse-to-fine generation strategy, which enhances both visual quality and semantic understanding. Key innovations include a semantic tokenizer for efficient compression of visual features, a language model that generates meaningful semantic tokens, and a streaming diffusion model that refines these tokens into high-quality videos. The results demonstrate that LanDiff outperforms existing state-of-the-art models in T2V tasks, particularly in generating long videos.'}, 'zh': {'title': 'LanDiffï¼šæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºLanDiffçš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆè‡ªå›å½’è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œä»¥å®ç°æ–‡æœ¬åˆ°è§†é¢‘çš„ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡ç²—åˆ°ç»†çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œå…‹æœäº†å„è‡ªçš„å±€é™æ€§ï¼Œæå‡äº†è§†è§‰è´¨é‡å’Œè¯­ä¹‰ç†è§£ã€‚LanDiffå¼•å…¥äº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼ŒåŒ…æ‹¬é«˜æ•ˆçš„è¯­ä¹‰å‹ç¼©æŠ€æœ¯ã€ç”Ÿæˆé«˜å±‚è¯­ä¹‰å…³ç³»çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥åŠå°†ç²—ç•¥è¯­ä¹‰ç²¾ç‚¼ä¸ºé«˜ä¿çœŸè§†é¢‘çš„æµå¼æ‰©æ•£æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLanDiffåœ¨VBench T2VåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºå’Œå•†ä¸šæ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04369', 'title': 'Lost in Literalism: How Supervised Training Shapes Translationese in LLMs', 'url': 'https://huggingface.co/papers/2503.04369', 'abstract': 'Large language models (LLMs) have achieved remarkable success in machine translation, demonstrating impressive performance across diverse languages. However, translationese, characterized by overly literal and unnatural translations, remains a persistent challenge in LLM-based translation systems. Despite their pre-training on vast corpora of natural utterances, LLMs exhibit translationese errors and generate unexpected unnatural translations, stemming from biases introduced during supervised fine-tuning (SFT). In this work, we systematically evaluate the prevalence of translationese in LLM-generated translations and investigate its roots during supervised training. We introduce methods to mitigate these biases, including polishing golden references and filtering unnatural training instances. Empirical evaluations demonstrate that these approaches significantly reduce translationese while improving translation naturalness, validated by human evaluations and automatic metrics. Our findings highlight the need for training-aware adjustments to optimize LLM translation outputs, paving the way for more fluent and target-language-consistent translations. We release the data and code at https://github.com/yafuly/LLM_Translationese.', 'score': 3, 'issue_id': 2586, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '7e6f548766f04dbf', 'authors': ['Yafu Li', 'Ronghao Zhang', 'Zhilin Wang', 'Huajian Zhang', 'Leyang Cui', 'Yongjing Yin', 'Tong Xiao', 'Yue Zhang'], 'affiliations': ['Northeastern University', 'Shanghai AI Laboratory', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2503.04369.jpg', 'data': {'categories': ['#training', '#multilingual', '#machine_translation', '#data'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Ğ‘Ğ¾Ñ€ÑŒĞ±Ğ° Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ¼: Ğ¿ÑƒÑ‚ÑŒ Ğº ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ°Ğ¼ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (translationese) Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´ÑƒÑÑ‚ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ°, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ² Ğ¸ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ­Ğ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ÑÑ‚ Ğ¿Ñ€Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ². Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ĞµÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ², Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ñ… LLM.'}, 'en': {'title': 'Reducing Translationese for Natural Language Translations', 'desc': 'This paper addresses the issue of translationese in large language models (LLMs) used for machine translation, which leads to unnatural and overly literal translations. The authors evaluate how prevalent translationese is in LLM outputs and identify biases introduced during supervised fine-tuning (SFT) as a key factor. They propose methods to reduce these biases, such as refining reference translations and filtering out unnatural training examples. Their empirical results show that these strategies significantly enhance the naturalness of translations, suggesting that careful adjustments during training can improve LLM performance in translation tasks.'}, 'zh': {'title': 'ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ç¿»è¯‘ï¼Œå‡å°‘ç¿»è¯‘è…”', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨ç¿»è¯‘ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†ä»é¢ä¸´ç¿»è¯‘è…”çš„é—®é¢˜ï¼Œå³ç¿»è¯‘è¿‡äºå­—é¢å’Œä¸è‡ªç„¶ã€‚å°½ç®¡åœ¨å¤§é‡è‡ªç„¶è¯­æ–™ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼ŒLLMsåœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­å¼•å…¥çš„åå·®å¯¼è‡´äº†ç¿»è¯‘è…”é”™è¯¯ã€‚æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†LLMç”Ÿæˆç¿»è¯‘ä¸­çš„ç¿»è¯‘è…”ç°è±¡ï¼Œå¹¶æ¢è®¨äº†å…¶åœ¨ç›‘ç£è®­ç»ƒä¸­çš„æ ¹æºã€‚æˆ‘ä»¬æå‡ºäº†å‡è½»è¿™äº›åå·®çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®è¯è¯„ä¼°è¯æ˜è¿™äº›æ–¹æ³•æ˜¾è‘—é™ä½äº†ç¿»è¯‘è…”ï¼Œæé«˜äº†ç¿»è¯‘çš„è‡ªç„¶æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.04378', 'title': 'Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks', 'url': 'https://huggingface.co/papers/2503.04378', 'abstract': 'Inference-Time Scaling has been critical to the success of recent models such as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for inference-time scaling require tasks to have answers that can be verified, limiting their application to domains such as math, coding and logical reasoning. We take inspiration from how humans make first attempts, ask for detailed feedback from others and make improvements based on such feedback across a wide spectrum of open-ended endeavors. To this end, we collect data for and train dedicated Feedback and Edit Models that are capable of performing inference-time scaling for open-ended general-domain tasks. In our setup, one model generates an initial response, which are given feedback by a second model, that are then used by a third model to edit the response. We show that performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo can be boosted by scaling the number of initial response drafts, effective feedback and edited responses. When scaled optimally, our setup based on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and DeepSeek R1 with 92.3.', 'score': 3, 'issue_id': 2578, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 6', 'zh': '3æœˆ6æ—¥'}, 'hash': '926e56aa51a0fefe', 'authors': ['Zhilin Wang', 'Jiaqi Zeng', 'Olivier Delalleau', 'Daniel Egert', 'Ellie Evans', 'Hoo-Chang Shin', 'Felipe Soares', 'Yi Dong', 'Oleksii Kuchaiev'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.04378.jpg', 'data': {'categories': ['#training', '#benchmark', '#inference', '#reasoning', '#rlhf', '#optimization'], 'emoji': 'ğŸ”„', 'ru': {'title': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡: Ğ¾Ñ‚ Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ¸ĞºĞ° Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñƒ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ‰ĞµĞ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¸Ğ· Ñ‚Ñ€ĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹: Ğ¾Ğ´Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚, Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½ÑƒÑ ÑĞ²ÑĞ·ÑŒ, Ğ° Ñ‚Ñ€ĞµÑ‚ÑŒÑ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ²ÑĞ·Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Arena Hard. ĞŸÑ€Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ° Llama 3 Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ¼ 70B Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ², Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ OpenAI o1 Ğ¸ DeepSeek R1.'}, 'en': {'title': 'Enhancing Open-Ended Task Performance through Feedback and Editing', 'desc': 'This paper discusses a novel approach to improve inference-time scaling in machine learning models, particularly for open-ended tasks. It introduces a system where one model generates an initial response, a second model provides feedback, and a third model edits the response based on that feedback. This method allows for more flexible applications beyond traditional tasks that require verifiable answers, such as math or coding. The authors demonstrate that by optimizing the number of drafts, feedback, and edits, their model achieves state-of-the-art performance on the Arena Hard benchmark, outperforming previous models.'}, 'zh': {'title': 'æ¨ç†æ—¶é—´æ‰©å±•ï¼šæå‡å¼€æ”¾æ€§ä»»åŠ¡çš„æ€§èƒ½', 'desc': 'æœ¬æ–‡æ¢è®¨äº†æ¨ç†æ—¶é—´æ‰©å±•åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„é‡è¦æ€§ï¼Œå°¤å…¶æ˜¯OpenAI o1å’ŒDeepSeek R1ç­‰æ¨¡å‹ã€‚è®¸å¤šç°æœ‰æŠ€æœ¯è¦æ±‚ä»»åŠ¡çš„ç­”æ¡ˆå¯éªŒè¯ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¼€æ”¾æ€§ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚æˆ‘ä»¬å€Ÿé‰´äººç±»å¦‚ä½•è¿›è¡Œåˆæ­¥å°è¯•ã€è¯·æ±‚åé¦ˆå¹¶æ ¹æ®åé¦ˆè¿›è¡Œæ”¹è¿›çš„è¿‡ç¨‹ï¼Œå¼€å‘äº†ä¸“é—¨çš„åé¦ˆå’Œç¼–è¾‘æ¨¡å‹ã€‚é€šè¿‡ä¼˜åŒ–åˆå§‹å“åº”è‰ç¨¿ã€æœ‰æ•ˆåé¦ˆå’Œç¼–è¾‘å“åº”çš„æ•°é‡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨Arena HardåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†92.7çš„æœ€æ–°æ€§èƒ½ï¼Œè¶…è¶Šäº†å…¶ä»–æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02495', 'title': 'Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer', 'url': 'https://huggingface.co/papers/2503.02495', 'abstract': "Mixture-of-Experts (MoE) enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement dynamic routing on input data and experts. Our approach advances MoE design with three key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the model employed with UoE surpass Full Attention, state-of-art MoEs and efficient transformers in several tasks across image and natural language domains. The source codes are available at https://github.com/YujiaoYang-work/UoE.", 'score': 2, 'issue_id': 2586, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': 'b879dd88adfef125', 'authors': ['Yujiao Yang', 'Jing Lian', 'Linhui Li'], 'affiliations': ['School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.02495.jpg', 'data': {'categories': ['#training', '#optimization', '#architecture', '#open_source'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'UoE: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ², Ğ½Ğ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ğ¹ Union-of-Experts (UoE). UoE ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Mixture-of-Experts (MoE), Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° Ğ³Ñ€ÑƒĞ¿Ğ¿Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ°Ğº Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼, Ñ‚Ğ°Ğº Ğ¸ Ğº ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ´ĞµĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² MLP Ğ¸ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ UoE Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Union-of-Experts: Enhancing Efficiency and Interaction in Transformers', 'desc': 'The paper introduces Union-of-Experts (UoE), a novel approach that enhances the Mixture-of-Experts (MoE) framework by improving expert interactions and extending its application to attention blocks. UoE decomposes transformers into groups of experts and utilizes dynamic routing to optimize input data processing. Key innovations include equitant expert decomposition, two routing paradigms for data and expert selection, and a new architecture featuring Selective Multi-Head Attention and Union-of-MLP-Experts. Experimental results show that UoE outperforms existing models in various tasks, demonstrating its effectiveness in both image and natural language processing.'}, 'zh': {'title': 'ä¸“å®¶è”åˆæ¨¡å‹ï¼šæå‡æ•ˆç‡ä¸æ€§èƒ½çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰é€šè¿‡æé«˜æ¨¡å‹æ€§èƒ½å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ï¼Œé€‚åˆå¤§è§„æ¨¡åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MoEæ¨¡å‹ä¸­çš„ä¸“å®¶ä½œä¸ºä¸ªä½“å·¥ä½œï¼Œç¼ºä¹é«˜è´¨é‡çš„ä¸“å®¶äº¤äº’ã€‚æ­¤å¤–ï¼ŒMoEå°šæœªæœ‰æ•ˆæ‰©å±•åˆ°æ³¨æ„åŠ›æ¨¡å—ï¼Œè¿™é™åˆ¶äº†è¿›ä¸€æ­¥çš„æ•ˆç‡æå‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“å®¶è”åˆæ¨¡å‹ï¼ˆUoEï¼‰ï¼Œé€šè¿‡å°†å˜æ¢å™¨åˆ†è§£ä¸ºç­‰æ•ˆçš„ä¸“å®¶ç»„ï¼Œå¹¶åœ¨è¾“å…¥æ•°æ®å’Œä¸“å®¶ä¹‹é—´å®ç°åŠ¨æ€è·¯ç”±ï¼Œä»è€Œæ”¹è¿›äº†MoEçš„è®¾è®¡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.01375', 'title': 'Combining Flow Matching and Transformers for Efficient Solution of Bayesian Inverse Problems', 'url': 'https://huggingface.co/papers/2503.01375', 'abstract': 'Solving Bayesian inverse problems efficiently remains a significant challenge due to the complexity of posterior distributions and the computational cost of traditional sampling methods. Given a series of observations and the forward model, we want to recover the distribution of the parameters, conditioned on observed experimental data. We show, that combining Conditional Flow Mathching (CFM) with transformer-based architecture, we can efficiently sample from such kind of distribution, conditioned on variable number of observations.', 'score': 2, 'issue_id': 2583, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 3', 'zh': '3æœˆ3æ—¥'}, 'hash': '9dd35b1faaa32b61', 'authors': ['Daniil Sherki', 'Ivan Oseledets', 'Ekaterina Muravleva'], 'affiliations': ['Artificial Intelligence Research Institute', 'Sberbank, AI4S Center', 'Skolkovo Institute of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.01375.jpg', 'data': {'categories': ['#architecture', '#math'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ° Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ CFM Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ±Ğ°Ğ¹ĞµÑĞ¾Ğ²ÑĞºĞ¸Ñ… Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Conditional Flow Matching (CFM) Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ¾Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ². Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ğ»ÑÑ‚ÑŒ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºÑƒ Ğ¸Ğ· ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¸Ğ¾Ñ€Ğ½Ñ‹Ñ… Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğ¹, Ğ¾Ğ±ÑƒÑĞ»Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ²Ğ°ĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸, ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Efficient Bayesian Inference with CFM and Transformers', 'desc': 'This paper addresses the challenge of efficiently solving Bayesian inverse problems, which involve estimating parameter distributions based on observed data. Traditional sampling methods can be computationally expensive, especially when dealing with complex posterior distributions. The authors propose a novel approach that combines Conditional Flow Matching (CFM) with transformer-based architectures to improve sampling efficiency. This method allows for effective sampling from parameter distributions conditioned on a variable number of observations, enhancing the ability to recover accurate parameter estimates.'}, 'zh': {'title': 'é«˜æ•ˆè´å¶æ–¯é€†é—®é¢˜æ±‚è§£çš„æ–°æ–¹æ³•', 'desc': 'è§£å†³è´å¶æ–¯é€†é—®é¢˜çš„æ•ˆç‡ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºåéªŒåˆ†å¸ƒçš„å¤æ‚æ€§å’Œä¼ ç»Ÿé‡‡æ ·æ–¹æ³•çš„è®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨ç»™å®šä¸€ç³»åˆ—è§‚æµ‹å’Œå‰å‘æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ¢å¤å‚æ•°çš„åˆ†å¸ƒï¼Œè¿™äº›åˆ†å¸ƒæ˜¯åŸºäºè§‚å¯Ÿåˆ°çš„å®éªŒæ•°æ®ã€‚æˆ‘ä»¬å±•ç¤ºäº†å°†æ¡ä»¶æµåŒ¹é…ï¼ˆCFMï¼‰ä¸åŸºäºå˜æ¢å™¨çš„æ¶æ„ç›¸ç»“åˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»è¿™ç§åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œä¸”è¯¥åˆ†å¸ƒå¯ä»¥æ ¹æ®è§‚æµ‹æ•°é‡çš„å˜åŒ–è€Œå˜åŒ–ã€‚æ­¤æ–¹æ³•ä¸ºè´å¶æ–¯æ¨æ–­æä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆè§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2503.02191', 'title': 'Understanding and Predicting Derailment in Toxic Conversations on GitHub', 'url': 'https://huggingface.co/papers/2503.02191', 'abstract': 'Software projects thrive on the involvement and contributions of individuals from different backgrounds. However, toxic language and negative interactions can hinder the participation and retention of contributors and alienate newcomers. Proactive moderation strategies aim to prevent toxicity from occurring by addressing conversations that have derailed from their intended purpose. This study aims to understand and predict conversational derailment leading to toxicity on GitHub.   To facilitate this research, we curate a novel dataset comprising 202 toxic conversations from GitHub with annotated derailment points, along with 696 non-toxic conversations as a baseline. Based on this dataset, we identify unique characteristics of toxic conversations and derailment points, including linguistic markers such as second-person pronouns, negation terms, and tones of Bitter Frustration and Impatience, as well as patterns in conversational dynamics between project contributors and external participants.   Leveraging these empirical observations, we propose a proactive moderation approach to automatically detect and address potentially harmful conversations before escalation. By utilizing modern LLMs, we develop a conversation trajectory summary technique that captures the evolution of discussions and identifies early signs of derailment. Our experiments demonstrate that LLM prompts tailored to provide summaries of GitHub conversations achieve 69% F1-Score in predicting conversational derailment, strongly improving over a set of baseline approaches.', 'score': 2, 'issue_id': 2581, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'en': 'March 4', 'zh': '3æœˆ4æ—¥'}, 'hash': '1d0bc1069249c9e1', 'authors': ['Mia Mohammad Imran', 'Robert Zita', 'Rebekah Copeland', 'Preetha Chatterjee', 'Rahat Rizvi Rahman', 'Kostadin Damevski'], 'affiliations': ['Drexel University, Philadelphia, PA, USA', 'Eastern Mennonite University, Harrisonburg, VA, USA', 'Elmhurst University, Elmhurst, IL, USA', 'Missouri University of Science and Technology, Rolla, MO, USA', 'Virginia Commonwealth University, Richmond, VA, USA'], 'pdf_title_img': 'assets/pdf/title_img/2503.02191.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multimodal', '#data'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ˜Ğ˜ Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ¶Ğµ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²Ğ¾Ğ¹ Ğ°Ñ‚Ğ¼Ğ¾ÑÑ„ĞµÑ€Ñ‹ Ğ² open-source ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ°Ñ…', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°Ñ… Ğ½Ğ° GitHub. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸Ğ· 202 Ñ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸ 696 Ğ½ĞµÑ‚Ğ¾ĞºÑĞ¸Ñ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ², Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ñ‹ Ğ¸ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ´ĞµÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ±Ñ‹Ğ»Ğ° Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ñ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 69% F1-Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ½Ğ½Ğ¸Ñ… ÑÑ‚Ğ°Ğ´Ğ¸ÑÑ….'}, 'en': {'title': 'Proactive Moderation: Detecting Toxicity Before It Escalates', 'desc': 'This paper investigates how toxic language in GitHub conversations can deter contributors and newcomers. It introduces a dataset of 202 toxic conversations with marked derailment points, alongside 696 non-toxic examples for comparison. The study identifies specific linguistic features and conversational dynamics that signal potential toxicity. Using large language models (LLMs), the authors propose a proactive moderation strategy that summarizes conversation trajectories to detect early signs of derailment, achieving a 69% F1-Score in predictions.'}, 'zh': {'title': 'ä¸»åŠ¨ç®¡ç†ï¼Œé˜²æ­¢å¯¹è¯åç¦»ä¸æœ‰æ¯’è¯­è¨€', 'desc': 'æœ¬ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åœ¨GitHubä¸Šé¢„æµ‹å’Œç†è§£å¯¹è¯åç¦»å¯¼è‡´çš„æœ‰æ¯’è¯­è¨€ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«202ä¸ªæœ‰æ¯’å¯¹è¯å’Œ696ä¸ªéæœ‰æ¯’å¯¹è¯ï¼Œå¹¶æ ‡æ³¨äº†åç¦»ç‚¹ã€‚é€šè¿‡åˆ†æè¿™äº›å¯¹è¯çš„è¯­è¨€ç‰¹å¾å’ŒåŠ¨æ€æ¨¡å¼ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºæœ‰æ¯’å¯¹è¯çš„ç‹¬ç‰¹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸»åŠ¨çš„ç®¡ç†ç­–ç•¥ï¼Œåˆ©ç”¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æ£€æµ‹å’Œå¤„ç†æ½œåœ¨çš„æœ‰å®³å¯¹è¯ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (2)', '#agi', '#alignment (1)', '#architecture (7)', '#audio (2)', '#benchmark (10)', '#cv', '#data (4)', '#dataset (7)', '#diffusion (1)', '#ethics (1)', '#games (1)', '#graphs', '#hallucinations (3)', '#healthcare (1)', '#inference (3)', '#interpretability', '#leakage', '#long_context (8)', '#low_resource (1)', '#machine_translation (1)', '#math (2)', '#multilingual (1)', '#multimodal (5)', '#open_source (8)', '#optimization (6)', '#plp', '#rag', '#reasoning (5)', '#rl (1)', '#rlhf (2)', '#robotics', '#science (1)', '#security', '#small_models (3)', '#story_generation', '#survey', '#synthetic (1)', '#training (9)', '#transfer_learning (1)', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-03-07 15:10',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-07 15:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-07 15:10')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    