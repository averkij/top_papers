
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 34 papers. March 25.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">25 марта</span> | <span id="title-articles-count">34 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-24.html">⬅️ <span id="prev-date">24.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-26.html">➡️ <span id="next-date">26.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '25 марта', 'en': 'March 25', 'zh': '3月25日'};
        let feedDateNext = {'ru': '26.03', 'en': '03/26', 'zh': '3月26日'};
        let feedDatePrev = {'ru': '24.03', 'en': '03/24', 'zh': '3月24日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.18878', 'title': 'I Have Covered All the Bases Here: Interpreting Reasoning Features in\n  Large Language Models via Sparse Autoencoders', 'url': 'https://huggingface.co/papers/2503.18878', 'abstract': "Large Language Models (LLMs) have achieved remarkable success in natural language processing. Recent advances have led to the developing of a new class of reasoning LLMs; for example, open-source DeepSeek-R1 has achieved state-of-the-art performance by integrating deep thinking and complex reasoning. Despite these impressive capabilities, the internal reasoning mechanisms of such models remain unexplored. In this work, we employ Sparse Autoencoders (SAEs), a method to learn a sparse decomposition of latent representations of a neural network into interpretable features, to identify features that drive reasoning in the DeepSeek-R1 series of models. First, we propose an approach to extract candidate ''reasoning features'' from SAE representations. We validate these features through empirical analysis and interpretability methods, demonstrating their direct correlation with the model's reasoning abilities. Crucially, we demonstrate that steering these features systematically enhances reasoning performance, offering the first mechanistic account of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning", 'score': 73, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a72d586944db2b55', 'authors': ['Andrey Galichin', 'Alexey Dontsov', 'Polina Druzhinina', 'Anton Razzhigaev', 'Oleg Y. Rogov', 'Elena Tutubalina', 'Ivan Oseledets'], 'affiliations': ['AIRI', 'HSE', 'MTUCI', 'Sber', 'Skoltech'], 'pdf_title_img': 'assets/pdf/title_img/2503.18878.jpg', 'data': {'categories': ['#training', '#interpretability', '#open_source', '#architecture', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Раскрывая тайны рассуждений искусственного интеллекта', 'desc': 'Исследователи изучили внутренние механизмы рассуждений в языковых моделях, используя разреженные автоэнкодеры (SAE). Они выделили ключевые признаки, отвечающие за способность модели к рассуждениям, в серии моделей DeepSeek-R1. Анализ показал прямую корреляцию этих признаков с рассуждениями модели. Управление выделенными признаками позволило систематически улучшать способность модели к рассуждениям, что дает первое механистическое объяснение рассуждений в больших языковых моделях.'}, 'en': {'title': 'Unlocking Reasoning in Large Language Models', 'desc': "This paper explores the internal reasoning mechanisms of Large Language Models (LLMs), specifically focusing on the DeepSeek-R1 model. The authors utilize Sparse Autoencoders (SAEs) to extract and identify 'reasoning features' that contribute to the model's reasoning capabilities. Through empirical analysis, they validate the correlation between these features and the model's performance in reasoning tasks. The study reveals that manipulating these features can systematically improve reasoning performance, providing new insights into how LLMs process and understand complex information."}, 'zh': {'title': '揭示大型语言模型的推理机制', 'desc': '大型语言模型（LLMs）在自然语言处理领域取得了显著成功。最近的进展催生了一类新的推理LLMs，例如开源的DeepSeek-R1，通过整合深度思考和复杂推理实现了最先进的性能。尽管这些模型的能力令人印象深刻，但其内部推理机制仍未被深入探索。本文采用稀疏自编码器（SAEs）来识别驱动DeepSeek-R1系列模型推理的特征，并通过实证分析验证这些特征与模型推理能力的直接关联。'}}}, {'id': 'https://huggingface.co/papers/2503.17359', 'title': 'Position: Interactive Generative Video as Next-Generation Game Engine', 'url': 'https://huggingface.co/papers/2503.17359', 'abstract': "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced.", 'score': 51, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '0046c940a41d8637', 'authors': ['Jiwen Yu', 'Yiran Qin', 'Haoxuan Che', 'Quande Liu', 'Xintao Wang', 'Pengfei Wan', 'Di Zhang', 'Xihui Liu'], 'affiliations': ['Kuaishou', 'The Hong Kong University of Science and Technology', 'The University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.17359.jpg', 'data': {'categories': ['#video', '#architecture', '#games', '#multimodal'], 'emoji': '🎮', 'ru': {'title': 'Революция в разработке игр: ИИ-генерируемые миры будущего', 'desc': 'Статья предлагает концепцию Генеративных Игровых Движков (GGE), основанных на Интерактивной Генеративной Видео технологии (IGV). GGE позволяет создавать неограниченный новый контент для игр следующего поколения, используя преимущества IGV в синтезе высококачественного контента, моделировании физики мира и интерактивности. Авторы представляют комплексную структуру основных модулей GGE и иерархическую дорожную карту зрелости (L0-L4) для его развития. Это исследование открывает новые перспективы для разработки игр в эпоху искусственного интеллекта.'}, 'en': {'title': 'Revolutionizing Game Development with AI-Driven Generative Engines', 'desc': 'This paper discusses the limitations of traditional game engines that rely on fixed content, which can hinder creativity and increase costs in game development. It introduces Interactive Generative Video (IGV) as a new approach to create Generative Game Engines (GGE), which can produce endless unique game content. GGE utilizes advanced features like high-quality content synthesis, physics-aware modeling, and user interactivity to enhance the gaming experience. The authors also outline a framework and roadmap for the development of GGE, aiming to transform the future of game creation through AI technologies.'}, 'zh': {'title': 'AI驱动的游戏创作新纪元', 'desc': '现代游戏开发面临着创造力和成本的重大挑战，传统游戏引擎的内容预设限制了创新。最近，视频生成模型的突破使得合成逼真且互动的虚拟环境成为可能，这为游戏创作带来了革命性的机会。我们提出了互动生成视频（IGV）作为生成游戏引擎（GGE）的基础，能够在下一代游戏中实现无限的新内容生成。GGE利用IGV在高质量内容合成、物理感知世界建模、用户控制互动、长期记忆能力和因果推理等方面的独特优势。'}}}, {'id': 'https://huggingface.co/papers/2503.18942', 'title': 'Video-T1: Test-Time Scaling for Video Generation', 'url': 'https://huggingface.co/papers/2503.18942', 'abstract': 'With the scale capability of increasing training data, model size, and computational cost, video generation has achieved impressive results in digital creation, enabling users to express creativity across various domains. Recently, researchers in Large Language Models (LLMs) have expanded the scaling to test-time, which can significantly improve LLM performance by using more inference-time computation. Instead of scaling up video foundation models through expensive training costs, we explore the power of Test-Time Scaling (TTS) in video generation, aiming to answer the question: if a video generation model is allowed to use non-trivial amount of inference-time compute, how much can it improve generation quality given a challenging text prompt. In this work, we reinterpret the test-time scaling of video generation as a searching problem to sample better trajectories from Gaussian noise space to the target video distribution. Specifically, we build the search space with test-time verifiers to provide feedback and heuristic algorithms to guide searching process. Given a text prompt, we first explore an intuitive linear search strategy by increasing noise candidates at inference time. As full-step denoising all frames simultaneously requires heavy test-time computation costs, we further design a more efficient TTS method for video generation called Tree-of-Frames (ToF) that adaptively expands and prunes video branches in an autoregressive manner. Extensive experiments on text-conditioned video generation benchmarks demonstrate that increasing test-time compute consistently leads to significant improvements in the quality of videos. Project page: https://liuff19.github.io/Video-T1', 'score': 48, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '1d482b72d90d6136', 'authors': ['Fangfu Liu', 'Hanyang Wang', 'Yimo Cai', 'Kaiyan Zhang', 'Xiaohang Zhan', 'Yueqi Duan'], 'affiliations': ['Tencent', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18942.jpg', 'data': {'categories': ['#video', '#inference', '#games', '#optimization', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Масштабирование времени тестирования: новый подход к улучшению генерации видео', 'desc': 'Статья исследует применение метода масштабирования времени тестирования (TTS) для улучшения качества генерации видео по текстовому описанию. Авторы представляют этот процесс как задачу поиска лучших траекторий от гауссова шума к целевому распределению видео. Они предлагают два подхода: линейный поиск с увеличением кандидатов шума и более эффективный метод Tree-of-Frames (ToF), который адаптивно расширяет и обрезает ветви видео авторегрессивным способом. Эксперименты показывают, что увеличение вычислительных ресурсов на этапе тестирования значительно улучшает качество генерируемых видео.'}, 'en': {'title': 'Unlocking Video Quality with Test-Time Scaling', 'desc': 'This paper explores the concept of Test-Time Scaling (TTS) in video generation, which allows models to utilize additional computational resources during inference to enhance video quality. Instead of focusing solely on training larger models, the authors investigate how increasing inference-time computation can improve the generation of videos from text prompts. They propose a method called Tree-of-Frames (ToF) that efficiently manages the search for better video outputs by adaptively expanding and pruning video branches. The results show that leveraging more computational power at test time leads to significant improvements in the quality of generated videos.'}, 'zh': {'title': '测试时间扩展：提升视频生成质量的新方法', 'desc': '随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。本文探讨了在视频生成中应用测试时间扩展（TTS）的潜力，旨在提高生成质量。我们将测试时间扩展重新解释为一个搜索问题，通过从高斯噪声空间中采样更好的轨迹来生成目标视频。实验结果表明，增加测试时间计算可以显著提升视频质量。'}}}, {'id': 'https://huggingface.co/papers/2503.18945', 'title': 'Aether: Geometric-Aware Unified World Modeling', 'url': 'https://huggingface.co/papers/2503.18945', 'abstract': 'The integration of geometric reconstruction and generative modeling remains a critical challenge in developing AI systems capable of human-like spatial reasoning. This paper proposes Aether, a unified framework that enables geometry-aware reasoning in world models by jointly optimizing three core capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video prediction, and (3) goal-conditioned visual planning. Through task-interleaved feature learning, Aether achieves synergistic knowledge sharing across reconstruction, prediction, and planning objectives. Building upon video generation models, our framework demonstrates unprecedented synthetic-to-real generalization despite never observing real-world data during training. Furthermore, our approach achieves zero-shot generalization in both action following and reconstruction tasks, thanks to its intrinsic geometric modeling. Remarkably, even without real-world data, its reconstruction performance far exceeds that of domain-specific models. Additionally, Aether leverages a geometry-informed action space to seamlessly translate predictions into actions, enabling effective autonomous trajectory planning. We hope our work inspires the community to explore new frontiers in physically-reasonable world modeling and its applications.', 'score': 21, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'e9f70faf8bc6d0d0', 'authors': ['Aether Team', 'Haoyi Zhu', 'Yifan Wang', 'Jianjun Zhou', 'Wenzheng Chang', 'Yang Zhou', 'Zizun Li', 'Junyi Chen', 'Chunhua Shen', 'Jiangmiao Pang', 'Tong He'], 'affiliations': ['Shanghai AI Laboratory'], 'pdf_title_img': 'assets/pdf/title_img/2503.18945.jpg', 'data': {'categories': ['#video', '#3d', '#reasoning', '#agents', '#synthetic'], 'emoji': '🧠', 'ru': {'title': 'Единая система для геометрического моделирования мира и планирования действий', 'desc': 'Статья представляет Aether - унифицированную систему для геометрически-осознанного рассуждения в моделях мира. Aether объединяет три ключевые возможности: 4D динамическую реконструкцию, предсказание видео с учетом действий и визуальное планирование с учетом целей. Благодаря совместному обучению признаков для различных задач, система демонстрирует беспрецедентную генерализацию с синтетических данных на реальные. Aether также показывает способность к обобщению без дополнительного обучения в задачах следования действиям и реконструкции.'}, 'en': {'title': 'Aether: Bridging Geometry and Generative Modeling for AI Spatial Reasoning', 'desc': "This paper introduces Aether, a framework that combines geometric reconstruction with generative modeling to enhance AI's spatial reasoning abilities. Aether optimizes three main functions: dynamic 4D reconstruction, action-based video prediction, and goal-oriented visual planning. By using task-interleaved feature learning, it allows for effective knowledge sharing among these functions, leading to improved performance. Notably, Aether achieves strong generalization to real-world scenarios without ever training on real data, showcasing its potential for autonomous trajectory planning and physical world modeling."}, 'zh': {'title': 'Aether：实现几何感知推理的统一框架', 'desc': '本论文提出了Aether框架，旨在解决几何重建与生成建模的整合问题，以实现类人空间推理。Aether通过联合优化四个核心能力，包括4D动态重建、基于动作的视频预测和基于目标的视觉规划，来实现几何感知推理。该框架通过任务交错特征学习，促进了重建、预测和规划目标之间的知识共享。尽管在训练过程中未观察到真实世界数据，Aether仍展现出前所未有的合成到真实的泛化能力，且在无监督情况下在动作跟随和重建任务中实现了零样本泛化。'}}}, {'id': 'https://huggingface.co/papers/2503.18033', 'title': 'OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18033', 'abstract': "Omnimatte aims to decompose a given video into semantically meaningful layers, including the background and individual objects along with their associated effects, such as shadows and reflections. Existing methods often require extensive training or costly self-supervised optimization. In this paper, we present OmnimatteZero, a training-free approach that leverages off-the-shelf pre-trained video diffusion models for omnimatte. It can remove objects from videos, extract individual object layers along with their effects, and composite those objects onto new videos. We accomplish this by adapting zero-shot image inpainting techniques for video object removal, a task they fail to handle effectively out-of-the-box. We then show that self-attention maps capture information about the object and its footprints and use them to inpaint the object's effects, leaving a clean background. Additionally, through simple latent arithmetic, object layers can be isolated and recombined seamlessly with new video layers to produce new videos. Evaluations show that OmnimatteZero not only achieves superior performance in terms of background reconstruction but also sets a new record for the fastest Omnimatte approach, achieving real-time performance with minimal frame runtime.", 'score': 19, 'issue_id': 2881, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': 'd43dc2371ac5a1e8', 'authors': ['Dvir Samuel', 'Matan Levy', 'Nir Darshan', 'Gal Chechik', 'Rami Ben-Ari'], 'affiliations': ['Bar-Ilan University, Ramat-Gan, Israel', 'NVIDIA Research, Tel-Aviv, Israel', 'OriginAI, Tel-Aviv, Israel', 'The Hebrew University of Jerusalem, Jerusalem, Israel'], 'pdf_title_img': 'assets/pdf/title_img/2503.18033.jpg', 'data': {'categories': ['#diffusion', '#cv', '#optimization', '#video'], 'emoji': '🎬', 'ru': {'title': 'OmnimatteZero: Быстрая и эффективная декомпозиция видео без обучения', 'desc': 'OmnimatteZero - это новый подход к декомпозиции видео на семантически значимые слои без необходимости обучения. Метод использует предобученные диффузионные модели для видео, позволяя удалять объекты, извлекать отдельные слои объектов вместе с их эффектами и компоновать их в новых видео. OmnimatteZero адаптирует технологии инпейнтинга изображений для удаления объектов из видео и использует карты самовнимания для восстановления эффектов объекта. Метод демонстрирует превосходную производительность в реконструкции фона и работает в режиме реального времени.'}, 'en': {'title': 'Revolutionizing Video Editing with Training-Free Layer Decomposition', 'desc': 'OmnimatteZero is a novel approach for video decomposition that separates a video into meaningful layers, such as backgrounds and individual objects, along with their effects like shadows and reflections. Unlike traditional methods that require extensive training, OmnimatteZero utilizes pre-trained video diffusion models, allowing for a training-free solution. It effectively employs zero-shot image inpainting techniques to remove objects and inpaint their effects, ensuring a clean background. The method also allows for the seamless recombination of object layers with new video content, achieving real-time performance and setting a new standard for speed in Omnimatte techniques.'}, 'zh': {'title': 'OmnimatteZero：无训练的视频对象分解新方法', 'desc': 'Omnimatte旨在将给定视频分解为具有语义意义的层，包括背景和单个对象及其相关效果，如阴影和反射。现有方法通常需要大量的训练或昂贵的自监督优化。本文提出了OmnimatteZero，这是一种无训练的方法，利用现成的预训练视频扩散模型进行omnimatte处理。通过适应零-shot图像修复技术，OmnimatteZero能够有效地从视频中移除对象，提取单个对象层及其效果，并将这些对象合成到新视频中。'}}}, {'id': 'https://huggingface.co/papers/2503.18892', 'title': 'SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for\n  Open Base Models in the Wild', 'url': 'https://huggingface.co/papers/2503.18892', 'abstract': 'DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.', 'score': 17, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '4c0c4ab2292562e4', 'authors': ['Weihao Zeng', 'Yuzhen Huang', 'Qian Liu', 'Wei Liu', 'Keqing He', 'Zejun Ma', 'Junxian He'], 'affiliations': ['BUPT', 'HKUST', 'TikTok'], 'pdf_title_img': 'assets/pdf/title_img/2503.18892.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#rl', '#training', '#small_models'], 'emoji': '🧠', 'ru': {'title': 'Развитие рассуждений в языковых моделях через RL с нуля', 'desc': 'Это исследование посвящено применению обучения с подкреплением (RL) без предварительной подготовки для развития способностей к рассуждениям у различных языковых моделей. Авторы провели эксперименты на 10 разных базовых моделях, включая LLama3, Mistral, DeepSeek-Math и Qwen2.5. Используя специальные стратегии, такие как настройка формата вознаграждения и контроль сложности запросов, удалось значительно улучшить точность рассуждений и длину ответов. Наблюдения показали, что разные модели демонстрируют различные паттерны в процессе обучения, причем увеличение длины ответа не всегда коррелирует с появлением определенных когнитивных способностей.'}, 'en': {'title': 'Unlocking Reasoning with Zero RL Training', 'desc': "The paper discusses DeepSeek-R1, which demonstrates that long chain-of-thought reasoning can be developed using a simple reinforcement learning framework with rule-based rewards, starting directly from base models, a method called zero RL training. The authors explore zero RL training across ten different base models, revealing that many of these models already possess strong instruction-following and self-reflection capabilities. They implement design strategies to enhance reasoning accuracy and response length, while also noting that training dynamics vary significantly among models. Importantly, they identify the 'aha moment' in smaller models outside the Qwen family, providing insights and open-sourcing their findings to support further research."}, 'zh': {'title': '零强化学习训练：推理与反思的新突破', 'desc': 'DeepSeek-R1展示了通过简单的强化学习框架和基于规则的奖励，长链思维推理可以自然出现。这种训练方法被称为零强化学习训练，允许直接从基础模型开始。我们研究了10种不同的基础模型，发现它们在推理准确性和响应长度上都有显著提升。我们还观察到，不同模型在训练过程中表现出不同的模式，特别是小模型首次出现了“恍然大悟”的现象。'}}}, {'id': 'https://huggingface.co/papers/2503.17489', 'title': 'Judge Anything: MLLM as a Judge Across Any Modality', 'url': 'https://huggingface.co/papers/2503.17489', 'abstract': 'Evaluating generative foundation models on open-ended multimodal understanding (MMU) and generation (MMG) tasks across diverse modalities (e.g., images, audio, video) poses significant challenges due to the complexity of cross-modal interactions. To this end, the idea of utilizing Multimodal LLMs (MLLMs) as automated judges has emerged, with encouraging results in assessing vision-language understanding tasks. Moving further, this paper extends MLLM-as-a-Judge across modalities to a unified manner by introducing two benchmarks, TaskAnything and JudgeAnything, to respectively evaluate the overall performance and judging capabilities of MLLMs across any-to-any modality tasks. Specifically, TaskAnything evaluates the MMU and MMG capabilities across 15 any-to-any modality categories, employing 1,500 queries curated from well-established benchmarks. Furthermore, JudgeAnything evaluates the judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from the perspectives of Pair Comparison and Score Evaluation, providing a standardized testbed that incorporates human judgments and detailed rubrics. Our extensive experiments reveal that while these MLLMs show promise in assessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting and 42.79% in Score Evaluation setting), they encounter significant challenges with MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and 30.05% in Score Evaluation setting), exposing cross-modality biases and hallucination issues. To address this, we present OmniArena, an automated platform for evaluating omni-models and multimodal reward models. Our work highlights the need for fairer evaluation protocols and stronger alignment with human preferences. The source code and dataset are publicly available at: https://urrealhero.github.io/judgeanythingweb/.', 'score': 13, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': 'bb040618997e1b0a', 'authors': ['Shu Pu', 'Yaochen Wang', 'Dongping Chen', 'Yuhang Chen', 'Guohao Wang', 'Qi Qin', 'Zhongyi Zhang', 'Zhiyuan Zhang', 'Zetong Zhou', 'Shuang Gong', 'Yi Gui', 'Yao Wan', 'Philip S. Yu'], 'affiliations': ['Huazhong University of Science and Technology', 'University of Illinois Chicago'], 'pdf_title_img': 'assets/pdf/title_img/2503.17489.jpg', 'data': {'categories': ['#multimodal', '#hallucinations', '#benchmark', '#alignment', '#open_source'], 'emoji': '🤖', 'ru': {'title': 'Универсальная оценка мультимодальных ИИ-моделей: от понимания к генерации', 'desc': 'Статья представляет новые бенчмарки TaskAnything и JudgeAnything для оценки мультимодальных языковых моделей (MLLM) в задачах понимания и генерации контента различных модальностей. TaskAnything оценивает способности MLLM в 15 категориях задач с различными комбинациями модальностей, используя 1500 запросов. JudgeAnything оценивает способности MLLM выступать в роли судей, сравнивая их оценки с человеческими по методикам попарного сравнения и балльной оценки. Результаты показывают, что MLLM лучше справляются с задачами понимания, чем с задачами генерации, выявляя проблемы межмодальных предубеждений и галлюцинаций.'}, 'en': {'title': 'Enhancing Multimodal Evaluation with MLLMs', 'desc': "This paper discusses the challenges of evaluating generative foundation models in tasks that involve multiple types of data, like images and audio. It introduces Multimodal LLMs (MLLMs) as automated judges to assess these models' understanding and generation capabilities across different modalities. The authors present two benchmarks, TaskAnything and JudgeAnything, to systematically evaluate MLLMs' performance and judging abilities. The findings reveal that while MLLMs perform reasonably well in understanding tasks, they struggle with generation tasks, highlighting the need for improved evaluation methods and alignment with human preferences."}, 'zh': {'title': '多模态评估的新视角', 'desc': '本论文探讨了在多模态理解（MMU）和生成（MMG）任务中评估生成基础模型的挑战，尤其是跨模态交互的复杂性。我们提出了使用多模态大语言模型（MLLMs）作为自动评估者的想法，并引入了两个基准：TaskAnything和JudgeAnything，分别用于评估MLLMs在任何模态任务中的整体性能和判断能力。实验结果显示，尽管MLLMs在MMU任务中表现出一定的潜力，但在MMG任务中面临显著挑战，暴露了跨模态偏见和幻觉问题。为了解决这些问题，我们提出了OmniArena，一个用于评估多模态模型和奖励模型的自动化平台。'}}}, {'id': 'https://huggingface.co/papers/2503.17439', 'title': 'LEMMA: Learning from Errors for MatheMatical Advancement in LLMs', 'url': 'https://huggingface.co/papers/2503.17439', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capability in solving mathematical problems. However, existing approaches primarily focus on improving the quality of correct training data, e.g., distilling high-quality correct solutions from advanced models, neglecting the value contained in error data, potentially hindering the model's reflective ability. Though some studies attempt to leverage error data, they often involve complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data consisting of an incorrect solution with an erroneous step and a reflection connection to a correct solution for fine-tuning. Specifically, we systematically analyze the model-generated error types and introduce an error-type grounded mistake augmentation method to collect diverse and representative errors. Correct solutions are either from fixing the errors or generating a fresh start. Through a model-aware smooth reflection connection, the erroneous solution is transferred to the correct one. By fine-tuning on the constructed dataset, the model is able to self-correct errors autonomously within the generation process without relying on external critique models. Experimental results demonstrate that LEMMA achieves significant performance improvements over other strong baselines.", 'score': 12, 'issue_id': 2875, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '946d486485fedb03', 'authors': ['Zhuoshi Pan', 'Yu Li', 'Honglin Lin', 'Qizhi Pei', 'Zinan Tang', 'Wei Wu', 'Chenlin Ming', 'H. Vicky Zhao', 'Conghui He', 'Lijun Wu'], 'affiliations': ['Renmin University of China', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.17439.jpg', 'data': {'categories': ['#math', '#training', '#reasoning', '#dataset', '#data'], 'emoji': '🧮', 'ru': {'title': 'Учимся на ошибках: новый подход к улучшению математических способностей ИИ', 'desc': 'Эта статья предлагает метод LEMMA для улучшения способности больших языковых моделей (LLM) решать математические задачи путем обучения на ошибках. LEMMA создает набор данных, состоящий из неправильных решений с ошибочными шагами и связями с правильными решениями для дообучения модели. Авторы вводят метод аугментации ошибок на основе типов ошибок для сбора разнообразных и репрезентативных ошибок. Эксперименты показывают, что LEMMA значительно улучшает производительность по сравнению с другими сильными базовыми моделями.'}, 'en': {'title': 'Empowering LLMs: Learning from Errors to Enhance Reasoning', 'desc': 'This paper introduces a novel approach called Learning from Errors for Mathematical Advancement (LEMMA) to improve the reasoning capabilities of large language models (LLMs) in solving mathematical problems. Unlike traditional methods that focus solely on enhancing correct training data, LEMMA leverages the value of error data by constructing a dataset that includes incorrect solutions paired with reflections on correct solutions. The method systematically analyzes error types and employs a mistake augmentation technique to gather diverse errors, allowing the model to learn from its mistakes. By fine-tuning on this enriched dataset, LEMMA enables LLMs to autonomously correct their errors during the generation process, leading to significant performance gains compared to existing methods.'}, 'zh': {'title': '从错误中学习，提升数学推理能力', 'desc': '大型语言模型（LLMs）在解决数学问题时展现了出色的推理能力。现有的方法主要关注提高正确训练数据的质量，而忽视了错误数据的价值，这可能会妨碍模型的反思能力。我们提出了一种通过学习错误来提升数学推理能力的方法，称为LEMMA。该方法通过构建包含错误步骤的错误解和与正确解的反思连接的数据集，来进行模型的微调，从而使模型能够在生成过程中自主纠正错误。'}}}, {'id': 'https://huggingface.co/papers/2503.18948', 'title': 'Equivariant Image Modeling', 'url': 'https://huggingface.co/papers/2503.18948', 'abstract': 'Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at https://github.com/drx-code/EquivariantModeling.', 'score': 11, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a098ae2b5e7412ee', 'authors': ['Ruixiao Dong', 'Mengde Xu', 'Zigang Geng', 'Li Li', 'Han Hu', 'Shuyang Gu'], 'affiliations': ['Tencent Hunyuan Research', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18948.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#training', '#open_source', '#cv'], 'emoji': '🖼️', 'ru': {'title': 'Эквивариантное моделирование для эффективной генерации изображений', 'desc': 'Статья представляет новый подход к генеративному моделированию изображений, основанный на принципе эквивариантности. Авторы предлагают метод токенизации по столбцам и оконное каузальное внимание для улучшения трансляционной симметрии. Эксперименты на ImageNet показывают, что модель достигает производительности на уровне современных авторегрессионных моделей при меньших вычислительных затратах. Подход демонстрирует улучшенную обобщающую способность и возможность синтеза сверхдлинных изображений.'}, 'en': {'title': 'Aligning Tasks for Efficient Generative Modeling', 'desc': 'This paper introduces a new framework for generative modeling that addresses conflicts in optimizing multiple subtasks. By utilizing the natural translation invariance found in visual data, the proposed method aligns optimization targets, leading to improved efficiency. Key innovations include column-wise tokenization for better symmetry and windowed causal attention to maintain contextual relationships. The framework shows competitive performance in image generation while reducing computational demands, enhancing generalization, and enabling longer image synthesis.'}, 'zh': {'title': '任务对齐的生成建模新框架', 'desc': '当前的生成模型，如自回归和扩散方法，将高维数据分布学习分解为一系列更简单的子任务。然而，在这些子任务的联合优化过程中会出现固有的冲突，现有的解决方案无法在不牺牲效率或可扩展性的情况下解决这些冲突。我们提出了一种新颖的等变图像建模框架，通过利用自然视觉信号的平移不变性，内在地对齐子任务之间的优化目标。我们的研究在256x256分辨率下的类条件ImageNet生成中表现出与最先进的自回归模型相当的性能，同时使用更少的计算资源。'}}}, {'id': 'https://huggingface.co/papers/2503.18940', 'title': 'Training-free Diffusion Acceleration with Bottleneck Sampling', 'url': 'https://huggingface.co/papers/2503.18940', 'abstract': 'Diffusion models have demonstrated remarkable capabilities in visual content generation but remain challenging to deploy due to their high computational cost during inference. This computational burden primarily arises from the quadratic complexity of self-attention with respect to image or video resolution. While existing acceleration methods often compromise output quality or necessitate costly retraining, we observe that most diffusion models are pre-trained at lower resolutions, presenting an opportunity to exploit these low-resolution priors for more efficient inference without degrading performance. In this work, we introduce Bottleneck Sampling, a training-free framework that leverages low-resolution priors to reduce computational overhead while preserving output fidelity. Bottleneck Sampling follows a high-low-high denoising workflow: it performs high-resolution denoising in the initial and final stages while operating at lower resolutions in intermediate steps. To mitigate aliasing and blurring artifacts, we further refine the resolution transition points and adaptively shift the denoising timesteps at each stage. We evaluate Bottleneck Sampling on both image and video generation tasks, where extensive experiments demonstrate that it accelerates inference by up to 3times for image generation and 2.5times for video generation, all while maintaining output quality comparable to the standard full-resolution sampling process across multiple evaluation metrics. Code is available at: https://github.com/tyfeld/Bottleneck-Sampling', 'score': 11, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '83ffcf1c20f5d4db', 'authors': ['Ye Tian', 'Xin Xia', 'Yuxi Ren', 'Shanchuan Lin', 'Xing Wang', 'Xuefeng Xiao', 'Yunhai Tong', 'Ling Yang', 'Bin Cui'], 'affiliations': ['Bytedance', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18940.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference', '#video'], 'emoji': '⏱️', 'ru': {'title': 'Ускорение диффузионных моделей без потери качества', 'desc': 'Статья представляет новый метод ускорения работы диффузионных моделей под названием Bottleneck Sampling. Этот подход использует предобученные низкоразрешающие модели для уменьшения вычислительных затрат без потери качества выходных данных. Метод следует схеме высокое-низкое-высокое разрешение при денойзинге, что позволяет ускорить вывод в 2.5-3 раза для задач генерации изображений и видео. Bottleneck Sampling не требует переобучения модели и сохраняет качество результатов на уровне стандартного полноразрешающего семплирования.'}, 'en': {'title': 'Speeding Up Diffusion Models with Bottleneck Sampling', 'desc': 'This paper presents Bottleneck Sampling, a new method to speed up diffusion models used for generating images and videos. Traditional diffusion models are slow because they use a complex self-attention mechanism that increases with image resolution. Bottleneck Sampling takes advantage of low-resolution training data to reduce the computational load during inference without sacrificing quality. By using a high-low-high denoising approach, it achieves significant speed improvements—up to 3 times faster for images and 2.5 times for videos—while still producing high-quality outputs.'}, 'zh': {'title': '瓶颈采样：高效的扩散模型推理', 'desc': '扩散模型在视觉内容生成方面表现出色，但在推理时由于计算成本高而难以部署。主要的计算负担来自于自注意力机制在图像或视频分辨率上的二次复杂性。我们提出了一种名为瓶颈采样的框架，利用低分辨率的先验知识来减少计算开销，同时保持输出质量。通过在高分辨率和低分辨率之间进行高低高的去噪工作流程，我们的实验表明，该方法在图像生成中加速推理速度可达3倍，在视频生成中可达2.5倍。'}}}, {'id': 'https://huggingface.co/papers/2503.18908', 'title': 'FFN Fusion: Rethinking Sequential Computation in Large Language Models', 'url': 'https://huggingface.co/papers/2503.18908', 'abstract': 'We introduce FFN Fusion, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create Llama-Nemotron-Ultra-253B-Base (Ultra-253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71X speedup in inference latency and 35X lower per-token cost while maintaining strong performance across benchmarks. Through extensive experiments on models from 49B to 253B parameters, we demonstrate that FFN Fusion becomes increasingly effective at larger scales and can complement existing optimization techniques like quantization and pruning. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design.', 'score': 10, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '77fd55a50b93d2d4', 'authors': ['Akhiad Bercovich', 'Mohammad Dabbah', 'Omri Puny', 'Ido Galil', 'Amnon Geifman', 'Yonatan Geifman', 'Izhak Golan', 'Ehud Karpas', 'Itay Levy', 'Zach Moshe', 'Najeeb Nabwani', 'Tomer Ronen', 'Itamar Schen', 'Elad Segal', 'Ido Shahaf', 'Oren Tropp', 'Ran Zilberstein', 'Ran El-Yaniv'], 'affiliations': ['NVIDIA'], 'pdf_title_img': 'assets/pdf/title_img/2503.18908.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение больших языковых моделей через параллелизацию FFN слоев', 'desc': 'Статья представляет технику оптимизации архитектуры под названием FFN Fusion, которая сокращает последовательные вычисления в больших языковых моделях. Метод идентифицирует последовательности слоев Feed-Forward Network (FFN), которые можно распараллелить с минимальным влиянием на точность модели. Применение этой техники к модели Llama-3.1-405B-Instruct позволило создать Llama-Nemotron-Ultra-253B-Base, достигающую ускорения в 1.71 раза при сохранении производительности. Исследования показали, что FFN Fusion особенно эффективен для крупных моделей и может сочетаться с другими методами оптимизации, такими как квантизация и прунинг.'}, 'en': {'title': 'Accelerating Language Models with FFN Fusion', 'desc': 'The paper presents FFN Fusion, a technique that optimizes large language models by enabling parallel computation of Feed-Forward Network (FFN) layers. This method identifies sequences of FFN layers that can be fused and executed in parallel, which reduces the time it takes for the model to make predictions without sacrificing accuracy. The authors demonstrate this approach on the Llama-3.1-405B-Instruct model, resulting in a new model, Llama-Nemotron-Ultra-253B-Base, that is significantly faster and cheaper to run. The findings suggest that FFN Fusion is particularly beneficial for larger models and can work alongside other optimization methods like quantization and pruning.'}, 'zh': {'title': 'FFN Fusion：提升大型语言模型的推理效率', 'desc': '我们提出了一种名为FFN Fusion的架构优化技术，旨在通过识别和利用并行化的自然机会来减少大型语言模型中的顺序计算。我们的关键见解是，去除特定注意力层后，前馈网络（FFN）层的序列通常可以以最小的准确性影响进行并行化。我们开发了一种原则性的方法来识别和融合这些序列，将其转化为并行操作，从而显著降低推理延迟，同时保持模型行为。通过将这些技术应用于Llama-3.1-405B-Instruct，我们创建了Llama-Nemotron-Ultra-253B-Base（Ultra-253B-Base），该模型在推理延迟上实现了1.71倍的加速，并且每个token的成本降低了35倍，同时在基准测试中保持了强劲的性能。'}}}, {'id': 'https://huggingface.co/papers/2503.18886', 'title': 'CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models', 'url': 'https://huggingface.co/papers/2503.18886', 'abstract': 'Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)', 'score': 10, 'issue_id': 2880, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '1e2a721645115955', 'authors': ['Weichen Fan', 'Amber Yijia Zheng', 'Raymond A. Yeh', 'Ziwei Liu'], 'affiliations': ['Department of Computer Science, Purdue University', 'S-Lab, Nanyang Technological University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18886.jpg', 'data': {'categories': ['#architecture', '#optimization', '#diffusion', '#video', '#training', '#cv'], 'emoji': '🔬', 'ru': {'title': 'CFG-Zero*: Оптимизированное руководство для улучшения генеративных моделей', 'desc': 'Статья посвящена улучшению техники Classifier-Free Guidance (CFG) для моделей диффузии и потоковых моделей. Авторы аналитически изучили влияние CFG на модели согласования потоков, обученные на гауссовых смесях. На основе наблюдений они предложили улучшенный метод CFG-Zero* с оптимизированным масштабированием и инициализацией нулями. Эксперименты на задачах генерации изображений и видео по тексту показали превосходство CFG-Zero* над стандартным CFG.'}, 'en': {'title': 'Enhancing Image Generation with CFG-Zero*', 'desc': 'This paper explores the limitations of Classifier-Free Guidance (CFG) in flow matching models, particularly during the early training phases when flow estimations are often inaccurate. The authors introduce CFG-Zero*, which enhances CFG by optimizing a scalar to adjust for these inaccuracies and implementing a zero-initialization strategy for the initial steps of the ODE solver. Through experiments in both text-to-image and text-to-video generation, CFG-Zero* shows significant improvements over traditional CFG in terms of image fidelity and controllability. The findings suggest that these modifications lead to better guidance for flow matching models, making them more effective in generating high-quality outputs.'}, 'zh': {'title': 'CFG-Zero*: 提升流模型的引导效果', 'desc': '本文研究了无分类器引导（CFG）在扩散/流模型中的应用，旨在提高图像的真实感和可控性。我们发现，在训练的早期阶段，流估计不准确时，CFG会将样本引导到错误的轨迹。为了解决这个问题，我们提出了CFG-Zero*，它通过优化比例和零初始化来改进CFG。实验结果表明，CFG-Zero*在文本到图像和文本到视频生成任务中均优于传统的CFG，证明了其在引导流匹配模型方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.18102', 'title': 'AgentRxiv: Towards Collaborative Autonomous Research', 'url': 'https://huggingface.co/papers/2503.18102', 'abstract': 'Progress in scientific discovery is rarely the result of a single "Eureka" moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other\'s research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.', 'score': 10, 'issue_id': 2879, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': 'f13710802c9e6fb0', 'authors': ['Samuel Schmidgall', 'Michael Moor'], 'affiliations': ['Department of Biosystems Science & Engineering, ETH Zurich', 'Department of Electrical & Computer Engineering, Johns Hopkins University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18102.jpg', 'data': {'categories': ['#agents', '#math', '#reasoning', '#science', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'Коллективный разум ИИ: AgentRxiv объединяет LLM-агентов для ускорения научных открытий', 'desc': 'Статья представляет AgentRxiv - фреймворк, позволяющий агентам на основе больших языковых моделей (LLM) обмениваться результатами исследований через общий препринт-сервер. Эксперименты показывают, что агенты с доступом к предыдущим исследованиям достигают более высоких результатов по сравнению с изолированными агентами. Несколько лабораторий агентов, обменивающихся исследованиями через AgentRxiv, способны быстрее продвигаться к общей цели. Результаты предполагают, что автономные агенты могут играть роль в разработке будущих систем искусственного интеллекта наряду с людьми.'}, 'en': {'title': 'Collaborative AI: Accelerating Research with AgentRxiv', 'desc': 'The paper introduces AgentRxiv, a collaborative framework for large language model (LLM) agents to share and build upon research findings. Unlike traditional isolated workflows, AgentRxiv allows agents to upload and retrieve reports from a shared preprint server, fostering collaboration and iterative improvement. The study shows that agents utilizing prior research achieve significant performance gains, with a 11.4% relative improvement on the MATH-500 benchmark. This collaborative approach not only enhances individual agent performance but also accelerates overall research progress, suggesting a promising future for autonomous agents in scientific discovery.'}, 'zh': {'title': 'AgentRxiv：促进代理实验室协作的研究框架', 'desc': '这篇论文介绍了AgentRxiv，一个框架使得大型语言模型（LLM）代理实验室能够上传和检索共享的预印本报告，从而实现合作和知识共享。通过这种方式，代理实验室可以在彼此的研究基础上进行迭代，提升研究成果。研究表明，能够访问先前研究的代理在性能上有显著提升，相比于孤立操作的代理，提升达到了11.4%。这些结果表明，多个代理实验室通过AgentRxiv合作，可以更快地朝着共同目标前进，并在整体准确性上实现更高的提升。'}}}, {'id': 'https://huggingface.co/papers/2503.18013', 'title': 'Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models\n  via Vision-Guided Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.18013', 'abstract': 'Large Vision-Language Models (LVLMs) typically follow a two-stage training paradigm-pretraining and supervised fine-tuning. Recently, preference optimization, derived from the language domain, has emerged as an effective post-training reinforcement strategy to enhance capabilities of LVLMs. However, constructing high-quality human-annotated preference data and developing robust reward models to mimic these preferences are both costly and challenging. Motivated by this observation, we propose Vision-R1, a novel vision-guided R1-like reinforcement learning algorithm for LVLMs that rewards models with definitive vision feedback. It only leverages curated instruction data, eliminating the need for specialized reward models and handcrafted preference datasets. We incorporate a criterion-driven reward function that further integrates multi-dimensional feedback to evaluate model completions comprehensively based on the vision task logic. Furthermore, we introduce a progressive rule refinement strategy that dynamically adjusts the reward criteria during training, enabling continuous model improvement and mitigating reward hacking. Extensive experiments on both in-distribution and out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with Vision-R1 achieves consistent performance gains, with even up to 50% improvement and surpassing the state-of-the-art 10x size model.', 'score': 10, 'issue_id': 2876, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '45029d297f1b8ac9', 'authors': ['Yufei Zhan', 'Yousong Zhu', 'Shurong Zheng', 'Hongyin Zhao', 'Fan Yang', 'Ming Tang', 'Jinqiao Wang'], 'affiliations': ['Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China', 'Peng Cheng Laboratory, Shenzhen, China', 'School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'Wuhan AI Research, Wuhan, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18013.jpg', 'data': {'categories': ['#optimization', '#rl', '#training', '#benchmark', '#rlhf'], 'emoji': '👁️', 'ru': {'title': 'Vision-R1: Революция в обучении визуально-языковых моделей без ручной разметки', 'desc': 'В статье представлен новый алгоритм обучения с подкреплением для крупных визуально-языковых моделей под названием Vision-R1. Этот метод использует обратную связь на основе зрения для улучшения моделей, не требуя специальных наборов данных о предпочтениях или моделей вознаграждения. Vision-R1 включает функцию вознаграждения на основе критериев и стратегию прогрессивного уточнения правил. Эксперименты показывают значительное улучшение производительности моделей, обученных с помощью Vision-R1, в некоторых случаях превосходящее модели в 10 раз большего размера.'}, 'en': {'title': 'Reinforcing Vision with Vision-R1: Simplifying LVLM Training', 'desc': 'This paper introduces Vision-R1, a new reinforcement learning algorithm designed for Large Vision-Language Models (LVLMs). Unlike traditional methods that require expensive human-annotated preference data, Vision-R1 uses curated instruction data to provide vision feedback directly to the models. The algorithm employs a criterion-driven reward function that assesses model outputs based on the logic of vision tasks, allowing for a more comprehensive evaluation. Additionally, it features a progressive rule refinement strategy that adapts reward criteria during training, leading to significant performance improvements in LVLMs without the need for complex reward models.'}, 'zh': {'title': '视觉引导的强化学习提升模型能力', 'desc': '大型视觉语言模型（LVLMs）通常采用两阶段训练方法：预训练和监督微调。最近，源自语言领域的偏好优化成为一种有效的后训练强化策略，用于提升LVLMs的能力。我们提出了一种新颖的视觉引导R1类强化学习算法Vision-R1，它通过明确的视觉反馈来奖励模型，避免了构建高质量人类标注的偏好数据和开发复杂的奖励模型的高成本。通过引入多维反馈的标准驱动奖励函数，Vision-R1能够全面评估模型的完成情况，并在训练过程中动态调整奖励标准，从而实现持续的模型改进。'}}}, {'id': 'https://huggingface.co/papers/2503.18923', 'title': 'Video SimpleQA: Towards Factuality Evaluation in Large Video Language\n  Models', 'url': 'https://huggingface.co/papers/2503.18923', 'abstract': 'Recent advancements in Large Video Language Models (LVLMs) have highlighted their potential for multi-modal understanding, yet evaluating their factual grounding in video contexts remains a critical unsolved challenge. To address this gap, we introduce Video SimpleQA, the first comprehensive benchmark tailored for factuality evaluation of LVLMs. Our work distinguishes from existing video benchmarks through the following key features: 1) Knowledge required: demanding integration of external knowledge beyond the explicit narrative; 2) Fact-seeking question: targeting objective, undisputed events or relationships, avoiding subjective interpretation; 3) Definitive & short-form answer: Answers are crafted as unambiguous and definitively correct in a short format, enabling automated evaluation through LLM-as-a-judge frameworks with minimal scoring variance; 4) External-source verified: All annotations undergo rigorous validation against authoritative external references to ensure the reliability; 5) Temporal reasoning required: The annotated question types encompass both static single-frame understanding and dynamic temporal reasoning, explicitly evaluating LVLMs factuality under the long-context dependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize key findings as follows: 1) Current LVLMs exhibit notable deficiencies in factual adherence, particularly for open-source models. The best-performing model Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute paradigms show insignificant performance gains, revealing fundamental constraints for enhancing factuality through post-hoc computation; 3) Retrieval-Augmented Generation demonstrates consistent improvements at the cost of additional inference time overhead, presenting a critical efficiency-performance trade-off.', 'score': 9, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '599abe342d833dd0', 'authors': ['Meng Cao', 'Pengfei Hu', 'Yingyao Wang', 'Jihao Gu', 'Haoran Tang', 'Haoze Zhao', 'Jiahua Dong', 'Wangbo Yu', 'Ge Zhang', 'Ian Reid', 'Xiaodan Liang'], 'affiliations': ['Alibaba Group', 'M-A-P', 'MBZUAI', 'Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18923.jpg', 'data': {'categories': ['#video', '#interpretability', '#reasoning', '#long_context', '#multimodal', '#rag', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Новый стандарт оценки фактической точности видео-языковых моделей', 'desc': 'Статья представляет Video SimpleQA - первый комплексный бенчмарк для оценки фактической точности Больших Видео-Языковых Моделей (LVLM). Бенчмарк отличается требованием интеграции внешних знаний, объективностью вопросов и верифицируемостью ответов. Оценка 41 современной LVLM показала значительные недостатки в фактической точности, при этом лучшая модель Gemini-1.5-Pro достигла F-меры всего 54.4%. Исследование выявило ограничения улучшения фактической точности через пост-обработку и компромисс между эффективностью и производительностью при использовании Retrieval-Augmented Generation.'}, 'en': {'title': 'Evaluating Factual Accuracy in Video Language Models', 'desc': 'This paper introduces Video SimpleQA, a new benchmark designed to evaluate the factual accuracy of Large Video Language Models (LVLMs). It focuses on assessing how well these models can integrate external knowledge and answer fact-based questions about video content. The benchmark emphasizes the need for definitive answers and includes rigorous validation against authoritative sources to ensure reliability. The evaluation of 41 LVLMs reveals significant shortcomings in factual adherence, particularly among open-source models, highlighting the challenges in improving factual accuracy in multi-modal contexts.'}, 'zh': {'title': '视频语言模型的事实性评估新基准', 'desc': '最近，大型视频语言模型（LVLMs）的进展显示了它们在多模态理解方面的潜力，但在视频上下文中评估其事实基础仍然是一个重要的未解决挑战。为了解决这个问题，我们引入了Video SimpleQA，这是第一个专门针对LVLMs事实性评估的综合基准。该基准的特点包括：需要整合外部知识、针对客观事件的问题、明确且简短的答案，以及经过外部来源验证的注释。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循方面存在显著不足，尤其是开源模型。'}}}, {'id': 'https://huggingface.co/papers/2503.18813', 'title': 'Defeating Prompt Injections by Design', 'url': 'https://huggingface.co/papers/2503.18813', 'abstract': 'Large Language Models (LLMs) are increasingly deployed in agentic systems that interact with an external environment. However, LLM agents are vulnerable to prompt injection attacks when handling untrusted data. In this paper we propose CaMeL, a robust defense that creates a protective system layer around the LLM, securing it even when underlying models may be susceptible to attacks. To operate, CaMeL explicitly extracts the control and data flows from the (trusted) query; therefore, the untrusted data retrieved by the LLM can never impact the program flow. To further improve security, CaMeL relies on a notion of a capability to prevent the exfiltration of private data over unauthorized data flows. We demonstrate effectiveness of CaMeL by solving 67% of tasks with provable security in AgentDojo [NeurIPS 2024], a recent agentic security benchmark.', 'score': 9, 'issue_id': 2879, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'a86a20fb5877c5ad', 'authors': ['Edoardo Debenedetti', 'Ilia Shumailov', 'Tianqi Fan', 'Jamie Hayes', 'Nicholas Carlini', 'Daniel Fabian', 'Christoph Kern', 'Chongyang Shi', 'Andreas Terzis', 'Florian Tramèr'], 'affiliations': ['ETH Zurich', 'Google', 'Google DeepMind'], 'pdf_title_img': 'assets/pdf/title_img/2503.18813.jpg', 'data': {'categories': ['#inference', '#security', '#agents', '#benchmark'], 'emoji': '🛡️', 'ru': {'title': 'CaMeL: Надежная защита LLM-агентов от атак внедрения промптов', 'desc': 'Статья представляет CaMeL - новый метод защиты агентов на основе больших языковых моделей (LLM) от атак внедрения промптов. CaMeL создает защитный системный слой вокруг LLM, извлекая потоки управления и данных из доверенного запроса. Это предотвращает влияние недоверенных данных на выполнение программы. Метод также использует концепцию возможностей для предотвращения утечки приватных данных. Эффективность CaMeL продемонстрирована решением 67% задач с доказуемой безопасностью в бенчмарке AgentDojo.'}, 'en': {'title': 'Securing LLMs Against Prompt Injection with CaMeL', 'desc': 'This paper introduces CaMeL, a defense mechanism designed to protect Large Language Models (LLMs) from prompt injection attacks when they interact with untrusted data. CaMeL works by creating a secure layer that separates control and data flows, ensuring that untrusted inputs do not affect the execution of the program. Additionally, it implements a capability system to prevent unauthorized access to private data. The effectiveness of CaMeL is demonstrated through its ability to solve 67% of tasks securely in the AgentDojo benchmark.'}, 'zh': {'title': 'CaMeL：保护大型语言模型的安全防线', 'desc': '大型语言模型（LLMs）在与外部环境交互的智能系统中越来越多地被使用。然而，当处理不可信数据时，LLM代理容易受到提示注入攻击。本文提出了CaMeL，这是一种强大的防御机制，它在LLM周围创建了一个保护系统层，即使底层模型可能容易受到攻击。CaMeL通过明确提取控制和数据流来操作，从而确保LLM检索到的不可信数据不会影响程序流程，并通过能力概念进一步提高安全性，防止私密数据通过未经授权的数据流泄露。'}}}, {'id': 'https://huggingface.co/papers/2503.17811', 'title': 'Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model\n  Collaboration Paradigm for Small Language Models', 'url': 'https://huggingface.co/papers/2503.17811', 'abstract': 'Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.', 'score': 8, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': 'a2f4f2ed59d6f67b', 'authors': ['Wenqi Pei', 'Hailing Xu', 'Hengyuan Zhao', 'Shizheng Hou', 'Han Chen', 'Zining Zhang', 'Pingyi Luo', 'Bingsheng He'], 'affiliations': ['4Paradigm', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2503.17811.jpg', 'data': {'categories': ['#optimization', '#dataset', '#small_models', '#reasoning', '#training'], 'emoji': '🪶', 'ru': {'title': 'Легковесный NL2SQL: Feather-SQL поднимает планку для малых языковых моделей', 'desc': "Статья представляет Feather-SQL - новый легковесный фреймворк для малых языковых моделей (SLM) в задаче преобразования естественного языка в SQL (NL2SQL). Feather-SQL улучшает исполняемость и точность SQL через оптимизацию схемы и многокандидатную генерацию. Авторы также предлагают парадигму сотрудничества '1+1 Model', объединяющую общую чат-модель со специализированной SQL-моделью. Эксперименты показывают значительное улучшение производительности SLM в задаче NL2SQL с использованием предложенного подхода."}, 'en': {'title': 'Empowering Small Models for SQL with Feather-SQL', 'desc': 'This paper presents Feather-SQL, a new framework designed to enhance the performance of small language models (SLMs) in converting natural language to SQL queries. It addresses the limitations of SLMs, which typically struggle with accuracy and compatibility in NL2SQL tasks. Feather-SQL employs techniques like schema pruning and multi-path generation to improve SQL executability and precision. Additionally, the introduction of the 1+1 Model Collaboration Paradigm allows for the combination of a general-purpose chat model with a specialized SQL model, significantly boosting the performance of SLMs in this domain.'}, 'zh': {'title': '轻量级框架，提升小型模型的SQL能力', 'desc': '自然语言转SQL（NL2SQL）在大型语言模型（LLMs）上取得了显著进展，但这些模型通常依赖于封闭源系统和高计算资源，给数据隐私和部署带来了挑战。相比之下，小型语言模型（SLMs）在NL2SQL任务上表现不佳，且与现有框架不兼容。为了解决这些问题，我们提出了Feather-SQL，这是一个专为SLMs设计的轻量级框架，通过模式修剪和链接、多路径和多候选生成来提高SQL的可执行性和准确性。此外，我们引入了1+1模型协作范式，将强大的通用聊天模型与经过微调的SQL专家配对，结合了强大的分析推理和高精度的SQL生成。'}}}, {'id': 'https://huggingface.co/papers/2503.14428', 'title': 'MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation', 'url': 'https://huggingface.co/papers/2503.14428', 'abstract': 'Text-to-video (T2V) generation has made significant strides with diffusion models. However, existing methods still struggle with accurately binding attributes, determining spatial relationships, and capturing complex action interactions between multiple subjects. To address these limitations, we propose MagicComp, a training-free method that enhances compositional T2V generation through dual-phase refinement. Specifically, (1) During the Conditioning Stage: We introduce the Semantic Anchor Disambiguation to reinforces subject-specific semantics and resolve inter-subject ambiguity by progressively injecting the directional vectors of semantic anchors into original text embedding; (2) During the Denoising Stage: We propose Dynamic Layout Fusion Attention, which integrates grounding priors and model-adaptive spatial perception to flexibly bind subjects to their spatiotemporal regions through masked attention modulation. Furthermore, MagicComp is a model-agnostic and versatile approach, which can be seamlessly integrated into existing T2V architectures. Extensive experiments on T2V-CompBench and VBench demonstrate that MagicComp outperforms state-of-the-art methods, highlighting its potential for applications such as complex prompt-based and trajectory-controllable video generation. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.', 'score': 7, 'issue_id': 2875, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '1cd532518024f266', 'authors': ['Hongyu Zhang', 'Yufan Deng', 'Shenghai Yuan', 'Peng Jin', 'Zesen Cheng', 'Yian Zhao', 'Chang Liu', 'Jie Chen'], 'affiliations': ['Peng Cheng Laboratory, Shenzhen, China', 'School of Electronic and Computer Engineering, Peking University, Shenzhen, China', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.14428.jpg', 'data': {'categories': ['#training', '#multimodal', '#architecture', '#games', '#diffusion', '#video'], 'emoji': '🎬', 'ru': {'title': 'MagicComp: Усовершенствованная генерация видео по тексту без дополнительного обучения', 'desc': 'MagicComp - это метод генерации видео по тексту, не требующий дополнительного обучения. Он использует двухфазовое уточнение для улучшения композиционной генерации: семантическое разрешение неоднозначности на этапе подготовки условий и динамическое слияние макетов на этапе шумоподавления. Метод решает проблемы связывания атрибутов, определения пространственных отношений и захвата сложных взаимодействий между несколькими объектами. MagicComp может быть интегрирован в существующие архитектуры генерации видео по тексту и превосходит современные методы в экспериментах.'}, 'en': {'title': 'Enhancing Text-to-Video Generation with MagicComp', 'desc': 'This paper presents MagicComp, a novel method for improving text-to-video (T2V) generation using diffusion models. It addresses challenges in accurately linking attributes and understanding spatial relationships between subjects in videos. The method consists of two main phases: the Conditioning Stage, which clarifies subject semantics using Semantic Anchor Disambiguation, and the Denoising Stage, which employs Dynamic Layout Fusion Attention to enhance spatial binding. MagicComp is designed to be adaptable and can be integrated into existing T2V systems, showing superior performance in various benchmarks.'}, 'zh': {'title': 'MagicComp：提升文本到视频生成的创新方法', 'desc': '本文提出了一种名为MagicComp的文本到视频生成方法，旨在解决现有方法在属性绑定、空间关系确定和复杂动作交互方面的不足。该方法通过双阶段的精炼过程来增强组合式T2V生成，首先在条件阶段引入语义锚点消歧，以强化特定主题的语义并解决主题间的歧义。其次，在去噪阶段，提出动态布局融合注意力，通过掩蔽注意力调制灵活绑定主题与其时空区域。MagicComp是一种与模型无关的通用方法，可以无缝集成到现有的T2V架构中，并在多个基准测试中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2503.18866', 'title': 'Reasoning to Learn from Latent Thoughts', 'url': 'https://huggingface.co/papers/2503.18866', 'abstract': 'Compute scaling for language model (LM) pretraining has outpaced the growth of human-written texts, leading to concerns that data will become the bottleneck to LM scaling. To continue scaling pretraining in this data-constrained regime, we propose that explicitly modeling and inferring the latent thoughts that underlie the text generation process can significantly improve pretraining data efficiency. Intuitively, our approach views web text as the compressed final outcome of a verbose human thought process and that the latent thoughts contain important contextual knowledge and reasoning steps that are critical to data-efficient learning. We empirically demonstrate the effectiveness of our approach through data-constrained continued pretraining for math. We first show that synthetic data approaches to inferring latent thoughts significantly improve data efficiency, outperforming training on the same amount of raw data (5.7\\% rightarrow 25.4\\% on MATH). Furthermore, we demonstrate latent thought inference without a strong teacher, where an LM bootstraps its own performance by using an EM algorithm to iteratively improve the capability of the trained LM and the quality of thought-augmented pretraining data. We show that a 1B LM can bootstrap its performance across at least three iterations and significantly outperform baselines trained on raw data, with increasing gains from additional inference compute when performing the E-step. The gains from inference scaling and EM iterations suggest new opportunities for scaling data-constrained pretraining.', 'score': 5, 'issue_id': 2876, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'ab963a9dd28b0934', 'authors': ['Yangjun Ruan', 'Neil Band', 'Chris J. Maddison', 'Tatsunori Hashimoto'], 'affiliations': ['Stanford University', 'University of Toronto', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.18866.jpg', 'data': {'categories': ['#optimization', '#training', '#transfer_learning', '#synthetic', '#data', '#math'], 'emoji': '🧠', 'ru': {'title': 'Раскрытие скрытых мыслей для эффективного обучения языковых моделей', 'desc': 'Статья предлагает метод повышения эффективности предобучения языковых моделей в условиях ограниченных данных. Авторы предлагают моделировать и выводить скрытые мысли, лежащие в основе процесса генерации текста. Этот подход рассматривает веб-текст как сжатый результат подробного мыслительного процесса человека. Эмпирические результаты показывают значительное улучшение эффективности обучения, особенно в области математики.'}, 'en': {'title': 'Unlocking Data Efficiency through Latent Thought Inference', 'desc': 'This paper addresses the challenge of limited human-written text data for training large language models (LMs). It proposes a method to model and infer the underlying thoughts that lead to text generation, which can enhance data efficiency during pretraining. By treating web text as a condensed version of human thought processes, the authors show that inferring these latent thoughts can lead to better learning outcomes, especially in data-scarce situations. Their experiments demonstrate that this approach not only improves performance on tasks like math but also allows LMs to iteratively enhance their own capabilities without relying heavily on external data.'}, 'zh': {'title': '潜在思维推断提升语言模型预训练效率', 'desc': '这篇论文探讨了在语言模型预训练中，数据增长速度慢于模型规模扩展的问题。作者提出通过显式建模和推断文本生成过程中的潜在思维，可以显著提高预训练的数据效率。研究表明，合成数据方法在推断潜在思维方面的应用，能够在数据受限的情况下，提升模型的学习效果。通过迭代的EM算法，模型能够自我提升性能，并在多个迭代中显著超越仅使用原始数据训练的基线模型。'}}}, {'id': 'https://huggingface.co/papers/2503.15879', 'title': 'Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering', 'url': 'https://huggingface.co/papers/2503.15879', 'abstract': 'Non-factoid question-answering (NFQA) poses a significant challenge due to its open-ended nature, diverse intents, and the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including retrieval-augmented generation (RAG), inadequate. Unlike factoid questions, non-factoid questions (NFQs) lack definitive answers and require synthesizing information from multiple sources across various reasoning dimensions. To address these limitations, we introduce Typed-RAG, a type-aware multi-aspect decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies NFQs into distinct types -- such as debate, experience, and comparison -- and applies aspect-based decomposition to refine retrieval and generation strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and aggregating the results, Typed-RAG generates more informative and contextually relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark dataset covering diverse NFQ types. Experimental results demonstrate that Typed-RAG outperforms baselines, thereby highlighting the importance of type-aware decomposition for effective retrieval and generation in NFQA. Our code and dataset are available at https://github.com/TeamNLP/Typed-RAG{https://github.com/TeamNLP/Typed-RAG}.', 'score': 5, 'issue_id': 2878, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '7bad41c869c73370', 'authors': ['DongGeon Lee', 'Ahjeong Park', 'Hyeri Lee', 'Hyeonseo Nam', 'Yunho Maeng'], 'affiliations': ['Ewha Womans University', 'Independent Researcher', 'KT', 'LLM Experimental Lab, MODULABS', 'Pohang University of Science and Technology', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2503.15879.jpg', 'data': {'categories': ['#rag', '#open_source', '#benchmark', '#optimization', '#dataset', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Typed-RAG: умное разложение вопросов для лучших ответов', 'desc': 'Статья представляет новый подход к ответам на нефактоидные вопросы (NFQA) под названием Typed-RAG. Этот метод классифицирует вопросы по типам и применяет декомпозицию на аспекты для улучшения стратегий поиска и генерации ответов. Авторы также представляют новый набор данных Wiki-NFQA для оценки эффективности Typed-RAG. Экспериментальные результаты показывают, что Typed-RAG превосходит базовые методы в задаче NFQA.'}, 'en': {'title': 'Typed-RAG: Enhancing NFQA with Type-Aware Decomposition', 'desc': 'This paper addresses the challenges of non-factoid question-answering (NFQA), which involves questions that do not have straightforward answers and require complex reasoning. The authors propose a new framework called Typed-RAG, which enhances the retrieval-augmented generation (RAG) approach by classifying NFQs into specific types and breaking them down into simpler sub-queries. By focusing on individual aspects of the questions, Typed-RAG improves the quality of the generated responses, making them more relevant and informative. The effectiveness of this method is validated through a new benchmark dataset, Wiki-NFQA, showing that type-aware decomposition significantly boosts performance in NFQA tasks.'}, 'zh': {'title': '类型感知，提升非事实问答的效果', 'desc': '非事实问答（NFQA）因其开放性、多样化的意图和多方面推理的需求而面临重大挑战，传统的事实问答方法（如检索增强生成RAG）无法满足这些需求。与事实问题不同，非事实问题（NFQ）没有明确的答案，需要从多个来源综合信息。为了解决这些问题，我们提出了Typed-RAG，这是一种在RAG范式下的类型感知多方面分解框架。Typed-RAG将NFQ分类为不同类型，并通过基于方面的分解来优化检索和生成策略，从而生成更具信息量和上下文相关的回答。'}}}, {'id': 'https://huggingface.co/papers/2503.18769', 'title': 'AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and\n  Symbolic Reasoning', 'url': 'https://huggingface.co/papers/2503.18769', 'abstract': 'This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of large language models (LLMs) for 3D Cartesian space navigation. AlphaSpace employs a semantics-based tokenization strategy, encoding height information through specialized semantic tokens, and integrates primarily symbolic synthetic reasoning data. This approach enables LLMs to accurately manipulate objects by positioning them at specific [x, y, z] coordinates. Experimental results demonstrate that AlphaSpace significantly outperforms existing models on manipulation subtasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet.', 'score': 4, 'issue_id': 2875, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'e92ee9df78b66019', 'authors': ['Alan Dao', 'Dinh Bach Vu', 'Bui Quang Huy'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2503.18769.jpg', 'data': {'categories': ['#architecture', '#synthetic', '#reasoning', '#3d'], 'emoji': '🧠', 'ru': {'title': 'AlphaSpace: Прорыв в пространственном мышлении ИИ', 'desc': 'AlphaSpace - это новая методология, разработанная для улучшения пространственного мышления больших языковых моделей (LLM) в навигации по 3D декартовому пространству. Она использует токенизацию на основе семантики, кодируя информацию о высоте через специальные семантические токены, и интегрирует преимущественно символические синтетические данные для рассуждений. Этот подход позволяет LLM точно манипулировать объектами, позиционируя их по конкретным координатам [x, y, z]. Экспериментальные результаты показывают, что AlphaSpace значительно превосходит существующие модели в подзадачах манипулирования, достигая общей точности 66,67%.'}, 'en': {'title': 'Enhancing 3D Navigation in Language Models with AlphaSpace', 'desc': 'This paper introduces AlphaSpace, a new method aimed at improving how large language models (LLMs) understand and navigate 3D spaces. It uses a unique tokenization method that incorporates height information through special semantic tokens, allowing for better spatial reasoning. By combining this with symbolic reasoning data, AlphaSpace enables LLMs to effectively manipulate objects in a 3D environment by placing them at precise coordinates. The results show that AlphaSpace achieves a notable accuracy of 66.67% in manipulation tasks, outperforming other models like GPT-4o and Claude 3.5 Sonnet.'}, 'zh': {'title': 'AlphaSpace：提升语言模型的空间推理能力', 'desc': '本文介绍了一种新方法AlphaSpace，旨在提升大型语言模型（LLMs）在三维笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的标记化策略，通过专门的语义标记编码高度信息，并主要整合符号合成推理数据。该方法使得LLMs能够准确地通过特定的[x, y, z]坐标来操作物体。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总准确率达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。'}}}, {'id': 'https://huggingface.co/papers/2503.18559', 'title': 'AMD-Hummingbird: Towards an Efficient Text-to-Video Model', 'url': 'https://huggingface.co/papers/2503.18559', 'abstract': 'Text-to-Video (T2V) generation has attracted significant attention for its ability to synthesize realistic videos from textual descriptions. However, existing models struggle to balance computational efficiency and high visual quality, particularly on resource-limited devices, e.g.,iGPUs and mobile phones. Most prior work prioritizes visual fidelity while overlooking the need for smaller, more efficient models suitable for real-world deployment. To address this challenge, we propose a lightweight T2V framework, termed Hummingbird, which prunes existing models and enhances visual quality through visual feedback learning. Our approach reduces the size of the U-Net from 1.4 billion to 0.7 billion parameters, significantly improving efficiency while preserving high-quality video generation. Additionally, we introduce a novel data processing pipeline that leverages Large Language Models (LLMs) and Video Quality Assessment (VQA) models to enhance the quality of both text prompts and video data. To support user-driven training and style customization, we publicly release the full training code, including data processing and model training. Extensive experiments show that our method achieves a 31X speedup compared to state-of-the-art models such as VideoCrafter2, while also attaining the highest overall score on VBench. Moreover, our method supports the generation of videos with up to 26 frames, addressing the limitations of existing U-Net-based methods in long video generation. Notably, the entire training process requires only four GPUs, yet delivers performance competitive with existing leading methods. Hummingbird presents a practical and efficient solution for T2V generation, combining high performance, scalability, and flexibility for real-world applications.', 'score': 3, 'issue_id': 2877, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': 'f6ded1274ae1fbf4', 'authors': ['Takashi Isobe', 'He Cui', 'Dong Zhou', 'Mengmeng Ge', 'Dong Li', 'Emad Barsoum'], 'affiliations': ['Advanced Micro Devices, Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18559.jpg', 'data': {'categories': ['#long_context', '#video', '#optimization', '#open_source', '#training', '#small_models', '#data'], 'emoji': '🐦', 'ru': {'title': 'Эффективная генерация видео по тексту для мобильных устройств', 'desc': 'Статья представляет легковесную модель генерации видео по тексту под названием Hummingbird. Авторы уменьшили размер U-Net с 1,4 до 0,7 миллиардов параметров, сохранив при этом высокое качество генерации. В работе используются большие языковые модели (LLM) и модели оценки качества видео (VQA) для улучшения текстовых запросов и видеоданных. Модель Hummingbird показывает 31-кратное ускорение по сравнению с современными аналогами и поддерживает генерацию видео длиной до 26 кадров.'}, 'en': {'title': 'Hummingbird: Efficient Text-to-Video Generation for Real-World Use', 'desc': 'This paper introduces Hummingbird, a lightweight framework for Text-to-Video (T2V) generation that aims to improve efficiency without sacrificing visual quality. By pruning the U-Net model from 1.4 billion to 0.7 billion parameters, Hummingbird achieves a significant speedup of 31 times compared to existing models like VideoCrafter2. The framework also incorporates a novel data processing pipeline that utilizes Large Language Models and Video Quality Assessment to enhance both text prompts and video data. With the ability to generate videos with up to 26 frames and requiring only four GPUs for training, Hummingbird offers a scalable and practical solution for real-world T2V applications.'}, 'zh': {'title': '轻量级文本到视频生成的高效解决方案', 'desc': '本文提出了一种轻量级的文本到视频生成框架，称为Hummingbird，旨在提高视频生成的效率和视觉质量。该框架通过剪枝现有模型，将U-Net的参数从14亿减少到7亿，从而在保持高质量视频生成的同时显著提高了计算效率。我们还引入了一种新的数据处理流程，利用大型语言模型和视频质量评估模型来提升文本提示和视频数据的质量。实验结果表明，Hummingbird在速度和性能上均优于现有的最先进模型，适用于资源有限的设备。'}}}, {'id': 'https://huggingface.co/papers/2503.18352', 'title': 'Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent\n  Diffusion Models', 'url': 'https://huggingface.co/papers/2503.18352', 'abstract': 'In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and Compression Ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis.', 'score': 3, 'issue_id': 2882, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '16c79c2acf245efb', 'authors': ['Jinjin Zhang', 'Qiuyu Huang', 'Junjie Liu', 'Xiefan Guo', 'Di Huang'], 'affiliations': ['Meituan', 'School of Computer Science and Engineering, Beihang University, Beijing 100191, China', 'State Key Laboratory of Complex and Critical Software Environment, Beihang University, Beijing 100191, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.18352.jpg', 'data': {'categories': ['#diffusion', '#dataset', '#training', '#cv', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Революция в генерации 4K изображений с помощью диффузионных моделей', 'desc': 'Статья представляет Diffusion-4K - новый фреймворк для синтеза ультра-высокого разрешения изображений с помощью диффузионных моделей текст-в-изображение. Авторы создали набор данных Aesthetic-4K и метрики для оценки качества 4K изображений. Они предложили метод файн-тюнинга на основе вейвлетов для обучения на фотореалистичных 4K изображениях. Эксперименты показали превосходство Diffusion-4K в синтезе изображений сверхвысокого разрешения.'}, 'en': {'title': 'Revolutionizing Ultra-High-Resolution Image Synthesis with Diffusion-4K', 'desc': 'This paper introduces Diffusion-4K, a new framework designed for creating ultra-high-resolution images directly from text using diffusion models. It presents the Aesthetic-4K Benchmark, a dataset of 4K images and captions, along with new metrics like GLCM Score and Compression Ratio for evaluating image quality. The authors also propose a wavelet-based fine-tuning method that enhances the training of diffusion models to produce detailed 4K images. Overall, Diffusion-4K shows significant improvements in image synthesis quality and adherence to text prompts compared to existing methods.'}, 'zh': {'title': 'Diffusion-4K：超高分辨率图像合成的新突破', 'desc': '本文提出了一种新颖的框架Diffusion-4K，用于直接生成超高分辨率图像，采用文本到图像的扩散模型。我们构建了Aesthetic-4K基准数据集，解决了缺乏公开4K图像合成数据集的问题，并引入了GLCM评分和压缩比等指标来评估图像细节。我们还提出了一种基于小波的微调方法，适用于各种潜在扩散模型，能够有效合成高细节的4K图像。实验结果表明，Diffusion-4K在高质量图像合成和文本提示遵循方面表现出色，尤其是在现代大规模扩散模型的支持下。'}}}, {'id': 'https://huggingface.co/papers/2503.18018', 'title': 'Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural\n  Contexts?', 'url': 'https://huggingface.co/papers/2503.18018', 'abstract': "Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, a widely used benchmark for assessing LLMs' mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide a more reliable framework for evaluating LLMs' mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at https://github.com/akarim23131/Lost_in_Cultural_Translation", 'score': 3, 'issue_id': 2883, 'pub_date': '2025-03-23', 'pub_date_card': {'ru': '23 марта', 'en': 'March 23', 'zh': '3月23日'}, 'hash': '38376682f477cce9', 'authors': ['Aabid Karim', 'Abdul Karim', 'Bhoomika Lohana', 'Matt Keon', 'Jaswinder Singh', 'Abdul Sattar'], 'affiliations': ['155mv Research Lab', 'Griffith University', 'Microsoft', 'Millcrest Technology'], 'pdf_title_img': 'assets/pdf/title_img/2503.18018.jpg', 'data': {'categories': ['#synthetic', '#dataset', '#math', '#data', '#reasoning', '#benchmark'], 'emoji': '🧮', 'ru': {'title': 'Культурный контекст влияет на математические способности ИИ', 'desc': 'Исследование показывает, что большие языковые модели (LLM) испытывают трудности с решением математических задач, когда меняется культурный контекст, несмотря на сохранение математической структуры. Авторы создали шесть синтетических культурных наборов данных на основе GSM8K, изменяя культурные элементы, такие как имена и названия мест. Результаты демонстрируют, что меньшие модели показывают большее снижение производительности по сравнению с более крупными. Исследование подчеркивает важность разнообразных и репрезентативных обучающих данных для повышения устойчивости LLM в реальных приложениях.'}, 'en': {'title': 'Cultural Context Matters: LLMs and Math Reasoning', 'desc': 'This paper investigates how Large Language Models (LLMs) perform on mathematical reasoning tasks when the problems are culturally adapted. It creates six synthetic datasets from the GSM8K benchmark, altering cultural elements while keeping the math intact. The study finds that LLMs struggle with culturally modified math problems, especially smaller models, indicating that cultural context significantly affects their reasoning abilities. Additionally, it suggests that familiarity with cultural references can enhance performance, highlighting the importance of diverse training data for improving LLM robustness.'}, 'zh': {'title': '文化背景影响数学推理能力', 'desc': '大型语言模型（LLMs）在编码、数学推理和逻辑问题解决等领域取得了显著进展。然而，当这些模型面对文化适应的数学问题时，它们的数学推理能力是否依然存在是一个重要问题。研究发现，当文化背景发生变化时，LLMs在解决数学问题时表现不佳，尽管数学结构保持不变。我们的研究强调了文化背景对LLMs数学推理能力的影响，表明需要更具多样性和代表性的训练数据以提高模型在实际应用中的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.17500', 'title': 'Variance Control via Weight Rescaling in LLM Pre-training', 'url': 'https://huggingface.co/papers/2503.17500', 'abstract': 'The outcome of Large Language Model (LLM) pre-training strongly depends on weight initialization and variance control strategies. Although the importance of initial variance control has been well documented in neural networks in general, the literature on initialization and management of its growth during LLM pre-training, specifically, is somewhat sparse. In this paper, we introduce the Layer Index Rescaling (LIR) weight initialization scheme, and the Target Variance Rescaling (TVR) variance control strategy. Experiments on a 1B parameter LLaMA model demonstrate that better variance management using these techniques yields substantial improvements in downstream task performance (up to 4.6% on common pre-training benchmarks) and reduces extreme activation values, thus mitigating challenges associated with quantization and low-precision training. Our code is available at: https://github.com/bluorion-com/weight_rescaling.', 'score': 3, 'issue_id': 2878, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '56b451c0f204aeda', 'authors': ['Louis Owen', 'Abhay Kumar', 'Nilabhra Roy Chowdhury', 'Fabian Güra'], 'affiliations': ['BluOrion'], 'pdf_title_img': 'assets/pdf/title_img/2503.17500.jpg', 'data': {'categories': ['#inference', '#benchmark', '#optimization', '#training', '#architecture'], 'emoji': '⚖️', 'ru': {'title': 'Точная настройка весов - ключ к эффективным языковым моделям', 'desc': 'Статья представляет новые методы инициализации весов и контроля дисперсии для предобучения больших языковых моделей (LLM). Авторы предлагают схему инициализации весов Layer Index Rescaling (LIR) и стратегию контроля дисперсии Target Variance Rescaling (TVR). Эксперименты на модели LLaMA с 1 млрд параметров показывают, что эти техники улучшают производительность на целевых задачах и уменьшают экстремальные значения активаций. Результаты демонстрируют важность правильного управления дисперсией при обучении LLM.'}, 'en': {'title': 'Enhancing LLM Performance through Innovative Weight and Variance Management', 'desc': 'This paper discusses how the performance of Large Language Models (LLMs) during pre-training is influenced by how their weights are initialized and how variance is controlled. It introduces two new techniques: Layer Index Rescaling (LIR) for weight initialization and Target Variance Rescaling (TVR) for managing variance growth. The authors show that applying these methods to a 1B parameter LLaMA model leads to significant improvements in performance on various tasks, achieving up to a 4.6% increase on standard benchmarks. Additionally, these techniques help reduce extreme activation values, which can complicate quantization and low-precision training.'}, 'zh': {'title': '优化大型语言模型的预训练性能', 'desc': '本论文探讨了大型语言模型（LLM）预训练中权重初始化和方差控制策略的重要性。我们提出了一种新的权重初始化方案，称为层索引重缩放（LIR），以及目标方差重缩放（TVR）方差控制策略。实验表明，使用这些技术进行更好的方差管理可以显著提高下游任务的性能，最高可达4.6%的提升，并减少极端激活值，从而缓解量化和低精度训练带来的挑战。我们的代码已在GitHub上发布，供研究人员使用。'}}}, {'id': 'https://huggingface.co/papers/2503.17422', 'title': 'V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V\n  Platforms', 'url': 'https://huggingface.co/papers/2503.17422', 'abstract': 'The recent exponential growth of Large Language Models (LLMs) has relied on GPU-based systems. However, CPUs are emerging as a flexible and lower-cost alternative, especially when targeting inference and reasoning workloads. RISC-V is rapidly gaining traction in this area, given its open and vendor-neutral ISA. However, the RISC-V hardware for LLM workloads and the corresponding software ecosystem are not fully mature and streamlined, given the requirement of domain-specific tuning. This paper aims at filling this gap, focusing on optimizing LLM inference on the Sophon SG2042, the first commercially available many-core RISC-V CPU with vector processing capabilities.   On two recent state-of-the-art LLMs optimized for reasoning, DeepSeek R1 Distill Llama 8B and DeepSeek R1 Distill QWEN 14B, we achieve 4.32/2.29 token/s for token generation and 6.54/3.68 token/s for prompt processing, with a speed up of up 2.9x/3.0x compared to our baseline.', 'score': 3, 'issue_id': 2876, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '3811c1f2a2e12813', 'authors': ['Javier J. Poveda Rodrigo', 'Mohamed Amine Ahmdi', 'Alessio Burrello', 'Daniele Jahier Pagliari', 'Luca Benini'], 'affiliations': ['DAUIN, Politecnico of Turin, Turin, Italy', 'ETHZ, Zurich, Switzerland'], 'pdf_title_img': 'assets/pdf/title_img/2503.17422.jpg', 'data': {'categories': ['#architecture', '#optimization', '#reasoning', '#inference'], 'emoji': '🚀', 'ru': {'title': 'Ускорение языковых моделей на RISC-V процессорах', 'desc': 'Данная статья посвящена оптимизации инференса больших языковых моделей (LLM) на процессорах RISC-V, в частности на Sophon SG2042. Авторы исследуют возможности использования CPU как альтернативы GPU для задач обработки естественного языка. В работе представлены результаты оптимизации двух современных LLM моделей - DeepSeek R1 Distill Llama 8B и DeepSeek R1 Distill QWEN 14B. Достигнуто значительное ускорение инференса по сравнению с базовой реализацией, до 2.9-3.0 раз.'}, 'en': {'title': 'Unlocking LLM Potential with RISC-V CPUs', 'desc': 'This paper discusses the potential of using CPUs, specifically RISC-V architecture, for optimizing Large Language Model (LLM) inference. It highlights the advantages of RISC-V, such as its flexibility and cost-effectiveness, especially for reasoning tasks. The authors focus on the Sophon SG2042, a many-core RISC-V CPU, and demonstrate significant performance improvements in token generation and prompt processing for two advanced LLMs. The results show up to 3x speed improvements compared to traditional systems, indicating a promising direction for LLM deployment on CPU architectures.'}, 'zh': {'title': '用RISC-V优化大型语言模型推理', 'desc': '近年来，大型语言模型（LLMs）的快速发展依赖于基于GPU的系统。然而，CPU作为一种灵活且成本更低的替代方案，正在逐渐崭露头角，特别是在推理和推断工作负载方面。RISC-V因其开放和中立的指令集架构（ISA）而在这一领域迅速获得关注。本文旨在优化在Sophon SG2042上进行LLM推理，展示了在两个最新的优化推理模型上实现的显著性能提升。'}}}, {'id': 'https://huggingface.co/papers/2503.18470', 'title': 'MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse', 'url': 'https://huggingface.co/papers/2503.18470', 'abstract': 'We present MetaSpatial, the first reinforcement learning (RL)-based framework designed to enhance 3D spatial reasoning in vision-language models (VLMs), enabling real-time 3D scene generation without the need for hard-coded optimizations. MetaSpatial addresses two core challenges: (i) the lack of internalized 3D spatial reasoning in VLMs, which limits their ability to generate realistic layouts, and (ii) the inefficiency of traditional supervised fine-tuning (SFT) for layout generation tasks, as perfect ground truth annotations are unavailable. Our key innovation is a multi-turn RL-based optimization mechanism that integrates physics-aware constraints and rendered image evaluations, ensuring generated 3D layouts are coherent, physically plausible, and aesthetically consistent. Methodologically, MetaSpatial introduces an adaptive, iterative reasoning process, where the VLM refines spatial arrangements over multiple turns by analyzing rendered outputs, improving scene coherence progressively. Empirical evaluations demonstrate that MetaSpatial significantly enhances the spatial consistency and formatting stability of various scale models. Post-training, object placements are more realistic, aligned, and functionally coherent, validating the effectiveness of RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game development applications. Our code, data, and training pipeline are publicly available at https://github.com/PzySeere/MetaSpatial.', 'score': 2, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '61b0d03727bc1ba4', 'authors': ['Zhenyu Pan', 'Han Liu'], 'affiliations': ['Department of Computer Science Northwestern University'], 'pdf_title_img': 'assets/pdf/title_img/2503.18470.jpg', 'data': {'categories': ['#3d', '#training', '#rl', '#optimization', '#games', '#open_source', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'MetaSpatial: Улучшение 3D пространственного мышления ИИ с помощью обучения с подкреплением', 'desc': 'MetaSpatial - это первая система на основе обучения с подкреплением для улучшения 3D пространственного мышления в визуально-языковых моделях. Она позволяет генерировать реалистичные 3D сцены в реальном времени без жестко закодированных оптимизаций. MetaSpatial использует многоэтапный механизм оптимизации с учетом физических ограничений и оценкой отрендеренных изображений. Система демонстрирует значительное улучшение пространственной согласованности и стабильности форматирования для моделей различного масштаба.'}, 'en': {'title': 'Revolutionizing 3D Spatial Reasoning with Reinforcement Learning', 'desc': 'MetaSpatial is a novel framework that uses reinforcement learning (RL) to improve 3D spatial reasoning in vision-language models (VLMs). It tackles the challenges of inadequate internal 3D reasoning and the limitations of traditional supervised fine-tuning for layout generation. The framework employs a multi-turn RL optimization process that incorporates physics-aware constraints and evaluations of rendered images to create realistic and coherent 3D layouts. Through iterative refinement, MetaSpatial enhances the spatial consistency and functional coherence of object placements, making it valuable for applications in the metaverse, AR/VR, and game development.'}, 'zh': {'title': 'MetaSpatial：提升3D空间推理的强化学习框架', 'desc': 'MetaSpatial是第一个基于强化学习的框架，旨在增强视觉语言模型中的3D空间推理能力，实现实时3D场景生成，而无需硬编码优化。该框架解决了两个核心挑战：一是视觉语言模型缺乏内在的3D空间推理能力，限制了其生成逼真布局的能力；二是传统的监督微调在布局生成任务中效率低下，因为完美的真实标注不可用。MetaSpatial的创新在于采用多轮强化学习优化机制，结合物理约束和渲染图像评估，确保生成的3D布局一致、物理上合理且美观。通过适应性迭代推理过程，MetaSpatial使视觉语言模型能够在多个回合中逐步改进空间安排，从而提高场景的一致性。'}}}, {'id': 'https://huggingface.co/papers/2503.17760', 'title': 'CODA: Repurposing Continuous VAEs for Discrete Tokenization', 'url': 'https://huggingface.co/papers/2503.17760', 'abstract': 'Discrete visual tokenizers transform images into a sequence of tokens, enabling token-based visual generation akin to language models. However, this process is inherently challenging, as it requires both compressing visual signals into a compact representation and discretizing them into a fixed set of codes. Traditional discrete tokenizers typically learn the two tasks jointly, often leading to unstable training, low codebook utilization, and limited reconstruction quality. In this paper, we introduce CODA(COntinuous-to-Discrete Adaptation), a framework that decouples compression and discretization. Instead of training discrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs -- already optimized for perceptual compression -- into discrete tokenizers via a carefully designed discretization process. By primarily focusing on discretization, CODA ensures stable and efficient training while retaining the strong visual fidelity of continuous VAEs. Empirically, with 6 times less training budget than standard VQGAN, our approach achieves a remarkable codebook utilization of 100% and notable reconstruction FID (rFID) of 0.43 and 1.34 for 8 times and 16 times compression on ImageNet 256times 256 benchmark.', 'score': 2, 'issue_id': 2887, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '5a93c7572e0fb46c', 'authors': ['Zeyu Liu', 'Zanlin Ni', 'Yeguo Hua', 'Xin Deng', 'Xiao Ma', 'Cheng Zhong', 'Gao Huang'], 'affiliations': ['Lenovo Research, AI Lab', 'Renmin University', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.17760.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#architecture', '#training'], 'emoji': '🖼️', 'ru': {'title': 'CODA: эффективная дискретизация изображений для генеративных моделей', 'desc': 'Статья представляет CODA - новый подход к дискретизации изображений для задач генерации. CODA адаптирует непрерывные вариационные автоэнкодеры в дискретные токенизаторы, разделяя процессы сжатия и дискретизации. Этот метод обеспечивает стабильное обучение, полное использование кодовой книги и высокое качество реконструкции. По сравнению со стандартным VQGAN, CODA достигает лучших результатов при меньших вычислительных затратах.'}, 'en': {'title': 'Decoupling Compression and Discretization for Better Visual Tokenization', 'desc': 'This paper presents CODA, a novel framework for transforming images into discrete tokens for visual generation. Unlike traditional methods that combine compression and discretization, CODA separates these tasks to enhance training stability and efficiency. By adapting existing continuous Variational Autoencoders (VAEs) for discretization, CODA achieves high-quality visual representations with optimal codebook usage. The results demonstrate that CODA requires significantly less training resources while achieving superior reconstruction quality compared to standard methods.'}, 'zh': {'title': 'CODA：高效的视觉标记离散化框架', 'desc': '本文介绍了一种新的框架CODA（连续到离散适应），用于将图像转换为离散的视觉标记序列。传统的离散标记器在压缩和离散化任务上通常会导致训练不稳定和重建质量低下。CODA通过将压缩和离散化过程解耦，利用现有的连续变分自编码器（VAE）进行适应，从而提高了训练的稳定性和效率。实验结果表明，CODA在训练预算上比标准VQGAN少6倍，同时实现了100%的代码本利用率和优异的重建FID值。'}}}, {'id': 'https://huggingface.co/papers/2503.17735', 'title': 'RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame\n  Animated Sticker Generation', 'url': 'https://huggingface.co/papers/2503.17735', 'abstract': 'Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available.', 'score': 2, 'issue_id': 2876, 'pub_date': '2025-03-22', 'pub_date_card': {'ru': '22 марта', 'en': 'March 22', 'zh': '3月22日'}, 'hash': '186b92c438925eb6', 'authors': ['Zhiqiang Yuan', 'Ting Zhang', 'Ying Deng', 'Jiapei Zhang', 'Yeshuang Zhu', 'Zexi Jia', 'Jie Zhou', 'Jinchao Zhang'], 'affiliations': ['Pattern Recognition Center, WeChat AI, Tencent'], 'pdf_title_img': 'assets/pdf/title_img/2503.17735.jpg', 'data': {'categories': ['#video', '#dataset', '#optimization', '#training', '#transfer_learning'], 'emoji': '🎬', 'ru': {'title': 'Эффективная генерация видео с нуля вместо тонкой настройки больших моделей', 'desc': 'Статья представляет новый подход к генерации видео в условиях ограниченных ресурсов. Авторы предлагают обучать с нуля небольшую модель на миллионах образцов вместо тонкой настройки больших предобученных моделей. Ключевые элементы подхода включают эффективное использование данных с помощью стратегии двойной маски и адаптивное обучение по учебной программе. Эксперименты на задаче генерации анимированных стикеров показывают превосходство предложенного метода над существующими подходами.'}, 'en': {'title': 'Train Small, Win Big: Efficient Video Generation Under Constraints', 'desc': 'This paper discusses advancements in video generation technology and its application in resource-limited environments. It challenges the effectiveness of parameter-efficient tuning methods like Adapter and Lora, suggesting that training smaller models from scratch can yield better results with limited data. The authors introduce a dual-mask data utilization strategy to enhance data diversity and a difficulty-adaptive curriculum learning method to improve model training. Their experiments show that this new approach outperforms existing tuning methods, demonstrating its potential for downstream applications.'}, 'zh': {'title': '资源受限下的视频生成新策略', 'desc': '最近，视频生成技术取得了显著进展，吸引了学者们的广泛关注。为了在资源受限的条件下应用这一技术，研究人员通常基于参数高效的调优方法对预训练模型进行微调。本文提出在资源受限的情况下，从头开始训练一个较小的视频生成模型，使用百万级样本，能够在下游应用中超越大型模型的参数高效调优。我们通过构建低帧率贴纸的离散帧生成网络和双掩码数据利用策略，结合难度自适应的课程学习方法，显著提高了模型的训练效果。'}}}, {'id': 'https://huggingface.co/papers/2503.16924', 'title': 'Optimized Minimal 3D Gaussian Splatting', 'url': 'https://huggingface.co/papers/2503.16924', 'abstract': '3D Gaussian Splatting (3DGS) has emerged as a powerful representation for real-time, high-performance rendering, enabling a wide range of applications. However, representing 3D scenes with numerous explicit Gaussian primitives imposes significant storage and memory overhead. Recent studies have shown that high-quality rendering can be achieved with a substantially reduced number of Gaussians when represented with high-precision attributes. Nevertheless, existing 3DGS compression methods still rely on a relatively large number of Gaussians, focusing primarily on attribute compression. This is because a smaller set of Gaussians becomes increasingly sensitive to lossy attribute compression, leading to severe quality degradation. Since the number of Gaussians is directly tied to computational costs, it is essential to reduce the number of Gaussians effectively rather than only optimizing storage. In this paper, we propose Optimized Minimal Gaussians representation (OMG), which significantly reduces storage while using a minimal number of primitives. First, we determine the distinct Gaussian from the near ones, minimizing redundancy without sacrificing quality. Second, we propose a compact and precise attribute representation that efficiently captures both continuity and irregularity among primitives. Additionally, we propose a sub-vector quantization technique for improved irregularity representation, maintaining fast training with a negligible codebook size. Extensive experiments demonstrate that OMG reduces storage requirements by nearly 50% compared to the previous state-of-the-art and enables 600+ FPS rendering while maintaining high rendering quality. Our source code is available at https://maincold2.github.io/omg/.', 'score': 2, 'issue_id': 2877, 'pub_date': '2025-03-21', 'pub_date_card': {'ru': '21 марта', 'en': 'March 21', 'zh': '3月21日'}, 'hash': '280ac899f8c492d0', 'authors': ['Joo Chan Lee', 'Jong Hwan Ko', 'Eunbyung Park'], 'affiliations': ['Sungkyunkwan University', 'Yonsei University'], 'pdf_title_img': 'assets/pdf/title_img/2503.16924.jpg', 'data': {'categories': ['#optimization', '#3d', '#inference', '#open_source'], 'emoji': '🎨', 'ru': {'title': 'OMG: Оптимизация 3D Gaussian Splatting для эффективного рендеринга', 'desc': 'В этой статье представлен метод Optimized Minimal Gaussians (OMG) для оптимизации 3D Gaussian Splatting. OMG значительно сокращает требования к хранению данных и использует минимальное количество примитивов, сохраняя при этом высокое качество рендеринга. Метод включает определение различных гауссиан среди близких и компактное представление атрибутов, эффективно capturing непрерывность и нерегулярность примитивов. Авторы также предлагают технику субвекторной квантизации для улучшенного представления нерегулярности, сохраняя быстрое обучение с незначительным размером кодовой книги.'}, 'en': {'title': 'Streamlining 3D Rendering with Minimal Gaussians', 'desc': 'This paper introduces the Optimized Minimal Gaussians (OMG) representation, which aims to enhance 3D Gaussian Splatting (3DGS) by significantly reducing the number of Gaussian primitives needed for high-quality rendering. The authors focus on minimizing redundancy among similar Gaussians while ensuring that the quality of the rendered scenes is preserved. They also present a compact attribute representation that captures both smooth and irregular features of the 3D scene, along with a sub-vector quantization method to improve efficiency. The results show that OMG can cut storage requirements by nearly 50% and achieve over 600 frames per second in rendering without compromising quality.'}, 'zh': {'title': '优化最小高斯表示，提升渲染效率！', 'desc': '3D高斯点云表示（3DGS）是一种用于实时高性能渲染的强大方法，但使用大量显式高斯原语会导致存储和内存开销大。本文提出了一种优化的最小高斯表示（OMG），通过减少高斯数量来显著降低存储需求，同时保持高渲染质量。我们通过识别相似高斯来减少冗余，并提出了一种紧凑的属性表示方法，以有效捕捉原语之间的连续性和不规则性。此外，我们还引入了一种子向量量化技术，以提高不规则性的表示效果。'}}}, {'id': 'https://huggingface.co/papers/2503.14774', 'title': 'Revisiting Image Fusion for Multi-Illuminant White-Balance Correction', 'url': 'https://huggingface.co/papers/2503.14774', 'abstract': 'White balance (WB) correction in scenes with multiple illuminants remains a persistent challenge in computer vision. Recent methods explored fusion-based approaches, where a neural network linearly blends multiple sRGB versions of an input image, each processed with predefined WB presets. However, we demonstrate that these methods are suboptimal for common multi-illuminant scenarios. Additionally, existing fusion-based methods rely on sRGB WB datasets lacking dedicated multi-illuminant images, limiting both training and evaluation. To address these challenges, we introduce two key contributions. First, we propose an efficient transformer-based model that effectively captures spatial dependencies across sRGB WB presets, substantially improving upon linear fusion techniques. Second, we introduce a large-scale multi-illuminant dataset comprising over 16,000 sRGB images rendered with five different WB settings, along with WB-corrected images. Our method achieves up to 100\\% improvement over existing techniques on our new multi-illuminant image fusion dataset.', 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-18', 'pub_date_card': {'ru': '18 марта', 'en': 'March 18', 'zh': '3月18日'}, 'hash': '654639b6ee1b8295', 'authors': ['David Serrano-Lozano', 'Aditya Arora', 'Luis Herranz', 'Konstantinos G. Derpanis', 'Michael S. Brown', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'Universidad Autonoma de Madrid', 'Universitat Aut`onoma de Barcelona', 'Vector Institute', 'York University'], 'pdf_title_img': 'assets/pdf/title_img/2503.14774.jpg', 'data': {'categories': ['#cv', '#dataset', '#optimization'], 'emoji': '📸', 'ru': {'title': 'Трансформеры и большие данные для идеального баланса белого', 'desc': 'Статья представляет новый подход к коррекции баланса белого в сценах с несколькими источниками освещения. Авторы предлагают эффективную модель на основе трансформеров, которая улучшает существующие методы линейного слияния. Также они представляют новый крупномасштабный набор данных с более чем 16 000 sRGB изображений с различными настройками баланса белого. Их метод показывает улучшение до 100% по сравнению с существующими техниками на новом наборе данных для слияния изображений с несколькими источниками освещения.'}, 'en': {'title': 'Transforming White Balance Correction with a New Dataset and Model', 'desc': 'This paper addresses the challenge of white balance (WB) correction in images with multiple light sources. It critiques existing fusion-based methods that blend different sRGB images but finds them lacking for complex lighting scenarios. The authors propose a new transformer-based model that better captures the relationships between different WB settings, leading to significant performance improvements. Additionally, they introduce a comprehensive dataset with over 16,000 images to support training and evaluation of WB correction methods in multi-illuminant contexts.'}, 'zh': {'title': '提升多光源场景下的白平衡校正效果', 'desc': '在计算机视觉中，白平衡（WB）校正在多光源场景中仍然是一个持续的挑战。最近的方法探索了基于融合的技术，通过神经网络线性混合多个sRGB版本的输入图像，但这些方法在常见的多光源场景中效果不佳。我们提出了一种高效的基于变换器的模型，能够有效捕捉不同sRGB WB预设之间的空间依赖性，显著改善了线性融合技术。此外，我们还引入了一个大型多光源数据集，包含超过16,000张使用五种不同WB设置渲染的sRGB图像及其WB校正图像。'}}}, {'id': 'https://huggingface.co/papers/2503.13074', 'title': 'Rethinking Image Evaluation in Super-Resolution', 'url': 'https://huggingface.co/papers/2503.13074', 'abstract': "While recent advancing image super-resolution (SR) techniques are continually improving the perceptual quality of their outputs, they can usually fail in quantitative evaluations. This inconsistency leads to a growing distrust in existing image metrics for SR evaluations. Though image evaluation depends on both the metric and the reference ground truth (GT), researchers typically do not inspect the role of GTs, as they are generally accepted as `perfect' references. However, due to the data being collected in the early years and the ignorance of controlling other types of distortions, we point out that GTs in existing SR datasets can exhibit relatively poor quality, which leads to biased evaluations. Following this observation, in this paper, we are interested in the following questions: Are GT images in existing SR datasets 100% trustworthy for model evaluations? How does GT quality affect this evaluation? And how to make fair evaluations if there exist imperfect GTs? To answer these questions, this paper presents two main contributions. First, by systematically analyzing seven state-of-the-art SR models across three real-world SR datasets, we show that SR performances can be consistently affected across models by low-quality GTs, and models can perform quite differently when GT quality is controlled. Second, we propose a novel perceptual quality metric, Relative Quality Index (RQI), that measures the relative quality discrepancy of image pairs, thus issuing the biased evaluations caused by unreliable GTs. Our proposed model achieves significantly better consistency with human opinions. We expect our work to provide insights for the SR community on how future datasets, models, and metrics should be developed.", 'score': 1, 'issue_id': 2887, 'pub_date': '2025-03-17', 'pub_date_card': {'ru': '17 марта', 'en': 'March 17', 'zh': '3月17日'}, 'hash': '7026b065c090a9c7', 'authors': ['Shaolin Su', 'Josep M. Rocafort', 'Danna Xue', 'David Serrano-Lozano', 'Lei Sun', 'Javier Vazquez-Corral'], 'affiliations': ['Computer Vision Center', 'INSAIT, Sofia University St. Kliment Ohridski', 'Universitat Autonoma de Barcelona'], 'pdf_title_img': 'assets/pdf/title_img/2503.13074.jpg', 'data': {'categories': ['#optimization', '#dataset', '#benchmark', '#cv', '#ethics'], 'emoji': '🔍', 'ru': {'title': 'Переосмысление эталонов в оценке алгоритмов сверхразрешения', 'desc': 'Статья исследует проблему оценки качества алгоритмов сверхразрешения изображений. Авторы обнаружили, что эталонные изображения в существующих наборах данных могут иметь низкое качество, что приводит к искаженным оценкам. Они проанализировали влияние качества эталонов на оценку современных моделей сверхразрешения. В результате был предложен новый метрический показатель Relative Quality Index (RQI), который измеряет относительную разницу качества пар изображений и лучше соответствует человеческому восприятию.'}, 'en': {'title': 'Rethinking Ground Truth: Enhancing Super-Resolution Evaluations', 'desc': 'This paper addresses the issue of trustworthiness in ground truth (GT) images used for evaluating image super-resolution (SR) models. It highlights that many existing GTs may not be perfect, leading to biased performance evaluations of SR techniques. The authors analyze various SR models and demonstrate that low-quality GTs can significantly impact model performance. They also introduce a new metric, the Relative Quality Index (RQI), which aims to provide a more reliable assessment of image quality by comparing pairs of images, aligning better with human judgment.'}, 'zh': {'title': '提升超分辨率评估的信任度', 'desc': '这篇论文探讨了图像超分辨率(SR)技术在定量评估中的不足，指出现有的图像评估指标可能不可靠。研究发现，现有SR数据集中作为参考的真实图像(GT)质量并不总是完美，可能导致评估结果的偏差。通过分析七种最先进的SR模型，论文表明低质量的GT会影响模型的表现，并提出了一种新的感知质量指标——相对质量指数(RQI)，用于衡量图像对之间的相对质量差异。该研究旨在为SR领域提供关于未来数据集、模型和评估指标开发的见解。'}}}, {'id': 'https://huggingface.co/papers/2503.18494', 'title': 'Verbal Process Supervision Elicits Better Coding Agents', 'url': 'https://huggingface.co/papers/2503.18494', 'abstract': 'The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.', 'score': 0, 'issue_id': 2881, 'pub_date': '2025-03-24', 'pub_date_card': {'ru': '24 марта', 'en': 'March 24', 'zh': '3月24日'}, 'hash': '97716e960b1a782d', 'authors': ['Hao-Yuan Chen', 'Cheng-Pong Huang', 'Jui-Ming Yao'], 'affiliations': ['Mindify AI, United States', 'National Taiwan University of Science and Technology, Taiwan', 'University of London, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2503.18494.jpg', 'data': {'categories': ['#training', '#benchmark', '#agents', '#agi', '#architecture', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'CURA: Агент с вербальным контролем для решения сложных задач программирования', 'desc': 'Статья представляет CURA - систему агента для понимания и рассуждения о коде, улучшенную с помощью вербального контроля процесса (VPS). CURA достигает улучшения на 3.65% по сравнению с базовыми моделями на сложных бенчмарках, таких как BigCodeBench. В сочетании с моделью o3-mini и техниками VPS, CURA достигает наилучших результатов в своей области. Это исследование представляет шаг вперед в интеграции архитектур, основанных на рассуждениях, с генерацией кода на основе больших языковых моделей.'}, 'en': {'title': 'CURA: Enhancing Code Generation with Reasoning and Supervision', 'desc': 'This paper presents CURA, a code understanding and reasoning agent that enhances large language models (LLMs) with verbal process supervision (VPS). CURA addresses the limitations of existing AI agents in tackling complex software engineering challenges by improving their reasoning capabilities. The system demonstrates a 3.65% performance boost on benchmarks like BigCodeBench compared to baseline models. By integrating reasoning-driven architectures with LLMs, CURA enables more effective code generation and problem-solving in software engineering tasks.'}, 'zh': {'title': 'CURA：提升代码理解与推理的智能代理系统', 'desc': '本论文介绍了一种名为CURA的代码理解与推理代理系统，旨在解决复杂的软件工程问题。CURA通过引入语言过程监督（VPS）技术，显著提高了在BigCodeBench等基准测试中的表现，提升幅度达到3.65%。此外，当CURA与o3-mini模型结合使用时，能够达到最先进的性能。该研究展示了将推理驱动架构与大型语言模型结合的潜力，为语言模型在复杂软件工程任务中的推理能力提供了新的方向。'}}}, {'id': 'https://huggingface.co/papers/2503.16426', 'title': 'DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding', 'url': 'https://huggingface.co/papers/2503.16426', 'abstract': "The advancement of remote sensing technology has improved the spatial resolution of satellite imagery, facilitating more detailed visual representations for diverse interpretations. However, existing methods exhibit limited generalization capabilities across varied applications. While some contemporary foundation models demonstrate potential, they are hindered by insufficient cross-task adaptability and primarily process low-resolution imagery of restricted sizes, thus failing to fully exploit high-resolution data or leverage comprehensive large-scene semantics. Crucially, remote sensing imagery differs fundamentally from natural images, as key foreground targets (eg., maritime objects, artificial structures) often occupy minimal spatial proportions (~1%) and exhibit sparse distributions. Efficiently modeling cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a significant challenge yet remains critical for remote sensing image understanding. Motivated by the selective attention mechanisms inherent to the human visual system, we propose DynamicVis, a dynamic visual perception foundation model for remote sensing imagery. The framework integrates a novel dynamic region perception backbone based on the selective state space model, which strategically balances localized detail extraction with global contextual integration, enabling computationally efficient encoding of large-scale data while maintaining architectural scalability. To enhance cross-task knowledge transferring, we introduce a multi-instance learning paradigm utilizing meta-embedding representations, trained on million-scale region-level annotations. Evaluations across nine downstream tasks demonstrate the model's versatility. DynamicVis achieves multi-level feature modeling with exceptional efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and 833 MB GPU memory (3% of ViT's).", 'score': 0, 'issue_id': 2885, 'pub_date': '2025-03-20', 'pub_date_card': {'ru': '20 марта', 'en': 'March 20', 'zh': '3月20日'}, 'hash': '973f386b0cfb485a', 'authors': ['Keyan Chen', 'Chenyang Liu', 'Bowen Chen', 'Wenyuan Li', 'Zhengxia Zou', 'Zhenwei Shi'], 'affiliations': ['Beihang University', 'the University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.16426.jpg', 'data': {'categories': ['#cv', '#optimization', '#transfer_learning', '#training', '#architecture'], 'emoji': '🛰️', 'ru': {'title': 'DynamicVis: эффективная модель машинного зрения для анализа спутниковых снимков', 'desc': 'Статья представляет DynamicVis - модель машинного зрения для анализа спутниковых снимков высокого разрешения. Модель использует селективный механизм внимания и пространственно-временную модель для эффективной обработки больших изображений. DynamicVis обучается на миллионах аннотаций на уровне регионов, что позволяет ей успешно решать различные задачи. Модель демонстрирует высокую эффективность, обрабатывая изображения 2048x2048 пикселей за 97 мс при использовании всего 833 МБ видеопамяти.'}, 'en': {'title': 'DynamicVis: Revolutionizing Remote Sensing with Efficient Cross-Task Learning', 'desc': 'This paper presents DynamicVis, a foundation model designed specifically for remote sensing imagery, which often contains sparse and small foreground targets. The model addresses the limitations of existing methods by enhancing cross-task adaptability and efficiently processing high-resolution satellite images. By employing a dynamic region perception backbone and a multi-instance learning paradigm, DynamicVis achieves effective feature modeling while maintaining low latency and memory usage. Evaluations show that it excels across various tasks, demonstrating its potential for improved understanding of remote sensing data.'}, 'zh': {'title': 'DynamicVis：遥感图像处理的新突破', 'desc': '本论文介绍了一种名为DynamicVis的动态视觉感知基础模型，专门用于遥感图像处理。该模型通过选择性状态空间模型，平衡局部细节提取与全局上下文整合，从而高效编码大规模数据。DynamicVis采用多实例学习范式，利用元嵌入表示，提升跨任务知识转移能力。实验结果表明，该模型在处理高分辨率图像时表现出色，具有极高的效率和灵活性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (4)', '#agents (4)', '#agi (1)', '#alignment (1)', '#architecture (12)', '#audio', '#benchmark (13)', '#cv (9)', '#data (4)', '#dataset (9)', '#diffusion (6)', '#ethics (1)', '#games (4)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (7)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (4)', '#multilingual', '#multimodal (4)', '#open_source (8)', '#optimization (20)', '#plp', '#rag (2)', '#reasoning (13)', '#rl (3)', '#rlhf (1)', '#robotics', '#science (1)', '#security (1)', '#small_models (3)', '#story_generation', '#survey', '#synthetic (4)', '#training (18)', '#transfer_learning (3)', '#video (10)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-25 15:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-25 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-25 15:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    