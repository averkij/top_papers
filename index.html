
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 12 papers. April 28.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ</span> | <span id="title-articles-count">12 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-25.html">â¬…ï¸ <span id="prev-date">25.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-29.html">â¡ï¸ <span id="next-date">29.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '28 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 28', 'zh': '4æœˆ28æ—¥'};
        let feedDateNext = {'ru': '29.04', 'en': '04/29', 'zh': '4æœˆ29æ—¥'};
        let feedDatePrev = {'ru': '25.04', 'en': '04/25', 'zh': '4æœˆ25æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.15376', 'title': 'Towards Understanding Camera Motions in Any Video', 'url': 'https://huggingface.co/papers/2504.15376', 'abstract': 'We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like "follow" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.', 'score': 118, 'issue_id': 3457, 'pub_date': '2025-04-21', 'pub_date_card': {'ru': '21 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 21', 'zh': '4æœˆ21æ—¥'}, 'hash': 'fb835f3848fe977a', 'authors': ['Zhiqiu Lin', 'Siyuan Cen', 'Daniel Jiang', 'Jay Karhade', 'Hewei Wang', 'Chancharik Mitra', 'Tiffany Ling', 'Yuhan Huang', 'Sifan Liu', 'Mingyu Chen', 'Rushikesh Zawar', 'Xue Bai', 'Yilun Du', 'Chuang Gan', 'Deva Ramanan'], 'affiliations': ['Adobe', 'CMU', 'Emerson', 'Harvard', 'MIT-IBM', 'UMass Amherst', 'USC'], 'pdf_title_img': 'assets/pdf/title_img/2504.15376.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#cv', '#multimodal'], 'emoji': 'ğŸ¥', 'ru': {'title': 'ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'CameraBench - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾ĞºĞ¾Ğ»Ğ¾ 3000 Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ²Ğ¸Ğ´ĞµĞ¾, Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ğ²Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ğ¾ Ñ ĞºĞ¸Ğ½Ğ¾Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ CameraBench, Ğ±Ñ‹Ğ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞµĞ½Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Structure-from-Motion (SfM) Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (VLM), Ğ²Ñ‹ÑĞ²Ğ¸Ğ² Ğ¸Ñ… ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹.'}, 'en': {'title': 'Unlocking Camera Motion Understanding with CameraBench', 'desc': 'CameraBench is a comprehensive dataset and benchmark aimed at enhancing the understanding of camera motion in videos. It includes around 3,000 videos that have been meticulously annotated by experts, ensuring high-quality data for training and evaluation. The paper introduces a taxonomy of camera motion primitives, which helps in categorizing different types of camera movements, such as tracking and zooming. The study reveals that while traditional Structure-from-Motion models struggle with semantic understanding, fine-tuning a generative Video-Language Model on CameraBench can effectively bridge the gap between geometric and semantic motion understanding.'}, 'zh': {'title': 'CameraBenchï¼šæå‡ç›¸æœºè¿åŠ¨ç†è§£çš„åŸºå‡†ä¸æ•°æ®é›†', 'desc': 'æœ¬æ–‡ä»‹ç»äº†CameraBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†å’ŒåŸºå‡†ï¼Œç”¨äºè¯„ä¼°å’Œæ”¹å–„å¯¹ç›¸æœºè¿åŠ¨çš„ç†è§£ã€‚CameraBenchåŒ…å«çº¦3000ä¸ªå¤šæ ·åŒ–çš„äº’è”ç½‘è§†é¢‘ï¼Œç»è¿‡ä¸“å®¶çš„ä¸¥æ ¼å¤šé˜¶æ®µè´¨é‡æ§åˆ¶è¿›è¡Œæ ‡æ³¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›¸æœºè¿åŠ¨åŸè¯­çš„åˆ†ç±»æ³•ï¼Œå¹¶ä¸ç”µå½±æ‘„å½±å¸ˆåˆä½œè®¾è®¡ã€‚ç ”ç©¶å‘ç°ï¼ŒæŸäº›è¿åŠ¨å¦‚â€œè·Ÿéšâ€éœ€è¦ç†è§£åœºæ™¯å†…å®¹ï¼Œè€Œé€šè¿‡CameraBenchè¯„ä¼°çš„æ¨¡å‹åœ¨æ•æ‰è¯­ä¹‰åŸè¯­å’Œå‡ ä½•åŸè¯­æ—¶å­˜åœ¨å›°éš¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.16656', 'title': 'Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning', 'url': 'https://huggingface.co/papers/2504.16656', 'abstract': "We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and 74.0 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B.", 'score': 41, 'issue_id': 3462, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 23', 'zh': '4æœˆ23æ—¥'}, 'hash': '2fbbb67c31db3cf1', 'authors': ['Chris', 'Yichen Wei', 'Yi Peng', 'Xiaokun Wang', 'Weijie Qiu', 'Wei Shen', 'Tianyidan Xie', 'Jiangbo Pei', 'Jianhao Zhang', 'Yunzhuo Hao', 'Xuchen Song', 'Yang Liu', 'Yahui Zhou'], 'affiliations': ['Skywork AI, Kunlun Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2504.16656.jpg', 'data': {'categories': ['#hallucinations', '#benchmark', '#training', '#agents', '#optimization', '#reasoning', '#multimodal', '#open_source', '#rl'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Skywork R1V2: ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑÑ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼', 'desc': "Skywork R1V2 - ÑÑ‚Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½ÑƒÑ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°Ğ¼Ğ¸, Ñ€ĞµÑˆĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹. Ğ’Ğ²ĞµĞ´ĞµĞ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Selective Sample Buffer Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğµ 'Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²' Ğ² Group Relative Policy Optimization. R1V2 Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°Ñ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼."}, 'en': {'title': 'Skywork R1V2: Elevating Multimodal Reasoning with Hybrid Learning', 'desc': 'Skywork R1V2 is an advanced multimodal reasoning model that improves upon its predecessor by integrating a hybrid reinforcement learning approach. This model combines reward-model guidance with rule-based strategies to enhance reasoning capabilities while ensuring broad generalization. To optimize training, it introduces the Selective Sample Buffer (SSB) mechanism, which prioritizes high-value samples and addresses the Vanishing Advantages issue in Group Relative Policy Optimization. The model has demonstrated outstanding performance on various benchmarks, outperforming existing open-source models and approaching the capabilities of leading proprietary systems.'}, 'zh': {'title': 'Skywork R1V2ï¼šå¤šæ¨¡æ€æ¨ç†çš„æ–°çªç ´', 'desc': 'Skywork R1V2æ˜¯ä¸€ä¸ªæ–°ä¸€ä»£çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œç›¸æ¯”äºå‰èº«Skywork R1Væœ‰äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚å®ƒå¼•å…¥äº†ä¸€ç§æ··åˆå¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œå°†å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ä¸åŸºäºè§„åˆ™çš„ç­–ç•¥ç›¸ç»“åˆï¼Œè§£å†³äº†å¤æ‚æ¨ç†èƒ½åŠ›ä¸å¹¿æ³›æ³›åŒ–ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡ï¼Œæå‡ºäº†é€‰æ‹©æ€§æ ·æœ¬ç¼“å†²æœºåˆ¶ï¼ˆSSBï¼‰ï¼Œæœ‰æ•ˆåº”å¯¹äº†ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ä¸­çš„â€œæ¶ˆå¤±ä¼˜åŠ¿â€å›°å¢ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒR1V2åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å¼€æºæ¨¡å‹ï¼Œç¼©å°äº†ä¸é¡¶çº§ä¸“æœ‰ç³»ç»Ÿçš„æ€§èƒ½å·®è·ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18415', 'title': 'BitNet v2: Native 4-bit Activations with Hadamard Transformation for\n  1-bit LLMs', 'url': 'https://huggingface.co/papers/2504.18415', 'abstract': 'Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.', 'score': 22, 'issue_id': 3458, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': '1b18f5117da74852', 'authors': ['Hongyu Wang', 'Shuming Ma', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2504.18415.jpg', 'data': {'categories': ['#small_models', '#optimization', '#inference', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿Ğ¾Ñ‚ĞµÑ€Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°', 'desc': 'BitNet v2 - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ 1-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ½Ğ° Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑÑ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑÑÑ‚ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ¾ Ğ¼Ğ°Ğ»Ğ¾Ğ³Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ° Ğ±Ğ¸Ñ‚. ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ - Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ H-BitLinear, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‰Ğ¸Ğ¹ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½-Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞĞ´Ğ°Ğ¼Ğ°Ñ€Ğ° Ğ¿ĞµÑ€ĞµĞ´ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ BitNet v2 Ñ 4-Ğ±Ğ¸Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ BitNet b1.58, Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹.'}, 'en': {'title': 'Revolutionizing 1-bit LLMs with Efficient 4-bit Quantization', 'desc': 'This paper presents BitNet v2, a new framework designed to improve the deployment of 1-bit Large Language Models (LLMs) by enabling efficient 4-bit activation quantization. The challenge of activation outliers, which complicate the quantization process, is addressed through a novel module called H-BitLinear that applies an online Hadamard transformation. This transformation helps to convert sharp activation distributions into smoother, Gaussian-like forms, making them more suitable for low-bit representation. The results demonstrate that BitNet v2 can be trained with 4-bit activations with minimal performance loss, significantly lowering memory usage and computational costs during inference.'}, 'zh': {'title': 'é«˜æ•ˆé‡åŒ–ï¼Œè½»æ¾éƒ¨ç½²1ä½è¯­è¨€æ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºBitNet v2çš„æ–°æ¡†æ¶ï¼Œæ—¨åœ¨é«˜æ•ˆéƒ¨ç½²1ä½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥H-BitLinearæ¨¡å—ï¼Œè§£å†³äº†åœ¨æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œæ¿€æ´»ä¸­å‡ºç°çš„å¼‚å¸¸å€¼é—®é¢˜ï¼Œä»è€Œå®ç°åŸç”Ÿ4ä½æ¿€æ´»é‡åŒ–ã€‚H-BitLinearæ¨¡å—é€šè¿‡åœ¨çº¿Hadamardå˜æ¢ï¼Œå°†æ¿€æ´»åˆ†å¸ƒå¹³æ»‘ä¸ºæ›´æ¥è¿‘é«˜æ–¯åˆ†å¸ƒçš„å½¢å¼ï¼Œé€‚åˆä½ä½è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨åŸç”Ÿ4ä½æ¿€æ´»è®­ç»ƒçš„BitNet v2åœ¨æ€§èƒ½ä¸Šå‡ ä¹æ²¡æœ‰ä¸‹é™ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†å†…å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.17821', 'title': 'VideoVista-CulturalLingo: 360^circ Horizons-Bridging Cultures,\n  Languages, and Domains in Video Comprehension', 'url': 'https://huggingface.co/papers/2504.17821', 'abstract': 'Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.', 'score': 18, 'issue_id': 3465, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 23', 'zh': '4æœˆ23æ—¥'}, 'hash': 'c1c204a6334a0547', 'authors': ['Xinyu Chen', 'Yunxin Li', 'Haoyuan Shi', 'Baotian Hu', 'Wenhan Luo', 'Yaowei Wang', 'Min Zhang'], 'affiliations': ['Harbin Institute of Technology, Shenzhen, China', 'Hong Kong University of Science and Technology'], 'pdf_title_img': 'assets/pdf/title_img/2504.17821.jpg', 'data': {'categories': ['#low_resource', '#multilingual', '#benchmark', '#open_source', '#reasoning', '#video', '#dataset', '#science'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ¾Ğ¼', 'desc': 'VideoVista-CulturalLingo - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸, ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾Ğµ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ· ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€ ĞšĞ¸Ñ‚Ğ°Ñ, Ğ¡ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ ĞĞ¼ĞµÑ€Ğ¸ĞºĞ¸ Ğ¸ Ğ•Ğ²Ñ€Ğ¾Ğ¿Ñ‹, Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ½Ğ° ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ ÑĞ¾Ñ‚Ğ½Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ¾Ğ². Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…ÑƒĞ¶Ğµ ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ¾Ğ¹, Ğ¸ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² Ğ²Ğ¸Ğ´ĞµĞ¾. ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ² Ñ‚Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ ĞºĞ°Ğº Ğ¿Ñ€Ğ¾Ğ¿Ñ€Ğ¸ĞµÑ‚Ğ°Ñ€Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ….'}, 'en': {'title': 'Bridging Cultures in Video Comprehension Evaluation', 'desc': 'This paper introduces VideoVista-CulturalLingo, a new benchmark for evaluating video comprehension in multimodal AI systems. It addresses the limitations of existing benchmarks by incorporating cultural diversity, offering questions in both Chinese and English, and covering a wide range of video domains. The study evaluates 24 video models and reveals that these models struggle with Chinese-centric questions and temporal understanding tasks. Additionally, it highlights the disparity in performance between mainstream and open-source models, particularly in scientific and mathematical contexts.'}, 'zh': {'title': 'è·¨æ–‡åŒ–è§†é¢‘ç†è§£çš„æ–°åŸºå‡†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†VideoVista-CulturalLingoï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è·¨è¶Šæ–‡åŒ–ã€è¯­è¨€å’Œé¢†åŸŸå·®å¼‚çš„è§†é¢‘ç†è§£è¯„ä¼°åŸºå‡†ã€‚è¯¥åŸºå‡†åŒ…å«æ¥è‡ªä¸­å›½ã€åŒ—ç¾å’Œæ¬§æ´²çš„æ–‡åŒ–å¤šæ ·æ€§ï¼Œé—®é¢˜ä»¥ä¸­æ–‡å’Œè‹±æ–‡ä¸¤ç§è¯­è¨€å‘ˆç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†24ä¸ªæœ€æ–°çš„è§†é¢‘å¤§æ¨¡å‹ï¼Œå‘ç°ç°æœ‰æ¨¡å‹åœ¨ä»¥ä¸­æ–‡ä¸ºä¸­å¿ƒçš„é—®é¢˜ä¸Šè¡¨ç°è¾ƒå·®ï¼Œå°¤å…¶æ˜¯ä¸ä¸­å›½å†å²ç›¸å…³çš„é—®é¢˜ã€‚å®éªŒç»“æœè¿˜æ˜¾ç¤ºï¼Œå½“å‰å¼€æºæ¨¡å‹åœ¨æ—¶é—´ç†è§£æ–¹é¢å­˜åœ¨å±€é™ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹ä»¶å®šä½ä»»åŠ¡ä¸­ï¼Œæœ€é«˜å¾—åˆ†ä»…ä¸º45.2%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.16427', 'title': 'Can Large Language Models Help Multimodal Language Analysis? MMLA: A\n  Comprehensive Benchmark', 'url': 'https://huggingface.co/papers/2504.16427', 'abstract': 'Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.', 'score': 11, 'issue_id': 3457, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 23', 'zh': '4æœˆ23æ—¥'}, 'hash': '1845c93e3bc64dd1', 'authors': ['Hanlei Zhang', 'Zhuohang Li', 'Yeshuang Zhu', 'Hua Xu', 'Peiwu Wang', 'Haige Zhu', 'Jie Zhou', 'Jinchao Zhang'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University', 'Kennesaw State University', 'Pattern Recognition Center, WeChat AI, Tencent Inc, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.16427.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#open_source', '#multimodal'], 'emoji': 'ğŸ—£ï¸', 'ru': {'title': 'MMLA: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MMLA - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºÑƒ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸. MMLA Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ 61 Ñ‚Ñ‹ÑÑÑ‡ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ 6 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ°ÑĞ¿ĞµĞºÑ‚Ğ¾Ğ² ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ğ±ÑˆĞ¸Ñ€Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»Ñ, Ñ‚Ğ¾Ğ½ĞºĞ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ»Ğ¸ÑˆÑŒ 60-70%, Ñ‡Ñ‚Ğ¾ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¯Ğœ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¾Ğ¹ Ñ€ĞµÑ‡Ğ¸.'}, 'en': {'title': 'Unlocking Multimodal Understanding in Language Models', 'desc': 'This paper introduces MMLA, a new benchmark for evaluating multimodal large language models (MLLMs) in understanding complex human conversations. It includes over 61,000 multimodal utterances that assess six key aspects of semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. The study evaluates various LLMs and MLLMs using different methods, revealing that even the best fine-tuned models only achieve 60-70% accuracy in understanding these nuances. The authors aim for MMLA to be a foundational resource for advancing research in multimodal language analysis.'}, 'zh': {'title': 'å¤šæ¨¡æ€è¯­è¨€åˆ†æçš„æ–°åŸºå‡†ï¼šMMLA', 'desc': 'å¤šæ¨¡æ€è¯­è¨€åˆ†ææ˜¯ä¸€ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸï¼Œåˆ©ç”¨å¤šç§æ¨¡æ€æ¥å¢å¼ºå¯¹äººç±»å¯¹è¯è¯­ä¹‰çš„ç†è§£ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œä½†å…³äºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç†è§£è®¤çŸ¥å±‚é¢è¯­ä¹‰çš„ç ”ç©¶ä»ç„¶è¾ƒå°‘ã€‚æœ¬æ–‡ä»‹ç»äº†MMLAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„åŸºå‡†ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼ŒåŒ…å«è¶…è¿‡61Kä¸ªæ¥è‡ªä¸åŒåœºæ™¯çš„å¤šæ¨¡æ€è¯è¯­ã€‚é€šè¿‡å¯¹å…«ç§ä¸»æµLLMså’ŒMLLMsçš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå³ä½¿ç»è¿‡å¾®è°ƒçš„æ¨¡å‹åœ¨ç†è§£å¤æ‚äººç±»è¯­è¨€æ–¹é¢çš„å‡†ç¡®ç‡ä¹Ÿä»…ä¸º60%~70%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.17768', 'title': 'The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs', 'url': 'https://huggingface.co/papers/2504.17768', 'abstract': 'Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.', 'score': 8, 'issue_id': 3462, 'pub_date': '2025-04-24', 'pub_date_card': {'ru': '24 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 24', 'zh': '4æœˆ24æ—¥'}, 'hash': 'ff60c7d30ad1f761', 'authors': ['Piotr Nawrot', 'Robert Li', 'Renjie Huang', 'Sebastian Ruder', 'Kelly Marchisio', 'Edoardo M. Ponti'], 'affiliations': ['Cohere', 'Meta', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2504.17768.jpg', 'data': {'categories': ['#long_context', '#architecture', '#training', '#optimization'], 'emoji': 'ğŸ•¸ï¸', 'ru': {'title': 'Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: ĞºĞ»ÑÑ‡ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…', 'desc': 'Ğ­Ñ‚Ğ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ„Ğ¾ĞºÑƒÑĞ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ² Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ´Ğ»Ğ¸Ğ½Ğ°Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½ÑÑ… Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ´Ğ»Ñ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚Ğ¸Ñ‚ĞµĞ»ÑŒĞ½ĞµĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ»ÑĞµÑ‚, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ¼Ğ¿Ñ€Ğ¾Ğ¼Ğ¸ÑÑĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğº Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Long-Context Potential with Sparse Attention', 'desc': 'This paper investigates the use of sparse attention in Transformer large language models (LLMs) to improve their ability to handle long sequences. The authors conduct a thorough analysis of various sparse attention methods, examining their performance across different model sizes, sequence lengths, and levels of sparsity. Key findings indicate that larger, sparse models are more effective for long sequences, but the optimal sparsity level varies depending on the task and phase of processing. The study also introduces new scaling laws for sparse attention, emphasizing the need for careful consideration of efficiency and accuracy trade-offs in practical applications.'}, 'zh': {'title': 'ç¨€ç–æ³¨æ„åŠ›ï¼šæå‡å˜æ¢å™¨æ¨¡å‹å¤„ç†é•¿åºåˆ—çš„å…³é”®å·¥å…·', 'desc': 'ç¨€ç–æ³¨æ„åŠ›æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„ç­–ç•¥ï¼Œå¯ä»¥æ‰©å±•å˜æ¢å™¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¤„ç†é•¿ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡å¯¹ä¸åŒæ¨¡å‹è§„æ¨¡ã€åºåˆ—é•¿åº¦å’Œç¨€ç–æ°´å¹³çš„è®­ç»ƒæ— å…³ç¨€ç–æ³¨æ„åŠ›æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œæ¢è®¨äº†å…¶æœ‰æ•ˆæ€§å’Œæ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¯¹äºéå¸¸é•¿çš„åºåˆ—ï¼Œè¾ƒå¤§ä¸”é«˜åº¦ç¨€ç–çš„æ¨¡å‹ä¼˜äºè¾ƒå°ä¸”å¯†é›†çš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œç¨€ç–æ³¨æ„åŠ›å¹¶ä¸æ˜¯é€šç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºåœ¨æŸäº›ä»»åŠ¡ä¸Šï¼Œå³ä½¿æ˜¯é€‚åº¦çš„ç¨€ç–æ°´å¹³ä¹Ÿä¼šå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.17816', 'title': 'Subject-driven Video Generation via Disentangled Identity and Motion', 'url': 'https://huggingface.co/papers/2504.17816', 'abstract': 'We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.', 'score': 8, 'issue_id': 3463, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 23', 'zh': '4æœˆ23æ—¥'}, 'hash': '01b4ca69c55b44f4', 'authors': ['Daneul Kim', 'Jingxu Zhang', 'Wonjoon Jin', 'Sunghyun Cho', 'Qi Dai', 'Jaesik Park', 'Chong Luo'], 'affiliations': ['Microsoft Research Asia', 'POSTECH', 'Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2504.17816.jpg', 'data': {'categories': ['#video', '#dataset', '#training'], 'emoji': 'ğŸ¬', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ˜Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸ĞºĞµ ÑÑƒĞ±ÑŠĞµĞºÑ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑÑ‚ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑÑ‚Ğ¾Ñ…Ğ°ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ… Ğ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Zero-Shot Video Customization through Subject-Driven Learning', 'desc': 'This paper presents a novel approach to video generation that separates the learning of subject-specific features from the temporal dynamics, allowing for zero-shot customization without the need for additional tuning. Instead of relying on large annotated video datasets, the authors utilize an image customization dataset to inject identity into the video generation process. They also preserve temporal modeling by using a small set of unannotated videos, employing an image-to-video training method. The introduction of techniques like random image token dropping and stochastic switching during optimization helps improve subject consistency and scalability, leading to superior performance compared to existing models.'}, 'zh': {'title': 'æ— è°ƒä¼˜çš„ä¸»é¢˜é©±åŠ¨è§†é¢‘ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡è§£è€¦ä¸»é¢˜ç‰¹å®šå­¦ä¹ ä¸æ—¶é—´åŠ¨æ€çš„æ–¹å¼ï¼Œè®­ç»ƒä¸»é¢˜é©±åŠ¨çš„å®šåˆ¶è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä¸”æ— éœ€é¢å¤–è°ƒä¼˜ã€‚ä¼ ç»Ÿçš„è§†é¢‘å®šåˆ¶æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤§å‹æ ‡æ³¨è§†é¢‘æ•°æ®é›†ï¼Œè¿™ä¸ä»…è®¡ç®—æˆæœ¬é«˜ï¼Œè€Œä¸”éœ€è¦å¤§é‡çš„æ ‡æ³¨å·¥ä½œã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ç›´æ¥åœ¨è®­ç»ƒè§†é¢‘å®šåˆ¶æ¨¡å‹æ—¶ä½¿ç”¨å›¾åƒå®šåˆ¶æ•°æ®é›†ï¼Œå°†è§†é¢‘å®šåˆ¶åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šé€šè¿‡å›¾åƒå®šåˆ¶æ•°æ®é›†è¿›è¡Œèº«ä»½æ³¨å…¥ï¼Œä»¥åŠé€šè¿‡å›¾åƒåˆ°è§†é¢‘çš„è®­ç»ƒæ–¹æ³•ä¿ç•™æ—¶é—´å»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„ä¸»é¢˜ä¸€è‡´æ€§å’Œå¯æ‰©å±•æ€§ï¼Œè¶…è¶Šäº†ç°æœ‰çš„è§†é¢‘å®šåˆ¶æ¨¡å‹ï¼Œè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18425', 'title': 'Kimi-Audio Technical Report', 'url': 'https://huggingface.co/papers/2504.18425', 'abstract': 'We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.', 'score': 6, 'issue_id': 3467, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': 'f06be2f46c80f40a', 'authors': ['KimiTeam', 'Ding Ding', 'Zeqian Ju', 'Yichong Leng', 'Songxiang Liu', 'Tong Liu', 'Zeyu Shang', 'Kai Shen', 'Wei Song', 'Xu Tan', 'Heyi Tang', 'Zhengtao Wang', 'Chu Wei', 'Yifei Xin', 'Xinran Xu', 'Jianwei Yu', 'Yutao Zhang', 'Xinyu Zhou', 'Y. Charles', 'Jun Chen', 'Yanru Chen', 'Yulun Du', 'Weiran He', 'Zhenxing Hu', 'Guokun Lai', 'Qingcheng Li', 'Yangyang Liu', 'Weidong Sun', 'Jianzhou Wang', 'Yuzhi Wang', 'Yuefeng Wu', 'Yuxin Wu', 'Dongchao Yang', 'Hao Yang', 'Ying Yang', 'Zhilin Yang', 'Aoxiong Yin', 'Ruibin Yuan', 'Yutong Zhang', 'Zaida Zhou'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.18425.jpg', 'data': {'categories': ['#inference', '#training', '#benchmark', '#dataset', '#open_source', '#audio', '#data'], 'emoji': 'ğŸµ', 'ru': {'title': 'Kimi-Audio: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞ¾ Ğ·Ğ²ÑƒĞºĞ¾Ğ¼', 'desc': 'Kimi-Audio - ÑÑ‚Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ² Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğ¸, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°ÑƒĞ´Ğ¸Ğ¾. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ¾Ğ¹ 12.5 Ğ“Ñ† Ğ¸ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ¸ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ. Ğ”Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ»ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡ĞµĞ¼ 13 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ñ‡Ğ°ÑĞ¾Ğ² Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. Kimi-Audio Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ½Ğ°Ğ¸Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€ÑĞ´Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ·Ğ°Ğ´Ğ°Ñ‡, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸.'}, 'en': {'title': 'Kimi-Audio: Revolutionizing Audio Understanding and Generation', 'desc': 'Kimi-Audio is an advanced open-source audio foundation model designed for tasks like audio understanding, generation, and conversation. The model utilizes a unique architecture that processes continuous audio features and outputs discrete tokens, supported by a specialized audio tokenizer. It has been trained on an extensive dataset of over 13 million hours of diverse audio, including speech and music, ensuring high-quality performance across various audio tasks. Evaluation results indicate that Kimi-Audio sets new benchmarks in areas such as speech recognition and audio question answering, making it a significant contribution to the field of audio machine learning.'}, 'zh': {'title': 'Kimi-Audioï¼šéŸ³é¢‘ç†è§£ä¸ç”Ÿæˆçš„å…ˆé”‹', 'desc': 'Kimi-Audioæ˜¯ä¸€ä¸ªå¼€æºçš„éŸ³é¢‘åŸºç¡€æ¨¡å‹ï¼Œä¸“æ³¨äºéŸ³é¢‘ç†è§£ã€ç”Ÿæˆå’Œå¯¹è¯ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†12.5Hzçš„éŸ³é¢‘æ ‡è®°å™¨ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ¶æ„ï¼Œè¾“å…¥ä¸ºè¿ç»­ç‰¹å¾ï¼Œè¾“å‡ºä¸ºç¦»æ•£æ ‡è®°ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡1300ä¸‡å°æ—¶éŸ³é¢‘æ•°æ®çš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ¶µç›–äº†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ç­‰å¤šç§æ¨¡æ€ã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼ŒKimi-Audioåœ¨è¯­éŸ³è¯†åˆ«ã€éŸ³é¢‘ç†è§£ã€éŸ³é¢‘é—®ç­”å’Œè¯­éŸ³å¯¹è¯ç­‰å¤šä¸ªéŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.15716', 'title': 'DianJin-R1: Evaluating and Enhancing Financial Reasoning in Large\n  Language Models', 'url': 'https://huggingface.co/papers/2504.15716', 'abstract': 'Effective reasoning remains a core challenge for large language models (LLMs) in the financial domain, where tasks often require domain-specific knowledge, precise numerical calculations, and strict adherence to compliance rules. We propose DianJin-R1, a reasoning-enhanced framework designed to address these challenges through reasoning-augmented supervision and reinforcement learning. Central to our approach is DianJin-R1-Data, a high-quality dataset constructed from CFLUE, FinQA, and a proprietary compliance corpus (Chinese Compliance Check, CCC), combining diverse financial reasoning scenarios with verified annotations. Our models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned from Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct using a structured format that generates both reasoning steps and final answers. To further refine reasoning quality, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning method that incorporates dual reward signals: one encouraging structured outputs and another rewarding answer correctness. We evaluate our models on five benchmarks: three financial datasets (CFLUE, FinQA, and CCC) and two general reasoning benchmarks (MATH-500 and GPQA-Diamond). Experimental results show that DianJin-R1 models consistently outperform their non-reasoning counterparts, especially on complex financial tasks. Moreover, on the real-world CCC dataset, our single-call reasoning models match or even surpass the performance of multi-agent systems that require significantly more computational cost. These findings demonstrate the effectiveness of DianJin-R1 in enhancing financial reasoning through structured supervision and reward-aligned learning, offering a scalable and practical solution for real-world applications.', 'score': 6, 'issue_id': 3463, 'pub_date': '2025-04-22', 'pub_date_card': {'ru': '22 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 22', 'zh': '4æœˆ22æ—¥'}, 'hash': '89be69a3fc0614a5', 'authors': ['Jie Zhu', 'Qian Chen', 'Huaixia Dou', 'Junhui Li', 'Lifan Guo', 'Feng Chen', 'Chi Zhang'], 'affiliations': ['Qwen DianJin Team, Alibaba Cloud Computing', 'Soochow University'], 'pdf_title_img': 'assets/pdf/title_img/2504.15716.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#dataset', '#reasoning'], 'emoji': 'ğŸ’¹', 'ru': {'title': 'Ğ£ÑĞ¸Ğ»ĞµĞ½Ğ¸Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ˜Ğ˜ Ñ‡ĞµÑ€ĞµĞ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ', 'desc': 'DianJin-R1 - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ DianJin-R1-Data Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ DianJin-R1-7B Ğ¸ DianJin-R1-32B Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ°Ğº ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹ Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ….'}, 'en': {'title': 'Enhancing Financial Reasoning with DianJin-R1', 'desc': 'This paper introduces DianJin-R1, a framework aimed at improving reasoning capabilities in large language models (LLMs) specifically for financial tasks. It utilizes a unique dataset, DianJin-R1-Data, which combines various financial reasoning scenarios with verified annotations to enhance model training. The models, DianJin-R1-7B and DianJin-R1-32B, are fine-tuned using structured formats that generate reasoning steps alongside final answers, and they leverage Group Relative Policy Optimization (GRPO) for better output quality. Results show that these models outperform traditional LLMs on complex financial tasks, demonstrating their effectiveness in real-world applications.'}, 'zh': {'title': 'DianJin-R1ï¼šæå‡é‡‘èæ¨ç†çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºDianJin-R1çš„æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é‡‘èé¢†åŸŸé¢ä¸´çš„æ¨ç†æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨ç†å¢å¼ºç›‘ç£å’Œå¼ºåŒ–å­¦ä¹ æ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½¿ç”¨äº†é«˜è´¨é‡çš„æ•°æ®é›†DianJin-R1-Dataï¼Œç»“åˆäº†å¤šç§é‡‘èæ¨ç†åœºæ™¯ã€‚æˆ‘ä»¬å¯¹DianJin-R1-7Bå’ŒDianJin-R1-32Bæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œé‡‡ç”¨ç»“æ„åŒ–æ ¼å¼ç”Ÿæˆæ¨ç†æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆï¼Œå¹¶åº”ç”¨äº†åŒé‡å¥–åŠ±ä¿¡å·çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ¥ä¼˜åŒ–æ¨ç†è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDianJin-R1æ¨¡å‹åœ¨å¤æ‚é‡‘èä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºéæ¨ç†æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.12080', 'title': 'DC-SAM: In-Context Segment Anything in Images and Videos via Dual\n  Consistency', 'url': 'https://huggingface.co/papers/2504.12080', 'abstract': "Given a single labeled example, in-context segmentation aims to segment corresponding objects. This setting, known as one-shot segmentation in few-shot learning, explores the segmentation model's generalization ability and has been applied to various vision tasks, including scene understanding and image/video editing. While recent Segment Anything Models have achieved state-of-the-art results in interactive segmentation, these approaches are not directly applicable to in-context segmentation. In this work, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos. Our key insights are to enhance the features of the SAM's prompt encoder in segmentation by providing high-quality visual prompts. When generating a mask prior, we fuse the SAM features to better align the prompt encoder. Then, we design a cycle-consistent cross-attention on fused features and initial visual prompts. Next, a dual-branch design is provided by using the discriminative positive and negative prompts in the prompt encoder. Furthermore, we design a simple mask-tube training strategy to adopt our proposed dual consistency method into the mask tube. Although the proposed DC-SAM is primarily designed for images, it can be seamlessly extended to the video domain with the support of SAM2. Given the absence of in-context segmentation in the video domain, we manually curate and construct the first benchmark from existing video segmentation datasets, named In-Context Video Object Segmentation (IC-VOS), to better assess the in-context capability of the model. Extensive experiments demonstrate that our method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on PASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our source code and benchmark are available at https://github.com/zaplm/DC-SAM.", 'score': 6, 'issue_id': 3459, 'pub_date': '2025-04-16', 'pub_date_card': {'ru': '16 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 16', 'zh': '4æœˆ16æ—¥'}, 'hash': 'e91663786d7f873e', 'authors': ['Mengshi Qi', 'Pengfei Zhu', 'Xiangtai Li', 'Xiaoyang Bi', 'Lu Qi', 'Huadong Ma', 'Ming-Hsuan Yang'], 'affiliations': ['Nanyang Technological University, Singapore', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China', 'UC Merced, US'], 'pdf_title_img': 'assets/pdf/title_img/2504.12080.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#cv', '#games'], 'emoji': 'ğŸ­', 'ru': {'title': 'DC-SAM: ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ°Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Dual Consistency SAM (DC-SAM) Ğ´Ğ»Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ SAM Ğ¸ SAM2 Ğº Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ ĞºĞ°Ğº Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°Ğº Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº SAM, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¸ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ´Ğ²ÑƒÑ…Ğ²ĞµÑ‚Ğ²ĞµĞ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ mask-tube Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´ĞµĞ¾. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… IC-VOS Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ.'}, 'en': {'title': 'Enhancing One-Shot Segmentation with Dual Consistency', 'desc': "This paper introduces the Dual Consistency SAM (DC-SAM) method for one-shot segmentation, which allows a model to segment objects using just one labeled example. It enhances the Segment Anything Model (SAM) by improving the prompt encoder's features through high-quality visual prompts and a cycle-consistent cross-attention mechanism. The authors also present a new benchmark for in-context video object segmentation, named IC-VOS, to evaluate their method's performance in video tasks. Experimental results show that DC-SAM outperforms existing models on several datasets, demonstrating its effectiveness in both image and video segmentation tasks."}, 'zh': {'title': 'åŒä¸€è‡´æ€§SAMï¼šæå‡ä¸Šä¸‹æ–‡åˆ†å‰²èƒ½åŠ›çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºåŒä¸€è‡´æ€§SAMï¼ˆDC-SAMï¼‰çš„æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å•ä¸ªæ ‡è®°ç¤ºä¾‹ä¸‹çš„ä¸Šä¸‹æ–‡åˆ†å‰²é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡æç¤ºè°ƒä¼˜æ¥é€‚åº”ç°æœ‰çš„åˆ†å‰²æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å›¾åƒå’Œè§†é¢‘çš„ä¸Šä¸‹æ–‡åˆ†å‰²ã€‚æˆ‘ä»¬é€šè¿‡æä¾›é«˜è´¨é‡çš„è§†è§‰æç¤ºæ¥å¢å¼ºåˆ†å‰²æ¨¡å‹çš„ç‰¹å¾ï¼Œå¹¶è®¾è®¡äº†å¾ªç¯ä¸€è‡´çš„äº¤å‰æ³¨æ„æœºåˆ¶æ¥ä¼˜åŒ–æç¤ºç¼–ç å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDC-SAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå±•ç¤ºäº†å…¶åœ¨ä¸Šä¸‹æ–‡åˆ†å‰²ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.17025', 'title': 'Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing\n  Efficiency Through Vocabulary Adaptation', 'url': 'https://huggingface.co/papers/2504.17025', 'abstract': 'The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.', 'score': 2, 'issue_id': 3466, 'pub_date': '2025-04-23', 'pub_date_card': {'ru': '23 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 23', 'zh': '4æœˆ23æ—¥'}, 'hash': '410439717a73203b', 'authors': ['Luca Moroni', 'Giovanni Puccetti', 'Pere-Lluis Huguet Cabot', 'Andrei Stefan Bejgu', 'Edoardo Barba', 'Alessio Miaschi', "Felice Dell'Orletta", 'Andrea Esuli', 'Roberto Navigli'], 'affiliations': ['Babelscape', 'ILC-CNR', 'ISTI-CNR', 'Sapienza University of Rome'], 'pdf_title_img': 'assets/pdf/title_img/2504.17025.jpg', 'data': {'categories': ['#small_models', '#multilingual', '#low_resource', '#transfer_learning', '#training'], 'emoji': 'ğŸ‡®ğŸ‡¹', 'ru': {'title': 'ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ³Ğ»Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… LLM Ğ´Ğ»Ñ Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğ´Ğ»Ñ Ğ¸Ñ‚Ğ°Ğ»ÑŒÑĞ½ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ SAVA, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ¼ĞµĞ½Ñ‹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ. ĞĞ½Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Mistral-7b-v0.1 Ğ¸ Llama-3.1-8B, ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ğ² Ñ‚Ğ¾ĞºĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ ÑĞ²Ğ¾Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.'}, 'en': {'title': 'Optimizing LLMs for Italian: SAVA Unleashed!', 'desc': 'This paper addresses the challenge of adapting Large Language Models (LLMs) for non-English languages, specifically Italian, which often suffer from inefficiencies due to language contamination. The authors introduce a new method called Semantic Alignment Vocabulary Adaptation (SAVA), which uses neural mapping to optimize vocabulary for better performance. By applying SAVA to two LLMs, they demonstrate a significant reduction in token fertility and a decrease in model parameters while maintaining competitive performance on various tasks. The results indicate that with proper vocabulary adaptation, LLMs can effectively learn and perform in target languages with minimal additional training.'}, 'zh': {'title': 'ä¼˜åŒ–è‹±è¯­æ¨¡å‹ï¼Œæå‡æ„å¤§åˆ©è¯­è¡¨ç°ï¼', 'desc': 'éšç€é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ•°é‡ä¸æ–­å¢åŠ ï¼Œå¤§å¤šæ•°æ¨¡å‹ä¸»è¦é’ˆå¯¹è‹±è¯­è®¾è®¡ã€‚è™½ç„¶æœ€å…ˆè¿›çš„LLMså¯ä»¥å¤„ç†å…¶ä»–è¯­è¨€ï¼Œä½†ç”±äºè¯­è¨€æ±¡æŸ“æˆ–å¤šè¯­è¨€é¢„è®­ç»ƒæ•°æ®çš„å½±å“ï¼Œå®ƒä»¬åœ¨éè‹±è¯­è¯­è¨€ä¸Šçš„è¡¨ç°å¹¶ä¸ç†æƒ³ï¼Œå¯¼è‡´ç¼–ç æ•ˆç‡ä½ä¸‹å’Œæ¨ç†é€Ÿåº¦ç¼“æ…¢ã€‚æœ¬æ–‡æ¯”è¾ƒäº†å¤šç§è¯æ±‡é€‚åº”æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–è‹±è¯­LLMsåœ¨æ„å¤§åˆ©è¯­ä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•â€”â€”è¯­ä¹‰å¯¹é½è¯æ±‡é€‚åº”ï¼ˆSAVAï¼‰ï¼Œåˆ©ç”¨ç¥ç»æ˜ å°„è¿›è¡Œè¯æ±‡æ›¿æ¢ã€‚é€šè¿‡å¯¹è¯æ±‡çš„é€‚åº”ï¼Œè¿™äº›æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€ä¸Šç»è¿‡æœ‰é™çš„æŒç»­è®­ç»ƒåï¼Œèƒ½å¤Ÿæ¢å¤å…¶æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2504.18225', 'title': 'Even Small Reasoners Should Quote Their Sources: Introducing the\n  Pleias-RAG Model Family', 'url': 'https://huggingface.co/papers/2504.18225', 'abstract': 'We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.', 'score': 1, 'issue_id': 3470, 'pub_date': '2025-04-25', 'pub_date_card': {'ru': '25 Ğ°Ğ¿Ñ€ĞµĞ»Ñ', 'en': 'April 25', 'zh': '4æœˆ25æ—¥'}, 'hash': '1dd188276b69d3b9', 'authors': ['Pierre-Carl Langlais', 'Pavel Chizhov', 'Mattia Nee', 'Carlos Rosas Hinostroza', 'Matthieu Delsart', 'IrÃ¨ne Girard', 'Othman Hicheur', 'Anastasia Stasenko', 'Ivan P. Yamshchikov'], 'affiliations': ['PleIAs, Paris, France'], 'pdf_title_img': 'assets/pdf/title_img/2504.18225.jpg', 'data': {'categories': ['#dataset', '#multilingual', '#reasoning', '#rag', '#benchmark', '#synthetic', '#small_models'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞœĞ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ RAG', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… RAG, Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² - Pleias-RAG-350m Ğ¸ Pleias-RAG-1B. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ÑĞ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¼ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ². ĞĞ½Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‚ Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ Ñ†Ğ¸Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¾ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ‚Ğ°Ñ‚, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°Ğ¼Ğ¸ RAG. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ SLM Ğ¼ĞµĞ½ÑŒÑˆĞµ 4 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ°Ñ€Ğ´Ğ¾Ğ² Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ğ¼ RAG Ğ¸ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ¾ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Small Models, Big Impact: Revolutionizing RAG with Pleias-RAG!', 'desc': 'The paper presents Pleias-RAG-350m and Pleias-RAG-1B, two small reasoning models designed for retrieval-augmented generation (RAG), search, and source summarization tasks. These models are trained on a synthetic dataset that simulates multilingual open-source retrieval, enhancing their ability to provide accurate citations and grounding. They incorporate advanced features like query routing and source reranking, outperforming smaller language models on RAG benchmarks while remaining competitive with larger models. Their compact size and deployment efficiency make them suitable for various generative AI applications, particularly in European languages.'}, 'zh': {'title': 'å°å‹æ¨ç†æ¨¡å‹ï¼Œè§£é”ç”ŸæˆAIæ–°åº”ç”¨', 'desc': 'æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°ä¸€ä»£çš„å°å‹æ¨ç†æ¨¡å‹ï¼Œé€‚ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€æœç´¢å’Œæºæ‘˜è¦ã€‚Pleias-RAG-350må’ŒPleias-RAG-1Båœ¨ä¸€ä¸ªå¤§å‹åˆæˆæ•°æ®é›†ä¸Šè¿›è¡Œä¸­ç­‰è®­ç»ƒï¼Œæ¨¡æ‹Ÿä»å…¬å…±è¯­æ–™åº“ä¸­æ£€ç´¢å¤šç§å¤šè¯­è¨€å¼€æ”¾æºçš„è¿‡ç¨‹ã€‚å®ƒä»¬åŸç”Ÿæ”¯æŒå¼•ç”¨å’ŒåŸºç¡€ï¼Œèƒ½å¤Ÿä½¿ç”¨ç›´æ¥å¼•ç”¨ï¼Œå¹¶é‡æ–°æ•´åˆä¸RAGå·¥ä½œæµç¨‹ç›¸å…³çš„å¤šä¸ªç‰¹æ€§ï¼Œå¦‚æŸ¥è¯¢è·¯ç”±ã€æŸ¥è¯¢é‡æ„å’Œæºé‡æ’åºã€‚è¿™äº›æ¨¡å‹åœ¨æ ‡å‡†åŒ–çš„RAGåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äº4äº¿å‚æ•°ä»¥ä¸‹çš„SLMï¼Œå¹¶ä¸ä¸€äº›æµè¡Œçš„å¤§å‹æ¨¡å‹ç«äº‰ï¼Œè§£é”äº†ç”ŸæˆAIçš„æ–°åº”ç”¨åœºæ™¯ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (1)', '#agi', '#alignment', '#architecture (1)', '#audio (1)', '#benchmark (8)', '#cv (2)', '#data (1)', '#dataset (8)', '#diffusion', '#ethics', '#games (1)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (2)', '#interpretability', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation', '#math', '#multilingual (3)', '#multimodal (3)', '#open_source (4)', '#optimization (3)', '#plp', '#rag (1)', '#reasoning (4)', '#rl (2)', '#rlhf', '#robotics', '#science (1)', '#security', '#small_models (3)', '#story_generation', '#survey', '#synthetic (1)', '#training (7)', '#transfer_learning (2)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-04-28 16:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-28 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-28 16:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    