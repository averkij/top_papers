
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. July 18.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">18 Ğ¸ÑĞ»Ñ</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-07-17.html">â¬…ï¸ <span id="prev-date">17.07</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-07-21.html">â¡ï¸ <span id="next-date">21.07</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-07.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '18 Ğ¸ÑĞ»Ñ', 'en': 'July 18', 'zh': '7æœˆ18æ—¥'};
        let feedDateNext = {'ru': '21.07', 'en': '07/21', 'zh': '7æœˆ21æ—¥'};
        let feedDatePrev = {'ru': '17.07', 'en': '07/17', 'zh': '7æœˆ17æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2507.13334', 'title': 'A Survey of Context Engineering for Large Language Models', 'url': 'https://huggingface.co/papers/2507.13334', 'abstract': 'Context Engineering systematically optimizes information payloads for Large Language Models, addressing gaps in generating sophisticated, long-form outputs.  \t\t\t\t\tAI-generated summary \t\t\t\t The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.', 'score': 76, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': 'e0191e89e0360224', 'authors': ['Lingrui Mei', 'Jiayu Yao', 'Yuyao Ge', 'Yiwei Wang', 'Baolong Bi', 'Yujun Cai', 'Jiazhi Liu', 'Mingyu Li', 'Zhong-Zhi Li', 'Duzhen Zhang', 'Chenlin Zhou', 'Jiayi Mao', 'Tianze Xia', 'Jiafeng Guo', 'Shenghua Liu'], 'affiliations': ['Institute of Computing Technology, Chinese Academy of Sciences', 'Peking University', 'The University of Queensland', 'Tsinghua University', 'University of California, Merced', 'University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2507.13334.jpg', 'data': {'categories': ['#data', '#survey', '#multimodal', '#long_context', '#architecture', '#rag'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ°Ñ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ñ: Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€Ğ°ÑĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ»Ğ° LLM', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸, Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ°ĞµĞ¼Ğ¾Ğ¹ Ğ² LLM Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ñ‚Ğ°ĞºĞ¸Ğµ Ñ‚ĞµĞ¼Ñ‹ ĞºĞ°Ğº Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°, ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹. Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ¾Ğ»ÑŒ Ğ¶Ğµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'}, 'en': {'title': 'Optimizing Context for Superior AI Outputs', 'desc': 'This paper introduces Context Engineering, a new approach to optimize the information provided to Large Language Models (LLMs) for better performance. It breaks down the process into key components like context retrieval, processing, and management, and shows how these can be combined in advanced systems like retrieval-augmented generation and multi-agent systems. The authors analyze over 1300 research papers to highlight a significant gap: while LLMs can understand complex contexts well, they struggle to generate sophisticated long-form content. The paper aims to provide a roadmap for future research to enhance the capabilities of LLMs in generating high-quality outputs.'}, 'zh': {'title': 'ä¼˜åŒ–ä¸Šä¸‹æ–‡ï¼Œæå‡è¯­è¨€æ¨¡å‹èƒ½åŠ›', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼ˆContext Engineeringï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¿¡æ¯è´Ÿè½½çš„æ­£å¼å­¦ç§‘ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒLLMsçš„æ€§èƒ½ä¸»è¦å–å†³äºæ¨ç†è¿‡ç¨‹ä¸­æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬å¯¹ä¸Šä¸‹æ–‡å·¥ç¨‹è¿›è¡Œäº†å…¨é¢çš„åˆ†ç±»ï¼Œåˆ†æäº†å…¶åŸºç¡€ç»„ä»¶åŠå…¶åœ¨æ™ºèƒ½ç³»ç»Ÿä¸­çš„å¤æ‚å®ç°ã€‚é€šè¿‡å¯¹1300å¤šç¯‡ç ”ç©¶è®ºæ–‡çš„ç³»ç»Ÿåˆ†æï¼Œæœ¬æ–‡æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç”Ÿæˆå¤æ‚é•¿æ–‡æœ¬è¾“å‡ºæ–¹é¢çš„æ˜¾è‘—å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†æœªæ¥ç ”ç©¶çš„ä¼˜å…ˆæ–¹å‘ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13348', 'title': 'VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning', 'url': 'https://huggingface.co/papers/2507.13348', 'abstract': 'VisionThink dynamically adjusts image resolution and visual token processing for efficient and effective vision-language tasks, improving performance on OCR tasks while reducing token usage in simpler tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.', 'score': 45, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '1f1a89020e55859c', 'authors': ['Senqiao Yang', 'Junyi Li', 'Xin Lai', 'Bei Yu', 'Hengshuang Zhao', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HKU', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2507.13348.jpg', 'data': {'categories': ['#cv', '#optimization', '#training', '#rl', '#games'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ', 'desc': "VisionThink - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞĞ½ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² (OCR), Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ² Ğ±Ğ¾Ğ»ĞµĞµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. VisionThink Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 'LLM-as-Judge' Ğ´Ğ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ½Ğ¾-Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° (VQA)."}, 'en': {'title': 'Dynamic Resolution for Efficient Vision-Language Processing', 'desc': 'VisionThink is a novel approach that optimizes image resolution and visual token processing for vision-language tasks. It intelligently adjusts the resolution of images based on the complexity of the task, allowing for efficient processing by reducing unnecessary visual tokens in simpler tasks. The model uses reinforcement learning to determine when to request higher-resolution images, enhancing performance on OCR tasks while maintaining accuracy in general visual question answering (VQA) tasks. This dynamic token compression strategy leads to improved efficiency and effectiveness in handling various vision-language challenges.'}, 'zh': {'title': 'åŠ¨æ€è°ƒæ•´ï¼Œæå‡è§†è§‰è¯­è¨€ä»»åŠ¡æ•ˆç‡', 'desc': 'VisionThink æ˜¯ä¸€ç§åŠ¨æ€è°ƒæ•´å›¾åƒåˆ†è¾¨ç‡å’Œè§†è§‰æ ‡è®°å¤„ç†çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è§†è§‰è¯­è¨€ä»»åŠ¡çš„æ•ˆç‡å’Œæ•ˆæœã€‚è¯¥æ–¹æ³•åœ¨å…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨ç®€å•ä»»åŠ¡ä¸­å‡å°‘äº†è§†è§‰æ ‡è®°çš„ä½¿ç”¨ã€‚é€šè¿‡æ™ºèƒ½åˆ¤æ–­å›¾åƒåˆ†è¾¨ç‡æ˜¯å¦è¶³å¤Ÿï¼ŒVisionThink å¯ä»¥åœ¨éœ€è¦æ—¶è¯·æ±‚æ›´é«˜åˆ†è¾¨ç‡çš„å›¾åƒã€‚ä¸ä¼ ç»Ÿçš„å›ºå®šå‹ç¼©æ–¹æ³•ä¸åŒï¼ŒVisionThink æ ¹æ®å…·ä½“æƒ…å†µè‡ªä¸»å†³å®šæ˜¯å¦å‹ç¼©æ ‡è®°ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶èŠ‚çœè®¡ç®—èµ„æºã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13347', 'title': 'Ï€^3: Scalable Permutation-Equivariant Visual Geometry Learning', 'url': 'https://huggingface.co/papers/2507.13347', 'abstract': 'A permutation-equivariant neural network, $\\pi^3$, reconstructs visual geometry without a fixed reference view, achieving state-of-the-art performance in camera pose estimation, depth estimation, and point map reconstruction.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce pi^3, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, pi^3 employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.', 'score': 32, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '006f7b52edb3a67a', 'authors': ['Yifan Wang', 'Jianjun Zhou', 'Haoyi Zhu', 'Wenzheng Chang', 'Yang Zhou', 'Zizun Li', 'Junyi Chen', 'Jiangmiao Pang', 'Chunhua Shen', 'Tong He'], 'affiliations': ['SII', 'Shanghai AI Lab', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2507.13347.jpg', 'data': {'categories': ['#open_source', '#cv', '#optimization', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ 3D ÑÑ†ĞµĞ½ Ğ±ĞµĞ· Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ñ‹Ñ… ĞºĞ°Ğ´Ñ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ï€^3 Ğ´Ğ»Ñ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ¿Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¸Ğ´Ğ°. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ï€^3 Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿ĞµÑ€ĞµÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾-ÑĞºĞ²Ğ¸Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ Ğ°Ñ„Ñ„Ğ¸Ğ½Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ· ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾-Ğ¸Ğ½Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚ Ñ‚Ğ¾Ñ‡ĞµĞº. Ğ¢Ğ°ĞºĞ¾Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ¾Ğ¹ Ğº Ğ¿Ğ¾Ñ€ÑĞ´ĞºÑƒ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ¹. Ï€^3 Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ€Ñ‚Ñ‹ Ñ‚Ğ¾Ñ‡ĞµĞº.'}, 'en': {'title': 'Revolutionizing Visual Geometry with Permutation-Equivariance', 'desc': 'The paper presents pi^3, a novel permutation-equivariant neural network designed for visual geometry reconstruction without relying on a fixed reference view. Traditional methods often depend on a specific viewpoint, which can introduce biases and lead to inaccuracies. In contrast, pi^3 utilizes a fully permutation-equivariant architecture to predict camera poses and point maps that are invariant to scale and reference frames. This innovative approach enhances robustness and scalability, allowing pi^3 to achieve state-of-the-art results in tasks like camera pose estimation and depth estimation.'}, 'zh': {'title': 'æ— å‚è€ƒè§†è§’çš„è§†è§‰å‡ ä½•é‡å»ºæ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºpi^3çš„ç¥ç»ç½‘ç»œï¼Œå®ƒåœ¨è§†è§‰å‡ ä½•é‡å»ºä¸­ä¸ä¾èµ–äºå›ºå®šçš„å‚è€ƒè§†è§’ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸å°†é‡å»ºé”šå®šåœ¨ç‰¹å®šçš„è§†ç‚¹ï¼Œè¿™ç§åç½®å¯èƒ½å¯¼è‡´ä¸ç¨³å®šå’Œå¤±è´¥ã€‚ä¸æ­¤ä¸åŒï¼Œpi^3é‡‡ç”¨å®Œå…¨çš„ç½®æ¢ç­‰å˜æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰å‚è€ƒæ¡†æ¶çš„æƒ…å†µä¸‹é¢„æµ‹ä»¿å°„ä¸å˜çš„ç›¸æœºå§¿æ€å’Œå°ºåº¦ä¸å˜çš„å±€éƒ¨ç‚¹å›¾ã€‚è¯¥æ¨¡å‹çš„è®¾è®¡ä½¿å…¶å¯¹è¾“å…¥é¡ºåºå…·æœ‰å†…åœ¨çš„é²æ£’æ€§ï¼Œå¹¶ä¸”å…·æœ‰å¾ˆé«˜çš„å¯æ‰©å±•æ€§ï¼Œä»è€Œåœ¨ç›¸æœºå§¿æ€ä¼°è®¡ã€å•ç›®/è§†é¢‘æ·±åº¦ä¼°è®¡å’Œå¯†é›†ç‚¹å›¾é‡å»ºç­‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13332', 'title': 'The Imitation Game: Turing Machine Imitator is Length Generalizable\n  Reasoner', 'url': 'https://huggingface.co/papers/2507.13332', 'abstract': 'TAIL, a method that imitates Turing Machine execution processes, enhances the length generalization and performance of LLMs by synthesizing chain-of-thought data and reducing shortcut learning.  \t\t\t\t\tAI-generated summary \t\t\t\t Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.', 'score': 31, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': 'd03adfbd7cec2623', 'authors': ['Zhouqi Hua', 'Wenwei Zhang', 'Chengqi Lyu', 'Yuzhe Gu', 'Songyang Gao', 'Kuikun Liu', 'Kai Chen'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2507.13332.jpg', 'data': {'categories': ['#data', '#synthetic', '#training', '#architecture', '#dataset', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ˜Ğ¼Ğ¸Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰Ğ°ÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'ĞœĞµÑ‚Ğ¾Ğ´ TAIL Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ½ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ°, Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑÑ ÑˆĞ°Ğ³Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ Ğ°Ñ‚Ğ¾Ğ¼Ğ°Ñ€Ğ½Ñ‹Ñ… ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹. TAIL Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğº Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Qwen2.5-7B Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸ Ğ¼Ğ°ÑˆĞ¸Ğ½Ñ‹ Ğ¢ÑŒÑÑ€Ğ¸Ğ½Ğ³Ğ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹ Ğ´Ğ»Ñ TAIL Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ.'}, 'en': {'title': 'Enhancing LLMs with Turing Machine Imitation for Better Generalization', 'desc': 'The paper introduces TAIL, a novel method that enhances the performance of large language models (LLMs) by mimicking the execution processes of Turing Machines. It addresses the challenge of length generalization, enabling LLMs to solve longer sequences than those seen during training. TAIL synthesizes chain-of-thought data to improve reasoning capabilities and reduce shortcut learning, which often leads to poor generalization. The method demonstrates significant improvements in performance across various tasks using synthetic data, highlighting the importance of Turing Machine concepts in LLM reasoning.'}, 'zh': {'title': 'æå‡LLMé•¿åº¦æ³›åŒ–èƒ½åŠ›çš„å›¾çµæœºæ¨¡ä»¿å­¦ä¹ ', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºTAILçš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚TAILé€šè¿‡æ¨¡æ‹Ÿå›¾çµæœºçš„æ‰§è¡Œè¿‡ç¨‹ï¼Œåˆæˆé“¾å¼æ€ç»´æ•°æ®ï¼Œä»è€Œå‡å°‘äº†å¿«æ·å­¦ä¹ ç°è±¡ã€‚è¯¥æ–¹æ³•é€šè¿‡çº¿æ€§æ‰©å±•æ¨ç†æ­¥éª¤ï¼Œæ”¹å–„äº†åŠ¨æ€å’Œé•¿è·ç¦»æ•°æ®è®¿é—®çš„éš¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAILåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†å›¾çµæœºçš„å…³é”®æ¦‚å¿µåœ¨é•¿åº¦æ³›åŒ–ä¸­çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.12841', 'title': 'AnyCap Project: A Unified Framework, Dataset, and Benchmark for\n  Controllable Omni-modal Captioning', 'url': 'https://huggingface.co/papers/2507.12841', 'abstract': "The AnyCap Project introduces a framework, dataset, and evaluation protocol to enhance controllability and reliability in multimodal captioning.  \t\t\t\t\tAI-generated summary \t\t\t\t Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.", 'score': 27, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '02e4f51787ec491d', 'authors': ['Yiming Ren', 'Zhiqiang Lin', 'Yu Li', 'Gao Meng', 'Weiyun Wang', 'Junjie Wang', 'Zicheng Lin', 'Jifeng Dai', 'Yujiu Yang', 'Wenhai Wang', 'Ruihang Chu'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'The Chinese University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2507.12841.jpg', 'data': {'categories': ['#multimodal', '#optimization', '#benchmark', '#games', '#dataset'], 'emoji': 'ğŸ›ï¸', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»ÑŒ Ğ½Ğ°Ğ´ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑÑĞ¼Ğ¸', 'desc': 'ĞŸÑ€Ğ¾ĞµĞºÑ‚ AnyCap Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ AnyCapModel (ACM), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸Ñ… Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… AnyCapDataset (ACD), Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ñ‚Ñ€Ğ¸ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ 28 Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº AnyCapEval Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞµĞ¹.'}, 'en': {'title': 'Enhancing Multimodal Captioning Control and Reliability', 'desc': "The AnyCap Project presents a comprehensive framework aimed at improving controllability and reliability in multimodal captioning tasks. It introduces the AnyCapModel (ACM), which enhances existing models' capabilities without the need for retraining, allowing for better alignment with user instructions and modality features. To support this, the AnyCapDataset (ACD) is created, featuring a diverse set of 300,000 high-quality entries across three modalities and 28 types of user instructions. Additionally, the AnyCapEval benchmark offers improved evaluation metrics that separate content accuracy from stylistic fidelity, demonstrating significant performance improvements in caption quality across various models."}, 'zh': {'title': 'æå‡å¤šæ¨¡æ€å­—å¹•ç”Ÿæˆçš„å¯æ§æ€§ä¸å¯é æ€§', 'desc': 'AnyCapé¡¹ç›®æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ã€æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€å­—å¹•ç”Ÿæˆçš„å¯æ§æ€§å’Œå¯é æ€§ã€‚è¯¥é¡¹ç›®çš„æ ¸å¿ƒæ˜¯AnyCapModelï¼ˆACMï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªè½»é‡çº§çš„æ’ä»¶æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¸é‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæé«˜ç°æœ‰æ¨¡å‹çš„å¯æ§æ€§ã€‚ä¸ºäº†åº”å¯¹å¯æ§å¤šæ¨¡æ€å­—å¹•ç”Ÿæˆä¸­çš„æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†AnyCapDatasetï¼ˆACDï¼‰ï¼Œæ¶µç›–ä¸‰ç§æ¨¡æ€ã€28ç§ç”¨æˆ·æŒ‡ä»¤ç±»å‹å’Œ30ä¸‡ä¸ªé«˜è´¨é‡æ•°æ®æ¡ç›®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†AnyCapEvalï¼Œä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæä¾›æ›´å¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé€šè¿‡è§£è€¦å†…å®¹å‡†ç¡®æ€§å’Œé£æ ¼ä¿çœŸåº¦æ¥è¯„ä¼°å¯æ§å­—å¹•ç”Ÿæˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13344', 'title': 'Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos\n  with Spatio-Temporal Diffusion Models', 'url': 'https://huggingface.co/papers/2507.13344', 'abstract': 'A sliding iterative denoising process is proposed to enhance spatio-temporal consistency in 4D diffusion models for high-fidelity view synthesis from sparse-view videos.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: https://diffuman4d.github.io/ .', 'score': 19, 'issue_id': 4889, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '02785244ea887bec', 'authors': ['Yudong Jin', 'Sida Peng', 'Xuan Wang', 'Tao Xie', 'Zhen Xu', 'Yifan Yang', 'Yujun Shen', 'Hujun Bao', 'Xiaowei Zhou'], 'affiliations': ['Ant Research', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2507.13344.jpg', 'data': {'categories': ['#video', '#cv', '#diffusion', '#dataset'], 'emoji': 'ğŸ¥', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ 4D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ Ğ²Ğ¸Ğ´Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞ³Ğ¾ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² 4D-Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ… Ğ´Ğ»Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğ° Ğ²Ğ¸Ğ´Ğ¾Ğ² Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ñ€ĞµĞ´ĞºĞ¸Ğ¼Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑˆÑƒĞ¼Ğ¾Ğ¿Ğ¾Ğ´Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ² ÑĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¹ ÑĞµÑ‚ĞºĞµ, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰ĞµĞ¹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ·Ñƒ ĞºĞ°Ğ¼ĞµÑ€Ñ‹ Ğ¸ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğµ Ñ€ĞµÑ†ĞµĞ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»Ğµ Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ 4D-ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ GPU. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ² ÑĞ¸Ğ½Ñ‚ĞµĞ·Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ€Ğ°ĞºÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Video Synthesis with Sliding Iterative Denoising', 'desc': 'This paper presents a new method to improve the quality of video synthesis from sparse-view inputs using 4D diffusion models. The authors introduce a sliding iterative denoising process that enhances spatio-temporal consistency, which is crucial for generating realistic videos. By organizing the data into a latent grid that captures image, camera, and human poses, the method allows for effective denoising across both spatial and temporal dimensions. Experiments show that this approach significantly outperforms previous techniques, producing high-fidelity and consistent videos.'}, 'zh': {'title': 'å¢å¼º4Dä¸€è‡´æ€§ï¼Œåˆæˆé«˜ä¿çœŸè§†å›¾', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ»‘åŠ¨è¿­ä»£å»å™ªè¿‡ç¨‹ï¼Œä»¥å¢å¼º4Dæ‰©æ•£æ¨¡å‹åœ¨ç¨€ç–è§†è§’è§†é¢‘ä¸­çš„æ—¶ç©ºä¸€è‡´æ€§ï¼Œä»è€Œå®ç°é«˜ä¿çœŸåº¦çš„è§†å›¾åˆæˆã€‚ä»¥å¾€çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨4Dæ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°è§†è§’çš„è§†é¢‘ï¼Œä½†ç”Ÿæˆçš„è§†é¢‘å¾€å¾€ç¼ºä¹æ—¶ç©ºä¸€è‡´æ€§ï¼Œå½±å“äº†åˆæˆè´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å®šä¹‰ä¸€ä¸ªæ½œåœ¨ç½‘æ ¼ï¼Œäº¤æ›¿æ²¿ç©ºé—´å’Œæ—¶é—´ç»´åº¦è¿›è¡Œå»å™ªï¼Œæœ€ç»ˆä»å»å™ªåçš„æ½œåœ¨è¡¨ç¤ºä¸­è§£ç å‡ºç›®æ ‡è§†è§’çš„è§†é¢‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨DNA-Renderingå’ŒActorsHQæ•°æ®é›†ä¸Šèƒ½å¤Ÿåˆæˆé«˜è´¨é‡ä¸”ä¸€è‡´çš„æ–°è§†è§’è§†é¢‘ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.12956', 'title': 'FantasyPortrait: Enhancing Multi-Character Portrait Animation with\n  Expression-Augmented Diffusion Transformers', 'url': 'https://huggingface.co/papers/2507.12956', 'abstract': "FantasyPortrait, a diffusion transformer framework, generates high-fidelity and emotion-rich facial animations for single and multi-character scenarios using implicit representations and a masked cross-attention mechanism.  \t\t\t\t\tAI-generated summary \t\t\t\t Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/.", 'score': 10, 'issue_id': 4887, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '4e0dcacd5b147ff7', 'authors': ['Qiang Wang', 'Mengchao Wang', 'Fan Jiang', 'Yaqi Fan', 'Yonggang Qi', 'Mu Xu'], 'affiliations': ['AMAP, Alibaba Group', 'Beijing University of Posts and Telecommunications'], 'pdf_title_img': 'assets/pdf/title_img/2507.12956.jpg', 'data': {'categories': ['#dataset', '#games', '#diffusion', '#benchmark', '#cv'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞĞ¶Ğ¸Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ²: Ğ¾Ñ‚ ÑÑ‚Ğ°Ñ‚Ğ¸ĞºĞ¸ Ğº ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞµ', 'desc': 'FantasyPortrait - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ±Ğ¾Ğ³Ğ°Ñ‚Ñ‹Ñ… Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Ğ»Ğ¸Ñ†. ĞĞ½ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½ĞµÑĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ°Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾, Ñ‚Ğ°Ğº Ğ¸ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ÑƒÑĞ¸Ğ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾Ğ¹ Ğ¾Ñ‚ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸ Ğ»Ğ¸Ñ†Ğ°. Ğ”Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ¶Ğ°Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¼Ğ°ÑĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºÑ€Ğ¾ÑÑ-Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼ÑƒÑ, Ğ½Ğ¾ ÑĞºĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'Transforming Static Images into Emotion-Rich Animations with FantasyPortrait', 'desc': "FantasyPortrait is a novel framework that utilizes diffusion transformers to create high-quality facial animations from static images, focusing on both single and multi-character scenarios. It overcomes limitations of previous methods that relied on explicit geometric features, which often led to artifacts and failed to capture subtle emotional expressions. The framework employs an expression-augmented learning strategy with implicit representations to enhance the rendering of fine-grained emotions. Additionally, a masked cross-attention mechanism allows for independent expression generation in multi-character settings, minimizing interference between different characters' features."}, 'zh': {'title': 'é«˜ä¿çœŸæƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»ç”Ÿæˆçš„åˆ›æ–°æ¡†æ¶', 'desc': 'FantasyPortraitæ˜¯ä¸€ç§åŸºäºæ‰©æ•£å˜æ¢å™¨çš„æ¡†æ¶ï¼Œèƒ½å¤Ÿä¸ºå•ä¸ªå’Œå¤šä¸ªè§’è‰²åœºæ™¯ç”Ÿæˆé«˜ä¿çœŸä¸”å¯Œæœ‰æƒ…æ„Ÿçš„é¢éƒ¨åŠ¨ç”»ã€‚è¯¥æ–¹æ³•é€šè¿‡éšå¼è¡¨ç¤ºå’Œæ©è”½äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•åœ¨é¢éƒ¨é‡ç°ä¸­çš„ä¼ªå½±é—®é¢˜ï¼Œå¹¶èƒ½å¤Ÿæ•æ‰ç»†è…»çš„æƒ…æ„Ÿå˜åŒ–ã€‚ä¸ºäº†æ”¯æŒå¤šè§’è‰²åŠ¨ç”»ï¼ŒFantasyPortraitè®¾è®¡äº†ä¸€ä¸ªæ©è”½äº¤å‰æ³¨æ„æœºåˆ¶ï¼Œç¡®ä¿ç‹¬ç«‹è€Œåè°ƒçš„è¡¨æƒ…ç”Ÿæˆï¼Œé¿å…ç‰¹å¾å¹²æ‰°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFantasyPortraitåœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå°¤å…¶åœ¨å¤æ‚çš„è·¨é‡ç°å’Œå¤šè§’è‰²åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.12508', 'title': 'MindJourney: Test-Time Scaling with World Models for Spatial Reasoning', 'url': 'https://huggingface.co/papers/2507.12508', 'abstract': 'MindJourney enhances vision-language models with 3D reasoning by coupling them with a video diffusion-based world model, achieving improved performance on spatial reasoning tasks without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.', 'score': 10, 'issue_id': 4883, 'pub_date': '2025-07-16', 'pub_date_card': {'ru': '16 Ğ¸ÑĞ»Ñ', 'en': 'July 16', 'zh': '7æœˆ16æ—¥'}, 'hash': '1a89f50f8edd267e', 'authors': ['Yuncong Yang', 'Jiageng Liu', 'Zheyuan Zhang', 'Siyuan Zhou', 'Reuben Tan', 'Jianwei Yang', 'Yilun Du', 'Chuang Gan'], 'affiliations': ['HKUST', 'Harvard', 'JHU', 'Microsoft Research', 'UMass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2507.12508.jpg', 'data': {'categories': ['#cv', '#3d', '#benchmark', '#diffusion', '#rl', '#reasoning', '#video'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'MindJourney: 3D-Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'MindJourney - ÑÑ‚Ğ¾ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğº Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ² 3D-Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ. ĞĞ½ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ VLM Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ğ¼Ğ¸Ñ€Ğ°, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ½Ğ° Ğ²Ğ¸Ğ´ĞµĞ¾Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¸. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ VLM ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ñ€Ğ°ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ĞºĞ°Ğ¼ĞµÑ€Ñ‹, Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¼Ğ¸Ñ€Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¸Ğ´Ñ‹ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑˆĞ°Ğ³Ğµ. MindJourney Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° 8% Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'MindJourney: Enhancing 3D Reasoning in Vision-Language Models', 'desc': 'MindJourney is a novel framework that enhances vision-language models (VLMs) by integrating them with a video diffusion-based world model, enabling better spatial reasoning in 3D environments. This approach allows VLMs to generate a camera trajectory and synthesize views dynamically, facilitating improved understanding of 3D dynamics without the need for fine-tuning. By leveraging multi-view evidence during interactive exploration, MindJourney achieves an average performance boost of over 8% on spatial reasoning tasks. This method demonstrates the effectiveness of coupling VLMs with world models for robust 3D reasoning, offering a straightforward solution for enhancing model capabilities at test time.'}, 'zh': {'title': 'MindJourneyï¼šæå‡è§†è§‰-è¯­è¨€æ¨¡å‹çš„3Dæ¨ç†èƒ½åŠ›', 'desc': 'MindJourney æ˜¯ä¸€ç§å¢å¼ºè§†è§‰-è¯­è¨€æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸åŸºäºè§†é¢‘æ‰©æ•£çš„ä¸–ç•Œæ¨¡å‹ç»“åˆï¼Œå®ç°äº†3Dæ¨ç†èƒ½åŠ›çš„æå‡ã€‚è¯¥æ–¹æ³•åœ¨ä¸è¿›è¡Œå¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†ç©ºé—´æ¨ç†ä»»åŠ¡çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨SATåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æå‡äº†8%ã€‚ä¼ ç»Ÿçš„è§†è§‰-è¯­è¨€æ¨¡å‹åœ¨å¤„ç†3DåŠ¨æ€æ—¶å¸¸å¸¸è¡¨ç°ä¸ä½³ï¼Œè€ŒMindJourneyé€šè¿‡è¿­ä»£ç”Ÿæˆç›¸æœºè½¨è¿¹å¹¶åˆæˆå¤šè§†å›¾è¯æ®ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£ç©ºé—´å…³ç³»ã€‚æ­¤æ–¹æ³•å±•ç¤ºäº†å°†ä¸–ç•Œæ¨¡å‹ä¸è§†è§‰-è¯­è¨€æ¨¡å‹ç»“åˆçš„æ½œåŠ›ï¼Œä¸º3Dæ¨ç†æä¾›äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13300', 'title': 'AbGen: Evaluating Large Language Models in Ablation Study Design and\n  Evaluation for Scientific Research', 'url': 'https://huggingface.co/papers/2507.13300', 'abstract': 'AbGen evaluates LLMs in designing ablation studies for scientific research, revealing performance gaps compared to human experts and highlighting the unreliability of current automated evaluation methods.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.', 'score': 9, 'issue_id': 4884, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '4836fc7ccb11ed5e', 'authors': ['Yilun Zhao', 'Weiyuan Chen', 'Zhijian Xu', 'Manasi Patwardhan', 'Yixin Liu', 'Chengye Wang', 'Lovekesh Vig', 'Arman Cohan'], 'affiliations': ['TCS Research', 'Yale NLP Lab'], 'pdf_title_img': 'assets/pdf/title_img/2507.13300.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#science', '#interpretability'], 'emoji': 'ğŸ§ª', 'ru': {'title': 'AbGen: Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ğ¾Ğ² Ğ² ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑÑ… Ğ˜Ğ˜ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹', 'desc': 'AbGen - ÑÑ‚Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ°Ğ±Ğ»Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚. ĞĞ½ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· 1500 Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ², Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸, Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ 807 ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ¿Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°. ĞÑ†ĞµĞ½ĞºĞ° Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ñ€Ñ‹Ğ² Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸ Ğ¸ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸-Ğ»ÑĞ´ÑŒĞ¼Ğ¸. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾ Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.'}, 'en': {'title': 'Bridging the Gap: Evaluating LLMs in Scientific Ablation Studies', 'desc': 'AbGen is a benchmark created to assess how well large language models (LLMs) can design ablation studies in scientific research. It includes 1,500 examples from 807 NLP papers, where LLMs must generate detailed study designs based on specific research contexts. The evaluation shows that LLMs like DeepSeek-R1-0528 and o4-mini fall short compared to human experts in key areas such as importance and soundness of the designs. Additionally, the study reveals that current automated evaluation methods are unreliable, prompting the development of AbGen-Eval to better assess LLM performance in this context.'}, 'zh': {'title': 'è¯„ä¼°LLMsåœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„æ¶ˆèç ”ç©¶è®¾è®¡èƒ½åŠ›', 'desc': 'AbGenæ˜¯ä¸€ä¸ªæ–°åŸºå‡†ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è®¾è®¡ç§‘å­¦ç ”ç©¶çš„æ¶ˆèç ”ç©¶ä¸­çš„èƒ½åŠ›ã€‚è¯¥åŸºå‡†åŒ…å«1500ä¸ªä¸“å®¶æ³¨é‡Šçš„ç¤ºä¾‹ï¼Œæ¥æºäº807ç¯‡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰è®ºæ–‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé¢†å…ˆçš„LLMsåœ¨æ¶ˆèç ”ç©¶è®¾è®¡çš„é‡è¦æ€§ã€çœŸå®æ€§å’Œåˆç†æ€§æ–¹é¢ï¼Œä¸äººç±»ä¸“å®¶å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å½“å‰çš„è‡ªåŠ¨è¯„ä¼°æ–¹æ³•åœ¨è¿™ä¸€ä»»åŠ¡ä¸­å¹¶ä¸å¯é ï¼Œå­˜åœ¨ä¸äººç±»è¯„ä¼°ç»“æœçš„æ˜¾è‘—å·®å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.12990', 'title': 'Teach Old SAEs New Domain Tricks with Boosting', 'url': 'https://huggingface.co/papers/2507.12990', 'abstract': 'A residual learning approach enhances Sparse Autoencoders to capture domain-specific features without retraining, improving interpretability and performance on specialized domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.', 'score': 5, 'issue_id': 4889, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '15daa9824ba8ff47', 'authors': ['Nikita Koriagin', 'Yaroslav Aksenov', 'Daniil Laptev', 'Gleb Gerasimov', 'Nikita Balagansky', 'Daniil Gavrilov'], 'affiliations': ['HSE University', 'Moscow Institute of Physics and Technology', 'T-Tech'], 'pdf_title_img': 'assets/pdf/title_img/2507.12990.jpg', 'data': {'categories': ['#training', '#architecture', '#data', '#interpretability', '#optimization'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ¾Ğ² (Sparse Autoencoders) Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM). ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (residual learning) Ğ´Ğ»Ñ Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ğ° ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¿Ñ€ĞµĞ´Ğ¼ĞµÑ‚Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ’Ñ‚Ğ¾Ñ€Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ñ€ĞµĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ğ½Ğ° Ñ‚ĞµĞºÑÑ‚Ğ°Ñ… Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸ Ğ´Ğ»Ñ LLM Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°Ñ….'}, 'en': {'title': 'Enhancing Sparse Autoencoders with Residual Learning for Domain-Specific Insights', 'desc': 'This paper presents a novel method that enhances Sparse Autoencoders (SAEs) using a residual learning approach to better capture domain-specific features. By training a secondary SAE to model the reconstruction error of a pretrained SAE, the method allows for the integration of new domain knowledge without the need for complete retraining. The results show significant improvements in performance metrics, such as cross-entropy and explained variance, across various specialized domains. This approach not only improves the interpretability of SAEs but also maintains their effectiveness on general tasks, paving the way for targeted mechanistic interpretability in large language models.'}, 'zh': {'title': 'æ®‹å·®å­¦ä¹ æå‡ç¨€ç–è‡ªç¼–ç å™¨çš„é¢†åŸŸç‰¹å¾æ•æ‰èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ®‹å·®å­¦ä¹ æ–¹æ³•ï¼Œå¢å¼ºäº†ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œä½¿å…¶èƒ½å¤Ÿæ•æ‰ç‰¹å®šé¢†åŸŸçš„ç‰¹å¾ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚é€šè¿‡è®­ç»ƒä¸€ä¸ªæ¬¡çº§SAEæ¥å»ºæ¨¡é¢„è®­ç»ƒSAEåœ¨ç‰¹å®šé¢†åŸŸæ–‡æœ¬ä¸Šçš„é‡æ„è¯¯å·®ï¼Œæœ‰æ•ˆåœ°æ•æ‰åˆ°ä¸»æ¨¡å‹é—æ¼çš„ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†ä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºç›¸åŠ ï¼Œæ˜¾è‘—æé«˜äº†å¤šä¸ªä¸“ä¸šé¢†åŸŸçš„LLMäº¤å‰ç†µå’Œè§£é‡Šæ–¹å·®æŒ‡æ ‡ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿé«˜æ•ˆåœ°å°†æ–°çš„é¢†åŸŸçŸ¥è¯†èå…¥ç°æœ‰çš„SAEï¼ŒåŒæ—¶ä¿æŒå…¶åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.12720', 'title': 'FLEXITOKENS: Flexible Tokenization for Evolving Language Models', 'url': 'https://huggingface.co/papers/2507.12720', 'abstract': 'FLEXITOKENS, a byte-level language model with a learnable tokenizer, reduces token over-fragmentation and improves performance across multilingual and morphologically diverse tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at https://github.com/owos/flexitokens', 'score': 4, 'issue_id': 4883, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '195b799c7dd66533', 'authors': ['Abraham Toluase Owodunni', 'Orevaoghene Ahia', 'Sachin Kumar'], 'affiliations': ['The Ohio State University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2507.12720.jpg', 'data': {'categories': ['#low_resource', '#training', '#architecture', '#dataset', '#optimization', '#multilingual'], 'emoji': 'ğŸ§©', 'ru': {'title': 'Ğ“Ğ¸Ğ±ĞºĞ°Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'FLEXITOKENS - ÑÑ‚Ğ¾ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ²Ğ°Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµĞ¼Ñ‹Ğ¼ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ñ€Ñ„Ğ¾Ğ»Ğ¾Ğ³Ğ¸ĞµĞ¹. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ´Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ±Ğ°Ğ¹Ñ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑ Ğ¸Ñ… Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ğ¸Ğ½Ñ‹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ±ĞµĞ· Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ², FLEXITOKENS Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½ÑƒÑ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ÑÑ‰ÑƒÑ Ğ±Ğ¾Ğ»ÑŒÑˆÑƒÑ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ FLEXITOKENS Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½ÑƒÑ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ´Ğ¾ 10% ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ÑĞ»Ğ¾Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğ¼Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'FLEXITOKENS: Adaptive Tokenization for Enhanced Language Model Performance', 'desc': 'FLEXITOKENS is a novel byte-level language model that introduces a learnable tokenizer to enhance adaptability in tokenization. Traditional subword tokenizers are rigid and often lead to over-fragmentation, especially in multilingual and morphologically diverse contexts. By allowing the model to learn token boundaries dynamically, FLEXITOKENS reduces inefficiencies and improves performance on various tasks. The results show up to a 10% increase in performance compared to existing tokenization methods, demonstrating its effectiveness in handling diverse data distributions.'}, 'zh': {'title': 'FLEXITOKENSï¼šçµæ´»çš„å­—èŠ‚çº§è¯­è¨€æ¨¡å‹', 'desc': 'FLEXITOKENSæ˜¯ä¸€ç§å­—èŠ‚çº§è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¯å­¦ä¹ çš„åˆ†è¯å™¨ï¼Œæ—¨åœ¨å‡å°‘åˆ†è¯è¿‡åº¦ç¢ç‰‡åŒ–çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„å­è¯åˆ†è¯å™¨åœ¨é€‚åº”æ–°æ•°æ®æ—¶å¾€å¾€ä¸å¤Ÿçµæ´»ï¼Œå¯¼è‡´åœ¨å¤„ç†ä¸åŒè¯­è¨€æˆ–è„šæœ¬æ—¶æ•ˆç‡ä½ä¸‹ã€‚é€šè¿‡å¼•å…¥å¯å­¦ä¹ çš„åˆ†è¯å™¨ï¼ŒFLEXITOKENSèƒ½å¤Ÿæ ¹æ®è¾“å…¥å­—èŠ‚åºåˆ—è‡ªé€‚åº”åœ°é¢„æµ‹åˆ†ç•Œï¼Œä»è€Œç”Ÿæˆå¯å˜é•¿åº¦çš„åˆ†æ®µã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFLEXITOKENSåœ¨å¤šè¯­è¨€åŸºå‡†æµ‹è¯•å’Œå½¢æ€å¤šæ ·æ€§ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½æå‡å¯è¾¾10%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.04984', 'title': 'TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame\n  Interpolation', 'url': 'https://huggingface.co/papers/2507.04984', 'abstract': 'Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) improves video frame interpolation by efficiently extracting temporal information, reducing parameters, and requiring less training data compared to existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Frame Interpolation (VFI) aims to predict the intermediate frame I_n (we use n to denote time in videos to avoid notation overload with the timestep t in diffusion models) based on two consecutive neighboring frames I_0 and I_1. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: https://zonglinl.github.io/tlbvfi_page.', 'score': 4, 'issue_id': 4884, 'pub_date': '2025-07-07', 'pub_date_card': {'ru': '7 Ğ¸ÑĞ»Ñ', 'en': 'July 7', 'zh': '7æœˆ7æ—¥'}, 'hash': 'b8c67ea4defb3288', 'authors': ['Zonglin Lyu', 'Chen Chen'], 'affiliations': ['Center for Research in Computer Vision, University of Central Florida'], 'pdf_title_img': 'assets/pdf/title_img/2507.04984.jpg', 'data': {'categories': ['#video', '#training', '#data', '#diffusion'], 'emoji': 'ğŸï¸', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾-Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ TLB-VFI, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ½Ğ° Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…. Ğ”Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 3D-Ğ²ĞµĞ¹Ğ²Ğ»ĞµÑ‚ Ğ³ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. TLB-VFI Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹, ÑƒĞ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ FID Ğ½Ğ° 20% Ğ½Ğ° ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ…, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ² 3 Ñ€Ğ°Ğ·Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². ĞœĞµÑ‚Ğ¾Ğ´ Ñ‚Ğ°ĞºĞ¶Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ² 9000 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸.'}, 'en': {'title': 'Efficient Video Frame Interpolation with Temporal Awareness', 'desc': 'Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) enhances the process of predicting intermediate video frames by effectively utilizing temporal information. It introduces a novel 3D-wavelet gating and a temporal-aware autoencoder to improve efficiency and reduce the number of parameters needed for training. Compared to existing methods, TLB-VFI achieves a significant performance boost while requiring much less training data and computational resources. This approach not only accelerates the inference process but also maintains high-quality output, making it a promising advancement in video frame interpolation.'}, 'zh': {'title': 'é«˜æ•ˆè§†é¢‘å¸§æ’å€¼çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºæ—¶é—´æ„ŸçŸ¥æ½œåœ¨å¸ƒæœ—æ¡¥æ‰©æ•£ï¼ˆTLB-VFIï¼‰çš„è§†é¢‘å¸§æ’å€¼æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡æå–ä¸°å¯Œçš„æ—¶é—´ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜äº†è§†é¢‘å¸§æ’å€¼çš„æ•ˆç‡ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒTLB-VFIåœ¨å‚æ•°æ•°é‡ä¸Šå‡å°‘äº†ä¸‰å€ï¼Œå¹¶ä¸”è®­ç»ƒæ•°æ®éœ€æ±‚é™ä½äº†9000å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ä¸Šç›¸è¾ƒäºæœ€æ–°çš„å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ŒFIDæŒ‡æ ‡æé«˜äº†20%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13255', 'title': 'Automating Steering for Safe Multimodal Large Language Models', 'url': 'https://huggingface.co/papers/2507.13255', 'abstract': "AutoSteer, a modular inference-time intervention technology, enhances the safety of Multimodal Large Language Models by reducing attack success rates across various threats without fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model's internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.", 'score': 2, 'issue_id': 4892, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': 'be41528bddf290cd', 'authors': ['Lyucheng Wu', 'Mengru Wang', 'Ziwen Xu', 'Tri Cao', 'Nay Oo', 'Bryan Hooi', 'Shumin Deng'], 'affiliations': ['National University of Singapore, NUS-NCS Joint Lab, Singapore', 'Zhejiang University', 'Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph'], 'pdf_title_img': 'assets/pdf/title_img/2507.13255.jpg', 'data': {'categories': ['#benchmark', '#inference', '#multimodal', '#security', '#interpretability'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'AutoSteer: Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜-ÑĞ¸ÑÑ‚ĞµĞ¼ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'AutoSteer - ÑÑ‚Ğ¾ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒĞ½Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM). ĞĞ½Ğ° ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒĞ³Ñ€Ğ¾Ğ· Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. AutoSteer Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ñ‚Ñ€Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°: Ğ¾Ñ†ĞµĞ½ĞºÑƒ Ğ¾ÑĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ·Ğ¾Ğ½Ğ´ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ¾Ñ‚ĞºĞ°Ğ·Ğ½ÑƒÑ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºÑƒ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ AutoSteer Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñƒ ÑƒÑĞ¿ĞµÑˆĞ½Ñ‹Ñ… Ğ°Ñ‚Ğ°Ğº Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ…, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ ĞºÑ€Ğ¾ÑÑ-Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ³Ñ€Ğ¾Ğ·, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Enhancing Safety in Multimodal AI with AutoSteer', 'desc': "AutoSteer is a new technology designed to make Multimodal Large Language Models (MLLMs) safer during their use. It works by implementing a modular system that does not require any changes to the original model, ensuring ease of use. The technology includes a Safety Awareness Score to identify safety issues, an adaptive safety prober to predict harmful outputs, and a Refusal Head to intervene when risks are detected. Tests show that AutoSteer effectively lowers the chances of successful attacks while keeping the model's performance intact."}, 'zh': {'title': 'AutoSteerï¼šæå‡å¤šæ¨¡æ€AIå®‰å…¨æ€§çš„åˆ›æ–°æŠ€æœ¯', 'desc': 'AutoSteeræ˜¯ä¸€ç§æ¨¡å—åŒ–çš„æ¨ç†æ—¶å¹²é¢„æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„å®‰å…¨æ€§ã€‚å®ƒé€šè¿‡å¼•å…¥å®‰å…¨æ„è¯†è¯„åˆ†ï¼ˆSASï¼‰å’Œè‡ªé€‚åº”å®‰å…¨æ¢æµ‹å™¨ï¼Œè‡ªåŠ¨è¯†åˆ«æ¨¡å‹å†…éƒ¨å±‚æ¬¡ä¸­ä¸å®‰å…¨ç›¸å…³çš„åŒºåˆ«ï¼Œä»è€Œé™ä½æ”»å‡»æˆåŠŸç‡ã€‚è¯¥æŠ€æœ¯ä¸éœ€è¦å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œèƒ½å¤Ÿåœ¨æ£€æµ‹åˆ°å®‰å…¨é£é™©æ—¶ï¼Œä½¿ç”¨è½»é‡çº§æ‹’ç»å¤´è¿›è¡Œå¹²é¢„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAutoSteeråœ¨å¤šç§å®‰å…¨å…³é”®åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—é™ä½äº†æ–‡æœ¬ã€è§†è§‰å’Œè·¨æ¨¡æ€å¨èƒçš„æ”»å‡»æˆåŠŸç‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.13264', 'title': 'Voxtral', 'url': 'https://huggingface.co/papers/2507.13264', 'abstract': 'Voxtral Mini and Voxtral Small are multimodal audio chat models that excel in understanding spoken audio and text, with a 32K context window for handling long audio files and conversations.  \t\t\t\t\tAI-generated summary \t\t\t\t We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.', 'score': 1, 'issue_id': 4892, 'pub_date': '2025-07-17', 'pub_date_card': {'ru': '17 Ğ¸ÑĞ»Ñ', 'en': 'July 17', 'zh': '7æœˆ17æ—¥'}, 'hash': '869f860fcaa0550b', 'authors': ['Alexander H. Liu', 'Andy Ehrenberg', 'Andy Lo', 'ClÃ©ment Denoix', 'Corentin Barreau', 'Guillaume Lample', 'Jean-Malo Delignon', 'Khyathi Raghavi Chandu', 'Patrick von Platen', 'Pavankumar Reddy Muddireddy', 'Sanchit Gandhi', 'Soham Ghosh', 'Srijan Mishra', 'Thomas Foubert', 'Abhinav Rastogi', 'Adam Yang', 'Albert Q. Jiang', 'Alexandre Sablayrolles', 'AmÃ©lie HÃ©liou', 'AmÃ©lie Martin', 'Anmol Agarwal', 'Antoine Roux', 'Arthur Darcet', 'Arthur Mensch', 'Baptiste Bout', 'Baptiste RoziÃ¨re', 'Baudouin De Monicault', 'Chris Bamford', 'Christian Wallenwein', 'Christophe Renaudin', 'ClÃ©mence Lanfranchi', 'Darius Dabert', 'Devendra Singh Chaplot', 'Devon Mizelle', 'Diego de las Casas', 'Elliot Chane-Sane', 'Emilien Fugier', 'Emma Bou Hanna', 'Gabrielle Berrada', 'Gauthier Delerce', 'Gauthier Guinet', 'Georgii Novikov', 'Guillaume Martin', 'Himanshu Jaju', 'Jan Ludziejewski', 'Jason Rute', 'Jean-Hadrien Chabran', 'Jessica Chudnovsky', 'Joachim Studnia', 'Joep Barmentlo', 'Jonas Amar', 'Josselin Somerville Roberts', 'Julien Denize', 'Karan Saxena', 'Karmesh Yadav', 'Kartik Khandelwal', 'Kush Jain', 'LÃ©lio Renard Lavaud', 'LÃ©onard Blier', 'Lingxiao Zhao', 'Louis Martin', 'Lucile Saulnier', 'Luyu Gao', 'Marie Pellat', 'Mathilde Guillaumin', 'Mathis Felardos', 'Matthieu Dinot', 'Maxime Darrin', 'Maximilian Augustin', 'MickaÃ«l Seznec', 'Neha Gupta', 'Nikhil Raghuraman', 'Olivier Duchenne', 'Patricia Wang', 'Patryk Saffer', 'Paul Jacob', 'Paul Wambergue', 'Paula Kurylowicz', 'PhilomÃ¨ne Chagniot', 'Pierre Stock', 'Pravesh Agrawal', 'RÃ©mi Delacourt', 'Romain Sauvestre', 'Roman Soletskyi', 'Sagar Vaze', 'Sandeep Subramanian', 'Saurabh Garg', 'Shashwat Dalal', 'Siddharth Gandhi', 'Sumukh Aithal', 'Szymon Antoniak', 'Teven Le Scao', 'Thibault Schueller', 'Thibaut Lavril', 'Thomas Robert', 'Thomas Wang', 'TimothÃ©e Lacroix', 'Tom Bewley', 'Valeriia Nemychnikova', 'Victor Paltz', 'Virgile Richard', 'Wen-Ding Li', 'William Marshall', 'Xuanyu Zhang', 'Yihan Wan', 'Yunhao Tang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2507.13264.jpg', 'data': {'categories': ['#benchmark', '#audio', '#small_models', '#multimodal', '#long_context', '#open_source'], 'emoji': 'ğŸ™ï¸', 'ru': {'title': 'ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Voxtral: Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ¾Ğ¼ ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Voxtral Mini Ğ¸ Voxtral Small - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‡Ğ°Ñ‚Ğ°, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ñ€ĞµÑ‡ÑŒ, Ñ‚Ğ°Ğº Ğ¸ Ñ‚ĞµĞºÑÑ‚. Ğ­Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ÑÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ…, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸. Voxtral Small Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ñ€ÑĞ´ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑÑ‚Ğ°Ğ²Ğ°ÑÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°. ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ² 32 Ñ‚Ñ‹ÑÑÑ‡Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸.'}, 'en': {'title': 'Revolutionizing Audio Understanding with Voxtral Models', 'desc': 'Voxtral Mini and Voxtral Small are advanced multimodal audio chat models designed to understand both spoken audio and text. They utilize a 32K context window, allowing them to process long audio files and extended conversations effectively. The models achieve top performance on various audio benchmarks while maintaining strong text comprehension capabilities. Additionally, Voxtral Small is efficient enough to run locally and surpasses several proprietary models in performance.'}, 'zh': {'title': 'Voxtralï¼šéŸ³é¢‘ä¸æ–‡æœ¬çš„å®Œç¾ç»“åˆ', 'desc': 'Voxtral Miniå’ŒVoxtral Smallæ˜¯ä¸¤ç§å¤šæ¨¡æ€éŸ³é¢‘èŠå¤©æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£è¯­éŸ³å’Œæ–‡æœ¬ã€‚å®ƒä»¬å…·æœ‰32Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¯ä»¥å¤„ç†é•¿è¾¾40åˆ†é’Ÿçš„éŸ³é¢‘æ–‡ä»¶å’Œå¤šè½®å¯¹è¯ã€‚Voxtralåœ¨å¤šä¸ªéŸ³é¢‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚Voxtral Smallåœ¨æœ¬åœ°è¿è¡Œæ—¶è¶…è¶Šäº†è®¸å¤šé—­æºæ¨¡å‹ï¼Œä¸”æˆ‘ä»¬è¿˜æä¾›äº†ä¸‰ä¸ªåŸºå‡†æµ‹è¯•æ¥è¯„ä¼°è¯­éŸ³ç†è§£æ¨¡å‹çš„çŸ¥è¯†å’Œå¸¸è¯†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2507.11589', 'title': 'Einstein Fields: A Neural Perspective To Computational General\n  Relativity', 'url': 'https://huggingface.co/papers/2507.11589', 'abstract': 'Einstein Fields, a neural tensor field representation, compresses four-dimensional numerical relativity simulations into neural network weights, enabling automatic differentiation and natural emergence of dynamics.  \t\t\t\t\tAI-generated summary \t\t\t\t We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are Neural Tensor Fields with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields', 'score': 1, 'issue_id': 4891, 'pub_date': '2025-07-15', 'pub_date_card': {'ru': '15 Ğ¸ÑĞ»Ñ', 'en': 'July 15', 'zh': '7æœˆ15æ—¥'}, 'hash': '3d0f8c09fa171915', 'authors': ['Sandeep Suresh Cranganore', 'Andrei Bodnar', 'Arturs Berzins', 'Johannes Brandstetter'], 'affiliations': ['Emmi AI GmbH, Linz, Austria', 'LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria', 'University of Manchester, United Kingdom'], 'pdf_title_img': 'assets/pdf/title_img/2507.11589.jpg', 'data': {'categories': ['#dataset', '#science', '#open_source', '#architecture'], 'emoji': 'ğŸŒŒ', 'ru': {'title': 'Einstein Fields: ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Einstein Fields - Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ĞµÑ…Ğ¼ĞµÑ€Ğ½Ñ‹Ñ… ÑĞ¸Ğ¼ÑƒĞ»ÑÑ†Ğ¸Ğ¹ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸. Ğ­Ñ‚Ğ° Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹, Ğ² Einstein Fields Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ²Ğ¾Ğ·Ğ½Ğ¸ĞºĞ°ĞµÑ‚ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ¼. ĞœĞµÑ‚Ğ¾Ğ´ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¾Ğ±ĞµÑ‰Ğ°ÑÑ‰Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµĞ½Ñ†Ğ¸Ğ°Ğ» Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞ¾Ñ€Ğ¸Ğ¸ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ 4D Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ°-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'}, 'en': {'title': 'Revolutionizing Numerical Relativity with Einstein Fields', 'desc': 'Einstein Fields is a novel neural tensor field representation that compresses complex four-dimensional numerical relativity simulations into compact neural network weights. This approach allows for automatic differentiation, enabling the extraction of physical quantities directly from the model. Unlike traditional neural fields, Einstein Fields naturally incorporate dynamics when encoding the spacetime geometry of general relativity. The framework demonstrates advantages such as efficient storage, high accuracy in derivatives, and versatility across different mesh types, making it a significant advancement in the field of numerical relativity.'}, 'zh': {'title': 'çˆ±å› æ–¯å¦åœºï¼šå‹ç¼©å››ç»´æ•°å€¼ç›¸å¯¹è®ºçš„ç¥ç»ç½‘ç»œæ–°æ–¹æ³•', 'desc': 'çˆ±å› æ–¯å¦åœºæ˜¯ä¸€ç§ç¥ç»å¼ é‡åœºè¡¨ç¤ºï¼Œæ—¨åœ¨å°†è®¡ç®—å¯†é›†å‹çš„å››ç»´æ•°å€¼ç›¸å¯¹è®ºæ¨¡æ‹Ÿå‹ç¼©ä¸ºç´§å‡‘çš„éšå¼ç¥ç»ç½‘ç»œæƒé‡ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å»ºæ¨¡åº¦é‡ï¼Œèƒ½å¤Ÿåˆ©ç”¨è‡ªåŠ¨å¾®åˆ†æ¨å¯¼ç‰©ç†é‡ã€‚ä¸ä¼ ç»Ÿçš„ç¥ç»åœºä¸åŒï¼Œçˆ±å› æ–¯å¦åœºåœ¨ç¼–ç å¹¿ä¹‰ç›¸å¯¹è®ºçš„æ—¶ç©ºå‡ ä½•æ—¶ï¼Œè‡ªç„¶åœ°äº§ç”ŸåŠ¨æ€æ•ˆæœã€‚è¯¥æ–¹æ³•å±•ç¤ºäº†åœ¨å››ç»´æ—¶ç©ºå»ºæ¨¡ã€å­˜å‚¨æ•ˆç‡å’Œæ˜“ç”¨æ€§ç­‰æ–¹é¢çš„æ˜¾è‘—æ½œåŠ›ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå¼€æºçš„JAXåº“ï¼Œä¿ƒè¿›äº†æ•°å€¼ç›¸å¯¹è®ºçš„å¯æ‰©å±•æ€§å’Œè¡¨ç°åŠ›ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents', '#agi', '#alignment', '#architecture (6)', '#audio (1)', '#benchmark (6)', '#cv (5)', '#data (4)', '#dataset (7)', '#diffusion (4)', '#ethics', '#games (3)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (3)', '#leakage', '#long_context (2)', '#low_resource (1)', '#machine_translation', '#math', '#multilingual (1)', '#multimodal (4)', '#open_source (3)', '#optimization (5)', '#plp', '#rag (1)', '#reasoning (2)', '#rl (2)', '#rlhf', '#robotics', '#science (2)', '#security (1)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (1)', '#training (5)', '#transfer_learning', '#video (3)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-07-18 12:23',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-07-18 12:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-07-18 12:23')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    