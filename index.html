
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 13 papers. February 27.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ</span> | <span id="title-articles-count">13 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-02-26.html">â¬…ï¸ <span id="prev-date">26.02</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-02-28.html">â¡ï¸ <span id="next-date">28.02</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-02.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '27 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 27', 'zh': '2æœˆ27æ—¥'};
        let feedDateNext = {'ru': '28.02', 'en': '02/28', 'zh': '2æœˆ28æ—¥'};
        let feedDatePrev = {'ru': '26.02', 'en': '02/26', 'zh': '2æœˆ26æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2502.18934', 'title': 'Kanana: Compute-efficient Bilingual Language Models', 'url': 'https://huggingface.co/papers/2502.18934', 'abstract': 'We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation. Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users. Lastly, the report elaborates on plausible approaches used for language model adaptation to specific scenarios, such as embedding, retrieval augmented generation, and function calling. The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.', 'score': 29, 'issue_id': 2433, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'ee34363239bc939d', 'authors': ['Kanana LLM Team', 'Yunju Bak', 'Hojin Lee', 'Minho Ryu', 'Jiyeon Ham', 'Seungjae Jung', 'Daniel Wontae Nam', 'Taegyeong Eo', 'Donghun Lee', 'Doohae Jung', 'Boseop Kim', 'Nayeon Kim', 'Jaesun Park', 'Hyunho Kim', 'Hyunwoong Ko', 'Changmin Lee', 'Kyoung-Woon On', 'Seulye Baeg', 'Junrae Cho', 'Sunghee Jung', 'Jieun Kang', 'EungGyun Kim', 'Eunhwa Kim', 'Byeongil Ko', 'Daniel Lee', 'Minchul Lee', 'Miok Lee', 'Shinbok Lee', 'Gaeun Seo'], 'affiliations': ['Kakao Corp'], 'pdf_title_img': 'assets/pdf/title_img/2502.18934.jpg', 'data': {'categories': ['#rag', '#open_source', '#machine_translation', '#low_resource', '#dataset', '#multilingual', '#data', '#small_models', '#training'], 'emoji': 'ğŸ‡°ğŸ‡·', 'ru': {'title': 'Kanana: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ³Ğ¾ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ³Ğ¾', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞµÑ€Ğ¸Ñ Ğ´Ğ²ÑƒÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Kanana, Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ñ… Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ² ĞºĞ¾Ñ€ĞµĞ¹ÑĞºĞ¾Ğ¼ Ğ¸ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞ°Ñ…. ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Kanana Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ¿Ğ¸ÑĞ°Ğ½Ñ‹ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¿Ğ¾ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ¸ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ. Ğ¢Ğ°ĞºĞ¶Ğµ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµĞ½Ñ‹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ñ‹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑĞ¼.'}, 'en': {'title': 'Kanana: Efficient Bilingual Language Models for Korean and English', 'desc': 'Kanana is a series of bilingual language models that excel in Korean and perform competitively in English, showcasing their versatility. The models are designed to be computationally efficient, using techniques like high-quality data filtering and staged pre-training to reduce costs while maintaining performance. Post-training methods such as supervised fine-tuning and preference optimization are employed to improve user interaction capabilities. The Kanana models range from 2.1B to 32.5B parameters, with the smaller models made publicly available to encourage research in Korean language processing.'}, 'zh': {'title': 'Kananaï¼šé«˜æ•ˆåŒè¯­è¯­è¨€æ¨¡å‹çš„åˆ›æ–°ä¹‹è·¯', 'desc': 'Kananaæ˜¯ä¸€ç³»åˆ—åŒè¯­è¯­è¨€æ¨¡å‹ï¼Œä¸»è¦åœ¨éŸ©è¯­ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è‹±è¯­ä¸Šä¹Ÿå…·æœ‰ç«äº‰åŠ›ã€‚ä¸åŒç±»æœ€å…ˆè¿›æ¨¡å‹ç›¸æ¯”ï¼ŒKananaçš„è®¡ç®—æˆæœ¬æ˜¾è‘—é™ä½ã€‚è¯¥æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬é«˜è´¨é‡æ•°æ®è¿‡æ»¤ã€åˆ†é˜¶æ®µé¢„è®­ç»ƒã€æ·±åº¦æ‰©å±•ä»¥åŠå‰ªæå’Œè’¸é¦ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜é˜è¿°äº†åœ¨åè®­ç»ƒé˜¶æ®µä½¿ç”¨çš„æ–¹æ³•ï¼Œå¦‚ç›‘ç£å¾®è°ƒå’Œåå¥½ä¼˜åŒ–ï¼Œä»¥å¢å¼ºæ¨¡å‹ä¸ç”¨æˆ·çš„æ— ç¼äº’åŠ¨èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19400', 'title': 'TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding', 'url': 'https://huggingface.co/papers/2502.19400', 'abstract': 'Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.', 'score': 14, 'issue_id': 2432, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '87db4026705c319e', 'authors': ['Max Ku', 'Thomas Chong', 'Jonathan Leung', 'Krish Shah', 'Alvin Yu', 'Wenhu Chen'], 'affiliations': ['University of Waterloo', 'Vector Institute', 'Votee AI'], 'pdf_title_img': 'assets/pdf/title_img/2502.19400.jpg', 'data': {'categories': ['#reasoning', '#video', '#agents', '#interpretability', '#multimodal', '#benchmark'], 'emoji': 'ğŸ§®', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ TheoremExplainAgent - Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾-Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¹ Manim. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº TheoremExplainBench Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 240 Ñ‚ĞµĞ¾Ñ€ĞµĞ¼ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… STEM-Ğ´Ğ¸ÑÑ†Ğ¸Ğ¿Ğ»Ğ¸Ğ½. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ°Ğ³ĞµĞ½Ñ‚ o3-mini Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ 93.8% ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸. ĞĞ´Ğ½Ğ°ĞºĞ¾ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸Ğ¼ĞµÑÑ‚ Ğ½ĞµĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½Ğ¾Ğ²ĞºĞ¾Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².'}, 'en': {'title': 'Enhancing Theorem Understanding with Visual Explanations', 'desc': 'This paper presents TheoremExplainAgent, a novel approach for creating long-form video explanations of mathematical theorems using animations. It addresses the limitations of large language models in generating coherent visual content for educational purposes. The authors introduce TheoremExplainBench, a benchmark for evaluating these multimodal explanations across various STEM fields, utilizing 240 theorems and five automated metrics. The findings indicate that while the agentic planning significantly enhances video quality, there are still challenges in visual layout and reasoning that need to be addressed.'}, 'zh': {'title': 'å¤šæ¨¡æ€è§£é‡Šï¼šè¶…è¶Šæ–‡æœ¬çš„ç†è§£', 'desc': 'ç†è§£ç‰¹å®šé¢†åŸŸçš„å®šç†ä¸ä»…éœ€è¦æ–‡æœ¬æ¨ç†ï¼Œè¿˜éœ€è¦é€šè¿‡ç»“æ„åŒ–çš„è§†è§‰è§£é‡Šè¿›è¡Œæœ‰æ•ˆæ²Ÿé€šï¼Œä»¥ä¾¿æ›´æ·±å…¥åœ°ç†è§£ã€‚å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŸºäºæ–‡æœ¬çš„å®šç†æ¨ç†ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬ç”Ÿæˆè¿è´¯ä¸”å…·æœ‰æ•™å­¦æ„ä¹‰çš„è§†è§‰è§£é‡Šçš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†TheoremExplainAgentï¼Œè¿™æ˜¯ä¸€ç§ç”Ÿæˆé•¿æ ¼å¼å®šç†è§£é‡Šè§†é¢‘çš„ä»£ç†æ–¹æ³•ï¼Œä½¿ç”¨ManimåŠ¨ç”»è¿›è¡Œå±•ç¤ºã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œä»£ç†è§„åˆ’å¯¹äºç”Ÿæˆè¯¦ç»†çš„é•¿æ ¼å¼è§†é¢‘è‡³å…³é‡è¦ï¼Œo3-miniä»£ç†çš„æˆåŠŸç‡è¾¾åˆ°93.8%ï¼Œä½†ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰å…ƒç´ å¸ƒå±€ä¸Šå­˜åœ¨ä¸€äº›å°é—®é¢˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19361', 'title': 'Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?', 'url': 'https://huggingface.co/papers/2502.19361', 'abstract': 'Recently, o1-like models have drawn significant attention, where these models produce the long Chain-of-Thought (CoT) reasoning steps to improve the reasoning abilities of existing Large Language Models (LLMs). In this paper, to understand the qualities of these long CoTs and measure the critique abilities of existing LLMs on these long CoTs, we introduce the DeltaBench, including the generated long CoTs from different o1-like models (e.g., QwQ, DeepSeek-R1) for different reasoning tasks (e.g., Math, Code, General Reasoning), to measure the ability to detect errors in long CoT reasoning. Based on DeltaBench, we first perform fine-grained analysis of the generated long CoTs to discover the effectiveness and efficiency of different o1-like models. Then, we conduct extensive evaluations of existing process reward models (PRMs) and critic models to detect the errors of each annotated process, which aims to investigate the boundaries and limitations of existing PRMs and critic models. Finally, we hope that DeltaBench could guide developers to better understand the long CoT reasoning abilities of their models.', 'score': 8, 'issue_id': 2433, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '5512631a44ece1da', 'authors': ['Yancheng He', 'Shilong Li', 'Jiaheng Liu', 'Weixun Wang', 'Xingyuan Bu', 'Ge Zhang', 'Zhongyuan Peng', 'Zhaoxiang Zhang', 'Wenbo Su', 'Bo Zheng'], 'affiliations': ['Alibaba Group', 'CASIA', 'M-A-P'], 'pdf_title_img': 'assets/pdf/title_img/2502.19361.jpg', 'data': {'categories': ['#long_context', '#dataset', '#benchmark', '#training', '#reasoning'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'DeltaBench: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞµĞº Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ DeltaBench - Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ°Ñ… Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ (CoT) Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° o1. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²Ğ¾Ğ´ÑÑ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… CoT Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ñ†ĞµĞ½Ğ¸Ñ‚ÑŒ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ o1. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ³Ñ€Ğ°Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° (PRM) Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. DeltaBench Ğ¿Ñ€Ğ¸Ğ·Ğ²Ğ°Ğ½ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‡ÑŒ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°Ğ¼ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¿Ğ¾Ğ½ÑÑ‚ÑŒ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸ÑĞ¼ CoT.'}, 'en': {'title': 'DeltaBench: Enhancing Long CoT Reasoning in LLMs', 'desc': 'This paper introduces DeltaBench, a benchmark designed to evaluate the long Chain-of-Thought (CoT) reasoning capabilities of various o1-like models. It focuses on analyzing the quality of these long CoTs generated for different reasoning tasks, such as Math and Code. The study also assesses the performance of existing process reward models (PRMs) and critic models in identifying errors within these long CoTs. Ultimately, DeltaBench aims to provide insights that help developers enhance the reasoning abilities of their Large Language Models (LLMs).'}, 'zh': {'title': 'DeltaBenchï¼šæå‡é•¿é“¾æ¨ç†èƒ½åŠ›çš„è¯„ä¼°å·¥å…·', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†å·¥å…·DeltaBenchï¼Œç”¨äºè¯„ä¼°o1ç±»æ¨¡å‹ç”Ÿæˆçš„é•¿é“¾æ¨ç†ï¼ˆCoTï¼‰çš„è´¨é‡ã€‚æˆ‘ä»¬é€šè¿‡DeltaBenchåˆ†æä¸åŒo1ç±»æ¨¡å‹åœ¨æ•°å­¦ã€ä»£ç å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œæ—¨åœ¨æµ‹é‡ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿CoTæ¨ç†ä¸­çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚ç ”ç©¶è¿˜å¯¹ç°æœ‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰å’Œè¯„è®ºæ¨¡å‹è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥æ¢è®¨å®ƒä»¬çš„å±€é™æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¸Œæœ›DeltaBenchèƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£æ¨¡å‹åœ¨é•¿CoTæ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19328', 'title': 'Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems', 'url': 'https://huggingface.co/papers/2502.19328', 'abstract': 'Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).', 'score': 8, 'issue_id': 2432, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '57e2794756efc952', 'authors': ['Hao Peng', 'Yunjia Qi', 'Xiaozhi Wang', 'Zijun Yao', 'Bin Xu', 'Lei Hou', 'Juanzi Li'], 'affiliations': ['Department of Computer Science and Technology, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.19328.jpg', 'data': {'categories': ['#alignment', '#training', '#open_source', '#rlhf', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞĞ³ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑˆĞ°Ğ³ Ğ² ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ´Ğ»Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ°Ğ³ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ¾Ñ‡ĞµÑ‚Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸ÑÑ… Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼Ñ‹Ğ¼Ğ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ°Ğ¼Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ RewardAgent, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ñ€ĞµĞ²Ğ·Ğ¾ÑˆĞ»Ğ° Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ RewardAgent Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ĞµĞ»Ğ¾ Ğº ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğ° Ñ€ÑĞ´Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°.'}, 'en': {'title': 'Enhancing LLM Training with Agentic Reward Modeling', 'desc': 'This paper introduces agentic reward modeling, which enhances the training of large language models (LLMs) by integrating human preferences with verifiable correctness signals. The proposed RewardAgent combines rewards based on factual accuracy and adherence to instructions, leading to more reliable training outcomes. Through extensive experiments, RewardAgent demonstrates superior performance over traditional reward models in various natural language processing tasks. The authors also provide their code to support further research in this area.'}, 'zh': {'title': 'ä»£ç†å¥–åŠ±å»ºæ¨¡ï¼šæå‡è¯­è¨€æ¨¡å‹çš„å¯é æ€§', 'desc': 'å¥–åŠ±æ¨¡å‹ï¼ˆRMsï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒå’Œæ¨ç†ä¸­è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¥–åŠ±æ¨¡å‹ä¸»è¦å…³æ³¨äººç±»åå¥½ï¼Œå¿½è§†äº†å¯éªŒè¯çš„æ­£ç¡®æ€§ä¿¡å·ï¼Œè€Œè¿™äº›ä¿¡å·åœ¨è®­ç»ƒLLMsä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ä»£ç†å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œå°†å¥–åŠ±æ¨¡å‹ä¸æ¥è‡ªä¸åŒæ–¹é¢çš„å¯éªŒè¯æ­£ç¡®æ€§ä¿¡å·ç›¸ç»“åˆï¼Œä»¥æä¾›æ›´å¯é çš„å¥–åŠ±ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåä¸ºRewardAgentçš„å¥–åŠ±ä»£ç†ï¼Œå®ƒç»“åˆäº†äººç±»åå¥½å¥–åŠ±å’Œä¸¤ä¸ªå¯éªŒè¯ä¿¡å·ï¼šäº‹å®æ€§å’ŒæŒ‡ä»¤éµå¾ªï¼Œä»è€Œåœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†åŸºå‡†ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18864', 'title': 'Towards an AI co-scientist', 'url': 'https://huggingface.co/papers/2502.18864', 'abstract': "Scientific discovery relies on scientists generating novel hypotheses that undergo rigorous experimental validation. To augment this process, we introduce an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI co-scientist is intended to help uncover new, original knowledge and to formulate demonstrably novel research hypotheses and proposals, building upon prior evidence and aligned to scientist-provided research objectives and guidance. The system's design incorporates a generate, debate, and evolve approach to hypothesis generation, inspired by the scientific method and accelerated by scaling test-time compute. Key contributions include: (1) a multi-agent architecture with an asynchronous task execution framework for flexible compute scaling; (2) a tournament evolution process for self-improving hypotheses generation. Automated evaluations show continued benefits of test-time compute, improving hypothesis quality. While general purpose, we focus development and validation in three biomedical areas: drug repurposing, novel target discovery, and explaining mechanisms of bacterial evolution and anti-microbial resistance. For drug repurposing, the system proposes candidates with promising validation findings, including candidates for acute myeloid leukemia that show tumor inhibition in vitro at clinically applicable concentrations. For novel target discovery, the AI co-scientist proposed new epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and liver cell regeneration in human hepatic organoids. Finally, the AI co-scientist recapitulated unpublished experimental results via a parallel in silico discovery of a novel gene transfer mechanism in bacterial evolution. These results, detailed in separate, co-timed reports, demonstrate the potential to augment biomedical and scientific discovery and usher an era of AI empowered scientists.", 'score': 6, 'issue_id': 2432, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'f5b7ae00d9351ca7', 'authors': ['Juraj Gottweis', 'Wei-Hung Weng', 'Alexander Daryin', 'Tao Tu', 'Anil Palepu', 'Petar Sirkovic', 'Artiom Myaskovsky', 'Felix Weissenberger', 'Keran Rong', 'Ryutaro Tanno', 'Khaled Saab', 'Dan Popovici', 'Jacob Blum', 'Fan Zhang', 'Katherine Chou', 'Avinatan Hassidim', 'Burak Gokturk', 'Amin Vahdat', 'Pushmeet Kohli', 'Yossi Matias', 'Andrew Carroll', 'Kavita Kulkarni', 'Nenad Tomasev', 'Yuan Guan', 'Vikram Dhillon', 'Eeshit Dhaval Vaishnav', 'Byron Lee', 'Tiago R D Costa', 'JosÃ© R PenadÃ©s', 'Gary Peltz', 'Yunhan Xu', 'Annalisa Pawlosky', 'Alan Karthikesalingam', 'Vivek Natarajan'], 'affiliations': ['Fleming Initiative and Imperial College London', 'Google Cloud AI Research', 'Google DeepMind', 'Google Research', 'Houston Methodist', 'Sequome', 'Stanford University School of Medicine'], 'pdf_title_img': 'assets/pdf/title_img/2502.18864.jpg', 'data': {'categories': ['#agents', '#science', '#healthcare', '#architecture'], 'emoji': 'ğŸ§¬', 'ru': {'title': 'Ğ˜Ğ˜-ÑĞ¾ÑƒÑ‡ĞµĞ½Ñ‹Ğ¹: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ğ¸', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ˜Ğ˜-ÑĞ¾ÑƒÑ‡ĞµĞ½Ğ¾Ğ³Ğ¾, Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ½Ğ° Ğ±Ğ°Ğ·Ğµ Gemini 2.0, Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ¸ ÑƒÑ‡ĞµĞ½Ñ‹Ğ¼ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ğ¸ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ, Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ' Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ·, Ğ²Ğ´Ğ¾Ñ…Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğ¼ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ°Ğ³ĞµĞ½Ñ‚Ğ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ñ Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞ· Ñ‡ĞµÑ€ĞµĞ· Ñ‚ÑƒÑ€Ğ½Ğ¸Ñ€Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ±Ñ‹Ğ»Ğ° Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ² Ñ‚Ñ€ĞµÑ… Ğ±Ğ¸Ğ¾Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…: Ğ¿ĞµÑ€ĞµĞ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ĞµĞºĞ°Ñ€ÑÑ‚Ğ², Ğ¿Ğ¾Ğ¸ÑĞº Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¸ÑˆĞµĞ½ĞµĞ¹ Ğ¸ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ±Ğ°ĞºÑ‚ĞµÑ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¸."}, 'en': {'title': 'Empowering Science with AI: Unleashing Novel Hypotheses', 'desc': "This paper presents an AI co-scientist, a multi-agent system designed to assist in scientific discovery by generating and validating novel research hypotheses. Built on the Gemini 2.0 framework, it employs a generate, debate, and evolve methodology that mimics the scientific method while leveraging enhanced computational resources. The system's architecture allows for flexible scaling of tasks and includes a tournament evolution process to improve hypothesis generation over time. Focused on biomedical applications, the AI co-scientist has successfully proposed drug candidates and novel targets, demonstrating its potential to significantly enhance research outcomes."}, 'zh': {'title': 'AIåŠ©åŠ›ç§‘å­¦å‘ç°çš„æ–°çºªå…ƒ', 'desc': 'è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAIå…±åŒç§‘å­¦å®¶çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ—¨åœ¨å¸®åŠ©ç§‘å­¦å®¶ç”Ÿæˆæ–°çš„ç ”ç©¶å‡è®¾å¹¶è¿›è¡ŒéªŒè¯ã€‚è¯¥ç³»ç»ŸåŸºäºGemini 2.0ï¼Œé‡‡ç”¨ç”Ÿæˆã€è¾©è®ºå’Œæ¼”å˜çš„æ–¹æ³•æ¥ç”Ÿæˆå‡è®¾ï¼Œçµæ„Ÿæ¥æºäºç§‘å­¦æ–¹æ³•ã€‚å®ƒåœ¨è¯ç‰©é‡å®šä½ã€æ–°é¶ç‚¹å‘ç°å’Œç»†èŒè¿›åŒ–æœºåˆ¶ç­‰ç”Ÿç‰©åŒ»å­¦é¢†åŸŸè¿›è¡Œäº†å¼€å‘å’ŒéªŒè¯ï¼Œå±•ç¤ºäº†å…¶åœ¨ç§‘å­¦å‘ç°ä¸­çš„æ½œåŠ›ã€‚é€šè¿‡è‡ªåŠ¨è¯„ä¼°ï¼Œç³»ç»Ÿåœ¨å‡è®¾è´¨é‡ä¸Šæ˜¾ç¤ºå‡ºæŒç»­çš„æ”¹è¿›ï¼Œè¡¨æ˜AIå¯ä»¥å¢å¼ºç§‘å­¦ç ”ç©¶çš„æ•ˆç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16776', 'title': 'AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement', 'url': 'https://huggingface.co/papers/2502.16776', 'abstract': 'As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.', 'score': 4, 'issue_id': 2432, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '3401008394925d16', 'authors': ['Zhexin Zhang', 'Leqi Lei', 'Junxiao Yang', 'Xijie Huang', 'Yida Lu', 'Shiyao Cui', 'Renmiao Chen', 'Qinglin Zhang', 'Xinyuan Wang', 'Hao Wang', 'Hao Li', 'Xianqi Lei', 'Chengwei Pan', 'Lei Sha', 'Hongning Wang', 'Minlie Huang'], 'affiliations': ['Beihang University, Beijing, China', 'The Conversational AI (CoAI) group, DCST, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2502.16776.jpg', 'data': {'categories': ['#benchmark', '#open_source', '#dataset', '#security'], 'emoji': 'ğŸ›¡ï¸', 'ru': {'title': 'Ğ•Ğ´Ğ¸Ğ½Ğ°Ñ Ğ»Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜', 'desc': 'AISafetyLab - ÑÑ‚Ğ¾ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼Ğ° Ğ¸ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°. ĞĞ½Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ°Ñ‚Ğ°Ğº, Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ˜Ğ˜, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Ğ¸Ğ½Ñ‚ÑƒĞ¸Ñ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Vicuna, Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Ğ°Ñ‚Ğ°Ğº Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹. ĞŸÑ€Ğ¾ĞµĞºÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ğ½Ğ° GitHub Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ ÑĞ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸ÑĞ¼ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚Ğ¸ Ğ˜Ğ˜.'}, 'en': {'title': 'AISafetyLab: A Unified Toolkit for AI Safety', 'desc': 'This paper addresses the critical challenge of ensuring AI safety as models are used in real-world applications. It introduces AISafetyLab, a comprehensive framework and toolkit that combines various attack, defense, and evaluation methods for AI safety. The toolkit is designed to be user-friendly, allowing developers to easily implement safety techniques while providing a flexible codebase for future enhancements. The authors also present empirical studies on the Vicuna model, comparing different safety strategies to highlight their effectiveness.'}, 'zh': {'title': 'æ„å»ºå®‰å…¨çš„äººå·¥æ™ºèƒ½ï¼šAISafetyLabæ¡†æ¶', 'desc': 'éšç€äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨å„ç§ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ï¼Œç¡®ä¿å…¶å®‰å…¨æ€§æˆä¸ºä¸€ä¸ªé‡è¦ä½†å°šæœªå……åˆ†æ¢ç´¢çš„æŒ‘æˆ˜ã€‚è™½ç„¶åœ¨è¯„ä¼°å’Œå¢å¼ºäººå·¥æ™ºèƒ½å®‰å…¨æ€§æ–¹é¢å·²ç»åšäº†å¤§é‡å·¥ä½œï¼Œä½†ç¼ºä¹æ ‡å‡†åŒ–æ¡†æ¶å’Œå…¨é¢å·¥å…·åŒ…ä»ç„¶æ˜¯ç³»ç»Ÿç ”ç©¶å’Œå®é™…åº”ç”¨çš„é‡å¤§éšœç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†AISafetyLabï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶å’Œå·¥å…·åŒ…ï¼Œé›†æˆäº†ä»£è¡¨æ€§çš„æ”»å‡»ã€é˜²å¾¡å’Œè¯„ä¼°æ–¹æ³•ã€‚AISafetyLabæä¾›äº†ç›´è§‚çš„ç•Œé¢ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿæ— ç¼åº”ç”¨å„ç§æŠ€æœ¯ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½ç»“æ„å’Œå¯æ‰©å±•çš„ä»£ç åº“ï¼Œä»¥ä¾¿æœªæ¥çš„è¿›æ­¥ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18906', 'title': 'VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model', 'url': 'https://huggingface.co/papers/2502.18906', 'abstract': "Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM). VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., Does this action advance the user's goal?). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated on Android-in-the-Wild benchmarks, VEM achieves state-of-the-art performance in both offline and online settings, outperforming environment-free baselines significantly and matching environment-based approaches without interaction costs. Importantly, VEM demonstrates that semantic-aware value estimation can achieve comparable performance with online-trained methods.", 'score': 4, 'issue_id': 2432, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '5ac6f89ffc897bbe', 'authors': ['Jiani Zheng', 'Lu Wang', 'Fangkai Yang', 'Chaoyun Zhang', 'Lingrui Mei', 'Wenjie Yin', 'Qingwei Lin', 'Dongmei Zhang', 'Saravan Rajmohan', 'Qi Zhang'], 'affiliations': ['KTH Royal Institute of Technology', 'Microsoft', 'Peking University', 'University of the Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.18906.jpg', 'data': {'categories': ['#reasoning', '#training', '#agents', '#rl', '#optimization'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ GUI Ğ±ĞµĞ· Ğ¿Ñ€ÑĞ¼Ğ¾Ğ³Ğ¾ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ·Ñ‹ĞºĞ° Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ±ĞµĞ·ÑÑ€ĞµĞ´Ğ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸ĞµĞ¼, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰ÑƒÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ (VEM). VEM Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¹ Ğ¸ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½-Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ€ĞµĞ´Ğ¾Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Android-in-the-Wild, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ±ĞµĞ·ÑÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹ Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ÑÑÑŒ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸, Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ ÑĞ¾ ÑÑ€ĞµĞ´Ğ¾Ğ¹.'}, 'en': {'title': 'Revolutionizing GUI Automation with Semantic-Aware Value Estimation', 'desc': 'This paper addresses the challenges of training Vision-Language Models (VLMs) for Graphical User Interfaces (GUIs) using Reinforcement Learning (RL). It introduces a novel environment-free RL framework that separates value estimation from policy optimization by utilizing a pretrained Value Environment Model (VEM). VEM predicts state-action values from offline data, allowing for better generalization and resilience to changes in the user interface. The proposed method shows superior performance on benchmarks, demonstrating that semantic reasoning can effectively guide GUI automation without the need for costly interactions.'}, 'zh': {'title': 'æ— ç¯å¢ƒå¼ºåŒ–å­¦ä¹ ï¼šæå‡GUIä»£ç†çš„æ™ºèƒ½åŒ–', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§ç¯å¢ƒæ— å…³çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä»¥æ”¯æŒå›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»£ç†ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„ä»·å€¼ç¯å¢ƒæ¨¡å‹ï¼ˆVEMï¼‰ï¼Œå°†ä»·å€¼ä¼°è®¡ä¸ç­–ç•¥ä¼˜åŒ–è§£è€¦ï¼Œé¿å…äº†ç¯å¢ƒåé¦ˆçš„éœ€æ±‚ã€‚VEMèƒ½å¤Ÿç›´æ¥ä»ç¦»çº¿æ•°æ®ä¸­é¢„æµ‹çŠ¶æ€-åŠ¨ä½œå€¼ï¼Œå¢å¼ºäº†å¯¹ç”¨æˆ·ç•Œé¢å˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œå¹¶ä¸“æ³¨äºè¯­ä¹‰æ¨ç†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVEMåœ¨Android-in-the-WildåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¶Šäº†ç¯å¢ƒæ— å…³çš„åŸºçº¿ï¼Œå¹¶åœ¨æ²¡æœ‰äº¤äº’æˆæœ¬çš„æƒ…å†µä¸‹ä¸åŸºäºç¯å¢ƒçš„æ–¹æ³•ç›¸åª²ç¾ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17955', 'title': "Language Models' Factuality Depends on the Language of Inquiry", 'url': 'https://huggingface.co/papers/2502.17955', 'abstract': "Multilingual language models (LMs) are expected to recall factual knowledge consistently across languages, yet they often fail to transfer knowledge between languages even when they possess the correct information in one of the languages. For example, we find that an LM may correctly identify Rashed Al Shashai as being from Saudi Arabia when asked in Arabic, but consistently fails to do so when asked in English or Swahili. To systematically investigate this limitation, we introduce a benchmark of 10,000 country-related facts across 13 languages and propose three novel metrics: Factual Recall Score, Knowledge Transferability Score, and Cross-Lingual Factual Knowledge Transferability Score-to quantify factual recall and knowledge transferability in LMs across different languages. Our results reveal fundamental weaknesses in today's state-of-the-art LMs, particularly in cross-lingual generalization where models fail to transfer knowledge effectively across different languages, leading to inconsistent performance sensitive to the language used. Our findings emphasize the need for LMs to recognize language-specific factual reliability and leverage the most trustworthy information across languages. We release our benchmark and evaluation framework to drive future research in multilingual knowledge transfer.", 'score': 3, 'issue_id': 2435, 'pub_date': '2025-02-25', 'pub_date_card': {'ru': '25 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 25', 'zh': '2æœˆ25æ—¥'}, 'hash': '25d78fa82880ea9e', 'authors': ['Tushar Aggarwal', 'Kumar Tanmay', 'Ayush Agrawal', 'Kumar Ayush', 'Hamid Palangi', 'Paul Pu Liang'], 'affiliations': ['Google', 'Harvard University', 'MIT', 'Microsoft Research', 'Mila', 'Stanford University', 'UniversitÃ© de MontrÃ©al'], 'pdf_title_img': 'assets/pdf/title_img/2502.17955.jpg', 'data': {'categories': ['#multilingual', '#benchmark', '#transfer_learning', '#low_resource'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸ÑÑ… Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚ĞµĞ¹', 'desc': 'Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ñ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ¸Ğ· 10 000 Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ°Ñ… Ğ½Ğ° 13 ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ²Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ¸ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ°Ğ±Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¾Ğ±Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ„Ğ°ĞºÑ‚Ğ¾Ğ² Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ°Ñ… Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ¾ÑÑ‚Ğ¾Ğ²ĞµÑ€Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ.'}, 'en': {'title': 'Bridging Language Gaps in Knowledge Transfer', 'desc': 'This paper investigates the limitations of multilingual language models (LMs) in transferring factual knowledge across different languages. Despite having the correct information in one language, these models often struggle to recall the same facts when queried in another language. The authors introduce a benchmark consisting of 10,000 country-related facts in 13 languages and propose new metrics to measure factual recall and knowledge transferability. The findings highlight significant weaknesses in current LMs, particularly in their ability to generalize knowledge across languages, underscoring the importance of improving cross-lingual factual reliability.'}, 'zh': {'title': 'è·¨è¯­è¨€çŸ¥è¯†è½¬ç§»çš„æŒ‘æˆ˜ä¸æœºé‡', 'desc': 'å¤šè¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰åœ¨ä¸åŒè¯­è¨€é—´åº”èƒ½ä¸€è‡´åœ°å›å¿†äº‹å®çŸ¥è¯†ï¼Œä½†å®ƒä»¬å¸¸å¸¸æ— æ³•æœ‰æ•ˆåœ°è½¬ç§»çŸ¥è¯†ã€‚æˆ‘ä»¬å‘ç°ï¼Œå°½ç®¡æ¨¡å‹åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­èƒ½æ­£ç¡®è¯†åˆ«æŸäº›ä¿¡æ¯ï¼Œä½†åœ¨è‹±è¯­æˆ–æ–¯ç“¦å¸Œé‡Œè¯­ä¸­å´æ— æ³•åšåˆ°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«10,000ä¸ªå›½å®¶ç›¸å…³äº‹å®çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶å¼•å…¥äº†ä¸‰ç§æ–°æŒ‡æ ‡æ¥é‡åŒ–æ¨¡å‹åœ¨ä¸åŒè¯­è¨€é—´çš„äº‹å®å›å¿†å’ŒçŸ¥è¯†è½¬ç§»èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„å¤šè¯­è¨€æ¨¡å‹åœ¨è·¨è¯­è¨€æ³›åŒ–æ–¹é¢çš„åŸºæœ¬å¼±ç‚¹ï¼Œå¼ºè°ƒäº†æ¨¡å‹éœ€è¦è¯†åˆ«è¯­è¨€ç‰¹å®šçš„äº‹å®å¯é æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19204', 'title': 'Distill Any Depth: Distillation Creates a Stronger Monocular Depth Estimator', 'url': 'https://huggingface.co/papers/2502.19204', 'abstract': 'Monocular depth estimation (MDE) aims to predict scene depth from a single RGB image and plays a crucial role in 3D scene understanding. Recent advances in zero-shot MDE leverage normalized depth representations and distillation-based learning to improve generalization across diverse scenes. However, current depth normalization methods for distillation, relying on global normalization, can amplify noisy pseudo-labels, reducing distillation effectiveness. In this paper, we systematically analyze the impact of different depth normalization strategies on pseudo-label distillation. Based on our findings, we propose Cross-Context Distillation, which integrates global and local depth cues to enhance pseudo-label quality. Additionally, we introduce a multi-teacher distillation framework that leverages complementary strengths of different depth estimation models, leading to more robust and accurate depth predictions. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms state-of-the-art methods, both quantitatively and qualitatively.', 'score': 3, 'issue_id': 2432, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '5f3b79d04ea09388', 'authors': ['Xiankang He', 'Dongyan Guo', 'Hongji Li', 'Ruibo Li', 'Ying Cui', 'Chi Zhang'], 'affiliations': ['AGI Lab, Westlake University', 'Lanzhou University', 'Nanyang Technological University', 'Zhejiang University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2502.19204.jpg', 'data': {'categories': ['#3d', '#cv', '#training', '#benchmark'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½ÑƒÑ Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾ÑĞ²ÑÑ‰ĞµĞ½Ğ° Ğ¼Ğ¾Ğ½Ğ¾ĞºÑƒĞ»ÑÑ€Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ ÑÑ†ĞµĞ½Ñ‹ Ğ¿Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ RGB-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Cross-Context Distillation, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿ÑĞµĞ²Ğ´Ğ¾-Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸. Ğ¢Ğ°ĞºĞ¶Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ´Ğ¸ÑÑ‚Ğ¸Ğ»Ğ»ÑÑ†Ğ¸Ğ¸ Ñ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ğ¼Ğ¸ ÑƒÑ‡Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ°Ñ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑĞ¼.'}, 'en': {'title': 'Enhancing Depth Estimation with Cross-Context Distillation', 'desc': 'This paper focuses on monocular depth estimation (MDE), which is the process of predicting depth information from a single RGB image. The authors identify that existing depth normalization techniques can worsen the quality of pseudo-labels used in distillation, which is crucial for improving model performance. To address this, they propose a new method called Cross-Context Distillation that combines both global and local depth information to enhance the quality of these pseudo-labels. Their multi-teacher distillation framework utilizes various depth estimation models to improve prediction accuracy, showing significant improvements over current leading methods in extensive experiments.'}, 'zh': {'title': 'è·¨ä¸Šä¸‹æ–‡è’¸é¦ï¼šæå‡æ·±åº¦ä¼°è®¡çš„è´¨é‡', 'desc': 'å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰æ—¨åœ¨ä»å•å¼ RGBå›¾åƒä¸­é¢„æµ‹åœºæ™¯æ·±åº¦ï¼Œå¯¹3Dåœºæ™¯ç†è§£è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„é›¶æ ·æœ¬MDEè¿›å±•åˆ©ç”¨å½’ä¸€åŒ–æ·±åº¦è¡¨ç¤ºå’ŒåŸºäºè’¸é¦çš„å­¦ä¹ æ¥æé«˜åœ¨ä¸åŒåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ·±åº¦å½’ä¸€åŒ–æ–¹æ³•ä¾èµ–äºå…¨å±€å½’ä¸€åŒ–ï¼Œå¯èƒ½ä¼šæ”¾å¤§å™ªå£°ä¼ªæ ‡ç­¾ï¼Œä»è€Œé™ä½è’¸é¦æ•ˆæœã€‚æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†ä¸åŒæ·±åº¦å½’ä¸€åŒ–ç­–ç•¥å¯¹ä¼ªæ ‡ç­¾è’¸é¦çš„å½±å“ï¼Œå¹¶æå‡ºäº†è·¨ä¸Šä¸‹æ–‡è’¸é¦æ–¹æ³•ï¼Œä»¥å¢å¼ºä¼ªæ ‡ç­¾è´¨é‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.18772', 'title': 'Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance', 'url': 'https://huggingface.co/papers/2502.18772', 'abstract': "Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance.", 'score': 2, 'issue_id': 2434, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': '0d131910a39317a9', 'authors': ['Xueqing Peng', 'Triantafillos Papadopoulos', 'Efstathia Soufleri', 'Polydoros Giannouris', 'Ruoyu Xiang', 'Yan Wang', 'Lingfei Qian', 'Jimin Huang', 'Qianqian Xie', 'Sophia Ananiadou'], 'affiliations': ['Archimedes/Athena RC Athens, Greece', 'Athens University of Economics and Business, Archimedes/Athena RC Athens, Greece', 'The Fin AI USA', 'The University of Manchester Manchester, UK'], 'pdf_title_img': 'assets/pdf/title_img/2502.18772.jpg', 'data': {'categories': ['#open_source', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': 'ğŸ’¶', 'ru': {'title': 'ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¼ NLP: Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¸Ğ¹ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ ÑÑ‚Ğ°Ğ»Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ Plutus-ben Ğ¸ Ğ¿ĞµÑ€Ğ²ÑƒÑ Ğ³Ñ€ĞµÑ‡ĞµÑĞºÑƒÑ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²ÑƒÑ ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Plutus-8B. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ñ‚Ñ€Ğ¸ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ²Ñ‹ÑĞ¾ĞºĞ¾ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ 22 ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Plutus-ben, Ğ²Ñ‹ÑĞ²Ğ»ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ NLP Ğ¸Ğ·-Ğ·Ğ° Ğ»Ğ¸Ğ½Ğ³Ğ²Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°ÑÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµĞ¶ÑŠÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ½Ğ¾ÑĞ° Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ¾Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¸Ğ·Ñ‹ Ğ² Ğ³Ñ€ĞµÑ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ….'}, 'en': {'title': 'Empowering Greek Financial NLP with Plutus-8B and Plutus-ben', 'desc': "This paper addresses the lack of large language models (LLMs) tailored for the Greek financial sector, highlighting the challenges posed by the Greek language's complexity and the absence of relevant datasets. It introduces Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, a specialized Greek Financial LLM, both designed to improve financial natural language processing (NLP) in Greek. The benchmark includes five key financial NLP tasks and is supported by three new, expertly annotated datasets. The study reveals significant challenges in adapting LLMs for Greek, emphasizing the need for models trained specifically on Greek financial data to enhance performance and understanding in this domain."}, 'zh': {'title': 'æ¨åŠ¨å¸Œè…Šé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†çš„åˆ›æ–°', 'desc': 'å°½ç®¡å¸Œè…Šåœ¨å…¨çƒç»æµä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä½†ç”±äºå¸Œè…Šè¯­è¨€çš„å¤æ‚æ€§å’Œç‰¹å®šé¢†åŸŸæ•°æ®é›†çš„ç¨€ç¼ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¸Œè…Šé‡‘èé¢†åŸŸçš„åº”ç”¨ä»ç„¶ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Plutus-benï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¸Œè…Šé‡‘èè¯„ä¼°åŸºå‡†ï¼Œä»¥åŠPlutus-8Bï¼Œè¿™æ˜¯é¦–ä¸ªç»è¿‡å¸Œè…Šç‰¹å®šé¢†åŸŸæ•°æ®å¾®è°ƒçš„å¸Œè…Šé‡‘èLLMã€‚Plutus-benæ¶µç›–äº†äº”ä¸ªæ ¸å¿ƒé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œæ—¨åœ¨ä¿ƒè¿›ç³»ç»ŸåŒ–å’Œå¯é‡å¤çš„LLMè¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¸Œè…Šé‡‘èè‡ªç„¶è¯­è¨€å¤„ç†é¢ä¸´è¯­è¨€å¤æ‚æ€§ã€ç‰¹å®šæœ¯è¯­å’Œé‡‘èæ¨ç†å·®è·ç­‰æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†å¼€å‘å¸Œè…Šè¯­é‡‘èæ¨¡å‹çš„å¿…è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.19279', 'title': 'CritiQ: Mining Data Quality Criteria from Human Preferences', 'url': 'https://huggingface.co/papers/2502.19279', 'abstract': 'Language model heavily depends on high-quality data for optimal performance. Existing approaches rely on manually designed heuristics, the perplexity of existing models, training classifiers, or careful prompt engineering, which require significant expert experience and human annotation effort while introduce biases. We introduce CritiQ, a novel data selection method that automatically mines criteria from human preferences for data quality with only sim30 human-annotated pairs and performs efficient data selection. The main component, CritiQ Flow, employs a manager agent to evolve quality criteria and worker agents to make pairwise judgments. We build a knowledge base that extracts quality criteria from previous work to boost CritiQ Flow. Compared to perplexity- and classifier- based methods, verbal criteria are more interpretable and possess reusable value. After deriving the criteria, we train the CritiQ Scorer to give quality scores and perform efficient data selection. We demonstrate the effectiveness of our method in the code, math, and logic domains, achieving high accuracy on human-annotated test sets. To validate the quality of the selected data, we continually train Llama 3.1 models and observe improved performance on downstream tasks compared to uniform sampling. Ablation studies validate the benefits of the knowledge base and the reflection process. We analyze how criteria evolve and the effectiveness of majority voting.', 'score': 1, 'issue_id': 2435, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 26', 'zh': '2æœˆ26æ—¥'}, 'hash': 'ba207b81c3e39cc5', 'authors': ['Honglin Guo', 'Kai Lv', 'Qipeng Guo', 'Tianyi Liang', 'Zhiheng Xi', 'Demin Song', 'Qiuyinzhe Zhang', 'Yu Sun', 'Kai Chen', 'Xipeng Qiu', 'Tao Gui'], 'affiliations': ['East China Normal University', 'Fudan University', 'Shanghai AI Laboratory', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2502.19279.jpg', 'data': {'categories': ['#data', '#optimization', '#interpretability', '#dataset', '#agents', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'Ğ£Ğ¼Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'CritiQ - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ½ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞºĞ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ²ÑĞµĞ³Ğ¾ Ğ¾ĞºĞ¾Ğ»Ğ¾ 30 Ñ€Ğ°Ğ·Ğ¼ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ€. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ CritiQ Flow Ñ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ğ°Ğ¼Ğ¸ Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ±Ğ°Ğ·Ñƒ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ñ ĞºÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸ÑĞ¼Ğ¸ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… Ñ€Ğ°Ğ±Ğ¾Ñ‚. CritiQ Scorer Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ñ‡Ñ‚Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° Ñ†ĞµĞ»ĞµĞ²Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ€Ğ°Ğ²Ğ½Ğ¾Ğ¼ĞµÑ€Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¾Ğ¹.'}, 'en': {'title': 'CritiQ: Automating Data Quality Selection for Better Language Models', 'desc': 'This paper presents CritiQ, a new method for selecting high-quality data for training language models. Unlike traditional approaches that rely on complex heuristics or human expertise, CritiQ uses a small set of human-annotated pairs to automatically derive quality criteria. The method features a manager agent that evolves these criteria and worker agents that make pairwise judgments, leading to more interpretable and reusable quality assessments. The results show that CritiQ significantly improves model performance in various domains by selecting better data compared to existing methods.'}, 'zh': {'title': 'è‡ªåŠ¨æŒ–æ˜æ•°æ®è´¨é‡æ ‡å‡†çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºCritiQçš„æ–°å‹æ•°æ®é€‰æ‹©æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡è‡ªåŠ¨æŒ–æ˜äººç±»åå¥½çš„æ ‡å‡†æ¥æé«˜æ•°æ®è´¨é‡ã€‚CritiQçš„ä¸»è¦ç»„æˆéƒ¨åˆ†æ˜¯CritiQ Flowï¼Œå®ƒåˆ©ç”¨ç®¡ç†ä»£ç†å’Œå·¥ä½œä»£ç†è¿›è¡Œè´¨é‡æ ‡å‡†çš„æ¼”å˜å’Œæˆå¯¹åˆ¤æ–­ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå›°æƒ‘åº¦å’Œåˆ†ç±»å™¨çš„æ–¹æ³•ç›¸æ¯”ï¼ŒCritiQæä¾›äº†æ›´å…·å¯è§£é‡Šæ€§çš„è¯­è¨€æ ‡å‡†ï¼Œå¹¶å…·æœ‰å¯é‡ç”¨çš„ä»·å€¼ã€‚é€šè¿‡è®­ç»ƒCritiQ Scorerï¼Œæˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°ä¸ºæ•°æ®æ‰“åˆ†å¹¶è¿›è¡Œé€‰æ‹©ï¼Œä»è€Œåœ¨ä»£ç ã€æ•°å­¦å’Œé€»è¾‘é¢†åŸŸå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.17540', 'title': 'PosterSum: A Multimodal Benchmark for Scientific Poster Summarization', 'url': 'https://huggingface.co/papers/2502.17540', 'abstract': 'Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce PosterSum, a novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on PosterSum and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose Segment & Summarize, a hierarchical method that outperforms current MLLMs on automated metrics, achieving a 3.14% gain in ROUGE-L. This will serve as a starting point for future research on poster summarization.', 'score': 1, 'issue_id': 2435, 'pub_date': '2025-02-24', 'pub_date_card': {'ru': '24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 24', 'zh': '2æœˆ24æ—¥'}, 'hash': '1a13cbace88a6862', 'authors': ['Rohit Saxena', 'Pasquale Minervini', 'Frank Keller'], 'affiliations': ['Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2502.17540.jpg', 'data': {'categories': ['#benchmark', '#science', '#dataset', '#multimodal'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'PosterSum: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ ÑÑƒĞ¼Ğ¼Ğ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº PosterSum Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¸ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ñ‹ Ğ² Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ñ‹. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ±Ğ¾Ğ»ĞµĞµ 16 Ñ‚Ñ‹ÑÑÑ‡ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ² Ñ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ°Ğ±ÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ñ… Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ğ½Ğ° ÑÑ‚Ğ¾Ğ¼ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞµ Ğ¸ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ¸Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ÑÑ‚ĞµÑ€Ğ¾Ğ². ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½ Ğ¸ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Segment & Summarize, Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ MLLM Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼.'}, 'en': {'title': 'Advancing Scientific Poster Summarization with PosterSum', 'desc': 'This paper presents PosterSum, a new benchmark designed to improve vision-language models for summarizing scientific posters into concise research paper abstracts. The dataset includes 16,305 conference posters with their corresponding abstracts, highlighting the challenges of interpreting complex visual content. The authors evaluate state-of-the-art Multimodal Large Language Models (MLLMs) on this dataset and find that they struggle with accurate summarization. To address this, they introduce a hierarchical method called Segment & Summarize, which shows improved performance over existing models, achieving a 3.14% increase in ROUGE-L scores.'}, 'zh': {'title': 'ç§‘å­¦æµ·æŠ¥æ‘˜è¦ç”Ÿæˆçš„æ–°çªç ´', 'desc': 'æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„åŸºå‡†æ•°æ®é›†PosterSumï¼Œæ—¨åœ¨æ¨åŠ¨è§†è§‰è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä»¥ç†è§£å’Œæ€»ç»“ç§‘å­¦æµ·æŠ¥ã€‚è¯¥æ•°æ®é›†åŒ…å«16,305ä¸ªä¼šè®®æµ·æŠ¥åŠå…¶å¯¹åº”çš„æ‘˜è¦ï¼Œæä¾›äº†å¤šæ ·çš„è§†è§‰ç†è§£æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¯¹å½“å‰æœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºå®ƒä»¬åœ¨å‡†ç¡®è§£è¯»å’Œæ€»ç»“ç§‘å­¦æµ·æŠ¥æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†æ®µä¸æ€»ç»“çš„å±‚æ¬¡æ–¹æ³•ï¼Œåœ¨è‡ªåŠ¨åŒ–æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ¨¡å‹ï¼ŒROUGE-LæŒ‡æ ‡æé«˜äº†3.14%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2502.16284', 'title': 'MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra', 'url': 'https://huggingface.co/papers/2502.16284', 'abstract': "Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in a significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer a more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using a contrastive objective, we enhance the 3D encoder's understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics.", 'score': 1, 'issue_id': 2433, 'pub_date': '2025-02-22', 'pub_date_card': {'ru': '22 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 'en': 'February 22', 'zh': '2æœˆ22æ—¥'}, 'hash': '59ce39707b24fae2', 'authors': ['Liang Wang', 'Shaozhen Liu', 'Yu Rong', 'Deli Zhao', 'Qiang Liu', 'Shu Wu', 'Liang Wang'], 'affiliations': ['DAMO Academy, Alibaba Group', 'New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)', 'School of Artificial Intelligence, University of Chinese Academy of Sciences'], 'pdf_title_img': 'assets/pdf/title_img/2502.16284.jpg', 'data': {'categories': ['#science', '#dataset', '#3d', '#benchmark', '#architecture'], 'emoji': 'ğŸ”¬', 'ru': {'title': 'ĞšĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ ÑĞºĞ°Ñ‡Ğ¾Ğº Ğ² Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ 3D-Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ². ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ MolSpectra, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸ĞºĞ¸ Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ¼ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ SpecFormer - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ»ÑÑ€Ğ½Ñ‹Ñ… ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¸ ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ² Ğ¼Ğ¾Ğ»ĞµĞºÑƒĞ» Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.'}, 'en': {'title': 'Enhancing 3D Molecular Representations with Quantum Insights', 'desc': 'This paper addresses the challenge of accurately modeling molecular energy states by incorporating quantum mechanical effects into 3D molecular representations. The authors introduce a novel approach called MolSpectra, which leverages energy spectra to improve the pre-training of these representations. They present SpecFormer, a multi-spectrum encoder that utilizes masked patch reconstruction to effectively encode molecular spectra. The results demonstrate that their method significantly outperforms traditional techniques in predicting molecular properties and understanding molecular dynamics.'}, 'zh': {'title': 'é‡å­åŠ›å­¦åŠ©åŠ›ä¸‰ç»´åˆ†å­è¡¨ç¤ºçš„æå‡', 'desc': 'æœ¬è®ºæ–‡æ¢è®¨äº†ä¸‰ç»´åˆ†å­ç»“æ„ä¸åˆ†å­ç³»ç»Ÿèƒ½é‡çŠ¶æ€ä¹‹é—´çš„å…³ç³»ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥å­¦ä¹ ä¸‰ç»´åˆ†å­è¡¨ç¤ºã€‚ç°æœ‰æ–¹æ³•ä¸»è¦åŸºäºç»å…¸åŠ›å­¦ï¼Œå¿½è§†äº†é‡å­åŠ›å­¦æ•ˆåº”ï¼Œå¦‚é‡å­åŒ–çš„èƒ½çº§ç»“æ„ï¼Œè¿™å¯¹èƒ½é‡çš„å‡†ç¡®ä¼°è®¡è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†MolSpectraï¼Œé€šè¿‡åˆ©ç”¨èƒ½é‡å…‰è°±æ¥å¢å¼ºä¸‰ç»´åˆ†å­è¡¨ç¤ºçš„é¢„è®­ç»ƒï¼Œç»“åˆé‡å­åŠ›å­¦çŸ¥è¯†ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®¾è®¡äº†SpecFormerï¼Œä¸€ä¸ªå¤šå…‰è°±ç¼–ç å™¨ï¼Œé€šè¿‡æ©è”½è¡¥ä¸é‡å»ºæ¥ç¼–ç åˆ†å­å…‰è°±ï¼Œä»è€Œæå‡ä¸‰ç»´ç¼–ç å™¨å¯¹åˆ†å­çš„ç†è§£ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (4)', '#agi', '#alignment (1)', '#architecture (2)', '#audio', '#benchmark (9)', '#cv (1)', '#data (2)', '#dataset (7)', '#diffusion', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare (1)', '#inference', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (3)', '#machine_translation (1)', '#math', '#multilingual (3)', '#multimodal (2)', '#open_source (4)', '#optimization (2)', '#plp', '#rag (1)', '#reasoning (3)', '#rl (1)', '#rlhf (1)', '#robotics', '#science (3)', '#security (1)', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (6)', '#transfer_learning (1)', '#video (1)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2025-02-27 06:14',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-02-27 06:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-02-27 06:14')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    