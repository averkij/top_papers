
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 15 papers. April 16.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">16 апреля</span> | <span id="title-articles-count">15 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-04-15.html">⬅️ <span id="prev-date">15.04</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-04-17.html">➡️ <span id="next-date">17.04</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-04.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '16 апреля', 'en': 'April 16', 'zh': '4月16日'};
        let feedDateNext = {'ru': '17.04', 'en': '04/17', 'zh': '4月17日'};
        let feedDatePrev = {'ru': '15.04', 'en': '04/15', 'zh': '4月15日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2504.10481', 'title': 'xVerify: Efficient Answer Verifier for Reasoning Model Evaluations', 'url': 'https://huggingface.co/papers/2504.10481', 'abstract': 'With the release of the o1 model by OpenAI, reasoning models adopting slow thinking strategies have gradually emerged. As the responses generated by such models often include complex reasoning, intermediate steps, and self-reflection, existing evaluation methods are often inadequate. They struggle to determine whether the LLM output is truly equivalent to the reference answer, and also have difficulty identifying and extracting the final answer from long, complex responses. To address this issue, we propose xVerify, an efficient answer verifier for reasoning model evaluations. xVerify demonstrates strong capability in equivalence judgment, enabling it to effectively determine whether the answers produced by reasoning models are equivalent to reference answers across various types of objective questions. To train and evaluate xVerify, we construct the VAR dataset by collecting question-answer pairs generated by multiple LLMs across various datasets, leveraging multiple reasoning models and challenging evaluation sets designed specifically for reasoning model assessment. A multi-round annotation process is employed to ensure label accuracy. Based on the VAR dataset, we train multiple xVerify models of different scales. In evaluation experiments conducted on both the test set and generalization set, all xVerify models achieve overall F1 scores and accuracy exceeding 95\\%. Notably, the smallest variant, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. These results validate the effectiveness and generalizability of xVerify.', 'score': 18, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '72678fc1ff453072', 'authors': ['Ding Chen', 'Qingchen Yu', 'Pengyuan Wang', 'Wentao Zhang', 'Bo Tang', 'Feiyu Xiong', 'Xinchi Li', 'Minchuan Yang', 'Zhiyu Li'], 'affiliations': ['Center for Data Science, Peking University', 'MemTensor (Shanghai) Technology Co., Ltd.', 'Research Institute of China Telecom, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.10481.jpg', 'data': {'categories': ['#interpretability', '#benchmark', '#dataset', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'xVerify: точная верификация ответов моделей рассуждения', 'desc': 'Статья представляет xVerify - эффективный верификатор ответов для оценки моделей рассуждения. xVerify способен определять эквивалентность ответов, генерируемых моделями рассуждения, эталонным ответам для различных типов объективных вопросов. Для обучения и оценки xVerify был создан набор данных VAR, содержащий пары вопросов-ответов от нескольких языковых моделей. Эксперименты показали, что модели xVerify достигают F1-меры и точности выше 95% на тестовом и обобщающем наборах данных.'}, 'en': {'title': 'xVerify: Elevating Reasoning Model Evaluation with Precision', 'desc': 'This paper introduces xVerify, a novel answer verification tool designed to evaluate reasoning models that utilize slow thinking strategies. Traditional evaluation methods struggle with complex outputs from large language models (LLMs), particularly in assessing the equivalence of answers and extracting final responses. xVerify addresses these challenges by leveraging a specially constructed VAR dataset, which includes diverse question-answer pairs generated by various LLMs. The results show that xVerify models achieve high accuracy and F1 scores, demonstrating their effectiveness in evaluating reasoning models compared to existing methods.'}, 'zh': {'title': 'xVerify：推理模型评估的新标准', 'desc': '随着OpenAI发布o1模型，采用慢思维策略的推理模型逐渐出现。这些模型生成的响应通常包含复杂的推理、中间步骤和自我反思，现有的评估方法往往无法有效判断LLM输出是否真正等同于参考答案。为了解决这个问题，我们提出了xVerify，一个高效的答案验证器，用于推理模型的评估。xVerify在等价判断方面表现出色，能够有效判断推理模型生成的答案是否与参考答案等价，并在多个客观问题类型中表现优异。'}}}, {'id': 'https://huggingface.co/papers/2504.10337', 'title': 'Heimdall: test-time scaling on the generative verification', 'url': 'https://huggingface.co/papers/2504.10337', 'abstract': 'An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5% to 94.5% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2% to 70.0% with 16x compute budget and to 83.3% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.', 'score': 17, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '6da5db970a101d21', 'authors': ['Wenlei Shi', 'Xing Jin'], 'affiliations': ['bytedance.com'], 'pdf_title_img': 'assets/pdf/title_img/2504.10337.jpg', 'data': {'categories': ['#dataset', '#reasoning', '#long_context', '#optimization', '#training', '#rl', '#math'], 'emoji': '🔍', 'ru': {'title': 'Heimdall: ИИ-верификатор для повышения надежности рассуждений языковых моделей', 'desc': 'Статья представляет Heimdall - модель верификации для длинных цепочек рассуждений (Chain-of-Thought). С помощью обучения с подкреплением точность верификации была повышена с 62.5% до 94.5% на сложных математических задачах. Предложен метод пессимистической верификации для масштабирования решения задач. Heimdall демонстрирует высокую способность к обобщению, обнаруживая ошибки даже в сложных математических доказательствах.'}, 'en': {'title': 'Heimdall: Elevating AI Verification with Chain-of-Thought Reasoning', 'desc': 'This paper introduces Heimdall, a long Chain-of-Thought (CoT) verification model designed to enhance the accuracy of solution verification in AI systems. By employing pure reinforcement learning, Heimdall significantly improves verification accuracy on competitive math problems from 62.5% to 94.5%, and with repeated sampling, it reaches 97.5%. The paper also presents Pessimistic Verification, which optimizes solution selection by minimizing uncertainty, leading to improved accuracy in problem-solving tasks. Additionally, Heimdall is part of an automatic knowledge discovery system that identifies flaws in datasets, revealing that a substantial portion of the data is incorrect, which is consistent with findings from previous studies.'}, 'zh': {'title': '提升AI知识验证能力的Heimdall模型', 'desc': '本文提出了一种名为Heimdall的长链思维验证大语言模型（LLM），旨在提高解决竞争性数学问题的解答准确性。通过纯强化学习，Heimdall的验证准确率从62.5%提升至94.5%，并通过重复采样进一步提高至97.5%。此外，Heimdall还展示了出色的泛化能力，能够检测出训练中未包含的复杂数学证明中的大多数问题。我们还提出了悲观验证方法，以扩展Heimdall的功能，显著提高了解决方案的准确性。'}}}, {'id': 'https://huggingface.co/papers/2504.10766', 'title': 'How Instruction and Reasoning Data shape Post-Training: Data Quality\n  through the Lens of Layer-wise Gradients', 'url': 'https://huggingface.co/papers/2504.10766', 'abstract': "As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.", 'score': 14, 'issue_id': 3258, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'b95bec819bad5307', 'authors': ['Ming Li', 'Yanhong Li', 'Ziyue Li', 'Tianyi Zhou'], 'affiliations': ['University of Chicago', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2504.10766.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#data', '#training'], 'emoji': '🧠', 'ru': {'title': 'Спектральный анализ раскрывает секреты качества данных в обучении языковых моделей', 'desc': 'Эта статья представляет спектральный анализ послойных градиентов, вызванных данными разного качества при дообучении больших языковых моделей (LLM). Исследование показывает, что метрики оценки данных, такие как IFD, InsTag, Difficulty и Reward, можно объяснить и объединить с помощью спектральных свойств, вычисленных из сингулярного разложения градиентов. Авторы обнаружили, что данные более высокого качества обычно связаны с более низкими ядерными нормами и более высокими эффективными рангами. Эксперименты также показывают, что модели одного семейства имеют схожие паттерны градиентов независимо от их размера, в то время как разные семейства моделей значительно различаются.'}, 'en': {'title': 'Unlocking the Secrets of Data Quality in LLM Fine-Tuning', 'desc': 'This paper investigates how the quality of data influences the fine-tuning of large language models (LLMs) during post-training, particularly for complex reasoning tasks. It employs spectral analysis of layer-wise gradients to understand the effects of low and high-quality instruction and reasoning data. The study finds that traditional metrics for data evaluation can be unified through the spectral properties derived from the singular value decomposition (SVD) of gradients. Notably, it shows that higher-quality data leads to lower nuclear norms and higher effective ranks, with effective rank being a more reliable measure for capturing quality differences, especially in reasoning tasks.'}, 'zh': {'title': '数据质量与训练稳定性的统一视角', 'desc': '本文探讨了大语言模型（LLM）在后训练阶段中，不同数据对微调动态的影响。我们通过对低质量和高质量指令及推理数据的层级梯度进行谱分析，发现常用的数据评估指标可以通过梯度的奇异值分解（SVD）谱特性来解释和统一。研究表明，高质量数据通常与较低的核范数和较高的有效秩相关，且有效秩在捕捉细微质量差异方面表现出更好的鲁棒性和分辨率。我们的实验还表明，同一家族的模型在梯度模式上相似，而不同模型家族之间则存在显著差异。'}}}, {'id': 'https://huggingface.co/papers/2504.10465', 'title': 'Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding', 'url': 'https://huggingface.co/papers/2504.10465', 'abstract': "Multimodal Large Language Models (MLLMs) achieve remarkable performance for fine-grained pixel-level understanding tasks. However, all the works rely heavily on extra components, such as vision encoder (CLIP), segmentation experts, leading to high system complexity and limiting model scaling. In this work, our goal is to explore a highly simplified MLLM without introducing extra components. Our work is motivated by the recent works on Single trAnsformer as a unified vIsion-Language Model (SAIL) design, where these works jointly learn vision tokens and text tokens in transformers. We present Pixel-SAIL, a single transformer for pixel-wise MLLM tasks. In particular, we present three technical improvements on the plain baseline. First, we design a learnable upsampling module to refine visual token features. Secondly, we propose a novel visual prompt injection strategy to enable the single transformer to understand visual prompt inputs and benefit from the early fusion of visual prompt embeddings and vision tokens. Thirdly, we introduce a vision expert distillation strategy to efficiently enhance the single transformer's fine-grained feature extraction capability. In addition, we have collected a comprehensive pixel understanding benchmark (PerBench), using a manual check. It includes three tasks: detailed object description, visual prompt-based question answering, and visual-text referring segmentation. Extensive experiments on four referring segmentation benchmarks, one visual prompt benchmark, and our PerBench show that our Pixel-SAIL achieves comparable or even better results with a much simpler pipeline. Code and model will be released at https://github.com/magic-research/Sa2VA.", 'score': 14, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '6c90d31f3f941694', 'authors': ['Tao Zhang', 'Xiangtai Li', 'Zilong Huang', 'Yanwei Li', 'Weixian Lei', 'Xueqing Deng', 'Shihao Chen', 'Shunping Ji', 'Jiashi Feng'], 'affiliations': ['Bytedance Seed', 'WHU'], 'pdf_title_img': 'assets/pdf/title_img/2504.10465.jpg', 'data': {'categories': ['#survey', '#optimization', '#benchmark', '#multimodal', '#training', '#architecture', '#games'], 'emoji': '🔍', 'ru': {'title': 'Единый трансформер для точного анализа изображений', 'desc': 'Статья представляет Pixel-SAIL - единую трансформерную модель для задач мультимодального машинного обучения на уровне пикселей. Авторы предлагают три ключевых улучшения: модуль повышающей дискретизации, стратегию внедрения визуальных подсказок и дистилляцию экспертных знаний. Модель показывает сопоставимые или лучшие результаты по сравнению с более сложными системами на нескольких эталонных наборах данных. Исследователи также представляют новый набор данных PerBench для комплексной оценки понимания изображений на уровне пикселей.'}, 'en': {'title': 'Simplifying Multimodal Learning with Pixel-SAIL', 'desc': "This paper introduces Pixel-SAIL, a simplified Multimodal Large Language Model (MLLM) designed for pixel-level understanding tasks without relying on additional components like vision encoders. The authors propose three key innovations: a learnable upsampling module for refining visual features, a visual prompt injection strategy for better integration of visual and text inputs, and a vision expert distillation method to enhance feature extraction. By focusing on a single transformer architecture, Pixel-SAIL aims to reduce system complexity while maintaining high performance. The paper also presents a new benchmark, PerBench, to evaluate the model's effectiveness across various pixel understanding tasks."}, 'zh': {'title': '简化的多模态语言模型：Pixel-SAIL', 'desc': '多模态大型语言模型（MLLMs）在细粒度像素级理解任务中表现出色，但大多数工作依赖于额外的组件，如视觉编码器和分割专家，导致系统复杂性高，限制了模型的扩展性。本文提出了一种简化的MLLM，名为Pixel-SAIL，旨在不引入额外组件的情况下进行像素级任务。我们通过设计可学习的上采样模块、视觉提示注入策略和视觉专家蒸馏策略，提升了单一变换器的特征提取能力。实验结果表明，Pixel-SAIL在多个基准测试中表现出色，且具有更简单的处理流程。'}}}, {'id': 'https://huggingface.co/papers/2504.11442', 'title': 'TextArena', 'url': 'https://huggingface.co/papers/2504.11442', 'abstract': 'TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.', 'score': 13, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '7c9ae6533757828a', 'authors': ['Leon Guertler', 'Bobby Cheng', 'Simon Yu', 'Bo Liu', 'Leshem Choshen', 'Cheston Tan'], 'affiliations': ['Centre for Frontier AI Research (CFAR), A*STAR Institute of High Performance Computing, A*STAR', 'MIT, MIT-IBM Watson AI Lab', 'National University of Singapore', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11442.jpg', 'data': {'categories': ['#agents', '#games', '#benchmark', '#open_source'], 'emoji': '🎮', 'ru': {'title': 'TextArena: Арена для оттачивания социального интеллекта языковых моделей', 'desc': 'TextArena - это открытый набор соревновательных текстовых игр для обучения и оценки агентного поведения больших языковых моделей (LLM). Он включает более 57 уникальных сред, позволяющих оценивать возможности моделей через систему онлайн-игры с рейтингом TrueSkill в реальном времени. TextArena восполняет пробел в оценке динамических социальных навыков, таких как ведение переговоров, теория разума и обман. Платформа разработана с учетом исследовательских целей, возможностей сообщества и расширяемости, облегчая добавление новых игр и тестирование моделей.'}, 'en': {'title': 'Train LLMs in Competitive Text Games!', 'desc': 'TextArena is a platform designed for training and evaluating Large Language Models (LLMs) through competitive text-based games. It features over 57 unique environments that support various gameplay modes, allowing models to interact with both human players and other models. This framework addresses the lack of benchmarks for assessing social skills like negotiation and deception, which are crucial for agentic behavior. TextArena is built for research and community engagement, making it easy to add new games and adapt the system for testing and training purposes.'}, 'zh': {'title': 'TextArena：提升语言模型的社交技能训练平台', 'desc': 'TextArena是一个开源的竞争性文本游戏集合，旨在训练和评估大型语言模型（LLMs）的代理行为。它包含57个以上独特的环境，支持单人、双人和多人设置，并通过在线游戏系统轻松评估模型能力。传统基准测试通常无法评估动态社交技能，如谈判、心智理论和欺骗，而TextArena正好填补了这一空白。该平台强调易于添加新游戏、适应框架、测试模型和训练模型，适合研究和社区使用。'}}}, {'id': 'https://huggingface.co/papers/2504.11346', 'title': 'Seedream 3.0 Technical Report', 'url': 'https://huggingface.co/papers/2504.11346', 'abstract': 'We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.', 'score': 11, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '3a4b797a3a9516d2', 'authors': ['Yu Gao', 'Lixue Gong', 'Qiushan Guo', 'Xiaoxia Hou', 'Zhichao Lai', 'Fanshi Li', 'Liang Li', 'Xiaochen Lian', 'Chao Liao', 'Liyang Liu', 'Wei Liu', 'Yichun Shi', 'Shiqi Sun', 'Yu Tian', 'Zhi Tian', 'Peng Wang', 'Rui Wang', 'Xuanda Wang', 'Xun Wang', 'Ye Wang', 'Guofeng Wu', 'Jie Wu', 'Xin Xia', 'Xuefeng Xiao', 'Zhonghua Zhai', 'Xinyu Zhang', 'Qi Zhang', 'Yuwei Zhang', 'Shijia Zhao', 'Jianchao Yang', 'Weilin Huang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2504.11346.jpg', 'data': {'categories': ['#alignment', '#data', '#optimization', '#dataset', '#multimodal', '#architecture', '#training'], 'emoji': '🎨', 'ru': {'title': 'Революция в генерации изображений: Seedream 3.0 поднимает планку', 'desc': 'Seedream 3.0 - это двуязычная модель генерации изображений, улучшающая предыдущую версию. Модель использует новые техники обработки данных, включая обучение на смешанных разрешениях и выравнивание представлений. Применяются усовершенствованные методы постобработки, такие как эстетические подписи и модель вознаграждения на основе VLM. Seedream 3.0 также вводит новую парадигму ускорения, позволяющую увеличить скорость в 4-8 раз без потери качества.'}, 'en': {'title': 'Revolutionizing Bilingual Image Generation with Seedream 3.0', 'desc': 'Seedream 3.0 is an advanced bilingual image generation model designed for Chinese and English. It improves upon its predecessor by enhancing prompt alignment, typography generation, and overall image quality through a series of technical upgrades. Key innovations include a larger dataset, mixed-resolution training, and a novel acceleration method that speeds up processing while preserving image fidelity. The model excels in generating high-resolution images and accurately rendering complex Chinese characters, making it valuable for professional typography applications.'}, 'zh': {'title': 'Seedream 3.0：高效的中英文图像生成新纪元', 'desc': 'Seedream 3.0 是一个高性能的中英文双语图像生成基础模型。它通过多项技术改进，解决了 Seedream 2.0 中存在的挑战，如复杂提示的对齐、细致的排版生成和图像分辨率限制。该模型在数据构建和模型部署的整个流程中进行了改进，采用了缺陷感知训练和双轴协作数据采样等方法。Seedream 3.0 还引入了新的加速范式，实现了图像质量与生成速度的显著提升，特别是在复杂汉字的文本渲染方面表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.10462', 'title': 'The Scalability of Simplicity: Empirical Analysis of Vision-Language\n  Learning with a Single Transformer', 'url': 'https://huggingface.co/papers/2504.10462', 'abstract': "This paper introduces SAIL, a single transformer unified multimodal large language model (MLLM) that integrates raw pixel encoding and language decoding within a singular architecture. Unlike existing modular MLLMs, which rely on a pre-trained vision transformer (ViT), SAIL eliminates the need for a separate vision encoder, presenting a more minimalist architecture design. Instead of introducing novel architectural components, SAIL adapts mix-attention mechanisms and multimodal positional encodings to better align with the distinct characteristics of visual and textual modalities. We systematically compare SAIL's properties-including scalability, cross-modal information flow patterns, and visual representation capabilities-with those of modular MLLMs. By scaling both training data and model size, SAIL achieves performance comparable to modular MLLMs. Notably, the removal of pretrained ViT components enhances SAIL's scalability and results in significantly different cross-modal information flow patterns. Moreover, SAIL demonstrates strong visual representation capabilities, achieving results on par with ViT-22B in vision tasks such as semantic segmentation. Code and models are available at https://github.com/bytedance/SAIL.", 'score': 8, 'issue_id': 3260, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '1f70a22447fd1fc5', 'authors': ['Weixian Lei', 'Jiacong Wang', 'Haochen Wang', 'Xiangtai Li', 'Jun Hao Liew', 'Jiashi Feng', 'Zilong Huang'], 'affiliations': ['Bytedance Seed'], 'pdf_title_img': 'assets/pdf/title_img/2504.10462.jpg', 'data': {'categories': ['#multimodal', '#agi', '#architecture', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'SAIL: единая архитектура для мультимодального машинного обучения', 'desc': 'SAIL - это унифицированная мультимодальная большая языковая модель, объединяющая обработку изображений и текста в единой архитектуре трансформера. В отличие от модульных моделей, SAIL не использует предобученный vision transformer, а напрямую кодирует пиксели изображений. Модель адаптирует механизмы смешанного внимания и мультимодальные позиционные кодировки для лучшего согласования визуальной и текстовой модальностей. SAIL демонстрирует сопоставимую производительность с модульными моделями и сильные возможности визуального представления.'}, 'en': {'title': 'SAIL: A Unified Approach to Multimodal Learning', 'desc': "This paper presents SAIL, a unified multimodal large language model that combines image and text processing in one architecture without needing a separate vision encoder. SAIL uses mix-attention mechanisms and multimodal positional encodings to effectively handle both visual and textual data. The study shows that SAIL can scale well with increased training data and model size, achieving performance similar to existing modular models. Additionally, SAIL's design leads to unique patterns in how information flows between modalities, while also excelling in visual tasks like semantic segmentation."}, 'zh': {'title': 'SAIL：简约架构下的多模态语言模型', 'desc': '本文介绍了SAIL，这是一种单一变换器统一多模态大语言模型（MLLM），它在一个架构中整合了原始像素编码和语言解码。与现有的模块化MLLM不同，SAIL不需要单独的视觉编码器，呈现出更简约的架构设计。SAIL采用混合注意力机制和多模态位置编码，以更好地适应视觉和文本模态的独特特征。通过扩大训练数据和模型规模，SAIL在性能上与模块化MLLM相当，同时在视觉表示能力上也表现出色。'}}}, {'id': 'https://huggingface.co/papers/2504.10559', 'title': 'Efficient Process Reward Model Training via Active Learning', 'url': 'https://huggingface.co/papers/2504.10559', 'abstract': "Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.", 'score': 6, 'issue_id': 3259, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': 'f718e5da41cde633', 'authors': ['Keyu Duan', 'Zichen Liu', 'Xin Mao', 'Tianyu Pang', 'Changyu Chen', 'Qiguang Chen', 'Michael Qizhe Shieh', 'Longxu Dou'], 'affiliations': ['National University of Singapore', 'Sea AI Lab', 'Singapore Management University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10559.jpg', 'data': {'categories': ['#data', '#reasoning', '#optimization', '#benchmark', '#math', '#training'], 'emoji': '🎯', 'ru': {'title': 'Активное обучение для эффективных моделей вознаграждения процессов', 'desc': 'Статья представляет ActPRM - подход активного обучения для моделей вознаграждения процессов (PRM). ActPRM выбирает наиболее неопределенные образцы для обучения, что значительно снижает затраты на разметку данных. Метод использует PRM для оценки неопределенности после прямого прохода, сохраняя только высоко неопределенные данные. Эксперименты показывают, что ActPRM сокращает аннотацию на 50% при сохранении или улучшении производительности по сравнению с обычной тонкой настройкой.'}, 'en': {'title': 'Efficient Learning with Uncertainty: ActPRM for Enhanced Model Training', 'desc': 'This paper introduces ActPRM, an active learning method designed to enhance Process Reward Models (PRMs) for training large language models (LLMs). By focusing on the most uncertain samples, ActPRM significantly cuts down the costs associated with data labeling while maintaining or improving model performance. The approach involves using the PRM to estimate uncertainty and selectively retaining only the most ambiguous data for labeling by a more complex reasoning model. The results show that ActPRM not only reduces annotation requirements by 50% but also achieves state-of-the-art performance on benchmark tasks.'}, 'zh': {'title': '主动学习提升模型训练效率', 'desc': '本文提出了一种主动学习方法ActPRM，用于提高大语言模型（LLMs）的训练效率。通过选择最不确定的样本进行训练，ActPRM显著降低了标注成本。训练过程中，使用过程奖励模型（PRM）来估计不确定性，仅保留高度不确定的数据进行标注。实验结果表明，ActPRM在减少50%标注的同时，性能与传统微调方法相当甚至更好。'}}}, {'id': 'https://huggingface.co/papers/2504.11427', 'title': 'NormalCrafter: Learning Temporally Consistent Normals from Video\n  Diffusion Priors', 'url': 'https://huggingface.co/papers/2504.11427', 'abstract': 'Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.', 'score': 3, 'issue_id': 3259, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'f1c298d14d78b468', 'authors': ['Yanrui Bin', 'Wenbo Hu', 'Haoyuan Wang', 'Xinya Chen', 'Bing Wang'], 'affiliations': ['ARC Lab, Tencent PCG', 'City University of Hong Kong', 'Huazhong University of Science and Technology', 'Spatial Intelligence Group, The Hong Kong Polytechnic University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11427.jpg', 'data': {'categories': ['#cv', '#long_context', '#diffusion', '#video', '#training'], 'emoji': '🎥', 'ru': {'title': 'NormalCrafter: Временная согласованность нормалей в видео с помощью диффузионных моделей', 'desc': 'Эта статья представляет NormalCrafter - новый метод для оценки поверхностных нормалей в видео с использованием диффузионных моделей. Авторы предлагают технику семантической регуляризации признаков (SFR) для улучшения согласованности оценок во времени. Также описывается двухэтапный протокол обучения, сочетающий обучение в латентном и пиксельном пространствах. Эксперименты показывают превосходство метода в генерации темпорально согласованных последовательностей нормалей с сохранением деталей.'}, 'en': {'title': 'NormalCrafter: Enhancing Video Normal Estimation with Semantic Insights', 'desc': "This paper introduces NormalCrafter, a novel approach for estimating surface normals in video sequences. It addresses the challenge of maintaining temporal coherence, which is often overlooked in traditional static image methods. The authors propose Semantic Feature Regularization (SFR) to enhance the model's focus on the scene's semantics, improving the quality of normal estimation. Additionally, a two-stage training protocol is implemented to balance spatial accuracy and long-term temporal context, resulting in high-fidelity normal sequences across various videos."}, 'zh': {'title': '视频法线估计的新方法：NormalCrafter', 'desc': '本论文提出了一种新的方法NormalCrafter，用于视频中的法线估计。与传统方法不同，我们利用视频扩散模型的时间先验，确保法线估计在时间上的一致性。我们引入了语义特征正则化（SFR），通过对齐扩散特征和语义线索，帮助模型关注场景的内在语义。通过两阶段的训练协议，我们在保持空间精度的同时，增强了对长时间上下文的学习，实验结果表明该方法在生成细节丰富且时间一致的法线序列方面表现优越。'}}}, {'id': 'https://huggingface.co/papers/2504.11343', 'title': 'A Minimalist Approach to LLM Reasoning: from Rejection Sampling to\n  Reinforce', 'url': 'https://huggingface.co/papers/2504.11343', 'abstract': "Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.", 'score': 3, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '72714d765a5a497f', 'authors': ['Wei Xiong', 'Jiarui Yao', 'Yuhui Xu', 'Bo Pang', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Nan Jiang', 'Tong Zhang', 'Caiming Xiong', 'Hanze Dong'], 'affiliations': ['Salesforce AI Research', 'University of Illinois Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2504.11343.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#interpretability'], 'emoji': '🧠', 'ru': {'title': 'Простота и эффективность в обучении языковых моделей с подкреплением', 'desc': 'Это исследование анализирует методы обучения с подкреплением (RL) для улучшения больших языковых моделей (LLM) в задачах рассуждения. Авторы обнаружили, что простой метод отбора положительных примеров RAFT показывает результаты, сравнимые с более сложными алгоритмами, такими как GRPO. На основе этого наблюдения предложен новый алгоритм Reinforce-Rej, который фильтрует как полностью неправильные, так и полностью правильные образцы. Исследование предлагает использовать RAFT как надежный базовый метод и рекомендует сосредоточиться на более обоснованном включении отрицательных примеров в будущих разработках.'}, 'en': {'title': 'Simplifying Reinforcement Learning for Better Language Model Training', 'desc': "This paper explores the effectiveness of the GRPO method in reinforcement learning for fine-tuning large language models on reasoning tasks. The authors discover that a simpler method, RAFT, which only uses positively rewarded samples, performs comparably to GRPO and PPO. They find that GRPO's strength lies in its ability to discard prompts with completely incorrect responses, rather than its reward normalization technique. To enhance performance, they introduce Reinforce-Rej, which filters out both incorrect and correct samples, improving efficiency and stability in training."}, 'zh': {'title': '强化学习的新视角：拒绝采样的力量', 'desc': '强化学习（RL）在复杂推理任务中对大型语言模型（LLM）的微调中变得越来越重要。本文重新审视了GRPO算法，发现一个简单的拒绝采样基线RAFT在训练中表现出色，甚至与GRPO和PPO相当。研究表明，GRPO的主要优势在于丢弃完全错误的提示，而不是其奖励归一化。基于这一发现，我们提出了Reinforce-Rej，这是一种过滤完全错误和完全正确样本的策略梯度扩展，能够提高KL效率和稳定性。'}}}, {'id': 'https://huggingface.co/papers/2504.11447', 'title': 'Diffusion Distillation With Direct Preference Optimization For Efficient\n  3D LiDAR Scene Completion', 'url': 'https://huggingface.co/papers/2504.11447', 'abstract': "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.", 'score': 2, 'issue_id': 3258, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'cda8d39111df28d8', 'authors': ['An Zhaol', 'Shengyuan Zhang', 'Ling Yang', 'Zejian Li', 'Jiale Wu', 'Haoran Xu', 'AnYang Wei', 'Perry Pengyun GU Lingyun Sun'], 'affiliations': ['Peking University', 'Zhejiang Green Zhixing Technology co., ltd', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.11447.jpg', 'data': {'categories': ['#open_source', '#3d', '#rlhf', '#training', '#diffusion', '#optimization'], 'emoji': '🚗', 'ru': {'title': 'Ускоренное и улучшенное заполнение сцен LiDAR с помощью Distillation-DPO', 'desc': 'Эта статья представляет новый метод под названием Distillation-DPO для ускорения и улучшения качества заполнения сцен LiDAR с использованием диффузионных моделей. Метод сочетает дистилляцию знаний с оптимизацией прямой политики (DPO), используя метрики оценки сцен LiDAR в качестве предпочтений. Distillation-DPO оптимизирует модель ученика, используя разницу в функциях оценки между учителем и учеником на парных сценах завершения. Эксперименты показывают, что метод достигает более высокого качества заполнения сцены при ускорении процесса более чем в 5 раз по сравнению с современными моделями.'}, 'en': {'title': 'Accelerating 3D LiDAR Scene Completion with Distillation-DPO', 'desc': 'This paper introduces Distillation-DPO, a new framework that enhances 3D LiDAR scene completion using diffusion models. It combines score distillation with direct policy optimization (DPO) to improve performance while speeding up the sampling process. The method generates paired scene completions with varying initial noises and uses LiDAR evaluation metrics to create winning and losing pairs for optimization. The results show that Distillation-DPO significantly outperforms existing models in both quality and speed, marking a novel approach in preference-aligned distillation for LiDAR applications.'}, 'zh': {'title': '提升LiDAR场景补全速度与质量的创新方法', 'desc': '本论文提出了一种新的扩散蒸馏框架，称为Distillation-DPO，用于3D LiDAR场景补全。该方法通过偏好对齐来优化学生模型，首先生成不同初始噪声的配对补全场景。然后，利用LiDAR场景评估指标构建胜负样本对，以此来优化模型。实验表明，Distillation-DPO在场景补全质量和速度上均优于现有的最先进模型，补全速度提高了5倍以上。'}}}, {'id': 'https://huggingface.co/papers/2504.10188', 'title': 'Efficient Generative Model Training via Embedded Representation Warmup', 'url': 'https://huggingface.co/papers/2504.10188', 'abstract': "Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40times acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https://github.com/LINs-lab/ERW.", 'score': 2, 'issue_id': 3261, 'pub_date': '2025-04-14', 'pub_date_card': {'ru': '14 апреля', 'en': 'April 14', 'zh': '4月14日'}, 'hash': '280d2a5386c25fa2', 'authors': ['Deyuan Liu', 'Peng Sun', 'Xufeng Li', 'Tao Lin'], 'affiliations': ['Nanjing University', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2504.10188.jpg', 'data': {'categories': ['#optimization', '#diffusion', '#architecture', '#training'], 'emoji': '🚀', 'ru': {'title': 'Ускорение обучения диффузионных моделей с помощью предобученных представлений', 'desc': 'Статья представляет новый метод обучения диффузионных моделей под названием Embedded Representation Warmup (ERW). ERW использует предобученные высококачественные представления для инициализации ранних слоев модели, что значительно ускоряет сходимость и улучшает качество генерации. Авторы идентифицируют ключевую область обработки представлений в нейронной сети, где происходит обучение семантическим и структурным паттернам. Теоретический и эмпирический анализ показывает, что ERW ускоряет обучение в 40 раз по сравнению с современными методами.'}, 'en': {'title': 'Accelerating Diffusion Models with Embedded Representation Warmup', 'desc': 'This paper addresses the limitations of diffusion models in generating high-dimensional data efficiently. It identifies that the slow training process is due to the underutilization of high-quality representations in the early layers of the model. The authors propose a method called Embedded Representation Warmup (ERW), which initializes these layers with pretrained representations to enhance learning speed and quality. Their results show that ERW significantly accelerates training convergence and improves representation quality, achieving a 40 times faster training speed compared to existing methods.'}, 'zh': {'title': '加速扩散模型训练的嵌入表示预热', 'desc': '扩散模型在生成高维数据方面表现出色，但在训练效率和表示质量上不如自监督方法。我们发现一个关键瓶颈：在训练过程中未充分利用高质量、语义丰富的表示，显著减缓了收敛速度。为了解决这个问题，我们提出了嵌入表示预热（ERW）框架，通过在初始阶段使用预训练的高质量表示来初始化扩散模型的早期层，从而加速收敛并提高性能。我们的理论分析表明，ERW的有效性依赖于其在特定神经网络层的精确集成，这些层主要处理和转换特征表示以便后续生成。'}}}, {'id': 'https://huggingface.co/papers/2504.06949', 'title': 'Adaptive Computation Pruning for the Forgetting Transformer', 'url': 'https://huggingface.co/papers/2504.06949', 'abstract': 'The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.', 'score': 2, 'issue_id': 3259, 'pub_date': '2025-04-09', 'pub_date_card': {'ru': '9 апреля', 'en': 'April 9', 'zh': '4月9日'}, 'hash': 'bda352daa194f6f8', 'authors': ['Zhixuan Lin', 'Johan Obando-Ceron', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2504.06949.jpg', 'data': {'categories': ['#architecture', '#inference', '#training', '#optimization'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка вычислений: быстрее, но не хуже', 'desc': 'Статья представляет Adaptive Computation Pruning (ACP) - метод динамической обрезки вычислений для модели Forgetting Transformer (FoX). ACP использует порог обрезки, чтобы исключить незначительные веса внимания, что позволяет сократить количество операций с плавающей запятой на 70% без потери производительности. Это приводит к увеличению скорости обучения на 10-35%, особенно для длинных контекстов. Анализ показывает эффективность метода для различных размеров моделей и длин контекста.'}, 'en': {'title': 'Boosting Efficiency with Adaptive Computation Pruning in FoX', 'desc': "The paper introduces the Forgetting Transformer (FoX), which enhances softmax attention by integrating a forget gate, leading to improved performance over traditional RoPE-based Transformers. It observes that many attention heads in FoX forget information quickly, focusing more on local context. To address this, the authors propose Adaptive Computation Pruning (ACP), which dynamically reduces unnecessary computations based on the forget gate's influence. This method significantly decreases the number of floating-point operations (FLOPs) during language model pretraining, improving training speed by 10% to 35% without sacrificing model performance."}, 'zh': {'title': '自适应计算剪枝提升FoX效率', 'desc': '最近提出的遗忘变换器（FoX）在软最大注意力中引入了遗忘门，与标准的RoPE变换器相比，表现出更好的性能。FoX中的许多注意力头快速遗忘，使得它们在每个时间步的输出主要依赖于局部上下文。基于这一观察，我们提出了自适应计算剪枝（ACP）方法，动态剪除与输入输出依赖关系强烈衰减的计算。通过在FoX的语言模型预训练中应用ACP，我们实现了约70%的计算量减少，同时训练吞吐量提高了10%到35%。'}}}, {'id': 'https://huggingface.co/papers/2504.11326', 'title': 'PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild', 'url': 'https://huggingface.co/papers/2504.11326', 'abstract': 'This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.', 'score': 1, 'issue_id': 3260, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': 'abbfe1ceb2f9688a', 'authors': ['Henghui Ding', 'Chang Liu', 'Nikhila Ravi', 'Shuting He', 'Yunchao Wei', 'Song Bai', 'Philip Torr', 'Kehuan Song', 'Xinglin Xie', 'Kexin Zhang', 'Licheng Jiao', 'Lingling Li', 'Shuyuan Yang', 'Xuqiang Cao', 'Linnan Zhao', 'Jiaxuan Zhao', 'Fang Liu', 'Mengjiao Wang', 'Junpei Zhang', 'Xu Liu', 'Yuting Yang', 'Mengru Ma', 'Hao Fang', 'Runmin Cong', 'Xiankai Lu', 'Zhiyang Che', 'Wei Zhan', 'Tianming Liang', 'Haichao Jiang', 'Wei-Shi Zheng', 'Jian-Fang Hu', 'Haobo Yuan', 'Xiangtai Li', 'Tao Zhang', 'Lu Qi', 'Ming-Hsuan Yang'], 'affiliations': ['the Institute of Big Data, Fudan University, Shanghai, China'], 'pdf_title_img': 'assets/pdf/title_img/2504.11326.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#video'], 'emoji': '🎥', 'ru': {'title': 'Прорыв в сегментации видео: новые горизонты понимания сложных сцен', 'desc': 'Статья описывает результаты 4-го конкурса PVUW по пониманию видео на уровне пикселей, проведенного в рамках CVPR 2025. Конкурс включал два трека: MOSE для сегментации объектов в сложных сценах и MeViS для сегментации на основе движения и языка. Были представлены новые, более сложные наборы данных, лучше отражающие реальные сценарии. Анализ результатов дает ценную информацию о современном состоянии и тенденциях в области сложной сегментации видео.'}, 'en': {'title': 'Advancing Video Segmentation: Insights from the PVUW Challenge', 'desc': 'This paper reviews the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, which took place at CVPR 2025. It highlights two main tracks: MOSE for complex scene video object segmentation and MeViS for motion-guided, language-based video segmentation. The challenge introduced new datasets that are more representative of real-world video scenarios, pushing the boundaries of current segmentation techniques. The findings provide insights into the latest advancements and future directions in the field of video segmentation.'}, 'zh': {'title': '推动复杂视频分割的前沿挑战', 'desc': '本报告全面概述了2025年CVPR会议期间举行的第四届像素级视频理解挑战赛（PVUW）。挑战赛包括两个赛道：MOSE专注于复杂场景的视频物体分割，而MeViS则针对基于运动引导和语言的视频分割。两个赛道都引入了新的、更具挑战性的数据集，以更好地反映现实世界的场景。通过详细的评估和分析，该挑战赛为复杂视频分割的最新技术状态和新兴趋势提供了宝贵的见解。'}}}, {'id': 'https://huggingface.co/papers/2504.11001', 'title': 'ReZero: Enhancing LLM search ability by trying one-more-time', 'url': 'https://huggingface.co/papers/2504.11001', 'abstract': 'Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.', 'score': 0, 'issue_id': 3263, 'pub_date': '2025-04-15', 'pub_date_card': {'ru': '15 апреля', 'en': 'April 15', 'zh': '4月15日'}, 'hash': '9e93b5032c2a9d9c', 'authors': ['Alan Dao', 'Thinh Le'], 'affiliations': ['Menlo Research'], 'pdf_title_img': 'assets/pdf/title_img/2504.11001.jpg', 'data': {'categories': ['#optimization', '#training', '#reasoning', '#rl', '#rag'], 'emoji': '🔍', 'ru': {'title': 'Настойчивость окупается: ReZero повышает эффективность поиска информации с помощью LLM', 'desc': 'В статье представлен новый подход ReZero для улучшения поиска информации с помощью больших языковых моделей (LLM). Метод использует обучение с подкреплением, чтобы поощрять модель делать повторные попытки поиска после неудачного запроса. Это позволяет LLM исследовать альтернативные формулировки запросов, вместо того чтобы преждевременно останавливаться. ReZero показывает значительное улучшение точности с 25% до 46.88% по сравнению с базовым методом.'}, 'en': {'title': 'Persistence Pays Off: Enhancing LLMs with ReZero', 'desc': 'This paper presents ReZero, a new reinforcement learning framework designed to improve the performance of Retrieval-Augmented Generation (RAG) in Large Language Models (LLMs). Unlike traditional methods that focus on refining search queries or analyzing results, ReZero encourages LLMs to persist and retry after an unsuccessful search. By rewarding the act of retrying, it promotes exploration of alternative queries, leading to better outcomes. The results show a significant accuracy increase, demonstrating that persistence can enhance LLM effectiveness in challenging knowledge-intensive tasks.'}, 'zh': {'title': '重试搜索，提升模型鲁棒性', 'desc': 'Retrieval-Augmented Generation（RAG）通过增强大型语言模型（LLM）在知识密集型任务上的表现，但其效果依赖于初始搜索查询的质量。现有方法通常使用强化学习（RL），主要关注查询的制定或结果的推理，而没有明确鼓励在搜索失败后继续尝试。我们提出了ReZero（Retry-Zero），一种新的强化学习框架，直接奖励在初次搜索失败后重试查询的行为。这种方法显著提高了LLM的鲁棒性，在复杂的信息检索场景中表现出更好的准确性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi (1)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (5)', '#cv (1)', '#data (3)', '#dataset (4)', '#diffusion (3)', '#ethics', '#games (2)', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (2)', '#leakage', '#long_context (2)', '#low_resource', '#machine_translation', '#math (2)', '#multilingual', '#multimodal (3)', '#open_source (3)', '#optimization (10)', '#plp', '#rag (1)', '#reasoning (6)', '#rl (3)', '#rlhf (1)', '#robotics', '#science', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic', '#training (12)', '#transfer_learning', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-04-16 07:11',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-04-16 07:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-04-16 07:11')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    