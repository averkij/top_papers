
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 14 papers. December 19.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">19 декабря</span> | <span id="title-articles-count">14 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-18.html">⬅️ <span id="prev-date">18.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-20.html">➡️ <span id="next-date">20.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '19 декабря', 'en': 'December 19', 'zh': '12月19日'};
        let feedDateNext = {'ru': '20.12', 'en': '12/20', 'zh': '12月20日'};
        let feedDatePrev = {'ru': '18.12', 'en': '12/18', 'zh': '12月18日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.14161', 'title': 'TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks', 'url': 'https://huggingface.co/papers/2412.14161', 'abstract': "We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at helping to accelerate or even autonomously perform work-related tasks? The answer to this question has important implications for both industry looking to adopt AI into their workflows, and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper, we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that with the most competitive agent, 24% of the tasks can be completed autonomously. This paints a nuanced picture on task automation with LM agents -- in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems.", 'score': 19, 'issue_id': 1204, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '4efc2187cb10b78e', 'authors': ['Frank F. Xu', 'Yufan Song', 'Boxuan Li', 'Yuxuan Tang', 'Kritanjali Jain', 'Mengxue Bao', 'Zora Z. Wang', 'Xuhui Zhou', 'Zhitong Guo', 'Murong Cao', 'Mingyang Yang', 'Hao Yang Lu', 'Amaad Martin', 'Zhe Su', 'Leander Maben', 'Raj Mehta', 'Wayne Chi', 'Lawrence Jang', 'Yiqing Xie', 'Shuyan Zhou', 'Graham Neubig'], 'affiliations': ['Carnegie Mellon University', 'Duke University', 'Independent'], 'pdf_title_img': 'assets/pdf/title_img/2412.14161.jpg', 'data': {'categories': ['#optimization', '#agi', '#agents', '#science', '#benchmark'], 'emoji': '🤖', 'ru': {'title': 'ИИ-агенты в офисе: прогресс и ограничения в автоматизации рабочих задач', 'desc': 'Статья представляет новый бенчмарк TheAgentCompany для оценки ИИ-агентов в выполнении профессиональных задач в виртуальной среде, имитирующей небольшую компанию-разработчика. Исследователи тестируют агентов на основе языковых моделей (ЯМ) в различных задачах, включая веб-серфинг, программирование и коммуникацию. Результаты показывают, что лучшие агенты способны автономно выполнить 24% задач. Это демонстрирует, что ИИ-агенты могут автоматизировать простые задачи, но сложные долгосрочные задачи все еще остаются недоступными для текущих систем.'}, 'en': {'title': 'Evaluating AI Agents: The Future of Work Automation', 'desc': 'This paper introduces TheAgentCompany, a benchmark designed to evaluate the performance of AI agents in completing work-related tasks similar to those of a digital worker. The study assesses how well these agents, powered by large language models (LLMs), can autonomously perform tasks such as web browsing, coding, and communication within a simulated software company environment. The results indicate that while 24% of tasks can be completed autonomously by the most effective agent, more complex tasks remain challenging for current AI systems. This research highlights the potential and limitations of AI in automating workplace functions, providing insights for industries considering AI integration.'}, 'zh': {'title': 'AI代理助力工作任务自动化的探索', 'desc': '本文介绍了一个名为TheAgentCompany的基准测试，用于评估人工智能代理在执行真实工作任务中的表现。我们创建了一个模拟小型软件公司的环境，设计了多种任务，代理可以通过浏览网页、编写代码和与同事沟通来完成这些任务。测试结果显示，最先进的代理能够自主完成24%的任务，这表明在简单任务的自动化方面，当前的语言模型代理表现良好。尽管如此，对于更复杂的长期任务，现有系统仍然无法胜任。'}}}, {'id': 'https://huggingface.co/papers/2412.14173', 'title': 'AniDoc: Animation Creation Made Easier', 'url': 'https://huggingface.co/papers/2412.14173', 'abstract': 'The production of 2D animation follows an industry-standard workflow, encompassing four essential stages: character design, keyframe animation, in-betweening, and coloring. Our research focuses on reducing the labor costs in the above process by harnessing the potential of increasingly powerful generative AI. Using video diffusion models as the foundation, AniDoc emerges as a video line art colorization tool, which automatically converts sketch sequences into colored animations following the reference character specification. Our model exploits correspondence matching as an explicit guidance, yielding strong robustness to the variations (e.g., posture) between the reference character and each line art frame. In addition, our model could even automate the in-betweening process, such that users can easily create a temporally consistent animation by simply providing a character image as well as the start and end sketches. Our code is available at: https://yihao-meng.github.io/AniDoc_demo.', 'score': 12, 'issue_id': 1206, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '97d73274256841ce', 'authors': ['Yihao Meng', 'Hao Ouyang', 'Hanlin Wang', 'Qiuyu Wang', 'Wen Wang', 'Ka Leong Cheng', 'Zhiheng Liu', 'Yujun Shen', 'Huamin Qu'], 'affiliations': ['Ant Group', 'HKU', 'HKUST', 'NJU', 'ZJU'], 'pdf_title_img': 'assets/pdf/title_img/2412.14173.jpg', 'data': {'categories': ['#open_source', '#cv', '#multimodal', '#diffusion', '#video'], 'emoji': '🎨', 'ru': {'title': 'ИИ-помощник для автоматизации 2D-анимации', 'desc': 'Статья описывает AniDoc - инструмент для автоматической раскраски анимационных скетчей с помощью генеративного ИИ. Модель использует видео-диффузию и сопоставление соответствий для надежной работы при изменениях позы персонажа. AniDoc также может автоматизировать процесс промежуточной анимации, создавая последовательные кадры на основе начального и конечного скетчей. Инструмент призван снизить трудозатраты в стандартном процессе создания 2D-анимации.'}, 'en': {'title': 'Revolutionizing 2D Animation with AI Automation', 'desc': 'This paper presents AniDoc, a generative AI tool designed to streamline the 2D animation workflow by automating key processes. It utilizes video diffusion models to transform sketch sequences into fully colored animations based on specified character designs. The model employs correspondence matching to ensure consistency and robustness, even when there are variations in character posture. Additionally, AniDoc simplifies the in-betweening process, allowing users to create smooth animations by inputting just a character image and the start and end sketches.'}, 'zh': {'title': '利用生成AI简化二维动画制作', 'desc': '本研究旨在通过利用生成性人工智能来降低二维动画制作过程中的人工成本。我们提出的AniDoc工具基于视频扩散模型，能够自动将线稿序列转换为彩色动画，并遵循参考角色的规范。该模型通过对应匹配提供明确的指导，从而增强了对参考角色与每个线稿帧之间变化（如姿势）的鲁棒性。此外，AniDoc还可以自动化中间帧生成，使用户只需提供角色图像及起始和结束草图即可轻松创建时间一致的动画。'}}}, {'id': 'https://huggingface.co/papers/2412.14168', 'title': 'FashionComposer: Compositional Fashion Image Generation', 'url': 'https://huggingface.co/papers/2412.14168', 'abstract': 'We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model\'s robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an "asset library" and employ a reference UNet to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different "assets" with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.', 'score': 8, 'issue_id': 1205, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': 'd5f4c5d585e5e03c', 'authors': ['Sihui Ji', 'Yiyang Wang', 'Xi Chen', 'Xiaogang Xu', 'Hao Luo', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group', 'Hupan Lab', 'The University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.14168.jpg', 'data': {'categories': ['#cv', '#multimodal'], 'emoji': '👗', 'ru': {'title': 'Создание персонализированных модных образов с помощью ИИ', 'desc': 'FashionComposer - это инновационная система для генерации композиционных модных изображений. Она отличается высокой гибкостью, принимая мультимодальные входные данные, включая текстовые подсказки, параметрическую модель человека, изображения одежды и лица. Система использует универсальную архитектуру для обработки различных типов входных данных и масштабируемый набор данных для обучения. FashionComposer применяет специальную технику внимания (subject-binding attention) для корректного сопоставления элементов одежды с нужными частями генерируемого изображения.'}, 'en': {'title': 'Revolutionizing Fashion Image Generation with Flexibility and Personalization', 'desc': "FashionComposer is a novel framework designed for generating fashion images with high flexibility. It allows users to input various types of data, such as text prompts and images of garments and faces, enabling personalized fashion compositions. The model utilizes a unique 'asset library' to manage multiple reference images and employs subject-binding attention to integrate appearance features accurately. This approach not only enhances the model's compositional abilities but also opens up applications like virtual try-ons and human album generation."}, 'zh': {'title': '时尚图像生成的灵活解决方案', 'desc': 'FashionComposer 是一种用于生成组合时尚图像的模型。与之前的方法不同，FashionComposer 具有高度的灵活性，可以处理多种输入形式，如文本提示、参数化人模型、服装图像和面部图像。它能够个性化人类的外观、姿势和体型，并在一次生成中分配多件服装。通过构建一个通用框架和使用参考 UNet 提取外观特征，FashionComposer 实现了对多种参考图像的无缝处理，支持多种应用，如人类相册生成和虚拟试穿任务。'}}}, {'id': 'https://huggingface.co/papers/2412.12953', 'title': 'Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning', 'url': 'https://huggingface.co/papers/2412.12953', 'abstract': "Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior. As models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. Therefore, continuing with the current architectures will present a computational roadblock. To address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling through sparse experts and noise-conditioned routing, reducing both active parameters by 40% and inference costs by 90% via expert caching. Our architecture combines this efficient scaling with noise-conditioned self-attention mechanism, enabling more effective denoising across different noise levels. MoDE achieves state-of-the-art performance on 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01 on CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and Transformer Diffusion Policies by an average of 57% across 4 benchmarks, while using 90% fewer FLOPs and fewer active parameters compared to default Diffusion Transformer architectures. Furthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies. Code and demonstrations are available at https://mbreuss.github.io/MoDE_Diffusion_Policy/.", 'score': 7, 'issue_id': 1208, 'pub_date': '2024-12-17', 'pub_date_card': {'ru': '17 декабря', 'en': 'December 17', 'zh': '12月17日'}, 'hash': '041d2162abff3a8d', 'authors': ['Moritz Reuss', 'Jyothish Pari', 'Pulkit Agrawal', 'Rudolf Lioutikov'], 'affiliations': ['Department of Electrical Engineering and Computer Science (EECS), MIT, USA', 'Intuitive Robots Lab (IRL), Karlsruhe Institute of Technology, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2412.12953.jpg', 'data': {'categories': ['#diffusion', '#optimization', '#rl', '#benchmark', '#training', '#architecture'], 'emoji': '🤖', 'ru': {'title': 'Эффективное масштабирование имитационного обучения с помощью экспертов-шумоподавителей', 'desc': 'Статья представляет новую архитектуру Mixture-of-Denoising Experts (MoDE) для имитационного обучения. MoDE превосходит существующие диффузионные политики на основе трансформеров, обеспечивая эффективное масштабирование с помощью разреженных экспертов и маршрутизации, обусловленной шумом. Архитектура сочетает эффективное масштабирование с механизмом самовнимания, обусловленным шумом, что позволяет более эффективно выполнять шумоподавление на разных уровнях шума. MoDE достигает наилучших результатов на 134 задачах в четырех эталонных тестах имитационного обучения, значительно превосходя существующие модели при использовании меньшего количества вычислительных ресурсов.'}, 'en': {'title': 'Efficient Imitation Learning with MoDE: Less is More!', 'desc': 'This paper introduces Mixture-of-Denoising Experts (MoDE), a new policy for Imitation Learning that improves upon existing Transformer-based Diffusion Policies. MoDE is designed to be more computationally efficient by utilizing sparse experts and noise-conditioned routing, which reduces the number of active parameters by 40% and inference costs by 90%. The architecture employs a noise-conditioned self-attention mechanism, enhancing its ability to denoise across various noise levels. MoDE achieves superior performance on multiple benchmarks, outperforming previous models while significantly lowering computational demands.'}, 'zh': {'title': '混合去噪专家：高效模仿学习的新选择', 'desc': '扩散策略在模仿学习中得到了广泛应用，具有生成多模态和不连续行为的优点。随着模型规模的增大，计算需求也随之增加，导致当前架构面临计算瓶颈。为了解决这个问题，我们提出了混合去噪专家（MoDE）作为一种新型的模仿学习策略。MoDE在多个基准测试中表现出色，显著减少了参数和推理成本，同时提高了去噪效果。'}}}, {'id': 'https://huggingface.co/papers/2412.14123', 'title': 'AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities', 'url': 'https://huggingface.co/papers/2412.14123', 'abstract': 'Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and 11 distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned, we achieve better or near state-of-the-art results on the datasets of GeoPlex and 4 additional ones for 5 environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, and flood segmentation. The code and models are available at https://github.com/gastruc/AnySat.', 'score': 4, 'issue_id': 1209, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '80c0cd569824e1cd', 'authors': ['Guillaume Astruc', 'Nicolas Gonthier', 'Clement Mallet', 'Loic Landrieu'], 'affiliations': ['CNES, France', 'IGN, France', 'LASTIG, Univ Gustave Eiffel, IGN, ENSG, France', 'LIGM, Ecole Nationale des Ponts et Chaussees, IP Paris, Univ Gustave Eiffel, CNRS, France'], 'pdf_title_img': 'assets/pdf/title_img/2412.14123.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#training'], 'emoji': '🛰️', 'ru': {'title': 'Единая модель для разнородных спутниковых данных', 'desc': 'Статья представляет AnySat - мультимодальную модель, основанную на архитектуре JEPA и адаптивных пространственных энкодерах. Модель обучается на разнородных данных дистанционного зондирования Земли в режиме самообучения. Авторы создали набор данных GeoPlex из 5 мультимодальных датасетов с 11 различными сенсорами. После дообучения модель показывает высокие результаты на 5 задачах мониторинга окружающей среды.'}, 'en': {'title': 'AnySat: A Unified Model for Diverse Earth Observation Data', 'desc': "This paper introduces AnySat, a multimodal machine learning model designed to handle the diverse nature of Earth observation data, which varies in resolution, scale, and type. Unlike traditional models that require fixed input configurations, AnySat utilizes a joint embedding predictive architecture (JEPA) and resolution-adaptive spatial encoders to effectively learn from heterogeneous datasets in a self-supervised way. The authors present GeoPlex, a new collection of 5 multimodal datasets with 11 different sensors, to showcase the model's capabilities. After training on these datasets, AnySat demonstrates superior performance on various environmental monitoring tasks, achieving state-of-the-art results in several cases."}, 'zh': {'title': 'AnySat：应对地球观测数据多样性的统一模型', 'desc': '本论文提出了一种名为AnySat的多模态模型，旨在解决地球观测数据在分辨率、尺度和模态上的多样性问题。现有方法通常依赖固定的输入配置，限制了其实际应用。AnySat采用联合嵌入预测架构（JEPA）和分辨率自适应空间编码器，使得我们能够在高度异构的数据上以自监督的方式训练单一模型。通过GeoPlex数据集的实验，我们在多个环境监测任务上取得了更好的或接近最先进的结果。'}}}, {'id': 'https://huggingface.co/papers/2412.14015', 'title': 'Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation', 'url': 'https://huggingface.co/papers/2412.14015', 'abstract': 'Prompts play a critical role in unleashing the power of language and vision foundation models for specific tasks. For the first time, we introduce prompting into depth foundation models, creating a new paradigm for metric depth estimation termed Prompt Depth Anything. Specifically, we use a low-cost LiDAR as the prompt to guide the Depth Anything model for accurate metric depth output, achieving up to 4K resolution. Our approach centers on a concise prompt fusion design that integrates the LiDAR at multiple scales within the depth decoder. To address training challenges posed by limited datasets containing both LiDAR depth and precise GT depth, we propose a scalable data pipeline that includes synthetic data LiDAR simulation and real data pseudo GT depth generation. Our approach sets new state-of-the-arts on the ARKitScenes and ScanNet++ datasets and benefits downstream applications, including 3D reconstruction and generalized robotic grasping.', 'score': 4, 'issue_id': 1204, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '1d55ea1f2eb90ac0', 'authors': ['Haotong Lin', 'Sida Peng', 'Jingxiao Chen', 'Songyou Peng', 'Jiaming Sun', 'Minghuan Liu', 'Hujun Bao', 'Jiashi Feng', 'Xiaowei Zhou', 'Bingyi Kang'], 'affiliations': ['ByteDance Seed', 'ETH Zurich', 'Shanghai Jiao Tong University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.14015.jpg', 'data': {'categories': ['#robotics', '#optimization', '#synthetic', '#data', '#3d', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Подсказки LiDAR для точной оценки глубины изображения', 'desc': 'Статья представляет новый подход к оценке метрической глубины, названный Prompt Depth Anything. Авторы используют недорогой LiDAR в качестве подсказки для модели Depth Anything, чтобы получить точную метрическую глубину с разрешением до 4K. Ключевым элементом является компактный дизайн объединения подсказок, интегрирующий LiDAR на нескольких уровнях в декодере глубины. Для решения проблемы ограниченности обучающих данных предложен масштабируемый конвейер, включающий симуляцию синтетических данных LiDAR и генерацию псевдо-GT глубины для реальных данных.'}, 'en': {'title': 'Revolutionizing Depth Estimation with LiDAR Prompts', 'desc': 'This paper introduces a novel method called Prompt Depth Anything, which integrates prompting techniques into depth estimation models. By using low-cost LiDAR data as a guiding prompt, the model can produce accurate metric depth outputs at high resolutions, up to 4K. The authors present a unique prompt fusion design that effectively incorporates LiDAR information at various scales within the depth decoder. To overcome the challenges of limited training data, they propose a scalable pipeline that combines synthetic LiDAR simulations with real data to generate pseudo ground truth depth, achieving state-of-the-art results on benchmark datasets.'}, 'zh': {'title': '利用提示技术提升深度估计精度', 'desc': '本文介绍了一种新的深度估计方法，称为Prompt Depth Anything，首次将提示技术应用于深度基础模型。我们使用低成本的LiDAR作为提示，指导Depth Anything模型输出准确的度量深度，分辨率高达4K。该方法通过在深度解码器中多尺度融合LiDAR提示，解决了有限数据集带来的训练挑战。我们的研究在ARKitScenes和ScanNet++数据集上设定了新的最先进水平，并对3D重建和通用机器人抓取等下游应用产生了积极影响。'}}}, {'id': 'https://huggingface.co/papers/2412.13746', 'title': 'RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment', 'url': 'https://huggingface.co/papers/2412.13746', 'abstract': 'Despite the significant progress made by existing retrieval augmented language models (RALMs) in providing trustworthy responses and grounding in reliable sources, they often overlook effective alignment with human preferences. In the alignment process, reward models (RMs) act as a crucial proxy for human values to guide optimization. However, it remains unclear how to evaluate and select a reliable RM for preference alignment in RALMs. To this end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG settings. First, we design four crucial and challenging RAG-specific scenarios to assess RMs, including multi-hop reasoning, fine-grained citation, appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG subsets, six retrievers, and 24 RALMs to increase the diversity of data sources. Finally, we adopt an LLM-as-a-judge approach to improve preference annotation efficiency and effectiveness, exhibiting a strong correlation with human annotations. Based on the RAG-RewardBench, we conduct a comprehensive evaluation of 45 RMs and uncover their limitations in RAG scenarios. Additionally, we also reveal that existing trained RALMs show almost no improvement in preference alignment, highlighting the need for a shift towards preference-aligned training.We release our benchmark and code publicly at https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.', 'score': 4, 'issue_id': 1204, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '9159fbad2530d02c', 'authors': ['Zhuoran Jin', 'Hongbang Yuan', 'Tianyi Men', 'Pengfei Cao', 'Yubo Chen', 'Kang Liu', 'Jun Zhao'], 'affiliations': ['School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China', 'The Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.13746.jpg', 'data': {'categories': ['#optimization', '#rag', '#open_source', '#rlhf', '#benchmark', '#alignment'], 'emoji': '🎯', 'ru': {'title': 'RAG-RewardBench: новый стандарт для оценки моделей вознаграждения в RAG', 'desc': 'Статья представляет новый бенчмарк RAG-RewardBench для оценки моделей вознаграждения в контексте retrieval-augmented generation (RAG). Авторы разработали четыре сценария для тестирования моделей вознаграждения, включая многоходовые рассуждения и устойчивость к конфликтам. Бенчмарк использует 18 наборов данных RAG, 6 ретриверов и 24 модели RAG для увеличения разнообразия. Результаты показывают ограничения существующих моделей вознаграждения в сценариях RAG и отсутствие улучшений в согласовании предпочтений у обученных моделей RAG.'}, 'en': {'title': 'Enhancing Human Preference Alignment in Retrieval Augmented Language Models', 'desc': 'This paper introduces RAG-RewardBench, a benchmark designed to evaluate reward models (RMs) used in retrieval augmented language models (RALMs). The authors identify the challenge of aligning RMs with human preferences and propose four specific scenarios to test their effectiveness. They incorporate a diverse set of RAG subsets, retrievers, and RALMs to ensure comprehensive evaluation. The findings reveal significant limitations in current RMs and emphasize the necessity for training methods that prioritize preference alignment.'}, 'zh': {'title': '提升人类偏好的检索增强模型对齐', 'desc': '尽管现有的检索增强语言模型（RALMs）在提供可信响应和可靠来源方面取得了显著进展，但它们在与人类偏好的有效对齐上仍存在不足。在对齐过程中，奖励模型（RMs）作为人类价值观的重要代理，指导优化过程。然而，如何评估和选择可靠的RM以实现RALMs中的偏好对齐仍不明确。为此，我们提出了RAG-RewardBench，这是第一个用于评估RAG环境中RM的基准，设计了四个关键的RAG特定场景，并进行了全面评估。'}}}, {'id': 'https://huggingface.co/papers/2412.13501', 'title': 'GUI Agents: A Survey', 'url': 'https://huggingface.co/papers/2412.13501', 'abstract': 'Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.', 'score': 4, 'issue_id': 1204, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '93d87411c70f693d', 'authors': ['Dang Nguyen', 'Jian Chen', 'Yu Wang', 'Gang Wu', 'Namyong Park', 'Zhengmian Hu', 'Hanjia Lyu', 'Junda Wu', 'Ryan Aponte', 'Yu Xia', 'Xintong Li', 'Jing Shi', 'Hongjie Chen', 'Viet Dac Lai', 'Zhouhang Xie', 'Sungchul Kim', 'Ruiyi Zhang', 'Tong Yu', 'Mehrab Tanjim', 'Nesreen K. Ahmed', 'Puneet Mathur', 'Seunghyun Yoon', 'Lina Yao', 'Branislav Kveton', 'Thien Huu Nguyen', 'Trung Bui', 'Tianyi Zhou', 'Ryan A. Rossi', 'Franck Dernoncourt'], 'affiliations': ['Adobe Research', 'Carnegie Mellon University', 'Dolby Labs', 'Intel AI Research', 'Meta AI', 'State University of New York at Buffalo', 'University of California, San Diego', 'University of Maryland', 'University of New South Wales', 'University of Oregon', 'University of Rochester'], 'pdf_title_img': 'assets/pdf/title_img/2412.13501.jpg', 'data': {'categories': ['#architecture', '#agents', '#benchmark', '#reasoning', '#training', '#survey'], 'emoji': '🤖', 'ru': {'title': 'GUI-агенты: новая эра автоматизации взаимодействия человека с компьютером', 'desc': 'Статья посвящена агентам с графическим пользовательским интерфейсом (GUI), работающим на основе больших языковых моделей. Эти агенты автономно взаимодействуют с цифровыми системами, имитируя действия человека. В работе представлен обзор бенчмарков, метрик оценки, архитектур и методов обучения GUI-агентов. Авторы предлагают единую структуру, описывающую возможности агентов в восприятии, рассуждении, планировании и действии.'}, 'en': {'title': 'Empowering Automation: The Rise of GUI Agents with Large Models', 'desc': 'This paper surveys the development of GUI agents that utilize Large Foundation Models to automate interactions with software applications. It categorizes various aspects of these agents, including their benchmarks, evaluation metrics, architectures, and training methods. The authors propose a unified framework that outlines the key capabilities of perception, reasoning, planning, and acting in GUI agents. Additionally, the paper highlights open challenges and future directions for research in this area, providing a foundational understanding for both practitioners and researchers.'}, 'zh': {'title': 'GUI代理：人机交互的未来', 'desc': '图形用户界面（GUI）代理是由大型基础模型驱动的，能够自动化人机交互。这些代理可以自主与数字系统或软件应用程序进行交互，模拟人类的点击、输入和导航等操作。本文提供了一个全面的调查，分类了GUI代理的基准、评估指标、架构和训练方法，并提出了一个统一框架，描述了它们的感知、推理、规划和行动能力。我们还识别了重要的开放挑战，并讨论了未来的关键方向，为从业者和研究人员提供了对当前进展、技术、基准和待解决的关键问题的直观理解。'}}}, {'id': 'https://huggingface.co/papers/2412.12571', 'title': 'ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers', 'url': 'https://huggingface.co/papers/2412.12571', 'abstract': 'Recent research arXiv:2410.15027 arXiv:2410.23775 has highlighted the inherent in-context generation capabilities of pretrained diffusion transformers (DiTs), enabling them to seamlessly adapt to diverse visual tasks with minimal or no architectural modifications. These capabilities are unlocked by concatenating self-attention tokens across multiple input and target images, combined with grouped and masked generation pipelines. Building upon this foundation, we present ChatDiT, a zero-shot, general-purpose, and interactive visual generation framework that leverages pretrained diffusion transformers in their original form, requiring no additional tuning, adapters, or modifications. Users can interact with ChatDiT to create interleaved text-image articles, multi-page picture books, edit images, design IP derivatives, or develop character design settings, all through free-form natural language across one or more conversational rounds. At its core, ChatDiT employs a multi-agent system comprising three key components: an Instruction-Parsing agent that interprets user-uploaded images and instructions, a Strategy-Planning agent that devises single-step or multi-step generation actions, and an Execution agent that performs these actions using an in-context toolkit of diffusion transformers. We thoroughly evaluate ChatDiT on IDEA-Bench arXiv:2412.11767, comprising 100 real-world design tasks and 275 cases with diverse instructions and varying numbers of input and target images. Despite its simplicity and training-free approach, ChatDiT surpasses all competitors, including those specifically designed and trained on extensive multi-task datasets. We further identify key limitations of pretrained DiTs in zero-shot adapting to tasks. We release all code, agents, results, and intermediate outputs to facilitate further research at https://github.com/ali-vilab/ChatDiT', 'score': 3, 'issue_id': 1204, 'pub_date': '2024-12-17', 'pub_date_card': {'ru': '17 декабря', 'en': 'December 17', 'zh': '12月17日'}, 'hash': '8fa92ec2d65420c8', 'authors': ['Lianghua Huang', 'Wei Wang', 'Zhi-Fan Wu', 'Yupeng Shi', 'Chen Liang', 'Tong Shen', 'Han Zhang', 'Huanzhang Dou', 'Yu Liu', 'Jingren Zhou'], 'affiliations': ['Alibaba Inc.', 'Institute of Automation, Chinese Academy of Sciences', 'Shanghai Jiao Tong University', 'Taobao', 'Tongyi Lab', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.12571.jpg', 'data': {'categories': ['#open_source', '#cv', '#agents', '#benchmark', '#multimodal', '#diffusion'], 'emoji': '🎨', 'ru': {'title': 'ChatDiT: Универсальная визуальная генерация без дополнительного обучения', 'desc': 'Исследование представляет ChatDiT - интерактивную систему для визуальной генерации, использующую предобученные диффузионные трансформеры без дополнительной настройки. ChatDiT использует многоагентный подход, включающий агентов для анализа инструкций, планирования стратегии и выполнения действий. Система позволяет создавать и редактировать изображения, дизайн-проекты и другие визуальные материалы с помощью естественного языка. ChatDiT превзошел конкурентов в тестировании на IDEA-Bench, несмотря на простоту подхода без дополнительного обучения.'}, 'en': {'title': 'ChatDiT: Interactive Visual Generation with Zero-Tuning Diffusion Transformers', 'desc': 'This paper introduces ChatDiT, a novel framework for visual generation that utilizes pretrained diffusion transformers (DiTs) without requiring any additional tuning or modifications. ChatDiT allows users to create and edit images, design characters, and generate text-image articles through natural language interactions. It operates using a multi-agent system that includes agents for instruction parsing, strategy planning, and execution of generation tasks. The framework has been evaluated on a diverse set of design tasks and has shown superior performance compared to other specialized models, highlighting the potential of DiTs in zero-shot learning scenarios.'}, 'zh': {'title': 'ChatDiT：零-shot互动视觉生成的未来', 'desc': '最近的研究表明，预训练的扩散变换器（DiTs）具有内在的上下文生成能力，使其能够在不同的视觉任务中无缝适应，几乎不需要架构修改。这些能力通过在多个输入和目标图像之间连接自注意力标记，以及结合分组和掩蔽生成管道来实现。在此基础上，我们提出了ChatDiT，这是一个零-shot、通用且互动的视觉生成框架，利用预训练的扩散变换器，用户可以通过自然语言与ChatDiT互动，创建文本-图像文章、编辑图像等。ChatDiT的核心是一个多代理系统，包括指令解析代理、策略规划代理和执行代理，能够高效地完成用户的生成任务。'}}}, {'id': 'https://huggingface.co/papers/2412.13795', 'title': 'Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN', 'url': 'https://huggingface.co/papers/2412.13795', 'abstract': 'Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN.', 'score': 2, 'issue_id': 1210, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': 'fa12c7af8cd19039', 'authors': ['Pengxiang Li', 'Lu Yin', 'Shiwei Liu'], 'affiliations': ['Dalian University of Technology', 'Eindhoven University of Technology', 'University of Oxford', 'University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2412.13795.jpg', 'data': {'categories': ['#inference', '#architecture', '#training', '#rlhf', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Mix-LN: Новый метод нормализации для раскрытия потенциала глубоких слоев в LLM', 'desc': 'Исследователи обнаружили, что глубокие слои больших языковых моделей (LLM) часто неэффективны из-за использования Pre-Layer Normalization (Pre-LN). Они предложили новый метод нормализации Mix-LN, который сочетает преимущества Pre-LN и Post-LN в одной модели. Mix-LN применяет Post-LN к ранним слоям и Pre-LN к глубоким слоям, обеспечивая более равномерные градиенты по всей сети. Эксперименты показали, что Mix-LN превосходит как Pre-LN, так и Post-LN, улучшая качество предобучения LLM и их производительность при тонкой настройке и обучении с подкреплением.'}, 'en': {'title': 'Unlocking LLM Potential with Mix-LN: A Balanced Approach to Normalization', 'desc': 'This paper discusses the limitations of using Pre-Layer Normalization (Pre-LN) in Large Language Models (LLMs), which can lead to ineffective deeper layers that can be pruned without loss of performance. The authors propose a new normalization technique called Mix-LN, which combines Pre-LN and Post-Layer Normalization (Post-LN) to improve gradient flow throughout the model. By applying Post-LN to earlier layers and Pre-LN to deeper layers, Mix-LN ensures that all layers contribute effectively during training. Experiments show that models using Mix-LN outperform those using only Pre-LN or Post-LN, leading to better performance in both pre-training and fine-tuning tasks.'}, 'zh': {'title': 'Mix-LN：提升大型语言模型的深层效能', 'desc': '大型语言模型（LLMs）在性能上取得了显著成功，但研究发现其深层的贡献有限，可以进行剪枝而不影响整体表现。我们认为这是由于广泛使用的预层归一化（Pre-LN）导致的训练不足。我们提出了一种新的归一化技术Mix-LN，它结合了预层归一化和后层归一化的优点，确保网络各层的梯度更加均匀。通过大量实验，Mix-LN在不同规模的模型中表现优于传统的归一化方法，提升了模型的预训练质量。'}}}, {'id': 'https://huggingface.co/papers/2412.14172', 'title': 'Learning from Massive Human Videos for Universal Humanoid Pose Control', 'url': 'https://huggingface.co/papers/2412.14172', 'abstract': 'Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.', 'score': 2, 'issue_id': 1209, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': 'a4db8a734e02835b', 'authors': ['Jiageng Mao', 'Siheng Zhao', 'Siqi Song', 'Tianheng Shi', 'Junjie Ye', 'Mingtong Zhang', 'Haoran Geng', 'Jitendra Malik', 'Vitor Guizilini', 'Yue Wang'], 'affiliations': ['Toyota Research Institute', 'UC Berkeley', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2412.14172.jpg', 'data': {'categories': ['#transfer_learning', '#optimization', '#rl', '#robotics', '#dataset', '#training', '#data'], 'emoji': '🤖', 'ru': {'title': 'Обучение гуманоидных роботов на основе видео с людьми', 'desc': 'Статья представляет Humanoid-X - крупномасштабный набор данных, содержащий более 20 миллионов поз гуманоидных роботов с соответствующими текстовыми описаниями движений. Этот датасет создан с использованием комплексного подхода, включающего сбор данных из интернета, генерацию подписей к видео и перенос движений человека на гуманоидных роботов. На основе Humanoid-X авторы обучили большую модель UH-1, способную управлять гуманоидным роботом по текстовым инструкциям. Эксперименты показали, что этот масштабируемый подход к обучению приводит к лучшей генерализации в управлении гуманоидами на основе текста.'}, 'en': {'title': 'Unlocking Humanoid Robots with Text and Motion Data', 'desc': "This paper presents Humanoid-X, a large dataset containing over 20 million humanoid robot poses paired with text-based motion descriptions. The dataset is created by mining human videos from the internet, generating captions, and retargeting human motions to humanoid robots. The authors train a humanoid model, UH-1, which can interpret text instructions to control a humanoid robot's actions. The results show that this approach improves the robot's ability to generalize in real-world scenarios, making it more adaptable for practical applications."}, 'zh': {'title': '利用视频数据提升人形机器人控制能力', 'desc': '本论文介绍了Humanoid-X，这是一个包含超过2000万个人形机器人姿势及其对应文本描述的大规模数据集。该数据集旨在利用人类视频中丰富的语义和运动信息，以提高人形机器人的泛化能力。通过从互联网挖掘数据、生成视频字幕、将人类动作转移到人形机器人以及进行策略学习，Humanoid-X为机器人控制提供了新的训练方式。实验结果表明，使用Humanoid-X训练的模型在文本驱动的人形控制任务中表现出色，推动了人形机器人在现实世界应用中的适应性。'}}}, {'id': 'https://huggingface.co/papers/2412.13061', 'title': 'VidTok: A Versatile and Open-Source Video Tokenizer', 'url': 'https://huggingface.co/papers/2412.13061', 'abstract': 'Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.', 'score': 2, 'issue_id': 1204, 'pub_date': '2024-12-17', 'pub_date_card': {'ru': '17 декабря', 'en': 'December 17', 'zh': '12月17日'}, 'hash': '488c580621c13ba2', 'authors': ['Anni Tang', 'Tianyu He', 'Junliang Guo', 'Xinle Cheng', 'Li Song', 'Jiang Bian'], 'affiliations': ['Microsoft Research', 'Peking University', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2412.13061.jpg', 'data': {'categories': ['#architecture', '#optimization', '#video', '#open_source', '#training'], 'emoji': '🎥', 'ru': {'title': 'VidTok: Прорыв в токенизации видео для эффективного машинного обучения', 'desc': 'VidTok - это новый универсальный токенизатор видео, обеспечивающий современную производительность в непрерывной и дискретной токенизации. Он включает усовершенствования в архитектуре модели, использует конечную скалярную квантизацию (FSQ) для решения проблем обучения, связанных с векторной квантизацией, и применяет улучшенные стратегии обучения. VidTok демонстрирует превосходную производительность по нескольким метрикам, включая PSNR, SSIM, LPIPS и FVD, в стандартизированных условиях оценки.'}, 'en': {'title': 'VidTok: Revolutionizing Video Tokenization for Enhanced Performance', 'desc': 'This paper presents VidTok, a new video tokenizer designed to efficiently convert video content into compact latent tokens, which helps reduce redundancy in pixel data. VidTok stands out by utilizing advanced model architecture, including convolutional layers and up/downsampling modules, to enhance performance. It also addresses common issues in traditional Vector Quantization by implementing Finite Scalar Quantization, which stabilizes training and prevents codebook collapse. Overall, VidTok shows significant improvements in video tokenization performance, as evidenced by better scores in metrics like PSNR, SSIM, LPIPS, and FVD compared to existing methods.'}, 'zh': {'title': 'VidTok：视频标记化的新突破', 'desc': '本论文介绍了一种名为VidTok的视频编码器，它能够将视频内容压缩为紧凑的潜在标记。VidTok在连续和离散标记化方面都表现出色，解决了传统向量量化方法中的训练不稳定性和代码本崩溃问题。通过采用卷积层、上下采样模块以及有限标量量化（FSQ），VidTok显著提高了视频标记化的性能。该方法在多个评估指标上，如PSNR、SSIM、LPIPS和FVD，均表现优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2412.14042', 'title': 'CAD-Recode: Reverse Engineering CAD Code from Point Clouds', 'url': 'https://huggingface.co/papers/2412.14042', 'abstract': 'Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and dataset. In particular, we represent CAD sketch-extrude sequences as Python code. The proposed CAD-Recode translates a point cloud into Python code that, when executed, reconstructs the CAD model. Taking advantage of the exposure of pre-trained Large Language Models (LLMs) to Python code, we leverage a relatively small LLM as a decoder for CAD-Recode and combine it with a lightweight point cloud projector. CAD-Recode is trained solely on a proposed synthetic dataset of one million diverse CAD sequences. CAD-Recode significantly outperforms existing methods across three datasets while requiring fewer input points. Notably, it achieves 10 times lower mean Chamfer distance than state-of-the-art methods on DeepCAD and Fusion360 datasets. Furthermore, we show that our CAD Python code output is interpretable by off-the-shelf LLMs, enabling CAD editing and CAD-specific question answering from point clouds.', 'score': 1, 'issue_id': 1211, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '8004ab61c7a90dc9', 'authors': ['Danila Rukhovich', 'Elona Dupont', 'Dimitrios Mallis', 'Kseniya Cherenkova', 'Anis Kacem', 'Djamila Aouada'], 'affiliations': ['Artec3D, Luxembourg', 'SnT, University of Luxembourg'], 'pdf_title_img': 'assets/pdf/title_img/2412.14042.jpg', 'data': {'categories': ['#3d', '#training', '#interpretability', '#dataset', '#architecture', '#synthetic'], 'emoji': '🖥️', 'ru': {'title': 'От облака точек к Python-коду: революция в обратной разработке CAD', 'desc': 'CAD-Recode - это новый подход к обратной разработке 3D CAD-моделей из облаков точек. Он использует представление CAD-последовательностей в виде Python-кода и применяет предобученную языковую модель (LLM) в качестве декодера. Метод обучается на синтетическом наборе данных из миллиона разнообразных CAD-последовательностей. CAD-Recode значительно превосходит существующие методы по точности реконструкции на нескольких наборах данных, требуя при этом меньше входных точек.'}, 'en': {'title': 'Transforming Point Clouds into CAD Models with Python Code', 'desc': 'This paper presents a method called CAD-Recode for reverse engineering 3D CAD models from point clouds. It introduces a novel representation of CAD sequences as Python code, allowing for the reconstruction of models through code execution. The approach utilizes a lightweight Large Language Model (LLM) to decode the CAD sequences and is trained on a synthetic dataset of one million CAD sequences. The results show that CAD-Recode outperforms existing techniques, achieving significantly lower mean Chamfer distance and enabling interpretability for CAD editing and question answering.'}, 'zh': {'title': 'CAD反向工程的新突破', 'desc': '本文探讨了3D计算机辅助设计（CAD）反向工程的问题，旨在从点云重建CAD草图和操作序列。我们提出了一种名为CAD-Recode的方法，将CAD草图-挤出序列表示为Python代码，并通过该代码重建CAD模型。该方法利用预训练的大型语言模型（LLMs）作为解码器，并结合轻量级的点云投影器进行训练。实验结果表明，CAD-Recode在多个数据集上显著优于现有方法，并且在DeepCAD和Fusion360数据集上实现了更低的平均Chamfer距离。'}}}, {'id': 'https://huggingface.co/papers/2412.13670', 'title': 'AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge', 'url': 'https://huggingface.co/papers/2412.13670', 'abstract': "Data contamination hinders fair LLM evaluation by introducing test data into newer models' training sets. Existing studies solve this challenge by updating benchmarks with newly collected data. However, they fail to guarantee contamination-free evaluation as the newly collected data may contain pre-existing knowledge, and their benchmark updates rely on intensive human labor. To address these issues, we in this paper propose AntiLeak-Bench, an automated anti-leakage benchmarking framework. Instead of simply using newly collected data, we construct samples with explicitly new knowledge absent from LLMs' training sets, which thus ensures strictly contamination-free evaluation. We further design a fully automated workflow to build and update our benchmark without human labor. This significantly reduces the cost of benchmark maintenance to accommodate emerging LLMs. Through extensive experiments, we highlight that data contamination likely exists before LLMs' cutoff time and demonstrate AntiLeak-Bench effectively overcomes this challenge.", 'score': 1, 'issue_id': 1211, 'pub_date': '2024-12-18', 'pub_date_card': {'ru': '18 декабря', 'en': 'December 18', 'zh': '12月18日'}, 'hash': '928c379891b2f907', 'authors': ['Xiaobao Wu', 'Liangming Pan', 'Yuxi Xie', 'Ruiwen Zhou', 'Shuai Zhao', 'Yubo Ma', 'Mingzhe Du', 'Rui Mao', 'Anh Tuan Luu', 'William Yang Wang'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Shanghai Jiao Tong University', 'University of Arizona', 'University of California, Santa Barbara'], 'pdf_title_img': 'assets/pdf/title_img/2412.13670.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#leakage'], 'emoji': '🧼', 'ru': {'title': 'Чистая оценка ЯМ: автоматизированные тесты без загрязнения данными', 'desc': 'Эта статья предлагает новый подход к оценке языковых моделей (ЯМ) без загрязнения данными. Авторы представляют AntiLeak-Bench - автоматизированную систему создания тестовых наборов с новыми знаниями, отсутствующими в обучающих данных ЯМ. Это обеспечивает строгую оценку без загрязнения и значительно снижает затраты на поддержание актуальности тестов. Эксперименты показывают, что загрязнение данных может существовать еще до даты отсечения обучающих данных ЯМ, и AntiLeak-Bench эффективно решает эту проблему.'}, 'en': {'title': 'Ensuring Fair Evaluation with AntiLeak-Bench', 'desc': "This paper addresses the problem of data contamination in evaluating large language models (LLMs), which occurs when test data is inadvertently included in the training sets of newer models. The authors introduce AntiLeak-Bench, a novel benchmarking framework that ensures evaluations are free from contamination by constructing samples that contain knowledge not present in the LLMs' training data. This framework automates the process of building and updating benchmarks, significantly reducing the need for intensive human labor. The experiments conducted show that data contamination can exist even before the cutoff time of LLMs, and AntiLeak-Bench effectively mitigates this issue."}, 'zh': {'title': '反泄漏基准：确保公平评估的自动化解决方案', 'desc': '数据污染会影响大型语言模型（LLM）的公平评估，因为测试数据可能被引入到新模型的训练集中。现有研究通过更新基准测试来解决这个问题，但无法保证评估不受污染，因为新收集的数据可能包含已有知识。为了解决这些问题，我们提出了AntiLeak-Bench，这是一个自动化的反泄漏基准框架。该框架通过构建缺乏LLM训练集中显式新知识的样本，确保了严格的不受污染评估，并设计了一个完全自动化的工作流程来维护基准，显著降低了维护成本。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (3)', '#agi (1)', '#alignment (1)', '#architecture (5)', '#audio', '#benchmark (6)', '#cv (3)', '#data (2)', '#dataset (5)', '#diffusion (3)', '#ethics', '#games', '#graphs', '#hallucinations', '#healthcare', '#inference (1)', '#interpretability (1)', '#leakage (1)', '#long_context', '#low_resource', '#machine_translation', '#math', '#multilingual', '#multimodal (4)', '#open_source (4)', '#optimization (7)', '#plp', '#rag (1)', '#reasoning (1)', '#rl (2)', '#rlhf (2)', '#robotics (2)', '#science (1)', '#security', '#small_models', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (7)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2024-12-19 11:09',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-19 11:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-19 11:09')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    