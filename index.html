
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 20 papers. June 5.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">5 июня</span> | <span id="title-articles-count">20 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-04.html">⬅️ <span id="prev-date">04.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-06.html">➡️ <span id="next-date">06.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '5 июня', 'en': 'June 5', 'zh': '6月5日'};
        let feedDateNext = {'ru': '06.06', 'en': '06/06', 'zh': '6月6日'};
        let feedDatePrev = {'ru': '04.06', 'en': '06/04', 'zh': '6月4日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.03569', 'title': 'MiMo-VL Technical Report', 'url': 'https://huggingface.co/papers/2506.03569', 'abstract': 'We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.', 'score': 26, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'cb568276c7e799cb', 'authors': ['Xiaomi LLM-Core Team', ':', 'Zihao Yue', 'Zhenru Lin', 'Yifan Song', 'Weikun Wang', 'Shuhuai Ren', 'Shuhao Gu', 'Shicheng Li', 'Peidian Li', 'Liang Zhao', 'Lei Li', 'Kainan Bao', 'Hao Tian', 'Hailin Zhang', 'Gang Wang', 'Dawei Zhu', 'Cici', 'Chenhong He', 'Bowen Ye', 'Bowen Shen', 'Zihan Zhang', 'Zihan Jiang', 'Zhixian Zheng', 'Zhichao Song', 'Zhenbo Luo', 'Yue Yu', 'Yudong Wang', 'Yuanyuan Tian', 'Yu Tu', 'Yihan Yan', 'Yi Huang', 'Xu Wang', 'Xinzhe Xu', 'Xingchen Song', 'Xing Zhang', 'Xing Yong', 'Xin Zhang', 'Xiangwei Deng', 'Wenyu Yang', 'Wenhan Ma', 'Weiwei Lv', 'Weiji Zhuang', 'Wei Liu', 'Sirui Deng', 'Shuo Liu', 'Shimao Chen', 'Shihua Yu', 'Shaohui Liu', 'Shande Wang', 'Rui Ma', 'Qiantong Wang', 'Peng Wang', 'Nuo Chen', 'Menghang Zhu', 'Kangyang Zhou', 'Kang Zhou', 'Kai Fang', 'Jun Shi', 'Jinhao Dong', 'Jiebao Xiao', 'Jiaming Xu', 'Huaqiu Liu', 'Hongshen Xu', 'Heng Qu', 'Haochen Zhao', 'Hanglong Lv', 'Guoan Wang', 'Duo Zhang', 'Dong Zhang', 'Di Zhang', 'Chong Ma', 'Chang Liu', 'Can Cai', 'Bingquan Xia'], 'affiliations': ['Xiaomi'], 'pdf_title_img': 'assets/pdf/title_img/2506.03569.jpg', 'data': {'categories': ['#training', '#rl', '#reasoning', '#multimodal', '#rlhf', '#benchmark', '#dataset', '#open_source'], 'emoji': '🧠', 'ru': {'title': 'Прорыв в мультимодальном ИИ: MiMo-VL устанавливает новые стандарты', 'desc': 'Исследователи представили две мощные мультимодальные модели MiMo-VL-7B-SFT и MiMo-VL-7B-RL, демонстрирующие передовые результаты в задачах визуального понимания и мультимодальных рассуждений. Модель MiMo-VL-7B-RL превосходит Qwen2.5-VL-7B в 35 из 40 оцениваемых задач и достигает 59.4 баллов на бенчмарке OlympiadBench. Обучение моделей включало четырехэтапное предобучение на 2.4 триллионах токенов и применение смешанного обучения с подкреплением (MORL). Авторы подчеркивают важность включения качественных данных для рассуждений с длинной цепочкой мыслей в этапы предобучения.'}, 'en': {'title': 'Revolutionizing Vision-Language Models with MiMo-VL', 'desc': 'The paper introduces two advanced vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, which excel in visual understanding and multimodal reasoning tasks. MiMo-VL-7B-RL demonstrates superior performance, outperforming other models on a majority of evaluated tasks and achieving high scores on benchmark datasets. The training methodology involves a four-stage pre-training process using a massive dataset and incorporates Mixed On-policy Reinforcement Learning to enhance model performance through diverse reward signals. Additionally, the authors emphasize the significance of high-quality reasoning data and provide a comprehensive evaluation suite to facilitate reproducibility in future research.'}, 'zh': {'title': '开创视觉-语言模型的新标准', 'desc': '我们开源了MiMo-VL-7B-SFT和MiMo-VL-7B-RL，这两个强大的视觉-语言模型在一般视觉理解和多模态推理方面表现出色。MiMo-VL-7B-RL在40个评估任务中有35个超越了Qwen2.5-VL-7B，并在OlympiadBench上得分59.4，超过了参数高达78B的模型。在GUI定位应用中，它在OSWorld-G上以56.1的分数设定了新标准，甚至超越了专门模型UI-TARS。我们的训练结合了四阶段的预训练（24万亿个标记）和混合在线强化学习（MORL），并强调了在预训练阶段融入高质量推理数据和长链思维的重要性。'}}}, {'id': 'https://huggingface.co/papers/2506.02921', 'title': 'A Controllable Examination for Long-Context Language Models', 'url': 'https://huggingface.co/papers/2506.02921', 'abstract': 'LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.  \t\t\t\t\tAI-generated summary \t\t\t\t Existing frameworks for evaluating long-context language models (LCLM) can be broadly categorized into real-world and synthetic tasks. Despite their utility, both approaches are accompanied by certain intrinsic limitations. Real-world tasks are too complex to interpret or characterize and are susceptible to data contamination. In contrast, synthetic tasks often adopt the needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the "needle" and the "haystack" compromises their validity as proxies for realistic applications. In response to these challenges, we posit that an ideal long-context evaluation framework should be characterized by three essential features: seamless context, controllable setting, and sound evaluation. This study introduces LongBioBench, a novel benchmark that utilizes artificially generated biographies as a controlled environment for assessing LCLMs across dimensions of understanding, reasoning, and trustworthiness. Our experimental evaluation, which includes 18 LCLMs in total, demonstrates that most models still exhibit deficiencies in semantic understanding and elementary reasoning over retrieved results and are less trustworthy as context length increases. Our further analysis indicates some design choices employed by existing synthetic benchmarks, such as contextual non-coherence, numerical needles, and the absence of distractors, rendering them vulnerable to test the model long-context capabilities. Moreover, we also reveal that long-context continual pretraining primarily adjusts RoPE embedding to accommodate extended context lengths. To sum up, compared to previous synthetic benchmarks, LongBioBench achieves a better trade-off between mirroring authentic language tasks and maintaining controllability, and is highly interpretable and configurable.', 'score': 18, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '073ae66fedf9c141', 'authors': ['Yijun Yang', 'Zeyu Huang', 'Wenhao Zhu', 'Zihan Qiu', 'Fei Yuan', 'Jeff Z. Pan', 'Ivan Titov'], 'affiliations': ['Nanjing University', 'Qwen Team, Alibaba Group', 'Shanghai Artificial Intelligence Laboratory', 'University of Amsterdam', 'University of Edinburgh'], 'pdf_title_img': 'assets/pdf/title_img/2506.02921.jpg', 'data': {'categories': ['#benchmark', '#synthetic', '#long_context', '#reasoning', '#interpretability'], 'emoji': '📊', 'ru': {'title': 'LongBioBench: Новый стандарт оценки языковых моделей с длинным контекстом', 'desc': 'LongBioBench - это новый бенчмарк для оценки языковых моделей с длинным контекстом, использующий искусственно сгенерированные биографии. Он оценивает модели по трем аспектам: понимание, рассуждение и надежность. Бенчмарк создан для преодоления ограничений существующих методов оценки, таких как сложность интерпретации реальных задач и недостаток когерентности в синтетических тестах. Эксперименты показали, что большинство моделей все еще имеют проблемы с семантическим пониманием и элементарными рассуждениями, а также становятся менее надежными при увеличении длины контекста.'}, 'en': {'title': 'LongBioBench: A New Standard for Evaluating Long-Context Language Models', 'desc': 'LongBioBench is a new benchmark designed to evaluate long-context language models (LCLMs) using artificially generated biographies. It addresses the limitations of existing evaluation frameworks by providing a controlled environment that emphasizes understanding, reasoning, and trustworthiness. The study reveals that many LCLMs struggle with semantic understanding and reasoning as context length increases, highlighting the need for better evaluation methods. LongBioBench offers a more coherent and interpretable approach compared to previous synthetic benchmarks, making it a valuable tool for assessing LCLMs.'}, 'zh': {'title': 'LongBioBench：评估长上下文语言模型的新基准', 'desc': 'LongBioBench 是一个新的基准，利用人工生成的传记来评估长上下文语言模型（LCLM）在理解、推理和可信度方面的表现，解决了现有框架的局限性。现有的评估框架分为真实世界任务和合成任务，但两者都有内在的缺陷。真实世界任务复杂且易受数据污染，而合成任务常常缺乏连贯性，影响其作为现实应用的有效性。LongBioBench 提供了一个受控环境，能够更好地评估 LCLM 的能力，实验结果显示大多数模型在语义理解和基本推理上仍存在不足。'}}}, {'id': 'https://huggingface.co/papers/2506.04180', 'title': 'SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models', 'url': 'https://huggingface.co/papers/2506.04180', 'abstract': 'Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.', 'score': 16, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '3f52b337c5fa3683', 'authors': ['Yuhao Wu', 'Yushi Bai', 'Zhiqiang Hu', 'Juanzi Li', 'Roy Ka-Wei Lee'], 'affiliations': ['Singapore University of Technology and Design, Singapore', 'Tsinghua University, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2506.04180.jpg', 'data': {'categories': ['#training', '#optimization', '#agents', '#long_context', '#rlhf', '#benchmark', '#dataset', '#story_generation'], 'emoji': '✍️', 'ru': {'title': 'Структурированное мышление для улучшения генерации длинных текстов', 'desc': 'SuperWriter-Agent - это новая система для улучшения качества генерации длинных текстов с помощью больших языковых моделей (LLM). Она вводит этапы планирования и уточнения в процесс генерации, имитируя подход профессионального писателя. Авторы обучили 7B-параметровую модель SuperWriter-LM на специально созданном наборе данных и разработали иерархическую процедуру оптимизации предпочтений (DPO) с использованием метода Монте-Карло. Эмпирические результаты показывают, что SuperWriter-LM превосходит более крупные базовые модели по автоматическим и человеческим оценкам.'}, 'en': {'title': 'Elevating Long-Form Text Generation with Structured Thinking', 'desc': 'This paper presents SuperWriter-Agent, a novel framework aimed at improving long-form text generation by large language models (LLMs). It introduces structured thinking through planning and refinement stages, which helps the model generate more coherent and logically consistent text. The framework is supported by a supervised fine-tuning dataset for training a 7B parameter model called SuperWriter-LM. Additionally, a hierarchical Direct Preference Optimization (DPO) method is employed, utilizing Monte Carlo Tree Search to enhance the quality of generated text, leading to superior performance on various benchmarks.'}, 'zh': {'title': '提升长文本生成质量的智能代理', 'desc': '长文本生成是大型语言模型（LLMs）面临的重要挑战，尤其是在保持连贯性、逻辑一致性和文本质量方面。为了解决这些问题，我们提出了SuperWriter-Agent，这是一个基于代理的框架，旨在提高长文本生成的质量和一致性。该框架通过规划和精炼阶段引入明确的结构化思维，指导模型遵循更有意识和认知基础的过程，类似于专业作家的写作方式。实验结果表明，SuperWriter-LM在多个基准测试中表现出色，超越了更大规模的基线模型，证明了分层直接偏好优化（DPO）和结构化思维步骤的有效性。'}}}, {'id': 'https://huggingface.co/papers/2505.24500', 'title': "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence", 'url': 'https://huggingface.co/papers/2505.24500', 'abstract': "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.", 'score': 10, 'issue_id': 4133, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ee580986393d0b7e', 'authors': ['Guiyang Hou', 'Xing Gao', 'Yuchuan Wu', 'Xiang Huang', 'Wenqi Zhang', 'Zhe Zheng', 'Yongliang Shen', 'Jialu Du', 'Fei Huang', 'Yongbin Li', 'Weiming Lu'], 'affiliations': ['Nanjing University', 'Tongyi Lab, Alibaba Group', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24500.jpg', 'data': {'categories': ['#rlhf', '#rl', '#reasoning', '#training'], 'emoji': '🧠', 'ru': {'title': 'Новый метод обучения для повышения социального интеллекта языковых моделей', 'desc': 'Исследователи представили метод Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) для улучшения социального интеллекта больших языковых моделей (LLM). Этот подход учитывает временную составляющую и иерархию когнитивных процессов, характерных для социальных взаимодействий. Метод TimeHC-RL показал превосходство над широко используемым методом обучения с подкреплением System 2 RL. Эксперименты продемонстрировали, что применение TimeHC-RL позволяет моделям с 7 миллиардами параметров достигать производительности передовых моделей в задачах социального интеллекта.'}, 'en': {'title': "Boosting LLMs' Social Intelligence with TimeHC-RL", 'desc': "This paper presents a new approach called Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) to improve the social intelligence of Large Language Models (LLMs). Unlike traditional methods that focus on logical reasoning, TimeHC-RL incorporates different cognitive processes, including intuitive and deliberate thinking, to better navigate social contexts. The authors conducted experiments across various datasets and compared TimeHC-RL with existing reinforcement learning methods, demonstrating its superior performance. The findings suggest that enhancing LLMs' cognitive abilities in social domains can significantly elevate their overall intelligence and effectiveness."}, 'zh': {'title': '提升社交智能的时间感知强化学习', 'desc': '本文提出了一种新的方法，称为时间感知层次认知强化学习（TimeHC-RL），旨在提升大型语言模型（LLMs）在社交领域的智能。与数学等依赖系统2认知的领域不同，社交领域需要更丰富的认知模式，包括直觉反应和表层思维。通过对八个不同数据集的实验，我们验证了TimeHC-RL方法的有效性，结果显示其在社交智能方面优于传统的系统2强化学习方法。该方法使得7B基础模型的表现接近于更先进的模型，如DeepSeek-R1和OpenAI-O3。'}}}, {'id': 'https://huggingface.co/papers/2506.04228', 'title': 'LayerFlow: A Unified Model for Layer-aware Video Generation', 'url': 'https://huggingface.co/papers/2506.04228', 'abstract': 'LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.  \t\t\t\t\tAI-generated summary \t\t\t\t We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos for different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.', 'score': 9, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '1e8f6532d8b54b21', 'authors': ['Sihui Ji', 'Hao Luo', 'Xi Chen', 'Yuanpeng Tu', 'Yiyang Wang', 'Hengshuang Zhao'], 'affiliations': ['DAMO Academy, Alibaba Group, China', 'Hupan Laboratory, China', 'The University of Hong Kong', 'The University of Hong Kong, Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2506.04228.jpg', 'data': {'categories': ['#multimodal', '#diffusion', '#synthetic', '#video', '#training'], 'emoji': '🎞️', 'ru': {'title': 'LayerFlow: Умная генерация многослойного видео по текстовым подсказкам', 'desc': 'LayerFlow - это унифицированная система для генерации видео с учетом слоев, использующая трансформер диффузии для преобразования текста в видео и встраивания слоев. Она поддерживает различные задачи генерации видео, включая создание прозрачного переднего плана, чистого фона и смешанной сцены. Система использует многоэтапную стратегию обучения для адаптации к статическим изображениям с высококачественными аннотациями слоев. LayerFlow применяет LoRA для настройки движения и содержания, что позволяет генерировать плавные видео с желаемыми слоями.'}, 'en': {'title': 'LayerFlow: Unified Layer-Aware Video Generation', 'desc': 'LayerFlow is a comprehensive framework designed for generating videos that are aware of different layers, such as foreground and background. It utilizes a text-to-video diffusion transformer to create videos based on specific prompts for each layer, allowing for various video generation tasks. The framework can decompose blended videos or generate backgrounds for given foregrounds, making it versatile. To address the challenge of limited high-quality training data, LayerFlow employs a multi-stage training strategy that begins with low-quality videos and progressively incorporates high-quality layered images.'}, 'zh': {'title': 'LayerFlow：统一的层感知视频生成框架', 'desc': 'LayerFlow是一个统一的框架，用于生成层感知的视频，利用文本到视频的扩散变换器和层嵌入。该框架支持多种视频生成任务，包括透明前景、干净背景和混合场景的视频生成。通过将视频按层组织为子剪辑，并利用层嵌入来区分每个剪辑及其对应的层级提示，LayerFlow实现了多种视频生成变体。为了克服高质量层级训练视频的缺乏，LayerFlow设计了多阶段训练策略，结合静态图像和高质量层注释进行训练。'}}}, {'id': 'https://huggingface.co/papers/2506.04158', 'title': 'Image Editing As Programs with Diffusion Models', 'url': 'https://huggingface.co/papers/2506.04158', 'abstract': 'While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.', 'score': 5, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'e5a32d484bb427f3', 'authors': ['Yujia Hu', 'Songhua Liu', 'Zhenxiong Tan', 'Xingyi Yang', 'Xinchao Wang'], 'affiliations': ['National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2506.04158.jpg', 'data': {'categories': ['#diffusion', '#benchmark', '#cv', '#architecture'], 'emoji': '🖼️', 'ru': {'title': 'Редактирование изображений как программирование: новый подход к ИИ-обработке визуального контента', 'desc': 'Исследователи представили новый подход к редактированию изображений с использованием искусственного интеллекта, названный IEAP (Image Editing As Programs). Эта система основана на архитектуре Diffusion Transformer и разбивает сложные инструкции по редактированию на последовательность простых операций. Каждая операция реализуется с помощью специализированного адаптера, использующего общий базовый DiT. IEAP значительно превосходит современные методы в различных задачах редактирования, особенно при сложных многоэтапных инструкциях.'}, 'en': {'title': 'Revolutionizing Image Editing with Programmatic Precision', 'desc': 'This paper addresses the limitations of diffusion models in instruction-driven image editing, particularly when it comes to making significant layout changes. The authors propose a new framework called Image Editing As Programs (IEAP), which utilizes the Diffusion Transformer (DiT) architecture to break down complex editing tasks into simpler, atomic operations. Each operation is executed by a lightweight adapter that specializes in a specific type of edit, allowing for more flexible and accurate transformations. The framework shows improved performance over existing methods, achieving higher accuracy and semantic fidelity in various editing scenarios, especially for complex instructions.'}, 'zh': {'title': '图像编辑的新方法：将复杂指令转化为简单操作', 'desc': '本研究提出了一种新的图像编辑框架，称为图像编辑作为程序（IEAP），旨在解决扩散模型在指令驱动的图像编辑中面临的挑战。IEAP基于扩散变换器（DiT）架构，通过将复杂的编辑指令分解为一系列原子操作来实现。每个操作由轻量级适配器实现，专门针对特定类型的编辑，能够支持任意和结构不一致的变换。实验结果表明，IEAP在各种编辑场景中显著优于现有的最先进方法，尤其在处理复杂的多步骤指令时表现出更高的准确性和语义保真度。'}}}, {'id': 'https://huggingface.co/papers/2506.03295', 'title': 'Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem', 'url': 'https://huggingface.co/papers/2506.03295', 'abstract': "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.  \t\t\t\t\tAI-generated summary \t\t\t\t We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.", 'score': 5, 'issue_id': 4135, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '03df39687a4bdf5a', 'authors': ['Yubo Wang', 'Ping Nie', 'Kai Zou', 'Lijun Wu', 'Wenhu Chen'], 'affiliations': ['Independent', 'Netmind.AI', 'Shanghai AI Lab', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2506.03295.jpg', 'data': {'categories': ['#training', '#rl', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Эффективное раскрытие потенциала ИИ через обучение на критике', 'desc': 'Статья представляет метод Critique Fine-Tuning (CFT) для улучшения способностей рассуждения больших языковых моделей (LLM). CFT использует обучение на критических отзывах о решениях одной задачи, генерируемых моделью-учителем. Эксперименты показывают, что CFT значительно повышает производительность моделей на различных задачах рассуждения при меньших вычислительных затратах по сравнению с обучением с подкреплением. Результаты демонстрируют эффективность CFT как простого и общего подхода к раскрытию потенциала современных LLM.'}, 'en': {'title': 'Unlocking Reasoning Power with Efficient Critique Fine-Tuning', 'desc': 'This paper introduces Critique Fine-Tuning (CFT) as a method to enhance the reasoning abilities of large language models (LLMs) like Qwen and Llama. By focusing on a single problem, CFT generates critique data from various model-generated solutions, which are then used to fine-tune the models. The results show that this approach leads to significant performance improvements on reasoning tasks with much lower computational costs compared to traditional reinforcement learning methods. The findings suggest that CFT is a robust and efficient strategy for maximizing the reasoning potential of LLMs.'}, 'zh': {'title': '批评微调：高效释放语言模型推理潜力的利器', 'desc': '本文提出了一种名为批评微调（Critique Fine-Tuning, CFT）的方法，旨在高效提升大型语言模型（LLM）的推理能力。通过对单一问题进行微调，CFT能够显著提高模型在推理任务上的表现，同时减少计算成本。研究表明，使用CFT方法，模型在多个推理基准测试中平均提升了15%到16%。与传统的强化学习方法相比，CFT在计算资源上更加高效，展示了其作为一种简单且通用的推理能力提升策略的潜力。'}}}, {'id': 'https://huggingface.co/papers/2506.02592', 'title': 'Beyond the Surface: Measuring Self-Preference in LLM Judgments', 'url': 'https://huggingface.co/papers/2506.02592', 'abstract': 'The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.', 'score': 5, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'ccdb2761a23fe0c8', 'authors': ['Zhi-Yuan Chen', 'Hao Wang', 'Xinyu Zhang', 'Enrui Hu', 'Yankai Lin'], 'affiliations': ['Beijing Key Laboratory of Research on Large Models and Intelligent Governance', 'Gaoling School of Artificial Intelligence, Renmin University of China', 'Huawei Poisson Lab'], 'pdf_title_img': 'assets/pdf/title_img/2506.02592.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#interpretability', '#ethics', '#training'], 'emoji': '⚖️', 'ru': {'title': 'DBG: Новый способ измерения предвзятости в языковых моделях', 'desc': 'Статья представляет новый метод измерения предвзятости самопредпочтения в больших языковых моделях (LLM). Авторы вводят показатель DBG, который использует эталонные оценки в качестве прокси для качества ответов. Этот подход позволяет отделить влияние качества ответов от фактической предвзятости модели. Исследователи провели эксперименты с различными LLM и изучили факторы, влияющие на предвзятость самопредпочтения.'}, 'en': {'title': 'Measuring Self-Preference Bias with the DBG Score', 'desc': 'This paper introduces the DBG score, a new metric designed to measure self-preference bias in large language models (LLMs) while accounting for response quality. Traditional methods for assessing this bias often confuse it with the quality of the responses, as higher quality can lead to misleading score differences. By using gold judgments as benchmarks for response quality, the DBG score effectively isolates self-preference bias from quality effects. The authors conduct experiments across various LLMs and examine factors that influence bias, providing insights into the mechanisms behind self-preference in model responses.'}, 'zh': {'title': '引入DBG评分，精准测量自我偏好偏差', 'desc': '本文提出了DBG评分，用于测量大型语言模型中的自我偏好偏差。通过使用金标准判断作为响应质量的代理，DBG评分解决了响应质量对偏差测量的混淆效应。研究表明，现有方法在评估自我偏好偏差时，往往将其与响应质量混为一谈。我们通过实验评估了不同版本、规模和推理能力的语言模型的自我偏好偏差，并探讨了影响该偏差的因素。'}}}, {'id': 'https://huggingface.co/papers/2506.03099', 'title': 'TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models', 'url': 'https://huggingface.co/papers/2506.03099', 'abstract': 'TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/', 'score': 4, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'eff27ca5fef5cdcf', 'authors': ['Chetwin Low', 'Weimin Wang'], 'affiliations': ['Character AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.03099.jpg', 'data': {'categories': ['#architecture', '#multimodal', '#inference', '#games', '#audio', '#video', '#optimization'], 'emoji': '🗣️', 'ru': {'title': 'Оживляем аватары: аудио-управляемая генерация видео в реальном времени', 'desc': 'TalkingMachines - это эффективная система, преобразующая предобученные модели генерации видео в аниматоры персонажей, управляемые аудио в реальном времени. Она адаптирует современную модель преобразования изображений в видео DiT для генерации аватаров на основе аудио с 18 миллиардами параметров. Система обеспечивает бесконечную потоковую передачу видео без накопления ошибок с помощью асимметричной дистилляции знаний. TalkingMachines также включает ряд инженерных оптимизаций для высокопроизводительного вывода с низкой задержкой.'}, 'en': {'title': 'Transforming Audio into Real-Time Avatar Animation', 'desc': 'TalkingMachines is a novel framework that converts existing image-to-video models into real-time, audio-responsive avatar generators. It combines a large language model (LLM) with a video generation foundation model to create engaging conversational avatars. The framework features a significant adaptation of a state-of-the-art (SOTA) image-to-video model, allowing for efficient infinite video streaming through advanced knowledge distillation techniques. Additionally, it incorporates engineering optimizations to enhance performance, such as distributing processing across devices and minimizing computation delays.'}, 'zh': {'title': '实时音频驱动的角色动画生成器', 'desc': '本文介绍了TalkingMachines，这是一个高效的框架，将预训练的视频生成模型转变为实时的音频驱动角色动画生成器。通过将音频大型语言模型（LLM）与视频生成基础模型结合，TalkingMachines能够实现自然的对话体验。我们的主要贡献包括：将一个预训练的最先进的图像到视频模型适配为一个具有180亿参数的音频驱动头像生成模型，以及通过不对称知识蒸馏实现无限视频流的生成。我们还设计了一个高吞吐量、低延迟的推理管道，结合了多项关键的工程优化。'}}}, {'id': 'https://huggingface.co/papers/2506.04207', 'title': 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.04207', 'abstract': 'Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.', 'score': 3, 'issue_id': 4135, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '61521f9ed974c930', 'authors': ['Shuang Chen', 'Yue Guo', 'Zhaochen Su', 'Yafu Li', 'Yulun Wu', 'Jiacheng Chen', 'Jiayu Chen', 'Weijie Wang', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Fudan University', 'Shanghai AI Laboratory', 'Soochow University', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04207.jpg', 'data': {'categories': ['#reasoning', '#open_source', '#multimodal', '#training', '#benchmark', '#rl'], 'emoji': '🧠', 'ru': {'title': 'Улучшение рассуждений MLLM: от инициализации до многоэтапного RL', 'desc': 'Статья посвящена улучшению способностей мультимодальных больших языковых моделей (MLLM) к рассуждению с помощью обучения с подкреплением (RL). Авторы выявили три ключевых феномена в процессе обучения: важность правильной инициализации, проблему стагнации градиентов при стандартном GRPO и эффективность последующего текстового RL. На основе этих наблюдений была разработана модель ReVisual-R1, достигшая нового уровня производительности среди открытых 7B MLLM на сложных бенчмарках.'}, 'en': {'title': 'Unlocking Reasoning in MLLMs with Smart Training Strategies', 'desc': 'This paper explores how to improve reasoning in Multimodal Large Language Models (MLLMs) by analyzing their training processes. It identifies that starting with well-chosen text data can significantly boost reasoning capabilities, even before applying multimodal reinforcement learning (RL). The authors also highlight that traditional gradient-based methods in multimodal RL can lead to stagnation, negatively impacting training effectiveness. By implementing a staged training approach that combines text-only RL after multimodal RL, they introduce ReVisual-R1, which sets new performance records on various complex benchmarks.'}, 'zh': {'title': '提升多模态推理的新方法', 'desc': '本文探讨了如何通过强化学习（RL）提升多模态大型语言模型（MLLM）的推理能力。研究发现，良好的冷启动初始化对于增强MLLM的推理至关重要，单独使用精心选择的文本数据即可超越许多近期的多模态推理模型。标准的GRPO在多模态RL中存在梯度停滞的问题，影响了训练的稳定性和性能。通过分阶段的训练方法，结合文本和多模态RL，提出了ReVisual-R1，达到了开源7B MLLM在多个基准测试中的新状态。'}}}, {'id': 'https://huggingface.co/papers/2506.04108', 'title': 'Rectified Sparse Attention', 'url': 'https://huggingface.co/papers/2506.04108', 'abstract': 'Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42times end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.', 'score': 3, 'issue_id': 4134, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'ff7222f16cd2bf28', 'authors': ['Yutao Sun', 'Tianzhu Ye', 'Li Dong', 'Yuqing Xia', 'Jian Chen', 'Yizhao Gao', 'Shijie Cao', 'Jianyong Wang', 'Furu Wei'], 'affiliations': ['Microsoft Research', 'The University of Hong Kong', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.04108.jpg', 'data': {'categories': ['#architecture', '#inference', '#long_context', '#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'Эффективная генерация длинных текстов без потери качества', 'desc': 'Метод Rectified Sparse Attention (ReSA) улучшает эффективность генерации длинных последовательностей в больших языковых моделях. Он сочетает блочно-разреженное внимание с периодической плотной ректификацией, что позволяет ограничить накопление ошибок. ReSA достигает качества генерации, близкого к безошибочному, при значительном повышении эффективности. Метод обеспечивает ускорение до 2,42 раза при декодировании последовательностей длиной 256 тысяч токенов.'}, 'en': {'title': 'Boosting Efficiency in Long-Sequence Generation with ReSA', 'desc': "Rectified Sparse Attention (ReSA) enhances the efficiency of generating long sequences in Large Language Models by integrating block-sparse attention with periodic dense rectification. This approach addresses the issue of KV cache misalignment that can lead to errors and reduced quality in generated outputs. By periodically refreshing the KV cache through a dense forward pass, ReSA minimizes error accumulation and maintains alignment with the model's pretraining data. Experimental results show that ReSA not only preserves high-quality generation but also achieves significant speed improvements, making it a viable option for long-context tasks."}, 'zh': {'title': '高效长序列生成的新方法：ReSA', 'desc': 'Rectified Sparse Attention（ReSA）是一种提高大型语言模型长序列生成效率的方法。它结合了块稀疏注意力和周期性密集整流，能够保持高质量的生成效果。通过在固定间隔内使用密集前向传递刷新KV缓存，ReSA限制了误差累积，并保持与预训练分布的对齐。实验表明，ReSA在数学推理、语言建模和检索任务中实现了接近无损的生成质量，并在256K序列长度下提供了高达2.42倍的端到端加速。'}}}, {'id': 'https://huggingface.co/papers/2506.03106', 'title': 'Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback', 'url': 'https://huggingface.co/papers/2506.03106', 'abstract': 'Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration.', 'score': 3, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '99f5fb3b08ab4205', 'authors': ['Xiaoying Zhang', 'Hao Sun', 'Yipeng Zhang', 'Kaituo Feng', 'Chaochao Lu', 'Chao Yang', 'Helen Meng'], 'affiliations': ['Shanghai Artificial Intelligence Laboratory', 'The Chinese University of Hong Kong, HCCL', 'The Chinese University of Hong Kong, MMLab', 'University of Cambridge'], 'pdf_title_img': 'assets/pdf/title_img/2506.03106.jpg', 'data': {'categories': ['#math', '#rl', '#reasoning', '#optimization', '#rlhf', '#training'], 'emoji': '🧠', 'ru': {'title': 'Critique-GRPO: Улучшение рассуждений ИИ через комбинированную обратную связь', 'desc': 'Статья представляет Critique-GRPO - новую систему обучения с подкреплением для улучшения рассуждений больших языковых моделей. Эта система объединяет числовую и текстовую обратную связь, что позволяет преодолеть ограничения существующих методов. Эксперименты показывают, что Critique-GRPO превосходит другие подходы на основе обучения с учителем и обучения с подкреплением на различных задачах рассуждения. Исследование также выявляет важные аспекты исследовательского поведения модели в процессе обучения.'}, 'en': {'title': 'Enhancing LLM Reasoning with Critique-GRPO: A Dual Feedback Approach', 'desc': 'Critique-GRPO is a reinforcement learning (RL) framework that enhances the reasoning abilities of large language models (LLMs) by combining numerical and natural language feedback. It addresses challenges faced by traditional RL methods that rely solely on numerical feedback, such as performance plateaus and ineffective self-reflection. By incorporating critiques in natural language, Critique-GRPO allows models to refine their responses and improve their performance on difficult tasks. Experimental results show that this approach significantly outperforms existing fine-tuning methods, achieving better results in various reasoning tasks.'}, 'zh': {'title': 'Critique-GRPO：自然语言与数值反馈的完美结合', 'desc': 'Critique-GRPO是一种结合数值反馈和自然语言反馈的强化学习框架，旨在提升大型语言模型（LLM）的推理能力。该框架解决了仅依赖数值反馈时遇到的性能停滞、自我反思效果有限和持续失败等挑战。通过利用自然语言反馈，Critique-GRPO能够在模型表现停滞时，生成正确的改进建议。实验结果表明，Critique-GRPO在多个复杂任务中表现优于现有的监督学习和强化学习方法，显著提高了模型的平均通过率。'}}}, {'id': 'https://huggingface.co/papers/2506.04133', 'title': 'TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems', 'url': 'https://huggingface.co/papers/2506.04133', 'abstract': 'A review of trust, risk, and security management in LLM-based agentic multi-agent systems, examining governance, explainability, ModelOps, and privacy/security.  \t\t\t\t\tAI-generated summary \t\t\t\t Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': 'a0a258935ed39508', 'authors': ['Shaina Raza', 'Ranjan Sapkota', 'Manoj Karkee', 'Christos Emmanouilidis'], 'affiliations': ['Cornell University, USA', 'University of Groningen, Netherlands', 'Vector Institute, Toronto, Canada'], 'pdf_title_img': 'assets/pdf/title_img/2506.04133.jpg', 'data': {'categories': ['#training', '#architecture', '#survey', '#agents', '#multimodal', '#security', '#alignment', '#benchmark', '#interpretability'], 'emoji': '🤖', 'ru': {'title': 'Безопасность и доверие в эпоху агентного ИИ', 'desc': 'Статья представляет структурированный анализ управления доверием, рисками и безопасностью (TRiSM) в контексте агентных мультиагентных систем на основе больших языковых моделей (LLM). Рассматриваются четыре основных аспекта: управление, объяснимость, ModelOps и конфиденциальность/безопасность. Авторы идентифицируют уникальные векторы угроз и представляют комплексную таксономию рисков для приложений агентного ИИ. Статья также исследует механизмы построения доверия, методы обеспечения прозрачности и надзора, а также современные стратегии объяснимости в распределенных системах агентов LLM.'}, 'en': {'title': 'Navigating Trust and Security in Agentic AI Systems', 'desc': 'This paper reviews the management of trust, risk, and security in multi-agent systems that use large language models (LLMs). It discusses how these agentic AI systems differ from traditional AI, focusing on their ability to operate autonomously and collaboratively. The authors outline four key areas of Trust, Risk, and Security Management (TRiSM): governance, explainability, ModelOps, and privacy/security, providing a framework for understanding the unique challenges these systems face. The paper also highlights the importance of building trust and ensuring transparency in these systems, while proposing future research directions for responsible deployment.'}, 'zh': {'title': '构建安全透明的代理人工智能系统', 'desc': '本文回顾了基于大型语言模型（LLM）的代理多智能体系统中的信任、风险和安全管理（TRiSM）。我们分析了代理人工智能的概念基础及其与传统人工智能代理的架构差异，并探讨了支持可扩展自主性的系统设计。文章详细阐述了TRiSM的四个支柱：治理、可解释性、模型操作和隐私/安全，并为代理LLM提供了具体的背景。最后，提出了负责任的代理人工智能的路线图，建议研究方向以确保新兴多智能体系统的安全、透明和负责任的部署。'}}}, {'id': 'https://huggingface.co/papers/2506.04034', 'title': 'Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning', 'url': 'https://huggingface.co/papers/2506.04034', 'abstract': 'Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings.', 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-04', 'pub_date_card': {'ru': '4 июня', 'en': 'June 4', 'zh': '6月4日'}, 'hash': '9d3dcbdd5158f101', 'authors': ['Qing Jiang', 'Xingyu Chen', 'Zhaoyang Zeng', 'Junzhi Yu', 'Lei Zhang'], 'affiliations': ['International Digital Economy Academy (IDEA)', 'Peking University', 'South China University of Technology'], 'pdf_title_img': 'assets/pdf/title_img/2506.04034.jpg', 'data': {'categories': ['#cv', '#rl', '#training', '#reasoning', '#hallucinations', '#interpretability', '#dataset'], 'emoji': '🔍', 'ru': {'title': 'Интерпретируемое объектное реферирование через пошаговые рассуждения', 'desc': 'Статья представляет новый подход к задаче объектного реферирования в компьютерном зрении, названный Rex-Thinker. Модель использует пошаговое рассуждение для оценки соответствия объектов заданному описанию, что повышает интерпретируемость и надежность предсказаний. Авторы создали датасет HumanRef-CoT для обучения модели структурированным рассуждениям. Rex-Thinker обучается в два этапа: контролируемая тонкая настройка и обучение с подкреплением, что улучшает точность и обобщающую способность модели.'}, 'en': {'title': 'Rex-Thinker: Grounded Object Referring with Explainable Reasoning', 'desc': 'This paper introduces Rex-Thinker, a model designed to enhance object referring in images by incorporating explainable and trustworthy reasoning. Unlike traditional methods that focus solely on bounding box predictions, Rex-Thinker employs a Chain of Thought (CoT) reasoning approach to evaluate candidate objects against natural language descriptions. The model is trained on a new dataset, HumanRef-CoT, which facilitates structured reasoning through a systematic planning and summarization process. Results indicate that Rex-Thinker not only improves precision and interpretability but also effectively rejects irrelevant predictions, showcasing its robustness in various scenarios.'}, 'zh': {'title': 'Rex-Thinker：可解释的物体指代模型', 'desc': '本文提出了一种新的物体指代模型Rex-Thinker，旨在通过明确的链式推理任务来检测与自然语言描述匹配的图像中的所有物体。该模型强调可验证性和可信性，确保其预测能够解释并与视觉证据相连。Rex-Thinker通过逐步推理候选物体实例，判断其是否符合给定的描述，从而做出最终预测。实验结果表明，该方法在精确度和可解释性方面优于传统基线，并在拒绝虚假输出和跨领域泛化能力上表现出色。'}}}, {'id': 'https://huggingface.co/papers/2506.03448', 'title': 'RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions', 'url': 'https://huggingface.co/papers/2506.03448', 'abstract': 'RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Despite recent advances in inversion and instruction-based image editing, existing approaches primarily excel at editing single, prominent objects but significantly struggle when applied to complex scenes containing multiple entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous real-world benchmark rooted in RefCOCO, where even baselines trained on millions of samples perform poorly. To overcome this limitation, we introduce RefEdit -- an instruction-based editing model trained on our scalable synthetic data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets, outperforms the Flux/SD3 model-based baselines trained on millions of data. Extensive evaluations across various benchmarks demonstrate that our model not only excels in referring expression tasks but also enhances performance on traditional benchmarks, achieving state-of-the-art results comparable to closed-source methods. We release data \\& checkpoint for reproducibility.', 'score': 2, 'issue_id': 4134, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': '717f877ff02ce882', 'authors': ['Bimsara Pathiraja', 'Maitreya Patel', 'Shivam Singh', 'Yezhou Yang', 'Chitta Baral'], 'affiliations': ['Arizona State University'], 'pdf_title_img': 'assets/pdf/title_img/2506.03448.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#cv', '#optimization', '#open_source'], 'emoji': '🖼️', 'ru': {'title': 'RefEdit: Прорыв в редактировании сложных изображений с помощью ИИ', 'desc': 'RefEdit - это модель редактирования изображений на основе инструкций, обученная на синтетических данных. Она превосходит базовые модели в задачах редактирования сложных сцен и работы с референсными выражениями. Авторы представили новый бенчмарк RefEdit-Bench для оценки таких моделей. RefEdit, обученная всего на 20 000 примерах, превзошла модели, обученные на миллионах образцов.'}, 'en': {'title': 'Revolutionizing Image Editing with Instruction-Based Learning', 'desc': 'RefEdit is a new model designed for editing images based on instructions, specifically focusing on complex scenes with multiple objects. Unlike previous models that struggle with such tasks, RefEdit is trained on a unique synthetic data generation pipeline, allowing it to learn effectively from a smaller dataset of 20,000 editing examples. The model significantly outperforms existing baselines, which were trained on millions of samples, in both referring expression tasks and traditional editing benchmarks. This advancement highlights the potential of instruction-based editing in achieving high performance in challenging image editing scenarios.'}, 'zh': {'title': 'RefEdit：复杂场景编辑的新突破', 'desc': 'RefEdit是一种基于指令的编辑模型，专门针对复杂场景中的编辑任务进行训练。与传统方法相比，RefEdit在处理多个实体的复杂场景时表现更为出色。我们还引入了RefEdit-Bench，这是一个基于RefCOCO的真实世界基准，用于量化现有方法的不足。通过使用合成数据生成管道，RefEdit在仅使用20,000个编辑三元组的情况下，超越了基于Flux/SD3模型的基线，展示了其在指代表达任务和传统基准上的优越性能。'}}}, {'id': 'https://huggingface.co/papers/2506.02945', 'title': 'Quantitative LLM Judges', 'url': 'https://huggingface.co/papers/2506.02945', 'abstract': "LLM-as-a-judge is a framework in which a large language model (LLM) automatically evaluates the output of another LLM. We propose quantitative LLM judges, which align evaluation scores of existing LLM judges to human scores in a given domain using regression models. The models are trained to improve the score of the original judge by using the judge's textual evaluation and score. We present four quantitative judges for different types of absolute and relative feedback, which showcases the generality and versatility of our framework. Our framework is more computationally efficient than supervised fine-tuning and can be more statistically efficient when human feedback is limited, which is expected in most applications of our work. We validate these claims empirically on four datasets using two base judges. Our experiments show that quantitative judges can effectively improve the predictive power of existing judges through post-hoc modeling.", 'score': 2, 'issue_id': 4133, 'pub_date': '2025-06-03', 'pub_date_card': {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'}, 'hash': 'de4ea9c8e4abb76a', 'authors': ['Aishwarya Sahoo', 'Jeevana Kruthi Karnuthala', 'Tushar Parmanand Budhwani', 'Pranchal Agarwal', 'Sankaran Vaidyanathan', 'Alexa Siu', 'Franck Dernoncourt', 'Jennifer Healey', 'Nedim Lipka', 'Ryan Rossi', 'Uttaran Bhattacharya', 'Branislav Kveton'], 'affiliations': ['Adobe Research', 'University of Massachusetts Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2506.02945.jpg', 'data': {'categories': ['#training', '#optimization', '#alignment', '#rlhf', '#dataset'], 'emoji': '⚖️', 'ru': {'title': 'LLM-судьи: автоматическая оценка языковых моделей с помощью регрессии', 'desc': 'Статья представляет фреймворк LLM-as-a-judge, где большая языковая модель оценивает результаты другой модели. Авторы предлагают количественных LLM-судей, которые согласуют оценки существующих судей с человеческими оценками в заданной области с помощью регрессионных моделей. Представлены четыре количественных судьи для различных типов абсолютной и относительной обратной связи. Эксперименты показывают, что количественные судьи могут эффективно улучшить предсказательную силу существующих судей через постобработку.'}, 'en': {'title': 'Enhancing LLM Evaluation with Quantitative Judges', 'desc': "The paper introduces a framework called LLM-as-a-judge, where a large language model (LLM) assesses the outputs of another LLM. It focuses on creating quantitative LLM judges that align their evaluation scores with human assessments using regression models. These models enhance the original judge's scoring by leveraging its textual evaluations and scores. The framework is shown to be more computationally and statistically efficient than traditional supervised fine-tuning, especially when human feedback is scarce, and is validated through experiments on multiple datasets."}, 'zh': {'title': '利用LLM提升评估效率的创新框架', 'desc': '本文提出了一种名为LLM-as-a-judge的框架，利用大型语言模型（LLM）自动评估另一个LLM的输出。我们引入了定量LLM评估者，通过回归模型将现有评估者的评分与人类评分对齐。该模型通过使用评估者的文本评价和评分来提高原始评估者的评分。我们的框架在计算效率上优于监督微调，并且在人工反馈有限的情况下，统计效率更高，适用于大多数应用场景。'}}}, {'id': 'https://huggingface.co/papers/2506.02294', 'title': 'Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation', 'url': 'https://huggingface.co/papers/2506.02294', 'abstract': 'A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.  \t\t\t\t\tAI-generated summary \t\t\t\t Large foundation models trained on extensive datasets demonstrate strong zero-shot capabilities in various domains. To replicate their success when data and model size are constrained, knowledge distillation has become an established tool for transferring knowledge from foundation models to small student networks. However, the effectiveness of distillation is critically limited by the available training data. This work addresses the common practical issue of covariate shift in knowledge distillation, where spurious features appear during training but not at test time. We ask the question: when these spurious features are unknown, yet a robust teacher is available, is it possible for a student to also become robust to them? We address this problem by introducing a novel diffusion-based data augmentation strategy that generates images by maximizing the disagreement between the teacher and the student, effectively creating challenging samples that the student struggles with. Experiments demonstrate that our approach significantly improves worst group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art diffusion-based data augmentation baselines', 'score': 1, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '740d99ccc158d514', 'authors': ['Niclas Popp', 'Kevin Alexander Laube', 'Matthias Hein', 'Lukas Schott'], 'affiliations': ['Bosch Center for Artificial Intelligence', 'University of Tübingen'], 'pdf_title_img': 'assets/pdf/title_img/2506.02294.jpg', 'data': {'categories': ['#data', '#transfer_learning', '#training', '#optimization', '#diffusion', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Повышение устойчивости моделей через генерацию сложных примеров', 'desc': 'Статья представляет новую стратегию аугментации данных на основе диффузии для улучшения робастности в процессе дистилляции знаний. Метод генерирует сложные образцы, максимизируя разногласие между учителем и учеником, что помогает преодолеть проблему ковариационного сдвига. Эксперименты показывают значительное улучшение точности на наихудших группах и средней точности по группам на датасетах CelebA и SpuCo Birds. Подход превосходит современные методы аугментации данных на основе диффузии.'}, 'en': {'title': 'Boosting Student Robustness with Diffusion Data Augmentation', 'desc': 'This paper presents a new data augmentation method using diffusion processes to enhance knowledge distillation. The approach generates challenging samples that help student networks learn to be more robust against spurious features that may not appear during testing. By maximizing the disagreement between a robust teacher model and the student model, the method effectively prepares the student for real-world scenarios where data may shift. Experiments show that this strategy improves accuracy and resilience against spurious features in various datasets, outperforming existing methods.'}, 'zh': {'title': '基于扩散的数据增强提升知识蒸馏鲁棒性', 'desc': '本文提出了一种基于扩散的数据增强策略，以提高知识蒸馏中的鲁棒性。该方法通过生成具有挑战性的样本，增强了学生网络对虚假特征的抵抗力。实验结果表明，在CelebA和SpuCo Birds数据集上，该策略显著提高了最差组和平均组的准确率。通过最大化教师和学生之间的分歧，本文有效地解决了知识蒸馏中的协变量偏移问题。'}}}, {'id': 'https://huggingface.co/papers/2506.01344', 'title': 'Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents', 'url': 'https://huggingface.co/papers/2506.01344', 'abstract': "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart's structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.", 'score': 1, 'issue_id': 4133, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '788495117e4bf1d7', 'authors': ['Manan Suri', 'Puneet Mathur', 'Nedim Lipka', 'Franck Dernoncourt', 'Ryan A. Rossi', 'Vivek Gupta', 'Dinesh Manocha'], 'affiliations': ['Adobe', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2506.01344.jpg', 'data': {'categories': ['#graphs', '#cv', '#reasoning', '#agents', '#hallucinations', '#multimodal', '#benchmark', '#interpretability'], 'emoji': '🔀', 'ru': {'title': 'Точная интерпретация блок-схем с помощью нейросимволического агента', 'desc': 'Статья представляет задачу точной атрибуции блок-схем и агента FlowPathAgent для ее решения. Авторы разработали нейросимволический подход, который сегментирует блок-схему, преобразует ее в структурированный символьный граф и использует агентный метод для генерации путей атрибуции. Также представлен новый бенчмарк FlowExplainBench для оценки атрибуций блок-схем. Результаты показывают, что FlowPathAgent снижает визуальные галлюцинации в ответах языковых моделей на вопросы по блок-схемам, превосходя базовые методы на 10-14%.'}, 'en': {'title': 'Enhancing Flowchart Interpretation with Fine-grained Attribution', 'desc': 'This paper addresses the challenges of interpreting flowcharts using large language models (LLMs) due to their complex structures and potential for hallucination. It introduces Fine-grained Flowchart Attribution, a method that links LLM responses to specific components of flowcharts, enhancing the reliability and explainability of automated processing. The authors present FlowPathAgent, a neurosymbolic agent that utilizes graph-based reasoning to segment flowcharts and create structured symbolic graphs for dynamic interaction. Experimental results demonstrate that FlowPathAgent significantly reduces hallucinations in LLM outputs, achieving improved performance on the newly introduced FlowExplainBench benchmark.'}, 'zh': {'title': '提升流程图解析的可靠性与可解释性', 'desc': '本论文介绍了一种新的任务，称为细粒度流程图归因，旨在提高大型语言模型（LLM）在处理流程图时的可靠性和可解释性。我们提出了FlowPathAgent，这是一种神经符号代理，通过图形推理进行细粒度的后期归因。该代理首先对流程图进行分割，然后将其转换为结构化的符号图，并动态与图进行交互，以生成归因路径。实验结果表明，FlowPathAgent在流程图问答中减少了视觉幻觉，相较于强基线提高了10-14%的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.23807', 'title': 'DLP: Dynamic Layerwise Pruning in Large Language Models', 'url': 'https://huggingface.co/papers/2505.23807', 'abstract': 'A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.  \t\t\t\t\tAI-generated summary \t\t\t\t Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.', 'score': 1, 'issue_id': 4133, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': 'a817afc0cdd35d8d', 'authors': ['Yuli Chen', 'Bo Cheng', 'Jiale Han', 'Yingying Zhang', 'Yingting Li', 'Shuhao Zhang'], 'affiliations': ['Hong Kong University of Science and Technology, Hong Kong, China', 'State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23807.jpg', 'data': {'categories': ['#optimization', '#inference', '#training'], 'emoji': '✂️', 'ru': {'title': 'Умная обрезка слоев для эффективных языковых моделей', 'desc': 'Предложен новый метод динамической послойной обрезки (DLP) для больших языковых моделей. DLP адаптивно определяет важность каждого слоя, комбинируя информацию о весах модели и активациях. Это позволяет сохранить производительность модели при высоком уровне разреженности. Эксперименты показали, что DLP превосходит существующие методы обрезки для различных языковых моделей.'}, 'en': {'title': 'Dynamic Layerwise Pruning: Smart Sparsity for Language Models', 'desc': 'This paper introduces a new method called Dynamic Layerwise Pruning (DLP) that improves the efficiency of large language models (LLMs) by adaptively determining the importance of each layer. Unlike traditional pruning methods that apply uniform strategies, DLP combines model weights and activation data to assign specific pruning rates to different layers. This approach helps maintain model performance even at high levels of sparsity, which is crucial for effective model compression. Experimental results demonstrate that DLP significantly enhances accuracy and reduces perplexity in LLMs compared to existing techniques.'}, 'zh': {'title': '动态剪枝，智能保持性能！', 'desc': '动态层级剪枝方法通过结合模型权重和激活信息，自适应地确定每一层的重要性，从而在高稀疏性下保持大型语言模型的性能。传统的剪枝技术通常采用均匀层级剪枝策略，这可能导致在高稀疏性水平下性能显著下降。动态层级剪枝（DLP）方法克服了这一限制，能够根据输入激活信息动态调整剪枝率。实验结果表明，DLP在多个大型语言模型中有效地保持了高稀疏性下的模型性能。'}}}, {'id': 'https://huggingface.co/papers/2505.21541', 'title': 'DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers', 'url': 'https://huggingface.co/papers/2505.21541', 'abstract': 'DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.  \t\t\t\t\tAI-generated summary \t\t\t\t Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.', 'score': 1, 'issue_id': 4133, 'pub_date': '2025-05-24', 'pub_date_card': {'ru': '24 мая', 'en': 'May 24', 'zh': '5月24日'}, 'hash': 'cda6015909393ad0', 'authors': ['Zitong Wang', 'Hang Zhao', 'Qianyu Zhou', 'Xuequan Lu', 'Xiangtai Li', 'Yiren Song'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.21541.jpg', 'data': {'categories': ['#diffusion', '#open_source', '#cv', '#dataset'], 'emoji': '🖼️', 'ru': {'title': 'Умное разделение изображений на слои с помощью ИИ', 'desc': 'DiffDecompose - это новая система на основе диффузионного трансформера для декомпозиции изображений на семантические слои. Она решает проблемы разделения полупрозрачных и прозрачных наложений, с которыми не справлялись предыдущие методы. Авторы создали датасет AlphaBlend для обучения модели работе с различными типами прозрачности. DiffDecompose использует условное генерирование и позиционное кодирование слоев для точного восстановления составляющих изображения.'}, 'en': {'title': 'Revolutionizing Image Layer Decomposition with DiffDecompose', 'desc': "This paper introduces DiffDecompose, a novel framework that uses diffusion Transformers to decompose images into their individual layers, particularly focusing on transparent and semi-transparent layers. The authors highlight the limitations of existing methods in handling complex occlusions and propose a new dataset called AlphaBlend, which is designed to support various real-world image decomposition tasks. DiffDecompose employs In-Context Decomposition to predict multiple layers without needing direct supervision for each layer, enhancing its ability to generalize across different scenarios. The framework's effectiveness is validated through extensive experiments on the AlphaBlend dataset and the public LOGO dataset, showcasing its potential in image processing applications."}, 'zh': {'title': '透明层分解的新突破：DiffDecompose', 'desc': 'DiffDecompose 是一个基于扩散 Transformer 的框架，能够有效地将图像分解为组成层，并使用语义提示来解决透明层分解中的挑战。该方法针对半透明和透明图层的非线性遮挡问题，提出了一种新的任务：逐层分解 alpha 合成图像。为了解决层模糊、泛化能力和数据稀缺的问题，研究者们首次引入了 AlphaBlend 数据集，支持多种实际应用场景。DiffDecompose 通过上下文分解的方法，能够在没有逐层监督的情况下预测一个或多个层，展示了其在图像分解任务中的有效性。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d', '#agents (3)', '#agi', '#alignment (2)', '#architecture (4)', '#audio (1)', '#benchmark (9)', '#cv (5)', '#data (2)', '#dataset (7)', '#diffusion (4)', '#ethics (1)', '#games (1)', '#graphs (1)', '#hallucinations (3)', '#healthcare', '#inference (3)', '#interpretability (5)', '#leakage', '#long_context (3)', '#low_resource', '#machine_translation', '#math (1)', '#multilingual', '#multimodal (6)', '#open_source (4)', '#optimization (9)', '#plp', '#rag', '#reasoning (8)', '#rl (6)', '#rlhf (5)', '#robotics', '#science', '#security (1)', '#small_models', '#story_generation (1)', '#survey (1)', '#synthetic (3)', '#training (14)', '#transfer_learning (1)', '#video (2)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-05 04:20',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-05 04:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-05 04:20')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    