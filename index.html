
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 31 papers. December 6.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ", "Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹", "Ğ¼Ğ¸Ğ½ÑƒÑ‚"],
                hour: ["Ñ‡Ğ°Ñ", "Ñ‡Ğ°ÑĞ°", "Ñ‡Ğ°ÑĞ¾Ğ²"],
                day: ["Ğ´ĞµĞ½ÑŒ", "Ğ´Ğ½Ñ", "Ğ´Ğ½ĞµĞ¹"],
                justNow: "Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ñ‚Ğ¾",
                ago: "Ğ½Ğ°Ğ·Ğ°Ğ´"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["åˆ†é’Ÿ", "åˆ†é’Ÿ", "åˆ†é’Ÿ"],
                hour: ["å°æ—¶", "å°æ—¶", "å°æ—¶"],
                day: ["å¤©", "å¤©", "å¤©"],
                justNow: "åˆšåˆš",
                ago: "å‰"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            } else if (lastDigit === 1) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒÑ";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "ÑÑ‚Ğ°Ñ‚ÑŒĞ¸";
            } else {
                word = "ÑÑ‚Ğ°Ñ‚ĞµĞ¹";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "ç¯‡è®ºæ–‡"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">ğŸ”º</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ</span> | <span id="title-articles-count">31 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2024-12-05.html">â¬…ï¸ <span id="prev-date">05.12</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2024-12-09.html">â¡ï¸ <span id="next-date">09.12</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2024-12.html">ğŸ“ˆ <span id='top-month-label'>ĞœĞµÑÑÑ†</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">ğŸ”€ <span id="sort-label-text">Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ</option>
                    <option value="pub_date">Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸</option>
                    <option value="issue_id">Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">ğŸ·ï¸ Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> AâˆªB</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> Aâˆ©B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">ğŸ§¹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> âœ–ï¸ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '6 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 6', 'zh': '12æœˆ6æ—¥'};
        let feedDateNext = {'ru': '09.12', 'en': '12/09', 'zh': '12æœˆ9æ—¥'};
        let feedDatePrev = {'ru': '05.12', 'en': '12/05', 'zh': '12æœˆ5æ—¥'};
        let filterLabel = {'ru': 'Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€', 'en': 'Topics', 'zh': 'ä¸»é¢˜ç­›é€‰'}
        let publishedLabel = {'ru': 'ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¾Ñ‚ ', 'en': 'published on ', 'zh': 'å‘è¡¨äº'}
        let sortLabel = {'ru': 'Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ¿Ğ¾', 'en': 'Sort by', 'zh': 'æ’åºæ–¹å¼'}
        let paperLabel = {'ru': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ', 'en': 'Paper', 'zh': 'è®ºæ–‡'}
        let topMonthLabel = {'ru': 'ĞœĞµÑÑÑ†', 'en': 'Month', 'zh': 'æœˆåº¦è®ºæ–‡'}
        let topDayLabel = {'ru': 'Ğ”ĞµĞ½ÑŒ', 'en': 'Day', 'zh': 'æ—¥åº¦è®ºæ–‡'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2412.04467', 'title': 'VisionZip: Longer is Better but Not Necessary in Vision Language Models', 'url': 'https://huggingface.co/papers/2412.04467', 'abstract': 'Recent advancements in vision-language models have enhanced performance by increasing the length of visual tokens, making them much longer than text tokens and significantly raising computational costs. However, we observe that the visual tokens generated by popular vision encoders, such as CLIP and SigLIP, contain significant redundancy. To address this, we introduce VisionZip, a simple yet effective method that selects a set of informative tokens for input to the language model, reducing visual token redundancy and improving efficiency while maintaining model performance. The proposed VisionZip can be widely applied to image and video understanding tasks and is well-suited for multi-turn dialogues in real-world scenarios, where previous methods tend to underperform. Experimental results show that VisionZip outperforms the previous state-of-the-art method by at least 5% performance gains across nearly all settings. Moreover, our method significantly enhances model inference speed, improving the prefilling time by 8x and enabling the LLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while achieving better results. Furthermore, we analyze the causes of this redundancy and encourage the community to focus on extracting better visual features rather than merely increasing token length. Our code is available at https://github.com/dvlab-research/VisionZip .', 'score': 64, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '5539efbb0d3e8e80', 'authors': ['Senqiao Yang', 'Yukang Chen', 'Zhuotao Tian', 'Chengyao Wang', 'Jingyao Li', 'Bei Yu', 'Jiaya Jia'], 'affiliations': ['CUHK', 'HITSZ', 'HKUST'], 'pdf_title_img': 'assets/pdf/title_img/2412.04467.jpg', 'data': {'categories': ['#inference', '#interpretability', '#multimodal', '#optimization', '#cv'], 'emoji': 'ğŸ—œï¸', 'ru': {'title': 'VisionZip: Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑƒÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ˜Ğ˜', 'desc': 'VisionZip - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ². ĞĞ½ Ñ€ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğ¼Ğ¸ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹, Ñ‚Ğ°ĞºĞ¸Ğ¼Ğ¸ ĞºĞ°Ğº CLIP Ğ¸ SigLIP. VisionZip Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ»ÑƒÑ‡ÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ², Ğ° Ğ½Ğµ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ° ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ğ¸Ğ½Ñ‹ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ².'}, 'en': {'title': 'Streamlining Visual Tokens for Enhanced Efficiency in Vision-Language Models', 'desc': 'This paper presents VisionZip, a method designed to reduce redundancy in visual tokens used in vision-language models. By selecting only the most informative tokens, VisionZip improves computational efficiency without sacrificing performance. The method shows significant improvements in both model inference speed and overall accuracy, outperforming previous state-of-the-art techniques by at least 5%. The authors emphasize the importance of optimizing visual feature extraction rather than simply increasing token length.'}, 'zh': {'title': 'VisionZipï¼šé«˜æ•ˆå‡å°‘è§†è§‰æ ‡è®°å†—ä½™çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ€è¿‘ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›å±•é€šè¿‡å¢åŠ è§†è§‰æ ‡è®°çš„é•¿åº¦æ¥æé«˜æ€§èƒ½ï¼Œä½†è¿™ä¹Ÿæ˜¾è‘—å¢åŠ äº†è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œæµè¡Œçš„è§†è§‰ç¼–ç å™¨ç”Ÿæˆçš„è§†è§‰æ ‡è®°å­˜åœ¨æ˜¾è‘—çš„å†—ä½™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VisionZipï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é€‰æ‹©ä¸€ç»„ä¿¡æ¯ä¸°å¯Œçš„æ ‡è®°è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»è€Œå‡å°‘è§†è§‰æ ‡è®°çš„å†—ä½™ï¼Œæé«˜æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisionZipåœ¨å‡ ä¹æ‰€æœ‰è®¾ç½®ä¸­æ¯”ä¹‹å‰çš„æœ€å…ˆè¿›æ–¹æ³•æé«˜äº†è‡³å°‘5%çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—æå‡äº†æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04424', 'title': 'Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion', 'url': 'https://huggingface.co/papers/2412.04424', 'abstract': 'We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2\'s visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose "depth-breath fusion (DBFusion)" to fuse the visual features extracted from different depths and under multiple prompts. Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. Our quantitative analysis and visualization of Florence-VL\'s visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced. https://github.com/JiuhaiChen/Florence-VL', 'score': 36, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '18a7fac6be50215d', 'authors': ['Jiuhai Chen', 'Jianwei Yang', 'Haiping Wu', 'Dianqi Li', 'Jianfeng Gao', 'Tianyi Zhou', 'Bin Xiao'], 'affiliations': ['Microsoft Research', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2412.04424.jpg', 'data': {'categories': ['#alignment', '#training', '#benchmark', '#hallucinations', '#open_source', '#architecture', '#multimodal'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'Florence-VL: ĞĞ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Florence-VL Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ½Ğ¾Ğ²Ğ¾Ğµ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼Ğ¸, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Florence-2 - Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ñ… Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² CLIP, Florence-2 ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ° Ğ·Ğ°Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ¸ Ğ°ÑĞ¿ĞµĞºÑ‚Ñ‹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ ÑĞ»Ğ¸ÑĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ¸ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ñ€ĞµÑ†ĞµĞ¿Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Florence-2 Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Florence-VL Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ MLLM Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ñ€Ğ¸ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ·Ñ€ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ°Ñ….'}, 'en': {'title': 'Florence-VL: Bridging Vision and Language with Depth-Breath Fusion', 'desc': 'Florence-VL is a new type of multimodal large language model that enhances visual understanding using advanced features from the Florence-2 vision model. Unlike traditional models that rely on contrastive learning, Florence-2 captures a wider range of visual details, making it adaptable for various tasks. The model employs a unique feature-fusion method called depth-breath fusion (DBFusion) to combine visual information from different levels and prompts effectively. As a result, Florence-VL outperforms existing models in multiple benchmarks related to vision and language tasks, and its training methods are available for further research.'}, 'zh': {'title': 'Florence-VLï¼šå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„æ–°çªç ´', 'desc': 'Florence-VLæ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç»“åˆäº†Florence-2ç”Ÿæˆçš„ä¸°å¯Œè§†è§‰è¡¨ç¤ºã€‚ä¸ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒçš„CLIPé£æ ¼è§†è§‰å˜æ¢å™¨ä¸åŒï¼ŒFlorence-2èƒ½å¤Ÿæ•æ‰ä¸åŒå±‚æ¬¡å’Œæ–¹é¢çš„è§†è§‰ç‰¹å¾ï¼Œæ›´åŠ çµæ´»åœ°é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç‰¹å¾èåˆæ¶æ„å’Œåˆ›æ–°çš„è®­ç»ƒæ–¹æ¡ˆï¼Œæœ‰æ•ˆåœ°å°†Florence-2çš„è§†è§‰ç‰¹å¾æ•´åˆåˆ°é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¸­ã€‚Florence-VLåœ¨å¤šç§å¤šæ¨¡æ€å’Œè§†è§‰ä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æå‡äº†æ€§èƒ½ï¼Œå±•ç¤ºäº†å…¶åœ¨è§†è§‰-è¯­è¨€å¯¹é½æ–¹é¢çš„ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04455', 'title': 'Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection', 'url': 'https://huggingface.co/papers/2412.04455', 'abstract': 'Automatic detection and prevention of open-set failures are crucial in closed-loop robotic systems. Recent studies often struggle to simultaneously identify unexpected failures reactively after they occur and prevent foreseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a novel paradigm leveraging the vision-language model (VLM) for both open-set reactive and proactive failure detection. The core of our method is to formulate both tasks as a unified set of spatio-temporal constraint satisfaction problems and use VLM-generated code to evaluate them for real-time monitoring. To enhance the accuracy and efficiency of monitoring, we further introduce constraint elements that abstract constraint-related entities or their parts into compact geometric elements. This approach offers greater generality, simplifies tracking, and facilitates constraint-aware visual programming by leveraging these elements as visual prompts. Experiments show that CaM achieves a 28.7% higher success rate and reduces execution time by 31.8% under severe disturbances compared to baselines across three simulators and a real-world setting. Moreover, CaM can be integrated with open-loop control policies to form closed-loop systems, enabling long-horizon tasks in cluttered scenes with dynamic environments.', 'score': 31, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a00b427eb116e4bf', 'authors': ['Enshen Zhou', 'Qi Su', 'Cheng Chi', 'Zhizheng Zhang', 'Zhongyuan Wang', 'Tiejun Huang', 'Lu Sheng', 'He Wang'], 'affiliations': ['Beijing Academy of Artificial Intelligence', 'School of Computer Science, Peking University', 'School of Software, Beihang University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04455.jpg', 'data': {'categories': ['#agents', '#robotics', '#optimization', '#cv', '#security'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞšĞ¾Ğ´ ĞºĞ°Ğº Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€: ÑƒĞ¼Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ğ²', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¼ĞµÑ‚Ğ¾Ğ´ Code-as-Monitor (CaM), Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ñ…. CaM Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ°Ğº Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ¸Ñ… Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ² Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ±ÑÑ‚Ñ€Ğ°Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹, Ñ‡Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±Ğ»ĞµĞ³Ñ‡Ğ°ĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ CaM Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ….'}, 'en': {'title': 'Revolutionizing Robotic Failure Detection with Code-as-Monitor', 'desc': 'This paper presents Code-as-Monitor (CaM), a new method for detecting and preventing failures in robotic systems that operate in unpredictable environments. CaM uses a vision-language model (VLM) to handle both reactive and proactive failure detection by treating these tasks as spatio-temporal constraint satisfaction problems. The method improves monitoring accuracy and efficiency by introducing compact geometric elements that represent constraints, making it easier to track and manage them visually. Experimental results demonstrate that CaM significantly outperforms existing methods, achieving higher success rates and faster execution times in both simulated and real-world scenarios.'}, 'zh': {'title': 'æ™ºèƒ½ç›‘æ§ï¼šæå‡æœºå™¨äººç³»ç»Ÿçš„æ•…éšœæ£€æµ‹ä¸é¢„é˜²', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCode-as-Monitorï¼ˆCaMï¼‰çš„æ–°æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹å’Œé¢„é˜²é—­ç¯æœºå™¨äººç³»ç»Ÿä¸­çš„å¼€æ”¾é›†æ•…éšœã€‚è¯¥æ–¹æ³•åˆ©ç”¨è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å°†ååº”æ€§å’Œä¸»åŠ¨æ€§æ•…éšœæ£€æµ‹ä»»åŠ¡ç»Ÿä¸€ä¸ºæ—¶ç©ºçº¦æŸæ»¡è¶³é—®é¢˜ã€‚é€šè¿‡ç”Ÿæˆçš„ä»£ç è¿›è¡Œå®æ—¶ç›‘æ§ï¼ŒCaMåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šéƒ½æœ‰æ˜¾è‘—æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCaMåœ¨ä¸¥é‡å¹²æ‰°ä¸‹çš„æˆåŠŸç‡æé«˜äº†28.7%ï¼Œæ‰§è¡Œæ—¶é—´å‡å°‘äº†31.8%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04454', 'title': 'Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction', 'url': 'https://huggingface.co/papers/2412.04454', 'abstract': 'Graphical User Interfaces (GUIs) are critical to human-computer interaction, yet automating GUI tasks remains challenging due to the complexity and variability of visual environments. Existing approaches often rely on textual representations of GUIs, which introduce limitations in generalization, efficiency, and scalability. In this paper, we introduce Aguvis, a unified pure vision-based framework for autonomous GUI agents that operates across various platforms. Our approach leverages image-based observations, and grounding instructions in natural language to visual elements, and employs a consistent action space to ensure cross-platform generalization. To address the limitations of previous work, we integrate explicit planning and reasoning within the model, enhancing its ability to autonomously navigate and interact with complex digital environments. We construct a large-scale dataset of GUI agent trajectories, incorporating multimodal reasoning and grounding, and employ a two-stage training pipeline that first focuses on general GUI grounding, followed by planning and reasoning. Through comprehensive experiments, we demonstrate that Aguvis surpasses previous state-of-the-art methods in both offline and real-world online scenarios, achieving, to our knowledge, the first fully autonomous pure vision GUI agent capable of performing tasks independently without collaboration with external closed-source models. We open-sourced all datasets, models, and training recipes to facilitate future research at https://aguvis-project.github.io/.', 'score': 30, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a088657cce2c618c', 'authors': ['Yiheng Xu', 'Zekun Wang', 'Junli Wang', 'Dunjie Lu', 'Tianbao Xie', 'Amrita Saha', 'Doyen Sahoo', 'Tao Yu', 'Caiming Xiong'], 'affiliations': ['Salesforce Research', 'University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2412.04454.jpg', 'data': {'categories': ['#open_source', '#games', '#multimodal', '#agents', '#training', '#reasoning', '#dataset'], 'emoji': 'ğŸ–¥ï¸', 'ru': {'title': 'Aguvis: ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ“Ğ˜ĞŸ-Ğ°Ğ³ĞµĞ½Ñ‚ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸', 'desc': 'ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Aguvis - ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ñ… Ğ°Ğ³ĞµĞ½Ñ‚Ğ¾Ğ² Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ğ“Ğ˜ĞŸ), Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‰ÑƒÑ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºÑƒ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹ Ğ½Ğ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ Ğº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼, Ñ‡Ñ‚Ğ¾ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºÑ€Ğ¾ÑÑ-Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ĞµĞ½Ğ½ÑƒÑ Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ. Aguvis Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ²Ğ½Ğ¾Ğµ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ² ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑÑ€ĞµĞ´Ğ°Ñ…. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Aguvis Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ² Ğ¾Ñ„Ğ»Ğ°Ğ¹Ğ½ Ğ¸ Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸ÑÑ…, Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.'}, 'en': {'title': 'Aguvis: Revolutionizing Autonomous GUI Interaction with Pure Vision', 'desc': 'This paper presents Aguvis, a novel framework designed for autonomous GUI agents that operates purely on visual inputs. Unlike previous methods that depend on textual representations, Aguvis utilizes image-based observations and natural language instructions to interact with GUI elements. The framework incorporates explicit planning and reasoning, allowing it to effectively navigate and perform tasks in complex digital environments. Through extensive testing, Aguvis demonstrates superior performance compared to existing methods, marking a significant advancement in the development of fully autonomous GUI agents.'}, 'zh': {'title': 'Aguvisï¼šå®Œå…¨è‡ªä¸»çš„è§†è§‰GUIä»£ç†', 'desc': 'æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºAguvisçš„æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä»»åŠ¡ã€‚è¯¥æ¡†æ¶åŸºäºçº¯è§†è§‰çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒå¹³å°ä¸Šæ“ä½œï¼Œå…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™æ€§ã€‚Aguvisé€šè¿‡å›¾åƒè§‚å¯Ÿå’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„ç»“åˆï¼Œå¢å¼ºäº†æ¨¡å‹çš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿç‹¬ç«‹å¯¼èˆªå’Œä¸å¤æ‚æ•°å­—ç¯å¢ƒäº’åŠ¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒAguvisåœ¨ç¦»çº¿å’Œåœ¨çº¿åœºæ™¯ä¸­å‡è¶…è¶Šäº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œæˆä¸ºé¦–ä¸ªå®Œå…¨è‡ªä¸»çš„çº¯è§†è§‰GUIä»£ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04468', 'title': 'NVILA: Efficient Frontier Visual Language Models', 'url': 'https://huggingface.co/papers/2412.04468', 'abstract': 'Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This "scale-then-compress" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.', 'score': 29, 'issue_id': 998, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '56c438ada1bc3480', 'authors': ['Zhijian Liu', 'Ligeng Zhu', 'Baifeng Shi', 'Zhuoyang Zhang', 'Yuming Lou', 'Shang Yang', 'Haocheng Xi', 'Shiyi Cao', 'Yuxian Gu', 'Dacheng Li', 'Xiuyu Li', 'Yunhao Fang', 'Yukang Chen', 'Cheng-Yu Hsieh', 'De-An Huang', 'An-Chieh Cheng', 'Vishwesh Nath', 'Jinyi Hu', 'Sifei Liu', 'Ranjay Krishna', 'Daguang Xu', 'Xiaolong Wang', 'Pavlo Molchanov', 'Jan Kautz', 'Hongxu Yin', 'Song Han', 'Yao Lu'], 'affiliations': ['MIT', 'NVIDIA', 'Tsinghua University', 'UC Berkeley', 'UC San Diego', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.04468.jpg', 'data': {'categories': ['#architecture', '#training', '#benchmark', '#video', '#open_source', '#optimization', '#cv'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'NVILA: ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ĞºĞ¾Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ ÑĞµĞ¼ĞµĞ¹ÑÑ‚Ğ²Ğ¾ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ NVILA, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ğ»Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ VILA, ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ² Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ ÑĞ¶Ğ°Ğ² Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹. NVILA Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºÑƒÑ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ… Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. ĞŸÑ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ñ„Ğ°Ğ¹Ğ½-Ñ‚ÑĞ½Ğ¸Ğ½Ğ³Ğµ Ğ¸ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞµ.'}, 'en': {'title': 'NVILA: Efficiency Meets Accuracy in Visual Language Models', 'desc': "This paper presents NVILA, a new family of visual language models (VLMs) that focus on improving both efficiency and accuracy. The authors enhance the existing VILA architecture by increasing spatial and temporal resolutions before compressing visual tokens, which allows NVILA to handle high-resolution images and long videos more effectively. They also perform a comprehensive analysis to boost NVILA's efficiency throughout its lifecycle, including training, fine-tuning, and deployment. As a result, NVILA achieves competitive accuracy while significantly reducing training and operational costs compared to other leading VLMs."}, 'zh': {'title': 'é«˜æ•ˆä¸å‡†ç¡®å¹¶é‡çš„è§†è§‰è¯­è¨€æ¨¡å‹', 'desc': 'è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰è¿‘å¹´æ¥åœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶æ•ˆç‡å´å—åˆ°è¾ƒå°‘å…³æ³¨ã€‚æœ¬æ–‡ä»‹ç»äº†NVILAï¼Œè¿™æ˜¯ä¸€ç³»åˆ—æ—¨åœ¨ä¼˜åŒ–æ•ˆç‡å’Œå‡†ç¡®æ€§çš„å¼€æ”¾å¼VLMã€‚æˆ‘ä»¬é€šè¿‡å…ˆæé«˜ç©ºé—´å’Œæ—¶é—´åˆ†è¾¨ç‡ï¼Œç„¶åå‹ç¼©è§†è§‰æ ‡è®°ï¼Œæ”¹è¿›äº†VILAçš„æ¨¡å‹æ¶æ„ã€‚è¿™ç§â€œå…ˆæ‰©å±•åå‹ç¼©â€çš„æ–¹æ³•ä½¿NVILAèƒ½å¤Ÿé«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒå’Œé•¿è§†é¢‘ï¼ŒåŒæ—¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ä¸é¢†å…ˆçš„VLMç›¸æ¯”ï¼Œå‡†ç¡®æ€§ç›¸å½“æˆ–æ›´é«˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03679', 'title': 'Evaluating Language Models as Synthetic Data Generators', 'url': 'https://huggingface.co/papers/2412.03679', 'abstract': "Given the increasing use of synthetic data in language model (LM) post-training, an LM's ability to generate high-quality data has become nearly as crucial as its ability to solve problems directly. While prior works have focused on developing effective data generation methods, they lack systematic comparison of different LMs as data generators in a unified setting. To address this gap, we propose AgoraBench, a benchmark that provides standardized settings and metrics to evaluate LMs' data generation abilities. Through synthesizing 1.26 million training instances using 6 LMs and training 99 student models, we uncover key insights about LMs' data generation capabilities. First, we observe that LMs exhibit distinct strengths. For instance, GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones. Furthermore, our analysis reveals that an LM's data generation ability doesn't necessarily correlate with its problem-solving ability. Instead, multiple intrinsic features of data quality-including response quality, perplexity, and instruction difficulty-collectively serve as better indicators. Finally, we demonstrate that strategic choices in output format and cost-conscious model selection significantly impact data generation effectiveness.", 'score': 25, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '2b80634d3f712590', 'authors': ['Seungone Kim', 'Juyoung Suk', 'Xiang Yue', 'Vijay Viswanathan', 'Seongyun Lee', 'Yizhong Wang', 'Kiril Gashteovski', 'Carolin Lawrence', 'Sean Welleck', 'Graham Neubig'], 'affiliations': ['Carnegie Mellon University', 'KAIST AI', 'NEC Laboratories Europe', 'Ss. Cyril and Methodius University of Skopje', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.03679.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#synthetic', '#data'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'AgoraBench: ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ AgoraBench - Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚, ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ² 1,26 Ğ¼Ğ¸Ğ»Ğ»Ğ¸Ğ¾Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ 6 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¯Ğœ Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ğ² 99 ÑÑ‚ÑƒĞ´ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ¯Ğœ Ğ¸Ğ¼ĞµÑÑ‚ ÑĞ²Ğ¾Ğ¸ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¸ ÑÑ‚Ğ° ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğµ Ğ²ÑĞµĞ³Ğ´Ğ° ĞºĞ¾Ñ€Ñ€ĞµĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ‚Ğ°ĞºĞ¶Ğµ Ğ²Ñ‹ÑĞ²Ğ¸Ğ»Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹, Ğ²Ğ»Ğ¸ÑÑÑ‰Ğ¸Ğµ Ğ½Ğ° ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¸ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.'}, 'en': {'title': 'Unlocking the Power of Language Models in Data Generation', 'desc': "This paper introduces AgoraBench, a benchmark designed to evaluate the data generation capabilities of various language models (LMs). It highlights the importance of synthetic data in enhancing the performance of LMs, especially in post-training scenarios. The study synthesizes a large dataset using six different LMs and trains 99 student models to analyze their data generation strengths and weaknesses. Key findings indicate that while some LMs excel in creating new problems, others are better at refining existing ones, and that data generation quality is influenced by several intrinsic features rather than just the LM's problem-solving skills."}, 'zh': {'title': 'è¯„ä¼°è¯­è¨€æ¨¡å‹æ•°æ®ç”Ÿæˆèƒ½åŠ›çš„æ–°åŸºå‡†', 'desc': 'éšç€åˆæˆæ•°æ®åœ¨è¯­è¨€æ¨¡å‹åæœŸè®­ç»ƒä¸­çš„ä½¿ç”¨å¢åŠ ï¼Œè¯­è¨€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ•°æ®çš„èƒ½åŠ›å˜å¾—ä¸ç›´æ¥è§£å†³é—®é¢˜çš„èƒ½åŠ›åŒæ ·é‡è¦ã€‚ä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼€å‘æœ‰æ•ˆçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œä½†ç¼ºä¹å¯¹ä¸åŒè¯­è¨€æ¨¡å‹ä½œä¸ºæ•°æ®ç”Ÿæˆå™¨çš„ç³»ç»Ÿæ¯”è¾ƒã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AgoraBenchï¼Œä¸€ä¸ªæä¾›æ ‡å‡†åŒ–è®¾ç½®å’ŒæŒ‡æ ‡çš„åŸºå‡†ï¼Œç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹çš„æ•°æ®ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œä¸åŒè¯­è¨€æ¨¡å‹åœ¨æ•°æ®ç”Ÿæˆæ–¹é¢å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”æ•°æ®ç”Ÿæˆèƒ½åŠ›ä¸è§£å†³é—®é¢˜çš„èƒ½åŠ›å¹¶ä¸æ€»æ˜¯ç›¸å…³ï¼Œè€Œæ˜¯ä¸æ•°æ®è´¨é‡çš„å¤šä¸ªå†…åœ¨ç‰¹å¾æœ‰å…³ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03895', 'title': 'A Noise is Worth Diffusion Guidance', 'url': 'https://huggingface.co/papers/2412.03895', 'abstract': "Diffusion models excel in generating high-quality images. However, current diffusion models struggle to produce reliable images without guidance methods, such as classifier-free guidance (CFG). Are guidance methods truly necessary? Observing that noise obtained via diffusion inversion can reconstruct high-quality images without guidance, we focus on the initial noise of the denoising pipeline. By mapping Gaussian noise to `guidance-free noise', we uncover that small low-magnitude low-frequency components significantly enhance the denoising process, removing the need for guidance and thus improving both inference throughput and memory. Expanding on this, we propose \\ours, a novel method that replaces guidance methods with a single refinement of the initial noise. This refined noise enables high-quality image generation without guidance, within the same diffusion pipeline. Our noise-refining model leverages efficient noise-space learning, achieving rapid convergence and strong performance with just 50K text-image pairs. We validate its effectiveness across diverse metrics and analyze how refined noise can eliminate the need for guidance. See our project page: https://cvlab-kaist.github.io/NoiseRefine/.", 'score': 24, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '1e3bc318f7457d8c', 'authors': ['Donghoon Ahn', 'Jiwon Kang', 'Sanghyun Lee', 'Jaewon Min', 'Minjae Kim', 'Wooseok Jang', 'Hyoungwon Cho', 'Sayak Paul', 'SeonHwa Kim', 'Eunju Cha', 'Kyong Hwan Jin', 'Seungryong Kim'], 'affiliations': ['Hugging Face', 'KAIST', 'Korea University', 'Sookmyung Womens University'], 'pdf_title_img': 'assets/pdf/title_img/2412.03895.jpg', 'data': {'categories': ['#cv', '#optimization', '#diffusion', '#inference'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµÑ…Ğ½Ğ¸Ğº Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ NoiseRefine, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ·Ğ°Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ Ğ¾Ğ´Ğ½Ğ¾ĞºÑ€Ğ°Ñ‚Ğ½Ñ‹Ğ¼ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑˆÑƒĞ¼Ğ°. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ±ĞµĞ· Ğ½Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€. ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ñ ÑˆÑƒĞ¼Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğµ ÑˆÑƒĞ¼Ğ¾Ğ², Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ²ÑĞµĞ³Ğ¾ Ğ½Ğ° 50 Ñ‚Ñ‹ÑÑÑ‡Ğ°Ñ… Ğ¿Ğ°Ñ€ Ñ‚ĞµĞºÑÑ‚-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ.'}, 'en': {'title': 'Guidance-Free Image Generation with Noise Refinement', 'desc': 'This paper investigates the necessity of guidance methods in diffusion models for image generation. It reveals that high-quality images can be produced by refining the initial noise in the denoising process, eliminating the reliance on techniques like classifier-free guidance. The authors introduce a new method called \textit{NoiseRefine}, which enhances the denoising process by focusing on low-magnitude low-frequency components of noise. Their approach demonstrates improved efficiency in image generation, achieving strong results with fewer training samples and without the need for additional guidance methods.'}, 'zh': {'title': 'æ— æŒ‡å¯¼ç”Ÿæˆé«˜è´¨é‡å›¾åƒçš„æ–°æ–¹æ³•', 'desc': 'æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç›®å‰çš„æ‰©æ•£æ¨¡å‹åœ¨æ²¡æœ‰æŒ‡å¯¼æ–¹æ³•çš„æƒ…å†µä¸‹éš¾ä»¥ç”Ÿæˆå¯é çš„å›¾åƒã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡æ‰©æ•£åæ¼”è·å¾—çš„å™ªå£°å¯ä»¥åœ¨æ²¡æœ‰æŒ‡å¯¼çš„æƒ…å†µä¸‹é‡å»ºé«˜è´¨é‡å›¾åƒï¼Œå› æ­¤æˆ‘ä»¬å…³æ³¨å»å™ªæµç¨‹ä¸­çš„åˆå§‹å™ªå£°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•\textit{ours}ï¼Œé€šè¿‡å¯¹åˆå§‹å™ªå£°è¿›è¡Œå•æ¬¡ç²¾ç‚¼ï¼Œæ›¿ä»£äº†ä¼ ç»Ÿçš„æŒ‡å¯¼æ–¹æ³•ï¼Œä»è€Œåœ¨åŒä¸€æ‰©æ•£æµç¨‹ä¸­å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨é«˜æ•ˆçš„å™ªå£°ç©ºé—´å­¦ä¹ ï¼Œå¿«é€Ÿæ”¶æ•›å¹¶åœ¨ä»…ä½¿ç”¨50Kæ–‡æœ¬-å›¾åƒå¯¹çš„æƒ…å†µä¸‹å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01506', 'title': 'Structured 3D Latents for Scalable and Versatile 3D Generation', 'url': 'https://huggingface.co/papers/2412.01506', 'abstract': 'We introduce a novel 3D generation method for versatile and high-quality 3D asset creation. The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding. We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released.', 'score': 20, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'c35e5586d464b27c', 'authors': ['Jianfeng Xiang', 'Zelong Lv', 'Sicheng Xu', 'Yu Deng', 'Ruicheng Wang', 'Bowen Zhang', 'Dong Chen', 'Xin Tong', 'Jiaolong Yang'], 'affiliations': ['Microsoft Research', 'Tsinghua University', 'USTC'], 'pdf_title_img': 'assets/pdf/title_img/2412.01506.jpg', 'data': {'categories': ['#3d', '#dataset', '#training', '#open_source'], 'emoji': 'ğŸ¨', 'ru': {'title': 'Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°. ĞÑĞ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑƒĞ½Ğ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ Ğ»Ğ°Ñ‚ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ (SLAT), Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰ĞµĞµ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹. SLAT Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½ÑƒÑ 3D-ÑĞµÑ‚ĞºÑƒ Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ°Ğ¼Ğ¸, Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸Ğ· Ğ¼Ğ¾Ñ‰Ğ½Ğ¾Ğ¹ Ñ„ÑƒĞ½Ğ´Ğ°Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 3D-Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ñ‹ Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ SLAT, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· 500 Ñ‚Ñ‹ÑÑÑ‡ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².'}, 'en': {'title': 'Revolutionizing 3D Asset Creation with SLAT!', 'desc': 'This paper presents a new method for creating high-quality 3D assets using a unified Structured LATent (SLAT) representation. The SLAT allows for flexible decoding into various formats like Radiance Fields, 3D Gaussians, and meshes by combining a sparse 3D grid with detailed visual features from a vision model. The authors utilize rectified flow transformers designed for SLAT, training models with up to 2 billion parameters on a large dataset of 500,000 diverse 3D objects. The results show significant improvements in quality and versatility, enabling local 3D editing and output format selection that previous models could not achieve.'}, 'zh': {'title': 'çµæ´»é«˜æ•ˆçš„3Dèµ„äº§ç”Ÿæˆæ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„3Dç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºå¤šåŠŸèƒ½å’Œé«˜è´¨é‡çš„3Dèµ„äº§åˆ›å»ºã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ç»Ÿä¸€çš„ç»“æ„åŒ–æ½œåœ¨(SLAT)è¡¨ç¤ºï¼Œèƒ½å¤Ÿè§£ç ä¸ºä¸åŒçš„è¾“å‡ºæ ¼å¼ï¼Œå¦‚è¾å°„åœºã€3Dé«˜æ–¯å’Œç½‘æ ¼ã€‚é€šè¿‡å°†ç¨€ç–çš„3Dç½‘æ ¼ä¸ä»å¼ºå¤§çš„è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–çš„å¯†é›†å¤šè§†è§’è§†è§‰ç‰¹å¾ç›¸ç»“åˆï¼Œæˆ‘ä»¬å…¨é¢æ•æ‰äº†ç»“æ„ï¼ˆå‡ ä½•ï¼‰å’Œçº¹ç†ï¼ˆå¤–è§‚ï¼‰ä¿¡æ¯ï¼ŒåŒæ—¶åœ¨è§£ç è¿‡ç¨‹ä¸­ä¿æŒçµæ´»æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨é’ˆå¯¹SLATçš„æ•´æµæµå˜æ¢å™¨è¿›è¡Œ3Dç”Ÿæˆï¼Œè®­ç»ƒäº†é«˜è¾¾20äº¿å‚æ•°çš„æ¨¡å‹ï¼Œç”Ÿæˆçš„ç»“æœåœ¨æ–‡æœ¬æˆ–å›¾åƒæ¡ä»¶ä¸‹çš„è´¨é‡æ˜¾è‘—è¶…è¿‡ç°æœ‰æ–¹æ³•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01339', 'title': 'Negative Token Merging: Image-based Adversarial Feature Guidance', 'url': 'https://huggingface.co/papers/2412.01339', 'abstract': 'Text-based adversarial guidance using a negative prompt has emerged as a widely adopted approach to push the output features away from undesired concepts. While useful, performing adversarial guidance using text alone can be insufficient to capture complex visual concepts and avoid undesired visual elements like copyrighted characters. In this paper, for the first time we explore an alternate modality in this direction by performing adversarial guidance directly using visual features from a reference image or other images in a batch. In particular, we introduce negative token merging (NegToMe), a simple but effective training-free approach which performs adversarial guidance by selectively pushing apart matching semantic features (between reference and output generation) during the reverse diffusion process. When used w.r.t. other images in the same batch, we observe that NegToMe significantly increases output diversity (racial, gender, visual) without sacrificing output image quality. Similarly, when used w.r.t. a reference copyrighted asset, NegToMe helps reduce visual similarity with copyrighted content by 34.57%. NegToMe is simple to implement using just few-lines of code, uses only marginally higher (<4%) inference times and generalizes to different diffusion architectures like Flux, which do not natively support the use of a separate negative prompt. Code is available at https://negtome.github.io', 'score': 19, 'issue_id': 982, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '2cfa8e0ec05b9ae1', 'authors': ['Jaskirat Singh', 'Lindsey Li', 'Weijia Shi', 'Ranjay Krishna', 'Yejin Choi', 'Pang Wei Koh', 'Michael F. Cohen', 'Stephen Gould', 'Liang Zheng', 'Luke Zettlemoyer'], 'affiliations': ['Allen Institute for AI', 'Australian National University', 'University of Washington'], 'pdf_title_img': 'assets/pdf/title_img/2412.01339.jpg', 'data': {'categories': ['#training', '#cv', '#ethics', '#multimodal', '#security', '#diffusion'], 'emoji': 'ğŸ¨', 'ru': {'title': 'NegToMe: Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ğ´Ğ»Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ”Ğ°Ğ½Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ NegToMe (negative token merging) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Ğ’ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ğµ Ğ¾Ñ‚ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ¾Ğ², NegToMe Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ñ€ĞµÑ„ĞµÑ€ĞµĞ½ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ğ¸Ñ… ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¼ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ°Ğ¼Ğ¸. NegToMe Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ»ĞµĞ³ĞºĞ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½ Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Enhancing Image Diversity and Copyright Safety with NegToMe', 'desc': 'This paper introduces a new method called Negative Token Merging (NegToMe) for adversarial guidance in image generation. Unlike traditional methods that rely solely on text prompts, NegToMe uses visual features from reference images to steer the output away from unwanted concepts. The approach selectively separates matching semantic features during the reverse diffusion process, enhancing the diversity of generated images while maintaining quality. Additionally, it effectively reduces visual similarity to copyrighted content, making it a practical tool for creators.'}, 'zh': {'title': 'é€šè¿‡è§†è§‰ç‰¹å¾å®ç°å¯¹æŠ—å¼•å¯¼çš„åˆ›æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¯¹æŠ—å¼•å¯¼æ–¹æ³•ï¼Œç§°ä¸ºè´Ÿä»¤ç‰Œåˆå¹¶ï¼ˆNegToMeï¼‰ï¼Œé€šè¿‡ç›´æ¥ä½¿ç”¨å‚è€ƒå›¾åƒçš„è§†è§‰ç‰¹å¾æ¥æ¨åŠ¨è¾“å‡ºç‰¹å¾è¿œç¦»ä¸å¸Œæœ›çš„æ¦‚å¿µã€‚ä¸ä»…ä½¿ç”¨æ–‡æœ¬è¿›è¡Œå¯¹æŠ—å¼•å¯¼ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å¤æ‚çš„è§†è§‰æ¦‚å¿µï¼Œå¹¶æœ‰æ•ˆé¿å…ç‰ˆæƒè§’è‰²ç­‰ä¸å¸Œæœ›çš„è§†è§‰å…ƒç´ ã€‚å®éªŒè¡¨æ˜ï¼ŒNegToMeæ˜¾è‘—æé«˜äº†è¾“å‡ºçš„å¤šæ ·æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å›¾åƒè´¨é‡ï¼Œå¹¶ä¸”åœ¨å‡å°‘ä¸ç‰ˆæƒå†…å®¹çš„è§†è§‰ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¯¥æ–¹æ³•å®ç°ç®€å•ï¼Œä»£ç é‡å°‘ï¼Œä¸”å¯¹ä¸åŒçš„æ‰©æ•£æ¶æ„å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04146', 'title': 'AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models', 'url': 'https://huggingface.co/papers/2412.04146', 'abstract': 'Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results.', 'score': 14, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'ed058640f1a96005', 'authors': ['Xinghui Li', 'Qichao Sun', 'Pengze Zhang', 'Fulong Ye', 'Zhichao Liao', 'Wanquan Feng', 'Songtao Zhao', 'Qian He'], 'affiliations': ['ByteDance', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04146.jpg', 'data': {'categories': ['#diffusion', '#games', '#cv', '#multimodal'], 'emoji': 'ğŸ‘š', 'ru': {'title': 'AnyDressing: Ğ’Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ° Ğ»ÑĞ±Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ AnyDressing Ğ´Ğ»Ñ Ğ²Ğ¸Ñ€Ñ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ĞºĞ¸ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞœĞµÑ‚Ğ¾Ğ´ ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹: GarmentsNet Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹ Ğ¸ DressingNet Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ’ GarmentsNet Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Garment-Specific Feature Extractor Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹. DressingNet Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Dressing-Attention Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Instance-Level Garment Localization Learning Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ´ĞµĞ¶Ğ´Ñ‹.'}, 'en': {'title': 'AnyDressing: Revolutionizing Multi-Garment Image Generation', 'desc': 'This paper introduces AnyDressing, a novel method for Multi-Garment Virtual Dressing that enhances image generation from text and image prompts. It features two main networks: GarmentsNet, which extracts detailed clothing features, and DressingNet, which generates customized images while maintaining text-image consistency. The method includes a Garment-Specific Feature Extractor to efficiently encode garment textures and an adaptive Dressing-Attention mechanism to accurately place multi-garment features. Overall, AnyDressing improves the diversity and controllability of synthesized images, achieving state-of-the-art performance in garment-centric image generation.'}, 'zh': {'title': 'AnyDressingï¼šå¤šæœè£…è™šæ‹Ÿç©¿è¡£çš„æ–°æ–¹æ³•', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šæœè£…è™šæ‹Ÿç©¿è¡£ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†AnyDressingæ–¹æ³•ï¼Œä»¥æ”¯æŒä»»æ„ç»„åˆçš„æœè£…å’Œä¸ªæ€§åŒ–æ–‡æœ¬æç¤ºã€‚AnyDressingåŒ…å«ä¸¤ä¸ªä¸»è¦ç½‘ç»œï¼šGarmentsNetç”¨äºæå–æœè£…ç»†èŠ‚ç‰¹å¾ï¼ŒDressingNetç”¨äºç”Ÿæˆå®šåˆ¶å›¾åƒã€‚æˆ‘ä»¬è®¾è®¡äº†é«˜æ•ˆçš„æœè£…ç‰¹å¾æå–æ¨¡å—ï¼Œé¿å…äº†æœè£…æ··æ·†ï¼ŒåŒæ—¶æé«˜äº†ç½‘ç»œæ•ˆç‡ã€‚æ­¤å¤–ï¼ŒDressingNetä¸­çš„è‡ªé€‚åº”æ³¨æ„æœºåˆ¶å’Œå®ä¾‹çº§æœè£…å®šä½å­¦ä¹ ç­–ç•¥ï¼Œç¡®ä¿äº†å¤šæœè£…ç‰¹å¾çš„å‡†ç¡®æ³¨å…¥ï¼Œä»è€Œæå‡äº†ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03632', 'title': 'MV-Adapter: Multi-view Consistent Image Generation Made Easy', 'url': 'https://huggingface.co/papers/2412.03632', 'abstract': 'Existing multi-view image generation methods often make invasive modifications to pre-trained text-to-image (T2I) models and require full fine-tuning, leading to (1) high computational costs, especially with large base models and high-resolution images, and (2) degradation in image quality due to optimization difficulties and scarce high-quality 3D data. In this paper, we propose the first adapter-based solution for multi-view image generation, and introduce MV-Adapter, a versatile plug-and-play adapter that enhances T2I models and their derivatives without altering the original network structure or feature space. By updating fewer parameters, MV-Adapter enables efficient training and preserves the prior knowledge embedded in pre-trained models, mitigating overfitting risks. To efficiently model the 3D geometric knowledge within the adapter, we introduce innovative designs that include duplicated self-attention layers and parallel attention architecture, enabling the adapter to inherit the powerful priors of the pre-trained models to model the novel 3D knowledge. Moreover, we present a unified condition encoder that seamlessly integrates camera parameters and geometric information, facilitating applications such as text- and image-based 3D generation and texturing. MV-Adapter achieves multi-view generation at 768 resolution on Stable Diffusion XL (SDXL), and demonstrates adaptability and versatility. It can also be extended to arbitrary view generation, enabling broader applications. We demonstrate that MV-Adapter sets a new quality standard for multi-view image generation, and opens up new possibilities due to its efficiency, adaptability and versatility.', 'score': 14, 'issue_id': 981, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'fe0891c026484abc', 'authors': ['Zehuan Huang', 'Yuan-Chen Guo', 'Haoran Wang', 'Ran Yi', 'Lizhuang Ma', 'Yan-Pei Cao', 'Lu Sheng'], 'affiliations': ['School of Software, Beihang University', 'Shanghai Jiao Tong University', 'VAST'], 'pdf_title_img': 'assets/pdf/title_img/2412.03632.jpg', 'data': {'categories': ['#training', '#synthetic', '#optimization', '#architecture', '#cv', '#3d'], 'emoji': 'ğŸ–¼ï¸', 'ru': {'title': 'MV-Adapter: Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ MV-Adapter - Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€Ğ½Ğ¾Ğµ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ»Ğ°Ğ³Ğ¸Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ½Ğ¸Ğ¶Ğ°Ñ Ñ€Ğ¸ÑĞº Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ. MV-Adapter Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ñ‹, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ´Ğ»Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ 3D-Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ğ¸. Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ñ€Ğ°ĞºÑƒÑ€ÑĞ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸ĞµĞ¼ 768 Ğ¿Ğ¸ĞºÑĞµĞ»ĞµĞ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Stable Diffusion XL.'}, 'en': {'title': 'Efficient Multi-View Image Generation with MV-Adapter', 'desc': 'This paper introduces MV-Adapter, a novel adapter-based approach for multi-view image generation that avoids invasive changes to pre-trained text-to-image (T2I) models. By updating fewer parameters, MV-Adapter enhances the efficiency of training while preserving the knowledge from pre-trained models, thus reducing the risk of overfitting. The design incorporates advanced features like duplicated self-attention layers and a unified condition encoder to effectively integrate 3D geometric information. MV-Adapter not only achieves high-quality multi-view generation but also allows for broader applications in arbitrary view generation.'}, 'zh': {'title': 'é«˜æ•ˆå¤šè§†è§’å›¾åƒç”Ÿæˆçš„æ–°æ ‡å‡†', 'desc': 'ç°æœ‰çš„å¤šè§†è§’å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦å¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹è¿›è¡Œå¤§å¹…ä¿®æ”¹ï¼Œå¹¶ä¸”éœ€è¦å®Œå…¨å¾®è°ƒï¼Œè¿™å¯¼è‡´äº†é«˜è®¡ç®—æˆæœ¬å’Œå›¾åƒè´¨é‡ä¸‹é™ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€‚é…å™¨çš„å¤šè§†è§’å›¾åƒç”Ÿæˆè§£å†³æ–¹æ¡ˆï¼ŒMV-Adapteræ˜¯ä¸€ç§å¯æ’æ‹”çš„é€‚é…å™¨ï¼Œå¯ä»¥å¢å¼ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè€Œæ— éœ€æ”¹å˜åŸå§‹ç½‘ç»œç»“æ„ã€‚é€šè¿‡æ›´æ–°æ›´å°‘çš„å‚æ•°ï¼ŒMV-Adapterå®ç°äº†é«˜æ•ˆè®­ç»ƒï¼Œå¹¶ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å…ˆéªŒçŸ¥è¯†ï¼Œé™ä½äº†è¿‡æ‹Ÿåˆé£é™©ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ç»Ÿä¸€çš„æ¡ä»¶ç¼–ç å™¨ï¼Œèƒ½å¤Ÿæ— ç¼æ•´åˆç›¸æœºå‚æ•°å’Œå‡ ä½•ä¿¡æ¯ï¼Œä¿ƒè¿›æ–‡æœ¬å’Œå›¾åƒåŸºç¡€çš„3Dç”Ÿæˆå’Œçº¹ç†å¤„ç†ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04431', 'title': 'Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis', 'url': 'https://huggingface.co/papers/2412.04431', 'abstract': 'We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.', 'score': 13, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'f297f288187d4cc4', 'authors': ['Jian Han', 'Jinlai Liu', 'Yi Jiang', 'Bin Yan', 'Yuqi Zhang', 'Zehuan Yuan', 'Bingyue Peng', 'Xiaobing Liu'], 'affiliations': ['ByteDance'], 'pdf_title_img': 'assets/pdf/title_img/2412.04431.jpg', 'data': {'categories': ['#open_source', '#diffusion', '#multimodal', '#cv', '#benchmark', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'Infinity: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼', 'desc': 'ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Infinity - Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾Ñ€ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ. Infinity Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ±ĞµÑĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¸ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ±Ğ¸Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ÑĞ°Ğ¼Ğ¾ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ²ĞµĞ´ÑƒÑ‰Ğ¸Ğµ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ğ¾ Ñ€ÑĞ´Ñƒ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ GenEval Ğ¸ ImageReward. Infinity Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ 1024x1024 Ğ·Ğ° 0.8 ÑĞµĞºÑƒĞ½Ğ´Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ ĞµĞµ ÑĞ°Ğ¼Ğ¾Ğ¹ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ text-to-image Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚.'}, 'en': {'title': 'Infinity: Redefining Text-to-Image Generation with Infinite Vocabulary', 'desc': 'Infinity is a novel Bitwise Visual AutoRegressive Model designed to create high-resolution, photorealistic images based on language instructions. It introduces an infinite-vocabulary tokenizer and classifier, along with a bitwise self-correction mechanism, which enhances the detail and quality of generated images. By scaling both the tokenizer and transformer sizes, Infinity significantly improves upon traditional visual autoregressive models. It sets new performance records in text-to-image generation, outperforming leading diffusion models and achieving faster generation times without additional optimization.'}, 'zh': {'title': 'Infinityï¼šæ— é™å¯èƒ½çš„è§†è§‰ç”Ÿæˆæ¨¡å‹', 'desc': 'æœ¬æ–‡ä»‹ç»äº†Infinityï¼Œä¸€ç§åŸºäºä½çš„è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆé«˜åˆ†è¾¨ç‡ã€é€¼çœŸçš„å›¾åƒã€‚Infinityåœ¨ä½å…ƒä»¤ç‰Œé¢„æµ‹æ¡†æ¶ä¸‹é‡æ–°å®šä¹‰äº†è§†è§‰è‡ªå›å½’æ¨¡å‹ï¼Œé‡‡ç”¨æ— é™è¯æ±‡é‡çš„ä»¤ç‰Œå™¨å’Œåˆ†ç±»å™¨ï¼Œä»¥åŠä½å…ƒè‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆèƒ½åŠ›å’Œç»†èŠ‚è¡¨ç°ã€‚é€šè¿‡ç†è®ºä¸Šå°†ä»¤ç‰Œå™¨çš„è¯æ±‡é‡æ‰©å±•åˆ°æ— é™ï¼Œå¹¶åŒæ—¶æ‰©å±•å˜æ¢å™¨çš„è§„æ¨¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸æ¯”ä¼ ç»Ÿçš„VARæ¨¡å‹é‡Šæ”¾äº†å¼ºå¤§çš„æ‰©å±•èƒ½åŠ›ã€‚Infinityåœ¨è‡ªå›å½’æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­åˆ›ä¸‹æ–°çºªå½•ï¼Œè¶…è¶Šäº†é¡¶çº§æ‰©æ•£æ¨¡å‹ï¼Œå¦‚SD3-Mediumå’ŒSDXLã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04280', 'title': 'HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing', 'url': 'https://huggingface.co/papers/2412.04280', 'abstract': 'We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution 1024 times 1024 content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.', 'score': 12, 'issue_id': 980, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4467ff5ceea9cb1d', 'authors': ['Jinbin Bai', 'Wei Chow', 'Ling Yang', 'Xiangtai Li', 'Juncheng Li', 'Hanwang Zhang', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Peking University', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04280.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#data'], 'emoji': 'ğŸ¨', 'ru': {'title': 'HumanEdit: Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸Ğ¼ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ¼', 'desc': 'HumanEdit - ÑÑ‚Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¹, ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ñ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸ĞµĞ¼ Ğ»ÑĞ´ĞµĞ¹. ĞĞ½ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ 5751 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ñ Ğ¼Ğ°ÑĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸ÑĞ¼Ğ¸ ÑˆĞµÑÑ‚Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²: Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğµ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ, Ğ¿Ğ¾Ğ´ÑÑ‡ĞµÑ‚, Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ, ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ğ¼ĞµĞ½Ğ°. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ĞµÑ‚ÑÑ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¸ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸ĞµĞ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ğ»ÑÑ Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸ĞµĞ¼ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ¸ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ². HumanEdit Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹.'}, 'en': {'title': 'HumanEdit: Elevating Image Editing with Human-Centric Instructions', 'desc': 'HumanEdit is a newly created dataset aimed at improving instruction-guided image editing by incorporating significant human feedback. Unlike previous datasets that lacked sufficient human input, HumanEdit utilizes human annotators to create data pairs and provide valuable feedback, ensuring alignment with human preferences. The dataset consists of 5,751 high-resolution images and includes six types of editing instructions, allowing for a wide range of image manipulation tasks. By offering detailed instructions and masks, HumanEdit sets a new standard for versatility and accuracy in image editing research.'}, 'zh': {'title': 'HumanEditï¼šç²¾å‡†å¤šæ ·çš„å›¾åƒç¼–è¾‘æ•°æ®é›†', 'desc': 'HumanEditæ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„äººç±»å¥–åŠ±æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºæŒ‡å¯¼å›¾åƒç¼–è¾‘ã€‚ä¸ä»¥å¾€çš„å¤§è§„æ¨¡ç¼–è¾‘æ•°æ®é›†ä¸åŒï¼ŒHumanEdité€šè¿‡äººç±»æ³¨é‡Šè€…æ„å»ºæ•°æ®å¯¹ï¼Œå¹¶ç”±ç®¡ç†å‘˜æä¾›åé¦ˆï¼Œç¡®ä¿æ•°æ®ä¸äººç±»åå¥½çš„å¯¹é½ã€‚è¯¥æ•°æ®é›†åŒ…å«5751å¼ å›¾åƒï¼Œç»è¿‡2500å¤šä¸ªå°æ—¶çš„äººå·¥åŠªåŠ›ï¼Œæ¶µç›–å…­ç§ä¸åŒç±»å‹çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œæ”¯æŒå¤šæ ·åŒ–çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚HumanEditä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„åŸºå‡†ï¼Œæ¨åŠ¨å›¾åƒç¼–è¾‘é¢†åŸŸçš„å‘å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04315', 'title': 'Densing Law of LLMs', 'url': 'https://huggingface.co/papers/2412.04315', 'abstract': "Large Language Models (LLMs) have emerged as a milestone in artificial intelligence, and their performance can improve as the model size increases. However, this scaling brings great challenges to training and inference efficiency, particularly for deploying LLMs in resource-constrained environments, and the scaling trend is becoming increasingly unsustainable. This paper introduces the concept of ``capacity density'' as a new metric to evaluate the quality of the LLMs across different scales and describes the trend of LLMs in terms of both effectiveness and efficiency. To calculate the capacity density of a given target LLM, we first introduce a set of reference models and develop a scaling law to predict the downstream performance of these reference models based on their parameter sizes. We then define the effective parameter size of the target LLM as the parameter size required by a reference model to achieve equivalent performance, and formalize the capacity density as the ratio of the effective parameter size to the actual parameter size of the target LLM. Capacity density provides a unified framework for assessing both model effectiveness and efficiency. Our further analysis of recent open-source base LLMs reveals an empirical law (the densing law)that the capacity density of LLMs grows exponentially over time. More specifically, using some widely used benchmarks for evaluation, the capacity density of LLMs doubles approximately every three months. The law provides new perspectives to guide future LLM development, emphasizing the importance of improving capacity density to achieve optimal results with minimal computational overhead.", 'score': 11, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4a5189762d711a19', 'authors': ['Chaojun Xiao', 'Jie Cai', 'Weilin Zhao', 'Guoyang Zeng', 'Xu Han', 'Zhiyuan Liu', 'Maosong Sun'], 'affiliations': ['ModelBest Inc.', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04315.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#optimization', '#architecture', '#inference'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'ĞŸĞ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸: Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ Ğº ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼', 'desc': "Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºÑƒ 'Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸' Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM) Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ°. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ²Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ĞºĞ°Ğº Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğº Ñ„Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼Ñƒ. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… LLM Ğ²Ñ‹ÑĞ²Ğ¸Ğ» ÑĞ¼Ğ¿Ğ¸Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°ĞºĞ¾Ğ½ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ€Ğ¾ÑÑ‚Ğ° Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½ĞµĞ¼. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ğ·Ğ³Ğ»ÑĞ´ Ğ½Ğ° Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ğµ LLM, Ğ¿Ğ¾Ğ´Ñ‡ĞµÑ€ĞºĞ¸Ğ²Ğ°Ñ Ğ²Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ°Ñ…."}, 'en': {'title': 'Maximizing Performance with Minimal Resources: The Capacity Density Revolution', 'desc': "This paper discusses the challenges of training and using large language models (LLMs) as they grow in size. It introduces a new metric called 'capacity density' to evaluate LLMs based on their effectiveness and efficiency. The authors develop a scaling law to predict how well different models perform relative to their size and define capacity density as the ratio of effective parameter size to actual parameter size. Their findings suggest that the capacity density of LLMs is increasing rapidly, which could guide future improvements in model design to optimize performance while reducing resource use."}, 'zh': {'title': 'æå‡å®¹é‡å¯†åº¦ï¼Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ•ˆç‡ä¸æ•ˆæœ', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†é‡è¦è¿›å±•ï¼Œä½†éšç€æ¨¡å‹è§„æ¨¡çš„å¢åŠ ï¼Œè®­ç»ƒå’Œæ¨ç†çš„æ•ˆç‡é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†â€œå®¹é‡å¯†åº¦â€è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°ä¸åŒè§„æ¨¡LLMsçš„è´¨é‡ï¼Œå¹¶æè¿°äº†LLMsåœ¨æœ‰æ•ˆæ€§å’Œæ•ˆç‡æ–¹é¢çš„è¶‹åŠ¿ã€‚é€šè¿‡å¼•å…¥ä¸€ç»„å‚è€ƒæ¨¡å‹å¹¶å¼€å‘ç¼©æ”¾æ³•åˆ™ï¼Œæœ¬æ–‡è®¡ç®—äº†ç›®æ ‡LLMçš„æœ‰æ•ˆå‚æ•°å¤§å°ï¼Œå¹¶å°†å®¹é‡å¯†åº¦å®šä¹‰ä¸ºæœ‰æ•ˆå‚æ•°å¤§å°ä¸å®é™…å‚æ•°å¤§å°çš„æ¯”ç‡ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ŒLLMsçš„å®¹é‡å¯†åº¦æ¯ä¸‰ä¸ªæœˆå¤§çº¦ç¿»å€ï¼Œä¸ºæœªæ¥çš„LLMå¼€å‘æä¾›äº†æ–°çš„è§†è§’ï¼Œå¼ºè°ƒäº†æé«˜å®¹é‡å¯†åº¦çš„é‡è¦æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04378', 'title': 'Discriminative Fine-tuning of LVLMs', 'url': 'https://huggingface.co/papers/2412.04378', 'abstract': 'Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.   In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.   Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework\'s components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.', 'score': 9, 'issue_id': 989, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '6d05f8d3eedf64ed', 'authors': ['Yassine Ouali', 'Adrian Bulat', 'Alexandros Xenos', 'Anestis Zaganidis', 'Ioannis Maniadis Metaxas', 'Georgios Tzimiropoulos', 'Brais Martinez'], 'affiliations': ['Queen Mary University of London', 'Samsung AI Cambridge', 'Technical University of Iasi'], 'pdf_title_img': 'assets/pdf/title_img/2412.04378.jpg', 'data': {'categories': ['#architecture', '#cv', '#reasoning', '#benchmark', '#optimization', '#multimodal', '#training'], 'emoji': 'ğŸ”', 'ru': {'title': 'ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ¼Ğ¸Ñ€Ğ¾Ğ²: Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ LVLM Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ°', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµÑ‚ÑÑ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾-ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LVLM) Ğ´Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€Ğ¸Ğ¼Ğ¸Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‚ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… LVLM, ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸ĞµĞ¼ ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğº ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑ…ĞµĞ¼Ñƒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ÑÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¸ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑĞ¼Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½ÑƒÑ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ². Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸ Ñ‚Ğ¸Ğ¿Ğ° CLIP Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¿Ğ¾Ğ¸ÑĞºĞ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ¾ Ñ‚ĞµĞºÑÑ‚Ñƒ Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Unlocking Discriminative Power in Vision-Language Models', 'desc': 'This paper introduces a new training method for Large Vision-Language Models (LVLMs) that enhances their ability to understand and discriminate between images and text. By combining contrastive learning with next-token prediction, the model achieves better performance in tasks requiring detailed vision-language reasoning. The authors also present a parameter-efficient adaptation technique that utilizes soft prompting and LoRA adapters, making the model more efficient. Overall, this work demonstrates significant improvements over existing models like CLIP, particularly in image-text retrieval and compositional understanding.'}, 'zh': {'title': 'ç»“åˆä¼˜åŠ¿ï¼Œæå‡è§†è§‰è¯­è¨€æ¨¡å‹çš„åŒºåˆ†èƒ½åŠ›', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç”¨äºå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰è¿›è¡ŒåŒºåˆ†æ€§å¾®è°ƒï¼Œä»è€Œæé«˜å…¶åœ¨å›¾åƒ-æ–‡æœ¬ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç»“åˆäº†å¯¹æ¯”å­¦ä¹ å’Œä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹æŸå¤±ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£è¯­è¨€å¹¶è¿›è¡Œå›¾åƒ-æ–‡æœ¬çš„åŒºåˆ†ã€‚é€šè¿‡ä½¿ç”¨å¯å˜é•¿åº¦å’Œç²’åº¦çš„å›¾åƒ-æ–‡æœ¬å¯¹è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬çš„æ¡†æ¶å±•ç¤ºäº†å…¶å„ä¸ªç»„æˆéƒ¨åˆ†çš„å¿…è¦æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†çš„å›¾åƒ-æ–‡æœ¬æ£€ç´¢åŸºå‡†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰çš„CLIPç±»æ¨¡å‹ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03304', 'title': 'Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation', 'url': 'https://huggingface.co/papers/2412.03304', 'abstract': 'Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artifacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global-MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global-MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.', 'score': 9, 'issue_id': 988, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'f9677fd07f780c7b', 'authors': ['Shivalika Singh', 'Angelika Romanou', 'ClÃ©mentine Fourrier', 'David I. Adelani', 'Jian Gang Ngui', 'Daniel Vila-Suero', 'Peerat Limkonchotiwat', 'Kelly Marchisio', 'Wei Qi Leong', 'Yosephine Susanto', 'Raymond Ng', 'Shayne Longpre', 'Wei-Yin Ko', 'Madeline Smith', 'Antoine Bosselut', 'Alice Oh', 'Andre F. T. Martins', 'Leshem Choshen', 'Daphne Ippolito', 'Enzo Ferrante', 'Marzieh Fadaee', 'Beyza Ermis', 'Sara Hooker'], 'affiliations': ['AI Singapore', 'CONICET & Universidad de Buenos Aires', 'Carnegie Mellon University', 'Cohere', 'Cohere For AI', 'EPFL', 'Hugging Face', 'Instituto Superior TÃ©cnico, Universidade de Lisboa', 'Instituto de TelecomunicaÃ§Ãµes', 'KAIST', 'MIT', 'MIT, MIT-IBM Watson AI Lab', 'Mila, McGill University & Canada CIFAR AI Chair', 'National University of Singapore'], 'pdf_title_img': 'assets/pdf/title_img/2412.03304.jpg', 'data': {'categories': ['#ethics', '#dataset', '#multilingual', '#low_resource', '#machine_translation'], 'emoji': 'ğŸŒ', 'ru': {'title': 'ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ¾Ğ² Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ² Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ğµ ÑÑ‚Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑƒĞ±ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ¾Ğ² ĞºĞ°Ğº Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ², Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ½Ğ°Ğ±Ğ¾Ñ€Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MMLU. Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚, Ñ‡Ñ‚Ğ¾ 28% Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² MMLU Ñ‚Ñ€ĞµĞ±ÑƒÑÑ‚ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ğ¾-ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹, Ğ° 84.9% Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ¿Ğ¾ Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ğ¸ ÑĞ¾ÑÑ€ĞµĞ´Ğ¾Ñ‚Ğ¾Ñ‡ĞµĞ½Ñ‹ Ğ½Ğ° Ğ¡ĞµĞ²ĞµÑ€Ğ½Ğ¾Ğ¹ ĞĞ¼ĞµÑ€Ğ¸ĞºĞµ Ğ¸ Ğ•Ğ²Ñ€Ğ¾Ğ¿Ğµ. Ğ’ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ñ‹Ğ» ÑĞ¾Ğ·Ğ´Ğ°Ğ½ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Global-MMLU, Ğ¾Ñ…Ğ²Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ 42 ÑĞ·Ñ‹ĞºĞ° Ğ¸ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ÑÑ‰Ğ¸Ğ¹ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Bridging Cultural Gaps in Multilingual Machine Learning', 'desc': 'This paper addresses the cultural biases found in multilingual datasets, which hinder their effectiveness as global benchmarks for machine learning models. It highlights how these biases arise not only from language differences but also from the cultural context needed to understand questions, particularly in datasets like MMLU. The authors demonstrate that relying solely on machine translation can lead to significant distortions in model performance and evaluation rankings, especially for questions requiring cultural or geographic knowledge. To combat these issues, they introduce Global-MMLU, a refined dataset that includes culturally sensitive annotations and improved translation quality, enabling more accurate evaluations across 42 languages.'}, 'zh': {'title': 'è§£å†³å¤šè¯­è¨€æ•°æ®é›†ä¸­çš„æ–‡åŒ–åè§', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤šè¯­è¨€æ•°æ®é›†ä¸­å­˜åœ¨çš„æ–‡åŒ–åè§å¯¹å…¨çƒåŸºå‡†çš„å½±å“ã€‚è¿™äº›åè§ä¸ä»…æºäºè¯­è¨€ï¼Œè¿˜æ¶‰åŠåˆ°ç†è§£é—®é¢˜æ‰€éœ€çš„æ–‡åŒ–çŸ¥è¯†ï¼Œé™ä½äº†ç¿»è¯‘æ•°æ®é›†çš„å®ç”¨æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè®¸å¤šé—®é¢˜éœ€è¦æ–‡åŒ–æ•æ„Ÿçš„çŸ¥è¯†ï¼Œå°¤å…¶æ˜¯å…³äºåœ°ç†çŸ¥è¯†çš„é—®é¢˜ï¼Œä¸»è¦é›†ä¸­åœ¨åŒ—ç¾å’Œæ¬§æ´²åœ°åŒºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…å‘å¸ƒäº†Global-MMLUï¼Œä¸€ä¸ªæ”¹è¿›çš„å¤šè¯­è¨€è¯„ä¼°é›†ï¼Œæ¶µç›–42ç§è¯­è¨€ï¼Œå¹¶é€šè¿‡ä¸“ä¸šå’Œç¤¾åŒºæ³¨é‡Šè€…éªŒè¯ç¿»è¯‘è´¨é‡ï¼Œè¯„ä¼°æ–‡åŒ–åè§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.02142', 'title': 'Personalized Multimodal Large Language Models: A Survey', 'url': 'https://huggingface.co/papers/2412.02142', 'abstract': 'Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.', 'score': 9, 'issue_id': 982, 'pub_date': '2024-12-03', 'pub_date_card': {'ru': '3 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 3', 'zh': '12æœˆ3æ—¥'}, 'hash': 'a34ccd5a86ab7840', 'authors': ['Junda Wu', 'Hanjia Lyu', 'Yu Xia', 'Zhehao Zhang', 'Joe Barrow', 'Ishita Kumar', 'Mehrnoosh Mirtaheri', 'Hongjie Chen', 'Ryan A. Rossi', 'Franck Dernoncourt', 'Tong Yu', 'Ruiyi Zhang', 'Jiuxiang Gu', 'Nesreen K. Ahmed', 'Yu Wang', 'Xiang Chen', 'Hanieh Deilamsalehy', 'Namyong Park', 'Sungchul Kim', 'Huanrui Yang', 'Subrata Mitra', 'Zhengmian Hu', 'Nedim Lipka', 'Dang Nguyen', 'Yue Zhao', 'Jiebo Luo', 'Julian McAuley'], 'affiliations': ['Adobe Research', 'Cisco Research', 'Dartmouth College', 'Meta AI', 'University of Arizona', 'University of California, San Diego', 'University of Maryland', 'University of Massachusetts at Amherst', 'University of Oregon', 'University of Rochester', 'University of Southern California', 'Virginia Tech'], 'pdf_title_img': 'assets/pdf/title_img/2412.02142.jpg', 'data': {'categories': ['#training', '#survey', '#multimodal', '#architecture', '#dataset', '#benchmark'], 'emoji': 'ğŸ¤–', 'ru': {'title': 'ĞŸĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ˜Ğ˜: Ğ¾Ñ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ´Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ', 'desc': 'Ğ’ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ¾Ğ±Ğ·Ğ¾Ñ€ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ĞœLLM). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ñ‚Ğ°ĞºÑĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞœLLM Ğ¸ Ğ¾Ğ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ, Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ. Ğ Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ÑÑ‚ÑÑ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ĞœLLM, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ°Ğ±Ğ¾Ñ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¸Ñ… Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ. Ğ’ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ‹ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ² ÑÑ‚Ğ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸.'}, 'en': {'title': 'Personalizing Multimodal Language Models for Enhanced User Experience', 'desc': 'This paper surveys personalized multimodal large language models (MLLMs), which integrate various data types like text, images, and audio for improved task performance. It introduces a taxonomy to categorize personalization techniques, detailing their architectures and training methods. The authors also discuss how these techniques can be adapted or combined, emphasizing their benefits and rationale. Additionally, the paper reviews existing personalization tasks, evaluation metrics, and relevant datasets, while identifying key challenges in the field.'}, 'zh': {'title': 'ä¸ªæ€§åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æœªæ¥ä¹‹è·¯', 'desc': 'å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨æ•´åˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿé«˜æ•ˆå®Œæˆå¤æ‚ä»»åŠ¡ã€‚æœ¬æ–‡å¯¹ä¸ªæ€§åŒ–å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„è°ƒæŸ¥ï¼Œé‡ç‚¹ä»‹ç»äº†å…¶æ¶æ„ã€è®­ç»ƒæ–¹æ³•å’Œåº”ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç›´è§‚çš„åˆ†ç±»æ³•ï¼Œç”¨äºå¯¹ä¸ªæ€§åŒ–MLLMsçš„æŠ€æœ¯è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è®¨è®ºäº†ç›¸åº”çš„æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“äº†ç°æœ‰ç ”ç©¶ä¸­è°ƒæŸ¥çš„ä¸ªæ€§åŒ–ä»»åŠ¡åŠå…¶è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„æŒ‘æˆ˜ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01169', 'title': 'OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows', 'url': 'https://huggingface.co/papers/2412.01169', 'abstract': 'We introduce OmniFlow, a novel generative model designed for any-to-any generation tasks such as text-to-image, text-to-audio, and audio-to-image synthesis. OmniFlow advances the rectified flow (RF) framework used in text-to-image models to handle the joint distribution of multiple modalities. It outperforms previous any-to-any models on a wide range of tasks, such as text-to-image and text-to-audio synthesis. Our work offers three key contributions: First, we extend RF to a multi-modal setting and introduce a novel guidance mechanism, enabling users to flexibly control the alignment between different modalities in the generated outputs. Second, we propose a novel architecture that extends the text-to-image MMDiT architecture of Stable Diffusion 3 and enables audio and text generation. The extended modules can be efficiently pretrained individually and merged with the vanilla text-to-image MMDiT for fine-tuning. Lastly, we conduct a comprehensive study on the design choices of rectified flow transformers for large-scale audio and text generation, providing valuable insights into optimizing performance across diverse modalities. The Code will be available at https://github.com/jacklishufan/OmniFlows.', 'score': 8, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': '7f3df6f7d4733664', 'authors': ['Shufan Li', 'Konstantinos Kallidromitis', 'Akash Gokul', 'Zichun Liao', 'Yusuke Kato', 'Kazuki Kozuka', 'Aditya Grover'], 'affiliations': ['Panasonic AI Research', 'Salesforce AI Research', 'UCLA'], 'pdf_title_img': 'assets/pdf/title_img/2412.01169.jpg', 'data': {'categories': ['#diffusion', '#multimodal', '#training', '#optimization', '#architecture'], 'emoji': 'ğŸ”„', 'ru': {'title': 'OmniFlow: ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸', 'desc': 'OmniFlow - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ»ÑĞ±Ğ¾Ğ¹ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ²-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ„Ñ€ĞµĞ¹Ğ¼Ğ²Ğ¾Ñ€Ğº Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ° (rectified flow) Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ½Ñ‹Ğ¼ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹. OmniFlow Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾Ğ¼ ÑĞ¿ĞµĞºÑ‚Ñ€Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ MMDiT Ğ¸Ğ· Stable Diffusion 3, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¸Ğ·ÑƒÑ‡Ğ°ÑÑ‚ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ¾Ğ² Ñ Ğ²Ñ‹Ğ¿Ñ€ÑĞ¼Ğ»ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°.'}, 'en': {'title': 'OmniFlow: Bridging Modalities for Any-to-Any Generation', 'desc': 'OmniFlow is a new generative model that can create outputs across different types of data, like turning text into images or audio. It builds on the rectified flow (RF) framework, allowing it to understand and generate multiple types of data together. This model not only improves performance on tasks like text-to-image and text-to-audio synthesis but also introduces a way for users to control how different data types relate to each other in the generated results. Additionally, it features a unique architecture that enhances existing models and provides insights into optimizing generative tasks across various modalities.'}, 'zh': {'title': 'OmniFlowï¼šå¤šæ¨¡æ€ç”Ÿæˆçš„çµæ´»è§£å†³æ–¹æ¡ˆ', 'desc': 'OmniFlowæ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨å¤„ç†ä»»æ„åˆ°ä»»æ„çš„ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒã€æ–‡æœ¬åˆ°éŸ³é¢‘å’ŒéŸ³é¢‘åˆ°å›¾åƒçš„åˆæˆã€‚å®ƒåœ¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æ”¹è¿›äº†ä¿®æ­£æµï¼ˆRFï¼‰æ¡†æ¶ï¼Œä»¥å¤„ç†å¤šç§æ¨¡æ€çš„è”åˆåˆ†å¸ƒã€‚OmniFlowåœ¨å¤šç§ä»»åŠ¡ä¸Šè¶…è¶Šäº†ä¹‹å‰çš„ä»»æ„åˆ°ä»»æ„æ¨¡å‹ï¼Œæä¾›äº†çµæ´»çš„æ¨¡æ€å¯¹é½æ§åˆ¶æœºåˆ¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜æ¢è®¨äº†ä¿®æ­£æµå˜æ¢å™¨åœ¨å¤§è§„æ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ç”Ÿæˆä¸­çš„è®¾è®¡é€‰æ‹©ï¼Œä¸ºä¼˜åŒ–ä¸åŒæ¨¡æ€çš„æ€§èƒ½æä¾›äº†å®è´µçš„è§è§£ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04062', 'title': 'ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality', 'url': 'https://huggingface.co/papers/2412.04062', 'abstract': "In this paper, we propose ZipAR, a training-free, plug-and-play parallel decoding framework for accelerating auto-regressive (AR) visual generation. The motivation stems from the observation that images exhibit local structures, and spatially distant regions tend to have minimal interdependence. Given a partially decoded set of visual tokens, in addition to the original next-token prediction scheme in the row dimension, the tokens corresponding to spatially adjacent regions in the column dimension can be decoded in parallel, enabling the ``next-set prediction'' paradigm. By decoding multiple tokens simultaneously in a single forward pass, the number of forward passes required to generate an image is significantly reduced, resulting in a substantial improvement in generation efficiency. Experiments demonstrate that ZipAR can reduce the number of model forward passes by up to 91% on the Emu3-Gen model without requiring any additional retraining.", 'score': 7, 'issue_id': 981, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '44d216e3fa2fb345', 'authors': ['Yefei He', 'Feng Chen', 'Yuanyu He', 'Shaoxuan He', 'Hong Zhou', 'Kaipeng Zhang', 'Bohan Zhuang'], 'affiliations': ['Shanghai AI Laboratory, China', 'The University of Adelaide, Australia', 'Zhejiang University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04062.jpg', 'data': {'categories': ['#optimization', '#training', '#cv'], 'emoji': 'ğŸš€', 'ru': {'title': 'ZipAR: Ğ£ÑĞºĞ¾Ñ€ÑĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼', 'desc': 'ZipAR - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ±ĞµĞ· Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. ĞœĞµÑ‚Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ğ½Ğ° Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸Ğ¼ĞµÑÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ. ZipAR Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹, ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ ÑĞ¼ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑĞ¼, Ñ‡Ñ‚Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ ZipAR Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ° 91% Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Emu3-Gen Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.'}, 'en': {'title': 'Accelerate Image Generation with Parallel Decoding!', 'desc': "This paper introduces ZipAR, a new framework designed to speed up the process of generating images using auto-regressive models without the need for retraining. It leverages the observation that images have local structures, allowing for the parallel decoding of visual tokens in adjacent regions. By implementing a 'next-set prediction' approach, ZipAR can decode multiple tokens at once, significantly cutting down the number of forward passes needed to create an image. Experiments show that this method can reduce forward passes by up to 91%, greatly enhancing generation efficiency."}, 'zh': {'title': 'ZipARï¼šåŠ é€Ÿè‡ªå›å½’è§†è§‰ç”Ÿæˆçš„é«˜æ•ˆè§£ç æ¡†æ¶', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºZipARçš„æ¡†æ¶ï¼Œæ—¨åœ¨åŠ é€Ÿè‡ªå›å½’è§†è§‰ç”Ÿæˆã€‚è¯¥æ¡†æ¶ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œèƒ½å¤Ÿå®ç°å¹¶è¡Œè§£ç ï¼Œåˆ©ç”¨å›¾åƒçš„å±€éƒ¨ç»“æ„ç‰¹æ€§ã€‚é€šè¿‡åœ¨åˆ—ç»´åº¦ä¸Šå¹¶è¡Œè§£ç ç©ºé—´ç›¸é‚»åŒºåŸŸçš„è§†è§‰æ ‡è®°ï¼ŒZipARæ˜¾è‘—å‡å°‘äº†ç”Ÿæˆå›¾åƒæ‰€éœ€çš„å‰å‘ä¼ æ’­æ¬¡æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒZipARåœ¨Emu3-Genæ¨¡å‹ä¸Šå¯ä»¥å°†å‰å‘ä¼ æ’­æ¬¡æ•°å‡å°‘å¤šè¾¾91%ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.01820', 'title': 'Towards Universal Soccer Video Understanding', 'url': 'https://huggingface.co/papers/2412.01820', 'abstract': 'As a globally celebrated sport, soccer has attracted widespread interest from fans all over the world. This paper aims to develop a comprehensive multi-modal framework for soccer video understanding. Specifically, we make the following contributions in this paper: (i) we introduce SoccerReplay-1988, the largest multi-modal soccer dataset to date, featuring videos and detailed annotations from 1,988 complete matches, with an automated annotation pipeline; (ii) we present the first visual-language foundation model in the soccer domain, MatchVision, which leverages spatiotemporal information across soccer videos and excels in various downstream tasks; (iii) we conduct extensive experiments and ablation studies on event classification, commentary generation, and multi-view foul recognition. MatchVision demonstrates state-of-the-art performance on all of them, substantially outperforming existing models, which highlights the superiority of our proposed data and model. We believe that this work will offer a standard paradigm for sports understanding research.', 'score': 7, 'issue_id': 980, 'pub_date': '2024-12-02', 'pub_date_card': {'ru': '2 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 2', 'zh': '12æœˆ2æ—¥'}, 'hash': 'fa9f7e4132ee5026', 'authors': ['Jiayuan Rao', 'Haoning Wu', 'Hao Jiang', 'Ya Zhang', 'Yanfeng Wang', 'Weidi Xie'], 'affiliations': ['Alibaba Group, China', 'CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.01820.jpg', 'data': {'categories': ['#dataset', '#architecture', '#multimodal', '#video'], 'emoji': 'âš½', 'ru': {'title': 'MatchVision: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸ Ğ´Ğ»Ñ Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»Ğ°', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ° Ğ¼Ğ½Ğ¾Ğ³Ğ¾Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MatchVision Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„ÑƒÑ‚Ğ±Ğ¾Ğ»ÑŒĞ½Ñ‹Ñ… Ğ²Ğ¸Ğ´ĞµĞ¾. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½ĞµĞ¹ÑˆĞ¸Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ SoccerReplay-1988, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¸ 1988 Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ¼Ğ°Ñ‚Ñ‡ĞµĞ¹. MatchVision Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾-Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹, Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ² Ğ¸ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°Ğ¼Ğ¸.'}, 'en': {'title': 'Revolutionizing Soccer Video Analysis with MatchVision', 'desc': 'This paper presents a new framework for understanding soccer videos using machine learning. It introduces SoccerReplay-1988, a large dataset containing videos and annotations from nearly 2,000 soccer matches, which helps in training models. The authors also develop MatchVision, a visual-language model that processes spatiotemporal data from soccer videos and performs well in tasks like event classification and commentary generation. The results show that MatchVision outperforms existing models, setting a new standard for research in sports video analysis.'}, 'zh': {'title': 'è¶³çƒè§†é¢‘ç†è§£çš„æ–°æ ‡å‡†', 'desc': 'è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œç”¨äºç†è§£è¶³çƒè§†é¢‘ã€‚æˆ‘ä»¬å¼•å…¥äº†SoccerReplay-1988ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„å¤šæ¨¡æ€è¶³çƒæ•°æ®é›†ï¼ŒåŒ…å«1988åœºå®Œæ•´æ¯”èµ›çš„è§†é¢‘å’Œè¯¦ç»†æ³¨é‡Šã€‚æˆ‘ä»¬è¿˜æå‡ºäº†è¶³çƒé¢†åŸŸçš„é¦–ä¸ªè§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹MatchVisionï¼Œèƒ½å¤Ÿåˆ©ç”¨è¶³çƒè§†é¢‘ä¸­çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œæ¶ˆèç ”ç©¶ï¼ŒMatchVisionåœ¨äº‹ä»¶åˆ†ç±»ã€è¯„è®ºç”Ÿæˆå’Œå¤šè§†è§’çŠ¯è§„è¯†åˆ«ç­‰ä»»åŠ¡ä¸Šå‡è¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬æå‡ºçš„æ•°æ®å’Œæ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04462', 'title': '4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion', 'url': 'https://huggingface.co/papers/2412.04462', 'abstract': 'We propose 4Real-Video, a novel framework for generating 4D videos, organized as a grid of video frames with both time and viewpoint axes. In this grid, each row contains frames sharing the same timestep, while each column contains frames from the same viewpoint. We propose a novel two-stream architecture. One stream performs viewpoint updates on columns, and the other stream performs temporal updates on rows. After each diffusion transformer layer, a synchronization layer exchanges information between the two token streams. We propose two implementations of the synchronization layer, using either hard or soft synchronization. This feedforward architecture improves upon previous work in three ways: higher inference speed, enhanced visual quality (measured by FVD, CLIP, and VideoScore), and improved temporal and viewpoint consistency (measured by VideoScore and Dust3R-Confidence).', 'score': 6, 'issue_id': 996, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '2046f12235c1227f', 'authors': ['Chaoyang Wang', 'Peiye Zhuang', 'Tuan Duc Ngo', 'Willi Menapace', 'Aliaksandr Siarohin', 'Michael Vasilkovsky', 'Ivan Skorokhodov', 'Sergey Tulyakov', 'Peter Wonka', 'Hsin-Ying Lee'], 'affiliations': ['KAUST', 'Snap Inc', 'Umass Amherst'], 'pdf_title_img': 'assets/pdf/title_img/2412.04462.jpg', 'data': {'categories': ['#video', '#architecture'], 'emoji': 'ğŸ¥', 'ru': {'title': '4Real-Video: Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾', 'desc': '4Real-Video - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ 4D-Ğ²Ğ¸Ğ´ĞµĞ¾, Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ²Ğ¸Ğ´Ğµ ÑĞµÑ‚ĞºĞ¸ ĞºĞ°Ğ´Ñ€Ğ¾Ğ² Ñ Ğ¾ÑÑĞ¼Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°. ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ° Ğ´Ğ²ÑƒÑ…Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: Ğ¾Ğ´Ğ¸Ğ½ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ Ñ€Ğ°ĞºÑƒÑ€ÑÑ‹ Ğ¿Ğ¾ ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ğ¼, Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ - Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼. ĞŸĞ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ°Ğ¼Ğ¸. Ğ­Ñ‚Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°, Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ñƒ Ğ¸ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ¸ Ñ€Ğ°ĞºÑƒÑ€ÑĞ°Ğ¼.'}, 'en': {'title': 'Revolutionizing 4D Video Generation with 4Real-Video', 'desc': 'The paper introduces 4Real-Video, a new framework designed to create 4D videos that are structured in a grid format, incorporating both time and viewpoint dimensions. It utilizes a two-stream architecture where one stream focuses on updating the viewpoint across columns, while the other stream handles temporal updates along the rows. A synchronization layer is implemented after each diffusion transformer layer to facilitate communication between the two streams, with options for hard or soft synchronization. This innovative approach results in faster inference, better visual quality, and improved consistency in both temporal and viewpoint aspects compared to previous methods.'}, 'zh': {'title': '4Real-Videoï¼šé«˜æ•ˆç”Ÿæˆ4Dè§†é¢‘çš„æ–°æ¡†æ¶', 'desc': 'æˆ‘ä»¬æå‡ºäº†4Real-Videoï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„4Dè§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œè§†é¢‘å¸§ä»¥æ—¶é—´å’Œè§†è§’ä¸ºè½´ç»„ç»‡æˆç½‘æ ¼ã€‚åœ¨è¿™ä¸ªç½‘æ ¼ä¸­ï¼Œæ¯ä¸€è¡ŒåŒ…å«ç›¸åŒæ—¶é—´æ­¥çš„å¸§ï¼Œè€Œæ¯ä¸€åˆ—åŒ…å«ç›¸åŒè§†è§’çš„å¸§ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„åŒæµæ¶æ„ï¼Œä¸€æ¡æµåœ¨åˆ—ä¸Šè¿›è¡Œè§†è§’æ›´æ–°ï¼Œå¦ä¸€æ¡æµåœ¨è¡Œä¸Šè¿›è¡Œæ—¶é—´æ›´æ–°ã€‚é€šè¿‡åŒæ­¥å±‚åœ¨ä¸¤ä¸ªæµä¹‹é—´äº¤æ¢ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æ¶æ„åœ¨æ¨ç†é€Ÿåº¦ã€è§†è§‰è´¨é‡å’Œæ—¶é—´ä¸è§†è§’ä¸€è‡´æ€§æ–¹é¢éƒ½æœ‰æ˜¾è‘—æå‡ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04003', 'title': 'Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement', 'url': 'https://huggingface.co/papers/2412.04003', 'abstract': 'Large Language Models (LLMs) have achieved remarkable progress in recent years; however, their excellent performance is still largely limited to major world languages, primarily English. Many LLMs continue to face challenges with multilingual tasks, especially when it comes to low-resource languages. To address this issue, we introduced Marco-LLM: Massive multilingual training for cross-lingual enhancement LLM. We have collected a substantial amount of multilingual data for several low-resource languages and conducted extensive continual pre-training using the Qwen2 models. This effort has resulted in a multilingual LLM named Marco-LLM. Through comprehensive evaluations on various multilingual benchmarks, including MMMLU, AGIEval, Belebele, Flores-200, XCOPA and many others, Marco-LLM has demonstrated substantial improvements over state-of-the-art LLMs. Furthermore, Marco-LLM achieved substantial enhancements in any-to-any machine translation tasks, showing the effectiveness of our multilingual LLM. Marco-LLM is a pioneering multilingual LLM designed to not only perform exceptionally well in multilingual tasks, including low-resource languages, but also maintain strong performance in English and other major languages, closing the performance gap between high- and low-resource language capabilities. By bridging languages, this effort demonstrates our dedication to ensuring LLMs work accurately across various languages.', 'score': 6, 'issue_id': 985, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '435ec5749dcb2e12', 'authors': ['Lingfeng Ming', 'Bo Zeng', 'Chenyang Lyu', 'Tianqi Shi', 'Yu Zhao', 'Xue Yang', 'Yefeng Liu', 'Yiyu Wang', 'Linlong Xu', 'Yangyang Liu', 'Xiaohu Zhao', 'Hao Wang', 'Heng Liu', 'Hao Zhou', 'Huifeng Yin', 'Zifu Shang', 'Haijun Li', 'Longyue Wang', 'Weihua Luo', 'Kaifu Zhang'], 'affiliations': ['MarcoPolo Team, Alibaba International Digital Commerce'], 'pdf_title_img': 'assets/pdf/title_img/2412.04003.jpg', 'data': {'categories': ['#training', '#machine_translation', '#dataset', '#low_resource', '#multilingual'], 'emoji': 'ğŸŒ', 'ru': {'title': 'Marco-LLM: ĞŸÑ€ĞµĞ¾Ğ´Ğ¾Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ±Ğ°Ñ€ÑŒĞµÑ€Ğ° Ğ² Ğ¼Ğ¸Ñ€Ğµ Ğ˜Ğ˜', 'desc': 'Marco-LLM - ÑÑ‚Ğ¾ Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (LLM), Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ñ€ĞµÑÑƒÑ€ÑĞ½Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ±Ñ‹Ğ»Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ½Ğ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¼ Ğ¾Ğ±ÑŠĞµĞ¼Ğµ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Qwen2. Marco-LLM Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ° Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ LLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ·Ñ‹Ñ‡Ğ½Ñ‹Ñ… Ñ‚ĞµÑÑ‚Ğ°Ñ…, Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ MMMLU, AGIEval, Belebele Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ¾Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ° ÑÑƒÑ‰ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑ Ğ² Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ»ÑĞ±Ñ‹Ğ¼Ğ¸ ÑĞ·Ñ‹ĞºĞ°Ğ¼Ğ¸.'}, 'en': {'title': 'Bridging Language Gaps with Marco-LLM', 'desc': 'This paper presents Marco-LLM, a large language model designed to improve performance in multilingual tasks, particularly for low-resource languages. The authors collected extensive multilingual data and performed continual pre-training using Qwen2 models to enhance cross-lingual capabilities. Evaluations on multiple benchmarks showed that Marco-LLM outperforms existing state-of-the-art models, especially in any-to-any machine translation tasks. The model aims to bridge the performance gap between high-resource and low-resource languages, ensuring effective communication across diverse linguistic backgrounds.'}, 'zh': {'title': 'Marco-LLMï¼šæ‰“ç ´è¯­è¨€å£å’çš„å¤šè¯­è¨€æ¨¡å‹', 'desc': 'å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¿‘å¹´æ¥å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å…¶ä¼˜ç§€è¡¨ç°ä¸»è¦é›†ä¸­åœ¨ä¸»è¦ä¸–ç•Œè¯­è¨€ä¸Šï¼Œå°¤å…¶æ˜¯è‹±è¯­ã€‚ä¸ºäº†æ”¹å–„ä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€ä»»åŠ¡è¡¨ç°ï¼Œæˆ‘ä»¬æå‡ºäº†Marco-LLMï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è·¨è¯­è¨€å¢å¼ºçš„å¤§è§„æ¨¡å¤šè¯­è¨€è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬æ”¶é›†äº†å¤§é‡ä½èµ„æºè¯­è¨€çš„å¤šè¯­è¨€æ•°æ®ï¼Œå¹¶ä½¿ç”¨Qwen2æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„æŒç»­é¢„è®­ç»ƒã€‚Marco-LLMåœ¨å¤šé¡¹å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ï¼Œå°¤å…¶åœ¨ä»»æ„å¯¹ä»»æ„çš„æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œå±•ç¤ºäº†å…¶å¤šè¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04139', 'title': 'Monet: Mixture of Monosemantic Experts for Transformers', 'url': 'https://huggingface.co/papers/2412.04139', 'abstract': 'Understanding the internal computations of large language models (LLMs) is crucial for aligning them with human values and preventing undesirable behaviors like toxic content generation. However, mechanistic interpretability is hindered by polysemanticity -- where individual neurons respond to multiple, unrelated concepts. While Sparse Autoencoders (SAEs) have attempted to disentangle these features through sparse dictionary learning, they have compromised LLM performance due to reliance on post-hoc reconstruction loss. To address this issue, we introduce Mixture of Monosemantic Experts for Transformers (Monet) architecture, which incorporates sparse dictionary learning directly into end-to-end Mixture-of-Experts pretraining. Our novel expert decomposition method enables scaling the expert count to 262,144 per layer while total parameters scale proportionally to the square root of the number of experts. Our analyses demonstrate mutual exclusivity of knowledge across experts and showcase the parametric knowledge encapsulated within individual experts. Moreover, Monet allows knowledge manipulation over domains, languages, and toxicity mitigation without degrading general performance. Our pursuit of transparent LLMs highlights the potential of scaling expert counts to enhance} mechanistic interpretability and directly resect the internal knowledge to fundamentally adjust} model behavior. The source code and pretrained checkpoints are available at https://github.com/dmis-lab/Monet.', 'score': 6, 'issue_id': 982, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '9853123764b31006', 'authors': ['Jungwoo Park', 'Young Jin Ahn', 'Kee-Eung Kim', 'Jaewoo Kang'], 'affiliations': ['AIGEN Sciences', 'KAIST', 'Korea University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04139.jpg', 'data': {'categories': ['#training', '#interpretability', '#optimization', '#open_source', '#architecture', '#alignment'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸÑ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ñ€Ğ°ÑÑˆĞ¸Ñ„Ñ€Ğ¾Ğ²ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ñ… Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ¼Ğ¾Ğ½Ğ¾ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ²', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²ÑƒÑ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Mixture of Monosemantic Experts for Transformers (Monet) Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. Monet Ğ²Ğ½ĞµĞ´Ñ€ÑĞµÑ‚ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ñ‚Ğ¸Ğ¿Ğ° Mixture-of-Experts. Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ² Ğ´Ğ¾ 262,144 Ğ½Ğ° ÑĞ»Ğ¾Ğ¹, Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ¾Ğ¼ Ğ¾Ğ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ€Ğ°ÑÑ‚ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¼Ñƒ ĞºĞ¾Ñ€Ğ½Ñ Ñ‡Ğ¸ÑĞ»Ğ° ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ¾Ğ². ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ÑÑ‰ĞµĞµ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¸ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ, Ğ¸Ğ½ĞºĞ°Ğ¿ÑÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ñ….'}, 'en': {'title': 'Unlocking LLMs: Expert Knowledge for Safer AI', 'desc': "This paper addresses the challenge of understanding how large language models (LLMs) work internally, particularly to align them with human values and reduce harmful outputs. The authors introduce a new architecture called Mixture of Monosemantic Experts for Transformers (Monet), which improves upon previous methods by integrating sparse dictionary learning directly into the model's training process. Monet allows for a large number of specialized experts, enhancing the model's ability to manage different types of knowledge while maintaining overall performance. The findings suggest that this approach not only improves interpretability but also enables better control over the model's behavior regarding various domains and toxicity levels."}, 'zh': {'title': 'æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„å¯è§£é‡Šæ€§ä¸æ€§èƒ½', 'desc': 'ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…éƒ¨è®¡ç®—å¯¹äºä½¿å…¶ä¸äººç±»ä»·å€¼è§‚å¯¹é½è‡³å…³é‡è¦ï¼Œå¹¶é˜²æ­¢ç”Ÿæˆæœ‰å®³å†…å®¹ã€‚ç„¶è€Œï¼Œæœºåˆ¶å¯è§£é‡Šæ€§å—åˆ°å¤šä¹‰æ€§çš„é˜»ç¢ï¼Œå³å•ä¸ªç¥ç»å…ƒå¯¹å¤šä¸ªæ— å…³æ¦‚å¿µçš„å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¶æ„ï¼Œç§°ä¸ºå•ä¹‰ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMonetï¼‰ï¼Œå®ƒå°†ç¨€ç–å­—å…¸å­¦ä¹ ç›´æ¥èå…¥ç«¯åˆ°ç«¯çš„ä¸“å®¶é¢„è®­ç»ƒä¸­ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”Monetå…è®¸åœ¨ä¸åŒé¢†åŸŸã€è¯­è¨€å’Œæ¯’æ€§å‡è½»æ–¹é¢è¿›è¡ŒçŸ¥è¯†æ“ä½œï¼Œè€Œä¸ä¼šé™ä½æ•´ä½“æ€§èƒ½ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.03704', 'title': 'Scaling Inference-Time Search with Vision Value Model for Improved Visual Comprehension', 'url': 'https://huggingface.co/papers/2412.03704', 'abstract': "Despite significant advancements in vision-language models (VLMs), there lacks effective approaches to enhance response quality by scaling inference-time computation. This capability is known to be a core step towards the self-improving models in recent large language model studies. In this paper, we present Vision Value Model (VisVM) that can guide VLM inference-time search to generate responses with better visual comprehension. Specifically, VisVM not only evaluates the generated sentence quality in the current search step, but also anticipates the quality of subsequent sentences that may result from the current step, thus providing a long-term value. In this way, VisVM steers VLMs away from generating sentences prone to hallucinations or insufficient detail, thereby producing higher quality responses. Experimental results demonstrate that VisVM-guided search significantly enhances VLMs' ability to generate descriptive captions with richer visual details and fewer hallucinations, compared with greedy decoding and search methods with other visual reward signals. Furthermore, we find that self-training the model with the VisVM-guided captions improve VLM's performance across a wide range of multimodal benchmarks, indicating the potential for developing self-improving VLMs. Our value model and code are available at https://github.com/si0wang/VisVM.", 'score': 5, 'issue_id': 993, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': 'ec4d70e89a11baa5', 'authors': ['Wang Xiyao', 'Yang Zhengyuan', 'Li Linjie', 'Lu Hongjin', 'Xu Yuancheng', 'Lin Chung-Ching Lin', 'Lin Kevin', 'Huang Furong', 'Wang Lijuan'], 'affiliations': ['Microsoft', 'University of Maryland, College Park'], 'pdf_title_img': 'assets/pdf/title_img/2412.03704.jpg', 'data': {'categories': ['#optimization', '#inference', '#cv', '#hallucinations', '#training', '#multimodal'], 'emoji': 'ğŸ”', 'ru': {'title': 'VisVM: Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ VLM Ñ‡ĞµÑ€ĞµĞ· Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Vision Value Model (VisVM) - Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, ÑƒĞ»ÑƒÑ‡ÑˆĞ°ÑÑ‰ÑƒÑ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° (VLM) Ğ¿ÑƒÑ‚ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ°. VisVM Ğ¾Ñ†ĞµĞ½Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ…, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ñ‚ÑŒ Ğ³Ğ°Ğ»Ğ»ÑÑ†Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹ Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ VisVM Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ VLM Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº. Ğ‘Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ğ³Ğ¾, ÑĞ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ VisVM ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ VLM Ğ½Ğ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ°Ñ….'}, 'en': {'title': 'Enhancing Visual Comprehension in VLMs with Vision Value Model', 'desc': 'This paper introduces the Vision Value Model (VisVM), which enhances the response quality of vision-language models (VLMs) during inference. VisVM evaluates not only the quality of the current generated sentence but also predicts the quality of future sentences, providing a long-term value assessment. By guiding the search process, VisVM helps VLMs avoid generating sentences that lack detail or contain inaccuracies, leading to more descriptive and accurate outputs. Experimental results show that using VisVM significantly improves the generation of captions with richer visual details and reduces hallucinations, paving the way for self-improving VLMs through self-training.'}, 'zh': {'title': 'æå‡è§†è§‰è¯­è¨€æ¨¡å‹å“åº”è´¨é‡çš„å…³é”®', 'desc': 'å°½ç®¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨æ¨ç†æ—¶è®¡ç®—èƒ½åŠ›çš„æå‡æ–¹é¢ä»ç¼ºä¹æœ‰æ•ˆçš„æ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è§†è§‰ä»·å€¼æ¨¡å‹ï¼ˆVisVMï¼‰ï¼Œå®ƒå¯ä»¥æŒ‡å¯¼VLMåœ¨æ¨ç†æ—¶çš„æœç´¢ï¼Œä»¥ç”Ÿæˆæ›´å…·è§†è§‰ç†è§£çš„å“åº”ã€‚VisVMä¸ä»…è¯„ä¼°å½“å‰æœç´¢æ­¥éª¤ç”Ÿæˆå¥å­çš„è´¨é‡ï¼Œè¿˜é¢„æµ‹åç»­å¥å­çš„è´¨é‡ï¼Œä»è€Œæä¾›é•¿æœŸä»·å€¼ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVisVMå¼•å¯¼çš„æœç´¢æ˜¾è‘—æé«˜äº†VLMç”Ÿæˆæè¿°æ€§æ ‡é¢˜çš„èƒ½åŠ›ï¼Œå‡å°‘äº†å¹»è§‰ç°è±¡ï¼Œæ˜¾ç¤ºå‡ºè‡ªæˆ‘æå‡VLMçš„æ½œåŠ›ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04448', 'title': 'MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation', 'url': 'https://huggingface.co/papers/2412.04448', 'abstract': 'Recent advances in video diffusion models have unlocked new potential for realistic audio-driven talking video generation. However, achieving seamless audio-lip synchronization, maintaining long-term identity consistency, and producing natural, audio-aligned expressions in generated talking videos remain significant challenges. To address these challenges, we propose Memory-guided EMOtion-aware diffusion (MEMO), an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Our approach is built around two key modules: (1) a memory-guided temporal module, which enhances long-term identity consistency and motion smoothness by developing memory states to store information from a longer past context to guide temporal modeling via linear attention; and (2) an emotion-aware audio module, which replaces traditional cross attention with multi-modal attention to enhance audio-video interaction, while detecting emotions from audio to refine facial expressions via emotion adaptive layer norm. Extensive quantitative and qualitative results demonstrate that MEMO generates more realistic talking videos across diverse image and audio types, outperforming state-of-the-art methods in overall quality, audio-lip synchronization, identity consistency, and expression-emotion alignment.', 'score': 5, 'issue_id': 993, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'cb01c778a4be31bd', 'authors': ['Longtao Zheng', 'Yifan Zhang', 'Hanzhong Guo', 'Jiachun Pan', 'Zhenxiong Tan', 'Jiahao Lu', 'Chuanxin Tang', 'Bo An', 'Shuicheng Yan'], 'affiliations': ['Nanyang Technological University', 'National University of Singapore', 'Skywork AI'], 'pdf_title_img': 'assets/pdf/title_img/2412.04448.jpg', 'data': {'categories': ['#long_context', '#multimodal', '#video', '#diffusion'], 'emoji': 'ğŸ­', 'ru': {'title': 'MEMO: Ğ ĞµĞ°Ğ»Ğ¸ÑÑ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ°Ğ½Ğ¸Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ¾Ğ² Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¹', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑÑ‰Ğ¸Ğ¼Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚Ñ€ĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ MEMO (Memory-guided EMOtion-aware diffusion). ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ MEMO Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ Ğ¿Ğ»Ğ°Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾-Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ´Ğ»Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾. MEMO Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸. Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´ÑÑ‚Ğ²Ğ¾ MEMO Ğ½Ğ°Ğ´ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³ÑƒĞ± Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾, ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ğ²Ñ‹Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ ÑĞ¼Ğ¾Ñ†Ğ¸ÑĞ¼.'}, 'en': {'title': 'MEMO: Revolutionizing Audio-Driven Talking Video Generation', 'desc': 'This paper introduces Memory-guided EMOtion-aware diffusion (MEMO), a novel approach for generating talking videos that are synchronized with audio. MEMO addresses key challenges such as audio-lip synchronization and maintaining consistent identities over time. It utilizes a memory-guided temporal module to enhance motion smoothness and identity consistency by leveraging past information. Additionally, an emotion-aware audio module improves the interaction between audio and video, ensuring that facial expressions align with detected emotions, resulting in high-quality, expressive talking videos.'}, 'zh': {'title': 'è®°å¿†å¼•å¯¼çš„æƒ…æ„Ÿæ„ŸçŸ¥æ‰©æ•£ï¼šç”Ÿæˆæ›´çœŸå®çš„å¯¹è¯è§†é¢‘', 'desc': 'æœ€è¿‘è§†é¢‘æ‰©æ•£æ¨¡å‹çš„è¿›å±•ä¸ºåŸºäºéŸ³é¢‘çš„çœŸå®æ„Ÿå¯¹è¯è§†é¢‘ç”Ÿæˆå¼€è¾Ÿäº†æ–°æ½œåŠ›ã€‚ç„¶è€Œï¼Œå®ç°æ— ç¼çš„éŸ³é¢‘ä¸å˜´å”‡åŒæ­¥ã€ä¿æŒé•¿æœŸèº«ä»½ä¸€è‡´æ€§ä»¥åŠç”Ÿæˆè‡ªç„¶çš„éŸ³é¢‘å¯¹é½è¡¨æƒ…ä»ç„¶æ˜¯é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è®°å¿†å¼•å¯¼çš„æƒ…æ„Ÿæ„ŸçŸ¥æ‰©æ•£ï¼ˆMEMOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆèº«ä»½ä¸€è‡´ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å¯¹è¯è§†é¢‘ã€‚æˆ‘ä»¬çš„æ–¹æ¡ˆå›´ç»•ä¸¤ä¸ªå…³é”®æ¨¡å—æ„å»ºï¼šè®°å¿†å¼•å¯¼çš„æ—¶é—´æ¨¡å—å’Œæƒ…æ„Ÿæ„ŸçŸ¥éŸ³é¢‘æ¨¡å—ï¼Œæ˜¾è‘—æå‡äº†è§†é¢‘ç”Ÿæˆçš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚'}}}, {'id': 'https://huggingface.co/papers/2411.19574', 'title': 'KV Shifting Attention Enhances Language Modeling', 'url': 'https://huggingface.co/papers/2411.19574', 'abstract': "The current large language models are mainly based on decode-only structure transformers, which have great in-context learning (ICL) capabilities. It is generally believed that the important foundation of its ICL capability is the induction heads mechanism, which requires at least two layers attention. In order to more efficiently implement the ability of the model's induction, we revisit the induction heads mechanism and proposed a KV shifting attention. We theoretically prove that the KV shifting attention reducing the model's requirements for the depth and width of the induction heads mechanism. Our experimental results demonstrate that KV shifting attention is beneficial to learning induction heads and language modeling, which lead to better performance or faster convergence from toy models to the pre-training models with more than 10 B parameters.", 'score': 5, 'issue_id': 987, 'pub_date': '2024-11-29', 'pub_date_card': {'ru': '29 Ğ½Ğ¾ÑĞ±Ñ€Ñ', 'en': 'November 29', 'zh': '11æœˆ29æ—¥'}, 'hash': 'feb0a592d740fe9b', 'authors': ['Mingyu Xu', 'Wei Cheng', 'Bingning Wang', 'Weipeng Chen'], 'affiliations': ['Baichuan Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2411.19574.jpg', 'data': {'categories': ['#optimization', '#architecture', '#training'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ¾Ğ² Ğ² ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…', 'desc': 'Ğ­Ñ‚Ğ° ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¸ÑÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ÑÑ…, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ°Ñ…. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¿Ğ¾Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ KV shifting attention Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸. Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾Ñ‚ Ğ¼ĞµÑ‚Ğ¾Ğ´ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğº Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ Ğ¸ ÑˆĞ¸Ñ€Ğ¸Ğ½Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚, Ñ‡Ñ‚Ğ¾ KV shifting attention ÑƒĞ»ÑƒÑ‡ÑˆĞ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ğ´ÑƒĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¼ Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼ Ğ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ¾Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ, Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñ Ğº Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ»Ğ¸ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ ÑÑ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸.'}, 'en': {'title': 'Enhancing Induction Heads with KV Shifting Attention', 'desc': "This paper discusses improvements in large language models that use a decode-only transformer structure, focusing on their in-context learning (ICL) abilities. The authors highlight the importance of the induction heads mechanism, which traditionally requires multiple layers of attention. They introduce a new approach called KV shifting attention, which simplifies the induction heads mechanism by reducing the model's depth and width requirements. Experimental results show that this new attention method enhances the learning of induction heads and improves language modeling performance, leading to faster convergence in models with over 10 billion parameters."}, 'zh': {'title': 'æå‡å½’çº³èƒ½åŠ›çš„KVä½ç§»æ³¨æ„åŠ›æœºåˆ¶', 'desc': 'å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åŸºäºè§£ç ç»“æ„çš„å˜æ¢å™¨ï¼Œå…·æœ‰å¾ˆå¼ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ã€‚äººä»¬æ™®éè®¤ä¸ºï¼Œè¿™ç§èƒ½åŠ›çš„é‡è¦åŸºç¡€æ˜¯å½’çº³å¤´æœºåˆ¶ï¼Œè€Œè¯¥æœºåˆ¶è‡³å°‘éœ€è¦ä¸¤å±‚æ³¨æ„åŠ›ã€‚ä¸ºäº†æ›´æœ‰æ•ˆåœ°å®ç°æ¨¡å‹çš„å½’çº³èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å½’çº³å¤´æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§KVä½ç§»æ³¨æ„åŠ›ã€‚æˆ‘ä»¬çš„ç†è®ºè¯æ˜è¡¨æ˜ï¼ŒKVä½ç§»æ³¨æ„åŠ›é™ä½äº†æ¨¡å‹å¯¹å½’çº³å¤´æœºåˆ¶çš„æ·±åº¦å’Œå®½åº¦è¦æ±‚ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå…¶åœ¨å­¦ä¹ å½’çº³å¤´å’Œè¯­è¨€å»ºæ¨¡æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04106', 'title': 'MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities', 'url': 'https://huggingface.co/papers/2412.04106', 'abstract': 'Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities.', 'score': 5, 'issue_id': 980, 'pub_date': '2024-12-04', 'pub_date_card': {'ru': '4 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 4', 'zh': '12æœˆ4æ—¥'}, 'hash': '7b2720f9a6c27027', 'authors': ['Haoning Wu', 'Ziheng Zhao', 'Ya Zhang', 'Weidi Xie', 'Yanfeng Wang'], 'affiliations': ['CMIC, Shanghai Jiao Tong University, China', 'School of Artificial Intelligence, Shanghai Jiao Tong University, China', 'Shanghai AI Laboratory, China'], 'pdf_title_img': 'assets/pdf/title_img/2412.04106.jpg', 'data': {'categories': ['#diffusion', '#healthcare', '#synthetic', '#cv', '#data', '#dataset'], 'emoji': 'ğŸ§ ', 'ru': {'title': 'Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ĞœĞ Ğ¢ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸', 'desc': 'Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº ÑĞ¸Ğ½Ñ‚ĞµĞ·Ñƒ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ MedGen-1M, ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‰Ğ¸Ğ¹ Ñ€Ğ°Ğ´Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸Ñ‡Ğ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¾Ğ². ĞĞ½Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ´Ğ¸Ñ„Ñ„ÑƒĞ·Ğ¸Ğ¾Ğ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ MRGen Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ĞœĞ Ğ¢-Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ Ğ¼Ğ°ÑĞ¾Ğº. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ° Ğ½ĞµĞ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… ĞœĞ Ğ¢.'}, 'en': {'title': 'Generating Synthetic Data for Medical Image Segmentation', 'desc': 'This paper presents a novel approach to medical image segmentation by using generative models to create synthetic data for modalities that lack mask annotations. The authors introduce a large dataset called MedGen-1M, which includes radiology images with modality labels and some organ mask annotations, facilitating research in medical image generation. They propose a diffusion-based data engine named MRGen that generates MR images based on text prompts and available masks, allowing for the training of segmentation models on unannotated data. Extensive experiments demonstrate the effectiveness of MRGen in synthesizing training samples and improving segmentation performance across various MRI modalities.'}, 'zh': {'title': 'åˆ©ç”¨ç”Ÿæˆæ¨¡å‹æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²çš„åˆ›æ–°', 'desc': 'è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ä½¿ç”¨ç”Ÿæˆæ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æœªæ ‡æ³¨æ¨¡æ€çš„æ•°æ®åˆæˆã€‚ç ”ç©¶è€…ä»¬æ”¶é›†å¹¶æ•´ç†äº†ä¸€ä¸ªå¤§å‹çš„æ”¾å°„å­¦å›¾åƒ-æ–‡æœ¬æ•°æ®é›†MedGen-1Mï¼ŒåŒ…å«æ¨¡æ€æ ‡ç­¾ã€å±æ€§ã€åŒºåŸŸå’Œå™¨å®˜ä¿¡æ¯ï¼Œä»¥åŠéƒ¨åˆ†å™¨å®˜çš„æ©è†œæ³¨é‡Šã€‚è®ºæ–‡ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„ç”Ÿæˆæ•°æ®å¼•æ“MRGenï¼Œå¯ä»¥æ ¹æ®æ–‡æœ¬æç¤ºå’Œæ©è†œç”Ÿæˆç¼ºä¹æ©è†œæ³¨é‡Šçš„MRå›¾åƒï¼Œä»è€Œè®­ç»ƒæœªæ ‡æ³¨æ¨¡æ€çš„åˆ†å‰²æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜è¯¥æ•°æ®å¼•æ“èƒ½å¤Ÿæœ‰æ•ˆåˆæˆè®­ç»ƒæ ·æœ¬ï¼Œæ¨åŠ¨MRIåˆ†å‰²å‘æœªæ ‡æ³¨æ¨¡æ€çš„æ‰©å±•ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04449', 'title': 'p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay', 'url': 'https://huggingface.co/papers/2412.04449', 'abstract': 'Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. The majority of computation stems from the overwhelming volume of vision tokens processed by the transformer decoder. In this paper, we propose to build efficient MLLMs by leveraging the Mixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layer and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. To validate the effectiveness of our approach, we conduct extensive experiments with two baseline models across 14 benchmarks. Our model, p-MoD, matches or even surpasses the performance of the baseline models, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and 77.7% GPU hours during training.', 'score': 4, 'issue_id': 996, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': 'a68320734b5346db', 'authors': ['Jun Zhang', 'Desen Meng', 'Ji Qi', 'Zhenpeng Huang', 'Tao Wu', 'Limin Wang'], 'affiliations': ['China Mobile (Suzhou) Software Technology Co., Ltd.', 'Shanghai AI Lab', 'State Key Laboratory for Novel Software Technology, Nanjing University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04449.jpg', 'data': {'categories': ['#optimization', '#inference', '#training', '#benchmark', '#multimodal'], 'emoji': 'ğŸš€', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ², Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ Ğº Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (MLLM) Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ğ·Ğ¼Ğ° Mixture-of-Depths (MoD). ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ÑÑ‚ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ MoD Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Ğ´Ğ²ÑƒÑ… Ğ½Ğ¾Ğ²Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¸Ğº: Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ² Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ½Ğ³ĞµĞ½ÑĞ° (TanhNorm) Ğ¸ ÑĞ¸Ğ¼Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¿ĞµÑ€ĞµĞ²Ğ·Ğ²ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (STRing). Ğ¢Ğ°ĞºĞ¶Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ ÑĞ¾Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸Ñ (PRD), ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¿Ğ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾ ÑĞ¾ĞºÑ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² Ğ¾Ñ‚ ÑĞ»Ğ¾Ñ Ğº ÑĞ»Ğ¾Ñ. Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¸, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ p-MoD Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ°ĞµÑ‚ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ²Ğ¾ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ².'}, 'en': {'title': 'Efficient Multimodal Learning with Selective Token Processing', 'desc': 'This paper introduces a method to enhance the efficiency of multimodal large language models (MLLMs) by using a Mixture-of-Depths (MoD) mechanism. The MoD mechanism allows the model to selectively process important vision tokens while ignoring redundant ones, reducing computational costs. To improve training and inference stability, the authors implement two innovative designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Their experiments show that the proposed model, p-MoD, achieves comparable or superior performance to baseline models while significantly lowering resource usage during both training and inference.'}, 'zh': {'title': 'é«˜æ•ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°è®¾è®¡', 'desc': 'æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œé€šè¿‡æ··åˆæ·±åº¦ï¼ˆMoDï¼‰æœºåˆ¶æ¥ä¼˜åŒ–è®¡ç®—æ•ˆç‡ã€‚è¯¥æœºåˆ¶å…è®¸æ¯ä¸ªå˜æ¢å™¨è§£ç å™¨å±‚é€‰æ‹©é‡è¦çš„è§†è§‰æ ‡è®°è¿›è¡Œå¤„ç†ï¼ŒåŒæ—¶è·³è¿‡å†—ä½™çš„æ ‡è®°ã€‚ä¸ºäº†è§£å†³è®­ç»ƒå’Œæ¨ç†çš„ç¨³å®šæ€§é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§æ–°è®¾è®¡ï¼štanhé—¨æ§æƒé‡å½’ä¸€åŒ–ï¼ˆTanhNormï¼‰å’Œå¯¹ç§°æ ‡è®°é‡åŠ æƒï¼ˆSTRingï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„p-MoDæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—èµ„æºçš„æ¶ˆè€—ã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04403', 'title': 'Establishing Task Scaling Laws via Compute-Efficient Model Ladders', 'url': 'https://huggingface.co/papers/2412.04403', 'abstract': 'We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: first use model and data size to predict a task-specific loss, and then use this task loss to predict task performance. We train a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks written in ranked classification format, we can predict the accuracy of both target models within 2 points of absolute error. We have higher prediction error on four other tasks (average absolute error 6.9) and find that these are often tasks with higher variance in task metrics. We also find that using less compute to train fewer ladder models tends to deteriorate predictions. Finally, we empirically show that our design choices and the two-step approach lead to superior performance in establishing scaling laws.', 'score': 2, 'issue_id': 1001, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '799212928bdda07f', 'authors': ['Akshita Bhagia', 'Jiacheng Liu', 'Alexander Wettig', 'David Heineman', 'Oyvind Tafjord', 'Ananya Harsh Jha', 'Luca Soldaini', 'Noah A. Smith', 'Dirk Groeneveld', 'Pang Wei Koh', 'Jesse Dodge', 'Hannaneh Hajishirzi'], 'affiliations': ['Allen Institute for Artificial Intelligence', 'Paul G. Allen School of Computer Science & Engineering, University of Washington', 'Princeton University'], 'pdf_title_img': 'assets/pdf/title_img/2412.04403.jpg', 'data': {'categories': ['#small_models', '#optimization', '#dataset', '#training'], 'emoji': 'ğŸ“ˆ', 'ru': {'title': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹', 'desc': "Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ñ… ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ½Ğ° ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ…. ĞĞ½Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ğ´Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´: ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½ÑƒÑ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ, Ğ° Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ½Ğ° ĞµĞµ Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ¸Ñ€ÑƒÑÑ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ. Ğ”Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… 'Ğ»ĞµÑÑ‚Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ñ…' Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‡Ñ‚Ğ¾ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ²ÑĞµĞ³Ğ¾ 1% Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Ğ¿Ğ¾ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ñ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. ĞĞ° Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°Ñ… ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ° Ğ² Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ°Ñ… 2 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ²."}, 'en': {'title': 'Predicting Performance with Model Ladders and Scaling Laws', 'desc': "This paper introduces a method to predict how well pretrained language models will perform on specific tasks, especially when they are overtrained. The authors find that traditional power laws do not effectively predict task performance, so they propose a two-step approach: first estimating task-specific loss based on model and data size, and then using that loss to predict actual task performance. They create smaller 'ladder' models to gather data and refine their predictions, achieving high accuracy in predicting the performance of larger models with significantly less computational cost. The study also highlights that while their method works well for some tasks, it struggles with others that have more variability in results, indicating the need for careful model training and selection."}, 'zh': {'title': 'ä»»åŠ¡è¡¨ç°é¢„æµ‹çš„æ–°æ–¹æ³•', 'desc': 'æˆ‘ä»¬å¼€å‘äº†ä»»åŠ¡ç¼©æ”¾æ³•åˆ™å’Œæ¨¡å‹é˜¶æ¢¯ï¼Œä»¥é¢„æµ‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨è¿‡åº¦è®­ç»ƒæƒ…å†µä¸‹çš„å•ä¸ªä»»åŠ¡è¡¨ç°ã€‚æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡æŸå¤±å¹‚å¾‹æ— æ³•å‡†ç¡®å»ºæ¨¡ä»»åŠ¡è¡¨ç°ï¼Œå› æ­¤æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤æ­¥é¢„æµ‹æ–¹æ³•ï¼šé¦–å…ˆä½¿ç”¨æ¨¡å‹å’Œæ•°æ®è§„æ¨¡é¢„æµ‹ç‰¹å®šä»»åŠ¡çš„æŸå¤±ï¼Œç„¶ååˆ©ç”¨è¯¥ä»»åŠ¡æŸå¤±é¢„æµ‹ä»»åŠ¡è¡¨ç°ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ç»„å°è§„æ¨¡çš„â€œé˜¶æ¢¯â€æ¨¡å‹ï¼Œæ”¶é›†æ•°æ®ç‚¹ä»¥æ‹Ÿåˆè¿™ä¸¤æ­¥é¢„æµ‹çš„å‚æ•°åŒ–å‡½æ•°ï¼Œå¹¶å¯¹ä¸¤ä¸ªç›®æ ‡æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œè¿™ç§è®¾è®¡é€‰æ‹©å’Œä¸¤æ­¥æ–¹æ³•åœ¨å»ºç«‹ç¼©æ”¾æ³•åˆ™æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04262', 'title': 'SynFinTabs: A Dataset of Synthetic Financial Tables for Information and Table Extraction', 'url': 'https://huggingface.co/papers/2412.04262', 'abstract': 'Table extraction from document images is a challenging AI problem, and labelled data for many content domains is difficult to come by. Existing table extraction datasets often focus on scientific tables due to the vast amount of academic articles that are readily available, along with their source code. However, there are significant layout and typographical differences between tables found across scientific, financial, and other domains. Current datasets often lack the words, and their positions, contained within the tables, instead relying on unreliable OCR to extract these features for training modern machine learning models on natural language processing tasks. Therefore, there is a need for a more general method of obtaining labelled data. We present SynFinTabs, a large-scale, labelled dataset of synthetic financial tables. Our hope is that our method of generating these synthetic tables is transferable to other domains. To demonstrate the effectiveness of our dataset in training models to extract information from table images, we create FinTabQA, a layout large language model trained on an extractive question-answering task. We test our model using real-world financial tables and compare it to a state-of-the-art generative model and discuss the results. We make the dataset, model, and dataset generation code publicly available.', 'score': 1, 'issue_id': 994, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '3fef2d0f6d8b7c5c', 'authors': ['Ethan Bradley', 'Muhammad Roman', 'Karen Rafferty', 'Barry Devereux'], 'affiliations': ['School of Electronics, Electrical Engineering and Computer Science, Queens University Belfast, Belfast, UK'], 'pdf_title_img': 'assets/pdf/title_img/2412.04262.jpg', 'data': {'categories': ['#open_source', '#data', '#dataset', '#synthetic', '#transfer_learning', '#benchmark'], 'emoji': 'ğŸ“Š', 'ru': {'title': 'Ğ¡Ğ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ÑÑ‚ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ SynFinTabs - ĞºÑ€ÑƒĞ¿Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½ Ğ¸ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…. ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ FinTabQA - ÑĞ·Ñ‹ĞºĞ¾Ğ²Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†. ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°Ñ… Ğ¸ ÑÑ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ»Ğ°ÑÑŒ Ñ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸. Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑÑ Ğ² Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¾Ğ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğµ.'}, 'en': {'title': 'Unlocking Financial Insights with Synthetic Table Data', 'desc': 'This paper addresses the challenge of table extraction from document images, particularly in the financial domain where labeled data is scarce. It introduces SynFinTabs, a large-scale dataset of synthetic financial tables designed to improve the training of machine learning models for this task. The authors also present FinTabQA, a layout large language model specifically trained for extractive question-answering on table images. By comparing their model with existing state-of-the-art generative models, they demonstrate the effectiveness of their synthetic dataset and make all resources publicly available for further research.'}, 'zh': {'title': 'åˆæˆé‡‘èè¡¨æ ¼æ•°æ®é›†çš„åˆ›æ–°åº”ç”¨', 'desc': 'æœ¬æ–‡è®¨è®ºäº†ä»æ–‡æ¡£å›¾åƒä¸­æå–è¡¨æ ¼çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ç¼ºä¹æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ã€‚ç°æœ‰çš„æ•°æ®é›†ä¸»è¦é›†ä¸­åœ¨ç§‘å­¦è¡¨æ ¼ä¸Šï¼Œå¿½è§†äº†é‡‘èç­‰å…¶ä»–é¢†åŸŸçš„è¡¨æ ¼å¸ƒå±€å’Œæ’ç‰ˆå·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†SynFinTabsï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„åˆæˆé‡‘èè¡¨æ ¼æ ‡æ³¨æ•°æ®é›†ï¼Œæ—¨åœ¨ä¸ºä¸åŒé¢†åŸŸæä¾›å¯è½¬ç§»çš„æ ‡æ³¨æ•°æ®ç”Ÿæˆæ–¹æ³•ã€‚é€šè¿‡åˆ›å»ºFinTabQAæ¨¡å‹ï¼Œä½œè€…å±•ç¤ºäº†è¯¥æ•°æ®é›†åœ¨è®­ç»ƒä¿¡æ¯æå–æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚'}}}, {'id': 'https://huggingface.co/papers/2412.04363', 'title': 'Challenges in Trustworthy Human Evaluation of Chatbots', 'url': 'https://huggingface.co/papers/2412.04363', 'abstract': 'Open community-driven platforms like Chatbot Arena that collect user preference data from site visitors have gained a reputation as one of the most trustworthy publicly available benchmarks for LLM performance. While now standard, it is tricky to implement effective guardrails to collect high-quality annotations from humans. In this paper, we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings. In particular, we show that only 10\\% of poor quality votes by apathetic (site visitors not appropriately incentivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard. Finally, we discuss open challenges in ensuring high-quality human annotations.', 'score': 1, 'issue_id': 992, 'pub_date': '2024-12-05', 'pub_date_card': {'ru': '5 Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ', 'en': 'December 5', 'zh': '12æœˆ5æ—¥'}, 'hash': '4fa16cf75a2af540', 'authors': ['Wenting Zhao', 'Alexander M. Rush', 'Tanya Goyal'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2412.04363.jpg', 'data': {'categories': ['#ethics', '#rlhf', '#hallucinations', '#alignment', '#benchmark'], 'emoji': 'ğŸ­', 'ru': {'title': 'ĞÑ…Ğ¸Ğ»Ğ»ĞµÑĞ¾Ğ²Ğ° Ğ¿ÑÑ‚Ğ° ĞºÑ€Ğ°ÑƒĞ´ÑĞ¾Ñ€ÑĞ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ±ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€ĞºĞ¾Ğ² Ğ˜Ğ˜', 'desc': 'Ğ¡Ñ‚Ğ°Ñ‚ÑŒÑ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ¿Ğ»Ğ°Ñ‚Ñ„Ğ¾Ñ€Ğ¼ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞ·Ñ‹ĞºĞ¾Ğ²Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹, Ñ‚Ğ°ĞºĞ¸Ñ… ĞºĞ°Ğº Chatbot Arena. ĞĞ²Ñ‚Ğ¾Ñ€Ñ‹ Ğ²Ñ‹ÑĞ²Ğ»ÑÑÑ‚ Ñ‚Ñ€Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ½ĞµĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ°Ñ†Ğ¸Ğ¹, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ñ… Ğ¸ÑĞºĞ°Ğ·Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹. ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµĞ³Ğ¾ 10% Ğ½ĞµĞ´Ğ¾Ğ±Ñ€Ğ¾ÑĞ¾Ğ²ĞµÑÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ†ĞµĞ½Ğ¾Ğº Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ğµ Ğ½Ğ° 5 Ğ¼ĞµÑÑ‚. ĞĞ±ÑÑƒĞ¶Ğ´Ğ°ÑÑ‚ÑÑ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ³Ğ¾ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ»ÑĞ´ÑŒĞ¼Ğ¸.'}, 'en': {'title': 'Guarding Against Bad Votes: Ensuring Trustworthy LLM Rankings', 'desc': 'This paper examines the challenges of maintaining high-quality annotations in community-driven platforms that evaluate large language models (LLMs). It identifies three main sources of poor annotations: apathetic users who lack motivation to provide accurate feedback, adversarial users who intentionally skew results, and other unspecified factors. The authors reveal that even a small percentage of low-quality votes can significantly impact the rankings of models on leaderboards. The paper concludes by highlighting the ongoing challenges in ensuring reliable human annotations for accurate model evaluation.'}, 'zh': {'title': 'ç¡®ä¿é«˜è´¨é‡æ³¨é‡Šï¼Œæå‡æ¨¡å‹æ’åçš„ä¿¡ä»»åº¦', 'desc': 'æœ¬æ–‡æ¢è®¨äº†å¼€æ”¾ç¤¾åŒºå¹³å°ï¼ˆå¦‚Chatbot Arenaï¼‰åœ¨æ”¶é›†ç”¨æˆ·åå¥½æ•°æ®æ—¶çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œä½è´¨é‡çš„æ³¨é‡Šæ¥æºä¸»è¦æœ‰ä¸‰ç§ï¼ŒåŒ…æ‹¬å†·æ¼ çš„ç”¨æˆ·å’Œæ¶æ„è¡Œä¸ºè€…ï¼Œè¿™äº›éƒ½å¯èƒ½å½±å“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ’åã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»…10%çš„ä½è´¨é‡æŠ•ç¥¨å°±èƒ½ä½¿æ¨¡å‹åœ¨æ’è¡Œæ¦œä¸Šå˜åŒ–å¤šè¾¾5ä¸ªåæ¬¡ã€‚æœ€åï¼Œæ–‡ç« è®¨è®ºäº†ç¡®ä¿é«˜è´¨é‡äººç±»æ³¨é‡Šçš„å¼€æ”¾æ€§æŒ‘æˆ˜ã€‚'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (2)', '#agents (2)', '#agi', '#alignment (3)', '#architecture (12)', '#audio', '#benchmark (11)', '#cv (12)', '#data (4)', '#dataset (11)', '#diffusion (7)', '#ethics (3)', '#games (2)', '#graphs', '#hallucinations (3)', '#healthcare (1)', '#inference (5)', '#interpretability (2)', '#leakage', '#long_context (1)', '#low_resource (2)', '#machine_translation (2)', '#math', '#multilingual (2)', '#multimodal (13)', '#open_source (8)', '#optimization (14)', '#plp', '#rag', '#reasoning (2)', '#rl', '#rlhf (1)', '#robotics (1)', '#science', '#security (2)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (4)', '#training (17)', '#transfer_learning (1)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `ğŸ·ï¸ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            ğŸ”º ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = 'ğŸ”„ ' + getTimeDiff('2024-12-07 12:41',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³Ñƒ",
                    pub_date: "Ğ´Ğ°Ñ‚Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸",
                    issue_id: "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ° HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "è¯„åˆ†",
                    pub_date: "å‘å¸ƒæ—¥æœŸ",
                    issue_id: "HFä¸Šä¼ æ—¥æœŸ"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2024-12-07 12:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2024-12-07 12:41')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    