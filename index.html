
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 42 papers. June 3.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">3 июня</span> | <span id="title-articles-count">42 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-06-02.html">⬅️ <span id="prev-date">02.06</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-06-04.html">➡️ <span id="next-date">04.06</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-06.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '3 июня', 'en': 'June 3', 'zh': '6月3日'};
        let feedDateNext = {'ru': '04.06', 'en': '06/04', 'zh': '6月4日'};
        let feedDatePrev = {'ru': '02.06', 'en': '06/02', 'zh': '6月2日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2506.01939', 'title': 'Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning', 'url': 'https://huggingface.co/papers/2506.01939', 'abstract': "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.", 'score': 66, 'issue_id': 4090, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'caf288ab8a8d11b2', 'authors': ['Shenzhi Wang', 'Le Yu', 'Chang Gao', 'Chujie Zheng', 'Shixuan Liu', 'Rui Lu', 'Kai Dang', 'Xionghui Chen', 'Jianxin Yang', 'Zhenru Zhang', 'Yuqiong Liu', 'An Yang', 'Andrew Zhao', 'Yang Yue', 'Shiji Song', 'Bowen Yu', 'Gao Huang', 'Junyang Lin'], 'affiliations': ['LeapLab, Tsinghua University', 'Qwen Team, Alibaba Inc.'], 'pdf_title_img': 'assets/pdf/title_img/2506.01939.jpg', 'data': {'categories': ['#rlhf', '#reasoning', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Высокоэнтропийные токены - ключ к улучшению рассуждений ИИ', 'desc': 'Исследование показывает, что паттерны энтропии токенов играют ключевую роль в обучении с подкреплением с проверяемыми вознаграждениями (RLVR) для улучшения рассуждений больших языковых моделей. Обнаружено, что небольшая часть токенов с высокой энтропией значительно влияет на производительность рассуждений. RLVR в основном корректирует энтропию высокоэнтропийных токенов, сохраняя общие паттерны базовой модели. Оптимизация только 20% токенов с высокой энтропией позволяет достичь результатов, сравнимых с полным градиентным обновлением, особенно для больших моделей.'}, 'en': {'title': 'Unlocking Reasoning Power with High-Entropy Tokens in RLVR', 'desc': "This paper investigates the role of token entropy patterns in Reinforcement Learning with Verifiable Rewards (RLVR) to enhance the reasoning abilities of Large Language Models (LLMs). It finds that high-entropy tokens, which are rare, significantly influence the model's reasoning pathways and performance. The study shows that by focusing on these high-entropy tokens during training, the model can achieve better results than traditional methods, even when using a smaller subset of tokens. Overall, the research emphasizes the importance of understanding and optimizing high-entropy tokens to improve RLVR outcomes."}, 'zh': {'title': '高熵令牌：提升RLVR推理能力的关键', 'desc': '本研究探讨了可验证奖励的强化学习（RLVR）在大语言模型（LLMs）推理能力提升中的作用，重点分析了令牌熵模式对推理性能的影响。我们发现，只有少量高熵令牌在推理过程中起到关键作用，能够引导模型走向多样化的推理路径。通过对RLVR训练中熵模式的演变进行研究，我们发现RLVR主要遵循基础模型的熵模式，主要调整高熵令牌的熵值。我们的结果表明，优化高熵令牌是提升RLVR效果的关键，利用这些令牌可以显著提高模型的推理性能。'}}}, {'id': 'https://huggingface.co/papers/2506.01049', 'title': 'Taming LLMs by Scaling Learning Rates with Gradient Grouping', 'url': 'https://huggingface.co/papers/2506.01049', 'abstract': 'SGG, an optimizer wrapper, enhances adaptive learning rates for large language models by grouping gradients and applying cluster-specific scaling, improving convergence and stability.  \t\t\t\t\tAI-generated summary \t\t\t\t Training large language models (LLMs) poses challenges due to their massive scale and heterogeneous architectures. While adaptive optimizers like AdamW help address gradient variations, they still struggle with efficient and effective parameter-wise learning rate estimation, resulting in training instability, slow convergence, and poor compatibility with parameter-efficient fine-tuning (PEFT) techniques. This work introduces Scaling with Gradient Grouping (SGG), an optimizer wrapper that improves adaptive learning rate estimation by dynamic grouping and group-specific scaling. SGG first groups gradient statistics in each layer into clusters and then applies cluster-specific scaling to calibrate learning rates for each parameter, thus imposing collective group-wise constraints while maintaining precise per-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that SGG integrates seamlessly with existing optimizers, and offers consistent gains and faster convergence over baselines, with various model sizes. Its stability across varying batch sizes and learning rates establishes SGG as a robust choice for LLM optimization.', 'score': 22, 'issue_id': 4087, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '350401d748400bad', 'authors': ['Siyuan Li', 'Juanxi Tian', 'Zedong Wang', 'Xin Jin', 'Zicheng Liu', 'Wentao Zhang', 'Dan Xu'], 'affiliations': ['Peking University', 'The Hong Kong University of Science and Technology', 'Westlake University', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01049.jpg', 'data': {'categories': ['#optimization', '#training'], 'emoji': '🚀', 'ru': {'title': 'SGG: Групповое масштабирование градиентов для эффективного обучения языковых моделей', 'desc': 'Статья представляет новый метод оптимизации для обучения больших языковых моделей под названием SGG (Scaling with Gradient Grouping). SGG группирует градиенты и применяет масштабирование для каждой группы, что улучшает адаптивную оценку скорости обучения. Этот подход повышает стабильность и скорость сходимости при обучении крупных языковых моделей. Эксперименты показывают, что SGG хорошо интегрируется с существующими оптимизаторами и дает стабильные улучшения на различных бенчмарках.'}, 'en': {'title': 'SGG: Optimizing Learning Rates for Better LLM Training', 'desc': 'This paper presents Scaling with Gradient Grouping (SGG), an innovative optimizer wrapper designed to enhance adaptive learning rates for large language models (LLMs). SGG addresses the challenges of training LLMs by dynamically grouping gradient statistics and applying specific scaling for each group, which improves convergence and stability. By imposing collective constraints on groups while allowing precise adjustments for individual parameters, SGG optimizes the learning process more effectively than traditional methods. Experimental results demonstrate that SGG integrates well with existing optimizers, leading to faster convergence and improved performance across various model sizes and training conditions.'}, 'zh': {'title': 'SGG：提升大语言模型训练的自适应学习率优化器', 'desc': 'SGG是一种优化器包装器，通过对梯度进行分组和应用特定于集群的缩放，增强了大语言模型的自适应学习率。它解决了大规模模型训练中的不稳定性和收敛速度慢的问题。SGG通过动态分组和集群特定的缩放来改善学习率估计，从而实现更精确的参数调整。实验结果表明，SGG与现有优化器无缝集成，并在不同模型规模上提供了一致的性能提升和更快的收敛速度。'}}}, {'id': 'https://huggingface.co/papers/2505.23590', 'title': 'Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with\n  Jigsaw Puzzles', 'url': 'https://huggingface.co/papers/2505.23590', 'abstract': 'The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.', 'score': 20, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '583be7c36c626f1e', 'authors': ['Zifu Wang', 'Junyi Zhu', 'Bo Tang', 'Zhiyu Li', 'Feiyu Xiong', 'Jiaqian Yu', 'Matthew B. Blaschko'], 'affiliations': ['ESAT-PSI, KU Leuven', 'Institute for Advanced Algorithms Research, Shanghai', 'Memory Tensor, Shanghai', 'Samsung R&D Institute China, Beijing', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.23590.jpg', 'data': {'categories': ['#multimodal', '#training', '#transfer_learning', '#rl', '#cv', '#open_source', '#reasoning', '#games'], 'emoji': '🧩', 'ru': {'title': 'Мультимодальные модели осваивают пазлы с помощью обучения с подкреплением', 'desc': 'Исследование применения обучения с подкреплением на основе правил к мультимодальным большим языковым моделям (MLLM) с использованием головоломок-пазлов в качестве экспериментальной базы. Результаты показывают, что MLLM могут достигать высокой точности и обобщения на сложных конфигурациях пазлов после дообучения. Обнаружено, что обучение на пазлах может улучшить результаты на других визуальных задачах, а обучение с подкреплением демонстрирует лучшее обобщение, чем обычная тонкая настройка. Исследование вносит вклад в понимание обучения с подкреплением на основе правил для визуальных задач в мультимодальном обучении.'}, 'en': {'title': 'Unlocking Multimodal Learning with Jigsaw Puzzles and RL', 'desc': 'This paper explores the use of rule-based reinforcement learning (RL) in multimodal large language models (MLLMs) through the lens of jigsaw puzzles. The study finds that MLLMs can improve from random guessing to near-perfect accuracy on jigsaw puzzles after fine-tuning, demonstrating their ability to generalize to more complex tasks. It also reveals that MLLMs can learn effectively with or without explicit reasoning, although they may not always utilize a step-by-step thought process. Additionally, the research shows that RL outperforms supervised fine-tuning in terms of generalization, highlighting the importance of training strategies in visual tasks.'}, 'zh': {'title': '基于规则的强化学习在多模态学习中的新发现', 'desc': '本研究探讨了基于规则的强化学习在多模态大型语言模型中的应用，特别是在视觉任务中的挑战。我们使用拼图作为实验框架，发现经过微调后，模型在简单拼图上的表现从随机猜测提升至接近完美的准确率，并能推广到复杂的未见配置。研究还表明，模型可以在有无显式推理的情况下学习和泛化，尽管开源模型更倾向于直接回答。最后，我们发现强化学习在泛化能力上优于监督微调，并且初始的监督微调冷启动阶段可能会妨碍后续的强化学习优化。'}}}, {'id': 'https://huggingface.co/papers/2506.00539', 'title': 'ARIA: Training Language Agents with Intention-Driven Reward Aggregation', 'url': 'https://huggingface.co/papers/2506.00539', 'abstract': 'ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.', 'score': 19, 'issue_id': 4090, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '49b915ea5a1db300', 'authors': ['Ruihan Yang', 'Yikai Zhang', 'Aili Chen', 'Xintao Wang', 'Siyu Yuan', 'Jiangjie Chen', 'Deqing Yang', 'Yanghua Xiao'], 'affiliations': ['Bytedance Seed', 'Fudan University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00539.jpg', 'data': {'categories': ['#games', '#reasoning', '#agents', '#rl', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'ARIA: Агрегация наград в пространстве намерений для эффективного обучения языковых ИИ-агентов', 'desc': 'ARIA - это метод, который агрегирует награды в пространстве намерений для эффективного обучения языковых агентов. Он проецирует действия на естественном языке из высокоразмерного пространства распределения токенов в низкоразмерное пространство намерений, где семантически похожие действия кластеризуются и получают общие награды. Это уменьшает дисперсию наград, уплотняя сигналы наград и способствуя лучшей оптимизации политики. Эксперименты показывают, что ARIA значительно снижает дисперсию градиента политики и дает существенный прирост производительности в среднем на 9,95% в четырех задачах.'}, 'en': {'title': 'Enhancing Language Agents with Intention-Based Reward Aggregation', 'desc': 'ARIA is a method designed to improve reinforcement learning in language-based tasks by addressing the issue of reward sparsity. It does this by aggregating rewards in an intention space, which clusters semantically similar actions and assigns them shared rewards. This approach reduces the variance of rewards, making it easier for agents to learn effective policies. Experiments show that ARIA leads to significant performance improvements across various tasks compared to traditional reinforcement learning methods.'}, 'zh': {'title': '意图空间中的奖励聚合，提升语言代理的学习效率', 'desc': 'ARIA是一种在意图空间中聚合奖励的方法，旨在缓解奖励稀疏性并改善基于语言的强化学习任务中的策略优化。通过将自然语言动作从高维的联合标记分布空间投影到低维的意图空间，ARIA能够将语义相似的动作聚集在一起并分配共享奖励。这种基于意图的奖励聚合减少了奖励方差，增强了奖励信号的密度，从而促进了更好的策略优化。实验结果表明，ARIA显著降低了策略梯度的方差，并在四个下游任务中平均提高了9.95%的性能，始终优于离线和在线强化学习基线。'}}}, {'id': 'https://huggingface.co/papers/2506.00411', 'title': 'LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks', 'url': 'https://huggingface.co/papers/2506.00411', 'abstract': 'A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Real-world embodied agents face long-horizon tasks, characterized by high-level goals demanding multi-step solutions beyond single actions. Successfully navigating these requires both high-level task planning (i.e., decomposing goals into sub-tasks) and low-level motion control (i.e., generating precise robot actions). While existing vision language action (VLA) models and hierarchical architectures offer potential in embodied tasks, the former often falter in planning, and the latter can suffer from coordination issues, both hampering performance. We introduce a new unified VLA framework for long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA leverages a large pretrained vision language model (VLM) as the backbone to jointly generate language and action tokens for sub-task generation and robot action prediction, respectively. This shared representation promotes better generalization across tasks. Additionally, LoHoVLA embraces a hierarchical closed-loop control mechanism to mitigate errors originating from both high-level planning and low-level control. To train LoHoVLA, we introduce LoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon tasks, each with 1,000 expert demonstrations composed of visual observations, linguistic goals, sub-tasks, and robot actions. Experimental results show that LoHoVLA significantly surpasses both hierarchical and standard VLA approaches on long-horizon embodied tasks in the Ravens simulator. These findings underscore the promise of unified architectures for advancing generalizable embodied intelligence.', 'score': 19, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '16fa44208bf3618c', 'authors': ['Yi Yang', 'Jiaxuan Sun', 'Siqi Kou', 'Yihan Wang', 'Zhijie Deng'], 'affiliations': ['Fudan University', 'Shanghai Jiao Tong University', 'ShanghaiTech University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00411.jpg', 'data': {'categories': ['#cv', '#architecture', '#robotics', '#agents', '#dataset', '#agi', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'Единая архитектура для долгосрочных задач воплощенного ИИ', 'desc': 'LoHoVLA - это новая унифицированная архитектура для решения долгосрочных задач воплощенного искусственного интеллекта. Она объединяет большую предобученную мультимодальную модель для работы с изображениями и текстом с иерархическим управлением с обратной связью. LoHoVLA использует общее представление для генерации подзадач на естественном языке и предсказания действий робота. Эксперименты показали значительное превосходство LoHoVLA над существующими подходами в симуляторе Ravens.'}, 'en': {'title': 'Empowering Robots with Unified Vision and Language for Complex Tasks', 'desc': 'The paper presents LoHoVLA, a new framework that integrates a large pretrained vision language model with hierarchical closed-loop control to enhance performance in long-horizon embodied tasks. It addresses the challenges of high-level task planning and low-level motion control by generating language and action tokens for effective sub-task generation and robot action prediction. The framework is trained on a unique dataset, LoHoSet, which includes a variety of long-horizon tasks with expert demonstrations. Experimental results demonstrate that LoHoVLA outperforms existing models, highlighting the potential of unified architectures in improving embodied intelligence.'}, 'zh': {'title': '统一框架提升长时间任务表现', 'desc': 'LoHoVLA是一个统一的视觉语言行动框架，旨在提高长时间任务的表现。它结合了大型预训练的视觉语言模型和分层闭环控制，能够更好地进行高层次任务规划和低层次运动控制。通过生成语言和动作标记，LoHoVLA促进了任务间的更好泛化。此外，LoHoVLA在Ravens模拟器上进行训练，展示了其在长时间任务中的显著优势。'}}}, {'id': 'https://huggingface.co/papers/2506.01844', 'title': 'SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics', 'url': 'https://huggingface.co/papers/2506.01844', 'abstract': 'SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision-language models (VLMs) pretrained on large-scale multimodal datasets encode rich visual and linguistic knowledge, making them a strong foundation for robotics. Rather than training robotic policies from scratch, recent approaches adapt VLMs into vision-language-action (VLA) models that enable natural language-driven perception and control. However, existing VLAs are typically massive--often with billions of parameters--leading to high training costs and limited real-world deployability. Moreover, they rely on academic and industrial datasets, overlooking the growing availability of community-collected data from affordable robotic platforms. In this work, we present SmolVLA, a small, efficient, and community-driven VLA that drastically reduces both training and inference costs, while retaining competitive performance. SmolVLA is designed to be trained on a single GPU and deployed on consumer-grade GPUs or even CPUs. To further improve responsiveness, we introduce an asynchronous inference stack decoupling perception and action prediction from action execution, allowing higher control rates with chunked action generation. Despite its compact size, SmolVLA achieves performance comparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both simulated as well as real-world robotic benchmarks and release all code, pretrained models, and training data.', 'score': 17, 'issue_id': 4094, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '64cdbe1cd5ffbfc4', 'authors': ['Mustafa Shukor', 'Dana Aubakirova', 'Francesco Capuano', 'Pepijn Kooijmans', 'Steven Palma', 'Adil Zouitine', 'Michel Aractingi', 'Caroline Pascal', 'Martino Russi', 'Andres Marafioti', 'Simon Alibert', 'Matthieu Cord', 'Thomas Wolf', 'Remi Cadene'], 'affiliations': ['Hugging Face', 'Sorbonne University', 'valeo.ai', 'École Normale Supérieure Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2506.01844.jpg', 'data': {'categories': ['#optimization', '#open_source', '#multimodal', '#small_models', '#training', '#benchmark', '#dataset', '#robotics'], 'emoji': '🤖', 'ru': {'title': 'Маленькая модель - большие возможности', 'desc': 'SmolVLA - это компактная и эффективная модель для обработки зрения, языка и действий в робототехнике. Она достигает конкурентоспособной производительности при сниженных вычислительных затратах и может быть развернута на обычном пользовательском оборудовании. SmolVLA обучается на одном GPU и может работать даже на CPU, что значительно снижает стоимость обучения и вывода. Несмотря на свой небольшой размер, SmolVLA показывает результаты, сравнимые с моделями, которые в 10 раз больше.'}, 'en': {'title': 'SmolVLA: Efficient Vision-Language-Action for Everyone', 'desc': 'SmolVLA is a compact vision-language-action model that efficiently integrates visual and linguistic understanding for robotics. It significantly reduces the computational costs associated with training and inference, making it suitable for consumer-grade hardware. By leveraging community-collected data, SmolVLA maintains competitive performance while being much smaller than traditional models. The introduction of an asynchronous inference stack enhances responsiveness, allowing for faster action execution without sacrificing accuracy.'}, 'zh': {'title': 'SmolVLA：小巧高效的视觉-语言-动作模型', 'desc': 'SmolVLA是一种紧凑高效的视觉-语言-动作模型，能够在降低计算成本的同时实现竞争力的性能，并可在消费级硬件上部署。该模型利用社区收集的数据，避免了传统模型对大型数据集的依赖，从而降低了训练和推理的成本。SmolVLA设计为可以在单个GPU上训练，并在消费级GPU或CPU上运行，提升了响应速度。尽管体积小，SmolVLA的性能与体积十倍的模型相当，适用于多种机器人基准测试。'}}}, {'id': 'https://huggingface.co/papers/2506.01943', 'title': 'Learning Video Generation for Robotic Manipulation with Collaborative\n  Trajectory Control', 'url': 'https://huggingface.co/papers/2506.01943', 'abstract': 'Recent advances in video diffusion models have demonstrated strong potential for generating robotic decision-making data, with trajectory conditions further enabling fine-grained control. However, existing trajectory-based methods primarily focus on individual object motion and struggle to capture multi-object interaction crucial in complex robotic manipulation. This limitation arises from multi-feature entanglement in overlapping regions, which leads to degraded visual fidelity. To address this, we present RoboMaster, a novel framework that models inter-object dynamics through a collaborative trajectory formulation. Unlike prior methods that decompose objects, our core is to decompose the interaction process into three sub-stages: pre-interaction, interaction, and post-interaction. Each stage is modeled using the feature of the dominant object, specifically the robotic arm in the pre- and post-interaction phases and the manipulated object during interaction, thereby mitigating the drawback of multi-object feature fusion present during interaction in prior work. To further ensure subject semantic consistency throughout the video, we incorporate appearance- and shape-aware latent representations for objects. Extensive experiments on the challenging Bridge V2 dataset, as well as in-the-wild evaluation, demonstrate that our method outperforms existing approaches, establishing new state-of-the-art performance in trajectory-controlled video generation for robotic manipulation.', 'score': 15, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '4acb1e4fc9635b8a', 'authors': ['Xiao Fu', 'Xintao Wang', 'Xian Liu', 'Jianhong Bai', 'Runsen Xu', 'Pengfei Wan', 'Di Zhang', 'Dahua Lin'], 'affiliations': ['Kuaishou Technology', 'The Chinese University of Hong Kong', 'Zhejiang University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01943.jpg', 'data': {'categories': ['#robotics', '#video', '#diffusion'], 'emoji': '🤖', 'ru': {'title': 'RoboMaster: новый подход к моделированию сложных взаимодействий в робототехнике', 'desc': 'В статье представлен новый подход RoboMaster для моделирования взаимодействия нескольких объектов в робототехнике. Метод разделяет процесс взаимодействия на три этапа: до, во время и после, используя характеристики доминирующего объекта на каждом этапе. Это позволяет избежать проблем, связанных со слиянием признаков нескольких объектов во время взаимодействия. Для обеспечения семантической согласованности на протяжении всего видео используются латентные представления, учитывающие внешний вид и форму объектов.'}, 'en': {'title': 'RoboMaster: Enhancing Robotic Video Generation through Interaction Modeling', 'desc': "This paper introduces RoboMaster, a new framework designed to improve video generation for robotic decision-making by focusing on multi-object interactions. Unlike previous methods that treat objects separately, RoboMaster breaks down the interaction process into three stages: pre-interaction, interaction, and post-interaction. By modeling these stages with the dominant object's features, it effectively addresses the challenges of overlapping features that degrade visual quality. The framework also uses advanced representations to maintain semantic consistency, resulting in superior performance on complex tasks compared to existing techniques."}, 'zh': {'title': 'RoboMaster：提升机器人操作的视频生成新框架', 'desc': '本论文提出了一种名为RoboMaster的新框架，用于建模多物体之间的动态交互，以改善机器人决策数据的生成。与以往方法不同，RoboMaster将交互过程分为三个子阶段：预交互、交互和后交互，分别使用主导物体的特征进行建模。通过这种方式，RoboMaster有效地解决了多物体特征融合带来的问题，提高了视觉质量。实验结果表明，该方法在复杂的机器人操作任务中表现优异，达到了新的最先进水平。'}}}, {'id': 'https://huggingface.co/papers/2506.01853', 'title': 'ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding', 'url': 'https://huggingface.co/papers/2506.01853', 'abstract': 'A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.  \t\t\t\t\tAI-generated summary \t\t\t\t Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni-a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, by performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset. Our work provides an effective attempt at extending multimodal models with basic 3D capabilities, which contributes to future research in 3D-native AI. Project page: https://github.com/JAMESYJL/ShapeLLM-Omni', 'score': 15, 'issue_id': 4091, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '241ff6937e6642f5', 'authors': ['Junliang Ye', 'Zhengyi Wang', 'Ruowen Zhao', 'Shenghao Xie', 'Jun Zhu'], 'affiliations': ['Peking University', 'ShengShu', 'Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01853.jpg', 'data': {'categories': ['#synthetic', '#3d', '#games', '#dataset', '#agi', '#multimodal'], 'emoji': '🧊', 'ru': {'title': 'Революция в 3D: языковая модель, понимающая трехмерное пространство', 'desc': 'Исследователи представили ShapeLLM-Omni - нативную 3D большую языковую модель, способную понимать и генерировать 3D-объекты и текст. Модель обучена с использованием 3D векторного квантованного вариационного автоэнкодера (VQVAE) и нового набора данных 3D-Alpaca. ShapeLLM-Omni расширяет возможности мультимодальных моделей, добавляя базовые 3D-возможности. Это открывает новые перспективы для исследований в области 3D-нативного искусственного интеллекта.'}, 'en': {'title': 'Bridging Text and 3D: ShapeLLM-Omni Unleashes Multimodal Potential', 'desc': 'ShapeLLM-Omni is a novel large language model designed to understand and generate 3D assets alongside text. It utilizes a 3D vector-quantized variational autoencoder (VQVAE) to efficiently represent and reconstruct 3D shapes in a discrete latent space. The model is trained on a new dataset called 3D-Alpaca, which includes diverse tasks such as generation, comprehension, and editing of 3D content. This research aims to enhance multimodal AI capabilities by integrating 3D understanding, paving the way for future advancements in 3D-native artificial intelligence.'}, 'zh': {'title': 'ShapeLLM-Omni：开启3D内容生成的新纪元', 'desc': '本文提出了一种名为ShapeLLM-Omni的原生3D大型语言模型，旨在理解和生成3D资产及文本。该模型使用3D向量量化变分自编码器（VQVAE）进行训练，将3D对象映射到离散潜在空间，以实现高效准确的形状表示和重建。我们还构建了一个名为3D-Alpaca的大规模连续训练数据集，涵盖生成、理解和编辑，为未来的研究和训练提供了丰富的资源。通过对Qwen-2.5-vl-7B-Instruct模型进行基于指令的训练，我们的工作为扩展多模态模型的基本3D能力提供了有效的尝试。'}}}, {'id': 'https://huggingface.co/papers/2505.24760', 'title': 'REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards', 'url': 'https://huggingface.co/papers/2505.24760', 'abstract': 'We introduce Reasoning Gym (RG), a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games. Its key innovation is the ability to generate virtually infinite training data with adjustable complexity, unlike most previous reasoning datasets, which are typically fixed. This procedural generation approach allows for continuous evaluation across varying difficulty levels. Our experimental results demonstrate the efficacy of RG in both evaluating and reinforcement learning of reasoning models.', 'score': 15, 'issue_id': 4088, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'c7587646c2140ddd', 'authors': ['Zafir Stojanovski', 'Oliver Stanley', 'Joe Sharratt', 'Richard Jones', 'Abdulhakeem Adefioye', 'Jean Kaddour', 'Andreas Köpf'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2505.24760.jpg', 'data': {'categories': ['#rl', '#benchmark', '#reasoning', '#games', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Бесконечная тренировка ИИ в искусстве рассуждений', 'desc': 'Reasoning Gym (RG) - это библиотека сред для обучения с подкреплением в задачах рассуждения с проверяемыми наградами. Она включает более 100 генераторов данных и верификаторов в различных областях, таких как алгебра, арифметика, вычисления, познание, геометрия, теория графов, логика и игры. Ключевое преимущество RG - возможность генерировать практически бесконечные обучающие данные с настраиваемой сложностью, в отличие от большинства существующих наборов данных фиксированного размера. Экспериментальные результаты подтверждают эффективность RG как для оценки, так и для обучения с подкреплением моделей рассуждения.'}, 'en': {'title': 'Unlock Infinite Reasoning with Reasoning Gym!', 'desc': 'The paper presents Reasoning Gym (RG), a new library designed for reinforcement learning that focuses on reasoning tasks with verifiable rewards. RG includes over 100 data generators and verifiers across diverse domains such as algebra, logic, and games, enabling a wide range of reasoning challenges. A significant feature of RG is its ability to create an almost limitless amount of training data with customizable complexity, which is a departure from traditional fixed reasoning datasets. The authors show that RG effectively supports the evaluation and training of reasoning models in reinforcement learning settings.'}, 'zh': {'title': '推理训练场：无限生成，持续评估', 'desc': '我们介绍了推理训练场（Reasoning Gym，RG），这是一个用于强化学习的推理环境库，具有可验证的奖励。它提供了超过100个数据生成器和验证器，涵盖代数、算术、计算、认知、几何、图论、逻辑和各种常见游戏等多个领域。其关键创新在于能够生成几乎无限的训练数据，并且可以调整复杂性，这与大多数固定的推理数据集不同。我们的实验结果表明，RG在评估和强化学习推理模型方面的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.00996', 'title': 'Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models', 'url': 'https://huggingface.co/papers/2506.00996', 'abstract': "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/", 'score': 14, 'issue_id': 4090, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': '25ea795d193b8719', 'authors': ['Kinam Kim', 'Junha Hyung', 'Jaegul Choo'], 'affiliations': ['KAIST AI'], 'pdf_title_img': 'assets/pdf/title_img/2506.00996.jpg', 'data': {'categories': ['#diffusion', '#inference', '#video', '#optimization', '#training'], 'emoji': '🎬', 'ru': {'title': 'Эффективная адаптация видеомоделей с минимальными данными', 'desc': 'Метод Temporal In-Context Fine-Tuning (TIC-FT) улучшает предобученные модели диффузии видео для разнообразных задач условной генерации с минимальными данными и без изменения архитектуры. TIC-FT объединяет кадры условия и целевые кадры по временной оси, вставляя промежуточные буферные кадры с постепенно увеличивающимся уровнем шума. Этот подход не требует архитектурных изменений и достигает высокой производительности всего на 10-30 обучающих примерах. Эксперименты показывают, что TIC-FT превосходит существующие базовые методы по точности соответствия условиям и визуальному качеству, оставаясь при этом высокоэффективным как при обучении, так и при инференсе.'}, 'en': {'title': 'Efficient Video Generation with Minimal Data Using TIC-FT', 'desc': "Temporal In-Context Fine-Tuning (TIC-FT) is a novel method that improves pretrained video diffusion models for various conditional generation tasks without needing extensive data or changing the model architecture. It works by combining condition and target frames along the time axis and adding intermediate buffer frames with increasing noise, which helps maintain the model's temporal coherence. This approach allows for effective fine-tuning with as few as 10-30 training samples, making it efficient and scalable. TIC-FT has been shown to outperform existing methods in generating high-quality videos while ensuring fidelity to the given conditions."}, 'zh': {'title': '时间上下文微调：高效的视频生成新方法', 'desc': '本文提出了一种新的方法，称为时间上下文微调（TIC-FT），用于增强预训练的视频扩散模型，以实现多样化的条件生成任务。TIC-FT通过在时间轴上连接条件帧和目标帧，并插入逐渐增加噪声水平的中间缓冲帧，来实现平滑过渡，从而与预训练模型的时间动态对齐。该方法无需对模型架构进行修改，且只需10到30个训练样本即可实现强大的性能。实验结果表明，TIC-FT在条件保真度和视觉质量方面均优于现有基线，同时在训练和推理过程中保持高效。'}}}, {'id': 'https://huggingface.co/papers/2505.24846', 'title': 'MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning', 'url': 'https://huggingface.co/papers/2505.24846', 'abstract': 'MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.  \t\t\t\t\tAI-generated summary \t\t\t\t Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization.', 'score': 11, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'ed5c25a307e9093d', 'authors': ['Jingyan Shen', 'Jiarui Yao', 'Rui Yang', 'Yifan Sun', 'Feng Luo', 'Rui Pan', 'Tong Zhang', 'Han Zhao'], 'affiliations': ['Columbia University', 'Rice University', 'University of Illinois at Urbana-Champaign'], 'pdf_title_img': 'assets/pdf/title_img/2505.24846.jpg', 'data': {'categories': ['#dataset', '#training', '#rlhf', '#alignment'], 'emoji': '🎭', 'ru': {'title': 'MiCRo: Персонализация языковых моделей без дополнительных аннотаций', 'desc': 'MiCRo - это двухэтапная система для улучшения персонализированного обучения предпочтениям в больших языковых моделях. Она использует наборы данных бинарных предпочтений и динамически адаптирует веса смесей на основе контекста. MiCRo эффективно захватывает разнообразные человеческие предпочтения без необходимости в детальных аннотациях. Эксперименты показывают, что MiCRo значительно улучшает последующую персонализацию в языковых моделях.'}, 'en': {'title': 'MiCRo: Dynamic Personalization for Diverse Human Preferences', 'desc': 'MiCRo is a two-stage framework designed to enhance personalized preference learning for large language models (LLMs). It utilizes binary preference datasets and employs a context-aware mixture modeling approach to better capture the diverse preferences of humans. The framework dynamically adjusts mixture weights based on the context, allowing for more accurate and scalable preference adaptation. Experimental results show that MiCRo significantly improves the ability of LLMs to personalize responses according to varied human preferences.'}, 'zh': {'title': 'MiCRo：捕捉多样化人类偏好的新框架', 'desc': 'MiCRo是一个两阶段框架，旨在改善大型语言模型的个性化偏好学习。它利用二元偏好数据集，并根据上下文动态调整混合权重，从而有效捕捉多样化的人类偏好。该方法通过引入上下文感知的混合建模，解决了传统模型无法充分反映人类多样性的问题。实验结果表明，MiCRo在多个偏好数据集上表现出色，显著提升了下游个性化效果。'}}}, {'id': 'https://huggingface.co/papers/2505.24298', 'title': 'AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning', 'url': 'https://huggingface.co/papers/2505.24298', 'abstract': 'AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Reinforcement learning (RL) has become a trending paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous by alternating generation and training in a batch setting, where the rollouts in each training batch are generated by the same (or latest) model. This stabilizes RL training but suffers from severe system-level inefficiency. Generation must wait until the longest output in the batch is completed before model update, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.57times training speedup compared to the best synchronous systems with the same number of GPUs and matched or even improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.', 'score': 10, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'fad566ec1d2ba264', 'authors': ['Wei Fu', 'Jiaxuan Gao', 'Xujie Shen', 'Chen Zhu', 'Zhiyu Mei', 'Chuyi He', 'Shusheng Xu', 'Guo Wei', 'Jun Mei', 'Jiashu Wang', 'Tongkai Yang', 'Binhang Yuan', 'Yi Wu'], 'affiliations': ['Ant Research', 'HKUST', 'IIIS, Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24298.jpg', 'data': {'categories': ['#training', '#reasoning', '#optimization', '#rl'], 'emoji': '🚀', 'ru': {'title': 'AReaL: Асинхронное обучение с подкреплением для ускорения ИИ', 'desc': 'AReaL - это асинхронная система обучения с подкреплением для больших языковых моделей. Она разделяет процессы генерации и обучения, что позволяет достичь более высокой утилизации GPU и ускорения обучения до 2.57 раз на задачах рассуждения. Система использует непрерывную генерацию данных и обновление модели, а также ряд оптимизаций для стабилизации обучения. Эксперименты показали превосходство AReaL над синхронными системами по скорости при сохранении или улучшении качества.'}, 'en': {'title': 'AReaL: Revolutionizing Reinforcement Learning with Asynchronous Training', 'desc': 'AReaL is an innovative reinforcement learning system designed to enhance the training of large language models by decoupling the generation and training processes. This fully asynchronous approach allows for continuous output generation without waiting for the longest tasks to finish, leading to improved GPU utilization. By balancing the workload between rollout and training workers, AReaL effectively manages data staleness and employs a modified Proximal Policy Optimization (PPO) to optimize training with outdated samples. Experimental results demonstrate that AReaL can achieve up to 2.57 times faster training speeds while maintaining or improving performance on reasoning tasks.'}, 'zh': {'title': 'AReaL：异步强化学习的高效训练新模式', 'desc': 'AReaL是一种完全异步的强化学习系统，它将生成和训练解耦，从而提高GPU的利用率，并在推理任务上实现了高达2.57倍的训练加速。传统的大规模强化学习系统通常是同步的，生成和训练交替进行，这导致了系统效率低下。AReaL通过让生成工作者持续生成新输出，而训练工作者在收集到一批数据后立即更新模型，解决了这一问题。通过平衡生成和训练工作者的工作负载，AReaL有效控制了数据的过时性，并采用了增强过时性的PPO变体来更好地处理过时的训练样本。'}}}, {'id': 'https://huggingface.co/papers/2505.23907', 'title': 'Cora: Correspondence-aware image editing using few step diffusion', 'url': 'https://huggingface.co/papers/2505.23907', 'abstract': 'Cora framework enhances image editing through correspondence-aware noise correction and interpolated attention maps, excelling in structure and texture preservation and generation.  \t\t\t\t\tAI-generated summary \t\t\t\t Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora, a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.', 'score': 8, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '7de1457440a0b449', 'authors': ['Amirhossein Almohammadi', 'Aryan Mikaeili', 'Sauradip Nag', 'Negar Hassanpour', 'Andrea Tagliasacchi', 'Ali Mahdavi-Amiri'], 'affiliations': ['Google Deepmind, Canada', 'Huawei, Canada', 'Simon Fraser University, Canada', 'University of Toronto'], 'pdf_title_img': 'assets/pdf/title_img/2505.23907.jpg', 'data': {'categories': ['#cv', '#video', '#diffusion'], 'emoji': '🖼️', 'ru': {'title': 'Умное редактирование изображений с сохранением структуры и текстур', 'desc': 'Cora - это новая система редактирования изображений, использующая коррекцию шума с учетом соответствий и интерполированные карты внимания. Она позволяет точно переносить текстуры и структуры между исходным и целевым изображениями, сохраняя ключевые атрибуты оригинала. Cora превосходит аналоги в сохранении структуры, текстур и идентичности при различных типах редактирования. Система обеспечивает контроль баланса между генерацией нового контента и сохранением исходного.'}, 'en': {'title': 'Cora: Revolutionizing Image Editing with Precision and Control', 'desc': 'The Cora framework improves image editing by using advanced techniques like correspondence-aware noise correction and interpolated attention maps. It effectively aligns textures and structures between source and target images, allowing for accurate texture transfer and content generation. This method addresses common issues in image editing, such as preserving key attributes and avoiding artifacts during significant structural changes. Extensive testing shows that Cora maintains high quality in structure, texture, and identity across various editing tasks, outperforming existing methods.'}, 'zh': {'title': 'Cora：图像编辑的新突破', 'desc': 'Cora框架通过引入对应感知噪声校正和插值注意力图，增强了图像编辑的效果。它能够在源图像和目标图像之间对齐纹理和结构，从而实现准确的纹理转移和必要的新内容生成。Cora在内容生成和保留之间提供了良好的控制，能够有效处理姿态变化、物体添加和纹理细化等多种编辑任务。实验结果表明，Cora在结构、纹理和身份的保持上表现优异，用户研究也证实了其优于其他方法的效果。'}}}, {'id': 'https://huggingface.co/papers/2505.23001', 'title': 'DyePack: Provably Flagging Test Set Contamination in LLMs Using\n  Backdoors', 'url': 'https://huggingface.co/papers/2505.23001', 'abstract': 'DyePack, a framework using backdoor attacks, identifies models that leveraged benchmark test sets during training by introducing benign backdoor samples, ensuring precise false positive rates while preventing false accusations.  \t\t\t\t\tAI-generated summary \t\t\t\t Open benchmarks are essential for evaluating and advancing large language models, offering reproducibility and transparency. However, their accessibility makes them likely targets of test set contamination. In this work, we introduce DyePack, a framework that leverages backdoor attacks to identify models that used benchmark test sets during training, without requiring access to the loss, logits, or any internal details of the model. Like how banks mix dye packs with their money to mark robbers, DyePack mixes backdoor samples with the test data to flag models that trained on it. We propose a principled design incorporating multiple backdoors with stochastic targets, enabling exact false positive rate (FPR) computation when flagging every model. This provably prevents false accusations while providing strong evidence for every detected case of contamination. We evaluate DyePack on five models across three datasets, covering both multiple-choice and open-ended generation tasks. For multiple-choice questions, it successfully detects all contaminated models with guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard using eight backdoors. For open-ended generation tasks, it generalizes well and identifies all contaminated models on Alpaca with a guaranteed false positive rate of just 0.127% using six backdoors.', 'score': 8, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'cd584a75fce48ae2', 'authors': ['Yize Cheng', 'Wenxiao Wang', 'Mazda Moayeri', 'Soheil Feizi'], 'affiliations': ['University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2505.23001.jpg', 'data': {'categories': ['#dataset', '#benchmark', '#security', '#leakage'], 'emoji': '🕵️', 'ru': {'title': 'DyePack: Ловушка для нечестных моделей машинного обучения', 'desc': 'DyePack - это фреймворк, использующий атаки типа backdoor для выявления моделей, которые использовали тестовые наборы данных во время обучения. Он вводит безвредные образцы backdoor в тестовые данные, чтобы пометить модели, обучавшиеся на них. DyePack обеспечивает точный расчет уровня ложноположительных результатов и предотвращает ложные обвинения. Фреймворк был успешно протестирован на пяти моделях и трех наборах данных, охватывающих задачи с множественным выбором и открытой генерацией.'}, 'en': {'title': 'DyePack: Safeguarding Model Integrity with Backdoor Detection', 'desc': 'DyePack is a novel framework designed to detect models that have been trained using benchmark test sets by employing backdoor attacks. It introduces benign backdoor samples into the test data, allowing for the identification of contaminated models without needing access to their internal workings. The framework ensures precise computation of false positive rates, effectively preventing wrongful accusations against models. Through extensive evaluation, DyePack demonstrates its capability to accurately flag contaminated models across various tasks while maintaining low false positive rates.'}, 'zh': {'title': 'DyePack：精准识别训练中使用基准测试集的模型', 'desc': 'DyePack是一个利用后门攻击的框架，用于识别在训练中使用基准测试集的模型。它通过引入良性后门样本，确保准确的假阳性率，同时防止错误指控。DyePack的设计结合了多个具有随机目标的后门，使得在标记每个模型时能够精确计算假阳性率。通过在多个模型和数据集上的评估，DyePack成功检测到所有受污染的模型，且假阳性率极低。'}}}, {'id': 'https://huggingface.co/papers/2506.00577', 'title': 'Reasoning Like an Economist: Post-Training on Economic Problems Induces\n  Strategic Generalization in LLMs', 'url': 'https://huggingface.co/papers/2506.00577', 'abstract': 'Post-training techniques such as Supervised Fine-Tuning and Reinforcement Learning with Verifiable Rewards improve the reasoning and economic rationality of Large Language Models in multi-agent scenarios through domain-aligned training.  \t\t\t\t\tAI-generated summary \t\t\t\t Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon .', 'score': 7, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0ed5d0b7064f9962', 'authors': ['Yufa Zhou', 'Shaobo Wang', 'Xingyu Dong', 'Xiangqi Jin', 'Yifang Chen', 'Yue Min', 'Kexin Yang', 'Xingzhang Ren', 'Dayiheng Liu', 'Linfeng Zhang'], 'affiliations': ['Duke University', 'EPIC Lab, Shanghai Jiao Tong University', 'Qwen Team, Alibaba Group', 'The University of Chicago', 'University of Pennsylvania'], 'pdf_title_img': 'assets/pdf/title_img/2506.00577.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#agents', '#open_source', '#reasoning', '#games', '#dataset'], 'emoji': '💡', 'ru': {'title': 'Дообучение языковых моделей улучшает экономические рассуждения', 'desc': 'Исследование показывает, что методы дообучения, такие как контролируемая тонкая настройка (SFT) и обучение с подкреплением с проверяемыми наградами (RLVR), могут улучшить рассуждения и экономическую рациональность больших языковых моделей в многоагентных сценариях. Авторы представляют Recon - 7B-параметровую модель, дообученную на наборе данных из 2100 задач по экономическим рассуждениям. Оценка на экономических тестах и многоагентных играх показала значительное улучшение структурированных рассуждений и экономической рациональности. Результаты подчеркивают перспективность дообучения в конкретной предметной области для улучшения рассуждений и согласования агентов.'}, 'en': {'title': 'Enhancing Economic Reasoning in LLMs through Post-Training Techniques', 'desc': "This paper investigates how post-training techniques can enhance the performance of Large Language Models (LLMs) in multi-agent environments. It specifically focuses on Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) to improve reasoning and economic decision-making. The authors introduce a model called Recon, which is trained on a dataset of economic reasoning problems, demonstrating significant advancements in structured reasoning capabilities. The findings suggest that domain-aligned post-training can effectively improve LLMs' reasoning and alignment in complex scenarios."}, 'zh': {'title': '后训练技术提升智能体推理与经济理性', 'desc': '本论文探讨了后训练技术如何提升大型语言模型在多智能体系统中的推理能力和经济理性。我们采用监督微调和可验证奖励的强化学习方法，针对经济推理进行训练。通过引入Recon模型，我们在高质量经济推理问题的数据集上进行了后训练，并在经济推理基准和多智能体游戏中进行了评估。结果显示，经过后训练的模型在结构化推理和经济理性方面有显著提升，证明了领域对齐的后训练在增强推理和智能体对齐方面的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.23977', 'title': 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL', 'url': 'https://huggingface.co/papers/2505.23977', 'abstract': 'VisualSphinx provides a large-scale synthetic dataset to improve multimodal reasoning in vision language models, enhancing performance on various logical reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Vision language models (VLMs) are expected to perform effective multimodal reasoning and make logically coherent decisions, which is critical to tasks such as diagram understanding and spatial problem solving. However, current VLM reasoning lacks large-scale and well-structured training datasets. To bridge this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic visual logical reasoning training data. To tackle the challenge of image synthesis with grounding answers, we propose a rule-to-image synthesis pipeline, which extracts and expands puzzle rules from seed questions and generates the code of grounding synthesis image synthesis for puzzle sample assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx benefit from logical coherence and readability of our dataset and exhibit improved performance on logical reasoning tasks. The enhanced reasoning capabilities developed from VisualSphinx also benefit other reasoning tasks such as algebraic reasoning, arithmetic reasoning and geometry reasoning.', 'score': 7, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'fef2cab0e56bc9bd', 'authors': ['Yichen Feng', 'Zhangchen Xu', 'Fengqing Jiang', 'Yuetai Li', 'Bhaskar Ramasubramanian', 'Luyao Niu', 'Bill Yuchen Lin', 'Radha Poovendran'], 'affiliations': ['University of Washington', 'Western Washington University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23977.jpg', 'data': {'categories': ['#synthetic', '#cv', '#multimodal', '#reasoning', '#dataset'], 'emoji': '🧠', 'ru': {'title': 'Синтетические данные для улучшения логического мышления ИИ', 'desc': 'VisualSphinx - это крупномасштабный синтетический набор данных для улучшения мультимодального рассуждения в визуально-языковых моделях. Он создан с помощью специального конвейера синтеза изображений на основе правил. Эксперименты показывают, что обучение на VisualSphinx улучшает способности моделей к логическому рассуждению. Усовершенствованные навыки рассуждения, полученные на VisualSphinx, также полезны для других задач, таких как алгебраические, арифметические и геометрические рассуждения.'}, 'en': {'title': 'Enhancing Multimodal Reasoning with VisualSphinx', 'desc': 'VisualSphinx is a synthetic dataset designed to enhance multimodal reasoning in vision language models (VLMs). It addresses the lack of large-scale, structured training data necessary for effective logical reasoning in tasks like diagram understanding. The dataset is created using a rule-to-image synthesis pipeline that generates images based on logical rules extracted from questions. Experiments show that VLMs trained on VisualSphinx demonstrate improved logical coherence and performance across various reasoning tasks, including algebra and geometry.'}, 'zh': {'title': 'VisualSphinx：提升视觉语言模型的逻辑推理能力', 'desc': 'VisualSphinx是一个大规模的合成数据集，旨在提升视觉语言模型在多模态推理方面的表现。该数据集专注于逻辑推理任务，解决了当前模型缺乏结构化训练数据的问题。通过规则到图像的合成流程，VisualSphinx能够生成与问题相关的图像，增强模型的逻辑一致性和可读性。实验表明，使用VisualSphinx训练的视觉语言模型在逻辑推理、代数推理、算术推理和几何推理等任务上表现更佳。'}}}, {'id': 'https://huggingface.co/papers/2505.23059', 'title': 'From Token to Action: State Machine Reasoning to Mitigate Overthinking\n  in Information Retrieval', 'url': 'https://huggingface.co/papers/2505.23059', 'abstract': 'State Machine Reasoning (SMR) improves information retrieval performance and reduces token usage in large language models by addressing overthinking through a discrete action framework.  \t\t\t\t\tAI-generated summary \t\t\t\t Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.', 'score': 7, 'issue_id': 4088, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': '83af42c01de2e64c', 'authors': ['Dohyeon Lee', 'Yeonseok Jeong', 'Seung-won Hwang'], 'affiliations': ['Computer Science and Engineering, Seoul National University', 'Interdisciplinary Program in Artificial Intelligence, Seoul National University'], 'pdf_title_img': 'assets/pdf/title_img/2505.23059.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#reasoning', '#optimization', '#dataset'], 'emoji': '🤖', 'ru': {'title': 'SMR: Эффективные рассуждения для языковых моделей', 'desc': 'Статья представляет новый метод рассуждений для больших языковых моделей под названием State Machine Reasoning (SMR). SMR улучшает производительность информационного поиска и снижает использование токенов, решая проблему избыточных рассуждений. Метод использует дискретные действия (уточнение, переранжирование, остановка) для более точного контроля процесса рассуждений. Эксперименты показали, что SMR повышает качество поиска на 3.4% при снижении использования токенов на 74.4%.'}, 'en': {'title': 'Streamlining Retrieval with State Machine Reasoning', 'desc': 'State Machine Reasoning (SMR) is a new framework designed to enhance information retrieval in large language models by minimizing unnecessary complexity. It tackles the problem of overthinking, which often results in lengthy and repetitive outputs that do not improve results. SMR introduces a set of discrete actions that allow models to make more efficient decisions, leading to better performance and reduced token usage. Experiments demonstrate that SMR significantly boosts retrieval accuracy while being adaptable across different models without needing specific adjustments.'}, 'zh': {'title': '状态机推理：提升检索效率，减少资源消耗', 'desc': '状态机推理（SMR）通过离散动作框架来改善信息检索性能，并减少大型语言模型的令牌使用，解决了过度思考的问题。该方法识别了信息检索中的两个主要挑战：冗余轨迹和误导性推理。SMR采用基于转移的推理框架，包含精细控制的离散动作（如精炼、重新排序和停止），支持早期停止。实验结果表明，SMR在BEIR和BRIGHT基准上提高了3.4%的检索性能，同时减少了74.4%的令牌使用。'}}}, {'id': 'https://huggingface.co/papers/2506.01881', 'title': 'WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent\n  Triggerability in Task-Oriented Dialogue', 'url': 'https://huggingface.co/papers/2506.01881', 'abstract': 'STORM frameworks facilitates collaborative intent formation in task-oriented dialogue systems by modeling asymmetric information dynamics between UserLLM and AgentLLM.  \t\t\t\t\tAI-generated summary \t\t\t\t Task-oriented dialogue systems often face difficulties when user utterances seem semantically complete but lack necessary structural information for appropriate system action. This arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. Current LLM-based agents cannot effectively distinguish between linguistically complete and contextually triggerable expressions, lacking frameworks for collaborative intent formation. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing expression trajectories and latent cognitive transitions, enabling systematic analysis of collaborative understanding development. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.', 'score': 6, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'e82ff37de8341d1a', 'authors': ['Yaoyao Qian', 'Jindan Huang', 'Yuanli Wang', 'Simon Yu', 'Kyrie Zhixuan Zhou', 'Jiayuan Mao', 'Mingfu Liang', 'Hanhan Zhou'], 'affiliations': ['Boston University, Boston, MA', 'George Washington University, Washington, DC', 'Massachusetts Institute of Technology, Cambridge, MA', 'Northeastern University, Boston, MA', 'Northwestern University, Evanston, IL', 'Tufts University, Medford, MA', 'University of Texas at San Antonio, San Antonio, TX'], 'pdf_title_img': 'assets/pdf/title_img/2506.01881.jpg', 'data': {'categories': ['#reasoning', '#multimodal', '#alignment', '#agents'], 'emoji': '🌪️', 'ru': {'title': 'STORM: асимметричное моделирование намерений в диалоговых системах', 'desc': 'Статья представляет фреймворк STORM для моделирования асимметричной динамики информации в диалоговых системах. STORM использует две языковые модели - UserLLM и AgentLLM - для имитации процесса формирования намерений пользователя. Фреймворк позволяет создавать аннотированные корпуса, отражающие эволюцию понимания в ходе диалога. Эксперименты показали, что умеренная неопределенность может превосходить полную прозрачность в некоторых сценариях взаимодействия человека и ИИ.'}, 'en': {'title': 'Enhancing Dialogue Systems through Collaborative Intent Formation', 'desc': 'The STORM framework enhances task-oriented dialogue systems by addressing the challenges of asymmetric information between users and AI agents. It recognizes that users often do not fully articulate their needs, leading to difficulties in intent recognition by the system. By modeling the dynamics of information exchange, STORM enables the development of annotated datasets that track how users and agents collaboratively form intents. The research shows that a moderate level of uncertainty can improve performance in certain contexts, suggesting that complete transparency is not always the best approach in human-AI interactions.'}, 'zh': {'title': 'STORM：促进人机协作的意图形成', 'desc': 'STORM框架通过建模用户和代理之间的信息不对称动态，促进了任务导向对话系统中的协作意图形成。用户的表达虽然在语言上完整，但往往缺乏系统所需的结构信息，导致系统无法正确响应。STORM框架能够捕捉表达轨迹和潜在的认知转变，从而系统化分析协作理解的发展。实验结果表明，在某些情况下，适度的不确定性（40-60%）可以优于完全透明的信息，这为人机协作中的信息完整性提供了新的思考。'}}}, {'id': 'https://huggingface.co/papers/2506.01667', 'title': 'EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models', 'url': 'https://huggingface.co/papers/2506.01667', 'abstract': 'EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large Multimodal Models (LMMs) have demonstrated strong performance in various vision-language tasks. However, they often struggle to comprehensively understand Earth Observation (EO) data, which is critical for monitoring the environment and the effects of human activity on it. In this work, we present EarthMind, a novel vision-language framework for multi-granular and multi-sensor EO data understanding. EarthMind features two core components: (1) Spatial Attention Prompting (SAP), which reallocates attention within the LLM to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns heterogeneous modalities into a shared space and adaptively reweighs tokens based on their information density for effective fusion. To facilitate multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive benchmark with over 2,000 human-annotated multi-sensor image-question pairs, covering a wide range of perception and reasoning tasks. Extensive experiments demonstrate the effectiveness of EarthMind. It achieves state-of-the-art performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in scale. Moreover, EarthMind outperforms existing methods on multiple public EO benchmarks, showcasing its potential to handle both multi-granular and multi-sensor challenges in a unified framework.', 'score': 6, 'issue_id': 4093, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '90528034771977ef', 'authors': ['Yan Shu', 'Bin Ren', 'Zhitong Xiong', 'Danda Pani Paudel', 'Luc Van Gool', 'Begum Demir', 'Nicu Sebe', 'Paolo Rota'], 'affiliations': ['INSAIT, Sofia University St. Kliment Ohridski', 'Technical University of Munich', 'Technische Universität Berlin', 'University of Pisa', 'University of Trento'], 'pdf_title_img': 'assets/pdf/title_img/2506.01667.jpg', 'data': {'categories': ['#benchmark', '#cv', '#survey', '#reasoning', '#multimodal'], 'emoji': '🌍', 'ru': {'title': 'EarthMind: Эффективное понимание многосенсорных данных наблюдения Земли', 'desc': 'EarthMind - это новая система анализа данных дистанционного зондирования Земли, использующая методы обработки естественного языка и компьютерного зрения. Она включает в себя пространственное внимание и кросс-модальное слияние для эффективной работы с разнородными данными. EarthMind превосходит более крупные модели на специализированных тестах, несмотря на меньший размер (4 миллиарда параметров). Система способна решать широкий спектр задач восприятия и рассуждения для многосенсорных данных наблюдения Земли.'}, 'en': {'title': 'EarthMind: Revolutionizing Earth Observation with Vision-Language Fusion', 'desc': "EarthMind is a new framework designed to improve the understanding of Earth Observation (EO) data by combining vision and language processing. It uses Spatial Attention Prompting (SAP) to focus on important details at the pixel level, enhancing the model's ability to interpret images. Additionally, Cross-modal Fusion aligns different types of data, allowing the model to weigh information based on its relevance. This approach not only outperforms larger models like GPT-4o on specialized benchmarks but also effectively handles complex multi-sensor data tasks."}, 'zh': {'title': 'EarthMind：高效理解地球观测数据的创新框架', 'desc': 'EarthMind是一个视觉-语言框架，旨在高效理解多粒度和多传感器的地球观测数据。它采用空间注意力提示和跨模态融合技术，能够在专门基准测试中超越更大的模型。EarthMind的两个核心组件分别是空间注意力提示（SAP），用于增强像素级理解，以及跨模态融合，能够将不同模态对齐到共享空间并根据信息密度自适应调整权重。通过EarthMind-Bench基准测试，EarthMind在多个公共地球观测基准上表现优异，展示了其在统一框架下处理多粒度和多传感器挑战的潜力。'}}}, {'id': 'https://huggingface.co/papers/2505.24625', 'title': 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors', 'url': 'https://huggingface.co/papers/2505.24625', 'abstract': "A novel Video-3D Geometry Large Language Model (VG LLM) extracts 3D information directly from video sequences to enhance 3D scene understanding without additional 3D data, achieving competitive results in various tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Previous research has investigated the application of Multimodal Large Language Models (MLLMs) in understanding 3D scenes by interpreting them as videos. These approaches generally depend on comprehensive 3D data inputs, such as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research, we advance this field by enhancing the capability of MLLMs to understand and reason in 3D spaces directly from video data, without the need for additional 3D input. We propose a novel and efficient method, the Video-3D Geometry Large Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that extracts 3D prior information from video sequences. This information is integrated with visual tokens and fed into the MLLM. Extensive experiments have shown that our method has achieved substantial improvements in various tasks related to 3D scene understanding and spatial reasoning, all directly learned from video sources. Impressively, our 4B model, which does not rely on explicit 3D data inputs, achieves competitive results compared to existing state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the VSI-Bench evaluations.", 'score': 6, 'issue_id': 4087, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '8bfa132788ee6990', 'authors': ['Duo Zheng', 'Shijia Huang', 'Yanyang Li', 'Liwei Wang'], 'affiliations': ['The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2505.24625.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#video', '#games', '#architecture', '#3d'], 'emoji': '🎥', 'ru': {'title': 'Революция в 3D-понимании: извлечение геометрии напрямую из видео', 'desc': 'Исследователи представили новую модель Video-3D Geometry Large Language Model (VG LLM), которая извлекает трехмерную информацию непосредственно из видеопоследовательностей для улучшения понимания 3D-сцен. В отличие от предыдущих подходов, VG LLM не требует дополнительных 3D-данных, таких как облака точек или реконструированные карты с видом сверху. Модель использует энкодер 3D-визуальной геометрии для извлечения априорной 3D-информации из видео, которая затем интегрируется с визуальными токенами и подается в мультимодальную языковую модель. Эксперименты показали, что VG LLM достигает конкурентоспособных результатов в различных задачах 3D-понимания сцен и пространственного рассуждения, превосходя некоторые современные методы.'}, 'en': {'title': 'Revolutionizing 3D Scene Understanding from Video Alone!', 'desc': 'The paper introduces the Video-3D Geometry Large Language Model (VG LLM), which enhances 3D scene understanding by extracting 3D information directly from video sequences. Unlike previous methods that require extensive 3D data inputs, VG LLM operates solely on video data, making it more efficient. It utilizes a 3D visual geometry encoder to gather 3D prior information, which is then combined with visual tokens for processing in a Multimodal Large Language Model. The results demonstrate that VG LLM achieves competitive performance in 3D tasks, outperforming existing models without the need for additional 3D data.'}, 'zh': {'title': '视频驱动的3D理解新突破', 'desc': '本文提出了一种新颖的视频-3D几何大语言模型（VG LLM），能够直接从视频序列中提取3D信息，从而增强3D场景理解，而无需额外的3D数据。该模型利用3D视觉几何编码器，从视频中提取3D先验信息，并将其与视觉标记结合，输入到多模态大语言模型中。通过大量实验，结果表明该方法在3D场景理解和空间推理等任务上取得了显著的改进。值得注意的是，我们的4B模型在不依赖显式3D数据输入的情况下，达到了与现有最先进方法相媲美的结果，甚至在VSI-Bench评估中超越了Gemini-1.5-Pro。'}}}, {'id': 'https://huggingface.co/papers/2506.01413', 'title': 'Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models', 'url': 'https://huggingface.co/papers/2506.01413', 'abstract': 'Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data are available at https://github.com/yuleiqin/RAIF.', 'score': 5, 'issue_id': 4088, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '3f0db6c1e3cc1878', 'authors': ['Yulei Qin', 'Gang Li', 'Zongyi Li', 'Zihan Xu', 'Yuchen Shi', 'Zhekai Lin', 'Xiao Cui', 'Ke Li', 'Xing Sun'], 'affiliations': ['Tencent YouTu Lab', 'The Chinese University of Hong Kong', 'Xiamen University'], 'pdf_title_img': 'assets/pdf/title_img/2506.01413.jpg', 'data': {'categories': ['#training', '#rl', '#benchmark', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Рассуждай умнее, а не больше: новый подход к обучению языковых моделей', 'desc': 'Эта статья посвящена улучшению способности больших языковых моделей (LLM) выполнять сложные инструкции с множественными ограничениями. Авторы предлагают систематический метод, основанный на поощрении рассуждений с помощью обучения с подкреплением. Они используют воспроизводимый метод сбора данных и применяют контрастное обучение для улучшения цепочки рассуждений. Результаты показывают значительное улучшение производительности, сравнимое с увеличением размера модели в несколько раз.'}, 'en': {'title': 'Enhancing LLMs: From Shallow Reasoning to Deep Understanding', 'desc': "This paper addresses the limitations of large language models (LLMs) in following complex instructions, particularly when these instructions involve multiple constraints. The authors critique the traditional chain-of-thought (CoT) approach, which often leads to poor performance due to its tendency to merely rephrase instructions without deep reasoning. To improve LLMs' ability to handle complex tasks, they propose a systematic method that includes decomposing instructions and using reinforcement learning with specific reward signals to enhance reasoning. Their extensive evaluations demonstrate that their approach significantly boosts performance, achieving results comparable to larger models with fewer parameters."}, 'zh': {'title': '提升大型语言模型处理复杂指令的能力', 'desc': '现有的大型语言模型（LLMs）在处理复杂指令时面临挑战，尤其是当指令包含多个并行、链式和分支结构的约束时。本文提出了一种系统的方法，通过激励推理来提升LLMs处理复杂指令的能力。我们利用强化学习（RL）和可验证的规则中心奖励信号，培养模型在指令跟随方面的推理能力。通过对比样本，我们解决了在复杂指令下推理的浅层和非本质特性，从而显著提高了模型的性能。'}}}, {'id': 'https://huggingface.co/papers/2505.24452', 'title': 'Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training', 'url': 'https://huggingface.co/papers/2505.24452', 'abstract': 'A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.  \t\t\t\t\tAI-generated summary \t\t\t\t The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets.While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations.In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient.In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets.First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations.From this framework, we derive the UBA schedule, controlled by a single hyper-parameter varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of varphi.We offer practical guidelines for its selection via theoretical analysis and empirical results.xtensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.', 'score': 5, 'issue_id': 4091, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '82972c2646341cc9', 'authors': ['Anda Tang', 'Yiming Dong', 'Yutao Zeng', 'zhou Xun', 'Zhouchen Lin'], 'affiliations': ['ByteDance Seed', 'Institute for Artificial Intelligence, Peking University', 'Pazhou Laboratory (Huangpu), Guangzhou, Guangdong, China', 'State Key Lab of General AI, School of Intelligence Science and Technology, Peking University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24452.jpg', 'data': {'categories': ['#training', '#optimization'], 'emoji': '📊', 'ru': {'title': 'Умный график обучения: максимум эффективности при ограниченных ресурсах', 'desc': 'Предложен унифицированный график скорости обучения с учетом бюджета для оптимизации обучения в условиях ограниченного количества итераций. Новый подход, названный Unified Budget-Aware (UBA), основан на теоретической базе и учитывает устойчивость к вариациям кривизны ландшафта оптимизации. UBA превосходит традиционные графики для различных задач и архитектур нейронных сетей. Метод контролируется одним гиперпараметром φ, который обеспечивает баланс между гибкостью и простотой.'}, 'en': {'title': 'Optimizing Training with Unified Budget-Aware Learning Rates', 'desc': 'This paper introduces a new learning rate schedule called the Unified Budget-Aware (UBA) schedule, designed to optimize training when there are limits on the number of iterations. Traditional learning rate schedules often rely on trial-and-error and lack a solid theoretical basis, making them inefficient. The UBA schedule is grounded in a novel optimization framework that considers the curvature of the loss landscape, allowing it to adapt better to various tasks and network architectures. Experimental results demonstrate that UBA outperforms standard schedules across different vision and language tasks, providing a more effective training strategy within constrained budgets.'}, 'zh': {'title': '统一预算感知学习率调度，优化有限训练预算', 'desc': '本文提出了一种统一的预算感知学习率调度（UBA），旨在优化在有限迭代预算下的训练效果。传统的学习率调度方法往往依赖经验，缺乏理论基础，而UBA则通过构建一个新的优化框架，考虑了对损失函数曲率变化的鲁棒性。该调度由一个超参数控制，能够在灵活性和简单性之间取得平衡，避免了对每个网络进行数值优化的需求。实验结果表明，UBA在多种视觉和语言任务中，均优于常用的学习率调度方法。'}}}, {'id': 'https://huggingface.co/papers/2506.00338', 'title': 'OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and\n  Cleaning', 'url': 'https://huggingface.co/papers/2506.00338', 'abstract': 'The OWSM project is enhanced with a large-scale, cleaned web dataset, leading to improved multilingual speech models comparable to leading industrial models.  \t\t\t\t\tAI-generated summary \t\t\t\t The Open Whisper-style Speech Models (OWSM) project has developed a series of fully open speech foundation models using academic-scale resources, but their training data remains insufficient. This work enhances OWSM by integrating YODAS, a large-scale web-crawled dataset with a Creative Commons license. However, incorporating YODAS is nontrivial due to its wild nature, which introduces challenges such as incorrect language labels and audio-text misalignments. To address this, we develop a scalable data-cleaning pipeline using public toolkits, yielding a dataset with 166,000 hours of speech across 75 languages. Our new series of OWSM v4 models, trained on this curated dataset alongside existing OWSM data, significantly outperform previous versions on multilingual benchmarks. Our models even match or surpass frontier industrial models like Whisper and MMS in multiple scenarios. We will publicly release the cleaned YODAS data, pre-trained models, and all associated scripts via the ESPnet toolkit.', 'score': 4, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2f4783eb2db68192', 'authors': ['Yifan Peng', 'Shakeel Muhammad', 'Yui Sudo', 'William Chen', 'Jinchuan Tian', 'Chyi-Jiunn Lin', 'Shinji Watanabe'], 'affiliations': ['Carnegie Mellon University, United States', 'Honda Research Institute Japan, Japan'], 'pdf_title_img': 'assets/pdf/title_img/2506.00338.jpg', 'data': {'categories': ['#training', '#low_resource', '#open_source', '#multilingual', '#data', '#audio', '#dataset'], 'emoji': '🌐', 'ru': {'title': 'Открытые речевые модели достигают уровня промышленных стандартов', 'desc': 'Проект OWSM улучшен с помощью масштабного очищенного веб-датасета, что привело к усовершенствованию мультиязычных речевых моделей. Разработан масштабируемый конвейер для очистки данных, результатом которого стал датасет с 166 000 часами речи на 75 языках. Новая серия моделей OWSM v4, обученная на этом курированном датасете, значительно превосходит предыдущие версии по мультиязычным бенчмаркам. Модели даже соответствуют или превосходят передовые промышленные модели, такие как Whisper и MMS, в нескольких сценариях.'}, 'en': {'title': 'Enhancing Multilingual Speech Models with Cleaned Web Data', 'desc': 'The OWSM project has improved its multilingual speech models by integrating a large-scale, cleaned web dataset called YODAS. This dataset, which contains 166,000 hours of speech in 75 languages, was challenging to incorporate due to issues like incorrect language labels and audio-text misalignments. To tackle these challenges, a scalable data-cleaning pipeline was developed, resulting in a high-quality dataset for training. The new OWSM v4 models, trained on this curated dataset, now perform comparably to leading industrial models, showcasing significant advancements in multilingual speech recognition.'}, 'zh': {'title': '提升多语言语音模型的开创性进展', 'desc': 'OWSM项目通过整合一个大型清洗过的网络数据集YODAS，提升了多语言语音模型的性能。YODAS数据集包含了大量的语音数据，但由于其原始特性，存在语言标签错误和音频文本不对齐等问题。为了解决这些问题，我们开发了一个可扩展的数据清洗流程，最终生成了一个包含75种语言、166,000小时语音的数据集。新的OWSM v4模型在多语言基准测试中表现优异，甚至在多个场景中与领先的工业模型相媲美。'}}}, {'id': 'https://huggingface.co/papers/2505.24183', 'title': 'CodeV-R1: Reasoning-Enhanced Verilog Generation', 'url': 'https://huggingface.co/papers/2505.24183', 'abstract': 'CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.', 'score': 4, 'issue_id': 4093, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': 'b542a58b96860ad6', 'authors': ['Yaoyu Zhu', 'Di Huang', 'Hanqi Lyu', 'Xiaoyun Zhang', 'Chongxiao Li', 'Wenxuan Shi', 'Yutong Wu', 'Jianan Mu', 'Jinghua Wang', 'Yang Zhao', 'Pengwei Jin', 'Shuyao Cheng', 'Shengwen Liang', 'Xishan Zhang', 'Rui Zhang', 'Zidong Du', 'Qi Guo', 'Xing Hu', 'Yunji Chen'], 'affiliations': ['Cambricon Technologies', 'SKL of Processors, Institute of Computing Technology, CAS', 'University of Chinese Academy of Sciences', 'University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2505.24183.jpg', 'data': {'categories': ['#games', '#rl', '#dataset', '#optimization', '#open_source', '#training'], 'emoji': '🔧', 'ru': {'title': 'CodeV-R1: Прорыв в автоматизации проектирования электроники', 'desc': 'В статье представлена новая система CodeV-R1 для генерации кода на языке Verilog с использованием LLM и метода RLVR. Основные проблемы, которые решает система, включают отсутствие автоматизированных сред верификации, нехватку качественных пар "естественный язык - код" и высокие вычислительные затраты. CodeV-R1 использует генератор тестов на основе правил и метод синтеза данных для создания высококачественного набора данных. Модель CodeV-R1-7B демонстрирует значительное улучшение производительности по сравнению с предыдущими методами, что способствует развитию исследований в области автоматизации проектирования электроники.'}, 'en': {'title': 'Revolutionizing Verilog Generation with CodeV-R1', 'desc': 'The paper presents CodeV-R1, a reinforcement learning with verifiable reward (RLVR) framework designed for generating Verilog code from natural language specifications. It addresses challenges in electronic design automation (EDA) by introducing a rule-based testbench for equivalence checking and a round-trip data synthesis method to create a high-quality dataset of NL-code pairs. The training process utilizes a two-stage approach, combining knowledge distillation with an adaptive RLVR algorithm to optimize training efficiency. CodeV-R1 demonstrates significant improvements in performance metrics, surpassing previous state-of-the-art models in Verilog generation tasks.'}, 'zh': {'title': 'CodeV-R1：电子设计自动化的强化学习新突破', 'desc': '本文介绍了CodeV-R1，这是一个用于Verilog生成的强化学习可验证奖励（RLVR）框架，旨在解决电子设计自动化（EDA）中的关键挑战。该框架通过开发基于规则的测试平台生成器和回合数据合成方法，确保生成的代码与自然语言描述之间的一致性。我们还采用了两阶段的训练流程，首先进行知识蒸馏以提升推理能力，然后使用自适应的RLVR算法降低训练成本。最终，CodeV-R1-7B模型在VerilogEval v2和RTLLM v1.1上取得了显著的性能提升，超越了之前的最佳结果。'}}}, {'id': 'https://huggingface.co/papers/2505.23504', 'title': 'VAU-R1: Advancing Video Anomaly Understanding via Reinforcement\n  Fine-Tuning', 'url': 'https://huggingface.co/papers/2505.23504', 'abstract': 'VAU-R1 uses Multimodal Large Language Models with Reinforcement Fine-Tuning to enhance video anomaly reasoning, complemented by VAU-Bench, a Chain-of-Thought benchmark for evaluating anomaly understanding.  \t\t\t\t\tAI-generated summary \t\t\t\t Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.', 'score': 4, 'issue_id': 4087, 'pub_date': '2025-05-29', 'pub_date_card': {'ru': '29 мая', 'en': 'May 29', 'zh': '5月29日'}, 'hash': 'c243189c9ec32d1f', 'authors': ['Liyun Zhu', 'Qixiang Chen', 'Xi Shen', 'Xiaodong Cun'], 'affiliations': ['Australian National University', 'GVC Lab, Great Bay University', 'Intellindust AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2505.23504.jpg', 'data': {'categories': ['#rl', '#interpretability', '#multimodal', '#reasoning', '#video', '#benchmark'], 'emoji': '🎥', 'ru': {'title': 'Умное видеонаблюдение: ИИ учится понимать аномалии', 'desc': 'VAU-R1 - это новая система для понимания аномалий в видео, использующая мультимодальные большие языковые модели (MLLM) и усиленное обучение. Авторы также представили VAU-Bench - первый бенчмарк для оценки рассуждений об аномалиях в видео, основанный на методе цепочки мыслей. Система VAU-R1 значительно улучшает точность ответов на вопросы, временную привязку и согласованность рассуждений в различных контекстах. Это исследование закладывает основу для интерпретируемого и основанного на рассуждениях понимания аномалий в видео.'}, 'en': {'title': 'Enhancing Video Anomaly Reasoning with VAU-R1 and VAU-Bench', 'desc': 'The paper introduces VAU-R1, a framework that uses Multimodal Large Language Models (MLLMs) and Reinforcement Fine-Tuning (RFT) to improve the understanding of video anomalies. It addresses the challenges of fine-grained spatio-temporal perception and the need for robust reasoning in ambiguous situations. Additionally, the authors present VAU-Bench, a new benchmark designed to evaluate reasoning capabilities in video anomaly scenarios through multiple-choice questions and detailed rationales. The results demonstrate that VAU-R1 enhances accuracy in question answering and improves the coherence of reasoning across various contexts.'}, 'zh': {'title': '提升视频异常推理的智能框架', 'desc': 'VAU-R1 是一个基于多模态大语言模型的框架，旨在提升视频异常推理能力。通过强化微调（Reinforcement Fine-Tuning），该方法能够更好地理解和解释异常事件。我们还提出了 VAU-Bench，这是一个专门用于视频异常推理的链式思维基准，包含多项选择问答、详细推理、时间标注和描述性标题。实验结果表明，VAU-R1 在问答准确性、时间定位和推理连贯性方面有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.21179', 'title': 'Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model', 'url': 'https://huggingface.co/papers/2505.21179', 'abstract': 'Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.  \t\t\t\t\tAI-generated summary \t\t\t\t Negative guidance -- explicitly suppressing unwanted attributes -- remains a fundamental challenge in diffusion models, particularly in few-step sampling regimes. While Classifier-Free Guidance (CFG) works well in standard settings, it fails under aggressive sampling step compression due to divergent predictions between positive and negative branches. We present Normalized Attention Guidance (NAG), an efficient, training-free mechanism that applies extrapolation in attention space with L1-based normalization and refinement. NAG restores effective negative guidance where CFG collapses while maintaining fidelity. Unlike existing approaches, NAG generalizes across architectures (UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image, video), functioning as a universal plug-in with minimal computational overhead. Through extensive experimentation, we demonstrate consistent improvements in text alignment (CLIP Score), fidelity (FID, PFID), and human-perceived quality (ImageReward). Our ablation studies validate each design component, while user studies confirm significant preference for NAG-guided outputs. As a model-agnostic inference-time approach requiring no retraining, NAG provides effortless negative guidance for all modern diffusion frameworks -- pseudocode in the Appendix!', 'score': 4, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '3e7694e3e9f014f5', 'authors': ['Dar-Yen Chen', 'Hmrishav Bandyopadhyay', 'Kai Zou', 'Yi-Zhe Song'], 'affiliations': ['NetMind.AI', 'SketchX, CVSSP, University of Surrey'], 'pdf_title_img': 'assets/pdf/title_img/2505.21179.jpg', 'data': {'categories': ['#diffusion', '#inference', '#cv', '#optimization', '#video'], 'emoji': '🧠', 'ru': {'title': 'NAG: универсальное негативное руководство для диффузионных моделей', 'desc': 'Статья представляет новый метод под названием Normalized Attention Guidance (NAG) для улучшения работы диффузионных моделей. NAG позволяет эффективно применять негативное руководство в различных режимах и модальностях без необходимости переобучения модели. В отличие от существующих подходов, NAG обобщается на разные архитектуры, режимы сэмплирования и модальности, функционируя как универсальный плагин с минимальными вычислительными затратами. Эксперименты показывают улучшения в соответствии текста и изображения, качестве генерации и восприятии человеком.'}, 'en': {'title': 'Effortless Negative Guidance for Diffusion Models with NAG', 'desc': 'Normalized Attention Guidance (NAG) is a novel method that improves diffusion models by providing effective negative guidance without the need for retraining. It addresses the challenge of suppressing unwanted attributes, especially in scenarios with few sampling steps where traditional methods like Classifier-Free Guidance (CFG) struggle. NAG utilizes an efficient mechanism that normalizes attention using L1-based techniques, allowing it to maintain high fidelity while enhancing negative guidance. This approach is versatile, working across different architectures, sampling regimes, and modalities, making it a universal solution for modern diffusion frameworks.'}, 'zh': {'title': '归一化注意力引导：无缝负引导的解决方案', 'desc': '归一化注意力引导（NAG）通过在不同的采样阶段和模态中提供有效的负引导，增强了扩散模型，而无需重新训练。负引导的挑战在于在少步采样中显得尤为突出，传统的无分类器引导（CFG）在激进的采样步骤压缩下表现不佳。NAG采用基于L1的归一化和精炼方法，在注意力空间中进行外推，恢复了有效的负引导。通过广泛的实验，我们证明了NAG在文本对齐、保真度和人类感知质量方面的一致性提升。'}}}, {'id': 'https://huggingface.co/papers/2506.01084', 'title': 'zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression', 'url': 'https://huggingface.co/papers/2506.01084', 'abstract': 'A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.  \t\t\t\t\tAI-generated summary \t\t\t\t Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized for general-purpose corpora. These tokenizers\' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a framework that enables LLMs to dynamically adjust token vocabulary at inference time, allowing for fewer generated tokens and thus faster inference. zip2zip consists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch (LZW) compression that incrementally compresses tokens into reusable "hypertokens" on the fly; (2) an embedding layer that computes embeddings for newly formed hypertokens at runtime; and (3) a causal language modeling variant that trains the model to operate on hypertokenized, compressed sequences. We show that an existing LLM can be zip2zip-fied in 10 GPU-hours via parameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to use hypertokens at inference time, reducing input and output sequence length by 20-60\\%, with significant improvements in inference latency.', 'score': 3, 'issue_id': 4093, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'f9927f51990f811a', 'authors': ['Saibo Geng', 'Nathan Ranchin', 'Yunzhen yao', 'Maxime Peyrard', 'Chris Wendler', 'Michael Gastpar', 'Robert West'], 'affiliations': ['EPFL', 'Microsoft', 'Northeastern University', 'Université Grenoble Alpes, CNRS, Grenoble INP, LIG'], 'pdf_title_img': 'assets/pdf/title_img/2506.01084.jpg', 'data': {'categories': ['#training', '#inference', '#architecture', '#optimization'], 'emoji': '⚡', 'ru': {'title': 'Ускорение LLM с помощью динамического сжатия токенов', 'desc': 'В статье представлена новая система zip2zip, которая улучшает работу LLM, динамически изменяя словарь токенов во время вывода. Это достигается с помощью сжатия LZW, что позволяет сократить длину последовательности токенов и ускорить процесс вывода. Система включает в себя токенизатор, основанный на LZW, слой для вычисления эмбеддингов новых токенов и модифицированную модель языкового моделирования. Результаты показывают, что zip2zip может сократить длину последовательностей на 20-60% и значительно уменьшить задержки при выводе.'}, 'en': {'title': 'Dynamic Tokenization for Faster Inference in LLMs', 'desc': "The paper presents zip2zip, a novel framework that enhances the efficiency of large language models (LLMs) by dynamically adjusting their token vocabulary during inference. By utilizing Lempel-Ziv-Welch (LZW) compression, zip2zip reduces the length of token sequences, which leads to faster inference speeds. The framework includes a tokenizer that creates reusable 'hypertokens', an embedding layer for these hypertokens, and a causal language model that operates on compressed sequences. The results demonstrate that zip2zip can significantly decrease input and output lengths by 20-60%, improving overall model performance and reducing computational costs."}, 'zh': {'title': '动态调整令牌，提升推理速度', 'desc': 'zip2zip是一个框架，它在推理时动态调整大型语言模型（LLMs）的令牌词汇，使用LZW压缩技术来减少令牌序列的长度，从而提高推理速度。传统的令牌化方法通常依赖于静态的令牌器，这些令牌器的固定词汇无法适应特定领域或语言的输入，导致生成更长的令牌序列和更高的计算成本。zip2zip通过三个关键组件实现其功能：基于LZW压缩的令牌器、实时计算新形成的超令牌的嵌入层，以及训练模型处理压缩序列的因果语言建模变体。实验表明，经过zip2zip处理的LLM在推理时能够有效使用超令牌，输入和输出序列长度减少20-60%，推理延迟显著降低。'}}}, {'id': 'https://huggingface.co/papers/2506.00643', 'title': 'SATA-BENCH: Select All That Apply Benchmark for Multiple Choice\n  Questions', 'url': 'https://huggingface.co/papers/2506.00643', 'abstract': "SATA-BENCH evaluates LLMs on multi-answer questions, revealing selections biases and proposing Choice Funnel to improve accuracy and reduce costs in multi-answer reasoning tasks.  \t\t\t\t\tAI-generated summary \t\t\t\t Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications.", 'score': 3, 'issue_id': 4088, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': 'f95c367c9eaf00a9', 'authors': ['Weijie Xu', 'Shixian Cui', 'Xi Fang', 'Chi Xue', 'Stephanie Eckman', 'Chandan Reddy'], 'affiliations': ['Amazon'], 'pdf_title_img': 'assets/pdf/title_img/2506.00643.jpg', 'data': {'categories': ['#training', '#benchmark', '#open_source', '#interpretability', '#data', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'Преодоление ограничений языковых моделей в задачах с множественным выбором', 'desc': 'Статья представляет SATA-BENCH - первый специализированный бенчмарк для оценки языковых моделей на вопросах с множественным выбором ответов. Исследование выявило значительные ограничения даже у самых сильных моделей, достигающих лишь 41.8% точного соответствия. Авторы обнаружили проблемы смещения выбора и смещения количества ответов у моделей. Для решения этих проблем предложена стратегия Choice Funnel, сочетающая дебиасинг токенов с адаптивным порогом.'}, 'en': {'title': 'Enhancing Multi-Answer Reasoning with SATA-BENCH and Choice Funnel', 'desc': 'The paper introduces SATA-BENCH, a benchmark designed to evaluate large language models (LLMs) on multi-answer questions, specifically Select All That Apply (SATA) tasks. It highlights significant performance gaps in current LLMs, with the best model achieving only 41.8% exact match in identifying all correct answers. The authors identify two main issues: selection bias, where models favor certain answers, and count bias, where they struggle to predict the correct number of answers. To mitigate these challenges, they propose a new decoding strategy called Choice Funnel, which enhances accuracy and reduces costs in multi-answer reasoning tasks.'}, 'zh': {'title': '提升多答案推理的准确性与效率', 'desc': '本文介绍了SATA-BENCH，这是一个专门用于评估大型语言模型（LLMs）在多答案问题上的基准测试。研究发现，现有模型在选择所有正确答案时存在显著的选择偏差和计数偏差，导致准确率低下。为了解决这些问题，提出了Choice Funnel解码策略，通过去偏和自适应阈值引导模型做出更完整和准确的选择。实验结果表明，Choice Funnel在准确匹配率上比竞争基线提高了29%，同时降低了推理成本超过64%。'}}}, {'id': 'https://huggingface.co/papers/2505.24842', 'title': 'Cascading Adversarial Bias from Injection to Distillation in Language\n  Models', 'url': 'https://huggingface.co/papers/2505.24842', 'abstract': 'Model distillation has become essential for creating smaller, deployable language models that retain larger system capabilities. However, widespread deployment raises concerns about resilience to adversarial manipulation. This paper investigates vulnerability of distilled models to adversarial injection of biased content during training. We demonstrate that adversaries can inject subtle biases into teacher models through minimal data poisoning, which propagates to student models and becomes significantly amplified. We propose two propagation modes: Untargeted Propagation, where bias affects multiple tasks, and Targeted Propagation, focusing on specific tasks while maintaining normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning rate), student models generate biased responses 76.9% of the time in targeted scenarios - higher than 69.4% in teacher models. For untargeted propagation, adversarial bias appears 6x-29x more frequently in student models on unseen tasks. We validate findings across six bias types (targeted advertisements, phishing links, narrative manipulations, insecure coding practices), various distillation methods, and different modalities spanning text and code generation. Our evaluation reveals shortcomings in current defenses - perplexity filtering, bias detection systems, and LLM-based autorater frameworks - against these attacks. Results expose significant security vulnerabilities in distilled models, highlighting need for specialized safeguards. We propose practical design principles for building effective adversarial bias mitigation strategies.', 'score': 3, 'issue_id': 4092, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '15a12805380711b7', 'authors': ['Harsh Chaudhari', 'Jamie Hayes', 'Matthew Jagielski', 'Ilia Shumailov', 'Milad Nasr', 'Alina Oprea'], 'affiliations': ['Google DeepMind', 'Northeastern University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24842.jpg', 'data': {'categories': ['#security', '#ethics', '#training', '#inference', '#data', '#dataset'], 'emoji': '🕵️', 'ru': {'title': 'Скрытая угроза: как предвзятость усиливается при дистилляции языковых моделей', 'desc': 'Статья исследует уязвимость дистиллированных языковых моделей к внедрению предвзятого контента во время обучения. Авторы демонстрируют, что даже минимальное отравление данных учителя может привести к значительному усилению предвзятости в модели ученика. Предложены два режима распространения предвзятости: нецеленаправленный, влияющий на множество задач, и целенаправленный, фокусирующийся на конкретных задачах. Результаты показывают недостатки существующих методов защиты и подчеркивают необходимость разработки специализированных мер безопасности для дистиллированных моделей.'}, 'en': {'title': 'Strengthening Distilled Models Against Adversarial Bias Injection', 'desc': 'This paper explores the vulnerabilities of distilled language models to adversarial attacks, specifically through the injection of biased content during their training phase. It shows that adversaries can subtly poison teacher models with minimal data, which then amplifies biases in the student models that are derived from them. The study identifies two modes of bias propagation: Untargeted, affecting multiple tasks, and Targeted, which focuses on specific tasks while keeping normal behavior intact. The findings reveal that current defenses are inadequate, emphasizing the need for improved strategies to safeguard against these security threats in distilled models.'}, 'zh': {'title': '保护蒸馏模型，抵御对抗性偏见攻击！', 'desc': '模型蒸馏在创建小型可部署语言模型中变得至关重要，这些模型保留了更大系统的能力。然而，广泛部署引发了对抗性操控的脆弱性问题。本文研究了蒸馏模型在训练过程中对偏见内容的对抗性注入的脆弱性。我们提出了两种传播模式，并展示了如何通过最小的数据污染使教师模型注入微妙的偏见，这些偏见在学生模型中被显著放大。'}}}, {'id': 'https://huggingface.co/papers/2505.24086', 'title': 'ComposeAnything: Composite Object Priors for Text-to-Image Generation', 'url': 'https://huggingface.co/papers/2505.24086', 'abstract': 'ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.  \t\t\t\t\tAI-generated summary \t\t\t\t Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text.', 'score': 3, 'issue_id': 4095, 'pub_date': '2025-05-30', 'pub_date_card': {'ru': '30 мая', 'en': 'May 30', 'zh': '5月30日'}, 'hash': '2bd92a7129e6945b', 'authors': ['Zeeshan Khan', 'Shizhe Chen', 'Cordelia Schmid'], 'affiliations': ['Inria, École normale supérieure, CNRS, PSL Research University'], 'pdf_title_img': 'assets/pdf/title_img/2505.24086.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#diffusion', '#interpretability', '#cv', '#benchmark'], 'emoji': '🖼️', 'ru': {'title': 'Улучшение композиции в генерации изображений с помощью 2.5D семантических макетов', 'desc': 'ComposeAnything - это новый подход к улучшению генерации изображений по текстовому описанию. Он использует языковые модели для создания 2.5D семантических макетов, включающих 2D ограничивающие рамки объектов с информацией о глубине и подробными подписями. Этот макет служит сильным и интерпретируемым приором, заменяющим стохастическую инициализацию шумом в диффузионных моделях генерации изображений. ComposeAnything превосходит современные методы на бенчмарках T2I-CompBench и NSR-1K для запросов с 2D/3D пространственными расположениями, большим количеством объектов и сюрреалистическими композициями.'}, 'en': {'title': 'ComposeAnything: Elevating Text-to-Image Generation with 2.5D Layouts', 'desc': 'ComposeAnything is a framework that enhances text-to-image generation by utilizing large language models (LLMs) to create 2.5D semantic layouts. This method improves the arrangement of objects in images by incorporating depth information, which helps maintain coherence and quality in the generated images. Unlike previous models that rely solely on 2D layouts, ComposeAnything provides a more accurate representation of spatial relationships, allowing for better object placement. The framework has shown superior performance on benchmark tests, producing high-quality images that align closely with the provided text descriptions.'}, 'zh': {'title': 'ComposeAnything：提升文本到图像生成的创新框架', 'desc': 'ComposeAnything 是一种新框架，旨在改善文本到图像生成的质量。它利用大型语言模型（LLMs）的推理能力，生成包含深度信息的2.5D语义布局，从而增强对象的放置和一致性。该方法不需要重新训练现有的文本到图像模型，而是通过生成空间和深度感知的粗略合成图像，来指导去噪过程。ComposeAnything 在处理复杂的2D/3D空间布局和超现实组合时，表现优于现有的最先进方法。'}}}, {'id': 'https://huggingface.co/papers/2506.01952', 'title': 'WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks', 'url': 'https://huggingface.co/papers/2506.01952', 'abstract': 'WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.  \t\t\t\t\tAI-generated summary \t\t\t\t Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.', 'score': 2, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'aa260fbf373a4f2c', 'authors': ['Atsuyuki Miyai', 'Zaiying Zhao', 'Kazuki Egashira', 'Atsuki Sato', 'Tatsumi Sunada', 'Shota Onohara', 'Hiromasa Yamanishi', 'Mashiro Toyooka', 'Kunato Nishina', 'Ryoma Maeda', 'Kiyoharu Aizawa', 'Toshihiko Yamasaki'], 'affiliations': ['The University of Tokyo'], 'pdf_title_img': 'assets/pdf/title_img/2506.01952.jpg', 'data': {'categories': ['#reasoning', '#agents', '#agi', '#benchmark', '#long_context'], 'emoji': '🤖', 'ru': {'title': 'WebChoreArena: новый рубеж в оценке возможностей ИИ-агентов для веб-задач', 'desc': 'WebChoreArena - это новый набор тестов, состоящий из 532 задач, который расширяет возможности WebArena для более сложных и утомительных задач веб-браузинга. Он включает в себя три ключевых вызова: задачи с массивной памятью, задачи с вычислениями и задачи с долговременной памятью. Эксперименты показывают, что по мере эволюции больших языковых моделей (LLM), таких как GPT-4, Claude 3.7 Sonnet и Gemini 2.5 Pro, наблюдается значительное улучшение производительности на WebChoreArena. Однако результаты также указывают на то, что даже с Gemini 2.5 Pro остается значительное пространство для улучшения по сравнению с WebArena.'}, 'en': {'title': 'WebChoreArena: Elevating LLMs to Tackle Tedious Web Tasks', 'desc': 'WebChoreArena is a new benchmark that includes 532 tasks designed to evaluate the capabilities of large language models (LLMs) in handling complex web browsing chores. It extends the previous WebArena benchmark by focusing on more tedious tasks that require advanced skills such as massive memory retrieval, precise calculations, and long-term memory management across multiple web pages. The benchmark allows for reproducible experiments and fair comparisons with existing models, showcasing the progress of LLMs like GPT-4o and Gemini 2.5 Pro. Despite improvements in performance, the results indicate that there is still significant room for enhancement in tackling the challenges presented by WebChoreArena compared to general browsing tasks.'}, 'zh': {'title': 'WebChoreArena：评估LLM在复杂任务中的能力', 'desc': 'WebChoreArena是一个新的基准测试，包含532个任务，旨在评估大型语言模型（LLM）在复杂和繁琐的网页浏览任务中的能力。该基准测试扩展了WebArena的范围，专注于人类通常避免的繁重任务。WebChoreArena整合了三大挑战：大规模记忆任务、计算任务和长期记忆任务，确保了严格的可重复性。实验结果表明，随着LLM的进步，性能显著提升，但仍有改进空间，显示出WebChoreArena的挑战性。'}}}, {'id': 'https://huggingface.co/papers/2506.01484', 'title': 'LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification', 'url': 'https://huggingface.co/papers/2506.01484', 'abstract': 'A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.  \t\t\t\t\tAI-generated summary \t\t\t\t Detoxification, the task of rewriting harmful language into non-toxic text, has become increasingly important amid the growing prevalence of toxic content online. However, high-quality parallel datasets for detoxification, especially for hate speech, remain scarce due to the cost and sensitivity of human annotation. In this paper, we propose a novel LLM-in-the-loop pipeline leveraging GPT-4o-mini for automated detoxification. We first replicate the ParaDetox pipeline by replacing human annotators with an LLM and show that the LLM performs comparably to human annotation. Building on this, we construct PARADEHATE, a large-scale parallel dataset specifically for hatespeech detoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate text pairs and evaluate a wide range of baseline methods. Experimental results show that models such as BART, fine-tuned on PARADEHATE, achieve better performance in style accuracy, content preservation, and fluency, demonstrating the effectiveness of LLM-generated detoxification text as a scalable alternative to human annotation.', 'score': 2, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '422c267bbe9577db', 'authors': ['Shuzhou Yuan', 'Ercong Nie', 'Lukas Kouba', 'Ashish Yashwanth Kangen', 'Helmut Schmid', 'Hinrich Schutze', 'Michael Farber'], 'affiliations': ['LMU Munich', 'Munich Center for Machine Learning (MCML)', 'ScaDS.AI and TU Dresden'], 'pdf_title_img': 'assets/pdf/title_img/2506.01484.jpg', 'data': {'categories': ['#ethics', '#benchmark', '#dataset', '#data', '#open_source'], 'emoji': '🧼', 'ru': {'title': 'ИИ очищает интернет от языка ненависти', 'desc': 'Статья представляет новый подход к детоксификации языка ненависти с использованием GPT-4o-mini. Авторы создали крупномасштабный датасет PARADEHATE, содержащий более 8000 пар токсичных и нетоксичных текстов. Эксперименты показали, что модели, обученные на этом датасете, демонстрируют улучшенные результаты по точности стиля, сохранению содержания и плавности текста. Этот метод предлагается как масштабируемая альтернатива ручной аннотации для создания данных по детоксификации.'}, 'en': {'title': 'Automating Hate Speech Detoxification with GPT-4o-mini', 'desc': "This paper introduces a new method for creating a large dataset aimed at detoxifying hate speech using the GPT-4o-mini model. Detoxification involves rewriting harmful language into non-toxic text, which is crucial due to the rise of toxic content online. The authors developed a pipeline that automates this process, replacing human annotators with a language model, and found that the model's performance is comparable to that of humans. They also created a dataset called PARADEHATE, consisting of over 8,000 pairs of hate and non-hate text, which significantly improves the performance of various models in terms of style accuracy, content preservation, and fluency."}, 'zh': {'title': '利用GPT-4o-mini生成仇恨言论去毒化数据集', 'desc': '本文提出了一种新颖的管道，利用GPT-4o-mini生成大规模的仇恨言论去毒化数据集，从而提高基线模型在风格准确性、内容保留和流畅性方面的表现。去毒化是将有害语言重写为非有害文本的任务，随着网络上有毒内容的增加，这一任务变得越来越重要。由于人工标注的成本和敏感性，高质量的去毒化平行数据集仍然稀缺。我们构建了PARADEHATE，这是一个专门用于仇恨言论去毒化的大规模平行数据集，并通过实验验证了基于该数据集的模型的有效性。'}}}, {'id': 'https://huggingface.co/papers/2506.00512', 'title': 'Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing', 'url': 'https://huggingface.co/papers/2506.00512', 'abstract': 'A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.  \t\t\t\t\tAI-generated summary \t\t\t\t Text-guided 3D editing aims to precisely edit semantically relevant local 3D regions, which has significant potential for various practical applications ranging from 3D games to film production. Existing methods typically follow a view-indiscriminate paradigm: editing 2D views indiscriminately and projecting them back into 3D space. However, they overlook the different cross-view interdependencies, resulting in inconsistent multi-view editing. In this study, we argue that ideal consistent 3D editing can be achieved through a progressive-views paradigm, which propagates editing semantics from the editing-salient view to other editing-sparse views. Specifically, we propose Pro3D-Editor, a novel framework, which mainly includes Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view Sampler dynamically samples and edits the most editing-salient view as the primary view. Key-view Render accurately propagates editing semantics from the primary view to other key views through its Mixture-of-View-Experts Low-Rank Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based on the edited multi-views. Extensive experiments demonstrate that our method outperforms existing methods in editing accuracy and spatial consistency.', 'score': 2, 'issue_id': 4094, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '0a9ce5d9ebc76a52', 'authors': ['Yang Zheng', 'Mengqi Huang', 'Nan Chen', 'Zhendong Mao'], 'affiliations': ['University of Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2506.00512.jpg', 'data': {'categories': ['#games', '#3d'], 'emoji': '🎨', 'ru': {'title': 'Прогрессивное 3D-редактирование: от текста к согласованным изменениям', 'desc': 'Статья представляет новый подход к редактированию 3D-объектов с помощью текстовых инструкций. Авторы предлагают парадигму прогрессивных видов, которая обеспечивает согласованное редактирование 3D-объектов путем распространения семантики от ключевых видов к менее отредактированным. Метод Pro3D-Editor включает в себя выборку основного вида, рендеринг ключевых видов и уточнение полного вида. Эксперименты показывают, что этот метод превосходит существующие подходы по точности редактирования и пространственной согласованности.'}, 'en': {'title': 'Achieving Consistent 3D Editing with Pro3D-Editor', 'desc': 'This paper introduces a new approach for 3D editing called Pro3D-Editor, which focuses on maintaining consistency across different views of a 3D object. Unlike traditional methods that treat all views equally, this framework uses a progressive-views paradigm to propagate editing information from the most important view to others. It consists of three main components: a Primary-view Sampler that identifies and edits the most relevant view, a Key-view Render that transfers the editing semantics to other views, and a Full-view Refiner that finalizes the 3D object based on the edited views. The results show that Pro3D-Editor achieves better accuracy and consistency compared to existing 3D editing techniques.'}, 'zh': {'title': '渐进视图范式实现一致的3D编辑', 'desc': '本文提出了一种渐进视图范式，通过Pro3D-Editor实现一致的3D编辑。该方法通过从关键视图向较少编辑的视图传播语义，解决了现有方法在多视图编辑中存在的不一致性问题。Pro3D-Editor框架包括主要视图采样器、关键视图渲染和全视图精炼器，能够动态选择最重要的视图进行编辑。实验结果表明，该方法在编辑精度和空间一致性方面优于现有技术。'}}}, {'id': 'https://huggingface.co/papers/2506.00385', 'title': 'MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation', 'url': 'https://huggingface.co/papers/2506.00385', 'abstract': 'MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.  \t\t\t\t\tAI-generated summary \t\t\t\t Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce MagiCodec, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.', 'score': 2, 'issue_id': 4091, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '421c283aaadfdd94', 'authors': ['Yakun Song', 'Jiawei Chen', 'Xiaobin Zhuang', 'Chenpeng Du', 'Ziyang Ma', 'Jian Wu', 'Jian Cong', 'Dongya Jia', 'Zhuo Chen', 'Yuping Wang', 'Yuxuan Wang', 'Xie Chen'], 'affiliations': ['Bytedance Inc.', 'Shanghai Jiao Tong University'], 'pdf_title_img': 'assets/pdf/title_img/2506.00385.jpg', 'data': {'categories': ['#audio', '#optimization', '#diffusion', '#open_source', '#multimodal'], 'emoji': '🎵', 'ru': {'title': 'MagiCodec: Семантическая токенизация аудио для улучшенной генерации', 'desc': 'MagiCodec - это новый аудио кодек на основе трансформера, разработанный для улучшения семантической токенизации при сохранении высокого качества реконструкции. Он использует многоступенчатый процесс обучения, включающий добавление гауссова шума и регуляризацию скрытого пространства. Эксперименты показывают, что MagiCodec превосходит современные кодеки как по качеству реконструкции, так и по эффективности в последующих задачах. Токены, создаваемые MagiCodec, имеют распределение, похожее на закон Ципфа, что улучшает совместимость с генеративными моделями на основе языковых моделей.'}, 'en': {'title': 'MagiCodec: Transforming Audio for Better AI Compatibility', 'desc': 'MagiCodec is a new audio codec that uses a Transformer model to improve how audio is represented as tokens. It focuses on enhancing the semantic meaning of these tokens while still ensuring that the audio can be reconstructed accurately. The codec employs a special training method that includes adding noise and regularization to make the tokens more expressive and useful for generative models. Tests show that MagiCodec outperforms existing codecs in both audio quality and its ability to work with other AI models.'}, 'zh': {'title': 'MagiCodec：提升音频语义表达的编解码器', 'desc': 'MagiCodec是一种基于Transformer的音频编解码器，旨在提高语义标记的表达能力，同时保持高质量的重建效果。与传统编解码器不同，MagiCodec在训练过程中引入了高斯噪声和潜在正则化，以增强生成代码的语义表现力。实验结果表明，MagiCodec在重建质量和下游任务上均优于现有的最先进编解码器。其生成的标记呈现出类似Zipf分布的特征，增强了与基于语言模型的生成架构的兼容性。'}}}, {'id': 'https://huggingface.co/papers/2505.21724', 'title': 'OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions', 'url': 'https://huggingface.co/papers/2505.21724', 'abstract': "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.  \t\t\t\t\tAI-generated summary \t\t\t\t In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality.", 'score': 2, 'issue_id': 4095, 'pub_date': '2025-05-27', 'pub_date_card': {'ru': '27 мая', 'en': 'May 27', 'zh': '5月27日'}, 'hash': '309d90ff41ad30b0', 'authors': ['Cheng Luo', 'Jianghui Wang', 'Bing Li', 'Siyang Song', 'Bernard Ghanem'], 'affiliations': ['King Abdullah University of Science and Technology', 'University of Exeter'], 'pdf_title_img': 'assets/pdf/title_img/2505.21724.jpg', 'data': {'categories': ['#multimodal', '#interpretability', '#cv', '#dataset', '#optimization', '#audio', '#games'], 'emoji': '🤖', 'ru': {'title': 'Мультимодальный ИИ для естественного диалога', 'desc': 'Статья представляет OmniResponse - мультимодальную большую языковую модель для генерации синхронизированных вербальных и невербальных ответов слушателя. Модель использует текст как промежуточную модальность для связи аудио и лицевых реакций. OmniResponse включает в себя компоненты Chrono-Text для временной привязки текстовых токенов и TempoVoice для синхронизированной генерации речи. Для обучения и оценки создан датасет ResponseNet с 696 диалогами, содержащими видео, аудио и аннотации.'}, 'en': {'title': 'Synchronized Responses for Natural Conversations', 'desc': 'This paper presents OmniResponse, a Multimodal Large Language Model designed to generate synchronized verbal and non-verbal responses in conversations. It introduces a new task called Online Multimodal Conversational Response Generation (OMCRG), which focuses on creating real-time feedback based on multimodal inputs from speakers. The model uses text as an intermediate step to ensure that audio and facial responses are well-coordinated. Additionally, it introduces two innovative components, Chrono-Text and TempoVoice, to enhance the quality and synchronization of the generated responses.'}, 'zh': {'title': 'OmniResponse：同步生成多模态响应的创新模型', 'desc': '本文介绍了一种新的任务，称为在线多模态对话响应生成（OMCRG），旨在根据说话者的多模态输入在线生成同步的语言和非语言反馈。为了解决生成的音频和面部反应之间的同步问题，研究者们创新性地引入了文本作为中介模态。我们提出了OmniResponse，这是一种多模态大型语言模型（MLLM），能够自回归地生成高质量的多模态听众响应。通过使用Chrono-Text和TempoVoice等新组件，OmniResponse在语义内容、音视频同步和生成质量方面显著优于基线模型。'}}}, {'id': 'https://huggingface.co/papers/2505.19621', 'title': 'Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models', 'url': 'https://huggingface.co/papers/2505.19621', 'abstract': "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.  \t\t\t\t\tAI-generated summary \t\t\t\t As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS", 'score': 2, 'issue_id': 4093, 'pub_date': '2025-05-26', 'pub_date_card': {'ru': '26 мая', 'en': 'May 26', 'zh': '5月26日'}, 'hash': '4358fd586e320601', 'authors': ['George Kour', 'Itay Nakash', 'Ateret Anaby-Tavor', 'Michal Shmueli-Scheuer'], 'affiliations': ['IBM'], 'pdf_title_img': 'assets/pdf/title_img/2505.19621.jpg', 'data': {'categories': ['#benchmark', '#data', '#hallucinations', '#multimodal', '#ethics', '#alignment'], 'emoji': '🧠', 'ru': {'title': 'Выявление скрытых предубеждений в языковых моделях', 'desc': 'Статья представляет новый бенчмарк под названием Preference, Opinion, and Belief survey (POBs) для оценки субъективных тенденций и предубеждений больших языковых моделей (LLM) в различных областях. Исследователи применили этот бенчмарк к ведущим открытым и закрытым LLM, измеряя такие желаемые свойства, как надежность, нейтральность и согласованность. Результаты показывают, что механизмы рассуждения и самоанализа предлагают лишь ограниченные улучшения в этой области. Обнаружено, что более новые версии моделей становятся менее согласованными и более предвзятыми к определенным точкам зрения.'}, 'en': {'title': 'Assessing Bias in Language Models: A Call for Neutrality', 'desc': 'This paper introduces the Preference, Opinion, and Belief survey (POBs), which evaluates the subjective biases of Large Language Models (LLMs) in various domains. It highlights that as LLMs are increasingly used in decision-making, their inherent biases can shape the advice they provide, potentially reinforcing certain viewpoints. The study assesses leading LLMs for properties like reliability and neutrality, revealing that newer models tend to exhibit greater bias and inconsistency. Additionally, it examines the impact of advanced reasoning techniques on these biases, finding only marginal improvements in performance.'}, 'zh': {'title': '评估大型语言模型的主观偏见', 'desc': '这篇论文介绍了一个名为偏好、观点和信念调查（POBs）的基准，用于评估大型语言模型（LLMs）在社会、文化、伦理和个人领域的主观倾向。研究发现，随着模型版本的更新，它们的偏见和不一致性有所增加，这可能影响它们对用户的建议和推荐。通过对领先的开源和闭源LLMs进行评估，论文测量了模型的可靠性、中立性和一致性等属性。结果表明，尽管推理和自我反思机制在其他任务中有效，但在本研究领域的提升有限，显示出模型在某些观点上的偏见加剧。'}}}, {'id': 'https://huggingface.co/papers/2506.00772', 'title': 'LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning', 'url': 'https://huggingface.co/papers/2506.00772', 'abstract': 'Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.  \t\t\t\t\tAI-generated summary \t\t\t\t Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.', 'score': 1, 'issue_id': 4095, 'pub_date': '2025-06-01', 'pub_date_card': {'ru': '1 июня', 'en': 'June 1', 'zh': '6月1日'}, 'hash': 'd66b2e2afe71cfd9', 'authors': ['Zihang Liu', 'Tianyu Pang', 'Oleg Balabanov', 'Chaoqun Yang', 'Tianjin Huang', 'Lu Yin', 'Yaoqing Yang', 'Shiwei Liu'], 'affiliations': ['Dartmouth College, NH, USA', 'Eindhoven University of Technology, the Netherlands', 'International Computer Science Institute, CA, USA', 'Lawrence Berkeley National Laboratory, CA, USA', 'Tsinghua University, China', 'University of California, Berkeley, CA, USA', 'University of Exeter, Exeter, UK', 'University of Oxford, Oxford, UK', 'University of Surrey, Guildford, UK'], 'pdf_title_img': 'assets/pdf/title_img/2506.00772.jpg', 'data': {'categories': ['#reasoning', '#optimization', '#low_resource', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективная точная настройка больших языковых моделей с помощью разреженного обновления весов', 'desc': 'Статья представляет новый метод точной настройки больших языковых моделей под названием LIFT (Low-rank Informed Sparse Fine-Tuning). LIFT использует низкоранговую аппроксимацию для выявления критически важных весов модели и обновляет только 5% от их общего числа. Этот подход позволяет достичь лучших результатов на задачах рассуждения по сравнению с полной точной настройкой, сохраняя при этом эффективность использования памяти. LIFT также лучше сохраняет знания из исходной предметной области по сравнению с другими методами точной настройки.'}, 'en': {'title': 'Efficient Fine-Tuning with Critical Weights', 'desc': 'This paper introduces a method called Low-rank Informed Sparse Fine-Tuning (LIFT) that improves the efficiency and performance of large language models (LLMs) by focusing on critical weights identified through low-rank approximation. Instead of updating all parameters during fine-tuning, LIFT selectively updates only the top 5% of Principal Weights, which are determined to be the most important for reasoning tasks. This approach not only enhances reasoning capabilities but also reduces the risk of overfitting and catastrophic forgetting, common issues in full fine-tuning. The results show that LIFT outperforms traditional full fine-tuning while preserving more knowledge from the original model, making it a promising strategy for efficient model adaptation.'}, 'zh': {'title': '低秩微调：提升大型语言模型的效率与性能', 'desc': '本论文提出了一种新的稀疏微调方法，称为低秩知情稀疏微调（LIFT），旨在提高大型语言模型的性能和效率。通过低秩近似，我们识别出对推理至关重要的权重，称为主权重，并仅更新这些权重的前5%。与完全微调相比，LIFT在推理任务上表现更好，同时在内存使用上保持高效。该方法在保持源领域知识的同时，能够有效避免过拟合和灾难性遗忘。'}}}, {'id': 'https://huggingface.co/papers/2506.00469', 'title': 'Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data', 'url': 'https://huggingface.co/papers/2506.00469', 'abstract': 'Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper investigates a critical design decision in the practice of massively multilingual continual pre-training -- the inclusion of parallel data. Specifically, we study the impact of bilingual translation data for massively multilingual language adaptation of the Llama3 family of models to 500 languages. To this end, we construct the MaLA bilingual translation corpus, containing data from more than 2,500 language pairs. Subsequently, we develop the EMMA-500 Llama 3 suite of four massively multilingual models -- continually pre-trained from the Llama 3 family of base models extensively on diverse data mixes up to 671B tokens -- and explore the effect of continual pre-training with or without bilingual translation data. Comprehensive evaluation across 7 tasks and 12 benchmarks demonstrates that bilingual data tends to enhance language transfer and performance, particularly for low-resource languages. We open-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model generations.', 'score': 1, 'issue_id': 4093, 'pub_date': '2025-05-31', 'pub_date_card': {'ru': '31 мая', 'en': 'May 31', 'zh': '5月31日'}, 'hash': '2ebeb941a6f4f7cc', 'authors': ['Shaoxiong Ji', 'Zihao Li', 'Jaakko Paavola', 'Indraneil Paul', 'Hengyu Luo', 'Jörg Tiedemann'], 'affiliations': ['Technical University of Darmstadt', 'University of Helsinki'], 'pdf_title_img': 'assets/pdf/title_img/2506.00469.jpg', 'data': {'categories': ['#benchmark', '#dataset', '#multilingual', '#transfer_learning', '#open_source', '#machine_translation', '#training', '#low_resource'], 'emoji': '🌐', 'ru': {'title': 'Двуязычные данные улучшают многоязычную адаптацию больших языковых моделей', 'desc': 'Это исследование посвящено влиянию двуязычных данных перевода на многоязычную адаптацию моделей семейства Llama3 к 500 языкам. Авторы создали корпус MaLA, содержащий данные более чем 2500 языковых пар, и разработали набор EMMA-500 Llama 3 из четырех многоязычных моделей. Эксперименты показали, что использование двуязычных данных улучшает языковой перенос и производительность, особенно для малоресурсных языков. Результаты оценивались на 7 задачах и 12 бенчмарках, а все ресурсы были открыты для общего доступа.'}, 'en': {'title': 'Boosting Multilingual Models with Bilingual Data', 'desc': 'This paper explores how using bilingual translation data can improve the performance of the Llama3 family of models in multilingual settings. It introduces the MaLA bilingual translation corpus, which includes data from over 2,500 language pairs, to facilitate better language adaptation. The authors develop the EMMA-500 suite of models, which are continually pre-trained on a vast amount of diverse data. Their findings show that incorporating bilingual data significantly enhances language transfer, especially for languages with fewer resources.'}, 'zh': {'title': '双语数据助力多语言模型提升性能', 'desc': '本文研究了在大规模多语言持续预训练中，双语翻译数据的关键设计决策。我们构建了MaLA双语翻译语料库，包含2500多个语言对的数据，以支持Llama3模型在500种语言上的适应。通过开发EMMA-500 Llama 3模型套件，我们评估了使用或不使用双语翻译数据的持续预训练效果。结果表明，双语数据能够增强语言迁移和性能，尤其是在资源稀缺的语言上。'}}}, {'id': 'https://huggingface.co/papers/2506.01920', 'title': 'From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.01920', 'abstract': 'A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.', 'score': 0, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '42a070cbc2d3afee', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Adel Ammar', 'Yasser Al-Habashi', 'Abdulrahman Al-Batati', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.01920.jpg', 'data': {'categories': ['#translation', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🇦🇪', 'ru': {'title': 'Культурно-ориентированная оценка арабских языковых моделей', 'desc': 'Представлен новый фреймворк оценки и набор данных ADMD для тестирования арабских языковых моделей. Проанализированы существующие наборы данных для оценки арабского языка, выявлены проблемы с лингвистической точностью и культурным соответствием. ADMD содержит 490 сложных вопросов по 10 основным областям для оценки языковых моделей. Результаты показали значительные различия в производительности моделей, особенно в областях, требующих глубокого понимания культуры.'}, 'en': {'title': 'Enhancing Arabic Language Models with Cultural Competence', 'desc': 'This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across various domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation.'}, 'zh': {'title': '提升阿拉伯语模型评估的文化能力', 'desc': '本文提出了一种新的评估框架和数据集ADMD，用于评估阿拉伯语模型，强调了性能差异和文化能力的重要性。我们分析了现有的阿拉伯语评估数据集，发现了语言准确性、文化对齐和方法论严谨性方面的重大问题。ADMD包含490个具有挑战性的问题，涵盖十个主要领域，旨在解决大型语言模型（LLMs）中的这些局限性。通过使用ADMD评估五个领先的语言模型，我们发现模型在不同领域的表现差异显著，尤其是在需要深厚文化理解和专业知识的领域。'}}}, {'id': 'https://huggingface.co/papers/2506.01920', 'title': 'From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation', 'url': 'https://huggingface.co/papers/2506.01920', 'abstract': 'A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.  \t\t\t\t\tAI-generated summary \t\t\t\t This paper addresses critical gaps in Arabic language model evaluation by establishing comprehensive theoretical guidelines and introducing a novel evaluation framework. We first analyze existing Arabic evaluation datasets, identifying significant issues in linguistic accuracy, cultural alignment, and methodological rigor. To address these limitations in LLMs, we present the Arabic Depth Mini Dataset (ADMD), a carefully curated collection of 490 challenging questions spanning ten major domains (42 sub-domains, see Figure 1. Using ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet, Gemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant variations in model performance across different domains, with particular challenges in areas requiring deep cultural understanding and specialized knowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%, showing relative strength in mathematical theory in Arabic, Arabic language, and islamic domains. This work provides both theoretical foundations and practical insights for improving Arabic language model evaluation, emphasizing the importance of cultural competence alongside technical capabilities.', 'score': 0, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': '42a070cbc2d3afee', 'authors': ['Serry Sibaee', 'Omer Nacar', 'Adel Ammar', 'Yasser Al-Habashi', 'Abdulrahman Al-Batati', 'Wadii Boulila'], 'affiliations': ['Prince Sultan University, Riyadh, Saudi Arabia'], 'pdf_title_img': 'assets/pdf/title_img/2506.01920.jpg', 'data': {'categories': ['#translation', '#benchmark', '#dataset', '#low_resource', '#multilingual'], 'emoji': '🕌', 'ru': {'title': 'Культурный контекст - ключ к оценке арабских языковых моделей', 'desc': 'Представлен новый фреймворк оценки и набор данных ADMD для оценки арабских языковых моделей. Исследование выявило значительные различия в производительности моделей в разных доменах, особенно в областях, требующих глубокого понимания культуры. Claude 3.5 Sonnet показала наивысшую общую точность в 30%, продемонстрировав относительную силу в математической теории на арабском, арабском языке и исламских доменах. Работа подчеркивает важность культурной компетентности наряду с техническими возможностями для улучшения оценки арабских языковых моделей.'}, 'en': {'title': 'Enhancing Arabic Language Models with Cultural Competence', 'desc': 'This paper introduces a new evaluation framework and dataset called ADMD to improve the assessment of Arabic language models. It identifies key issues in existing Arabic evaluation datasets, such as linguistic accuracy and cultural alignment. The ADMD consists of 490 challenging questions across ten major domains, which are used to evaluate five leading language models. The findings highlight significant performance variations among the models, particularly in areas requiring deep cultural understanding, underscoring the need for cultural competence in language model evaluation.'}, 'zh': {'title': '提升阿拉伯语模型评估的文化能力', 'desc': '本文提出了一种新的评估框架和数据集ADMD，用于评估阿拉伯语模型，强调了性能差异和文化能力的重要性。我们分析了现有的阿拉伯语评估数据集，发现了语言准确性、文化对齐和方法论严谨性方面的重大问题。ADMD包含490个具有挑战性的问题，涵盖十个主要领域，旨在解决大型语言模型（LLMs）中的这些局限性。通过使用ADMD评估五个领先的语言模型，我们发现模型在不同领域的表现差异显著，尤其是在需要深厚文化理解和专业知识的领域。'}}}, {'id': 'https://huggingface.co/papers/2506.01713', 'title': 'SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2506.01713', 'abstract': 'Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality.', 'score': 0, 'issue_id': 4095, 'pub_date': '2025-06-02', 'pub_date_card': {'ru': '2 июня', 'en': 'June 2', 'zh': '6月2日'}, 'hash': 'fc955e3f149c1b08', 'authors': ['Zhongwei Wan', 'Zhihao Dou', 'Che Liu', 'Yu Zhang', 'Dongfei Cui', 'Qinjian Zhao', 'Hui Shen', 'Jing Xiong', 'Yi Xin', 'Yifan Jiang', 'Yangfan He', 'Mi Zhang', 'Shen Yan'], 'affiliations': ['ByteDance Seed', 'Case Western Reserve University', 'Duke University', 'Imperial College London', 'Kean University Minnesota', 'Nanjing University', 'The Ohio State University', 'The University of Hong Kong', 'Tongji University', 'University of Michigan', 'University of Southern California'], 'pdf_title_img': 'assets/pdf/title_img/2506.01713.jpg', 'data': {'categories': ['#multimodal', '#reasoning', '#rl', '#benchmark', '#dataset', '#optimization'], 'emoji': '🤖', 'ru': {'title': 'SRPO: Усиление мультимодальных ИИ через самоанализ', 'desc': 'Статья представляет новый метод под названием SRPO для улучшения рассуждений мультимодальных больших языковых моделей (MLLM). SRPO использует двухэтапный подход с обучением с подкреплением, включающий создание набора данных с рефлексией и новый механизм вознаграждения. Метод нацелен на преодоление ограничений существующих MLLM в сложных задачах, требующих самоанализа и самокоррекции. Эксперименты показывают значительное улучшение точности рассуждений и качества рефлексии на нескольких мультимодальных тестах.'}, 'en': {'title': 'Enhancing Multimodal Reasoning through Self-Reflection', 'desc': "This paper introduces a new approach called Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO) to improve the reasoning abilities of multimodal large language models (MLLMs). The authors identify that existing reflection methods are inadequate for generating useful feedback, which limits the models' performance on complex reasoning tasks. SRPO consists of two stages: first, it creates a high-quality dataset for training the model to reflect on its responses, and second, it implements a reward mechanism that promotes meaningful reflections. Experimental results show that SRPO significantly enhances both the accuracy of reasoning and the quality of reflections compared to current leading models."}, 'zh': {'title': '提升多模态推理能力的自我反思框架', 'desc': '多模态大型语言模型（MLLMs）在推理任务中表现出色，但在需要明确自我反思和自我纠正的复杂问题上仍然存在困难。现有的反思方法过于简单，难以生成有意义和指导性的反馈，因为预训练模型的推理能力和知识在初始训练期间基本固定。为了解决这些挑战，我们提出了一种名为自我反思增强推理的多模态自我反思框架（SRPO），该框架通过两阶段的反思意识强化学习（RL）来提升多模态LLM的推理能力。实验结果表明，SRPO在多个多模态推理基准测试中显著优于现有模型，推理准确性和反思质量都有显著提升。'}}}, {'id': 'https://huggingface.co/papers/2505.15772', 'title': 'MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling', 'url': 'https://huggingface.co/papers/2505.15772', 'abstract': 'Acquiring large-scale emotional speech data with strong consistency remains a challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated multimodal pipeline for extracting high-consistency emotional speech from unlabeled video data. Leveraging face detection and tracking algorithms, we developed an automatic emotion analysis system using a multimodal large language model (MLLM). Our results demonstrate that MIKU-PAL can achieve human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss kappa score) while being much cheaper and faster than human annotation. With the high-quality, flexible, and consistent annotation from MIKU-PAL, we can annotate fine-grained speech emotion categories of up to 26 types, validated by human annotators with 83% rationality ratings. Based on our proposed system, we further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2 hours) as a new benchmark for emotional text-to-speech and visual voice cloning.', 'score': 0, 'issue_id': 4095, 'pub_date': '2025-05-21', 'pub_date_card': {'ru': '21 мая', 'en': 'May 21', 'zh': '5月21日'}, 'hash': '6b39ad10d21b05d8', 'authors': ['Yifan Cheng', 'Ruoyi Zhang', 'Jiatong Shi'], 'affiliations': ['Carnegie Mellon University, Pittsburgh, PA, USA', 'Fish Audio, Santa Clara, CA, USA', 'Huazhong University of Science and Technology, Wuhan, Hubei, China', 'Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China'], 'pdf_title_img': 'assets/pdf/title_img/2505.15772.jpg', 'data': {'categories': ['#multimodal', '#benchmark', '#dataset', '#audio', '#data'], 'emoji': '🎭', 'ru': {'title': 'Автоматизированная система для создания высококачественных наборов данных эмоциональной речи', 'desc': 'MIKU-PAL - это автоматизированная мультимодальная система для извлечения эмоциональной речи из немаркированных видеоданных. Система использует алгоритмы обнаружения и отслеживания лиц, а также мультимодальную большую языковую модель (MLLM) для анализа эмоций. MIKU-PAL достигает точности на уровне человека (68,5% на наборе данных MELD) и высокой согласованности (0,93 по шкале Флейса каппа), при этом работая быстрее и дешевле ручной разметки. На основе этой системы был создан набор данных MIKU-EmoBench объемом 131,2 часа, содержащий 26 типов эмоциональной речи.'}, 'en': {'title': 'Automating Emotional Speech Extraction with MIKU-PAL', 'desc': 'This paper introduces MIKU-PAL, an automated system designed to extract emotional speech data from unlabeled video sources. It utilizes face detection and tracking, combined with a multimodal large language model, to analyze emotions effectively. The system achieves high accuracy and consistency in emotional speech annotation, outperforming traditional human methods in both cost and speed. Additionally, it provides a new dataset, MIKU-EmoBench, which includes a diverse range of emotional speech categories for further research in speech synthesis.'}, 'zh': {'title': 'MIKU-PAL：高效一致的情感语音提取新方法', 'desc': '本论文提出了一种名为MIKU-PAL的全自动多模态管道，用于从未标记的视频数据中提取高一致性的情感语音。我们利用人脸检测和跟踪算法，开发了一个自动情感分析系统，使用多模态大语言模型（MLLM）。实验结果表明，MIKU-PAL在情感识别上达到了人类水平的准确率（68.5%），并且一致性显著优于人工标注（0.93 Fleiss kappa分数）。基于该系统，我们还发布了一个细粒度情感语音数据集MIKU-EmoBench（131.2小时），为情感文本到语音和视觉语音克隆提供了新的基准。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (3)', '#agents (5)', '#agi (3)', '#alignment (3)', '#architecture (3)', '#audio (4)', '#benchmark (18)', '#cv (8)', '#data (6)', '#dataset (19)', '#diffusion (6)', '#ethics (3)', '#games (9)', '#graphs', '#hallucinations (1)', '#healthcare', '#inference (4)', '#interpretability (4)', '#leakage (1)', '#long_context (2)', '#low_resource (5)', '#machine_translation (1)', '#math', '#multilingual (4)', '#multimodal (15)', '#open_source (9)', '#optimization (16)', '#plp', '#rag', '#reasoning (18)', '#rl (10)', '#rlhf (2)', '#robotics (3)', '#science', '#security (2)', '#small_models (1)', '#story_generation', '#survey (1)', '#synthetic (2)', '#training (18)', '#transfer_learning (2)', '#video (6)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-06-03 10:13',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-06-03 10:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-06-03 10:13')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    