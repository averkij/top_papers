
<!DOCTYPE html>
<html>
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C1CRWDNJ1J"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-C1CRWDNJ1J');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"><title>HF. 23 papers. March 10.</title>
<link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@100..900&family=Tiny5&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: cornflowerblue;
            --primary-color-dark: #fffd87cf;
            --secondary-color: #fff;
            --background-color: #eee;
            --text-color: #333333;
            --header-color: cornflowerblue;
            --body-color: #eee;
            --menu-color: #002370;
        }
        .background-digit {
            position: absolute;
            font-family: 'Tiny5';
            bottom: -20px;
            right: -10px;
            font-size: 8em;
            font-weight: 400;
            color: #0989ea22;
            z-index: 2;
            line-height: 1;
        }
        .dark-theme .background-digit {
            color: #e9e78f3d;
        }
        body {
            font-family: 'Roboto Slab', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            min-height: 100vh;
            display: flex;
            flex-direction: column;
        }
        .container {
            max-width: 1500px;
            margin: 0 auto;
            flex: 1 0 auto;
            width: 100%
        }
        .a-clean {
            color: var(--secondary-color);
            text-decoration: none;
        }
        .a-clean:hover {
            color: #fff;
        }
        header {
            padding: 3.6em 0 2.4em 0;
            text-align: center;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            margin-top: 2em;
            flex-shrink: 0;
            padding: 20px;
        }
        h1 {
            font-size: 2.4em;
            margin: 0;
            font-weight: 700;
        }
        .article-title-cont {
            margin: -21px -21px 0px -21px;
            padding: 10px 20px;
            background: cornflowerblue;
            display: table;
            min-height: 5.9em;
        }
        .dark-theme .article-title-cont {
            background: #444444;
        }
        .article-title {
            color: white;           
        }
        .article-title h2 {
            margin: 0px;
            padding: 0px;
            font-weight: 400;
            text-align:center;
        }
        h2 {
            # color: var(--primary-color);
            font-size: 1.2em;
            margin-top: 0;
            margin-bottom: 0.5em;
        }
        header p {
            font-size: 1.2em;
            margin-top: 0.5em;
            font-weight: 300;
        }
        main {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 1.5em;
            padding: 10px 20px 20px 20px;
        }
        body.dark-tmeme>header {
            background-color: background-color: #333333;
            color: white;
        }
        body.dark-theme>div>main>article>div.article-content>p.meta {
            color: #fff;
        }
        body.light-theme>div>main>article>div.article-content>p.meta {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>p.pub-date {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>p.pub-date {
            color: #555;
        }
        body.dark-theme>div>main>article>div.article-content>div.tags {
            color: #ccc;
        }
        body.light-theme>div>main>article>div.article-content>div.tags {
            color: #fff;
        }
        body.light-theme>header {
            background-color: var(--header-color);
            color: white;
        }
        article {
            display: flex;
            flex-direction: row;
            justify-content: center;
        }
        .article-content {
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow: hidden;
            transition: background-color 0.2s ease;
            padding: 1.3em;
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            position: relative;
            z-index: 1;
            cursor: pointer;
            max-width: 800px;
            position: relative;
        }
        body.dark-theme>div>main>article>div.article-content {
            background-color: #444;
            border: none;
        }
        body.light-theme>div>main>article>div.article-content {
            background-color: #fff;
        }
        body.dark-theme>div>main>article>div.article-content:hover {
            background-color: #414141;
        }
        body.light-theme>div>main>article>div.article-content:hover {
            background-color: #fafafa;
        }
        .meta {
            font-size: 0.9em;
            margin-bottom: 0em;
            font-weight: 500;
            margin: 20px 0 0px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #ddd;
        }
        .pub-date {
            font-size: 0.8em;
            margin-bottom: 0.8em;
            font-weight: 400;
            text-align: right;
            font-family: Roboto;
        }
        .tags {
            font-size: 0.9em;
            margin-bottom: 0;
            position: absolute;
            bottom: 0px;
            font-weight: 300;
            font-family: 'Roboto Slab';
            background: #555;
            left: 0;
            width: 100%;
            padding: 10px 20px;
        }
        .abstract {
            position: relative;
            max-height: 170px;
            overflow: hidden;
            transition: max-height 0.3s ease;
            cursor: pointer;
        }
        .abstract.expanded {
            max-height: 1000px;
        }
        .abstract-toggle {
            position: absolute;
            bottom: 4px;
            right: 0;
            cursor: pointer;
            color: var(--primary-color);
            float: right;
            font-weight: 400;
        }
        .explanation {
            background-color: #e8f5e9;
            border-left: 4px solid var(--secondary-color);
            padding: 1em;
            margin-top: 1.5em;
        }
        .links {
            margin-top: 1.5em;
            margin-bottom: 20px;
        }
        .affiliations {
            margin-bottom: 50px;
            padding:10px;
            font-size: 0.9em;
            text-align: center
        }
        a {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }
        .dark-theme a {
            color: var(--primary-color-dark);
        }
        a:hover {
            color: #e73838;
        }
        .light-theme {
            background-color: var(--body-color);
            color: #333333;
        }
        .dark-theme {
            background-color: #333333;
            color: #ffffff;
        }
        .theme-switch {
            position: absolute;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
        }
        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 30px;
        }
        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
            border-radius: 30px;
        }
        .slider:before {
            position: absolute;
            content: "";
            height: 24px;
            width: 24px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: .4s;
            border-radius: 50%;
        }
        input:checked + .slider {
            background-color: var(--primary-color);
        }
        input:checked + .slider:before {
            transform: translateX(20px);
        }
        .switch-label {
            margin-right: 10px;
        }

        .sub-header-container {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 7px;
            padding: 0 20px;
        }
        .sub-header-container-2 {
            display: flex;
            justify-content: left;
            align-items: center;
            flex-wrap: wrap;
            gap: 15px;
            margin: 0 auto;
            padding: 0 20px;
        }
        .update-info-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: left;
            flex: 1;
        }
        .sort-container {
            margin-top: 15px;
            margin-bottom: 0px;
            text-align: right;
            flex: 2;
        }
        
        .category-toggle-container {
            display: inline-block;
            margin-top: 15px;
            margin-bottom: 10px;
            cursor: pointer;
        }
        .category-option-container {
            margin-top: 15px;
            margin-bottom: 10px;
            display: none;
            margin-left: auto;
        }
        .category-option-container.expanded {
            display: block;
        }

        .sort-dropdown {
            padding: 5px 10px;
            font-size: 16px;
            border-radius: 5px;
            border: 1px solid #ccc;
            background-color: white;
            color: var(--text-color);
            font-family: 'Roboto Slab', sans-serif;
        }
        .sort-label {
            margin-right: 10px;
            font-size: 1.0em !important;
        }        
        .dark-theme .sort-dropdown {
            background-color: #444;
            color: white;
            border-color: var(--text-color);
        }
        .title-sign {
            display: inline-block;
            transition: all 0.5s ease;            
        }
        .rotate {
            transform: rotate(45deg) translateY(-6px);
            transform-origin: center;
        }
        .title-text {
            display: inline;
            padding-left: 10px;
        }
        .summary_title {
            font-size: 1.2em;
            font-weight: bold;
            color: #222;
            margin-bottom: 5px;
        }
        .summary_text {

        }
        .summary_image {
            max-height: 500px;
            max-width: 100%;
            align: center;
            margin-top: 40px;        
            margin-bottom: 60px;        
        }
        .category-filters {
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
            display: none;
        }
        .category-filters.expanded {
            display: block;
            margin-top: 10px;
        }
        .category-button {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .category-button.active {
            background-color: var(--primary-color);
            color: white;
        }
        .category-button.inactive:not(.active) {
            color: #ccc;
        }
        .dark-theme .category-button {
            background-color: #555;
            color: #fff;
        }
        .dark-theme .category-button.active {
            background-color: var(--primary-color);
        }
        .dark-theme .category-button.inactive:not(.active) {
            color: #888;
        }
        .clear-categories {
            display: inline-block;
            margin: 5px;
            padding: 5px 10px;
            border-radius: 15px;
            background-color: #f0f0f0;
            color: #333;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }
        .clear-categories:hover {
            background-color: #bbb;
        }
        .svg-container {
            display: inline-block;
            position: relative;
            overflow: hidden;
        }
        .svg-container span {
            position: relative;
            z-index: 1;
        }
        .svg-container svg {
            position: absolute;
            bottom: 0;
            left: 0;
            z-index: 0;
        }

        .nav-menu {
            background-color: var(--menu-color);
            padding: 2px 0 2px 0;
            display: inline-block;
            position: relative;
            overflow: hidden;
            width: 100%;
        }        
        .nav-container {
            max-width: 1500px;
            margin: 0 auto;
            display: flex;
            justify-content: left;
            gap: 3em;
        }
        .nav-container span a {
            color: white;
        }        
        .nav-item {
            color: white;
            padding: 3px 0px;
            cursor: pointer;
            font-weight: 400;
        }         
        .nav-prev {
            margin-left: 20px;
        }        
        .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.1);
            border-color: rgba(255, 255, 255, 0.3);
        }        
        .language-flags {
            display: flex;
            gap: 7px;
            padding: 5px 20px 0 0;
            margin-left: auto;
        }
        .flag-svg {
            width: 22px;
            height: 22px;
            cursor: pointer;
            opacity: 0.4;
            transition: opacity 0.3s ease;
            border-radius: 2px;
        }
        .flag-svg.active {
            opacity: 1;
        }
        .flag-svg:hover {
            opacity: 0.8;
        }
        
        .dark-theme .nav-menu {
            background-color: #333;
        }
        .dark-theme .nav-item {
            color: white;
        }
        
        .dark-theme .nav-item:hover {
            background-color: rgba(255, 255, 255, 0.05);
        }

        .pointer { cursor: pointer; }

        .article-pdf-title-img {
            max-width: 100%;
            max-height: 400px;
            display: inline-block;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .article-pdf-title-img-cont {
            text-align: center;
        }
        .dark-theme .article-pdf-title-img {
            opacity: 0.8;
            filter: grayscale(1);
        }

        @media (max-width: 600px) {
            .nav-container {
                flex-direction: row;
                gap: 1.5em;
            }            
            .nav-item {
                padding: 3px 0px;
            }
        }
        
        @media (max-width: 768px) {
            .category-filters {
                display: none;
            }
            .category-toggle {
                display: inline-block;
                width: 100%;
                text-align: left;
            }
            .category-filters.expanded {
                display: block;
                margin-top: 10px;
            }
        }
        @media (max-width: 600px) {
            .sub-header-container {
                flex-direction: column;
                align-items: flex-start;
            }
            .sort-container {
                width: 100%;
                display: flex;
                justify-content: left;
                margin: 0 auto;
            }
            .sort-dropdown {
                margin-left: auto;
            }
            .sort-label {
                margin-top: 5px;
                float: left;
            }

            .sub-header-container-2 {
                flex-direction: row;
                align-items: flex-start;
            }
            .update-info-container {
                text-align: left;
                width: 100%;
                margin-bottom: 0px;
            }
            .category-toggle-container {
                margin-top: 15px;
                text-align: left;
                margin-bottom: 10px;
            }
            .category-option-container {
                margin-top: 15px;
                text-align: center;
                margin-bottom: 10px;
            }            
            main {
                grid-template-columns: repeat(auto-fit);
                gap: 0em;
                padding: 10px 0 20px 0;
            }
            footer {
                margin-top: -20px;
            }
            article>div.article-content {
                border-radius: 0px;
            }
        }
    </style>
    <script>
    function toggleAbstract(id) {
        var abstract = document.getElementById('abstract-' + id);
        var toggle = document.getElementById('toggle-' + id);
        if (abstract.classList.contains('expanded')) {
            abstract.classList.remove('expanded');
            toggle.textContent = '...';
        } else {
            abstract.classList.add('expanded');
            toggle.textContent = '';
        }
    }
    function getTimeDiff(dateString, lang='ru') {
        const timeUnits = {
            ru: {
                minute: ["минуту", "минуты", "минут"],
                hour: ["час", "часа", "часов"],
                day: ["день", "дня", "дней"],
                justNow: "только что",
                ago: "назад"
            },
            en: {
                minute: ["minute", "minutes", "minutes"],
                hour: ["hour", "hours", "hours"],
                day: ["day", "days", "days"],
                justNow: "just now",
                ago: "ago"
            },
            zh: {
                minute: ["分钟", "分钟", "分钟"],
                hour: ["小时", "小时", "小时"],
                day: ["天", "天", "天"],
                justNow: "刚刚",
                ago: "前"
            }
        };

        function getPlural(number, words, lang) {
            if (lang === 'ru') {
                if (number % 10 === 1 && number % 100 !== 11) {
                    return words[0];
                } else if (number % 10 >= 2 && number % 10 <= 4 && (number % 100 < 10 || number % 100 >= 20)) {
                    return words[1];
                } else {
                    return words[2];
                }
            } else if (lang === 'en') {
                return number === 1 ? words[0] : words[1];
            } else {
                // Chinese doesn't need plural forms
                return words[0];
            }
        }

        function formatTimeDiff(number, unit, lang) {
            const unitWord = getPlural(number, timeUnits[lang][unit], lang);
            
            if (lang === 'zh') {
                return `${number}${unitWord}${timeUnits[lang].ago}`;
            } else {
                return `${number} ${unitWord} ${timeUnits[lang].ago}`;
            }
        }

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        const pastDate = new Date(dateString.replace(" ", "T") + ":00Z");
        const currentDate = new Date();
        const diffInSeconds = Math.floor((currentDate - pastDate) / 1000);
        
        const minutes = Math.floor(diffInSeconds / 60);
        const hours = Math.floor(diffInSeconds / 3600);
        const days = Math.floor(diffInSeconds / 86400);

        if (minutes === 0) {
            return timeUnits[lang].justNow;
        } else if (minutes < 60) {
            return formatTimeDiff(minutes, 'minute', lang);
        } else if (hours < 24) {
            return formatTimeDiff(hours, 'hour', lang);
        } else {
            return formatTimeDiff(days, 'day', lang);
        }
    }
    function isToday(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth() &&
            inputDate.getDate() === today.getDate()
        );
    }
    function isCurrentMonth(dateString) {
        const inputDate = new Date(dateString);
        const today = new Date();
        return (
            inputDate.getFullYear() === today.getFullYear() &&
            inputDate.getMonth() === today.getMonth()
        );
    }
    function formatArticlesTitle(number, lang='ru') {
        const lastDigit = number % 10;
        const lastTwoDigits = number % 100;
        let word;

        if (!['ru', 'en', 'zh'].includes(lang)) {
            throw new Error('Unsupported language. Supported languages are: ru, en, zh');
        }

        if (lang === 'ru') {
            if (lastTwoDigits >= 11 && lastTwoDigits <= 14) {
                word = "статей";
            } else if (lastDigit === 1) {
                word = "статья";
            } else if (lastDigit >= 2 && lastDigit <= 4) {
                word = "статьи";
            } else {
                word = "статей";
            }
        } else if (lang === 'en') {
            if (number === 1) {
                word = 'paper'
            } else {
                word = 'papers'
            }
        } else if (lang === 'zh') {
            word = "篇论文"
        }

        if (lang === 'zh') {
            return `${number}${word}`;
        } else {
            return `${number} ${word}`;
        }
    }
    </script>
</head>
<body class="light-theme">
    <header>
        <div class="container">            
            <a href="https://hfday.ru" class="a-clean"><h1 class="title-sign" id="doomgrad-icon">🔺</h1><h1 class="title-text" id="doomgrad">hf daily</h1></a>
            <p><span id="title-date">10 марта</span> | <span id="title-articles-count">23 papers</span></p>
        </div>
        <div class="theme-switch">
            <label class="switch">
                <input type="checkbox" id="theme-toggle">
                <span class="slider"></span>
            </label>
        </div>
    </header>
    <div class="nav-menu">
        <div class="nav-container">
            <span class="nav-item nav-prev" id="nav-prev"><a href="/d/2025-03-07.html">⬅️ <span id="prev-date">07.03</span></a></span>
            <span class="nav-item" id="nav-next"><a href="/d/2025-03-11.html">➡️ <span id="next-date">11.03</span></a></span>
            <span class="nav-item" id="nav-monthly"><a href="/m/2025-03.html">📈 <span id='top-month-label'>Месяц</span></a></span>
            <div class="language-flags">
                <svg class="flag-svg" data-lang="ru" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><path fill="#1435a1" d="M1 11H31V21H1z"></path><path d="M5,4H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" fill="#fff"></path><path d="M5,20H27c2.208,0,4,1.792,4,4v4H1v-4c0-2.208,1.792-4,4-4Z" transform="rotate(180 16 24)" fill="#c53a28"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="zh" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#db362f"></rect><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path fill="#ff0" d="M7.958 10.152L7.19 7.786 6.421 10.152 3.934 10.152 5.946 11.614 5.177 13.979 7.19 12.517 9.202 13.979 8.433 11.614 10.446 10.152 7.958 10.152z"></path><path fill="#ff0" d="M12.725 8.187L13.152 8.898 13.224 8.072 14.032 7.886 13.269 7.562 13.342 6.736 12.798 7.361 12.035 7.037 12.461 7.748 11.917 8.373 12.725 8.187z"></path><path fill="#ff0" d="M14.865 10.372L14.982 11.193 15.37 10.46 16.187 10.602 15.61 10.007 15.997 9.274 15.253 9.639 14.675 9.044 14.793 9.865 14.048 10.23 14.865 10.372z"></path><path fill="#ff0" d="M15.597 13.612L16.25 13.101 15.421 13.13 15.137 12.352 14.909 13.149 14.081 13.179 14.769 13.642 14.541 14.439 15.194 13.928 15.881 14.391 15.597 13.612z"></path><path fill="#ff0" d="M13.26 15.535L13.298 14.707 12.78 15.354 12.005 15.062 12.46 15.754 11.942 16.402 12.742 16.182 13.198 16.875 13.236 16.047 14.036 15.827 13.26 15.535z"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path></svg>
                <svg class="flag-svg" data-lang="en" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32"><rect x="1" y="4" width="30" height="24" rx="4" ry="4" fill="#fff"></rect><path d="M1.638,5.846H30.362c-.711-1.108-1.947-1.846-3.362-1.846H5c-1.414,0-2.65,.738-3.362,1.846Z" fill="#a62842"></path><path d="M2.03,7.692c-.008,.103-.03,.202-.03,.308v1.539H31v-1.539c0-.105-.022-.204-.03-.308H2.03Z" fill="#a62842"></path><path fill="#a62842" d="M2 11.385H31V13.231H2z"></path><path fill="#a62842" d="M2 15.077H31V16.923000000000002H2z"></path><path fill="#a62842" d="M1 18.769H31V20.615H1z"></path><path d="M1,24c0,.105,.023,.204,.031,.308H30.969c.008-.103,.031-.202,.031-.308v-1.539H1v1.539Z" fill="#a62842"></path><path d="M30.362,26.154H1.638c.711,1.108,1.947,1.846,3.362,1.846H27c1.414,0,2.65-.738,3.362-1.846Z" fill="#a62842"></path><path d="M5,4h11v12.923H1V8c0-2.208,1.792-4,4-4Z" fill="#102d5e"></path><path d="M27,4H5c-2.209,0-4,1.791-4,4V24c0,2.209,1.791,4,4,4H27c2.209,0,4-1.791,4-4V8c0-2.209-1.791-4-4-4Zm3,20c0,1.654-1.346,3-3,3H5c-1.654,0-3-1.346-3-3V8c0-1.654,1.346-3,3-3H27c1.654,0,3,1.346,3,3V24Z" opacity=".15"></path><path d="M27,5H5c-1.657,0-3,1.343-3,3v1c0-1.657,1.343-3,3-3H27c1.657,0,3,1.343,3,3v-1c0-1.657-1.343-3-3-3Z" fill="#fff" opacity=".2"></path><path fill="#fff" d="M4.601 7.463L5.193 7.033 4.462 7.033 4.236 6.338 4.01 7.033 3.279 7.033 3.87 7.463 3.644 8.158 4.236 7.729 4.827 8.158 4.601 7.463z"></path><path fill="#fff" d="M7.58 7.463L8.172 7.033 7.441 7.033 7.215 6.338 6.989 7.033 6.258 7.033 6.849 7.463 6.623 8.158 7.215 7.729 7.806 8.158 7.58 7.463z"></path><path fill="#fff" d="M10.56 7.463L11.151 7.033 10.42 7.033 10.194 6.338 9.968 7.033 9.237 7.033 9.828 7.463 9.603 8.158 10.194 7.729 10.785 8.158 10.56 7.463z"></path><path fill="#fff" d="M6.066 9.283L6.658 8.854 5.927 8.854 5.701 8.158 5.475 8.854 4.744 8.854 5.335 9.283 5.109 9.979 5.701 9.549 6.292 9.979 6.066 9.283z"></path><path fill="#fff" d="M9.046 9.283L9.637 8.854 8.906 8.854 8.68 8.158 8.454 8.854 7.723 8.854 8.314 9.283 8.089 9.979 8.68 9.549 9.271 9.979 9.046 9.283z"></path><path fill="#fff" d="M12.025 9.283L12.616 8.854 11.885 8.854 11.659 8.158 11.433 8.854 10.702 8.854 11.294 9.283 11.068 9.979 11.659 9.549 12.251 9.979 12.025 9.283z"></path><path fill="#fff" d="M6.066 12.924L6.658 12.494 5.927 12.494 5.701 11.799 5.475 12.494 4.744 12.494 5.335 12.924 5.109 13.619 5.701 13.19 6.292 13.619 6.066 12.924z"></path><path fill="#fff" d="M9.046 12.924L9.637 12.494 8.906 12.494 8.68 11.799 8.454 12.494 7.723 12.494 8.314 12.924 8.089 13.619 8.68 13.19 9.271 13.619 9.046 12.924z"></path><path fill="#fff" d="M12.025 12.924L12.616 12.494 11.885 12.494 11.659 11.799 11.433 12.494 10.702 12.494 11.294 12.924 11.068 13.619 11.659 13.19 12.251 13.619 12.025 12.924z"></path><path fill="#fff" d="M13.539 7.463L14.13 7.033 13.399 7.033 13.173 6.338 12.947 7.033 12.216 7.033 12.808 7.463 12.582 8.158 13.173 7.729 13.765 8.158 13.539 7.463z"></path><path fill="#fff" d="M4.601 11.104L5.193 10.674 4.462 10.674 4.236 9.979 4.01 10.674 3.279 10.674 3.87 11.104 3.644 11.799 4.236 11.369 4.827 11.799 4.601 11.104z"></path><path fill="#fff" d="M7.58 11.104L8.172 10.674 7.441 10.674 7.215 9.979 6.989 10.674 6.258 10.674 6.849 11.104 6.623 11.799 7.215 11.369 7.806 11.799 7.58 11.104z"></path><path fill="#fff" d="M10.56 11.104L11.151 10.674 10.42 10.674 10.194 9.979 9.968 10.674 9.237 10.674 9.828 11.104 9.603 11.799 10.194 11.369 10.785 11.799 10.56 11.104z"></path><path fill="#fff" d="M13.539 11.104L14.13 10.674 13.399 10.674 13.173 9.979 12.947 10.674 12.216 10.674 12.808 11.104 12.582 11.799 13.173 11.369 13.765 11.799 13.539 11.104z"></path><path fill="#fff" d="M4.601 14.744L5.193 14.315 4.462 14.315 4.236 13.619 4.01 14.315 3.279 14.315 3.87 14.744 3.644 15.44 4.236 15.01 4.827 15.44 4.601 14.744z"></path><path fill="#fff" d="M7.58 14.744L8.172 14.315 7.441 14.315 7.215 13.619 6.989 14.315 6.258 14.315 6.849 14.744 6.623 15.44 7.215 15.01 7.806 15.44 7.58 14.744z"></path><path fill="#fff" d="M10.56 14.744L11.151 14.315 10.42 14.315 10.194 13.619 9.968 14.315 9.237 14.315 9.828 14.744 9.603 15.44 10.194 15.01 10.785 15.44 10.56 14.744z"></path><path fill="#fff" d="M13.539 14.744L14.13 14.315 13.399 14.315 13.173 13.619 12.947 14.315 12.216 14.315 12.808 14.744 12.582 15.44 13.173 15.01 13.765 15.44 13.539 14.744z"></path></svg>
            </div>
        </div>
    </div>
    <div class="container">
        <div class="sub-header-container">
            <div class="update-info-container">
                <label class="update-info-label" id="timeDiff"></label>
            </div>
            <div class="sort-container">
                <label class="sort-label">🔀 <span id="sort-label-text">Сортировка по</span></label>
                <select id="sort-dropdown" class="sort-dropdown">
                    <option value="default">рейтингу</option>
                    <option value="pub_date">дате публикации</option>
                    <option value="issue_id">добавлению на HF</option>
                </select>
            </div>
        </div>
        <div class="sub-header-container-2">
            <div class="category-toggle-container">
                <div class="svg-container">
                    <span id="category-toggle">🏷️ Фильтр</span>
                    <svg height="3" width="200">
                        <line x1="0" y1="0" x2="200" y2="0" 
                            stroke="black" 
                            stroke-width="2" 
                            stroke-dasharray="3, 3" />
                    </svg>
                </div>
            </div>
            <div class="category-option-container" id="category-options">                
                <label class="pointer" for="filter-logic-or"><input type="radio" id="filter-logic-or" name="filter-logic" value="or"> A∪B</label>
                <label class="pointer" for="filter-logic-and"><input type="radio" id="filter-logic-and" name="filter-logic" value="and"> A∩B</label>
            </div> 
        </div>
        <div class="category-filters" id="category-filters">
            <span class="clear-categories" id="clear-categories">🧹</span>
            <!-- Categories -->
        </div>
        <main id="articles-container">
            <!-- Articles -->
        </main>
    </div>
    <footer>
        <div class="container">
            <p><a style="color:white;" href="https://t.me/doomgrad">doomgrad</a> ✖️ <a style="color:white;" href="https://huggingface.co/papers">hugging face</a></p>
        </div>
    </footer>
    <script>
        // Language handling
        let currentLang = localStorage.getItem('selectedLang') || 'en';
        let feedDate = {'ru': '10 марта', 'en': 'March 10', 'zh': '3月10日'};
        let feedDateNext = {'ru': '11.03', 'en': '03/11', 'zh': '3月11日'};
        let feedDatePrev = {'ru': '07.03', 'en': '03/07', 'zh': '3月7日'};
        let filterLabel = {'ru': 'Фильтр', 'en': 'Topics', 'zh': '主题筛选'}
        let publishedLabel = {'ru': 'статья от ', 'en': 'published on ', 'zh': '发表于'}
        let sortLabel = {'ru': 'Сортировка по', 'en': 'Sort by', 'zh': '排序方式'}
        let paperLabel = {'ru': 'Статья', 'en': 'Paper', 'zh': '论文'}
        let topMonthLabel = {'ru': 'Месяц', 'en': 'Month', 'zh': '月度论文'}
        let topDayLabel = {'ru': 'День', 'en': 'Day', 'zh': '日度论文'}
        
        function initializeLanguageFlags() {
            const flags = document.querySelectorAll('.flag-svg');
            flags.forEach(flag => {
                if (flag.dataset.lang === currentLang) {
                    flag.classList.add('active');
                }
                flag.addEventListener('click', () => {
                    flags.forEach(f => f.classList.remove('active'));
                    flag.classList.add('active');
                    currentLang = flag.dataset.lang;
                    localStorage.setItem('selectedLang', currentLang);
                    updateTimeDiffs();
                    updateLocalization();
                    filterAndRenderArticles();
                });
            });
        }
        function toggleTheme() {
            const body = document.body;
            body.classList.toggle('light-theme');
            body.classList.toggle('dark-theme');

            const isDarkMode = body.classList.contains('dark-theme');
            localStorage.setItem('darkMode', isDarkMode);
            
            if (isDarkMode) {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }  else {
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf daily";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.remove('rotate');
            }
        }

        const articlesData = [{'id': 'https://huggingface.co/papers/2503.05236', 'title': 'Unified Reward Model for Multimodal Understanding and Generation', 'url': 'https://huggingface.co/papers/2503.05236', 'abstract': 'Recent advances in human preference alignment have significantly enhanced multimodal generation and understanding. A key approach is training reward models to guide preference optimization. However, existing models are often task-specific, limiting their adaptability across diverse visual applications. We also argue that jointly learning to assess multiple tasks may foster a synergistic effect, where improved image understanding enhances image generation assessment, and refined image evaluation benefits video assessment through better frame analysis. To this end, this paper proposes UnifiedReward, the first unified reward model for multimodal understanding and generation assessment, enabling both pairwise ranking and pointwise scoring, which can be employed for vision model preference alignment. Specifically, (1) we first develop UnifiedReward on our constructed large-scale human preference dataset, including both image and video generation/understanding tasks. (2) Then, it is utilized to automatically construct high-quality preference pair data based on the vision models, fine-gradually filtering their outputs through pair ranking and point sifting. (3) Finally, these data are used for their preference alignment through Direct Preference Optimization (DPO). Experimental results demonstrate that joint learning to assess diverse visual tasks can lead to substantial mutual benefits and we apply our pipeline to both image and video understanding/generation tasks, significantly improving the performance in each domain.', 'score': 80, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6ebf61a6777b8e4d', 'authors': ['Yibin Wang', 'Yuhang Zang', 'Hao Li', 'Cheng Jin', 'Jiaqi Wang'], 'affiliations': [], 'pdf_title_img': 'assets/pdf/title_img/2503.05236.jpg', 'data': {'categories': ['#multimodal', '#video', '#cv', '#alignment', '#dataset', '#rlhf', '#rag'], 'emoji': '🤖', 'ru': {'title': 'Единая модель вознаграждения для улучшения мультимодальных AI-систем', 'desc': 'Статья представляет UnifiedReward - первую унифицированную модель вознаграждения для оценки мультимодального понимания и генерации. Модель обучается на большом наборе данных о человеческих предпочтениях, включающем задачи по генерации и пониманию изображений и видео. UnifiedReward используется для автоматического создания высококачественных пар предпочтений на основе моделей компьютерного зрения. Затем эти данные применяются для настройки предпочтений моделей с помощью метода Direct Preference Optimization (DPO).'}, 'en': {'title': 'UnifiedReward: Enhancing Multimodal Learning through Joint Preference Alignment', 'desc': 'This paper introduces UnifiedReward, a novel reward model designed to enhance multimodal understanding and generation in machine learning. It addresses the limitation of existing task-specific models by enabling joint learning across various visual tasks, which improves both image and video assessments. The model is trained on a large-scale human preference dataset and utilizes techniques like pairwise ranking and pointwise scoring for effective preference alignment. Experimental results show that this unified approach leads to significant performance improvements in both image and video tasks, demonstrating the benefits of synergistic learning.'}, 'zh': {'title': '统一奖励模型，提升多模态理解与生成', 'desc': '最近在人类偏好对齐方面的进展显著提升了多模态生成和理解的能力。关键方法是训练奖励模型来指导偏好优化。然而，现有模型通常是特定于任务的，限制了它们在不同视觉应用中的适应性。本文提出了UnifiedReward，这是第一个用于多模态理解和生成评估的统一奖励模型，能够同时进行成对排名和逐点评分，从而实现视觉模型的偏好对齐。'}}}, {'id': 'https://huggingface.co/papers/2502.21263', 'title': 'RuCCoD: Towards Automated ICD Coding in Russian', 'url': 'https://huggingface.co/papers/2502.21263', 'abstract': 'This study investigates the feasibility of automating clinical coding in Russian, a language with limited biomedical resources. We present a new dataset for ICD coding, which includes diagnosis fields from electronic health records (EHRs) annotated with over 10,000 entities and more than 1,500 unique ICD codes. This dataset serves as a benchmark for several state-of-the-art models, including BERT, LLaMA with LoRA, and RAG, with additional experiments examining transfer learning across domains (from PubMed abstracts to medical diagnosis) and terminologies (from UMLS concepts to ICD codes). We then apply the best-performing model to label an in-house EHR dataset containing patient histories from 2017 to 2021. Our experiments, conducted on a carefully curated test set, demonstrate that training with the automated predicted codes leads to a significant improvement in accuracy compared to manually annotated data from physicians. We believe our findings offer valuable insights into the potential for automating clinical coding in resource-limited languages like Russian, which could enhance clinical efficiency and data accuracy in these contexts.', 'score': 61, 'issue_id': 2617, 'pub_date': '2025-02-28', 'pub_date_card': {'ru': '28 февраля', 'en': 'February 28', 'zh': '2月28日'}, 'hash': '89ad8229208a4f98', 'authors': ['Aleksandr Nesterov', 'Andrey Sakhovskiy', 'Ivan Sviridov', 'Airat Valiev', 'Vladimir Makharev', 'Petr Anokhin', 'Galina Zubkova', 'Elena Tutubalina'], 'affiliations': ['AIRI, Moscow, Russia', 'HSE University, Moscow, Russia', 'ISP RAS Research Center for Trusted Artificial Intelligence, Moscow, Russia', 'Sber AI Lab, Moscow, Russia', 'Sber AI, Moscow, Russia'], 'pdf_title_img': 'assets/pdf/title_img/2502.21263.jpg', 'data': {'categories': ['#dataset', '#transfer_learning', '#benchmark', '#science', '#low_resource', '#healthcare', '#training'], 'emoji': '🏥', 'ru': {'title': 'Автоматизация клинического кодирования на русском языке: прорыв в эффективности и точности', 'desc': 'Исследование посвящено автоматизации клинического кодирования на русском языке с использованием методов машинного обучения. Авторы представляют новый набор данных для кодирования по МКБ, включающий более 10000 сущностей и 1500 уникальных кодов МКБ. Проводится сравнение современных моделей, таких как BERT, LLaMA с LoRA и RAG, а также исследуется перенос обучения между доменами и терминологиями. Результаты показывают, что обучение на автоматически предсказанных кодах значительно повышает точность по сравнению с ручной разметкой врачей.'}, 'en': {'title': 'Automating Clinical Coding: A Leap for Russian Healthcare', 'desc': 'This paper explores the automation of clinical coding in the Russian language, which lacks extensive biomedical resources. It introduces a new dataset for ICD coding, featuring over 10,000 annotated entities and 1,500 unique ICD codes derived from electronic health records. The study benchmarks various advanced models, including BERT and LLaMA, and investigates transfer learning from different medical domains and terminologies. Results show that using automated coding significantly improves accuracy over traditional manual coding by physicians, highlighting the potential for enhanced clinical efficiency in resource-limited settings.'}, 'zh': {'title': '自动化临床编码：提升俄语医疗效率的希望', 'desc': '本研究探讨了在俄语中自动化临床编码的可行性，俄语的生物医学资源相对有限。我们提出了一个新的ICD编码数据集，该数据集包含来自电子健康记录（EHR）的诊断字段，标注了超过10,000个实体和1,500多个独特的ICD代码。通过对多种先进模型（如BERT、LLaMA与LoRA、RAG）的基准测试，以及跨领域和术语的迁移学习实验，我们验证了模型的有效性。实验结果表明，使用自动预测的代码进行训练，相较于医生手动标注的数据，显著提高了准确性，显示了在资源有限的语言中自动化临床编码的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.05500', 'title': 'EuroBERT: Scaling Multilingual Encoders for European Languages', 'url': 'https://huggingface.co/papers/2503.05500', 'abstract': 'General-purpose multilingual vector representations, used in retrieval, regression and classification, are traditionally obtained from bidirectional encoder models. Despite their wide applicability, encoders have been recently overshadowed by advances in generative decoder-only models. However, many innovations driving this progress are not inherently tied to decoders. In this paper, we revisit the development of multilingual encoders through the lens of these advances, and introduce EuroBERT, a family of multilingual encoders covering European and widely spoken global languages. Our models outperform existing alternatives across a diverse range of tasks, spanning multilingual capabilities, mathematics, and coding, and natively supporting sequences of up to 8,192 tokens. We also examine the design decisions behind EuroBERT, offering insights into our dataset composition and training pipeline. We publicly release the EuroBERT models, including intermediate training checkpoints, together with our training framework.', 'score': 46, 'issue_id': 2613, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'befb9ffdb6a6cd73', 'authors': ['Nicolas Boizard', 'Hippolyte Gisserot-Boukhlef', 'Duarte M. Alves', 'André Martins', 'Ayoub Hammal', 'Caio Corro', 'Céline Hudelot', 'Emmanuel Malherbe', 'Etienne Malaboeuf', 'Fanny Jourdan', 'Gabriel Hautreux', 'João Alves', 'Kevin El-Haddad', 'Manuel Faysse', 'Maxime Peyrard', 'Nuno M. Guerreiro', 'Patrick Fernandes', 'Ricardo Rei', 'Pierre Colombo'], 'affiliations': ['Artefact', 'CINES', 'CNRS', 'Carnegie Mellon University', 'Diabolocom', 'Equall', 'INSA Rennes', 'IRISA', 'IRT Saint Exupery', 'ISIA Lab', 'Illuin Technology', 'Instituto Superior Tecnico & Universidade de Lisboa (Lisbon ELLIS Unit)', 'Instituto de Telecomunicacoes', 'LISN', 'MICS, CentraleSupelec, Universite Paris-Saclay', 'Unbabel', 'Universite Grenoble Alpes, Grenoble INP, LIG', 'Universite Paris-Saclay'], 'pdf_title_img': 'assets/pdf/title_img/2503.05500.jpg', 'data': {'categories': ['#multilingual', '#open_source', '#architecture', '#training', '#dataset', '#long_context'], 'emoji': '🌍', 'ru': {'title': 'EuroBERT: Возрождение многоязычных энкодеров в эпоху генеративных моделей', 'desc': 'В статье представлена модель EuroBERT - семейство многоязычных энкодеров для европейских и широко распространенных мировых языков. Эти энкодеры превосходят существующие аналоги в различных задачах, включая многоязычные возможности, математику и программирование. Модели EuroBERT поддерживают последовательности длиной до 8192 токенов и были разработаны с учетом последних достижений в области генеративных декодер-моделей. Авторы также описывают процесс создания датасета и pipeline обучения модели.'}, 'en': {'title': 'EuroBERT: Advancing Multilingual Encoders for Diverse Tasks', 'desc': 'This paper presents EuroBERT, a new family of multilingual encoder models designed to improve performance in various tasks such as retrieval, regression, and classification. Unlike traditional models that rely heavily on bidirectional encoders, EuroBERT leverages recent advancements in generative models while maintaining the strengths of encoders. The models are capable of handling sequences of up to 8,192 tokens and demonstrate superior performance across multilingual tasks, mathematics, and coding challenges. The authors also provide insights into the dataset and training processes used to develop EuroBERT, along with public access to the models and training framework.'}, 'zh': {'title': 'EuroBERT：提升多语言处理的新选择', 'desc': '本文介绍了一种新的多语言编码器模型，称为EuroBERT，旨在提升多语言检索、回归和分类任务的性能。尽管生成解码器模型近年来取得了显著进展，但我们认为多语言编码器仍然具有重要价值。EuroBERT覆盖了欧洲及广泛使用的全球语言，并在多种任务中表现优于现有模型。我们还分享了EuroBERT的设计决策、数据集构成和训练流程，并公开发布了模型及其训练框架。'}}}, {'id': 'https://huggingface.co/papers/2503.05085', 'title': 'S2S-Arena, Evaluating Speech2Speech Protocols on Instruction Following\n  with Paralinguistic Information', 'url': 'https://huggingface.co/papers/2503.05085', 'abstract': 'The rapid development of large language models (LLMs) has brought significant attention to speech models, particularly recent progress in speech2speech protocols supporting speech input and output. However, the existing benchmarks adopt automatic text-based evaluators for evaluating the instruction following ability of these models lack consideration for paralinguistic information in both speech understanding and generation. To address these issues, we introduce S2S-Arena, a novel arena-style S2S benchmark that evaluates instruction-following capabilities with paralinguistic information in both speech-in and speech-out across real-world tasks. We design 154 samples that fused TTS and live recordings in four domains with 21 tasks and manually evaluate existing popular speech models in an arena-style manner. The experimental results show that: (1) in addition to the superior performance of GPT-4o, the speech model of cascaded ASR, LLM, and TTS outperforms the jointly trained model after text-speech alignment in speech2speech protocols; (2) considering paralinguistic information, the knowledgeability of the speech model mainly depends on the LLM backbone, and the multilingual support of that is limited by the speech module; (3) excellent speech models can already understand the paralinguistic information in speech input, but generating appropriate audio with paralinguistic information is still a challenge.', 'score': 37, 'issue_id': 2619, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'aa9d1f284b6fe901', 'authors': ['Feng Jiang', 'Zhiyu Lin', 'Fan Bu', 'Yuhao Du', 'Benyou Wang', 'Haizhou Li'], 'affiliations': ['The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2503.05085.jpg', 'data': {'categories': ['#audio', '#multilingual', '#benchmark'], 'emoji': '🗣️', 'ru': {'title': 'S2S-Arena: новый подход к оценке речевых моделей с учетом паралингвистики', 'desc': 'Статья представляет S2S-Arena - новый бенчмарк для оценки речевых моделей, учитывающий паралингвистическую информацию. Авторы создали 154 образца в 4 доменах с 21 задачей, объединяющих синтезированную и живую речь. Результаты показывают преимущество каскадных моделей (ASR+LLM+TTS) над совместно обученными в задачах speech2speech. Выявлено, что современные речевые модели хорошо понимают паралингвистическую информацию во входной речи, но генерация соответствующего аудио остается сложной задачей.'}, 'en': {'title': 'Enhancing Speech Models with Paralinguistic Insights', 'desc': "This paper presents S2S-Arena, a new benchmark for evaluating speech-to-speech (S2S) models that incorporates paralinguistic information, which includes elements like tone and emotion in speech. The authors highlight that current evaluation methods primarily rely on text-based metrics, which do not adequately assess the models' ability to understand and generate speech with these nuances. Through a series of 154 samples across various tasks, the study reveals that models like GPT-4o and cascaded ASR-LLM-TTS outperform others when considering paralinguistic factors. The findings indicate that while advanced speech models can comprehend paralinguistic cues, generating speech that accurately reflects these cues remains a significant challenge."}, 'zh': {'title': '引入副语言信息的语音模型评估新标准', 'desc': '随着大型语言模型（LLMs）的快速发展，语音模型也受到了广泛关注，尤其是在语音输入和输出的语音到语音（speech2speech）协议方面的进展。然而，现有的基准测试主要依赖自动文本评估器来评估这些模型的指令遵循能力，未能考虑语音理解和生成中的副语言信息。为了解决这些问题，我们提出了S2S-Arena，这是一个新颖的竞技场风格的S2S基准，评估在真实任务中语音输入和输出的指令遵循能力，并考虑副语言信息。实验结果表明，尽管GPT-4o表现优越，但在语音到语音协议中，级联的ASR、LLM和TTS语音模型在文本-语音对齐后优于联合训练模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05179', 'title': 'Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching', 'url': 'https://huggingface.co/papers/2503.05179', 'abstract': 'Recent advances in large language models have demonstrated remarkable reasoning capabilities through Chain of Thought (CoT) prompting, but often at the cost of excessive verbosity in their intermediate outputs, which increases computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting framework that combines cognitive-inspired reasoning paradigms with linguistic constraints to minimize token usage while preserving reasoning accuracy. SoT is designed as a flexible framework that can incorporate any custom reasoning paradigms based on cognitive science, and we instantiate it with three such paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each tailored to different reasoning tasks and selected dynamically via a lightweight routing model. Through comprehensive evaluation across 15 reasoning datasets with multiple languages and multimodal scenarios, we demonstrate that SoT achieves token reductions of 76% with negligible accuracy impact. In certain domains like mathematical and multi-hop reasoning, it even improves accuracy while using significantly fewer tokens. Our code is publicly available: https://www.github.com/SimonAytes/SoT.', 'score': 31, 'issue_id': 2607, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': 'e02cb6f62715b753', 'authors': ['Simon A. Aytes', 'Jinheon Baek', 'Sung Ju Hwang'], 'affiliations': ['DeepAuto.ai', 'KAIST'], 'pdf_title_img': 'assets/pdf/title_img/2503.05179.jpg', 'data': {'categories': ['#multimodal', '#dataset', '#reasoning', '#multilingual', '#optimization', '#open_source', '#math', '#training'], 'emoji': '🧠', 'ru': {'title': 'Эффективное рассуждение с минимальным использованием токенов', 'desc': 'Статья представляет новый метод промптинга под названием Sketch-of-Thought (SoT), который сочетает когнитивно-вдохновленные парадигмы рассуждений с лингвистическими ограничениями. SoT направлен на минимизацию использования токенов при сохранении точности рассуждений в больших языковых моделях. Метод включает три парадигмы: Conceptual Chaining, Chunked Symbolism и Expert Lexicons, каждая из которых адаптирована для различных задач рассуждения. Эксперименты на 15 наборах данных показали, что SoT сокращает использование токенов на 76% без значительного влияния на точность, а в некоторых областях даже улучшает ее.'}, 'en': {'title': 'Efficient Reasoning with Sketch-of-Thought', 'desc': 'This paper presents a new prompting framework called Sketch-of-Thought (SoT) that enhances reasoning in large language models while reducing the number of tokens used. SoT integrates cognitive-inspired reasoning methods with linguistic constraints to maintain accuracy without excessive verbosity. The framework is adaptable, allowing for the inclusion of various reasoning paradigms, which are dynamically selected based on the task at hand. Evaluation shows that SoT can reduce token usage by 76% with little to no loss in accuracy, and in some cases, it even improves performance in specific reasoning tasks.'}, 'zh': {'title': '思维草图：高效推理的新方法', 'desc': '本文介绍了一种新的提示框架，称为思维草图（Sketch-of-Thought，SoT），旨在提高大型语言模型的推理能力，同时减少中间输出的冗长性。SoT结合了认知科学的推理范式和语言约束，以最小化令牌使用量，同时保持推理的准确性。该框架灵活，可以根据认知科学的不同推理范式进行定制，并通过轻量级路由模型动态选择。通过在15个推理数据集上的全面评估，SoT实现了76%的令牌减少，且对准确性影响微乎其微，甚至在某些领域提高了准确性。'}}}, {'id': 'https://huggingface.co/papers/2503.02130', 'title': 'Forgetting Transformer: Softmax Attention with a Forget Gate', 'url': 'https://huggingface.co/papers/2503.02130', 'abstract': 'An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism the Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer\'s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a "Pro" block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at https://github.com/zhixuan-lin/forgetting-transformer.', 'score': 16, 'issue_id': 2607, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': '4c39f334b6c4ed28', 'authors': ['Zhixuan Lin', 'Evgenii Nikishin', 'Xu Owen He', 'Aaron Courville'], 'affiliations': ['MakerMaker AI', 'Mila & Universite de Montreal'], 'pdf_title_img': 'assets/pdf/title_img/2503.02130.jpg', 'data': {'categories': ['#long_context', '#architecture', '#optimization', '#training'], 'emoji': '🧠', 'ru': {'title': 'Forgetting Transformer: Улучшение обработки длинных последовательностей в трансформерах', 'desc': "Исследователи представили новую модель под названием Forgetting Transformer (FoX), которая включает механизм 'забывающего внимания' в архитектуру трансформера. FoX превосходит стандартный трансформер в задачах моделирования языка с длинным контекстом и экстраполяции длины, сохраняя при этом высокую производительность на задачах с коротким контекстом. Модель совместима с алгоритмом FlashAttention и не требует позиционных эмбеддингов. Анализ показывает, что FoX сохраняет преимущества трансформера в обработке длинного контекста по сравнению с рекуррентными моделями."}, 'en': {'title': 'Enhancing Transformers with Forgetting Attention for Superior Performance', 'desc': 'This paper introduces a new attention mechanism called Forgetting Attention, which integrates a forget gate into Transformer models. The Forgetting Transformer (FoX) leverages this mechanism to improve performance on various language modeling tasks, particularly those involving long contexts. FoX not only matches the performance of traditional Transformers on long-context tasks but also excels in short-context and length extrapolation tasks. Additionally, it is compatible with the FlashAttention algorithm and eliminates the need for positional embeddings, enhancing its efficiency and effectiveness.'}, 'zh': {'title': '遗忘变换器：提升长上下文建模的利器', 'desc': '本文提出了一种新的注意力机制，称为遗忘注意力（Forgetting Attention），可以有效地将遗忘门集成到Transformer模型中。通过以数据为依赖的方式降低未归一化注意力分数，遗忘注意力使得Transformer在长上下文语言建模和长度外推任务中表现优于传统的Transformer。我们还设计了一个“Pro”模块，结合了递归序列模型中的一些常见架构组件，显著提升了FoX和Transformer的性能。此外，FoX在长上下文任务中保持了Transformer的优势，超越了其他递归序列模型。'}}}, {'id': 'https://huggingface.co/papers/2503.05592', 'title': 'R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.05592', 'abstract': 'Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models~(LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledge-intensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. % effectively generalizing to out-of-domain datasets and supporting both Base and Instruct models. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini.', 'score': 12, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '6af1f8cd890c69ae', 'authors': ['Huatong Song', 'Jinhao Jiang', 'Yingqian Min', 'Jie Chen', 'Zhipeng Chen', 'Wayne Xin Zhao', 'Lei Fang', 'Ji-Rong Wen'], 'affiliations': ['DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05592.jpg', 'data': {'categories': ['#hallucinations', '#optimization', '#rl', '#rag', '#reasoning'], 'emoji': '🔍', 'ru': {'title': 'Усиление поисковых способностей ИИ через обучение с подкреплением', 'desc': 'R1-Searcher - это новый двухэтапный подход к обучению с подкреплением, улучшающий способности больших языковых моделей к поиску информации. Он позволяет моделям автономно обращаться к внешним поисковым системам во время рассуждений. Метод основан исключительно на обучении с подкреплением, без необходимости в процессных наградах или дистилляции. Эксперименты показывают, что R1-Searcher превосходит предыдущие методы RAG, даже в сравнении с закрытой моделью GPT-4o-mini.'}, 'en': {'title': 'Enhancing LLM Reasoning with External Knowledge Search', 'desc': 'This paper introduces R1-Searcher, a new approach that improves the reasoning abilities of Large Language Models (LLMs) using reinforcement learning (RL). Unlike existing models that depend solely on their internal knowledge, R1-Searcher enables LLMs to access external search systems for additional information, which helps in answering complex and time-sensitive questions more accurately. The method operates in two stages and does not require initial rewards or distillation, making it easier to implement. Experimental results show that R1-Searcher outperforms previous retrieval-augmented generation (RAG) methods, demonstrating its effectiveness across various datasets.'}, 'zh': {'title': '增强推理能力，R1-Searcher助力LLMs', 'desc': '现有的大型推理模型（LRMs）展示了强化学习（RL）在增强大型语言模型（LLMs）复杂推理能力方面的潜力。尽管它们在数学和编程等挑战性任务上表现出色，但在处理时间敏感或知识密集的问题时，往往依赖内部知识，导致不准确和幻觉现象。为了解决这个问题，我们提出了R1-Searcher，这是一种新颖的基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。该方法允许LLMs在推理过程中自主调用外部搜索系统，以获取额外知识，从而显著提高性能。'}}}, {'id': 'https://huggingface.co/papers/2503.05639', 'title': 'VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play\n  Context Control', 'url': 'https://huggingface.co/papers/2503.05639', 'abstract': "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.", 'score': 11, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '735dd34a3043623a', 'authors': ['Yuxuan Bian', 'Zhaoyang Zhang', 'Xuan Ju', 'Mingdeng Cao', 'Liangbin Xie', 'Ying Shan', 'Qiang Xu'], 'affiliations': ['Tencent ARC Lab, China', 'The Chinese University of Hong Kong, China', 'The University of Tokyo, Japan', 'University of Macau, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.05639.jpg', 'data': {'categories': ['#video', '#benchmark', '#dataset', '#games', '#architecture', '#optimization'], 'emoji': '🎥', 'ru': {'title': 'VideoPainter: Революция в восстановлении видео с помощью двухпоточной архитектуры', 'desc': 'Статья представляет новый подход к восстановлению поврежденного видеоконтента - VideoPainter. Модель использует двухпоточную архитектуру с эффективным кодировщиком контекста для обработки маскированных видео и внедрения контекстуальных подсказок в предобученную видео-модель DiT. Авторы также вводят технику ресемплинга ID целевой области, позволяющую выполнять инпейнтинг видео любой длины. Дополнительно создан масштабируемый конвейер данных VPData и бенчмарк VPBench для обучения и оценки сегментационного инпейнтинга.'}, 'en': {'title': 'Revolutionizing Video Inpainting with VideoPainter', 'desc': "This paper presents VideoPainter, a new approach to video inpainting that effectively restores missing video content. It introduces a dual-stream architecture that uses a lightweight context encoder to enhance background information while maintaining the integrity of foreground objects. The method also features a novel target region ID resampling technique, allowing for flexible inpainting of videos of any length. Additionally, the authors provide a comprehensive dataset and benchmark for training and evaluating video inpainting models, showcasing VideoPainter's strong performance across various metrics."}, 'zh': {'title': '视频修复的新纪元：VideoPainter', 'desc': '视频修复旨在恢复损坏的视频内容，近年来取得了显著进展。现有方法在生成完全遮挡的物体或平衡背景保留与前景生成方面面临挑战。为了解决这些问题，我们提出了一种新颖的双流架构VideoPainter，利用高效的上下文编码器处理遮挡视频，并将背景上下文信息注入到预训练的视频模型中。我们还引入了一种新的目标区域ID重采样技术，支持任意长度的视频修复，极大地提升了实际应用的可行性。'}}}, {'id': 'https://huggingface.co/papers/2503.05379', 'title': 'R1-Omni: Explainable Omni-Multimodal Emotion Recognition with\n  Reinforcing Learning', 'url': 'https://huggingface.co/papers/2503.05379', 'abstract': "In this work, we present the first application of Reinforcement Learning with Verifiable Reward (RLVR) to an Omni-multimodal large language model in the context of emotion recognition, a task where both visual and audio modalities play crucial roles. We leverage RLVR to optimize the Omni model, significantly enhancing its performance in three key aspects: reasoning capability, emotion recognition accuracy, and generalization ability. The introduction of RLVR not only improves the model's overall performance on in-distribution data but also demonstrates superior robustness when evaluated on out-of-distribution datasets. More importantly, the improved reasoning capability enables clear analysis of the contributions of different modalities, particularly visual and audio information, in the emotion recognition process. This provides valuable insights into the optimization of multimodal large language models.", 'score': 9, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '34c6afbd7ae83841', 'authors': ['Jiaxing Zhao', 'Xihan Wei', 'Liefeng Bo'], 'affiliations': ['Tongyi Lab, Alibaba Group'], 'pdf_title_img': 'assets/pdf/title_img/2503.05379.jpg', 'data': {'categories': ['#optimization', '#rl', '#multimodal', '#audio', '#cv', '#reasoning'], 'emoji': '🤖', 'ru': {'title': 'RLVR: Революция в мультимодальном распознавании эмоций', 'desc': 'В статье представлено первое применение обучения с подкреплением с проверяемым вознаграждением (RLVR) к мультимодальной большой языковой модели для распознавания эмоций. RLVR используется для оптимизации Omni-модели, значительно улучшая её способности к рассуждению, точность распознавания эмоций и способность к обобщению. Модель демонстрирует повышенную производительность на исходных данных и устойчивость на новых наборах данных. Улучшенная способность к рассуждениям позволяет анализировать вклад различных модальностей в процесс распознавания эмоций.'}, 'en': {'title': 'Enhancing Emotion Recognition with RLVR in Multimodal Models', 'desc': "This paper introduces a novel approach called Reinforcement Learning with Verifiable Reward (RLVR) applied to an Omni-multimodal large language model for emotion recognition. The use of RLVR enhances the model's reasoning skills, accuracy in recognizing emotions, and its ability to generalize across different datasets. The model not only performs better on familiar data but also shows increased robustness when tested on new, unseen data. Additionally, the improved reasoning capability allows for a detailed understanding of how visual and audio inputs contribute to the emotion recognition task."}, 'zh': {'title': '情感识别中的全模态强化学习新突破', 'desc': '本研究首次将可验证奖励的强化学习（RLVR）应用于情感识别的全模态大型语言模型中。在这个任务中，视觉和音频模态起着至关重要的作用。通过使用RLVR，我们显著提升了模型在推理能力、情感识别准确性和泛化能力等三个关键方面的表现。此外，RLVR的引入不仅提高了模型在同分布数据上的整体性能，还在异分布数据集上展现出更强的鲁棒性。'}}}, {'id': 'https://huggingface.co/papers/2503.05652', 'title': 'BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation\n  for Everyday Household Activities', 'url': 'https://huggingface.co/papers/2503.05652', 'abstract': "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/", 'score': 8, 'issue_id': 2608, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '52a7efb2f40f020a', 'authors': ['Yunfan Jiang', 'Ruohan Zhang', 'Josiah Wong', 'Chen Wang', 'Yanjie Ze', 'Hang Yin', 'Cem Gokmen', 'Shuran Song', 'Jiajun Wu', 'Li Fei-Fei'], 'affiliations': ['Stanford University'], 'pdf_title_img': 'assets/pdf/title_img/2503.05652.jpg', 'data': {'categories': ['#robotics', '#open_source', '#dataset', '#training'], 'emoji': '🤖', 'ru': {'title': 'Комплексная система для обучения роботов домашним задачам', 'desc': 'Статья представляет BEHAVIOR Robot Suite (BRS) - комплексную систему для манипуляции роботов в домашних условиях. BRS основан на двуруком колесном роботе с 4-осевым торсом и включает интерфейс телеуправления для сбора данных и новый алгоритм обучения визуомоторным политикам. Система оценивается на пяти сложных бытовых задачах, требующих бимануальной координации, точной навигации и широкой досягаемости манипуляторов. BRS представляет значительный шаг вперед в решении задач роботизированной манипуляции в реальных домашних условиях.'}, 'en': {'title': 'Empowering Robots for Everyday Household Tasks with BRS', 'desc': 'This paper presents the BEHAVIOR Robot Suite (BRS), a framework designed to enhance mobile manipulation robots for household tasks. It identifies three essential capabilities for effective task performance: bimanual coordination, stable navigation, and extensive reachability. The BRS integrates a teleoperation interface for data collection and a novel algorithm for learning visuomotor policies, addressing the complexities of hardware design and policy learning. The framework is evaluated on five challenging tasks that test these capabilities in real-world scenarios, aiming to improve robotic manipulation in everyday environments.'}, 'zh': {'title': '实现家庭任务的全身操控机器人', 'desc': '本论文介绍了BEHAVIOR机器人套件（BRS），旨在解决移动操作机器人在家庭任务中面临的挑战。研究表明，成功完成任务依赖于三项关键的全身控制能力：双手协调、稳定精确的导航和广泛的末端执行器可达性。BRS框架结合了一个双手轮式机器人和4自由度的躯干，提供了一种经济高效的全身遥操作接口用于数据收集，并提出了一种新算法用于学习全身视觉运动策略。通过在五个复杂的家庭任务上评估BRS，展示了其在长距离导航、与可动和可变形物体的交互以及在狭小空间中的操作能力。'}}}, {'id': 'https://huggingface.co/papers/2503.05638', 'title': 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos\n  via Diffusion Models', 'url': 'https://huggingface.co/papers/2503.05638', 'abstract': 'We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method.', 'score': 8, 'issue_id': 2612, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '33c7af8e9a7df61e', 'authors': ['Mark YU', 'Wenbo Hu', 'Jinbo Xing', 'Ying Shan'], 'affiliations': ['ARC Lab, Tencent PCG'], 'pdf_title_img': 'assets/pdf/title_img/2503.05638.jpg', 'data': {'categories': ['#dataset', '#3d', '#optimization', '#diffusion', '#video'], 'emoji': '🎥', 'ru': {'title': 'Управление траекторией камеры в видео с помощью искусственного интеллекта', 'desc': 'TrajectoryCrafter - это новый подход к перенаправлению траекторий камеры для монокулярных видео. Метод разделяет детерминированные преобразования вида и стохастическую генерацию контента, обеспечивая точный контроль над заданными пользователем траекториями камеры. Авторы предлагают двухпоточную условную модель диффузии видео, интегрирующую рендеры облака точек и исходные видео. Для обучения используется гибридный набор данных, сочетающий монокулярные видео и статические многоракурсные датасеты.'}, 'en': {'title': 'Mastering Camera Movement in Monocular Videos', 'desc': 'TrajectoryCrafter is a new method designed to change camera paths in single-camera videos. It separates the predictable changes in view from the random elements in the video content, allowing for better control over how the camera moves. The approach uses a dual-stream conditional video diffusion model that combines 3D point cloud images and original videos to ensure smooth transitions and realistic video generation. By creating a unique training dataset that merges large amounts of single-camera videos with some multi-camera data, the method shows improved performance across various scenes.'}, 'zh': {'title': '精准控制摄像机轨迹的创新方法', 'desc': '我们提出了一种名为TrajectoryCrafter的新方法，用于重定向单目视频的摄像机轨迹。通过将确定性的视图变换与随机内容生成分离，我们的方法能够精确控制用户指定的摄像机轨迹。我们提出了一种新颖的双流条件视频扩散模型，同时整合点云渲染和源视频作为条件，确保准确的视图变换和一致的4D内容生成。通过创新的双重重投影策略，我们结合了网络规模的单目视频和静态多视角数据集，显著提高了在不同场景中的鲁棒性和泛化能力。'}}}, {'id': 'https://huggingface.co/papers/2503.04824', 'title': 'ProReflow: Progressive Reflow with Decomposed Velocity', 'url': 'https://huggingface.co/papers/2503.04824', 'abstract': 'Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, flow matching aims to reflow the diffusion process of diffusion models into a straight line for a few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of flow matching is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05).', 'score': 8, 'issue_id': 2616, 'pub_date': '2025-03-05', 'pub_date_card': {'ru': '5 марта', 'en': 'March 5', 'zh': '3月5日'}, 'hash': '3380a5ba02714266', 'authors': ['Lei Ke', 'Haohang Xu', 'Xuefei Ning', 'Yu Li', 'Jiajun Li', 'Haoling Li', 'Yuxuan Lin', 'Dongsheng Jiang', 'Yujiu Yang', 'Linfeng Zhang'], 'affiliations': ['Huawei Inc.', 'Shanghai Jiao Tong University', 'Tsinghua University', 'University of Electronic Science and Technology of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04824.jpg', 'data': {'categories': ['#training', '#cv', '#diffusion', '#optimization'], 'emoji': '🌊', 'ru': {'title': 'Оптимизация потоков для быстрой генерации изображений', 'desc': "Статья представляет усовершенствованный метод генерации изображений и видео с использованием диффузионных моделей. Авторы предлагают технику 'progressive reflow', которая постепенно перестраивает диффузионный процесс, упрощая задачу согласования потоков. Также вводится концепция 'aligned v-prediction', подчеркивающая важность соответствия направлений в процессе согласования потоков. Экспериментальные результаты на моделях SDv1.5 и SDXL демонстрируют эффективность предложенного метода, достигая высокого качества генерации при значительно меньшем количестве шагов сэмплирования."}, 'en': {'title': 'Streamlining Diffusion: Faster Generation with Flow Matching Enhancements', 'desc': 'This paper addresses the high computational costs associated with diffusion models used for image and video generation. It proposes flow matching as a method to streamline the diffusion process, allowing for faster generation in fewer steps. The authors introduce two enhancements: progressive reflow, which simplifies the flow matching by gradually adjusting local timesteps, and aligned v-prediction, which emphasizes the importance of direction over magnitude in flow matching. Experimental results show that their approach significantly improves performance, achieving competitive results with fewer sampling steps compared to traditional methods.'}, 'zh': {'title': '优化扩散模型的流匹配训练', 'desc': '扩散模型在图像和视频生成方面取得了显著进展，但计算成本仍然很高。为了解决这个问题，流匹配技术将扩散过程重新调整为直线，以实现少步甚至一步的生成。本文提出了两种改进流匹配训练流程的技术：逐步重新流动和对齐的v预测。实验结果表明，我们的方法在生成质量上接近教师模型，同时大幅减少了采样步骤。'}}}, {'id': 'https://huggingface.co/papers/2503.04872', 'title': 'TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation', 'url': 'https://huggingface.co/papers/2503.04872', 'abstract': 'The challenge of reducing the size of Large Language Models (LLMs) while maintaining their performance has gained significant attention. However, existing methods, such as model distillation and transfer learning, often fail to achieve high accuracy. To address this limitation, we introduce the Branch-Merge distillation approach, which enhances model compression through two phases: (1) the Branch Phase, where knowledge from a large teacher model is selectively distilled into specialized student models via domain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where these student models are merged to enable cross-domain knowledge transfer and improve generalization. We validate our distillation approach using DeepSeek-R1 as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting merged model, TinyR1-32B-Preview, outperforms its counterpart DeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics (+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving near-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge distillation approach provides a scalable solution for creating smaller, high-performing LLMs with reduced computational cost and time.', 'score': 6, 'issue_id': 2609, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '94defd7f9d19776e', 'authors': ['Lin Sun', 'Guangxiang Zhao', 'Xiaoqi Jian', 'Yuhan Wu', 'Weihong Lin', 'Yongfu Zhu', 'Change Jia', 'Linglin Zhang', 'Jinzhu Wu', 'Junfeng Ran', 'Sai-er Hu', 'Zihan Jiang', 'Junting Zhou', 'Wenrui Liu', 'Bin Cui', 'Tong Yang', 'Xiangzheng Zhang'], 'affiliations': ['Peking University', 'Qiyuan Tech'], 'pdf_title_img': 'assets/pdf/title_img/2503.04872.jpg', 'data': {'categories': ['#small_models', '#training', '#transfer_learning', '#optimization'], 'emoji': '🌳', 'ru': {'title': 'Ветвление и слияние: новый путь к компактным и мощным языковым моделям', 'desc': 'Статья представляет новый подход к сжатию больших языковых моделей (LLM) под названием Branch-Merge distillation. Метод состоит из двух фаз: Branch, где знания из большой модели-учителя дистиллируются в специализированные модели-ученики, и Merge, где эти модели объединяются для улучшения обобщения. Эксперименты показали, что полученная модель TinyR1-32B-Preview превосходит аналоги по нескольким бенчмаркам. Этот подход предлагает масштабируемое решение для создания меньших, но эффективных LLM с пониженными вычислительными затратами.'}, 'en': {'title': 'Branch-Merge: Compressing LLMs Without Compromise!', 'desc': 'This paper presents a new method called Branch-Merge distillation to reduce the size of Large Language Models (LLMs) while keeping their performance high. It consists of two main phases: the Branch Phase, where knowledge from a large teacher model is distilled into smaller, specialized student models through supervised fine-tuning, and the Merge Phase, where these student models are combined to enhance knowledge transfer across different domains. The approach was tested using specific models and showed that the merged model, TinyR1-32B-Preview, outperformed the individual student model in various tasks, including Mathematics, Coding, and Science. Overall, this method offers an effective way to create smaller LLMs that maintain strong performance and are more efficient in terms of computational resources.'}, 'zh': {'title': '分支合并蒸馏：高效压缩大型语言模型的创新方法', 'desc': '本文提出了一种新的模型蒸馏方法，称为分支合并蒸馏，旨在在保持性能的同时减少大型语言模型的体积。该方法分为两个阶段：分支阶段通过领域特定的监督微调将知识从大型教师模型选择性地蒸馏到专门的学生模型中；合并阶段则将这些学生模型合并，以实现跨领域知识转移并提高模型的泛化能力。实验结果表明，合并后的模型TinyR1-32B-Preview在多个基准测试中表现优于其对应的学生模型DeepSeek-R1-Distill-Qwen-32B。该方法为创建更小且高性能的语言模型提供了一种可扩展的解决方案，降低了计算成本和时间。'}}}, {'id': 'https://huggingface.co/papers/2503.05447', 'title': 'Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts', 'url': 'https://huggingface.co/papers/2503.05447', 'abstract': 'Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.', 'score': 5, 'issue_id': 2614, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '3975a97b4236e791', 'authors': ['Weigao Sun', 'Disen Lan', 'Tong Zhu', 'Xiaoye Qu', 'Yu Cheng'], 'affiliations': ['Shanghai AI Laboratory', 'Soochow University', 'South China University of Technology', 'The Chinese University of Hong Kong'], 'pdf_title_img': 'assets/pdf/title_img/2503.05447.jpg', 'data': {'categories': ['#optimization', '#training', '#architecture'], 'emoji': '🧠', 'ru': {'title': 'Linear-MoE: Объединение линейного моделирования и смеси экспертов для эффективных крупномасштабных моделей', 'desc': 'Эта статья представляет Linear-MoE - систему для моделирования и обучения крупномасштабных моделей, объединяющую линейное моделирование последовательностей (LSM) с методом смеси экспертов (MoE). Linear-MoE использует преимущества LSM-модулей для линейного моделирования последовательностей и MoE-слоев для разреженной активации, стремясь обеспечить высокую производительность при эффективном обучении. Система включает подсистему моделирования, предоставляющую унифицированную структуру для всех экземпляров LSM, и подсистему обучения с различными технологиями параллелизма. Оценки на двух сериях моделей показывают, что Linear-MoE достигает повышения эффективности при сохранении конкурентоспособной производительности на различных тестах.'}, 'en': {'title': 'Efficient Modeling with Linear-MoE: Merging LSM and MoE for High Performance', 'desc': 'This paper presents Linear-MoE, a new system that combines Linear Sequence Modeling (LSM) with Mixture-of-Experts (MoE) to improve the efficiency and performance of large-scale models. Linear-MoE utilizes linear-complexity sequence modeling from LSM and the sparsity benefits of MoE layers, allowing for effective training and high performance. The system includes a modeling subsystem for LSM and a training subsystem that employs advanced parallelism techniques, particularly Sequence Parallelism. Experiments show that Linear-MoE achieves better efficiency while delivering competitive results on various benchmarks, indicating its promise as a foundational model architecture.'}, 'zh': {'title': '线性-MoE：高效的序列建模与训练新架构', 'desc': '线性序列建模（LSM）和专家混合模型（MoE）最近成为重要的架构改进。本文介绍了一种名为Linear-MoE的系统，它将LSM与MoE结合，用于建模和训练大规模模型。Linear-MoE利用LSM模块的线性复杂度序列建模优势和MoE层的稀疏激活特性，旨在提供高性能和高效训练。通过对A0.3B-2B和A1B-7B模型系列的评估，Linear-MoE在保持竞争性能的同时实现了效率提升，展示了其作为下一代基础模型架构的潜力。'}}}, {'id': 'https://huggingface.co/papers/2503.04548', 'title': 'An Empirical Study on Eliciting and Improving R1-like Reasoning Models', 'url': 'https://huggingface.co/papers/2503.04548', 'abstract': 'In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.', 'score': 5, 'issue_id': 2615, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': 'defc053b9079a4a2', 'authors': ['Zhipeng Chen', 'Yingqian Min', 'Beichen Zhang', 'Jie Chen', 'Jinhao Jiang', 'Daixuan Cheng', 'Wayne Xin Zhao', 'Zheng Liu', 'Xu Miao', 'Yang Lu', 'Lei Fang', 'Zhongyuan Wang', 'Ji-Rong Wen'], 'affiliations': ['BAAI', 'DataCanvas', 'Gaoling School of Artificial Intelligence, Renmin University of China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04548.jpg', 'data': {'categories': ['#reasoning', '#training', '#rl', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Усиление способностей языковых моделей к рассуждению через обучение с подкреплением', 'desc': 'В этом отчете представлены результаты экспериментов по улучшению моделей машинного обучения с помощью обучения с подкреплением (RL). Исследователи систематически изучали влияние различных факторов на RL-обучение как базовых, так и дообученных моделей. Было показано, что RL-обучение улучшает характеристики модели Qwen2.5-32B, увеличивая длину ответов и точность на тестах. Кроме того, использование инструментов значительно повысило способности моделей к рассуждению, достигнув точности 86.67% на наборе данных AIME 2024.'}, 'en': {'title': 'Enhancing Reasoning with Reinforcement Learning and Tool Manipulation', 'desc': 'This paper discusses advancements in slow-thinking models within the STILL project, focusing on reinforcement learning (RL) training techniques. The authors conduct systematic experiments to analyze how different factors affect RL training, leading to improvements in the Qwen2.5-32B base models. They demonstrate that even high-performing models can be further enhanced through RL training, achieving notable accuracy on benchmark tasks. Additionally, the study highlights the benefits of tool manipulation in improving reasoning performance, achieving impressive results on the AIME 2024 challenge.'}, 'zh': {'title': '强化学习提升推理模型的能力', 'desc': '本报告介绍了STILL项目中慢思维模型发展的第三个技术报告。我们系统地实验并记录了影响强化学习（RL）训练的各种因素，特别是在基础模型和微调模型上的实验。我们的研究表明，RL训练方法能够显著提高Qwen2.5-32B基础模型的响应长度和测试准确性。此外，我们还发现工具操作的使用显著提升了大型推理模型的推理性能，达到了86.67%的准确率。'}}}, {'id': 'https://huggingface.co/papers/2503.04359', 'title': 'LONGCODEU: Benchmarking Long-Context Language Models on Long Code\n  Understanding', 'url': 'https://huggingface.co/papers/2503.04359', 'abstract': "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", 'score': 5, 'issue_id': 2617, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '808bf0135ca11113', 'authors': ['Jia Li', 'Xuyuan Guo', 'Lei Li', 'Kechi Zhang', 'Ge Li', 'Jia Li', 'Zhengwei Tao', 'Fang Liu', 'Chongyang Tao', 'Yuqi Zhu', 'Zhi Jin'], 'affiliations': ['Key Lab of High Confidence Software Technology (Peking University), MoE, School of Computer Science, Peking University, China'], 'pdf_title_img': 'assets/pdf/title_img/2503.04359.jpg', 'data': {'categories': ['#dataset', '#long_context', '#benchmark', '#optimization'], 'emoji': '📏', 'ru': {'title': 'Новый бенчмарк раскрывает ограничения языковых моделей в понимании длинного кода', 'desc': 'Статья представляет новый бенчмарк LONGCODEU для оценки способности языковых моделей с длинным контекстом (LCLM) понимать длинный программный код. Бенчмарк включает 8 задач в 4 аспектах: восприятие кодовых единиц, понимание внутри кодовых единиц, понимание связей между кодовыми единицами и понимание документации длинного кода. Результаты оценки 9 популярных LCLM показали, что их производительность значительно падает при длине кода более 32K токенов. Наиболее сложным аспектом для моделей оказалось понимание связей между кодовыми единицами.'}, 'en': {'title': 'Bridging the Gap in Long Code Understanding for LCLMs', 'desc': 'This paper addresses the challenges faced by advanced long-context language models (LCLMs) in understanding long code, which is crucial for software engineering. It introduces a new benchmark called LONGCODEU, designed to evaluate LCLMs across eight tasks that cover various aspects of long code comprehension. The study evaluates nine popular LCLMs and finds significant performance drops when handling code longer than 32K tokens, indicating limitations in their capabilities. The findings highlight that understanding relationships between code units is particularly difficult for these models, providing insights for future improvements in LCLMs.'}, 'zh': {'title': '提升长代码理解能力的关键评估基准', 'desc': '当前先进的长文本语言模型在软件工程应用中具有巨大潜力。然而，缺乏严格的评估框架限制了这一领域的进展。为了解决这个问题，我们提出了一个名为LONGCODEU的长代码理解基准，涵盖了四个方面的八个任务，以评估长代码理解能力。我们的实验结果显示，当前的长代码语言模型在处理超过32K的长代码时性能显著下降，尤其是在理解代码单元之间的关系方面最具挑战性。'}}}, {'id': 'https://huggingface.co/papers/2503.04808', 'title': 'Learning from Failures in Multi-Attempt Reinforcement Learning', 'url': 'https://huggingface.co/papers/2503.04808', 'abstract': "Recent advancements in reinforcement learning (RL) for large language models (LLMs), exemplified by DeepSeek R1, have shown that even a simple question-answering task can substantially improve an LLM's reasoning capabilities. In this work, we extend this approach by modifying the task into a multi-attempt setting. Instead of generating a single response per question, the model is given multiple attempts, with feedback provided after incorrect responses. The multi-attempt task encourages the model to refine its previous attempts and improve search efficiency. Experimental results show that even a small LLM trained on a multi-attempt task achieves significantly higher accuracy when evaluated with more attempts, improving from 45.6% with 1 attempt to 52.5% with 2 attempts on the math benchmark. In contrast, the same LLM trained on a standard single-turn task exhibits only a marginal improvement, increasing from 42.3% to 43.2% when given more attempts during evaluation. The results indicate that, compared to the standard single-turn task, an LLM trained on a multi-attempt task achieves slightly better performance on math benchmarks while also learning to refine its responses more effectively based on user feedback. Full code is available at https://github.com/DualityRL/multi-attempt", 'score': 5, 'issue_id': 2609, 'pub_date': '2025-03-04', 'pub_date_card': {'ru': '4 марта', 'en': 'March 4', 'zh': '3月4日'}, 'hash': 'fb2db794d0ea3c11', 'authors': ['Stephen Chung', 'Wenyu Du', 'Jie Fu'], 'affiliations': ['DualityRL', 'Shanghai AI Lab'], 'pdf_title_img': 'assets/pdf/title_img/2503.04808.jpg', 'data': {'categories': ['#optimization', '#rl', '#math', '#training', '#rlhf', '#reasoning'], 'emoji': '🔁', 'ru': {'title': 'Многопопыточное обучение: путь к более эффективным языковым моделям', 'desc': 'Это исследование расширяет подход обучения с подкреплением для больших языковых моделей, внедряя многопопыточную задачу вместо стандартной однопопыточной. Модель получает несколько попыток ответить на вопрос, получая обратную связь после неверных ответов, что способствует улучшению рассуждений и эффективности поиска. Эксперименты показывают, что даже небольшая языковая модель, обученная на многопопыточной задаче, достигает значительно более высокой точности при оценке с большим количеством попыток. Результаты демонстрируют, что модель, обученная на многопопыточной задаче, не только показывает лучшие результаты на математических тестах, но и эффективнее улучшает свои ответы на основе обратной связи пользователя.'}, 'en': {'title': 'Enhancing LLMs with Multi-Attempt Learning', 'desc': "This paper explores how modifying reinforcement learning tasks can enhance the reasoning abilities of large language models (LLMs). By implementing a multi-attempt question-answering framework, the model receives feedback on incorrect answers, allowing it to improve its responses iteratively. Experimental results demonstrate that even a small LLM can achieve better accuracy on math benchmarks when trained with this multi-attempt approach, compared to traditional single-turn tasks. The findings suggest that providing multiple attempts and feedback significantly aids in refining the model's performance."}, 'zh': {'title': '多次尝试，提升推理能力！', 'desc': '本研究探讨了在大型语言模型（LLM）中应用强化学习（RL）的新方法，特别是通过多次尝试的任务设置来提升模型的推理能力。与传统的单次回答不同，模型在每个问题上可以进行多次尝试，并在错误回答后获得反馈。这种多次尝试的任务设置促使模型改进之前的回答，从而提高搜索效率。实验结果表明，即使是小型LLM，在多次尝试的任务训练下，其准确率显著提高，显示出多次尝试对模型学习和反馈的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.01713', 'title': 'SAGE: A Framework of Precise Retrieval for RAG', 'url': 'https://huggingface.co/papers/2503.01713', 'abstract': 'Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.   In this paper, we introduce a RAG framework (SAGE), to overcome these limitations. First, to address the segmentation issue without considering semantics, we propose to train a semantic segmentation model. This model is trained to segment the corpus into semantically complete chunks. Second, to ensure that only the most relevant chunks are retrieved while the irrelevant ones are ignored, we design a chunk selection algorithm to dynamically select chunks based on the decreasing speed of the relevance score, leading to a more relevant selection. Third, to further ensure the precision of the retrieved chunks, we propose letting LLMs assess whether retrieved chunks are excessive or lacking and then adjust the amount of context accordingly. Experiments show that SAGE outperforms baselines by 61.25% in the quality of QA on average. Moreover, by avoiding retrieving noisy context, SAGE lowers the cost of the tokens consumed in LLM inference and achieves a 49.41% enhancement in cost efficiency on average. Additionally, our work offers valuable insights for boosting RAG.', 'score': 4, 'issue_id': 2613, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'dc1c8022a96cab3e', 'authors': ['Jintao Zhang', 'Guoliang Li', 'Jinyang Su'], 'affiliations': ['Department of Computer Science Tsinghua University'], 'pdf_title_img': 'assets/pdf/title_img/2503.01713.jpg', 'data': {'categories': ['#rag', '#optimization', '#reasoning'], 'emoji': '🧠', 'ru': {'title': 'SAGE: Семантически улучшенный RAG для точных ответов на вопросы', 'desc': 'Статья представляет новый фреймворк SAGE для улучшения retrieval-augmented generation (RAG) в задачах вопросно-ответных систем. Авторы предлагают модель семантической сегментации корпуса текстов, алгоритм динамического выбора наиболее релевантных фрагментов и механизм оценки достаточности контекста с помощью языковых моделей. Эксперименты показывают, что SAGE превосходит базовые методы на 61.25% по качеству ответов и на 49.41% по эффективности использования токенов. Предложенный подход позволяет преодолеть ограничения существующих методов RAG и повысить точность извлечения релевантной информации.'}, 'en': {'title': 'SAGE: Smarter Retrieval for Better Question-Answering', 'desc': 'This paper presents a new framework called SAGE to improve retrieval-augmented generation (RAG) for question-answering tasks. It addresses two main issues: the ineffective segmentation of the corpus that ignores semantics and the trade-off between retrieving too little or too much context. SAGE introduces a semantic segmentation model to create meaningful chunks and a dynamic chunk selection algorithm to ensure only the most relevant information is retrieved. The results show that SAGE significantly enhances QA quality and cost efficiency compared to existing methods.'}, 'zh': {'title': '提升问答质量的智能检索框架', 'desc': '本文提出了一种新的检索增强生成框架（SAGE），旨在解决现有RAG方法在问答任务中的局限性。首先，SAGE通过训练语义分割模型，将语料库分割成语义完整的块，以提高相关性。其次，设计了一种动态选择算法，根据相关性得分的下降速度选择最相关的块，从而避免无关信息的干扰。实验结果表明，SAGE在问答质量上比基线提高了61.25%，并且在成本效率上提升了49.41%。'}}}, {'id': 'https://huggingface.co/papers/2503.05132', 'title': 'R1-Zero\'s "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model', 'url': 'https://huggingface.co/papers/2503.05132', 'abstract': 'Recently DeepSeek R1 demonstrated how reinforcement learning with simple rule-based incentives can enable autonomous development of complex reasoning in large language models, characterized by the "aha moment", in which the model manifest self-reflection and increased response length during training. However, attempts to extend this success to multimodal reasoning often failed to reproduce these key characteristics. In this report, we present the first successful replication of these emergent characteristics for multimodal reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset, our model achieves 59.47% accuracy on CVBench, outperforming the base model by approximately ~30% and exceeding both SFT setting by ~2%. In addition, we share our failed attempts and insights in attempting to achieve R1-like reasoning using RL with instruct models. aiming to shed light on the challenges involved. Our key observations include: (1) applying RL on instruct model often results in trivial reasoning trajectories, and (2) naive length reward are ineffective in eliciting reasoning capabilities. The project code is available at https://github.com/turningpoint-ai/VisualThinker-R1-Zero', 'score': 3, 'issue_id': 2609, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '760e01cf2a414aeb', 'authors': ['Hengguang Zhou', 'Xirui Li', 'Ruochen Wang', 'Minhao Cheng', 'Tianyi Zhou', 'Cho-Jui Hsieh'], 'affiliations': ['Pennsylvania State University', 'University of California, LA', 'University of Maryland'], 'pdf_title_img': 'assets/pdf/title_img/2503.05132.jpg', 'data': {'categories': ['#rl', '#multimodal', '#training', '#reasoning', '#optimization'], 'emoji': '🧠', 'ru': {'title': 'Новые горизонты мультимодального обучения', 'desc': 'Исследователи из DeepSeek R1 показали, как обучение с подкреплением может помочь LLM развивать сложные навыки рассуждения, включая моменты "эврики". Однако при попытках применить эти методы к мультимодальному обучению часто не удавалось достичь таких же результатов. В этом исследовании удалось впервые воспроизвести эти характеристики на мультимодальной модели без использования SFT, достигнув высокой точности. Авторы также делятся неудачными попытками и выводами, чтобы лучше понять возникающие трудности.'}, 'en': {'title': 'Unlocking Multimodal Reasoning with Reinforcement Learning', 'desc': "This paper discusses the application of reinforcement learning (RL) to enhance multimodal reasoning in large language models, specifically using the Qwen2-VL-2B model. The authors successfully replicated the 'aha moment' phenomenon, where the model demonstrates self-reflection and improved response length during training, achieving a notable accuracy of 59.47% on the CVBench dataset. They also share insights from their unsuccessful attempts to replicate similar reasoning capabilities using instruct models, highlighting challenges such as trivial reasoning paths and ineffective reward structures. The findings suggest that while RL can significantly improve reasoning in multimodal contexts, careful consideration of reward mechanisms is crucial for success."}, 'zh': {'title': '强化学习助力多模态推理的突破', 'desc': '最近，DeepSeek R1展示了如何通过简单的基于规则的激励来实现强化学习，使大型语言模型能够自主发展复杂的推理能力。这种能力在训练过程中表现为“恍然大悟”的时刻，模型会自我反思并增加响应长度。然而，尝试将这种成功扩展到多模态推理时，往往无法重现这些关键特征。在本报告中，我们首次成功复制了这些特征，并在非SFT的2B模型上实现了多模态推理的进展。'}}}, {'id': 'https://huggingface.co/papers/2503.05315', 'title': 'LoRACode: LoRA Adapters for Code Embeddings', 'url': 'https://huggingface.co/papers/2503.05315', 'abstract': 'Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations.', 'score': 2, 'issue_id': 2610, 'pub_date': '2025-03-07', 'pub_date_card': {'ru': '7 марта', 'en': 'March 7', 'zh': '3月7日'}, 'hash': '95dca112be949ba8', 'authors': ['Saumya Chaturvedi', 'Aman Chadha', 'Laurent Bindschaedler'], 'affiliations': ['AWS GenAI Santa Clara, CA, USA', 'Max Planck Institute for Software Systems Saarbrucken, Germany'], 'pdf_title_img': 'assets/pdf/title_img/2503.05315.jpg', 'data': {'categories': ['#data', '#open_source', '#training', '#optimization', '#plp'], 'emoji': '🔍', 'ru': {'title': 'LoRA: Эффективная тонкая настройка для точного поиска кода', 'desc': 'Статья представляет новый метод тонкой настройки для семантического поиска кода, основанный на Low-Rank Adaptation (LoRA). Этот подход значительно сокращает количество обучаемых параметров, позволяя быстро настраивать модели на больших объемах кода. Эксперименты показывают существенное улучшение показателей MRR для задач поиска Code2Code и Text2Code. Метод позволяет исследовать чувствительность поиска кода к синтаксическим и языковым вариациям.'}, 'en': {'title': 'Efficient Code Retrieval with Low-Rank Adaptation', 'desc': 'This paper addresses the challenges of semantic code search by improving how code embeddings are generated. It highlights the limitations of existing models like CodeBERT and UniXcoder in terms of scalability and efficiency. The authors propose a new method using Low-Rank Adaptation (LoRA) to create task-specific adapters, significantly reducing the number of trainable parameters. Their approach allows for quick fine-tuning on large code datasets, resulting in notable improvements in retrieval performance across various programming languages.'}, 'zh': {'title': '高效代码检索的低秩适应方法', 'desc': '本文提出了一种基于低秩适应（LoRA）的参数高效微调方法，用于构建特定任务的代码检索适配器。该方法将可训练参数减少到基础模型的不到2%，使得在大规模代码语料库上进行快速微调成为可能。实验结果显示，在代码到代码的检索任务中，平均倒数排名（MRR）提高了9.1%，而文本到代码的检索任务提高了86.69%。通过任务和语言的适应性区分，本文探讨了代码检索对语法和语言变体的敏感性。'}}}, {'id': 'https://huggingface.co/papers/2502.18968', 'title': 'Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles', 'url': 'https://huggingface.co/papers/2502.18968', 'abstract': 'User simulators are crucial for replicating human interactions with dialogue systems, supporting both collaborative training and automatic evaluation, especially for large language models (LLMs). However, existing simulators often rely solely on text utterances, missing implicit user traits such as personality, speaking style, and goals. In contrast, persona-based methods lack generalizability, as they depend on predefined profiles of famous individuals or archetypes. To address these challenges, we propose User Simulator with implicit Profiles (USP), a framework that infers implicit user profiles from human-machine conversations and uses them to generate more personalized and realistic dialogues. We first develop an LLM-driven extractor with a comprehensive profile schema. Then, we refine the simulation through conditional supervised fine-tuning and reinforcement learning with cycle consistency, optimizing it at both the utterance and conversation levels. Finally, we adopt a diverse profile sampler to capture the distribution of real-world user profiles. Experimental results demonstrate that USP outperforms strong baselines in terms of authenticity and diversity while achieving comparable performance in consistency. Furthermore, dynamic multi-turn evaluations based on USP strongly align with mainstream benchmarks, demonstrating its effectiveness in real-world applications.', 'score': 2, 'issue_id': 2619, 'pub_date': '2025-02-26', 'pub_date_card': {'ru': '26 февраля', 'en': 'February 26', 'zh': '2月26日'}, 'hash': '32f11f6f4b295fc5', 'authors': ['Kuang Wang', 'Xianfei Li', 'Shenghao Yang', 'Li Zhou', 'Feng Jiang', 'Haizhou Li'], 'affiliations': ['Shenzhen Research Institute of Big Data', 'Shenzhen University of Advanced Technology', 'The Chinese University of Hong Kong, Shenzhen'], 'pdf_title_img': 'assets/pdf/title_img/2502.18968.jpg', 'data': {'categories': ['#rl', '#benchmark', '#training', '#optimization', '#games', '#agents', '#interpretability'], 'emoji': '🎭', 'ru': {'title': 'Реалистичная симуляция пользователей на основе неявных профилей', 'desc': 'Статья представляет новый подход к симуляции пользователей для диалоговых систем - User Simulator with implicit Profiles (USP). USP извлекает неявные профили пользователей из диалогов человека с машиной и использует их для генерации более персонализированных и реалистичных диалогов. Метод включает экстрактор на основе большой языковой модели, условное обучение с учителем и обучение с подкреплением. Эксперименты показывают превосходство USP над базовыми методами по аутентичности и разнообразию генерируемых диалогов.'}, 'en': {'title': 'Enhancing Dialogue Systems with Implicit User Profiles', 'desc': 'This paper introduces the User Simulator with implicit Profiles (USP), a novel framework designed to enhance dialogue systems by incorporating implicit user traits like personality and goals. Unlike traditional simulators that focus only on text, USP infers user profiles from actual human-machine interactions, allowing for more personalized dialogue generation. The framework employs a large language model (LLM) to extract user profiles and utilizes conditional supervised fine-tuning and reinforcement learning to improve dialogue quality. Experimental results show that USP significantly improves the authenticity and diversity of generated dialogues while maintaining consistency, making it effective for real-world applications.'}, 'zh': {'title': '隐式用户档案，提升对话真实感', 'desc': '本文提出了一种用户模拟器（USP），旨在通过隐式用户特征生成更个性化和真实的对话。现有的模拟器通常只依赖文本，忽视了用户的个性、说话风格和目标等隐性特征。USP通过从人机对话中推断隐式用户档案，结合大语言模型（LLM）和强化学习，优化对话生成过程。实验结果表明，USP在真实性和多样性方面优于现有方法，同时在一致性上表现相当，显示出其在实际应用中的有效性。'}}}, {'id': 'https://huggingface.co/papers/2503.04504', 'title': 'AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM', 'url': 'https://huggingface.co/papers/2503.04504', 'abstract': 'Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive performance on VAD benchmark datasets, achieving state-of-the-art results on the UBnormal dataset and outperforming other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.', 'score': 1, 'issue_id': 2613, 'pub_date': '2025-03-06', 'pub_date_card': {'ru': '6 марта', 'en': 'March 6', 'zh': '3月6日'}, 'hash': '7dbd20628d3eb105', 'authors': ['Sunghyun Ahn', 'Youngwan Jo', 'Kijung Lee', 'Sein Kwon', 'Inpyo Hong', 'Sanghyun Park'], 'affiliations': ['Yonsei University, Seoul, Korea'], 'pdf_title_img': 'assets/pdf/title_img/2503.04504.jpg', 'data': {'categories': ['#open_source', '#optimization', '#dataset', '#benchmark', '#cv', '#video'], 'emoji': '🕵️', 'ru': {'title': 'Универсальное обнаружение аномалий в видео без переобучения', 'desc': 'Исследователи представили новый подход к обнаружению аномалий в видео, названный C-VAD. Они разработали модель AnyAnomaly, которая может обнаруживать аномальные события, определенные пользователем, без необходимости переобучения для новых сред. Модель использует контекстно-зависимое визуальное понимание вопросов и ответов на основе большой мультимодальной языковой модели. Эксперименты показали превосходство AnyAnomaly на специально созданных наборах данных C-VAD и конкурентоспособные результаты на стандартных бенчмарках обнаружения видеоаномалий.'}, 'en': {'title': 'Customizable Anomaly Detection for Diverse Video Environments', 'desc': 'This paper introduces a new approach to video anomaly detection (VAD) called customizable video anomaly detection (C-VAD). Unlike traditional VAD models that require retraining for different environments, C-VAD allows users to define what constitutes an abnormal event using text input. The AnyAnomaly model leverages a context-aware visual question answering system, enabling it to detect specified events without the need for extensive fine-tuning. The results show that AnyAnomaly not only performs well on custom datasets but also achieves state-of-the-art results on established VAD benchmarks, demonstrating its versatility and effectiveness.'}, 'zh': {'title': '可定制的视频异常检测，轻松应对多样环境', 'desc': '视频异常检测（VAD）在计算机视觉中的视频分析和监控中至关重要。现有的VAD模型依赖于学习到的正常模式，这使得它们在不同环境中的应用变得困难。为了解决这些问题，本研究提出了可定制的视频异常检测（C-VAD）技术和AnyAnomaly模型，允许用户定义异常事件并检测视频中的相关帧。我们的模型在多个基准数据集上表现出色，尤其在UBnormal数据集上达到了最先进的结果，展示了其在泛化能力上的优势。'}}}, {'id': 'https://huggingface.co/papers/2503.01840', 'title': 'EAGLE-3: Scaling up Inference Acceleration of Large Language Models via\n  Training-Time Test', 'url': 'https://huggingface.co/papers/2503.01840', 'abstract': "The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.", 'score': 1, 'issue_id': 2619, 'pub_date': '2025-03-03', 'pub_date_card': {'ru': '3 марта', 'en': 'March 3', 'zh': '3月3日'}, 'hash': 'cf0ee90637c3e71a', 'authors': ['Yuhui Li', 'Fangyun Wei', 'Chao Zhang', 'Hongyang Zhang'], 'affiliations': ['Microsoft Research', 'Peking University', 'University of Waterloo', 'Vector Institute'], 'pdf_title_img': 'assets/pdf/title_img/2503.01840.jpg', 'data': {'categories': ['#training', '#inference', '#optimization', '#reasoning'], 'emoji': '🚀', 'ru': {'title': 'EAGLE-3: Революционное ускорение LLM без потери качества', 'desc': 'Статья представляет EAGLE-3, новый метод спекулятивной выборки для ускорения работы больших языковых моделей (LLM). В отличие от предыдущих версий, EAGLE-3 использует прямое предсказание токенов и многослойное слияние признаков, что позволяет полностью использовать преимущества увеличения объема обучающих данных. Эксперименты показывают, что EAGLE-3 достигает ускорения до 6,5 раз по сравнению с обычной авторегрессией, что примерно в 1,4 раза лучше, чем EAGLE-2. Метод был протестирован на различных задачах с использованием как диалоговых, так и рассуждающих моделей.'}, 'en': {'title': 'EAGLE-3: Speeding Up LLMs with Direct Token Prediction', 'desc': 'This paper presents EAGLE-3, an advanced model that improves the efficiency of large language models (LLMs) by shifting from feature prediction to direct token prediction. By utilizing multi-layer feature fusion instead of relying solely on top-layer features, EAGLE-3 enhances performance and allows for better utilization of larger training datasets. The authors demonstrate that EAGLE-3 achieves significant speed improvements, with a speedup ratio of up to 6.5 times compared to previous methods. The results indicate that EAGLE-3 not only accelerates inference but also improves model intelligence, making it a valuable advancement in the field of machine learning.'}, 'zh': {'title': 'EAGLE-3：提升大语言模型推理速度的创新方案', 'desc': '现代大语言模型（LLM）的顺序特性使其在推理时成本高且速度慢，推测采样是一种有效的解决方案。EAGLE方法在特征层面进行自回归，利用目标模型的顶层特征，取得比传统推测采样更好的效果。尽管在LLM社区中，扩大训练数据以提高模型智能的趋势日益增长，但我们发现对于EAGLE来说，扩大数据的效果有限。本文提出了EAGLE-3，放弃特征预测，采用直接的标记预测，并通过训练时测试的技术实现多层特征融合，从而显著提升性能。'}}}];
        const articlesContainer = document.getElementById('articles-container');
        const sortDropdown = document.getElementById('sort-dropdown');
        const categoryFiltersContainer = document.getElementById('category-filters');
        const categoryFiltersLogicOptions = document.getElementById('category-options');
        const categoryToggle = document.getElementById('category-toggle');
        const clearCategoriesButton = document.getElementById('clear-categories');
        let selectedCategories = [];
        let selectedArticles = [];
        let sortBy = 'issue_id';     
        let showLimitHint = false; 
        let filterLogicIsAnd = false;

        function getUrlParameters() {
            const urlParams = new URLSearchParams(window.location.search);
            const categoriesParam = urlParams.get('cat');
            let categories = categoriesParam ? categoriesParam.split(',') : [];
            categories = categories.map(element => `#${element}`);
            return categories
        }

        function updateUrlWithCategories() {
            let cleanedCategories = selectedCategories.map(element => element.replace(/^#/, ''));
            const newUrl = cleanedCategories.length > 0 
                ? `${window.location.pathname}?cat=${cleanedCategories.join(',')}`
                : window.location.pathname;
            console.log("cleanedCategories", cleanedCategories)
            window.history.pushState({}, '', newUrl);
        }

        function loadSettings() {
            const themeToggle = document.getElementById('theme-toggle');
            const sortDropdown = document.getElementById('sort-dropdown');

            const isDarkMode = localStorage.getItem('darkMode') === 'true';
            let settingSortBy = localStorage.getItem('sort_by');
            filterLogicIsAnd = localStorage.getItem('filter_logic_is_and') === 'true';
            
            if (isDarkMode) {
                document.body.classList.remove('light-theme');
                document.body.classList.add('dark-theme');
                themeToggle.checked = true;
                const title = document.getElementById('doomgrad');
                title.innerHTML = "hf nightly";
                const titleSign = document.getElementById('doomgrad-icon');
                titleSign.classList.add('rotate');
            }

            if ((!settingSortBy) || (settingSortBy === 'null')) {
                settingSortBy = 'issue_id';
            }

            if (filterLogicIsAnd) {
                document.getElementById('filter-logic-and').checked = true;
            } else {
                document.getElementById('filter-logic-or').checked = true;
            }

            sortDropdown.value = settingSortBy;
            sortBy = settingSortBy;
        }

        document.getElementById('theme-toggle').addEventListener('change', toggleTheme);
        document.getElementById('filter-logic-and').addEventListener('change', () => {
            filterLogicIsAnd = true;
            localStorage.setItem('filter_logic_is_and', 'true');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });
        document.getElementById('filter-logic-or').addEventListener('change', () => {
            filterLogicIsAnd = false;
            localStorage.setItem('filter_logic_is_and', 'false');
            filterAndRenderArticles();
            updateSelectedArticlesTitle();
        });

        function getUniqueCategories(articles) {
            const categories = new Set();
            articles.forEach(article => {
                if (article.data && article.data.categories) {
                    article.data.categories.forEach(cat => categories.add(cat));
                }
            });
            let res = Array.from(categories);
            res.sort();
            return res;
        }

        function createCategoryButtons() {
            //const categories = getUniqueCategories(articlesData);
            const categories = ['#3d (1)', '#agents (1)', '#agi', '#alignment (1)', '#architecture (4)', '#audio (2)', '#benchmark (6)', '#cv (4)', '#data (1)', '#dataset (9)', '#diffusion (2)', '#ethics', '#games (2)', '#graphs', '#hallucinations (1)', '#healthcare (1)', '#inference (1)', '#interpretability (1)', '#leakage', '#long_context (3)', '#low_resource (1)', '#machine_translation', '#math (2)', '#multilingual (3)', '#multimodal (4)', '#open_source (5)', '#optimization (18)', '#plp (1)', '#rag (3)', '#reasoning (8)', '#rl (6)', '#rlhf (2)', '#robotics (1)', '#science (1)', '#security', '#small_models (1)', '#story_generation', '#survey', '#synthetic', '#training (14)', '#transfer_learning (2)', '#video (4)'];

            categories.forEach(category => {
                let catNameSplitted = category.split(/(\s+)/);
                let catName = catNameSplitted[0];
                const button = document.createElement('span');
                button.textContent = catName;
                button.className = 'category-button';
                if (catNameSplitted.length < 2) {
                    button.classList.add('inactive');
                };
                button.onclick = () => toggleCategory(catName, button);
                categoryFiltersContainer.appendChild(button);
            });
        }

        function toggleCategory(category, button) {
            const index = selectedCategories.indexOf(category);
            if (index === -1) {
                selectedCategories.push(category);
                button.classList.add('active');
            } else {
                selectedCategories.splice(index, 1);
                button.classList.remove('active');
            }         
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
            setFilterOptionsVisibility();
        }

        function saveCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify(selectedCategories));
        }

        function updateSelectedArticlesTitle() {
            if ((selectedArticles.length === articlesData.length) & (selectedCategories.length === 0)) {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]}`;
            } else {
                categoryToggle.textContent = `🏷️ ${filterLabel[currentLang]} (${formatArticlesTitle(selectedArticles.length, currentLang)})`;
            }
        }

        function cleanCategorySelection() {
            localStorage.setItem('selectedCategories', JSON.stringify('[]'));
        }

        function loadCategorySelection() {
            const urlCategories = getUrlParameters();
            if (urlCategories.length > 0) {
                selectedCategories = urlCategories;
                saveCategorySelection();
            } else {
                const savedCategories = localStorage.getItem('selectedCategories');
                if (savedCategories && savedCategories !== '"[]"') {
                    selectedCategories = JSON.parse(savedCategories);                    
                }
            }
            updateCategoryButtonStates();
        }

        function updateCategoryButtonStates() {
            const buttons = categoryFiltersContainer.getElementsByClassName('category-button');
            Array.from(buttons).forEach(button => {
                if (selectedCategories.includes(button.textContent)) {
                    button.classList.add('active');
                } else {
                    button.classList.remove('active');
                }
            });
        }

        function filterAndRenderArticles() {
            console.log(selectedCategories);
            let filteredArticles; 

            if (filterLogicIsAnd) {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        selectedCategories.every(cat => article.data.categories.includes(cat))
                );
            } else {
                filteredArticles = selectedCategories.length === 0
                    ? articlesData
                    : articlesData.filter(article => 
                        article.data && article.data.categories && 
                        article.data.categories.some(cat => selectedCategories.includes(cat))
                    );            
            }

            console.log('filteredArticles', filteredArticles)

            selectedArticles = filteredArticles;
            sortArticles(selectedArticles);
        }

        function clearAllCategories() {
            selectedCategories = [];
            updateCategoryButtonStates();
            filterAndRenderArticles();
            saveCategorySelection();
            updateSelectedArticlesTitle();
            updateUrlWithCategories();
        }

        function renderArticles(articles) {
            if (articles.length > 50) {
                articles = articles.slice(0, 50);
                showLimitHint = true;
            } else {
                showLimitHint = false;
            }
            console.log(articles);
            articlesContainer.innerHTML = '';
            articles.forEach((item, index) => {
                if ("error" in item) {
                    console.log(`Omitting JSON. ${item["raw_data"]}`);
                    return;
                }
                
                let explanation = item["data"][currentLang]["desc"];
                let title = item["data"][currentLang]["title"];

                const cats = item["data"]["categories"].slice(0, 5).join(" ");
                
                let affiliations = ""
                if ('affiliations' in item) {
                    affiliations = item["affiliations"].slice(0, 10).join(", ");
                }

                let pdfImg = "https://hfday.ru/img/title_stub.png"
                if ('pdf_title_img' in item) {
                    pdfImg = 'https://hfday.ru/' + item['pdf_title_img']
                    
                }                

                const articleHTML = `
                    <article class='x${item["hash"]}'>
                        <div class="article-content" onclick="toggleAbstract(${index})">
                            <div class="background-digit">${index + 1}</div>
                            <div class="article-title-cont">
                                <div style="display:table-cell; vertical-align: middle;">
                                    <div class="article-title"><h2>${item['data']['emoji']} ${title}</h2></div>
                                </div>
                            </div>
                            <p class="meta">
                            🔺 ${item['score']}. ${item['title']}</p>
                            <p class="pub-date">${publishedLabel[currentLang]}${item['pub_date_card'][currentLang]}</p>
                            
                            <div class="article-pdf-title-img-cont"><img class="article-pdf-title-img" src="${pdfImg}"/></div>
                            
                            <div id="abstract-${index}" class="abstract">
                                <p>${explanation}</p>
                                <div id="toggle-${index}" class="abstract-toggle">...</div>
                            </div>

                            

                            <div class="links">
                                <a href="${item['url']}" target="_blank">${paperLabel[currentLang]}</a>
                            </div>

                            <div class="affiliations">${affiliations}</div>

                            <div class="tags">${cats}</div>
                        </div>
                    </article>
                `;
                articlesContainer.innerHTML += articleHTML;
            });
        }
        
        function sortArticles() {
            let sortedArticles = [...selectedArticles];
            if (sortBy === 'issue_id') {
                sortedArticles.sort((a, b) => b.issue_id - a.issue_id);
            } else if (sortBy === 'pub_date') {
                sortedArticles.sort((a, b) => b.pub_date.localeCompare(a.pub_date));
            } else {
                sortedArticles.sort((a, b) => b.score - a.score);
            }
            renderArticles(sortedArticles);
            localStorage.setItem('sort_by', sortBy);
        }
        
        sortDropdown.addEventListener('change', (event) => {
            sortBy = event.target.value;
            sortArticles(event.target.value);
        });

        categoryToggle.addEventListener('click', () => {
            categoryFiltersContainer.classList.toggle('expanded');
            setFilterOptionsVisibility();
        });

        clearCategoriesButton.addEventListener('click', () => {
            clearAllCategories();
            setFilterOptionsVisibility();
        });

        function setFilterOptionsVisibility() {
            if (selectedCategories.length > 0) {
                categoryFiltersLogicOptions.style.display = 'inline-block';
            } else {
                categoryFiltersLogicOptions.style.display = 'none';
            }
        } 
        
        function updateTimeDiffs() {
            const timeDiff = document.getElementById('timeDiff');
            timeDiff.innerHTML = '🔄 ' + getTimeDiff('2025-03-10 15:12',lang=currentLang);
        }
        function updateSortingOptions() {
            const sortingLabels = {
                ru: {
                    default: "рейтингу",
                    pub_date: "дате публикации",
                    issue_id: "добавлению на HF"
                },
                en: {
                    default: "rating",
                    pub_date: "publication date",
                    issue_id: "HF addition date"
                },
                zh: {
                    default: "评分",
                    pub_date: "发布日期",
                    issue_id: "HF上传日期"
                }
            };

            const dropdown = document.getElementById('sort-dropdown');
            const options = dropdown.options;

            for (let i = 0; i < options.length; i++) {
                const optionValue = options[i].value;
                console.log(sortingLabels)
                options[i].text = sortingLabels[currentLang][optionValue];
            }
        }
        function updateLocalization() {
            const titleDate = document.getElementById('title-date');
            const prevDate = document.getElementById('prev-date');
            const nextDate = document.getElementById('next-date');
            const topMonth = document.getElementById('top-month-label');
            const topDay = document.getElementById('top-day-label');
            const papersCount = document.getElementById('title-articles-count');
            const sortLabelText = document.getElementById('sort-label-text');
            titleDate.innerHTML = feedDate[currentLang];
            prevDate.innerHTML = feedDatePrev[currentLang];
            nextDate.innerHTML = feedDateNext[currentLang];
            papersCount.innerHTML = formatArticlesTitle(articlesData.length, currentLang);
            sortLabelText.innerHTML = sortLabel[currentLang];
            if (topMonth) {
                topMonth.innerHTML = topMonthLabel[currentLang];
            }  
            if (topDay) {
                topDay.innerHTML = topDayLabel[currentLang];
            }             
            updateSelectedArticlesTitle();
            updateSortingOptions();
        } 
        function hideNextLink(format) {
            if (format === 'monthly') {
                if (isCurrentMonth('2025-03-10 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            } else {            
                if (isToday('2025-03-10 15:12')) {
                    const element = document.getElementById('nav-next');
                    if (element) {    
                        element.style.display = 'none';
                    }
                }
            }
        }

        loadSettings();
        createCategoryButtons();
        loadCategorySelection();
        filterAndRenderArticles();
        updateSelectedArticlesTitle();
        updateTimeDiffs();
        hideNextLink('daily'); 
        initializeLanguageFlags();
        updateLocalization();
        setFilterOptionsVisibility();
    </script>
</body>
</html>
    